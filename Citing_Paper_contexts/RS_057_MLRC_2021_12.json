{
    "offset": 0,
    "data": [
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1dea89ecac64772dc43d8bb7337f851dc49f28a3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-08774",
                    "ArXiv": "2308.08774",
                    "DOI": "10.48550/arXiv.2308.08774",
                    "CorpusId": 260957084
                },
                "corpusId": 260957084,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1dea89ecac64772dc43d8bb7337f851dc49f28a3",
                "title": "Differential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models",
                "abstract": "Language models such as mBERT, XLM-R, and BLOOM aim to achieve multilingual generalization or compression to facilitate transfer to a large number of (potentially unseen) languages. However, these models should ideally also be private, linguistically fair, and transparent, by relating their predictions to training data. Can these requirements be simultaneously satisfied? We show that multilingual compression and linguistic fairness are compatible with differential privacy, but that differential privacy is at odds with training data influence sparsity, an objective for transparency. We further present a series of experiments on two common NLP tasks and evaluate multilingual compression and training data influence sparsity under different privacy guarantees, exploring these trade-offs in more detail. Our results suggest that we need to develop ways to jointly optimize for these objectives in order to find practical trade-offs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1660797358",
                        "name": "Phillip Rust"
                    },
                    {
                        "authorId": "1700187",
                        "name": "Anders S\u00f8gaard"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5cd4fe8174b582f27d79859e4b79e0eba2a1d5f0",
                "externalIds": {
                    "ArXiv": "2306.07656",
                    "DBLP": "journals/corr/abs-2306-07656",
                    "DOI": "10.48550/arXiv.2306.07656",
                    "CorpusId": 259144911
                },
                "corpusId": 259144911,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5cd4fe8174b582f27d79859e4b79e0eba2a1d5f0",
                "title": "Is Anisotropy Inherent to Transformers?",
                "abstract": "The representation degeneration problem is a phenomenon that is widely observed among self-supervised learning methods based on Transformers. In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity). Some recent works tend to show that anisotropy is a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens. We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences. We also show that the anisotropy problem extends to Transformers trained on other modalities. Our observations tend to demonstrate that anisotropy might actually be inherent to Transformers-based models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2196939517",
                        "name": "Nathan Godey"
                    },
                    {
                        "authorId": "1400417301",
                        "name": "Eric Villemonte de la Clergerie"
                    },
                    {
                        "authorId": "68990982",
                        "name": "Beno\u00eet Sagot"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "086ed6f81cabfbd5f8794d4f275847347ed56f42",
                "externalIds": {
                    "ArXiv": "2306.00458",
                    "DBLP": "conf/acl/HammerlFL023",
                    "DOI": "10.48550/arXiv.2306.00458",
                    "CorpusId": 258999913
                },
                "corpusId": 258999913,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/086ed6f81cabfbd5f8794d4f275847347ed56f42",
                "title": "Exploring Anisotropy and Outliers in Multilingual Language Models for Cross-Lingual Semantic Sentence Similarity",
                "abstract": "Previous work has shown that the representations output by contextual language models are more anisotropic than static type embeddings, and typically display outlier dimensions. This seems to be true for both monolingual and multilingual models, although much less work has been done on the multilingual context. Why these outliers occur and how they affect the representations is still an active area of research. We investigate outlier dimensions and their relationship to anisotropy in multiple pre-trained multilingual language models. We focus on cross-lingual semantic similarity tasks, as these are natural tasks for evaluating multilingual representations. Specifically, we examine sentence representations. Sentence transformers which are fine-tuned on parallel resources (that are not always available) perform better on this task, and we show that their representations are more isotropic. However, we aim to improve multilingual representations in general. We investigate how much of the performance difference can be made up by only transforming the embedding space without fine-tuning, and visualise the resulting spaces. We test different operations: Removing individual outlier dimensions, cluster-based isotropy enhancement, and ZCA whitening. We publish our code for reproducibility.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2159197037",
                        "name": "Katharina H\u00e4mmerl"
                    },
                    {
                        "authorId": "2219243831",
                        "name": "Alina Fastowski"
                    },
                    {
                        "authorId": "3448602",
                        "name": "Jind\u0159ich Libovick\u00fd"
                    },
                    {
                        "authorId": "2277248",
                        "name": "Alexander M. Fraser"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Several slight variations have occurred on the All-But-The-Top algorithm where the top principal components of the last hidden state of LLMs are masked or removed [18, 3, 14, 15, 23]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f9224fb07700b70e6b65012b41992d3d10fcd550",
                "externalIds": {
                    "ArXiv": "2305.19358",
                    "DBLP": "journals/corr/abs-2305-19358",
                    "DOI": "10.48550/arXiv.2305.19358",
                    "CorpusId": 258987812
                },
                "corpusId": 258987812,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f9224fb07700b70e6b65012b41992d3d10fcd550",
                "title": "Stable Anisotropic Regularization",
                "abstract": "Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few ``outlier dimensions'' with exceedingly high variance and magnitude. Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space). Isotropy is thought to be a desirable property for LLMs that improves model performance and more closely aligns textual representations with human intuition. However, many of the claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy. In this paper, we propose I-STAR: IsoScore*-based STable Anisotropic Regularization, a novel regularization method that can be used to increase or decrease levels of isotropy in embedding space during training. I-STAR uses IsoScore*, the first accurate measure of isotropy that is both differentiable and stable on mini-batch computations. In contrast to several previous works, we find that decreasing isotropy in contextualized embeddings improves performance on the majority of tasks and models considered in this paper.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2166313068",
                        "name": "William Rudman"
                    },
                    {
                        "authorId": "30044743",
                        "name": "Carsten Eickhoff"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Second, previous work has demonstrated that the word-level similarity estimates obtained from the vector space of contextualized LMs might be distorted due to the anisotropy of the space (Ethayarajh, 2019; Rajaee and Pilehvar, 2021).",
                "Anisotropic word representations occupy a narrow cone instead of being uniformly distributed in the vector space, resulting in highly positive correlations even for unrelated words, thus negatively impacting the quality of the similarity estimates that can be drawn from the space (Ethayarajh, 2019; Gao et al., 2019; Cai et al., 2021; Rajaee and Pilehvar, 2021)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e583a6f446511a72f7f1a764c3cfc9cb905a1c4d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-18657",
                    "ACL": "2023.starsem-1.32",
                    "ArXiv": "2305.18657",
                    "DOI": "10.48550/arXiv.2305.18657",
                    "CorpusId": 258967455
                },
                "corpusId": 258967455,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e583a6f446511a72f7f1a764c3cfc9cb905a1c4d",
                "title": "Representation of Lexical Stylistic Features in Language Models\u2019 Embedding Space",
                "abstract": "The representation space of pretrained Language Models (LMs) encodes rich information about words and their relationships (e.g., similarity, hypernymy, polysemy) as well as abstract semantic notions (e.g., intensity). In this paper, we demonstrate that lexical stylistic notions such as complexity, formality, and figurativeness, can also be identified in this space. We show that it is possible to derive a vector representation for each of these stylistic notions from only a small number of seed pairs. Using these vectors, we can characterize new texts in terms of these dimensions by performing simple calculations in the corresponding embedding space. We conduct experiments on five datasets and find that static embeddings encode these features more accurately at the level of words and phrases, whereas contextualized LMs perform better on sentences. The lower performance of contextualized representations at the word level is partially attributable to the anisotropy of their vector space, which can be corrected to some extent using techniques like standardization.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1904906987",
                        "name": "QING LYU"
                    },
                    {
                        "authorId": "2817917",
                        "name": "Marianna Apidianaki"
                    },
                    {
                        "authorId": "1763608",
                        "name": "Chris Callison-Burch"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "07d1a406fa8c3d343101ebf060950af879b99910",
                "externalIds": {
                    "DBLP": "journals/ipm/SasakiHSI23",
                    "DOI": "10.1016/j.ipm.2023.103272",
                    "CorpusId": 256266221
                },
                "corpusId": 256266221,
                "publicationVenue": {
                    "id": "37f5b9b7-f828-4ae1-a174-45b538cbd4e4",
                    "name": "Information Processing & Management",
                    "type": "journal",
                    "alternate_names": [
                        "Inf Process Manag",
                        "Inf Process  Manag",
                        "Information Processing and Management"
                    ],
                    "issn": "0306-4573",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/244/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/information-processing-and-management/",
                        "http://www.sciencedirect.com/science/journal/03064573",
                        "http://www.journals.elsevier.com/information-processing-and-management/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/07d1a406fa8c3d343101ebf060950af879b99910",
                "title": "Examining the effect of whitening on static and contextualized word embeddings",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47482840",
                        "name": "S. Sasaki"
                    },
                    {
                        "authorId": "2266692",
                        "name": "Benjamin Heinzerling"
                    },
                    {
                        "authorId": "153211541",
                        "name": "Jun Suzuki"
                    },
                    {
                        "authorId": "3040648",
                        "name": "Kentaro Inui"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "993e28920d7a546472f43c3ac4339649d0b9c7d2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-11309",
                    "ArXiv": "2301.11309",
                    "DOI": "10.48550/arXiv.2301.11309",
                    "CorpusId": 256274863
                },
                "corpusId": 256274863,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/993e28920d7a546472f43c3ac4339649d0b9c7d2",
                "title": "SemSup-XC: Semantic Supervision for Zero and Few-shot Extreme Classification",
                "abstract": "Extreme classification (XC) involves predicting over large numbers of classes (thousands to millions), with real-world applications like news article classification and e-commerce product tagging. The zero-shot version of this task requires generalization to novel classes without additional supervision. In this paper, we develop SemSup-XC, a model that achieves state-of-the-art zero-shot and few-shot performance on three XC datasets derived from legal, e-commerce, and Wikipedia data. To develop SemSup-XC, we use automatically collected semantic class descriptions to represent classes and facilitate generalization through a novel hybrid matching module that matches input instances to class descriptions using a combination of semantic and lexical similarity. Trained with contrastive learning, SemSup-XC significantly outperforms baselines and establishes state-of-the-art performance on all three datasets considered, gaining up to 12 precision points on zero-shot and more than 10 precision points on one-shot tests, with similar gains for recall@10. Our ablation studies highlight the relative importance of our hybrid matching module and automatically collected class descriptions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2114841965",
                        "name": "Pranjal Aggarwal"
                    },
                    {
                        "authorId": "33341943",
                        "name": "A. Deshpande"
                    },
                    {
                        "authorId": "2135381714",
                        "name": "Karthik Narasimhan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8db5adadc0ab39f95a26a2eb6499d340d6c5ea21",
                "externalIds": {
                    "ACL": "2023.cl-2.7",
                    "DBLP": "journals/coling/Apidianaki23",
                    "DOI": "10.1162/coli_a_00474",
                    "CorpusId": 254960044
                },
                "corpusId": 254960044,
                "publicationVenue": {
                    "id": "30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
                    "name": "International Conference on Computational Logic",
                    "type": "conference",
                    "alternate_names": [
                        "CL",
                        "Int Conf Comput Log"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8db5adadc0ab39f95a26a2eb6499d340d6c5ea21",
                "title": "From Word Types to Tokens and Back: A Survey of Approaches to Word Meaning Representation and Interpretation",
                "abstract": "Vector-based word representation paradigms situate lexical meaning at different levels of abstraction. Distributional and static embedding models generate a single vector per word type, which is an aggregate across the instances of the word in a corpus. Contextual language models, on the contrary, directly capture the meaning of individual word instances. The goal of this survey is to provide an overview of word meaning representation methods, and of the strategies that have been proposed for improving the quality of the generated vectors. These often involve injecting external knowledge about lexical semantic relationships, or refining the vectors to describe different senses. The survey also covers recent approaches for obtaining word type-level representations from token-level ones, and for combining static and contextualized representations. Special focus is given to probing and interpretation studies aimed at discovering the lexical semantic knowledge that is encoded in contextualized representations. The challenges posed by this exploration have motivated the interest towards static embedding derivation from contextualized embeddings, and for methods aimed at improving the similarity estimates that can be drawn from the space of contextual language models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2817917",
                        "name": "Marianna Apidianaki"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Some work suggest that the high anisotropy is inherent to, or least a by-product of contextualization [73, 79], Gao et al."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1dd5beb70fe2a4ace695d3fc7f1fb0c53f757d87",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-03834",
                    "ArXiv": "2209.03834",
                    "DOI": "10.48550/arXiv.2209.03834",
                    "CorpusId": 252118723
                },
                "corpusId": 252118723,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1dd5beb70fe2a4ace695d3fc7f1fb0c53f757d87",
                "title": "Pre-Training a Graph Recurrent Network for Language Representation",
                "abstract": "Transformer-based pre-trained models have gained much advance in recent years, becoming one of the most important backbones in natural language processing. Recent work shows that the attention mechanism inside Transformer may not be necessary, both convolutional neural networks and multi-layer perceptron based models have also been investigated as Transformer alternatives. In this paper, we consider a graph recurrent network for language model pre-training, which builds a graph structure for each sequence with local token-level communications, together with a sentence-level representation decoupled from other tokens. The original model performs well in domain-specific text classification under supervised training, however, its potential in learning transfer knowledge by self-supervised way has not been fully exploited. We fill this gap by optimizing the architecture and verifying its effectiveness in more general language understanding tasks, for both English and Chinese languages. As for model efficiency, instead of the quadratic complexity in Transformer-based models, our model has linear complexity and performs more efficiently during inference. Moreover, we find that our model can generate more diverse outputs with less contextualized feature redundancy than existing attention-based models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108102904",
                        "name": "Yile Wang"
                    },
                    {
                        "authorId": "2145500840",
                        "name": "Linyi Yang"
                    },
                    {
                        "authorId": "2272668",
                        "name": "Zhiyang Teng"
                    },
                    {
                        "authorId": "143849609",
                        "name": "M. Zhou"
                    },
                    {
                        "authorId": "39939186",
                        "name": "Yue Zhang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bde36696e15210050b5a73c90750cbe1204a120f",
                "externalIds": {
                    "DOI": "10.54097/hset.v7i.1094",
                    "CorpusId": 251534781
                },
                "corpusId": 251534781,
                "publicationVenue": {
                    "id": "8139fd99-9b3e-49a1-a8f1-f73376f8bd1a",
                    "name": "Highlights in Science Engineering and Technology",
                    "alternate_names": [
                        "Highlight Sci Eng Technol"
                    ],
                    "issn": "2791-0210"
                },
                "url": "https://www.semanticscholar.org/paper/bde36696e15210050b5a73c90750cbe1204a120f",
                "title": "A survey of text classification: problem statement, latest methods and popular datasets",
                "abstract": "Considering the important role text classification plays in natural language processing tasks, improving the accuracy and efficiency of text classification has been a priority in recent work. In this paper, we focus on the latest text classification methods and sort them into three categories: embedding methods, language models, and various neural networks. We summarize the state of current research and the insufficiencies which may be directions for future study.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2055920926",
                        "name": "Siyu Tian"
                    },
                    {
                        "authorId": "2152664623",
                        "name": "Xinyao Huang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "477ca6071e7ecee225580e61ab6ff6166a82d06d",
                "externalIds": {
                    "ArXiv": "2205.07208",
                    "DBLP": "conf/naacl/ZhangLZZ0LL22",
                    "ACL": "2022.naacl-main.39",
                    "DOI": "10.48550/arXiv.2205.07208",
                    "CorpusId": 248810745
                },
                "corpusId": 248810745,
                "publicationVenue": {
                    "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
                    "name": "North American Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "North Am Chapter Assoc Comput Linguistics",
                        "NAACL"
                    ],
                    "url": "https://www.aclweb.org/portal/naacl"
                },
                "url": "https://www.semanticscholar.org/paper/477ca6071e7ecee225580e61ab6ff6166a82d06d",
                "title": "Fine-tuning Pre-trained Language Models for Few-shot Intent Detection: Supervised Pre-training and Isotropization",
                "abstract": "It is challenging to train a good intent classifier for a task-oriented dialogue system with only a few annotations. Recent studies have shown that fine-tuning pre-trained language models with a small set of labeled utterances from public benchmarks in a supervised manner is extremely helpful. However, we find that supervised pre-training yields an anisotropic feature space, which may suppress the expressive power of the semantic representations. Inspired by recent research in isotropization, we propose to improve supervised pre-training by regularizing the feature space towards isotropy. We propose two regularizers based on contrastive learning and correlation matrix respectively, and demonstrate their effectiveness through extensive experiments. Our main finding is that it is promising to regularize supervised pre-training with isotropization to further improve the performance of few-shot intent detection. The source code can be found at https://github.com/fanolabs/isoIntentBert-main.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2135719214",
                        "name": "Haode Zhang"
                    },
                    {
                        "authorId": "2152874181",
                        "name": "Haowen Liang"
                    },
                    {
                        "authorId": "2108424077",
                        "name": "Yuwei Zhang"
                    },
                    {
                        "authorId": "2061239456",
                        "name": "Li-Ming Zhan"
                    },
                    {
                        "authorId": "19195265",
                        "name": "Xiao-Ming Wu"
                    },
                    {
                        "authorId": "1491232153",
                        "name": "Xiaolei Lu"
                    },
                    {
                        "authorId": "1902169",
                        "name": "Albert Y. S. Lam"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "Because of the re-training cost, other light approaches have been presented as a post-processing step (Li et al., 2020; Rajaee and Pilehvar, 2021).",
                "To investigate the effect of isotropy enhancement for the multilingual embedding space, we opted for our cluster-based approach (Rajaee and Pilehvar, 2021), which is a recent example from the latter category.",
                "By applying a cluster-based isotropy enhancement method (Rajaee and Pilehvar, 2021), we demonstrate that increasing isotropy of multilingual embedding space can result in significant performance improvements on semantic textual similarity tasks.",
                "A similar pattern can be observed for the English BERT CWRs (Rajaee and Pilehvar, 2021), with the only",
                "A similar pattern can be observed for the English BERT CWRs (Rajaee and Pilehvar, 2021), with the only difference that in mBERT, low-frequency words are distributed near the origin and frequent words are far from it."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1234fcc1577a32b829d2886fdf68375b9d4525e9",
                "externalIds": {
                    "ACL": "2022.findings-acl.103",
                    "DBLP": "journals/corr/abs-2110-04504",
                    "ArXiv": "2110.04504",
                    "DOI": "10.18653/v1/2022.findings-acl.103",
                    "CorpusId": 238583197
                },
                "corpusId": 238583197,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1234fcc1577a32b829d2886fdf68375b9d4525e9",
                "title": "An Isotropy Analysis in the Multilingual BERT Embedding Space",
                "abstract": "Several studies have explored various advantages of multilingual pre-trained models (such as multilingual BERT) in capturing shared linguistic knowledge. However, less attention has been paid to their limitations. In this paper, we investigate the multilingual BERT for two known issues of the monolingual models: anisotropic embedding space and outlier dimensions. We show that, unlike its monolingual counterpart, the multilingual BERT model exhibits no outlier dimension in its representations while it has a highly anisotropic space. There are a few dimensions in the monolingual BERT with high contributions to the anisotropic distribution. However, we observe no such dimensions in the multilingual BERT. Furthermore, our experimental results demonstrate that increasing the isotropy of multilingual space can significantly improve its representation power and performance, similarly to what had been observed for monolingual CWRs on semantic similarity tasks. Our analysis indicates that, despite having different degenerated directions, the embedding spaces in various languages tend to be partially similar with respect to their structures.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "31280249",
                        "name": "S. Rajaee"
                    },
                    {
                        "authorId": "1717641",
                        "name": "Mohammad Taher Pilehvar"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6fcd0907b3660ed1f21bfcf6131630f7d5309a9f",
                "externalIds": {
                    "ACL": "2021.conll-1.44",
                    "ArXiv": "2109.09237",
                    "DBLP": "journals/corr/abs-2109-09237",
                    "DOI": "10.18653/v1/2021.conll-1.44",
                    "CorpusId": 237571775
                },
                "corpusId": 237571775,
                "publicationVenue": {
                    "id": "3779a5a7-9119-4f69-84fe-f7eef193eb49",
                    "name": "Conference on Computational Natural Language Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoNLL",
                        "Conf Comput Nat Lang Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6fcd0907b3660ed1f21bfcf6131630f7d5309a9f",
                "title": "MirrorWiC: On Eliciting Word-in-Context Representations from Pretrained Language Models",
                "abstract": "Recent work indicated that pretrained language models (PLMs) such as BERT and RoBERTa can be transformed into effective sentence and word encoders even via simple self-supervised techniques. Inspired by this line of work, in this paper we propose a fully unsupervised approach to improving word-in-context (WiC) representations in PLMs, achieved via a simple and efficient WiC-targeted fine-tuning procedure: MirrorWiC. The proposed method leverages only raw texts sampled from Wikipedia, assuming no sense-annotated data, and learns context-aware word representations within a standard contrastive learning setup. We experiment with a series of standard and comprehensive WiC benchmarks across multiple languages. Our proposed fully unsupervised MirrorWiC models obtain substantial gains over off-the-shelf PLMs across all monolingual, multilingual and cross-lingual setups. Moreover, on some standard WiC benchmarks, MirrorWiC is even on-par with supervised models fine-tuned with in-task data and sense labels.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145484051",
                        "name": "Qianchu Liu"
                    },
                    {
                        "authorId": "144097210",
                        "name": "Fangyu Liu"
                    },
                    {
                        "authorId": "50638196",
                        "name": "Nigel Collier"
                    },
                    {
                        "authorId": "145762466",
                        "name": "A. Korhonen"
                    },
                    {
                        "authorId": "1747849",
                        "name": "Ivan Vulic"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                ", 2019), this method can significantly improve the performance of contextual embedding spaces as well as their isotropy (Rajaee and Pilehvar, 2021)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ceef266c59698999c9283a0cda852d8bc1ce27ea",
                "externalIds": {
                    "ArXiv": "2109.04740",
                    "DBLP": "conf/emnlp/RajaeeP21",
                    "DOI": "10.18653/v1/2021.findings-emnlp.261",
                    "CorpusId": 237485161
                },
                "corpusId": 237485161,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/ceef266c59698999c9283a0cda852d8bc1ce27ea",
                "title": "How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy",
                "abstract": "It is widely accepted that fine-tuning pre-trained language models usually brings about performance improvements in downstream tasks. However, there are limited studies on the reasons behind this effectiveness, particularly from the viewpoint of structural changes in the embedding space. Trying to fill this gap, in this paper, we analyze the extent to which the isotropy of the embedding space changes after fine-tuning. We demonstrate that, even though isotropy is a desirable geometrical property, fine-tuning does not necessarily result in isotropy enhancements. Moreover, local structures in pre-trained contextual word representations (CWRs), such as those encoding token types or frequency, undergo a massive change during fine-tuning. Our experiments show dramatic growth in the number of elongated directions in the embedding space, which, in contrast to pre-trained CWRs, carry the essential linguistic knowledge in the fine-tuned embedding space, making existing isotropy enhancement methods ineffective.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "31280249",
                        "name": "S. Rajaee"
                    },
                    {
                        "authorId": "1717641",
                        "name": "Mohammad Taher Pilehvar"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "embedding space performs well in downstream tasks(Bi\u015b et al., 2021; Rajaee and Pilehvar, 2021)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bb30fd75dabfa3f31dc7c73b9c5e636416813c8c",
                "externalIds": {
                    "DBLP": "conf/acl/YuSK0RY22",
                    "ACL": "2022.acl-long.3",
                    "ArXiv": "2109.03127",
                    "DOI": "10.18653/v1/2022.acl-long.3",
                    "CorpusId": 247476436
                },
                "corpusId": 247476436,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/bb30fd75dabfa3f31dc7c73b9c5e636416813c8c",
                "title": "Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via Adaptive Gradient Gating for Rare Token Embeddings",
                "abstract": "Recent studies have determined that the learned token embeddings of large-scale neural language models are degenerated to be anisotropic with a narrow-cone shape. This phenomenon, called the representation degeneration problem, facilitates an increase in the overall similarity between token embeddings that negatively affect the performance of the models. Although the existing methods that address the degeneration problem based on observations of the phenomenon triggered by the problem improves the performance of the text generation, the training dynamics of token embeddings behind the degeneration problem are still not explored. In this study, we analyze the training dynamics of the token embeddings focusing on rare token embedding. We demonstrate that the specific part of the gradient for rare token embeddings is the key cause of the degeneration problem for all tokens during training stage. Based on the analysis, we propose a novel method called, adaptive gradient gating(AGG). AGG addresses the degeneration problem by gating the specific part of the gradient for rare token embeddings. Experimental results from language modeling, word similarity, and machine translation tasks quantitatively and qualitatively verify the effectiveness of AGG.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2148401281",
                        "name": "Sangwon Yu"
                    },
                    {
                        "authorId": "10788591",
                        "name": "Jongyoon Song"
                    },
                    {
                        "authorId": "2115191328",
                        "name": "Heeseung Kim"
                    },
                    {
                        "authorId": "2108643540",
                        "name": "SeongEun Lee"
                    },
                    {
                        "authorId": "2188063",
                        "name": "Woo-Jong Ryu"
                    },
                    {
                        "authorId": "2152497729",
                        "name": "Sung-Hoon Yoon"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2435c04832d486975304a094e55ecbab8acf8a5f",
                "externalIds": {
                    "ACL": "2021.emnlp-main.109",
                    "DBLP": "journals/corr/abs-2104-08027",
                    "ArXiv": "2104.08027",
                    "DOI": "10.18653/v1/2021.emnlp-main.109",
                    "CorpusId": 233289620
                },
                "corpusId": 233289620,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/2435c04832d486975304a094e55ecbab8acf8a5f",
                "title": "Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders",
                "abstract": "Previous work has indicated that pretrained Masked Language Models (MLMs) are not effective as universal lexical and sentence encoders off-the-shelf, i.e., without further task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective lexical and sentence encoders even without any additional data, relying simply on self-supervision. We propose an extremely simple, fast, and effective contrastive learning technique, termed Mirror-BERT, which converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30 seconds with no access to additional external knowledge. Mirror-BERT relies on identical and slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during \u201cidentity fine-tuning\u201d. We report huge gains over off-the-shelf MLMs with Mirror-BERT both in lexical-level and in sentence-level tasks, across different domains and different languages. Notably, in sentence similarity (STS) and question-answer entailment (QNLI) tasks, our self-supervised Mirror-BERT model even matches the performance of the Sentence-BERT models from prior work which rely on annotated task data. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple Mirror-BERT fine-tuning approach can yield effective universal lexical and sentence encoders.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144097210",
                        "name": "Fangyu Liu"
                    },
                    {
                        "authorId": "1747849",
                        "name": "Ivan Vulic"
                    },
                    {
                        "authorId": "145762466",
                        "name": "A. Korhonen"
                    },
                    {
                        "authorId": "50638196",
                        "name": "Nigel Collier"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Secondly, Rajaee & Pilehvar (2021a) increased the isotropy by clustering the embeddings and nulling the principal components of each cluster.",
                "Furthermore, Rajaee & Pilehvar (2021b) showed that improving the isotropy, in general, does not immediately result in a better performance for the model.",
                "As mentioned previously, improving the isotropy by itself is not sufficient (Rajaee & Pilehvar, 2021b).",
                "As shown by Rajaee & Pilehvar (2021b), it is not sufficient to only improve the isotropy of the embeddings, as the embeddings need to maintain the semantics required for the downstream task.",
                "More specifically, Rajaee & Pilehvar (2021b) highlighted that the Classification ([CLS]) token representations are much more anisotropic than all representations in the fine-tuned space."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "3d67937d4d4013a9951f73c56b42a9ffe1d2aceb",
                "externalIds": {
                    "DBLP": "conf/icdar/AttiehZVFB23",
                    "DOI": "10.1007/978-3-031-41734-4_8",
                    "CorpusId": 260004414
                },
                "corpusId": 260004414,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/3d67937d4d4013a9951f73c56b42a9ffe1d2aceb",
                "title": "Optimizing the Performance of Text Classification Models by Improving the Isotropy of the Embeddings Using a Joint Loss Function",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2198247487",
                        "name": "Joseph Attieh"
                    },
                    {
                        "authorId": "51129595",
                        "name": "Abraham Woubie Zewoudie"
                    },
                    {
                        "authorId": "145852480",
                        "name": "Vladimir Vlassov"
                    },
                    {
                        "authorId": "144795201",
                        "name": "Adrian Flanagan"
                    },
                    {
                        "authorId": "2062891014",
                        "name": "Tomas B\u00e4ckstr\u00f6m"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The (Rajaee and Pilehvar, 2021) work demonstrates that isotropic embeddings have a significant improvement in downstream task performance, as they capture more semantic information and reduce noise."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "097c59467df301a34660f91b9dd2ca76507d2b30",
                "externalIds": {
                    "DBLP": "conf/semeval/OskueeRGS23",
                    "ACL": "2023.semeval-1.82",
                    "DOI": "10.18653/v1/2023.semeval-1.82",
                    "CorpusId": 259376508
                },
                "corpusId": 259376508,
                "publicationVenue": {
                    "id": "70713d09-6e4b-4554-9d3f-94d08aba320c",
                    "name": "International Workshop on Semantic Evaluation",
                    "type": "conference",
                    "alternate_names": [
                        "SemEval ",
                        "Int Workshop Semantic Evaluation"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/097c59467df301a34660f91b9dd2ca76507d2b30",
                "title": "T.M. Scanlon at SemEval-2023 Task 4: Leveraging Pretrained Language Models for Human Value Argument Mining with Contrastive Learning",
                "abstract": "Human values are of great concern to social sciences which refer to when people have different beliefs and priorities of what is generally worth striving for and how to do so. This paper presents an approach for human value argument mining using contrastive learning to leverage the isotropy of language models. We fine-tuned DeBERTa-Large in a multi-label classification fashion and achieved an F1 score of 49% for the task, resulting in a rank of 11. Our proposed model provides a valuable tool for analyzing arguments related to human values and highlights the significance of leveraging the isotropy of large language models for identifying human values.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2221316009",
                        "name": "Milad Molazadeh Oskuee"
                    },
                    {
                        "authorId": "51160349",
                        "name": "Mostafa Rahgouy"
                    },
                    {
                        "authorId": "51164027",
                        "name": "Hamed Babaei Giglou"
                    },
                    {
                        "authorId": "2221317251",
                        "name": "Cheryl D. Seals"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "Multiple research efforts have shown that architectures that utilise contextual and non-contextual word representations lack isotropy (i.e anisotropic) (Devlin et al., 2018) (Rajaee and Pilehvar, 2021b)(Rajaee and Pilehvar, 2021a).",
                ", 2018) (Rajaee and Pilehvar, 2021b)(Rajaee and Pilehvar, 2021a).",
                "We follow (Mu et al., 2017) (Rajaee and Pilehvar, 2021a) in quantifying isotropy using the metric in 1, and the partition function 2 by (Arora et al., 2016):\nI(W ) = minu\u2208UF (u)\nmaxu\u2208UF (u) (1)\nF (u) = N\u2211 i=1 eu Twi (2)\nwhere:\n\u2022 U : is the set of eigenvectors of the embedding matrix W\n\u2022 u: is the\u2026",
                ", 2017) (Rajaee and Pilehvar, 2021a) in quantifying isotropy using the metric in 1, and the partition function 2 by (Arora et al.",
                "Experiments in (Rajaee and Pilehvar, 2021a) illustrated the use of such method which improved classification performance and surpassed baseline values in pretrained language models, particularly, BERT(Devlin et al., 2018).",
                "(Ioffe and Szegedy, 2015)(Wang et al., 2019) (Rajaee and Pilehvar, 2021a).",
                "\u2026word representation of the ith word in embedding matrix W where W \u2208 RN\u00d7D with N being the size of the vocabulary and D the size of the embedding\nAs cited by (Rajaee and Pilehvar, 2021a), (Arora et al., 2016) proved that using a constant for isotropic embedding spaces, F (u) can be\u2026",
                "We use (Rajaee and Pilehvar, 2021a)\u2019s cluster-based approach in our work to achieve an embedding space with representations homogeneously dispersed.",
                "To refine MARBERT\u2019s isotropy, we use a clusterbased approach (Rajaee and Pilehvar, 2021a) which builds on top of (Mu et al., 2017) technique to improve isotropy in non-contextual word embeddings.",
                "To refine MARBERT\u2019s isotropy, we use a clusterbased approach (Rajaee and Pilehvar, 2021a) which builds on top of (Mu et al.",
                "Experiments in (Rajaee and Pilehvar, 2021a) illustrated the use of such method which improved classification performance and surpassed baseline values in pretrained language models, particularly, BERT(Devlin et al."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "431989d4d467df0a22df7c118fe8a7233c6dff2f",
                "externalIds": {
                    "ACL": "2022.osact-1.27",
                    "CorpusId": 252624554
                },
                "corpusId": 252624554,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/431989d4d467df0a22df7c118fe8a7233c6dff2f",
                "title": "GUCT at Arabic Hate Speech 2022: Towards a Better Isotropy for Hatespeech Detection",
                "abstract": "Hate Speech is an increasingly common occurrence in verbal and textual exchanges on online platforms, where many users, especially those from vulnerable minorities, are in danger of being attacked or harassed via text messages, posts, comments, or articles. Therefore, it is crucial to detect and filter out hate speech in the various forms of text encountered on online and social platforms. In this paper, we present our work on the shared task of detecting hate speech in dialectical Arabic tweets as part of the OSACT shared task on Fine-grained Hate Speech Detection. Normally, tweets have a short length, and hence do not have sufficient context for language models, which in turn makes a classification task challenging. To contribute to sub-task A, we leverage MARBERT\u2019s pre-trained contextual word representations and aim to improve their semantic quality using a cluster-based approach. Our work explores MARBERT\u2019s embedding space and assess its geometric properties in-order to achieve better representations and subsequently better classification performance. We propose to improve the isotropic word representations of MARBERT via clustering. we compare the word representations generated by our approach to MARBERT\u2019s default word representations via feeding each to a bidirectional LSTM to detect offensive and non-offensive tweets. Our results show that enhancing the isotropy of an embedding space can boost performance. Our system scores 81.2% on accuracy and a macro-averaged F1 score of 79.1% on sub-task A\u2019s development set and achieves 76.5% for accuracy and an F1 score of 74.2% on the test set.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2186551655",
                        "name": "Nehal Elkaref"
                    },
                    {
                        "authorId": "1402984321",
                        "name": "Mervat Abu-Elkheir"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7450a612ef291216b0cd48e09b8879be4675c6eb",
                "externalIds": {
                    "DBLP": "conf/acl/ChangM22",
                    "ACL": "2022.acl-long.554",
                    "DOI": "10.18653/v1/2022.acl-long.554",
                    "CorpusId": 248780434
                },
                "corpusId": 248780434,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/7450a612ef291216b0cd48e09b8879be4675c6eb",
                "title": "Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions",
                "abstract": "Neural language models (LMs) such as GPT-2 estimate the probability distribution over the next word by a softmax over the vocabulary. The softmax layer produces the distribution based on the dot products of a single hidden state and the embeddings of words in the vocabulary. However, we discover that this single hidden state cannot produce all probability distributions regardless of the LM size or training data size because the single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously when there are other interfering word embeddings between them. In this work, we demonstrate the importance of this limitation both theoretically and practically. Our work not only deepens our understanding of softmax bottleneck and mixture of softmax (MoS) but also inspires us to propose multi-facet softmax (MFS) to address the limitations of MoS. Extensive empirical analyses confirm our findings and show that against MoS, the proposed MFS achieves two-fold improvements in the perplexity of GPT-2 and BERT.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144827671",
                        "name": "Haw-Shiuan Chang"
                    },
                    {
                        "authorId": "143753639",
                        "name": "A. McCallum"
                    }
                ]
            }
        }
    ]
}