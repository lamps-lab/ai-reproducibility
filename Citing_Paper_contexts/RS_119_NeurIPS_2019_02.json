{
    "offset": 0,
    "data": [
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "14e1bdeebeb0dff97ce0431a4e316ddec4981e66",
                "externalIds": {
                    "ArXiv": "2310.00034",
                    "CorpusId": 263333921
                },
                "corpusId": 263333921,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/14e1bdeebeb0dff97ce0431a4e316ddec4981e66",
                "title": "PB-LLM: Partially Binarized Large Language Models",
                "abstract": "This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, i.e., partially-binarization. PB-LLM is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts from GPTQ, we reconstruct the binarized weight matrix guided by the Hessian matrix and successfully recover the reasoning capacity of PB-LLM in low-bit. Under QAT, we freeze the salient weights during training, explore the derivation of optimal scaling factors crucial for minimizing the quantization error, and propose a scaling mechanism based on this derived scaling strategy for residual binarized weights. Those explorations and the developed methodologies significantly contribute to rejuvenating the performance of low-bit quantized LLMs and present substantial advancements in the field of network binarization for LLMs.The code is available at https://github.com/hahnyuan/BinaryLLM.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2249764234",
                        "name": "Yuzhang Shang"
                    },
                    {
                        "authorId": "2256166978",
                        "name": "Zhihang Yuan"
                    },
                    {
                        "authorId": "2249848412",
                        "name": "Qiang Wu"
                    },
                    {
                        "authorId": "2256465619",
                        "name": "Zhen Dong"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Among them, network quantization [8, 44, 32, 21, 1] is a kind of excellent method with high compression ratio and little performance degradation."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6ea40abfa887709e2b8da79c35ec75e4003291fa",
                "externalIds": {
                    "ArXiv": "2308.06689",
                    "DBLP": "journals/corr/abs-2308-06689",
                    "DOI": "10.48550/arXiv.2308.06689",
                    "CorpusId": 260887569
                },
                "corpusId": 260887569,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6ea40abfa887709e2b8da79c35ec75e4003291fa",
                "title": "Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training",
                "abstract": "Binarization of neural networks is a dominant paradigm in neural networks compression. The pioneering work BinaryConnect uses Straight Through Estimator (STE) to mimic the gradients of the sign function, but it also causes the crucial inconsistency problem. Most of the previous methods design different estimators instead of STE to mitigate it. However, they ignore the fact that when reducing the estimating error, the gradient stability will decrease concomitantly. These highly divergent gradients will harm the model training and increase the risk of gradient vanishing and gradient exploding. To fully take the gradient stability into consideration, we present a new perspective to the BNNs training, regarding it as the equilibrium between the estimating error and the gradient stability. In this view, we firstly design two indicators to quantitatively demonstrate the equilibrium phenomenon. In addition, in order to balance the estimating error and the gradient stability well, we revise the original straight through estimator and propose a power function based estimator, Rectified Straight Through Estimator (ReSTE for short). Comparing to other estimators, ReSTE is rational and capable of flexibly balancing the estimating error with the gradient stability. Extensive experiments on CIFAR-10 and ImageNet datasets show that ReSTE has excellent performance and surpasses the state-of-the-art methods without any auxiliary modules or losses.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2224384249",
                        "name": "Xiao-Ming Wu"
                    },
                    {
                        "authorId": "2224337700",
                        "name": "Dian Zheng"
                    },
                    {
                        "authorId": "1720679125",
                        "name": "Zuhao Liu"
                    },
                    {
                        "authorId": "3333315",
                        "name": "Weishi Zheng"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e76b9b74738c74fe36efc5291c5c4d96338e3ffa",
                "externalIds": {
                    "DBLP": "conf/icassp/Milioris23",
                    "DOI": "10.1109/ICASSPW59220.2023.10193347",
                    "CorpusId": 260387307
                },
                "corpusId": 260387307,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e76b9b74738c74fe36efc5291c5c4d96338e3ffa",
                "title": "Efficient Moving Target Detection Using Resource-Constrained Neural Networks",
                "abstract": "In recent years, the widespread use of autonomous vehicles, such as aerial and automotive, has enhanced our abilities to perform target tracking, dispensing our over-reliance on visual features. With the development of computer vision and deep learning techniques, vision-based classification and recognition have recently received special attention in the scientific community. Moreover, recent advances in the field of neural networks with quantized weights and activations down to single bit precision have allowed the development of models that can be deployed in resource-constrained settings, where a trade-off between task performance and efficiency is accepted. In this work we design an efficient single stage object detector based on CenterNet containing a combination of full precision and binary layers. Our model is easy to train and achieves comparable results with a full precision network trained from scratch while requiring an order of magnitude less FLOP. This opens the possibility of deploying an object detector in applications where time is of the essence and a graphical processing unit (GPU) is absent. We train our model and evaluate its performance by comparing with state-of-the-art techniques, obtaining higher accurate results and provide an insight into the design process of resource constrained neural networks involving trade-offs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2117291",
                        "name": "Dimitris Milioris"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Continuous weights in our framework are updated with Adam optimisation while the binary weights in R are updated using the Bop algorithm proposed by [14].",
                "This is achieved by first learning the edges or equivalently the relations/partOf predicate with a binary layer [14] between symbols in consecutive codebooks with weights: wl \u2208 RMi\u00d7Mi+1 \u2208 {0, 1}.",
                "This is achieved in our works using a binary neural network [14]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6c496ccde44486dabf1b6eb965b7825858172536",
                "externalIds": {
                    "DBLP": "conf/cvpr/SanthirasekaramKWRTG23",
                    "DOI": "10.1109/CVPRW59228.2023.00063",
                    "CorpusId": 260912788
                },
                "corpusId": 260912788,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6c496ccde44486dabf1b6eb965b7825858172536",
                "title": "Robust Hierarchical Symbolic Explanations in Hyperbolic Space for Image Classification",
                "abstract": "Explanations for black-box models help us to understand model decisions as well as provide information on model biases and inconsistencies. Most of the current post-hoc explainability techniques provide a single level of explanation, often in terms of feature importance scores or feature attention maps in the input space. The explanations provided by these methods are also sensitive to perturbations in the input space. Our focus is on explaining deep discriminative models for images at multiple levels of abstraction, from fine-grained to fully abstract explanations. We use the natural properties of hyperbolic geometry to more efficiently model a hierarchical relationship of symbolic features with decreased distortion to generate robust hierarchical explanations. Specifically, we distill the underpinning knowledge in an image classifier by quantising the continuous latent space to form hyperbolic symbols and learn the relations between these symbols in a hierarchical manner to induce a knowledge tree. We traverse the tree to extract hierarchical explanations in terms of chains of symbols and their corresponding visual semantics. Code is available at https://github.com/AinkaranSanthi/HyperbolicReasoning",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "9696089",
                        "name": "Ainkaran Santhirasekaram"
                    },
                    {
                        "authorId": "35982249",
                        "name": "A. Kori"
                    },
                    {
                        "authorId": "2174812191",
                        "name": "Mathias Winkler"
                    },
                    {
                        "authorId": "2064337286",
                        "name": "A. Rockall"
                    },
                    {
                        "authorId": "49973505",
                        "name": "Francesca Toni"
                    },
                    {
                        "authorId": "1709824",
                        "name": "Ben Glocker"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f86e7962e6722acd3beb4d8cd0dfc0f61b907271",
                "externalIds": {
                    "DBLP": "journals/ijon/ShanZCW23",
                    "DOI": "10.1016/j.neucom.2023.126431",
                    "CorpusId": 259128664
                },
                "corpusId": 259128664,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f86e7962e6722acd3beb4d8cd0dfc0f61b907271",
                "title": "SGDAT: An optimization method for binary neural networks",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219842906",
                        "name": "Gu Shan"
                    },
                    {
                        "authorId": "2115981711",
                        "name": "Guoyin Zhang"
                    },
                    {
                        "authorId": "2239511010",
                        "name": "Chengwei Jia"
                    },
                    {
                        "authorId": "48607553",
                        "name": "Yanxia Wu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Bop [28] and UniQ [29] each propose a new optimizer for training BNN."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "297510f0785b8df3ac86fe193a2708ddb6b3b218",
                "externalIds": {
                    "DOI": "10.1155/2023/5870630",
                    "CorpusId": 258862479
                },
                "corpusId": 258862479,
                "publicationVenue": {
                    "id": "6b6df2de-21bc-4137-9859-3fcef46f6a21",
                    "name": "Mobile Information Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Mob Inf Syst"
                    ],
                    "issn": "1574-017X",
                    "url": "https://www.hindawi.com/journals/misy/",
                    "alternate_urls": [
                        "https://www.iospress.nl/journal/mobile-information-systems/",
                        "https://www.iospress.nl/html/1574017x.php",
                        "http://content.iospress.com/journals/mobile-information-systems"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/297510f0785b8df3ac86fe193a2708ddb6b3b218",
                "title": "A Lightweight Binarized Convolutional Neural Network Model for Small Memory and Low-Cost Mobile Devices",
                "abstract": "In recent years, the high cost of implementing deep neural networks due to their large model size and parameter complexity has made it a challenging problem to design lightweight models that reduce application costs. The existing binarized neural networks suffer from both the large memory occupancy and the big number of trainable params they use. We propose a lightweight binarized convolutional neural network (CBCNN) model to address the multiclass classification/identification problem. We use both binary weights and activation. We show experimentally that a model using only 0.59\u2009M trainable params is sufficient to reach about 92.94% accuracy on the GTSRB dataset, and it has similar performances compared to other methods on MNIST and Fashion-MNIST datasets. Accordingly, most arithmetic operations with bitwise operations are simplified, thus both used memory size and memory accesses are reduced by 32 times. Moreover, the color information was removed, which also reduced drastically the training time. All these together allow our architecture to run effectively and in real time on simple CPUs (rather than GPUs). Through the results we obtained, we show that despite simplifications and color information removal, our network achieves similar performances compared to classical CNNs with lower costs in both in training and embedded deployment.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1388659627",
                        "name": "Xuan Qi"
                    },
                    {
                        "authorId": "2178563968",
                        "name": "Zegang Sun"
                    },
                    {
                        "authorId": "2067541735",
                        "name": "Xue Mei"
                    },
                    {
                        "authorId": "3027401",
                        "name": "R. Chellali"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "061c281d9621c7fd9302d5a1adb37ea528a4d53d",
                "externalIds": {
                    "ArXiv": "2304.00952",
                    "DBLP": "journals/corr/abs-2304-00952",
                    "DOI": "10.48550/arXiv.2304.00952",
                    "CorpusId": 257912944
                },
                "corpusId": 257912944,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/061c281d9621c7fd9302d5a1adb37ea528a4d53d",
                "title": "Optimizing data-flow in Binary Neural Networks",
                "abstract": "Binary Neural Networks (BNNs) can significantly accelerate the inference time of a neural network by replacing its expensive floating-point arithmetic with bitwise operations. Most existing solutions, however, do not fully optimize data flow through the BNN layers, and intermediate conversions from 1 to 16/32 bits often further hinder efficiency. We propose a novel training scheme that can increase data flow and parallelism in the BNN pipeline; specifically, we introduce a clipping block that decreases the data-width from 32 bits to 8. Furthermore, we reduce the internal accumulator size of a binary layer, usually kept using 32-bit to prevent data overflow without losing accuracy. Additionally, we provide an optimization of the Batch Normalization layer that both reduces latency and simplifies deployment. Finally, we present an optimized implementation of the Binary Direct Convolution for ARM instruction sets. Our experiments show a consistent improvement of the inference speed (up to 1.91 and 2.73x compared to two state-of-the-art BNNs frameworks) with no drop in accuracy for at least one full-precision model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2213441368",
                        "name": "L. Vorabbi"
                    },
                    {
                        "authorId": "1687735",
                        "name": "D. Maltoni"
                    },
                    {
                        "authorId": "32063560",
                        "name": "S. Santi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Specifically, these networks have significantly lower memory footprint, less computational complexity, and consume less energy (Helwegen et al., 2019)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1d1088efd299f1d174e8c304ca8cdcf270414570",
                "externalIds": {
                    "DBLP": "conf/iclr/ErgenGLP23",
                    "ArXiv": "2303.03382",
                    "DOI": "10.48550/arXiv.2303.03382",
                    "CorpusId": 257365770
                },
                "corpusId": 257365770,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1d1088efd299f1d174e8c304ca8cdcf270414570",
                "title": "Globally Optimal Training of Neural Networks with Threshold Activation Functions",
                "abstract": "Threshold activation functions are highly preferable in neural networks due to their efficiency in hardware implementations. Moreover, their mode of operation is more interpretable and resembles that of biological neurons. However, traditional gradient based algorithms such as Gradient Descent cannot be used to train the parameters of neural networks with threshold activations since the activation function has zero gradient except at a single non-differentiable point. To this end, we study weight decay regularized training problems of deep neural networks with threshold activations. We first show that regularized deep threshold network training problems can be equivalently formulated as a standard convex optimization problem, which parallels the LASSO method, provided that the last hidden layer width exceeds a certain threshold. We also derive a simplified convex optimization formulation when the dataset can be shattered at a certain layer of the network. We corroborate our theoretical results with various numerical experiments.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "19278348",
                        "name": "Tolga Ergen"
                    },
                    {
                        "authorId": "2050160081",
                        "name": "Halil Ibrahim Gulluk"
                    },
                    {
                        "authorId": "37669328",
                        "name": "Jonathan Lacotte"
                    },
                    {
                        "authorId": "3173667",
                        "name": "Mert Pilanci"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "We then convert to the latent-weight free setting of Helwegen et al. (2019) where latent weights are interpreted as accumulating negative gradients.",
                "We draw inspiration from the seminal work of Helwegen et al. (2019), who reinterpret latent weights from an inertia perspective and state that latent weights do not exist.",
                "We start with a latent weights BNN and convert it to an equivalent latent-weight free setting, as in Helwegen et al. (2019).",
                "In binary network optimization, Bop (Helwegen et al., 2019) and its extension (Suarez-Ramirez et al., 2021) introduce a threshold to compare with the smoothed gradient by EMA to determine whether to flip a binary weight.",
                "In binary network optimization, Bop (Helwegen et al., 2019) and its extension (Suarez-Ramirez et al."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6d0c8934f62d605d7345de39759cd2b1df393e4d",
                "externalIds": {
                    "DBLP": "conf/iclr/QuistLG23",
                    "ArXiv": "2303.02452",
                    "DOI": "10.48550/arXiv.2303.02452",
                    "CorpusId": 257365819
                },
                "corpusId": 257365819,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6d0c8934f62d605d7345de39759cd2b1df393e4d",
                "title": "Understanding weight-magnitude hyperparameters in training binary networks",
                "abstract": "Binary Neural Networks (BNNs) are compact and efficient by using binary weights instead of real-valued weights. Current BNNs use latent real-valued weights during training, where several training hyper-parameters are inherited from real-valued networks. The interpretation of several of these hyperparameters is based on the magnitude of the real-valued weights. For BNNs, however, the magnitude of binary weights is not meaningful, and thus it is unclear what these hyperparameters actually do. One example is weight-decay, which aims to keep the magnitude of real-valued weights small. Other examples are latent weight initialization, the learning rate, and learning rate decay, which influence the magnitude of the real-valued weights. The magnitude is interpretable for real-valued weights, but loses its meaning for binary weights. In this paper we offer a new interpretation of these magnitude-based hyperparameters based on higher-order gradient filtering during network optimization. Our analysis makes it possible to understand how magnitude-based hyperparameters influence the training of binary networks which allows for new optimization filters specifically designed for binary neural networks that are independent of their real-valued interpretation. Moreover, our improved understanding reduces the number of hyperparameters, which in turn eases the hyperparameter tuning effort which may lead to better hyperparameter values for improved accuracy. Code is available at https://github.com/jorisquist/Understanding-WM-HP-in-BNNs",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2210799035",
                        "name": "Joris Quist"
                    },
                    {
                        "authorId": "152998393",
                        "name": "Yun-qiang Li"
                    },
                    {
                        "authorId": "1738975",
                        "name": "J. V. Gemert"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "95b0634c6ff3fc372c36e380232d881f6af757f1",
                "externalIds": {
                    "ArXiv": "2302.11286",
                    "DBLP": "journals/corr/abs-2302-11286",
                    "DOI": "10.48550/arXiv.2302.11286",
                    "CorpusId": 257079072
                },
                "corpusId": 257079072,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/95b0634c6ff3fc372c36e380232d881f6af757f1",
                "title": "Neural-based classification rule learning for sequential data",
                "abstract": "Discovering interpretable patterns for classification of sequential data is of key importance for a variety of fields, ranging from genomics to fraud detection or more generally interpretable decision-making. In this paper, we propose a novel differentiable fully interpretable method to discover both local and global patterns (i.e. catching a relative or absolute temporal dependency) for rule-based binary classification. It consists of a convolutional binary neural network with an interpretable neural filter and a training strategy based on dynamically-enforced sparsity. We demonstrate the validity and usefulness of the approach on synthetic datasets and on an open-source peptides dataset. Key to this end-to-end differentiable method is that the expressive patterns used in the rules are learned alongside the rules themselves.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "32587610",
                        "name": "Marine Collery"
                    },
                    {
                        "authorId": "2058815357",
                        "name": "Philippe Bonnard"
                    },
                    {
                        "authorId": "51319509",
                        "name": "Franccois Fages"
                    },
                    {
                        "authorId": "2007729358",
                        "name": "R. Kusters"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The intuition of CGA also aligns with the weight confidence proposition in (Helwegen et al., 2019; Liu et al., 2021a): the confidence level of a quantized weight is proportional to the distance of its real-valued latent weights to the closest threshold."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "42d3b9e9111efb70a11167096738d7dd344f7e5a",
                "externalIds": {
                    "DBLP": "conf/icml/LiuLC23",
                    "ArXiv": "2302.02210",
                    "DOI": "10.48550/arXiv.2302.02210",
                    "CorpusId": 256615191
                },
                "corpusId": 256615191,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/42d3b9e9111efb70a11167096738d7dd344f7e5a",
                "title": "Oscillation-free Quantization for Low-bit Vision Transformers",
                "abstract": "Weight oscillation is an undesirable side effect of quantization-aware training, in which quantized weights frequently jump between two quantized levels, resulting in training instability and a sub-optimal final model. We discover that the learnable scaling factor, a widely-used $\\textit{de facto}$ setting in quantization aggravates weight oscillation. In this study, we investigate the connection between the learnable scaling factor and quantized weight oscillation and use ViT as a case driver to illustrate the findings and remedies. In addition, we also found that the interdependence between quantized weights in $\\textit{query}$ and $\\textit{key}$ of a self-attention layer makes ViT vulnerable to oscillation. We, therefore, propose three techniques accordingly: statistical weight quantization ($\\rm StatsQ$) to improve quantization robustness compared to the prevalent learnable-scale-based method; confidence-guided annealing ($\\rm CGA$) that freezes the weights with $\\textit{high confidence}$ and calms the oscillating weights; and $\\textit{query}$-$\\textit{key}$ reparameterization ($\\rm QKR$) to resolve the query-key intertwined oscillation and mitigate the resulting gradient misestimation. Extensive experiments demonstrate that these proposed techniques successfully abate weight oscillation and consistently achieve substantial accuracy improvement on ImageNet. Specifically, our 2-bit DeiT-T/DeiT-S algorithms outperform the previous state-of-the-art by 9.8% and 7.7%, respectively. Code and models are available at: https://github.com/nbasyl/OFQ.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2131155302",
                        "name": "Shi Liu"
                    },
                    {
                        "authorId": "2109370860",
                        "name": "Zechun Liu"
                    },
                    {
                        "authorId": "145210800",
                        "name": "Kwang-Ting Cheng"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "As a compression approach that reduces the bit-width to 1-bit, network binarization is regarded as the most aggressive quantization technology (Rusci et al., 2020; Choukroun et al., 2019; Qin et al., 2022; Shang et al., 2022b; Zhang et al., 2022b; Bethge et al., 2020; 2019; Martinez et al., 2019; Helwegen et al., 2019).",
                "\u2026that reduces the bit-width to 1-bit, network binarization is regarded as the most aggressive quantization technology (Rusci et al., 2020; Choukroun et al., 2019; Qin et al., 2022a; Shang et al., 2022b; Zhang et al., 2022b; Bethge et al., 2020; 2019; Martinez et al., 2019; Helwegen et al., 2019)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f39f01f39813fd8ed791f88ec7378a44dbf2a175",
                "externalIds": {
                    "DBLP": "conf/icml/QinZDLC0Y023",
                    "ArXiv": "2301.11233",
                    "DOI": "10.48550/arXiv.2301.11233",
                    "CorpusId": 256274810
                },
                "corpusId": 256274810,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f39f01f39813fd8ed791f88ec7378a44dbf2a175",
                "title": "BiBench: Benchmarking and Analyzing Network Binarization",
                "abstract": "Network binarization emerges as one of the most promising compression approaches offering extraordinary computation and memory savings by minimizing the bit-width. However, recent research has shown that applying existing binarization algorithms to diverse tasks, architectures, and hardware in realistic scenarios is still not straightforward. Common challenges of binarization, such as accuracy degradation and efficiency limitation, suggest that its attributes are not fully understood. To close this gap, we present BiBench, a rigorously designed benchmark with in-depth analysis for network binarization. We first carefully scrutinize the requirements of binarization in the actual production and define evaluation tracks and metrics for a comprehensive and fair investigation. Then, we evaluate and analyze a series of milestone binarization algorithms that function at the operator level and with extensive influence. Our benchmark reveals that 1) the binarized operator has a crucial impact on the performance and deployability of binarized networks; 2) the accuracy of binarization varies significantly across different learning tasks and neural architectures; 3) binarization has demonstrated promising efficiency potential on edge devices despite the limited hardware support. The results and analysis also lead to a promising paradigm for accurate and efficient binarization. We believe that BiBench will contribute to the broader adoption of binarization and serve as a foundation for future research. The code for our BiBench is released https://github.com/htqin/BiBench .",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1381853008",
                        "name": "Haotong Qin"
                    },
                    {
                        "authorId": "2119682611",
                        "name": "Mingyuan Zhang"
                    },
                    {
                        "authorId": "71056959",
                        "name": "Yifu Ding"
                    },
                    {
                        "authorId": "2126952544",
                        "name": "Aoyu Li"
                    },
                    {
                        "authorId": "66562436",
                        "name": "Zhongang Cai"
                    },
                    {
                        "authorId": "2164276341",
                        "name": "Ziwei Liu"
                    },
                    {
                        "authorId": "1807197",
                        "name": "F. Yu"
                    },
                    {
                        "authorId": "6820648",
                        "name": "Xianglong Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Also called the latent weight [9], it is used to calculate the pseudogradient during backpropagation.",
                "Latent weights are real-valued weights that are used to obtain the pseudogradient vector during backpropagation as the real gradient vector cannot be obtained from binary weights [9].",
                "The sign and magnitude can be thought of separately as follows [9]:"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "11f36811cf5daa9cd7e96346c5fa4a0747c1841d",
                "externalIds": {
                    "DBLP": "conf/aspdac/ChenA023",
                    "DOI": "10.1145/3566097.3567873",
                    "CorpusId": 256417288
                },
                "corpusId": 256417288,
                "publicationVenue": {
                    "id": "9b12daea-e3b6-4d55-92a5-6803ca0d3e3d",
                    "name": "Asia and South Pacific Design Automation Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Asia s pac Des Autom Conf",
                        "ASP-DAC"
                    ],
                    "url": "http://www.aspdac.com/"
                },
                "url": "https://www.semanticscholar.org/paper/11f36811cf5daa9cd7e96346c5fa4a0747c1841d",
                "title": "Latent Weight-based Pruning for Small Binary Neural Networks",
                "abstract": "Binary neural networks (BNNs) substitute complex arithmetic oper-ations with simple bit-wise operations. The binarized weights and activations in BNN s can drastically reduce memory requirement and energy consumption, making it attractive for edge ML applications with limited resources. However, the severe memory capacity and energy constraints of low-power edge devices call for further reduction of BNN models beyond binarization. Weight pruning is a proven solution for reducing the size of many neural network (NN) models, but the binary nature of BNN weights make it difficult to identify insignificant weights to remove. In this paper, we present a pruning method based on latent weight with layer-level pruning sensitivity analysis which reduces the over-parameterization of BNN s, allowing for accuracy gains while drastically reducing the model size. Our method advocates for a heuristics that distinguishes weights by their latent weights, a real-valued vector used to compute the pseduogradient during backpropagation. It is tested using three different convolutional NNs on the MNIST, CIFAR-10, and Imagenette datasets with results indicating a 33%-46% reduction in operation count, with no accu-racy loss, improving upon previous works in accuracy, model size, and total operation count.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "152448775",
                        "name": "Tianen Chen"
                    },
                    {
                        "authorId": "2203443370",
                        "name": "Noah Anderson"
                    },
                    {
                        "authorId": "2157302397",
                        "name": "Younghyun Kim"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[15] revisited the functional role of latent weights in BNNs and proposed a specialized optimizer BOP to flip the binary states."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f47ae084b19cd872996fc046a9b3745ab5b5a47e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-12933",
                    "ArXiv": "2211.12933",
                    "DOI": "10.48550/arXiv.2211.12933",
                    "CorpusId": 253801527
                },
                "corpusId": 253801527,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f47ae084b19cd872996fc046a9b3745ab5b5a47e",
                "title": "Join the High Accuracy Club on ImageNet with A Binary Neural Network Ticket",
                "abstract": "Binary neural networks are the extreme case of network quantization, which has long been thought of as a potential edge machine learning solution. However, the significant accuracy gap to the full-precision counterparts restricts their creative potential for mobile applications. In this work, we revisit the potential of binary neural networks and focus on a compelling but unanswered problem: how can a binary neural network achieve the crucial accuracy level (e.g., 80%) on ILSVRC-2012 ImageNet? We achieve this goal by enhancing the optimization process from three complementary perspectives: (1) We design a novel binary architecture BNext based on a comprehensive study of binary architectures and their optimization process. (2) We propose a novel knowledge-distillation technique to alleviate the counter-intuitive overfitting problem observed when attempting to train extremely accurate binary models. (3) We analyze the data augmentation pipeline for binary networks and modernize it with up-to-date techniques from full-precision models. The evaluation results on ImageNet show that BNext, for the first time, pushes the binary model accuracy boundary to 80.57% and significantly outperforms all the existing binary networks. Code and trained models are available at: https://github.com/hpi-xnor/BNext.git.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1865717558",
                        "name": "Nianhui Guo"
                    },
                    {
                        "authorId": "31890223",
                        "name": "Joseph Bethge"
                    },
                    {
                        "authorId": "1708312",
                        "name": "C. Meinel"
                    },
                    {
                        "authorId": "1688587",
                        "name": "Haojin Yang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "QUANTITATIVE RESULTS ON TOP-1 AND TOP-5 ACCURACY (%) FOR DIFFERENT BINARIZATION METHODS, INCLUDING REAL-TO-BINARY NET [17], REACTNET [19], IR-NET [22], BOP [35], CI-NET [51], BONN [39], BI-REAL NET [13], AND XNOR-NET [12].",
                "We compare our method with several relevant state-ofthe-art methods, including Real-to-Binary Net [17], ReActNet [19], IR-Net [22], Bop [35], CI-Net [51], BONN [39], Bi-Real Net [13], and XNOR-Net [12] in Table VI.",
                "A binary optimizer is introduced in [35] to determine whether to flip a weight or not based on a sequence of gradients while treating"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e1541e4a18df4957719d04cb0abad6e1db5f3446",
                "externalIds": {
                    "DBLP": "journals/tcsv/LiuDCZWZZH22",
                    "DOI": "10.1109/TCSVT.2022.3166803",
                    "CorpusId": 248135260
                },
                "corpusId": 248135260,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e1541e4a18df4957719d04cb0abad6e1db5f3446",
                "title": "RB-Net: Training Highly Accurate and Efficient Binary Neural Networks With Reshaped Point-Wise Convolution and Balanced Activation",
                "abstract": "In this paper, we find that the conventional convolution operation becomes the bottleneck for extremely efficient binary neural networks (BNNs). To address this issue, we open up a new direction by introducing a reshaped point-wise convolution (RPC) to replace the conventional one to build BNNs. Specifically, we conduct a point-wise convolution after rearranging the spatial information into depth, with which at least $2.25\\times $ computation reduction can be achieved. Such an efficient RPC allows us to explore more powerful representational capacity of BNNs under a given computation complexity budget. Moreover, we propose to use a balanced activation (BA) to adjust the distribution of the scaled activations after binarization, which enables significant performance improvement of BNNs. After integrating RPC and BA, the proposed network, dubbed as RB-Net, strikes a good trade-off between accuracy and efficiency, achieving superior performance with lower computational cost against the state-of-the-art BNN methods. Specifically, our RB-Net achieves 66.8% Top-1 accuracy with ResNet-18 backbone on ImageNet, exceeding the state-of-the-art Real-to-Binary Net (65.4%) by 1.4% while achieving more than $3\\times $ reduction (52M vs. 165M) in computational complexity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49046587",
                        "name": "Chunlei Liu"
                    },
                    {
                        "authorId": "2911928",
                        "name": "Wenrui Ding"
                    },
                    {
                        "authorId": "2158171227",
                        "name": "Peng Chen"
                    },
                    {
                        "authorId": "3194022",
                        "name": "Bohan Zhuang"
                    },
                    {
                        "authorId": "2108111804",
                        "name": "Yufeng Wang"
                    },
                    {
                        "authorId": "2184077955",
                        "name": "Yang Zhao"
                    },
                    {
                        "authorId": "1740430",
                        "name": "Baochang Zhang"
                    },
                    {
                        "authorId": "143692666",
                        "name": "Yuqi Han"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c7ce87a9af40f8849cff2558d43734f7ec418f45",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-06893",
                    "ArXiv": "2207.06893",
                    "DOI": "10.1109/TIP.2023.3315540",
                    "CorpusId": 250526227,
                    "PubMed": "37756178"
                },
                "corpusId": 250526227,
                "publicationVenue": {
                    "id": "e117fa7f-05b7-4dd6-b64b-0a0a7c0393d8",
                    "name": "IEEE Transactions on Image Processing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Image Process"
                    ],
                    "issn": "1057-7149",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=83",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c7ce87a9af40f8849cff2558d43734f7ec418f45",
                "title": "E2FIF: Push the Limit of Binarized Deep Imagery Super-Resolution Using End-to-End Full-Precision Information Flow",
                "abstract": "Binary neural network (BNN) provides a promising solution to deploy parameter-intensive deep single image super-resolution (SISR) models onto real devices with limited storage and computational resources. To achieve comparable performance with the full-precision counterpart, most existing BNNs for SISR mainly focus on compensating for the information loss incurred by binarizing weights and activations in the network through better approximations to the binarized convolution. In this study, we revisit the difference between BNNs and their full-precision counterparts and argue that the key to good generalization performance of BNNs lies on preserving a complete full-precision information flow along with an accurate gradient flow passing through each binarized convolution layer. Inspired by this, we propose to introduce a full-precision skip connection, or a variant thereof, over each binarized convolution layer across the entire network, which can increase the forward expressive capability and the accuracy of back-propagated gradient, thus enhancing the generalization performance. More importantly, such a scheme can be applied to any existing BNN backbones for SISR without introducing any additional computation cost. To validate the efficacy of the proposed approach, we evaluate it using four different backbones for SISR on four benchmark datasets and report obviously superior performance over existing BNNs and even some 4-bit competitors.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2202128657",
                        "name": "Chongxing Song"
                    },
                    {
                        "authorId": "1412217815",
                        "name": "Zhiqiang Lang"
                    },
                    {
                        "authorId": "145673165",
                        "name": "Wei Wei"
                    },
                    {
                        "authorId": "50081808",
                        "name": "Lei Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "To encode the momentum values in Bop for BNN training, a standard FP format [2] and the brain FP format [10] have been used.",
                "For the batch normalization parameters, we use Adam optimization as proposed by [2], with an initial learning rate of 10\u22122 for FC and VGG3, and 10\u22123 for VGG7.",
                "The activation function is the binarization function with a learned threshold Algorithm 1: Training BNNs with Binary Optimizer (Bop) [2], recapped for completeness.",
                "However, if \u03b3 is chosen to be large, then the training is highly unstable, as reported in [2].",
                "Other variants of BNNs, such as XNOR-Net and BiReal-Net (presented in [2]), are easier to train and lead to higher accuracy.",
                "The algorithm for training BNNs with Bop [2] is recapped in Alg.",
                "When one of the most memory-efficient training procedures for BNNs, the Binary optimizer (Bop) [2], is used, one momentum value encoded as FP per binary weight is stored.",
                "To update the binary weights, the study in [2] proposes the binary optimizer (Bop) to directly flip the signs based on momentum signals, which are exponential moving averages of the partial derivatives."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "27abf245328bb81aabfd58a0030992daeef95703",
                "externalIds": {
                    "DBLP": "conf/dac/YaylaC22",
                    "DOI": "10.1145/3489517.3530496",
                    "CorpusId": 251744372
                },
                "corpusId": 251744372,
                "publicationVenue": {
                    "id": "021b37d3-cef1-4c12-a442-257f7900c23d",
                    "name": "Design Automation Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Des Autom Conf",
                        "DAC"
                    ],
                    "url": "http://www.dac.com/"
                },
                "url": "https://www.semanticscholar.org/paper/27abf245328bb81aabfd58a0030992daeef95703",
                "title": "Memory-efficient training of binarized neural networks on the edge",
                "abstract": "A visionary computing paradigm is to train resource efficient neural networks on the edge using dedicated low-power accelerators instead of cloud infrastructures, eliminating communication overheads and privacy concerns. One promising resource-efficient approach for inference is binarized neural networks (BNNs), which binarize parameters and activations. However, training BNNs remains resource demanding. State-of-the-art BNN training methods, such as the binary optimizer (Bop), require to store and update a large number of momentum values in the floating point (FP) format. In this work, we focus on memory-efficient FP encodings for the momentum values in Bop. To achieve this, we first investigate the impact of arbitrary FP encodings. When the FP format is not properly chosen, we prove that the updates of the momentum values can be lost and the quality of training is therefore dropped. With the insights, we formulate a metric to determine the number of unchanged momentum values in a training iteration due to the FP encoding. Based on the metric, we develop an algorithm to find FP encodings that are more memory-efficient than the standard FP encodings. In our experiments, the memory usage in BNN training is decreased by factors 2.47x, 2.43x, 2.04x, depending on the BNN model, with minimal accuracy cost (smaller than 1%) compared to using 32-bit FP encoding.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51044780",
                        "name": "Mikail Yayla"
                    },
                    {
                        "authorId": "2155560085",
                        "name": "Jian-Jia Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "This is shown in Equation 11 below, where the first line corresponds to gradient strength where \u03b3 is the adaptivity rate [49] and the second line defines the update rule [49].",
                "This is achieved by first learning the edges or equivalently the property of conjunction over symbols with a binary attention layer [49] in R, with weights w l \u2208 Rj\u00d7k \u2208 {0, 1}, j = 0, 1 .",
                "In our case, we map our weights to {0, 1} rather than {-1, 1}, by initializing weights randomly \u2208 {0, 1} and modify the update rule proposed in [49].",
                "All continuous weights in our model framework are updated with Adam optimisation while the binary weights inR are updated using the Bop algorithm proposed by [49]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "514eec7a2c3c9445b4a8a8408c47594709cf9be1",
                "externalIds": {
                    "ArXiv": "2207.01916",
                    "DBLP": "journals/corr/abs-2207-01916",
                    "DOI": "10.48550/arXiv.2207.01916",
                    "CorpusId": 250279756
                },
                "corpusId": 250279756,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/514eec7a2c3c9445b4a8a8408c47594709cf9be1",
                "title": "Hierarchical Symbolic Reasoning in Hyperbolic Space for Deep Discriminative Models",
                "abstract": "Explanations for \\emph{black-box} models help us understand model decisions as well as provide information on model biases and inconsistencies. Most of the current explainability techniques provide a single level of explanation, often in terms of feature importance scores or feature attention maps in input space. Our focus is on explaining deep discriminative models at \\emph{multiple levels of abstraction}, from fine-grained to fully abstract explanations. We achieve this by using the natural properties of \\emph{hyperbolic geometry} to more efficiently model a hierarchy of symbolic features and generate \\emph{hierarchical symbolic rules} as part of our explanations. Specifically, for any given deep discriminative model, we distill the underpinning knowledge by discretisation of the continuous latent space using vector quantisation to form symbols, followed by a \\emph{hyperbolic reasoning block} to induce an \\emph{abstraction tree}. We traverse the tree to extract explanations in terms of symbolic rules and its corresponding visual semantics. We demonstrate the effectiveness of our method on the MNIST and AFHQ high-resolution animal faces dataset. Our framework is available at \\url{https://github.com/koriavinash1/SymbolicInterpretability}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9696089",
                        "name": "Ainkaran Santhirasekaram"
                    },
                    {
                        "authorId": "35982249",
                        "name": "A. Kori"
                    },
                    {
                        "authorId": "2064337286",
                        "name": "A. Rockall"
                    },
                    {
                        "authorId": "2174812191",
                        "name": "Mathias Winkler"
                    },
                    {
                        "authorId": "49973505",
                        "name": "Francesca Toni"
                    },
                    {
                        "authorId": "1709824",
                        "name": "Ben Glocker"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The quantization noise hinders the training step to find its optimized weights (Helwegen et al. (2019); Xu et al. (2021a)).",
                "There have been several works to overcome the degraded performance in low-bit quantized models (Helwegen et al. (2019); Kim et al. (2020); Martinez et al. (2020); Xu et al. (2021a,b))."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "59b8be33f311f793a5f4ed30260e8ea25e02716f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-12794",
                    "ArXiv": "2206.12794",
                    "DOI": "10.48550/arXiv.2206.12794",
                    "CorpusId": 250073215
                },
                "corpusId": 250073215,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/59b8be33f311f793a5f4ed30260e8ea25e02716f",
                "title": "CTMQ: Cyclic Training of Convolutional Neural Networks with Multiple Quantization Steps",
                "abstract": "This paper proposes a training method having multiple cyclic training for achieving enhanced performance in low-bit quantized convolutional neural networks (CNNs). Quantization is a popular method for obtaining lightweight CNNs, where the initialization with a pretrained model is widely used to overcome degraded performance in low-resolution quantization. However, large quantization errors between real values and their low-bit quantized ones cause difficulties in achieving acceptable performance for complex networks and large datasets. The proposed training method softly delivers the knowledge of pretrained models to low-bit quantized models in multiple quantization steps. In each quantization step, the trained weights of a model are used to initialize the weights of the next model with the quantization bit depth reduced by one. With small change of the quantization bit depth, the performance gap can be bridged, thus providing better weight initialization. In cyclic training, after training a low-bit quantized model, its trained weights are used in the initialization of its accurate model to be trained. By using better training ability of the accurate model in an iterative manner, the proposed method can produce enhanced trained weights for the low-bit quantized model in each cycle. Notably, the training method can advance Top-1 and Top-5 accuracies of the binarized ResNet-18 on the ImageNet dataset by 5.80% and 6.85%, respectively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109872525",
                        "name": "Hyunjin Kim"
                    },
                    {
                        "authorId": "121510164",
                        "name": "Jungwoon Shin"
                    },
                    {
                        "authorId": "34294222",
                        "name": "Alberto A. Del Barrio"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "Next to the default optimizers, [18, 41] have developed new optimizers that are dedicated to BNNs, which are respectively called Bop and Bop2ndOrder.",
                "1 n/a BN n/a N ADAM LF n/a N N Latent Weights [18] 2019 n/a Custom BENN (6x) [55] 2019 61 LC_1 LC_1 N ADAM EN n/a N N Circulant BNN [28] 2019 61."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "193de75aab367bef1a7b9519f69c655353e4cc65",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-12322",
                    "ArXiv": "2206.12322",
                    "DOI": "10.48550/arXiv.2206.12322",
                    "CorpusId": 250048655
                },
                "corpusId": 250048655,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/193de75aab367bef1a7b9519f69c655353e4cc65",
                "title": "How to train accurate BNNs for embedded systems?",
                "abstract": "A key enabler of deploying convolutional neural networks on resource-constrained embedded systems is the binary neural network (BNN). BNNs save on memory and simplify computation by binarizing both features and weights. Unfor-tunately, binarization is inevitably accompanied by a severe decrease in accuracy. To reduce the accuracy gap between binary and full-precision networks, many repair methods have been proposed in the recent past, which we have classi\ufb01ed and put into a single overview in this chapter. The repair methods are divided into two main branches, training techniques and network topology changes, which can further be split into smaller categories. The latter category introduces additional cost (energy consumption or additional area) for an embedded system, while the former does not. From our overview, we observe that progress has been made in reducing the accuracy gap, but BNN papers are not aligned on what repair methods should be used to get highly accurate BNNs. Therefore, this chapter contains an empirical review that evaluates the bene\ufb01ts of many repair methods in isolation over the ResNet-20&CIFAR10 and ResNet-18&CIFAR100 benchmarks. We found three repair categories most bene\ufb01cial: feature binarizer, feature normalization, and double residual. Based on this review we discuss future directions and research opportuni-ties. We sketch the bene\ufb01t and costs associated with BNNs on embedded systems because it remains to be seen whether BNNs will be able to close the accuracy gap while staying highly energy-e\ufb03cient on resource-constrained embedded systems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2173449926",
                        "name": "F. Putter"
                    },
                    {
                        "authorId": "1684335",
                        "name": "H. Corporaal"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "We use the Larq [11] framework for implementation of BNNs and utilize Binary Optimizer (Bop) [22] and Adam [23] for training models with a learning rate of 0."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3fd99a934a2151d2c2744f0f9895723312e6f8cf",
                "externalIds": {
                    "DBLP": "conf/interspeech/Saeed22",
                    "ArXiv": "2206.09029",
                    "DOI": "10.48550/arXiv.2206.09029",
                    "CorpusId": 249889628
                },
                "corpusId": 249889628,
                "publicationVenue": {
                    "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
                    "name": "Interspeech",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Int Speech Commun Assoc",
                        "INTERSPEECH",
                        "Conference of the International Speech Communication Association"
                    ],
                    "issn": "2308-457X",
                    "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
                    "alternate_urls": [
                        "http://www.isca-speech.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3fd99a934a2151d2c2744f0f9895723312e6f8cf",
                "title": "Binary Early-Exit Network for Adaptive Inference on Low-Resource Devices",
                "abstract": "Deep neural networks have signi\ufb01cantly improved performance on a range of tasks with the increasing demand for computational resources, leaving deployment on low-resource devices (with limited memory and battery power) infeasible. Binary neural networks (BNNs) tackle the issue to an extent with extreme compression and speed-up gains compared to real-valued models. We propose a simple but effective method to accelerate inference through unifying BNNs with an early-exiting strategy. Our approach allows simple instances to exit early based on a decision threshold and utilizes output layers added to different intermediate layers to avoid executing the entire binary model. We extensively evaluate our method on three audio classi\ufb01cation tasks and across four BNNs architectures. Our method demonstrates favorable quality-ef\ufb01ciency trade-offs while being controllable with an entropy-based threshold speci\ufb01ed by the system user. It also results in better speed-ups (latency less than 6ms) with a single model based on existing BNN architectures without retraining for different ef\ufb01ciency levels. It also provides a straightforward way to estimate sample dif\ufb01culty and better understanding of uncertainty around certain classes within the dataset.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9261711",
                        "name": "Aaqib Saeed"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Therefore, in the future, we will explore the efficacy brought by employing a binary optimizer [Helwegen et al., 2019] that only modifies signs of weights without the need for latent parameters like the sign supermasks."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f4e8c760a9c81f574595f29f5bfbb3a9e42c6136",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-04385",
                    "ArXiv": "2206.04385",
                    "DOI": "10.48550/arXiv.2206.04385",
                    "CorpusId": 249538406
                },
                "corpusId": 249538406,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f4e8c760a9c81f574595f29f5bfbb3a9e42c6136",
                "title": "HideNseek: Federated Lottery Ticket via Server-side Pruning and Sign Supermask",
                "abstract": "Federated learning alleviates the privacy risk in distributed learning by transmitting only the local model updates to the central server. However, it faces challenges including statistical heterogeneity of clients' datasets and resource constraints of client devices, which severely impact the training performance and user experience. Prior works have tackled these challenges by combining personalization with model compression schemes including quantization and pruning. However, the pruning is data-dependent and thus must be done on the client side which requires considerable computation cost. Moreover, the pruning normally trains a binary supermask $\\in \\{0, 1\\}$ which significantly limits the model capacity yet with no computation benefit. Consequently, the training requires high computation cost and a long time to converge while the model performance does not pay off. In this work, we propose HideNseek which employs one-shot data-agnostic pruning at initialization to get a subnetwork based on weights' synaptic saliency. Each client then optimizes a sign supermask $\\in \\{-1, +1\\}$ multiplied by the unpruned weights to allow faster convergence with the same compression rates as state-of-the-art. Empirical results from three datasets demonstrate that compared to state-of-the-art, HideNseek improves inferences accuracies by up to 40.6\\% while reducing the communication cost and training time by up to 39.7\\% and 46.8\\% respectively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2129455340",
                        "name": "Anish K. Vallapuram"
                    },
                    {
                        "authorId": "72050450",
                        "name": "Pengyuan Zhou"
                    },
                    {
                        "authorId": "97517796",
                        "name": "Young D. Kwon"
                    },
                    {
                        "authorId": "41177600",
                        "name": "Lik-Hang Lee"
                    },
                    {
                        "authorId": "2169543509",
                        "name": "Hengwei Xu"
                    },
                    {
                        "authorId": "2106066733",
                        "name": "Pan Hui"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "These include adding learnable scaling factors (also known as gain term) [33]\u2013[36], adopting multiple basis [33]\u2013[35], [37], designing BNN-oriented network structure [16], [18], [23], [38], and reforming BNN training methodologies [22], [23], [36], [39]\u2013[41]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3cc2bf067d5e7707285fd2c9b4851707c5dfab29",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-03291",
                    "ArXiv": "2206.03291",
                    "DOI": "10.48550/arXiv.2206.03291",
                    "CorpusId": 249431571
                },
                "corpusId": 249431571,
                "publicationVenue": {
                    "id": "5a5c57f0-8a1a-42e5-852c-75e4a107c103",
                    "name": "Tsinghua Science and Technology",
                    "type": "journal",
                    "alternate_names": [
                        "Tsinghua Science & Technology",
                        "Tsinghua Sci Technol",
                        "Tsinghua Sci  Technol"
                    ],
                    "issn": "1007-0214",
                    "url": "http://www.sciencedirect.com/science/journal/10070214"
                },
                "url": "https://www.semanticscholar.org/paper/3cc2bf067d5e7707285fd2c9b4851707c5dfab29",
                "title": "GAAF: Searching Activation Functions for Binary Neural Networks through Genetic Algorithm",
                "abstract": "\u2014Binary neural networks (BNNs) show promising utilization in cost and power-restricted domains such as edge devices and mobile systems. This is due to its signi\ufb01cantly less computation and storage demand, but at the cost of degraded performance. To close the accuracy gap, in this paper we propose to add a complementary activation function (AF) ahead of the sign based binarization, and rely on the genetic algorithm (GA) to automatically search for the ideal AFs. These AFs can help extract extra information from the input data in the forward pass, while allowing improved gradient approximation in the backward pass. Fifteen novel AFs are identi\ufb01ed through our GA- based search, while most of them show improved performance (up to 2.54% on ImageNet) when testing on different datasets and network models. Our method offers a novel approach for designing general and application-speci\ufb01c BNN architecture. Our code is available at http://github.com/\ufb02ying-Yan/GAAF.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110499822",
                        "name": "Yanfei Li"
                    },
                    {
                        "authorId": "4444498",
                        "name": "Tong Geng"
                    },
                    {
                        "authorId": "1949572253",
                        "name": "S. Stein"
                    },
                    {
                        "authorId": "145476833",
                        "name": "Ang Li"
                    },
                    {
                        "authorId": "2152696937",
                        "name": "Hui-Ling Yu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Alizadeh et al. [46] empirically studied various optimizer and found that adapting the learning rate using secondmoment methods was crucial for the successful use of STE. Binary Optimizer [47] introduced a BNN-specific optimizer by viewing the latent weights as inertia.",
                "Binary Optimizer [47] introduced a BNN-specific optimizer by viewing the latent weights as inertia."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "03118f13104431e10fdf0ac800b6098e32867554",
                "externalIds": {
                    "ArXiv": "2206.03325",
                    "DBLP": "journals/corr/abs-2206-03325",
                    "DOI": "10.48550/arXiv.2206.03325",
                    "CorpusId": 249431430
                },
                "corpusId": 249431430,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/03118f13104431e10fdf0ac800b6098e32867554",
                "title": "Searching Similarity Measure for Binarized Neural Networks",
                "abstract": "Being a promising model to be deployed in resource-limited devices, Binarized Neural Networks (BNNs) have drawn extensive attention from both academic and industry. However, comparing to the full-precision deep neural networks (DNNs), BNNs suffer from non-trivial accuracy degradation, limiting its applicability in various domains. This is partially because existing network components, such as the similarity measure, are specially designed for DNNs, and might be sub-optimal for BNNs. In this work, we focus on the key component of BNNs -- the similarity measure, which quantifies the distance between input feature maps and filters, and propose an automatic searching method, based on genetic algorithm, for BNN-tailored similarity measure. Evaluation results on Cifar10 and Cifar100 using ResNet, NIN and VGG show that most of the identified similarty measure can achieve considerable accuracy improvement (up to 3.39%) over the commonly-used cross-correlation approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110499822",
                        "name": "Yanfei Li"
                    },
                    {
                        "authorId": "2141519914",
                        "name": "Ang Li"
                    },
                    {
                        "authorId": "2118683251",
                        "name": "Huimin Yu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The concept of latent weights is introduced aside quantized weights to be used by the photonic hardware [54]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a8b16d822fcd724252a416863678ffeeff34dfda",
                "externalIds": {
                    "DBLP": "journals/nca/PaoliniMCVMA22",
                    "DOI": "10.1007/s00521-022-07243-z",
                    "CorpusId": 252091791
                },
                "corpusId": 252091791,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a8b16d822fcd724252a416863678ffeeff34dfda",
                "title": "Photonic-aware neural networks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2178889344",
                        "name": "Emilio Paolini"
                    },
                    {
                        "authorId": "1470737705",
                        "name": "L. De Marinis"
                    },
                    {
                        "authorId": "3079976",
                        "name": "M. Cococcioni"
                    },
                    {
                        "authorId": "1790498",
                        "name": "L. Valcarenghi"
                    },
                    {
                        "authorId": "3298379",
                        "name": "Luca Maggiani"
                    },
                    {
                        "authorId": "1819889",
                        "name": "N. Andriolli"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "As described in (Helwegen et al., 2019) the underlying weights of a neural network when trained with STE are not representative for the network\u2019s performance, but rather can be interpreted as a reservoir that slowly accumulates small gradient updates."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7a7a4f41f9ca5682b1444140799ca4dde44352f5",
                "externalIds": {
                    "ArXiv": "2203.11086",
                    "DBLP": "conf/icml/NagelFBB22",
                    "DOI": "10.48550/arXiv.2203.11086",
                    "CorpusId": 247595112
                },
                "corpusId": 247595112,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7a7a4f41f9ca5682b1444140799ca4dde44352f5",
                "title": "Overcoming Oscillations in Quantization-Aware Training",
                "abstract": "When training neural networks with simulated quantization, we observe that quantized weights can, rather unexpectedly, oscillate between two grid-points. The importance of this effect and its impact on quantization-aware training (QAT) are not well-understood or investigated in literature. In this paper, we delve deeper into the phenomenon of weight oscillations and show that it can lead to a significant accuracy degradation due to wrongly estimated batch-normalization statistics during inference and increased noise during training. These effects are particularly pronounced in low-bit ($\\leq$ 4-bits) quantization of efficient networks with depth-wise separable layers, such as MobileNets and EfficientNets. In our analysis we investigate several previously proposed QAT algorithms and show that most of these are unable to overcome oscillations. Finally, we propose two novel QAT algorithms to overcome oscillations during training: oscillation dampening and iterative weight freezing. We demonstrate that our algorithms achieve state-of-the-art accuracy for low-bit (3&4 bits) weight and activation quantization of efficient architectures, such as MobileNetV2, MobileNetV3, and EfficentNet-lite on ImageNet. Our source code is available at {https://github.com/qualcomm-ai-research/oscillations-qat}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "41229153",
                        "name": "Markus Nagel"
                    },
                    {
                        "authorId": "2090358941",
                        "name": "Marios Fournarakis"
                    },
                    {
                        "authorId": "2112207572",
                        "name": "Yelysei Bondarenko"
                    },
                    {
                        "authorId": "83133279",
                        "name": "Tijmen Blankevoort"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b6e573ff13133a1894c908598e34577ed94ac9ea",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-09082",
                    "ArXiv": "2203.09082",
                    "DOI": "10.48550/arXiv.2203.09082",
                    "CorpusId": 247518708
                },
                "corpusId": 247518708,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b6e573ff13133a1894c908598e34577ed94ac9ea",
                "title": "Confidence Dimension for Deep Learning based on Hoeffding Inequality and Relative Evaluation",
                "abstract": "Research on the generalization ability of deep neural networks (DNNs) has recently attracted a great deal of attention. However, due to their complex architectures and large numbers of parameters, measuring the generalization ability of specific DNN models remains an open challenge. In this paper, we propose to use multiple factors to measure and rank the relative generalization of DNNs based on a new concept of confidence dimension (CD). Furthermore, we provide a feasible framework in our CD to theoretically calculate the upper bound of generalization based on the conventional Vapnik-Chervonenk dimension (VC-dimension) and Hoeffding\u2019s inequality. Experimental results on image classification and object detection demonstrate that our CD can reflect the relative generalization ability for different DNNs. In addition to full-precision DNNs, we also analyze the generalization ability of binary neural networks (BNNs), whose generalization ability remains an unsolved problem. Our CD yields a consistent and reliable measure and ranking for both full-precision DNNs and BNNs on all the tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2041702379",
                        "name": "Runqi Wang"
                    },
                    {
                        "authorId": "120378243",
                        "name": "Linlin Yang"
                    },
                    {
                        "authorId": "1740430",
                        "name": "Baochang Zhang"
                    },
                    {
                        "authorId": "50017718",
                        "name": "Wentao Zhu"
                    },
                    {
                        "authorId": "48471936",
                        "name": "D. Doermann"
                    },
                    {
                        "authorId": "2067614206",
                        "name": "Guodong Guo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1b3e38bf2e42ede4aeab2f6fde8f4103562a53f6",
                "externalIds": {
                    "ArXiv": "2201.08050",
                    "DBLP": "journals/corr/abs-2201-08050",
                    "CorpusId": 246063758
                },
                "corpusId": 246063758,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1b3e38bf2e42ede4aeab2f6fde8f4103562a53f6",
                "title": "TerViT: An Efficient Ternary Vision Transformer",
                "abstract": "Vision transformers (ViTs) have demonstrated great potential in various visual tasks, but suffer from expensive computational and memory cost problems when deployed on resource-constrained devices. In this paper, we introduce a ternary vision transformer (TerViT) to ternarize the weights in ViTs, which are challenged by the large loss surface gap between real-valued and ternary parameters. To address the issue, we introduce a progressive training scheme by first training 8-bit transformers and then TerViT, and achieve a better optimization than conventional methods. Furthermore, we introduce channel-wise ternarization, by partitioning each matrix to different channels, each of which is with an unique distribution and ternarization interval. We apply our methods to popular DeiT and Swin backbones, and extensive results show that we can achieve competitive performance. For example, TerViT can quantize Swin-S to 13.1MB model size while achieving above 79% Top-1 accuracy on ImageNet dataset.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110923498",
                        "name": "Sheng Xu"
                    },
                    {
                        "authorId": "2110500972",
                        "name": "Yanjing Li"
                    },
                    {
                        "authorId": "1390452961",
                        "name": "Teli Ma"
                    },
                    {
                        "authorId": "2121357933",
                        "name": "Bo-Wen Zeng"
                    },
                    {
                        "authorId": "1740430",
                        "name": "Baochang Zhang"
                    },
                    {
                        "authorId": "144740494",
                        "name": "Peng Gao"
                    },
                    {
                        "authorId": "102230404",
                        "name": "Jinhu Lv"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "17aa8bd6e0d336063b62c9c098db543453e61189",
                "externalIds": {
                    "DOI": "10.1201/9781003162810",
                    "CorpusId": 245925499
                },
                "corpusId": 245925499,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/17aa8bd6e0d336063b62c9c098db543453e61189",
                "title": "Low-Power Computer Vision",
                "abstract": "This chapter describes the history of IEEE History of Low-Power Computer Vision Challenge 2015\u20132020.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145078593",
                        "name": "G. Thiruvathukal"
                    },
                    {
                        "authorId": "2140045017",
                        "name": "Yu Lu"
                    },
                    {
                        "authorId": "2145425550",
                        "name": "Jaeyoun Kim"
                    },
                    {
                        "authorId": "2154549078",
                        "name": "Yiran Chen"
                    },
                    {
                        "authorId": "2152689644",
                        "name": "Bo Chen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e47e006b9396f0e3a3d1878881cd9359c0a40510",
                "externalIds": {
                    "PubMedCentral": "8771783",
                    "DBLP": "journals/peerj-cs/ShinK22",
                    "DOI": "10.7717/peerj-cs.842",
                    "CorpusId": 245698533,
                    "PubMed": "35111925"
                },
                "corpusId": 245698533,
                "publicationVenue": {
                    "id": "138e58fc-8643-41c2-a92c-068ea4f4c607",
                    "name": "PeerJ Computer Science",
                    "alternate_names": [
                        "Peerj Comput Sci"
                    ],
                    "issn": "2376-5992",
                    "url": "https://peerj.com/archives/?journal=cs",
                    "alternate_urls": [
                        "https://peerj.com/computer-science/",
                        "http://peerj.com/cs"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e47e006b9396f0e3a3d1878881cd9359c0a40510",
                "title": "PresB-Net: parametric binarized neural network with learnable activations and shuffled grouped convolution",
                "abstract": "In this study, we present a novel performance-enhancing binarized neural network model called PresB-Net: Parametric Binarized Neural Network. A binarized neural network (BNN) model can achieve fast output computation with low hardware costs by using binarized weights and features. However, performance degradation is the most critical problem in BNN models. Our PresB-Net combines several state-of-the-art BNN structures including the learnable activation with additional trainable parameters and shuffled grouped convolution. Notably, we propose a new normalization approach, which reduces the imbalance between the shuffled groups occurring in shuffled grouped convolutions. Besides, the proposed normalization approach helps gradient convergence so that the unstableness of the learning can be amortized when applying the learnable activation. Our novel BNN model enhances the classification performance compared with other existing BNN models. Notably, the proposed PresB-Net-18 achieves 73.84% Top-1 inference accuracy for the CIFAR-100 dataset, outperforming other existing counterparts.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "121510164",
                        "name": "Jungwoon Shin"
                    },
                    {
                        "authorId": "2109872525",
                        "name": "Hyunjin Kim"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Similar to early research, [18] does not introduce latent real-valued weights, but rather updates the binary weights directly using a momentum based optimizer designed specifically for BiNNs."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ea79fa7e4fef32732ffee4d2994bd8a6fd6f423e",
                "externalIds": {
                    "ArXiv": "2112.02880",
                    "DBLP": "journals/corr/abs-2112-02880",
                    "DOI": "10.1109/CVPR52688.2022.00055",
                    "CorpusId": 244908546
                },
                "corpusId": 244908546,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ea79fa7e4fef32732ffee4d2994bd8a6fd6f423e",
                "title": "AdaSTE: An Adaptive Straight-Through Estimator to Train Binary Neural Networks",
                "abstract": "We propose a new algorithm for training deep neural networks (DNNs) with binary weights. In particular, we first cast the problem of training binary neural networks (BiNNs) as a bilevel optimization instance and subsequently construct flexible relaxations of this bilevel program. The resulting training method shares its algorithmic simplicity with several existing approaches to train BiNNs, in particular with the straight-through gradient estimator successfully employed in BinaryConnect and subsequent methods. Infact, our proposed method can be interpreted as an adaptive variant of the original straight-through estimator that conditionally (but not always) acts like a linear mapping in the backward pass of error propagation. Experimental results demonstrate that our new algorithm offers favorable performance compared to existing approaches.11This work was partially supported by theWallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145553617",
                        "name": "Huu Le"
                    },
                    {
                        "authorId": "1413268591",
                        "name": "Rasmus H\u00f8ier"
                    },
                    {
                        "authorId": "2159385",
                        "name": "Che-Tsung Lin"
                    },
                    {
                        "authorId": "1713941",
                        "name": "C. Zach"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Commonly, binarization methods define each binary weight update by considering only its associated value in the real-valued latent weights (Bulat and Tzimiropoulos 2019; Li et al. 2017; Rastegari et al. 2016; Liu et al. 2018) or in the gradient vector (Helwegen et al. 2019).",
                "From a different perspective, recent work tries to sidestep having to approximate the gradient of the sign function, and uses bit flips to train binary networks (Helwegen et al. 2019).",
                "Here, we also use filterweight statistics to update the binary weights, however similar to (Helwegen et al. 2019) Table 1(d)) we do not rely on the sign function for binarization, but instead use binary weight flips."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "686193a33d56d6cc0f9df299f92f3f0bbf555810",
                "externalIds": {
                    "DBLP": "conf/aaai/LiPG22",
                    "ArXiv": "2112.03406",
                    "DOI": "10.1609/aaai.v36i2.20039",
                    "CorpusId": 244920648
                },
                "corpusId": 244920648,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/686193a33d56d6cc0f9df299f92f3f0bbf555810",
                "title": "Equal Bits: Enforcing Equally Distributed Binary Network Weights",
                "abstract": "Binary networks are extremely efficient as they use only two symbols to define the network: {+1, \u22121}. One can make the prior distribution of these symbols a design choice. The recent IR-Net of Qin et al. argues that imposing a Bernoulli distribution with equal priors (equal bit ratios) over the binary weights leads to maximum entropy and thus minimizes information loss. However, prior work cannot precisely control the binary weight distribution during training, and therefore cannot guarantee maximum entropy. Here, we show that quantizing using optimal transport can guarantee any bit ratio, including equal ratios. We investigate experimentally that equal bit ratios are indeed preferable and show that our method leads to optimization benefits. We show that our quantization method is effective when compared to state-of-the-art binarization methods, even when using binary weight pruning. Our code is available at https://github.com/liyunqianggyn/Equal-Bits-BNN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "152998393",
                        "name": "Yun-qiang Li"
                    },
                    {
                        "authorId": "37041694",
                        "name": "S. Pintea"
                    },
                    {
                        "authorId": "1738975",
                        "name": "J. V. Gemert"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "346778afdda4731310b7db17856cf27adb411d6b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-13220",
                    "ArXiv": "2110.13220",
                    "CorpusId": 239885955
                },
                "corpusId": 239885955,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/346778afdda4731310b7db17856cf27adb411d6b",
                "title": "Demystifying and Generalizing BinaryConnect",
                "abstract": "BinaryConnect (BC) and its many variations have become the de facto standard for neural network quantization. However, our understanding of the inner workings of BC is still quite limited. We attempt to close this gap in four different aspects: (a) we show that existing quantization algorithms, including post-training quantization, are surprisingly similar to each other; (b) we argue for proximal maps as a natural family of quantizers that is both easy to design and analyze; (c) we refine the observation that BC is a special case of dual averaging, which itself is a special case of the generalized conditional gradient algorithm; (d) consequently, we propose ProxConnect (PC) as a generalization of BC and we prove its convergence properties by exploiting the established connections. We conduct experiments on CIFAR-10 and ImageNet, and verify that PC achieves competitive performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "102541178",
                        "name": "Tim Dockhorn"
                    },
                    {
                        "authorId": "40508553",
                        "name": "Yaoliang Yu"
                    },
                    {
                        "authorId": "122819100",
                        "name": "Eyyub Sari"
                    },
                    {
                        "authorId": "1672959337",
                        "name": "Mahdi Zolnouri"
                    },
                    {
                        "authorId": "10127337",
                        "name": "V. Nia"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "An equivalent number of weight updates for both offline and online training processes is assumed, in agreement with [30].",
                "Again, the 1S1R-based BNN synaptic weight storage for online training feasibility is extrapolated, assuming the equivalent number of binary weight updates for both offline and online training processes [30].",
                "Training is performed over 30 epochs using the Bop approach [30].",
                "Training is performed over 30 epochs using the binary optimizer (Bop) approach [30]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a6fa3c0d42d48c82b9adbb45c394b47db7e8d03d",
                "externalIds": {
                    "DOI": "10.1088/1361-6641/ac31e2",
                    "CorpusId": 239607664
                },
                "corpusId": 239607664,
                "publicationVenue": {
                    "id": "3e3bfce5-b19d-400a-8a76-43fb6d8f4c43",
                    "name": "Semiconductor Science and Technology",
                    "type": "journal",
                    "alternate_names": [
                        "Semicond Sci Technol"
                    ],
                    "issn": "0268-1242",
                    "url": "https://iopscience.iop.org/0268-1242",
                    "alternate_urls": [
                        "http://iopscience.org/sst",
                        "http://iopscience.iop.org/0268-1242/1"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a6fa3c0d42d48c82b9adbb45c394b47db7e8d03d",
                "title": "OxRAM + OTS optimization for binarized neural network hardware implementation",
                "abstract": "Low-power memristive devices embedded on graphics or central processing units logic core are a very promising non-von-Neumann approach to improve significantly the speed and power consumption of deep learning accelerators, enhancing their deployment on embedded systems. Among various non-ideal emerging neuromorphic memory devices, synaptic weight hardware implementation using resistive random-access memories (RRAMs) within 1T1R architectures promises high performance on low precision binarized neural networks (BNN). Taking advantage of the RRAM capabilities and allowing to substantially improve the density thanks to the ovonic threshold selector (OTS) selector, this work proposes to replace the standard 1T1R architecture with a denser 1S1R crossbar system, where an HfO2-based resistive oxide memory (OxRAM) is co-integrated with a Ge-Se-Sb-N-based OTS. In this context, an extensive experimental study is performed to optimize the 1S1R stack and programming conditions for extended read window margin and endurance characteristics. Focusing on the standard machine learning MNIST image recognition task, we perform offline training simulations in order to define the constraints on the devices during the training process. A very promising bit error rate of \u223c10\u22123 is demonstrated together with 1S1R 104 error-free programming endurance characteristics, fulfilling the requirements for the application of interest. Based on this simulation and experimental study, BNN figures of merit (system footprint, number of weight updates, accuracy, inference speed, electrical consumption per image classification and tolerance to errors) are optimized by engineering the number of learnable parameters of the system. Altogether, an inherent BNN resilience to 1S1R parasitic bit errors is demonstrated.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1742035985",
                        "name": "J. Minguet Lopez"
                    },
                    {
                        "authorId": "22328712",
                        "name": "T. Hirtzlin"
                    },
                    {
                        "authorId": "2144028420",
                        "name": "M. Dampfhoffer"
                    },
                    {
                        "authorId": "1705050",
                        "name": "L. Grenouillet"
                    },
                    {
                        "authorId": "2106685600",
                        "name": "L. Reganaz"
                    },
                    {
                        "authorId": "46539090",
                        "name": "G. Navarro"
                    },
                    {
                        "authorId": "2562990",
                        "name": "C. Carabasse"
                    },
                    {
                        "authorId": "2111089",
                        "name": "E. Vianello"
                    },
                    {
                        "authorId": "13069661",
                        "name": "T. Magis"
                    },
                    {
                        "authorId": "50343707",
                        "name": "D. Deleruyelle"
                    },
                    {
                        "authorId": "115652824",
                        "name": "M. Bocquet"
                    },
                    {
                        "authorId": "2058375936",
                        "name": "J. Portal"
                    },
                    {
                        "authorId": "2819565",
                        "name": "F. Andrieu"
                    },
                    {
                        "authorId": "50037350",
                        "name": "G. Molas"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ac4ff56b0e62139e1eb768d5ef622ccc67d7044b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-05850",
                    "ArXiv": "2110.05850",
                    "CorpusId": 238634735
                },
                "corpusId": 238634735,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ac4ff56b0e62139e1eb768d5ef622ccc67d7044b",
                "title": "Improving Binary Neural Networks through Fully Utilizing Latent Weights",
                "abstract": "Binary Neural Networks (BNNs) rely on a real-valued auxiliary variable W to help binary training. However, pioneering binary works only use W to accumulate gradient updates during backward propagation, which can not fully exploit its power and may hinder novel advances in BNNs. In this work, we explore the role of W in training besides acting as a latent variable. Notably, we propose to add W into the computation graph, making it perform as a real-valued feature extractor to aid the binary training. We make different attempts on how to utilize the real-valued weights and propose a specialized supervision. Visualization experiments qualitatively verify the effectiveness of our approach in making it easier to distinguish between different categories. Quantitative experiments show that our approach outperforms current state-of-the-arts, further closing the performance gap between floating-point networks and BNNs. Evaluation on ImageNet with ResNet-18 (Top-1 63.4%), ResNet-34 (Top-1 67.0%) achieves new state-of-the-art.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7978284",
                        "name": "Weixiang Xu"
                    },
                    {
                        "authorId": "2146380250",
                        "name": "Qiang Chen"
                    },
                    {
                        "authorId": "2143944611",
                        "name": "Xiangyu He"
                    },
                    {
                        "authorId": "1656803942",
                        "name": "Peisong Wang"
                    },
                    {
                        "authorId": "2118866407",
                        "name": "Jian Cheng"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "24160840d800329abc47960f4c015c10bfacde6d",
                "externalIds": {
                    "DBLP": "journals/air/YuanA23",
                    "ArXiv": "2110.06804",
                    "DOI": "10.1007/s10462-023-10464-w",
                    "CorpusId": 238743860
                },
                "corpusId": 238743860,
                "publicationVenue": {
                    "id": "ea8553fe-2467-4367-afee-c4deb3754820",
                    "name": "Artificial Intelligence Review",
                    "type": "journal",
                    "alternate_names": [
                        "Artif Intell Rev"
                    ],
                    "issn": "0269-2821",
                    "url": "https://link.springer.com/journal/10462"
                },
                "url": "https://www.semanticscholar.org/paper/24160840d800329abc47960f4c015c10bfacde6d",
                "title": "A comprehensive review of Binary Neural Network",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47177391",
                        "name": "Chunyu Yuan"
                    },
                    {
                        "authorId": "8973902",
                        "name": "S. Agaian"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0f71c63515a2579a2432858766fcc57a8b9a1511",
                "externalIds": {
                    "DBLP": "conf/ijcnn/ZhangGDLZ21",
                    "DOI": "10.1109/IJCNN52387.2021.9533480",
                    "CorpusId": 237599229
                },
                "corpusId": 237599229,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/0f71c63515a2579a2432858766fcc57a8b9a1511",
                "title": "Learning to Binarize Convolutional Neural Networks with Adaptive Neural Encoder",
                "abstract": "The high computational complexity and memory consumption of the deep Convolutional Neural Networks (CNNs) restrict their deployability in resource-limited embedded devices. To address this challenge, emerging solutions are proposed for neural network quantization and compression. Among them, Binary Neural Networks (BNNs) show their potential in reducing computational and memory complexity; however, they suffer from considerable performance degradation. One of the major causes is their non-differentiable discrete quantization implemented using a fixed sign function, which leads to output distribution distortion. In this paper, instead of using the fixed and naive sign function, we propose a novel adaptive Neural Encoder (NE), which learns to quantize the full-precision weights as binary values. Inspired by the research of neural network distillation, a distribution loss is introduced as a regularizer to minimize the Kullback-Leibler divergence between the outputs of the full-precision model and the encoded binary model. With an end-to-end backpropagation training process, the adaptive neural encoder, along with the binary convolutional neural network, could reach convergence iteratively. Comprehensive experiments with different network structures and datasets show that the proposed method can improve the performance of the baselines and also outperform many state-of-the-art approaches. The source code of the proposed method is publicly available at https://github.com/CQUlearningsystemgroup/LearningToBinarize.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2155802771",
                        "name": "Shuai Zhang"
                    },
                    {
                        "authorId": "1738540217",
                        "name": "Fangyuan Ge"
                    },
                    {
                        "authorId": "2058085459",
                        "name": "Rui Ding"
                    },
                    {
                        "authorId": "2136363889",
                        "name": "Haijun Liu"
                    },
                    {
                        "authorId": "2109054962",
                        "name": "Xichuan Zhou"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7f401bb88652c5880289c24f8de214374f5df9f8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-04562",
                    "ArXiv": "2107.04562",
                    "CorpusId": 235790670
                },
                "corpusId": 235790670,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7f401bb88652c5880289c24f8de214374f5df9f8",
                "title": "The Bayesian Learning Rule",
                "abstract": "We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145901278",
                        "name": "M. E. Khan"
                    },
                    {
                        "authorId": "72271894",
                        "name": "H. Rue"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "\u2026SGD, including BNN (Courbariaux et al., 2016) and XNOR-Net (Rastegari et al., 2016), Real-to-Binary Network (Brais Martinez, 2020), Structured BNN (Zhuang et al., 2019), ReActNet (Liu et al., 2020), etc. Helwegen et al. proposed a new binary optimizer design based on Adam (Helwegen et al., 2019).",
                "Thus, it is not far-fetched that real-valued weights can be regarded as the confidence of a binary value to be -1 or +1, as also being mentioned in (Helwegen et al., 2019).",
                "The magnitude of these real-valued weights are regarded as \u2018inertial\u2019 (Helwegen et al., 2019), indicating how likely the corresponding binary weights are going to change their signs.",
                "proposed a new binary optimizer design based on Adam (Helwegen et al., 2019)."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2e5aafa18a0034940bcc48e5bac5851df5627f61",
                "externalIds": {
                    "DBLP": "conf/icml/LiuSLHHC21",
                    "ArXiv": "2106.11309",
                    "CorpusId": 235398404
                },
                "corpusId": 235398404,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2e5aafa18a0034940bcc48e5bac5851df5627f61",
                "title": "How Do Adam and Training Strategies Help BNNs Optimization?",
                "abstract": "The best performing Binary Neural Networks (BNNs) are usually attained using Adam optimization and its multi-step training variants. However, to the best of our knowledge, few studies explore the fundamental reasons why Adam is superior to other optimizers like SGD for BNN optimization or provide analytical explanations that support specific training strategies. To address this, in this paper we first investigate the trajectories of gradients and weights in BNNs during the training process. We show the regularization effect of second-order momentum in Adam is crucial to revitalize the weights that are dead due to the activation saturation in BNNs. We find that Adam, through its adaptive learning rate strategy, is better equipped to handle the rugged loss surface of BNNs and reaches a better optimum with higher generalization ability. Furthermore, we inspect the intriguing role of the real-valued weights in binary networks, and reveal the effect of weight decay on the stability and sluggishness of BNN optimization. Through extensive experiments and analysis, we derive a simple training scheme, building on existing Adam-based optimization, which achieves 70.5% top-1 accuracy on the ImageNet dataset using the same architecture as the state-of-the-art ReActNet while achieving 1.1% higher accuracy. Code and models are available at https://github.com/liuzechun/AdamBNN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109370860",
                        "name": "Zechun Liu"
                    },
                    {
                        "authorId": "145314568",
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "authorId": "48830672",
                        "name": "Shichao Li"
                    },
                    {
                        "authorId": "108586041",
                        "name": "K. Helwegen"
                    },
                    {
                        "authorId": "145252513",
                        "name": "Dong Huang"
                    },
                    {
                        "authorId": "145210800",
                        "name": "Kwang-Ting Cheng"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We relied on recommendations from [6] to define our custom BNN architectures, and took notions from [4] and used the Bop optimizer[23] to speed up training."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "684b0808bc93429acec45290d828aa68f8fd63cb",
                "externalIds": {
                    "MAG": "3127347079",
                    "DBLP": "conf/ih/IbarrondoCO21",
                    "DOI": "10.1145/3437880.3460394",
                    "CorpusId": 231732445
                },
                "corpusId": 231732445,
                "publicationVenue": {
                    "id": "166fd2b5-a928-4a98-a449-3b90935cc101",
                    "name": "IACR Cryptology ePrint Archive",
                    "type": "journal",
                    "alternate_names": [
                        "IACR Cryptol eprint Arch"
                    ],
                    "url": "http://eprint.iacr.org/"
                },
                "url": "https://www.semanticscholar.org/paper/684b0808bc93429acec45290d828aa68f8fd63cb",
                "title": "Banners: Binarized Neural Networks with Replicated Secret Sharing",
                "abstract": "Binarized Neural Networks (BNN) provide efficient implementations of Convolutional Neural Networks (CNN). This makes them particularly suitable to perform fast and memory-light inference of neural networks running on resource-constrained devices. Motivated by the growing interest in CNN-based biometric recognition on potentially insecure devices, or as part of strong multi-factor authentication for sensitive applications, the protection of BNN inference on edge devices is rendered imperative. We propose a new method to perform secure inference of BNN relying on secure multiparty computation. While preceding papers offered security in a semi-honest setting for BNN or malicious security for standard CNN, our work yields security with abort against one malicious adversary for BNN by leveraging on Replicated Secret Sharing (RSS) for an honest majority with three computing parties. Experimentally, we implement Banners on top of MP-SPDZ and compare it with prior work over binarized models trained for MNIST and CIFAR10 image classification datasets. Our results attest the efficiency of Banners as a privacy-preserving inference technique.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51230157",
                        "name": "Alberto Ibarrondo"
                    },
                    {
                        "authorId": "2188953",
                        "name": "H. Chabanne"
                    },
                    {
                        "authorId": "3249805",
                        "name": "Melek \u00d6nen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "accuracy gap, including specific architecture design [30, 4, 3, 29], real-valued weight and activation approximation [27, 48], specific training recipes [34], a dedicated optimizer [20], leveraging neural architecture search [6, 46] and dynamic networks [7]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4ab5df93460f4766143b915ab38550d5c31f8a49",
                "externalIds": {
                    "ArXiv": "2106.06991",
                    "DBLP": "journals/corr/abs-2106-06991",
                    "CorpusId": 235421623
                },
                "corpusId": 235421623,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4ab5df93460f4766143b915ab38550d5c31f8a49",
                "title": "BoolNet: Minimizing The Energy Consumption of Binary Neural Networks",
                "abstract": "Recent works on Binary Neural Networks (BNNs) have made promising progress in narrowing the accuracy gap of BNNs to their 32-bit counterparts. However, the accuracy gains are often based on specialized model designs using additional 32-bit components. Furthermore, almost all previous BNNs use 32-bit for feature maps and the shortcuts enclosing the corresponding binary convolution blocks, which helps to effectively maintain the accuracy, but is not friendly to hardware accelerators with limited memory, energy, and computing resources. Thus, we raise the following question: How can accuracy and energy consumption be balanced in a BNN network design? We extensively study this fundamental problem in this work and propose a novel BNN architecture without most commonly used 32-bit components: \\textit{BoolNet}. Experimental results on ImageNet demonstrate that BoolNet can achieve 4.6x energy reduction coupled with 1.2\\% higher accuracy than the commonly used BNN architecture Bi-RealNet. Code and trained models are available at: https://github.com/hpi-xnor/BoolNet.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1865717558",
                        "name": "Nianhui Guo"
                    },
                    {
                        "authorId": "31890223",
                        "name": "Joseph Bethge"
                    },
                    {
                        "authorId": "1688587",
                        "name": "Haojin Yang"
                    },
                    {
                        "authorId": "144474630",
                        "name": "Kai Zhong"
                    },
                    {
                        "authorId": "6636914",
                        "name": "Xuefei Ning"
                    },
                    {
                        "authorId": "1708312",
                        "name": "C. Meinel"
                    },
                    {
                        "authorId": "2153606861",
                        "name": "Yu Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "In this section, we first discuss the difference of SA-BNN with related methods in (Helwegen et al. 2019; Bai, Wang, and Liberty 2018), and then further analyze the effectiveness of the proposed SA-BNN.",
                "In particular, Helwegen et al. (Helwegen et al. 2019) argue that latent weights are not necessary for gradient-based\noptimization of BNNs, and they directly update the state of binarized weights with:\nwt = { \u2212wt\u22121 if |gt| \u2265 \u03b2 and sign(gt) = sign(wt\u22121) wt\u22121 otherwise ,\n(11) where gt is exponential moving average of gradient (gt = (1\u2212 \u03b3)gt\u22121 + \u03b3 \u2202L\u2202wt , where \u03b3 is the adaptive rate) and \u03b2 is a manually defined threshold to control the weight flipping.",
                "To solve this problem, Helwegen et al. (Helwegen et al. 2019) introduce an optimizer specifically designed for BNNs by directly updating the state of binarized weights.",
                "To better understand our SA-BNNs, we follow (Helwegen et al. 2019; Bai, Wang, and Liberty 2018) to calculate the flipping state at each epoch and set the ratio of sequential weight flip as\n\u03c0t =\n\u2211 w At \u2227 \u00b7 \u00b7 \u00b7 \u2227 At+m\u22121\n||sign(W )||1 , (12)\nwhere At represents sign(wt) 6= sign(wt+1) and m is the examined epoch interval.",
                "To better understand our SA-BNNs, we follow (Helwegen et al. 2019; Bai, Wang, and Liberty 2018) to calculate the flipping state at each epoch and set the ratio of sequential weight flip as\n\u03c0t =\n\u2211 w At \u2227 \u00b7 \u00b7 \u00b7 \u2227 At+m\u22121\n||sign(W )||1 , (12)\nwhere At represents sign(wt) 6= sign(wt+1) and m is the\u2026",
                "We carry out a comparative study with six methods: IR-Net (Qin et al. 2019), Bop (Helwegen et al. 2019), CI-Net (Wang et al. 2019), BONN (Gu et al. 2019b), Bi-Real Net (Liu et al. 2018), and XNOR-Net (Rastegari et al. 2016) on ResNet18, ResNet-34 and ResNet-50 in Table 3.",
                "(Helwegen et al. 2019) argue that latent weights are not necessary for gradient-based",
                "To better understand our SA-BNNs, we follow (Helwegen et al. 2019; Bai, Wang, and Liberty 2018) to calculate the flipping state at each epoch and set the ratio of sequential weight flip as",
                "However, the method in (Helwegen et al. 2019) suppresses the weight flip equally for different states, while SA-BNN treats different binarization states distinctively by employing an independent coefficient for each state.",
                "(Helwegen et al. 2019) introduce an optimizer specifically designed for BNNs by directly updating the state of binarized weights.",
                "From this perspective, threshold \u03b2 in (Helwegen et al. 2019) is consistent with the coefficients \u03c4 in our method.",
                "In particular, Helwegen et al. (Helwegen et al. 2019) argue that latent weights are not necessary for gradient-based\noptimization of BNNs, and they directly update the state of binarized weights with:\nwt = { \u2212wt\u22121 if |gt| \u2265 \u03b2 and sign(gt) = sign(wt\u22121) wt\u22121 otherwise ,\n(11) where gt is exponential\u2026"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "cbc644f26710b7b2cd96200878ca1c469daf1997",
                "externalIds": {
                    "DBLP": "conf/aaai/0001CZS0D21",
                    "DOI": "10.1609/aaai.v35i3.16306",
                    "CorpusId": 235306564
                },
                "corpusId": 235306564,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/cbc644f26710b7b2cd96200878ca1c469daf1997",
                "title": "SA-BNN: State-Aware Binary Neural Network",
                "abstract": "Binary Neural Networks (BNNs) have received significant attention due to the memory and computation efficiency recently. However, the considerable accuracy gap between BNNs and their full-precision counterparts hinders BNNs to be deployed to resource-constrained platforms. One of the main reasons for the performance gap can be attributed to the frequent weight flip, which is caused by the misleading weight update in BNNs. To address this issue, we propose a state-aware binary neural network (SA-BNN) equipped with the well designed state-aware gradient. Our SA-BNN is inspired by the observation that the frequent weight flip is more likely to occur, when the gradient magnitude for all quantization states {-1,1} is identical. Accordingly, we propose to employ independent gradient coefficients for different states when updating the weights. Furthermore, we also analyze the effectiveness of the state-aware gradient on suppressing the frequent weight flip problem. Experiments on ImageNet show that the proposed SA-BNN outperforms the current state-of-the-arts (e.g., Bi-Real Net) by more than 3% when using a ResNet architecture. Specifically, we achieve 61.7%, 65.5% and 68.7% Top-1 accuracy with ResNet-18, ResNet-34 and ResNet-50 on ImageNet, respectively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49046587",
                        "name": "Chunlei Liu"
                    },
                    {
                        "authorId": "2158171227",
                        "name": "Peng Chen"
                    },
                    {
                        "authorId": "3194022",
                        "name": "Bohan Zhuang"
                    },
                    {
                        "authorId": "1780381",
                        "name": "Chunhua Shen"
                    },
                    {
                        "authorId": "1740430",
                        "name": "Baochang Zhang"
                    },
                    {
                        "authorId": "2911928",
                        "name": "Wenrui Ding"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "A Binary Neural Network (BNN) [13, 14, 34, 73, 51, 14, 36, 48, 44, 29, 72, 39, 25, 37, 10, 57, 20, 66] represents the most extreme form of model quantization as it quantizes weights in convolution layers to only 1 bit, enjoying great speed-up compared with its full-precision counterpart.",
                "(ii) optimizationbased BNNs techniques, including minimizing the quantization error [73, 51, 14, 36], improving the network loss function [48, 44, 29, 72], and reducing the gradient error [39, 25, 37, 10]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1f8f6a63f8586cb491a1ba9955f94104e80f7efd",
                "externalIds": {
                    "ArXiv": "2104.08215",
                    "DBLP": "journals/corr/abs-2104-08215",
                    "DOI": "10.1109/CVPRW53098.2021.00520",
                    "CorpusId": 233289954
                },
                "corpusId": 233289954,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1f8f6a63f8586cb491a1ba9955f94104e80f7efd",
                "title": "\u201cBNN - BN = ?\u201d: Training Binary Neural Networks without Batch Normalization",
                "abstract": "Batch normalization (BN) is a key facilitator and considered essential for state-of-the-art binary neural networks (BNN). However, the BN layer is costly to calculate and is typically implemented with non-binary parameters, leaving a hurdle for the efficient implementation of BNN training. It also introduces undesirable dependence between samples within each batch. Inspired by the latest advance on Batch Normalization Free (BN-Free) training [7], we extend their framework to training BNNs, and for the first time demonstrate that BNs can be completely removed from BNN training and inference regimes. By plugging in and customizing techniques including adaptive gradient clipping, scale weight standardization, and specialized bottleneck block, a BN-free BNN is capable of maintaining competitive accuracy compared to its BN-based counterpart. Extensive experiments validate the effectiveness of our proposal across diverse BNN backbones and datasets. For example, after removing BNs from the state-of-the-art ReActNets [38], it can still be trained with our proposed methodology to achieve 92.08%, 68.34%, and 68.0% accuracy on CIFAR-10, CIFAR-100, and ImageNet respectively, with marginal performance drop (0.23% \u223c 0.44% on CIFAR and 1.40% on ImageNet). Codes and pre-trained models are available at: https://github.com/VITA-Group/BNN_NoBN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2109338656",
                        "name": "Zhenyu (Allen) Zhang"
                    },
                    {
                        "authorId": "33628507",
                        "name": "Xu Ouyang"
                    },
                    {
                        "authorId": "2109370860",
                        "name": "Zechun Liu"
                    },
                    {
                        "authorId": "145314568",
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "118638c768e7116f58a48545d514c4e3640ec2a3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-05124",
                    "ArXiv": "2104.05124",
                    "DOI": "10.1109/CVPRW53098.2021.00140",
                    "CorpusId": 233209728
                },
                "corpusId": 233209728,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/118638c768e7116f58a48545d514c4e3640ec2a3",
                "title": "A Bop and Beyond: A Second Order Optimizer for Binarized Neural Networks",
                "abstract": "The optimization of Binary Neural Networks (BNNs) relies on approximating the real-valued weights with their binarized representations. Current techniques for weight-updating use the same approaches as traditional Neural Networks (NNs) with the extra requirement of using an approximation to the derivative of the sign function - as it is the Dirac-Delta function - for back-propagation; thus, efforts are focused adapting full-precision techniques to work on BNNs. In the literature, only one previous effort has tackled the problem of directly training the BNNs with bit-flips by using the first raw moment estimate of the gradients and comparing it against a threshold for deciding when to flip a weight (Bop). In this paper, we take an approach parallel to Adam which also uses the second raw moment estimate to normalize the first raw moment before doing the comparison with the threshold, we call this method Bop2ndOrder. We present two versions of the proposed optimizer: a biased one and a bias-corrected one, each with its own applications. Also, we present a complete ablation study of the hyperparameters space, as well as the effect of using schedulers on each of them. For these studies, we tested the optimizer in CIFAR10 using the BinaryNet architecture. Also, we tested it in ImageNet 2012 with the XnorNet and BiRealNet architectures for accuracy. In both datasets our approach proved to converge faster, was robust to changes of the hyperparameters, and achieved better accuracy values.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2072732454",
                        "name": "C. Suarez-Ramirez"
                    },
                    {
                        "authorId": "1390007889",
                        "name": "M. Gonz\u00e1lez-Mendoza"
                    },
                    {
                        "authorId": "1409262328",
                        "name": "Leonardo Chang-Fern\u00e1ndez"
                    },
                    {
                        "authorId": "1398679483",
                        "name": "G. Ochoa-Ruiz"
                    },
                    {
                        "authorId": "2072732885",
                        "name": "M. Duran-Vega"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "viewed the latent weights as inertia, and introduced a BNN-specific optimizer called Binary Optimizer (Bop) [43] for the training."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a3912f4251b194b026d9a656b6db64fce6920ebd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-10044",
                    "ArXiv": "2104.10044",
                    "DOI": "10.1016/j.micpro.2021.104359",
                    "CorpusId": 233307050
                },
                "corpusId": 233307050,
                "publicationVenue": {
                    "id": "0307d116-8505-412a-a866-605d66251194",
                    "name": "Microprocessors and microsystems",
                    "type": "journal",
                    "alternate_names": [
                        "Microprocess microsystems",
                        "Microprocessors and Microsystems",
                        "Microprocess Microsystems"
                    ],
                    "issn": "0141-9331",
                    "url": "https://www.journals.elsevier.com/microprocessors-and-microsystems",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/01419331"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a3912f4251b194b026d9a656b6db64fce6920ebd",
                "title": "BCNN: Binary Complex Neural Network",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110499822",
                        "name": "Yanfei Li"
                    },
                    {
                        "authorId": "4444498",
                        "name": "Tong Geng"
                    },
                    {
                        "authorId": "150909408",
                        "name": "Ang Li"
                    },
                    {
                        "authorId": "2118683251",
                        "name": "Huimin Yu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Besides, new training methods and optimizing tricks for BNNs have been researched for obtaining better classification accuracy (Alizadeh et al., 2018; Bulat & Tzimiropoulos, 2019; Zhu, Dong & Su, 2019; Wang et al., 2019; Hubara et al., 2017; Ghasemzadeh, Samragh & Koushanfar, 2018; Gu et al., 2019; Helwegen et al., 2019; Ding et al., 2019; Martinez et al., 2020)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "31b76dec9008faebdde281f118159517dd7c557f",
                "externalIds": {
                    "PubMedCentral": "8022573",
                    "DBLP": "journals/peerj-cs/Kim21",
                    "DOI": "10.7717/peerj-cs.454",
                    "CorpusId": 233170991,
                    "PubMed": "33834112"
                },
                "corpusId": 233170991,
                "publicationVenue": {
                    "id": "138e58fc-8643-41c2-a92c-068ea4f4c607",
                    "name": "PeerJ Computer Science",
                    "alternate_names": [
                        "Peerj Comput Sci"
                    ],
                    "issn": "2376-5992",
                    "url": "https://peerj.com/archives/?journal=cs",
                    "alternate_urls": [
                        "https://peerj.com/computer-science/",
                        "http://peerj.com/cs"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/31b76dec9008faebdde281f118159517dd7c557f",
                "title": "AresB-Net: accurate residual binarized neural networks using shortcut concatenation and shuffled grouped convolution",
                "abstract": "This article proposes a novel network model to achieve better accurate residual binarized convolutional neural networks (CNNs), denoted as AresB-Net. Even though residual CNNs enhance the classification accuracy of binarized neural networks with increasing feature resolution, the degraded classification accuracy is still the primary concern compared with real-valued residual CNNs. AresB-Net consists of novel basic blocks to amortize the severe error from the binarization, suggesting a well-balanced pyramid structure without downsampling convolution. In each basic block, the shortcut is added to the convolution output and then concatenated, and then the expanded channels are shuffled for the next grouped convolution. In the downsampling when stride >1, our model adopts only the max-pooling layer for generating low-cost shortcut. This structure facilitates the feature reuse from the previous layers, thus alleviating the error from the binarized convolution and increasing the classification accuracy with reduced computational costs and small weight storage requirements. Despite low hardware costs from the binarized computations, the proposed model achieves remarkable classification accuracies on the CIFAR and ImageNet datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109872525",
                        "name": "Hyunjin Kim"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "04e283adccf66742130bde4a4dedcda8f549dd7e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-13630",
                    "ArXiv": "2103.13630",
                    "DOI": "10.1201/9781003162810-13",
                    "CorpusId": 232352683
                },
                "corpusId": 232352683,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/04e283adccf66742130bde4a4dedcda8f549dd7e",
                "title": "A Survey of Quantization Methods for Efficient Neural Network Inference",
                "abstract": "As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose. Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations? This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas. Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice in these applications. Thus, it is not surprising that quantization has emerged recently as an important and very active sub-area of research in the efficient implementation of computations associated with Neural Networks. In this article, we survey approaches to the problem of quantizing the numerical values in deep Neural Network computations, covering the advantages/disadvantages of current methods. With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "10419477",
                        "name": "A. Gholami"
                    },
                    {
                        "authorId": "2109586102",
                        "name": "Sehoon Kim"
                    },
                    {
                        "authorId": "143879884",
                        "name": "Zhen Dong"
                    },
                    {
                        "authorId": "9088433",
                        "name": "Z. Yao"
                    },
                    {
                        "authorId": "1717098",
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "authorId": "1732330",
                        "name": "K. Keutzer"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Inspired by [23], the latent weights, which refer to the realvalued weights used during backpropagation, play an important role in binarizing DNNs."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c8ca3f71a5471eda41b19036cf54f9f15f0c1058",
                "externalIds": {
                    "DBLP": "conf/iccv/XuLL0000J21",
                    "ArXiv": "2103.12369",
                    "DOI": "10.1109/ICCV48922.2021.00515",
                    "CorpusId": 232320506
                },
                "corpusId": 232320506,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/c8ca3f71a5471eda41b19036cf54f9f15f0c1058",
                "title": "ReCU: Reviving the Dead Weights in Binary Neural Networks",
                "abstract": "Binary neural networks (BNNs) have received increasing attention due to their superior reductions of computation and memory. Most existing works focus on either lessening the quantization error by minimizing the gap between the full-precision weights and their binarization or designing a gradient approximation to mitigate the gradient mismatch, while leaving the \"dead weights\" untouched. This leads to slow convergence when training BNNs. In this paper, for the first time, we explore the influence of \"dead weights\" which refer to a group of weights that are barely updated during the training of BNNs, and then introduce rectified clamp unit (ReCU) to revive the \"dead weights\" for updating. We prove that reviving the \"dead weights\" by ReCU can result in a smaller quantization error. Besides, we also take into account the information entropy of the weights, and then mathematically analyze why the weight standardization can benefit BNNs. We demonstrate the inherent contradiction between minimizing the quantization error and maximizing the information entropy, and then propose an adaptive exponential scheduler to identify the range of the \"dead weights\". By considering the \"dead weights\", our method offers not only faster BNN training, but also state-of-the-art performance on CIFAR-10 and ImageNet, compared with recent methods. Code can be available at https://github.com/z-hXu/ReCU.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2143541728",
                        "name": "Zihan Xu"
                    },
                    {
                        "authorId": "49352079",
                        "name": "Mingbao Lin"
                    },
                    {
                        "authorId": "2144167531",
                        "name": "Jianzhuang Liu"
                    },
                    {
                        "authorId": "2155099622",
                        "name": "Jie Chen"
                    },
                    {
                        "authorId": "144082425",
                        "name": "Ling Shao"
                    },
                    {
                        "authorId": "35350470",
                        "name": "Yue Gao"
                    },
                    {
                        "authorId": "40161651",
                        "name": "Yonghong Tian"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[13] proposed a Binary Optimizer (BOP) which flips the binary weights solely based on the value of their associated momentum (without latent weights per se): if the momentum is large enough and crosses a threshold from below, the binary weight is switched.",
                "[13] suggested that they were not weights in the strictest sense (they are not used at run time) but were only meant to convey inertia for the optimization of the binary weights.",
                "Similarly to [13], we monitor the number of weight flips per epoch and layer-wise in order to tune the hyperparameters of BOP, using the metric:",
                "We first leverage the recent progress made in Binary Neural Networks (BNNs) optimization [13], to binarize the synapses in energy-based models trained by EP."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "0b77c60826c080841d2fc163e93b459eb20a1d34",
                "externalIds": {
                    "ArXiv": "2103.08953",
                    "DBLP": "journals/corr/abs-2103-08953",
                    "DOI": "10.1109/CVPRW53098.2021.00522",
                    "CorpusId": 232240634
                },
                "corpusId": 232240634,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0b77c60826c080841d2fc163e93b459eb20a1d34",
                "title": "Training Dynamical Binary Neural Networks with Equilibrium Propagation",
                "abstract": "Equilibrium Propagation (EP) is an algorithm intrinsically adapted to the training of physical networks, thanks to the local updates of weights given by the internal dynamics of the system. However, the construction of such a hardware requires to make the algorithm compatible with existing neuromorphic CMOS technologies, which generally exploit digital communication between neurons and offer a limited amount of local memory. In this work, we demonstrate that EP can train dynamical networks with binary activations and weights. We first train systems with binary weights and full-precision activations, achieving an accuracy equivalent to that of full-precision models trained by standard EP on MNIST, and losing only 1.9% accuracy on CIFAR-10 with equal architecture. We then extend our method to the training of models with binary activations and weights on MNIST, achieving an accuracy within 1% of the full-precision reference for fully connected architectures and reaching the full-precision accuracy for convolutional architectures. Our extension of EP to binary networks opens new solutions for on-chip learning and provides a compact framework for training BNNs end-to-end with the same circuitry as for inference.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1995844892",
                        "name": "J\u00e9r\u00e9mie Laydevant"
                    },
                    {
                        "authorId": "5904564",
                        "name": "M. Ernoult"
                    },
                    {
                        "authorId": "1793153",
                        "name": "D. Querlioz"
                    },
                    {
                        "authorId": "2595706",
                        "name": "J. Grollier"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6ded1348c76d25a420abe85a70b91e9ed6f6c2b8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-00841",
                    "ArXiv": "2103.00841",
                    "CorpusId": 232075671
                },
                "corpusId": 232075671,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6ded1348c76d25a420abe85a70b91e9ed6f6c2b8",
                "title": "Learning Frequency Domain Approximation for Binary Neural Networks",
                "abstract": "Binary neural networks (BNNs) represent original full-precision weights and activations into 1-bit with sign function. Since the gradient of the conventional sign function is almost zero everywhere which cannot be used for back-propagation, several attempts have been proposed to alleviate the optimization difficulty by using approximate gradient. However, those approximations corrupt the main direction of factual gradient. To this end, we propose to estimate the gradient of sign function in the Fourier frequency domain using the combination of sine functions for training BNNs, namely frequency domain approximation (FDA). The proposed approach does not affect the low-frequency information of the original sign function which occupies most of the overall energy, and high-frequency coefficients will be ignored to avoid the huge computational overhead. In addition, we embed a noise adaptation module into the training phase to compensate the approximation error. The experiments on several benchmark datasets and neural architectures illustrate that the binary network learned using our method achieves the state-of-the-art accuracy. Code will be available at \\textit{https://gitee.com/mindspore/models/tree/master/research/cv/FDA-BNN}.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "6898202",
                        "name": "Yixing Xu"
                    },
                    {
                        "authorId": "3826388",
                        "name": "Kai Han"
                    },
                    {
                        "authorId": null,
                        "name": "Chang Xu"
                    },
                    {
                        "authorId": "103603255",
                        "name": "Yehui Tang"
                    },
                    {
                        "authorId": "1691522",
                        "name": "Chunjing Xu"
                    },
                    {
                        "authorId": "2108702980",
                        "name": "Yunhe Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "There are also abundant works that explore the optimization of BNNs [33], [45], [46], [47], [48] and explain their effectiveness [49]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "331bbb8107b3f300f39ebeaeb263e8cd2fdc101e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-07981",
                    "ArXiv": "2102.07981",
                    "DOI": "10.1109/TPAMI.2022.3212615",
                    "CorpusId": 231934194,
                    "PubMed": "36215372"
                },
                "corpusId": 231934194,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/331bbb8107b3f300f39ebeaeb263e8cd2fdc101e",
                "title": "SiMaN: Sign-to-Magnitude Network Binarization",
                "abstract": "Binary neural networks (BNNs) have attracted broad research interest due to their efficient storage and computational ability. Nevertheless, a significant challenge of BNNs lies in handling discrete constraints while ensuring bit entropy maximization, which typically makes their weight optimization very difficult. Existing methods relax the learning using the sign function, which simply encodes positive weights into <inline-formula><tex-math notation=\"LaTeX\">$+1$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"ji-ieq1-3212615.gif\"/></alternatives></inline-formula>s, and <inline-formula><tex-math notation=\"LaTeX\">$-1$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"ji-ieq2-3212615.gif\"/></alternatives></inline-formula>s otherwise. Alternatively, we formulate an angle alignment objective to constrain the weight binarization to <inline-formula><tex-math notation=\"LaTeX\">$\\lbrace 0,+1\\rbrace$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"ji-ieq3-3212615.gif\"/></alternatives></inline-formula> to solve the challenge. In this article, we show that our weight binarization provides an analytical solution by encoding high-magnitude weights into <inline-formula><tex-math notation=\"LaTeX\">$+1$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"ji-ieq4-3212615.gif\"/></alternatives></inline-formula>s, and 0s otherwise. Therefore, a high-quality discrete solution is established in a computationally efficient manner without the sign function. We prove that the learned weights of binarized networks roughly follow a Laplacian distribution that does not allow entropy maximization, and further demonstrate that it can be effectively solved by simply removing the <inline-formula><tex-math notation=\"LaTeX\">$\\ell _{2}$</tex-math><alternatives><mml:math><mml:msub><mml:mi>\u2113</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href=\"ji-ieq5-3212615.gif\"/></alternatives></inline-formula> regularization during network training. Our method, dubbed sign-to-magnitude network binarization (SiMaN), is evaluated on CIFAR-10 and ImageNet, demonstrating its superiority over the sign-based state-of-the-arts. Our source code, experimental settings, training logs and binary models are available at <uri>https://github.com/lmbxmu/SiMaN</uri>.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49352079",
                        "name": "Mingbao Lin"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    },
                    {
                        "authorId": "48559591",
                        "name": "Zi-Han Xu"
                    },
                    {
                        "authorId": "1740430",
                        "name": "Baochang Zhang"
                    },
                    {
                        "authorId": "2070266415",
                        "name": "Fei Chao"
                    },
                    {
                        "authorId": "2285442",
                        "name": "Mingliang Xu"
                    },
                    {
                        "authorId": "46246806",
                        "name": "Chia-Wen Lin"
                    },
                    {
                        "authorId": "144082425",
                        "name": "Ling Shao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3c3e9c0864fca9225c0291ed74b75af92fdd1779",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-04270",
                    "ArXiv": "2102.04270",
                    "DOI": "10.1145/3626100",
                    "CorpusId": 231846514
                },
                "corpusId": 231846514,
                "publicationVenue": {
                    "id": "13925159-5423-4d57-9ac9-3a05a713e0ef",
                    "name": "ACM Transactions on Embedded Computing Systems",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Transactions in Embedded Computing Systems",
                        "ACM Trans Embed Comput Syst"
                    ],
                    "issn": "1539-9087",
                    "url": "http://www.acm.org/pubs/contents/journals/tecs/",
                    "alternate_urls": [
                        "http://www.acm.org/tecs/",
                        "http://portal.acm.org/tecs"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3c3e9c0864fca9225c0291ed74b75af92fdd1779",
                "title": "Enabling Binary Neural Network Training on the Edge",
                "abstract": "The ever-growing computational demands of increasingly complex machine learning models frequently necessitate the use of powerful cloud-based infrastructure for their training. Binary neural networks are known to be promising candidates for on-device inference due to their extreme compute and memory savings over higher-precision alternatives. However, their existing training methods require the concurrent storage of high-precision activations for all layers, generally making learning on memory-constrained devices infeasible. In this article, we demonstrate that the backward propagation operations needed for binary neural network training are strongly robust to quantization, thereby making on-the-edge learning with modern models a practical proposition. We introduce a low-cost binary neural network training strategy exhibiting sizable memory footprint reductions while inducing little to no accuracy loss vs Courbariaux & Bengio\u2019s standard approach. These decreases are primarily enabled through the retention of activations exclusively in binary format. Against the latter algorithm, our drop-in replacement sees memory requirement reductions of 3\u20135 \u00d7, while reaching similar test accuracy (\u00b1 2 pp) in comparable time, across a range of small-scale models trained to classify popular datasets. We also demonstrate from-scratch ImageNet training of binarized ResNet-18, achieving a 3.78 \u00d7 memory reduction. Our work is open-source, and includes the Raspberry Pi-targeted prototype we used to verify our modeled memory decreases and capture the associated energy drops. Such savings will allow for unnecessary cloud offloading to be avoided, reducing latency, increasing energy efficiency, and safeguarding end-user privacy.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "32468485",
                        "name": "Erwei Wang"
                    },
                    {
                        "authorId": "46549179",
                        "name": "James J. Davis"
                    },
                    {
                        "authorId": "2064469062",
                        "name": "Daniele Moro"
                    },
                    {
                        "authorId": "2079501",
                        "name": "Piotr Zielinski"
                    },
                    {
                        "authorId": "2060330564",
                        "name": "C. Coelho"
                    },
                    {
                        "authorId": "2743487",
                        "name": "S. Chatterjee"
                    },
                    {
                        "authorId": "144480916",
                        "name": "P. Cheung"
                    },
                    {
                        "authorId": "35011876",
                        "name": "G. Constantinides"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "useful in another latest work on optimizing binary neural networks (BNNs) [47]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b1ef6bc7598cb7d337de36892356d8a79bcc280f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-01163",
                    "ArXiv": "2101.01163",
                    "DOI": "10.1109/TNNLS.2021.3138056",
                    "CorpusId": 230437922,
                    "PubMed": "35235521"
                },
                "corpusId": 230437922,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b1ef6bc7598cb7d337de36892356d8a79bcc280f",
                "title": "SmartDeal: Remodeling Deep Network Weights for Efficient Inference and Training",
                "abstract": "The record-breaking performance of deep neural networks (DNNs) comes with heavy parameter budgets, which leads to external dynamic random access memory (DRAM) for storage. The prohibitive energy of DRAM accesses makes it nontrivial for DNN deployment on resource-constrained devices, calling for minimizing the movements of weights and data in order to improve the energy efficiency. Driven by this critical bottleneck, we present SmartDeal, a hardware-friendly algorithm framework to trade higher-cost memory storage/access for lower-cost computation, in order to aggressively boost the storage and energy efficiency, for both DNN inference and training. The core technique of SmartDeal is a novel DNN weight matrix decomposition framework with respective structural constraints on each matrix factor, carefully crafted to unleash the hardware-aware efficiency potential. Specifically, we decompose each weight tensor as the product of a small basis matrix and a large structurally sparse coefficient matrix whose nonzero elements are readily quantized to the power-of-2. The resulting sparse and readily quantized DNNs enjoy greatly reduced energy consumption in data movement as well as weight storage, while incurring minimal overhead to recover the original weights thanks to the required sparse bit-operations and cost-favorable computations. Beyond inference, we take another leap to embrace energy-efficient training, by introducing several customized techniques to address the unique roadblocks arising in training while preserving the SmartDeal structures. We also design a dedicated hardware accelerator to fully utilize the new weight structure to improve the real energy efficiency and latency performance. We conduct experiments on both vision and language tasks, with nine models, four datasets, and three settings (inference-only, adaptation, and fine-tuning). Our extensive results show that 1) being applied to inference, SmartDeal achieves up to $2.44\\times $ improvement in energy efficiency as evaluated using real hardware implementations and 2) being applied to training, SmartDeal can lead to $10.56\\times $ and $4.48\\times $ reduction in the storage and the training energy cost, respectively, with usually negligible accuracy loss, compared to state-of-the-art training baselines. Our source codes are available at: https://github.com/VITA-Group/SmartDeal.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "66273769",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "2118834108",
                        "name": "Yang Zhao"
                    },
                    {
                        "authorId": "2118461722",
                        "name": "Yue Wang"
                    },
                    {
                        "authorId": "2153916160",
                        "name": "Pengfei Xu"
                    },
                    {
                        "authorId": "47113848",
                        "name": "Haoran You"
                    },
                    {
                        "authorId": "28987646",
                        "name": "Chaojian Li"
                    },
                    {
                        "authorId": "108145103",
                        "name": "Y. Fu"
                    },
                    {
                        "authorId": "3138925",
                        "name": "Yingyan Lin"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "41438b5e345da7cd87d573f6b06a54d0f1c727ba",
                "externalIds": {
                    "ArXiv": "2011.09398",
                    "DBLP": "journals/corr/abs-2011-09398",
                    "MAG": "3102865167",
                    "CorpusId": 227013458
                },
                "corpusId": 227013458,
                "publicationVenue": {
                    "id": "3bcf77b3-860b-4dd7-84ae-9fe9414c6c6a",
                    "name": "Conference on Machine Learning and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "MLSys",
                        "Conf Mach Learn Syst"
                    ],
                    "url": "https://mlsys.org/"
                },
                "url": "https://www.semanticscholar.org/paper/41438b5e345da7cd87d573f6b06a54d0f1c727ba",
                "title": "Larq Compute Engine: Design, Benchmark, and Deploy State-of-the-Art Binarized Neural Networks",
                "abstract": "We introduce Larq Compute Engine, the world's fastest Binarized Neural Network (BNN) inference engine, and use this framework to investigate several important questions about the efficiency of BNNs and to design a new state-of-the-art BNN architecture. LCE provides highly optimized implementations of binary operations and accelerates binary convolutions by 8.5 - 18.5x compared to their full-precision counterparts on Pixel 1 phones. LCE's integration with Larq and a sophisticated MLIR-based converter allow users to move smoothly from training to deployment. By extending TensorFlow and TensorFlow Lite, LCE supports models which combine binary and full-precision layers, and can be easily integrated into existing applications. Using LCE, we analyze the performance of existing BNN computer vision architectures and develop QuickNet, a simple, easy-to-reproduce BNN that outperforms existing binary networks in terms of latency and accuracy on ImageNet. Furthermore, we investigate the impact of full-precision shortcuts and the relationship between number of MACs and model latency. We are convinced that empirical performance should drive BNN architecture design and hope this work will facilitate others to design, benchmark and deploy binary models.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3451343",
                        "name": "T. Bannink"
                    },
                    {
                        "authorId": "7280215",
                        "name": "A. Bakhtiari"
                    },
                    {
                        "authorId": "2073268697",
                        "name": "Adam Hillier"
                    },
                    {
                        "authorId": "102673339",
                        "name": "Lukas Geiger"
                    },
                    {
                        "authorId": "10412912",
                        "name": "T. D. Bruin"
                    },
                    {
                        "authorId": "30077774",
                        "name": "Leon Overweel"
                    },
                    {
                        "authorId": "51131050",
                        "name": "J. Neeven"
                    },
                    {
                        "authorId": "108586041",
                        "name": "K. Helwegen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0fa13a5ef36168ff3fd08b03fd30f1f935d6a18a",
                "externalIds": {
                    "ArXiv": "2011.00241",
                    "DBLP": "journals/access/VaderaA22",
                    "MAG": "3096215947",
                    "DOI": "10.1109/ACCESS.2022.3182659",
                    "CorpusId": 226226764
                },
                "corpusId": 226226764,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0fa13a5ef36168ff3fd08b03fd30f1f935d6a18a",
                "title": "Methods for Pruning Deep Neural Networks",
                "abstract": "This paper presents a survey of methods for pruning deep neural networks. It begins by categorising over 150 studies based on the underlying approach used and then focuses on three categories: methods that use magnitude based pruning, methods that utilise clustering to identify redundancy, and methods that use sensitivity analysis to assess the effect of pruning. Some of the key influencing studies within these categories are presented to highlight the underlying approaches and results achieved. Most studies present results which are distributed in the literature as new architectures, algorithms and data sets have developed with time, making comparison across different studied difficult. The paper therefore provides a resource for the community that can be used to quickly compare the results from many different methods on a variety of data sets, and a range of architectures, including AlexNet, ResNet, DenseNet and VGG. The resource is illustrated by comparing the results published for pruning AlexNet and ResNet50 on ImageNet and ResNet56 and VGG16 on the CIFAR10 data to reveal which pruning methods work well in terms of retaining accuracy whilst achieving good compression rates. The paper concludes by identifying some research gaps and promising directions for future research.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3197685",
                        "name": "S. Vadera"
                    },
                    {
                        "authorId": "32051512",
                        "name": "Salem Ameen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[52] took the view that the real-valued latent weights cannot be treated analogously to weights in realvalued networks, while their main role is to provide inertia during training."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "09ea3fbcc2d28b7dacffa86934307f0eeea525b2",
                "externalIds": {
                    "ArXiv": "2011.14824",
                    "DBLP": "journals/corr/abs-2011-14824",
                    "MAG": "3094844246",
                    "DOI": "10.1109/JMASS.2020.3034205",
                    "CorpusId": 227227911
                },
                "corpusId": 227227911,
                "publicationVenue": {
                    "id": "38faca20-d93d-4137-892a-332e3bede55a",
                    "name": "IEEE Journal on Miniaturization for Air and Space Systems",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE J Miniaturization Air Space Syst"
                    ],
                    "issn": "2576-3164",
                    "url": "https://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=8253411"
                },
                "url": "https://www.semanticscholar.org/paper/09ea3fbcc2d28b7dacffa86934307f0eeea525b2",
                "title": "A Review of Recent Advances of Binary Neural Networks for Edge Computing",
                "abstract": "Edge computing is promising to become one of the next hottest topics in artificial intelligence because it benefits various evolving domains, such as real-time unmanned aerial systems, industrial applications, and the demand for privacy protection. This article reviews the recent advances on binary neural network (BNN) and 1-bit convolutional neural network technologies that are well suitable for front-end, edge-based computing. We introduce and summarize existing work and classify them based on gradient approximation, quantization, architecture, loss functions, optimization method, and binary neural architecture search. We also introduce applications in the areas of computer vision and speech recognition and discuss future applications for edge computing.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2118227588",
                        "name": "Wenyu Zhao"
                    },
                    {
                        "authorId": "1390452961",
                        "name": "Teli Ma"
                    },
                    {
                        "authorId": "2075407256",
                        "name": "Xuan Gong"
                    },
                    {
                        "authorId": "1740430",
                        "name": "Baochang Zhang"
                    },
                    {
                        "authorId": "48471936",
                        "name": "D. Doermann"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Shrinking the cost-dominate multiplications has been widely considered in many DNN designs for reducing the computational complexity [10, 11]: [10] decomposes the convolutions into separate depthwise and pointwise modules which require fewer multiplications; and [12, 13, 14] binarize the weights or activations to construct DNNs consisting of sign changes paired with much fewer multiplications."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a124ef47b879d80f1379b07edfe597ba20cebdd5",
                "externalIds": {
                    "ArXiv": "2010.12785",
                    "MAG": "3106390353",
                    "DBLP": "journals/corr/abs-2010-12785",
                    "CorpusId": 225067910
                },
                "corpusId": 225067910,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a124ef47b879d80f1379b07edfe597ba20cebdd5",
                "title": "ShiftAddNet: A Hardware-Inspired Deep Network",
                "abstract": "Multiplication (e.g., convolution) is arguably a cornerstone of modern deep neural networks (DNNs). However, intensive multiplications cause expensive resource costs that challenge DNNs' deployment on resource-constrained edge devices, driving several attempts for multiplication-less deep networks. This paper presented ShiftAddNet, whose main inspiration is drawn from a common practice in energy-efficient hardware implementation, that is, multiplication can be instead performed with additions and logical bit-shifts. We leverage this idea to explicitly parameterize deep networks in this way, yielding a new type of deep network that involves only bit-shift and additive weight layers. This hardware-inspired ShiftAddNet immediately leads to both energy-efficient inference and training, without compromising the expressive capacity compared to standard DNNs. The two complementary operation types (bit-shift and add) additionally enable finer-grained control of the model's learning capacity, leading to more flexible trade-off between accuracy and (training) efficiency, as well as improved robustness to quantization and pruning. We conduct extensive experiments and ablation studies, all backed up by our FPGA-based ShiftAddNet implementation and energy measurements. Compared to existing DNNs or other multiplication-less models, ShiftAddNet aggressively reduces over 80% hardware-quantified energy cost of DNNs training and inference, while offering comparable or better accuracies. Codes and pre-trained models are available at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "47113848",
                        "name": "Haoran You"
                    },
                    {
                        "authorId": "66273769",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "2108455959",
                        "name": "Yongan Zhang"
                    },
                    {
                        "authorId": "28987646",
                        "name": "Chaojian Li"
                    },
                    {
                        "authorId": "3285742",
                        "name": "Sicheng Li"
                    },
                    {
                        "authorId": "1994319142",
                        "name": "Zihao Liu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "3138925",
                        "name": "Yingyan Lin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                ", 2019), Bop (Helwegen et al., 2019), GBCN (Liu et al.",
                "The compared binary neural network methods include BinaryNet (Hubara et al., 2016), Dorefa-Net (Zhou et al., 2016), XNOR-Net (Rastegari et al., 2016), Bireal-Net (Liu et al., 2018), PCNN (Gu et al., 2019), Bop (Helwegen et al., 2019), GBCN (Liu et al., 2019), etc.",
                "3% Bop (Helwegen et al., 2019) 1 1 34 Mbit 163 M 54."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "98c7509d3845b5be1c9ea6103d38a12ab60266e5",
                "externalIds": {
                    "DBLP": "conf/icml/HanWXXWX20",
                    "MAG": "3093994940",
                    "ArXiv": "2010.04871",
                    "CorpusId": 221088631
                },
                "corpusId": 221088631,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/98c7509d3845b5be1c9ea6103d38a12ab60266e5",
                "title": "Training Binary Neural Networks through Learning with Noisy Supervision",
                "abstract": "This paper formalizes the binarization operations over neural networks from a learning perspective. In contrast to classical hand crafted rules (\\eg hard thresholding) to binarize full-precision neurons, we propose to learn a mapping from full-precision neurons to the target binary ones. Each individual weight entry will not be binarized independently. Instead, they are taken as a whole to accomplish the binarization, just as they work together in generating convolution features. To help the training of the binarization mapping, the full-precision neurons after taking sign operations is regarded as some auxiliary supervision signal, which is noisy but still has valuable guidance. An unbiased estimator is therefore introduced to mitigate the influence of the supervision noise. Experimental results on benchmark datasets indicate that the proposed binarization technique attains consistent improvements over baselines.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3826388",
                        "name": "Kai Han"
                    },
                    {
                        "authorId": "2108702980",
                        "name": "Yunhe Wang"
                    },
                    {
                        "authorId": "6898202",
                        "name": "Yixing Xu"
                    },
                    {
                        "authorId": "1691522",
                        "name": "Chunjing Xu"
                    },
                    {
                        "authorId": "145344139",
                        "name": "E. Wu"
                    },
                    {
                        "authorId": null,
                        "name": "Chang Xu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4601c9cff54825bffef98c78f113a78039ee356c",
                "externalIds": {
                    "MAG": "3092385463",
                    "DBLP": "journals/corr/abs-2010-05077",
                    "ArXiv": "2010.05077",
                    "CorpusId": 222290478
                },
                "corpusId": 222290478,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4601c9cff54825bffef98c78f113a78039ee356c",
                "title": "Maximin Optimization for Binary Regression",
                "abstract": "We consider regression problems with binary weights. Such optimization problems are ubiquitous in quantized learning models and digital communication systems. A natural approach is to optimize the corresponding Lagrangian using variants of the gradient ascent-descent method. Such maximin techniques are still poorly understood even in the concave-convex case. The non-convex binary constraints may lead to spurious local minima. Interestingly, we prove that this approach is optimal in linear regression with low noise conditions as well as robust regression with a small number of outliers. Practically, the method also performs well in regression with cross entropy loss, as well as non-convex multi-layer neural networks. Taken together our approach highlights the potential of saddle-point optimization for learning constrained models.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1993970015",
                        "name": "Nisan Chiprut"
                    },
                    {
                        "authorId": "1786843",
                        "name": "A. Globerson"
                    },
                    {
                        "authorId": "2327205",
                        "name": "A. Wiesel"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "BNN [22] 56."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "06c4ba737d5bae10de2476ccc902bec6fb76cfc2",
                "externalIds": {
                    "MAG": "3092259765",
                    "ArXiv": "2010.03558",
                    "DBLP": "journals/corr/abs-2010-03558",
                    "CorpusId": 222177403
                },
                "corpusId": 222177403,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/06c4ba737d5bae10de2476ccc902bec6fb76cfc2",
                "title": "High-Capacity Expert Binary Networks",
                "abstract": "Network binarization is a promising hardware-aware direction for creating efficient deep models. Despite its memory and computational advantages, reducing the accuracy gap between such models and their real-valued counterparts remains an unsolved challenging research problem. To this end, we make the following 3 contributions: (a) To increase model capacity, we propose Expert Binary Convolution, which, for the first time, tailors conditional computing to binary networks by learning to select one data-specific expert binary filter at a time conditioned on input features. (b) To increase representation capacity, we propose to address the inherent information bottleneck in binary networks by introducing an efficient width expansion mechanism which keeps the binary operations within the same budget. (c) To improve network design, we propose a principled binary network growth mechanism that unveils a set of network topologies of favorable properties. Overall, our method improves upon prior work, with no increase in computational cost by ~6%, reaching a groundbreaking ~71% on ImageNet classification.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145245424",
                        "name": "Adrian Bulat"
                    },
                    {
                        "authorId": "145944235",
                        "name": "Brais Mart\u00ednez"
                    },
                    {
                        "authorId": "2610880",
                        "name": "Georgios Tzimiropoulos"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "To compensate, [21, 37] strengthen the learning ability of BNNs during network training via increasing the probability of weight flips.",
                "To solve this problem, many compression techniques have been proposed including network pruning [30, 14, 29], low-rank decomposition [12, 43, 18], efficient architecture design [24, 42, 7] and network quantization [28, 2, 21], etc.",
                "However, the scaling factor results in a small ratio of flipping weights thus leading to little information gain in the training process [21, 37]2."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c5546f28c2fdf5a32fe3298e47f552fd28e1cf76",
                "externalIds": {
                    "MAG": "3104427119",
                    "DBLP": "journals/corr/abs-2009-13055",
                    "ArXiv": "2009.13055",
                    "CorpusId": 221970417
                },
                "corpusId": 221970417,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c5546f28c2fdf5a32fe3298e47f552fd28e1cf76",
                "title": "Rotated Binary Neural Network",
                "abstract": "Binary Neural Network (BNN) shows its predominance in reducing the complexity of deep neural networks. However, it suffers severe performance degradation. One of the major impediments is the large quantization error between the full-precision weight vector and its binary vector. Previous works focus on compensating for the norm gap while leaving the angular bias hardly touched. In this paper, for the first time, we explore the influence of angular bias on the quantization error and then introduce a Rotated Binary Neural Network (RBNN), which considers the angle alignment between the full-precision weight vector and its binarized version. At the beginning of each training epoch, we propose to rotate the full-precision weight vector to its binary vector to reduce the angular bias. To avoid the high complexity of learning a large rotation matrix, we further introduce a bi-rotation formulation that learns two smaller rotation matrices. In the training stage, we devise an adjustable rotated weight vector for binarization to escape the potential local optimum. Our rotation leads to around 50% weight flips which maximize the information gain. Finally, we propose a training-aware approximation of the sign function for the gradient backward. Experiments on CIFAR-10 and ImageNet demonstrate the superiorities of RBNN over many state-of-the-arts. Our source code, experimental settings, training logs and binary models are available at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "49352079",
                        "name": "Mingbao Lin"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    },
                    {
                        "authorId": "48559591",
                        "name": "Zi-Han Xu"
                    },
                    {
                        "authorId": "1740430",
                        "name": "Baochang Zhang"
                    },
                    {
                        "authorId": "2152543905",
                        "name": "Yan Wang"
                    },
                    {
                        "authorId": "47096329",
                        "name": "Yongjian Wu"
                    },
                    {
                        "authorId": "1835006",
                        "name": "Feiyue Huang"
                    },
                    {
                        "authorId": "46246806",
                        "name": "Chia-Wen Lin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "A common problem in existing quantization methods is the estimated gradients for quantization functions, using either STE [6, 39] or self-designed gradient computation manner [34].",
                "A common problem in existing quantization methods is the estimated gradients for quantization functions, using either STE [8, 52, 29, 47, 59, 4] or self-designed gradient computation manner [46].",
                "The Straight Through Estimator (STE) [6] is a widely used strategy to estimate the gradient of W latq , i.e.,\n\u2207W latq = \u2207Wq (2)\nwhere the approximate gradients\u2207W latq are use to update weights W lat q ,\n\u02c6W latq =W lat q \u2212 \u03b7 \u00b7 \u2207W latq \u00b7 \u03c3(\u2207W latq ), (3)\nwhere \u03b7 is the learning rate and \u03c3(\u00b7) is the gradient clip function."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b0349a32497cf2586d784b639999c38d6a167c9d",
                "externalIds": {
                    "ArXiv": "2009.08695",
                    "DBLP": "conf/nips/YangWHX0T020",
                    "MAG": "3087150032",
                    "CorpusId": 221802416
                },
                "corpusId": 221802416,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b0349a32497cf2586d784b639999c38d6a167c9d",
                "title": "Searching for Low-Bit Weights in Quantized Neural Networks",
                "abstract": "Quantized neural networks with low-bit weights and activations are attractive for developing AI accelerators. However, the quantization functions used in most conventional quantization methods are non-differentiable, which increases the optimization difficulty of quantized networks. Compared with full-precision parameters (i.e., 32-bit floating numbers), low-bit values are selected from a much smaller set. For example, there are only 16 possibilities in 4-bit space. Thus, we present to regard the discrete weights in an arbitrary quantized neural network as searchable variables, and utilize a differential method to search them accurately. In particular, each weight is represented as a probability distribution over the discrete value set. The probabilities are optimized during training and the values with the highest probability are selected to establish the desired quantized network. Experimental results on benchmarks demonstrate that the proposed method is able to produce quantized neural networks with higher performance over the state-of-the-art methods on both image classification and super-resolution tasks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2116369043",
                        "name": "Zhaohui Yang"
                    },
                    {
                        "authorId": "2108702980",
                        "name": "Yunhe Wang"
                    },
                    {
                        "authorId": "3826388",
                        "name": "Kai Han"
                    },
                    {
                        "authorId": "1691522",
                        "name": "Chunjing Xu"
                    },
                    {
                        "authorId": "2004428678",
                        "name": "Chao Xu"
                    },
                    {
                        "authorId": "143719920",
                        "name": "D. Tao"
                    },
                    {
                        "authorId": null,
                        "name": "Chang Xu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                " of network lters pruning [31,14,30] and compact architecture design [26,6], how to alleviate the performance degradation in quantized model [19] is still unsolved, especially for the binarized model [39,1,15]. Deterministic Weight Quantization Through introducing a deterministic function, traditional methods quantize network weights (or activations) by minimizing quantization errors. For examples, BinaryC",
                "g the vanishing gradients. The Straight-Through Estimator (STE) [3] is commonly used to estimate the vanishing gradients during the back-propagation, while the well-known gradient mismatching problem [39,19,15] is introduced. As the number of quantized bits decrease, the gradients estimated by STE depart further from the real gradients. Thus, the gradient mismatching is considered as the main bottleneck of "
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "900a8c0674ff89b1c5e440b89477a8acd58e48af",
                "externalIds": {
                    "MAG": "3084424860",
                    "DBLP": "journals/corr/abs-2009-04626",
                    "ArXiv": "2009.04626",
                    "DOI": "10.1007/978-3-030-68238-5_4",
                    "CorpusId": 221586321
                },
                "corpusId": 221586321,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/900a8c0674ff89b1c5e440b89477a8acd58e48af",
                "title": "QuantNet: Learning to Quantize by Learning within Fully Differentiable Framework",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2108526145",
                        "name": "Junjie Liu"
                    },
                    {
                        "authorId": "115712451",
                        "name": "Dongchao Wen"
                    },
                    {
                        "authorId": "1844767631",
                        "name": "Deyu Wang"
                    },
                    {
                        "authorId": "2112528277",
                        "name": "Wei Tao"
                    },
                    {
                        "authorId": "2145223699",
                        "name": "Tse-Wei Chen"
                    },
                    {
                        "authorId": "47373253",
                        "name": "Kinya Osa"
                    },
                    {
                        "authorId": "1845879900",
                        "name": "Masami Kato"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Following this insight, the authors of [19] proposed the first optimizer that is specifically designed for BNN known as Binary Optimizer (Bop).",
                "In [19], it is shown that this is not necessarily true and instead the real values parameters should be viewed as inertia parameters for each binary weights.",
                "Following initialization, these values should be seen as the momentum of inertia [19] in rest and it may take a few epochs to break out of this, and then, the binary weights start flipping the sign."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "b888f157d84bf6ab8a47dba8de52de05454e3d43",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-07522",
                    "ArXiv": "2006.07522",
                    "MAG": "3034922011",
                    "CorpusId": 219687176
                },
                "corpusId": 219687176,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b888f157d84bf6ab8a47dba8de52de05454e3d43",
                "title": "Understanding Learning Dynamics of Binary Neural Networks via Information Bottleneck",
                "abstract": "Compact neural networks are essential for affordable and power efficient deep learning solutions. Binary Neural Networks (BNNs) take compactification to the extreme by constraining both weights and activations to two levels, $\\{+1, -1\\}$. However, training BNNs are not easy due to the discontinuity in activation functions, and the training dynamics of BNNs is not well understood. In this paper, we present an information-theoretic perspective of BNN training. We analyze BNNs through the Information Bottleneck principle and observe that the training dynamics of BNNs is considerably different from that of Deep Neural Networks (DNNs). While DNNs have a separate empirical risk minimization and representation compression phases, our numerical experiments show that in BNNs, both these phases are simultaneous. Since BNNs have a less expressive capacity, they tend to find efficient hidden representations concurrently with label fitting. Experiments in multiple datasets support these observations, and we see a consistent behavior across different activation functions in BNNs.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "46637452",
                        "name": "Vishnu Raj"
                    },
                    {
                        "authorId": "151152600",
                        "name": "Nancy Nayak"
                    },
                    {
                        "authorId": "143968719",
                        "name": "S. Kalyani"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Responding to the work \u201cLatent weights do not exist: Rethinking binarized neural network optimization\u201d [24] and the lack of formal basis to introduce latent weights in the literature (e.",
                "[24] argues that \u201clatent weights do not exist\u201d meaning that discrete optimization over binary weights needs to be considered."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1da436b5c3340d4a5710cbf3e8ee93ba8d2177c4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-06880",
                    "ArXiv": "2006.06880",
                    "MAG": "3034730115",
                    "DOI": "10.1007/978-3-030-92659-5_7",
                    "CorpusId": 219636350
                },
                "corpusId": 219636350,
                "publicationVenue": {
                    "id": "4bdc459e-68eb-4596-8e24-85dd8a047952",
                    "name": "German Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Ger Conf Pattern Recognit",
                        "GCPR"
                    ],
                    "url": "http://www.dagm.de/"
                },
                "url": "https://www.semanticscholar.org/paper/1da436b5c3340d4a5710cbf3e8ee93ba8d2177c4",
                "title": "Reintroducing Straight-Through Estimators as Principled Methods for Stochastic Binary Networks",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "66784412",
                        "name": "V. Yanush"
                    },
                    {
                        "authorId": "145965041",
                        "name": "A. Shekhovtsov"
                    },
                    {
                        "authorId": "8796171",
                        "name": "Dmitry Molchanov"
                    },
                    {
                        "authorId": "2492721",
                        "name": "D. Vetrov"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Moreover, the trained model can achieve comparable performance than the specific discrete optimizers [39, 18].",
                "1In this paper, we follow the existing works [18] that focus on the most extreme case of defining the deep quantized model as the BNNs.",
                "Despite the achieved success from gradient approximation, the optimization method [18] argues that the latent variable is not necessary for training BNNs and raises a question if a general optimization method exists to well optimize both full-precision and quantized models with the theoretical guarantee.",
                "Next, we generate the empirical results through comparing our algorithm with existing optimization methods including SGD(M) [40], Adam [10], AMSGrad [42], AdaBound [36], and specific binary optimization method [18], and it mainly evaluates these optimization methods in the setting with different datasets and network architectures.",
                "In brief, for a 3-Layer LSTM models, our method improves the performance than the traditionally adaptive methods [42, 36] and specific binary optimizer [18] in terms of perplexity.",
                "Specially, the binary weights are not learned directly, but are learned with the scaling factor [18] during the training.",
                "The resulting algorithm has a very similar convergence speed of training the full-precision models with the adaptive method [36] even better than the specific binary optimizer [18].",
                "For the generalization ability shown in the test accuracy, we find that our method always obtains the best accuracy by comparing to the traditional adaptive methods [42, 36] and specific binary optimizer [18].",
                "9) mAP than the latest optimization methods [42, 36, 18].",
                "Furthermore, the Bop [18] claims that the momentum estimated by past gradients history [48] is the key issue, as it can avoid a rapid sign change of binary weights during the training."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "58b09b9fb1fd18418a8bea0e849349551148f626",
                "externalIds": {
                    "MAG": "3091253283",
                    "DBLP": "conf/cvpr/LiuWWTCOK20",
                    "ArXiv": "2009.13799",
                    "DOI": "10.1109/CVPRW50498.2020.00345",
                    "CorpusId": 220886980
                },
                "corpusId": 220886980,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/58b09b9fb1fd18418a8bea0e849349551148f626",
                "title": "BAMSProd: A Step towards Generalizing the Adaptive Optimization Methods to Deep Binary Model",
                "abstract": "Recent methods have significantly reduced the performance degradation of Binary Neural Networks (BNNs), but guaranteeing the effective and efficient training of BNNs is an unsolved problem. The main reason is that the estimated gradients produced by the Straight-Through-Estimator (STE) mismatches with the gradients of the real derivatives. In this paper, we provide an explicit convex optimization example where training the BNNs with the traditionally adaptive optimization methods still faces the risk of non-convergence, and identify that constraining the range of gradients is critical for optimizing the deep binary model to avoid highly suboptimal solutions. Besides, we propose a BAMSProd algorithm with a key observation that the convergence property of optimizing deep binary model is strongly related to the quantization errors. In brief, it employs an adaptive range constraint via an errors measurement for smoothing the gradients transition while follows the exponential moving strategy from AMSGrad to avoid errors accumulation during the optimization. The experiments verify the corollary of theoretical convergence analysis, and further demonstrate that our optimization method can speed up the convergence about 1.2\u00d7 and boost the performance of BNNs to a significant level than the specific binary optimizer about 3.7%, even in a highly non-convex optimization problem.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2108526145",
                        "name": "Junjie Liu"
                    },
                    {
                        "authorId": "115712451",
                        "name": "Dongchao Wen"
                    },
                    {
                        "authorId": "1844767631",
                        "name": "Deyu Wang"
                    },
                    {
                        "authorId": "2112528277",
                        "name": "Wei Tao"
                    },
                    {
                        "authorId": "2145223699",
                        "name": "Tse-Wei Chen"
                    },
                    {
                        "authorId": "47373253",
                        "name": "Kinya Osa"
                    },
                    {
                        "authorId": "1845879900",
                        "name": "Masami Kato"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "Intuitively, low resolution arithmetic might not match the setting of gradient-based training because the gradient only gives reliable information in a small neighborhood around the current model parameters\u2019circumstantial evidence for this is the significant amount of work on the improvement of training methods in the context of low-resolution weights (e.g., M\u00fcller et al., 2017; Alizadeh et al., 2019; Helwegen et al., 2019)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b30e6bb39e44efcc2ec778f354bc222330d44e6b",
                "externalIds": {
                    "PubMedCentral": "7270357",
                    "MAG": "3032685633",
                    "DOI": "10.3389/fnins.2020.00437",
                    "CorpusId": 218905230,
                    "PubMed": "32547357"
                },
                "corpusId": 218905230,
                "publicationVenue": {
                    "id": "2ca4279c-8ed7-4280-8022-09e577923a09",
                    "name": "Frontiers in Neuroscience",
                    "type": "journal",
                    "alternate_names": [
                        "Front Neurosci"
                    ],
                    "issn": "1662-453X",
                    "url": "https://www.frontiersin.org/journals/neuroscience",
                    "alternate_urls": [
                        "http://journal.frontiersin.org/journal/neuroscience",
                        "http://www.frontiersin.org/neuroscience/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b30e6bb39e44efcc2ec778f354bc222330d44e6b",
                "title": "Neuromorphic Systems Design by Matching Inductive Biases to Hardware Constraints",
                "abstract": "Neuromorphic systems are designed with careful consideration of the physical properties of the computational substrate they use. Neuromorphic engineers often exploit physical phenomena to directly implement a desired functionality, enabled by \u201cthe isomorphism between physical processes in different media\u201d (Douglas et al., 1995). This bottom-up design methodology could be described as matching computational primitives to physical phenomena. In this paper, we propose a top-down counterpart to the bottom-up approach to neuromorphic design. Our top-down approach, termed \u201cbias matching,\u201d is to match the inductive biases required in a learning system to the hardware constraints of its implementation; a well-known example is enforcing translation equivariance in a neural network by tying weights (replacing vector-matrix multiplications with convolutions), which reduces memory requirements. We give numerous examples from the literature and explain how they can be understood from this perspective. Furthermore, we propose novel network designs based on this approach in the context of collaborative filtering. Our simulation results underline our central conclusions: additional hardware constraints can improve the predictions of a Machine Learning system, and understanding the inductive biases that underlie these performance gains can be useful in finding applications for a given constraint.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3444613",
                        "name": "Lorenz K. Muller"
                    },
                    {
                        "authorId": "152179072",
                        "name": "P. Stark"
                    },
                    {
                        "authorId": "31091331",
                        "name": "B. Offrein"
                    },
                    {
                        "authorId": "50514972",
                        "name": "S. Abel"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Binary Neural Networks (BNNs) recently have attracted many interests and achieved significant improvements [39, 15, 32, 3, 45].",
                "Our proposed method using evolutionary search based on recent ideas of one-shot neural architecture search aims at exploring the group architecture design for BNNs improvement.",
                "In this section, to evaluate the proposed method we compare our BNNs with several recent works: Binary Connect [8], BNNs [19], ABC-Net [26], DoReFa-Net [53], XNORNet [39], etc.",
                "First, we describe experi-\nAlgorithm 2 Overall Training BNNs\nInput: Full binary neural model and inputs for evolutionary search Output: New optimal binary neural model with new group structure.",
                "Then we compare with the state-of-the-arts to see improvement impacts of our proposed BNNs. Finally, the computation analysis are presented.",
                "The main objective in our work is to explore efficient designs of BNNs with the hope that techniques in neural architecture search (NAS) can leverage the exploration for compact structures.",
                "These binary models described a deployment of compact modules with skip connection and group convolution to enhance the capability of BNNs in terms of feature representation.",
                "A recent work on BNNs [15] introduced Binary Optimizer to remove the dependency of binary weights from the real values, opening a new way to improve the BNNs.",
                "Efficient group design for BNNs can yield good outcomes.",
                "Our work sheds the light on a new direction for enhancing the capability of BNNs.\n\u2022 We propose an adaptive weight-sharing training mechanism that automatically searches in the group space\nto build efficient BNNs.",
                "Binary Neural Networks (BNNs), known to be one among the effectively compact network architectures, have achieved great outcomes in the visual tasks."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c426835798773feecbdd35bf0d3e9affeecd6d91",
                "externalIds": {
                    "MAG": "3034681682",
                    "DBLP": "conf/cvpr/PhanLHSCS20",
                    "ArXiv": "2005.06305",
                    "DOI": "10.1109/cvpr42600.2020.01343",
                    "CorpusId": 218614013
                },
                "corpusId": 218614013,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c426835798773feecbdd35bf0d3e9affeecd6d91",
                "title": "Binarizing MobileNet via Evolution-Based Searching",
                "abstract": "Binary Neural Networks (BNNs), known to be one among the effectively compact network architectures, have achieved great outcomes in the visual tasks. Designing efficient binary architectures is not trivial due to the binary nature of the network. In this paper, we propose a use of evolutionary search to facilitate the construction and training scheme when binarizing MobileNet, a compact network with separable depth-wise convolution. Being inspired by one-shot architecture search frameworks, we manipulate the idea of group convolution to design efficient 1-Bit Convolutional Neural Networks (CNNs), assuming an approximately optimal trade-off between computational cost and model accuracy. Our objective is to come up with a tiny yet efficient binary neural architecture by exploring the best candidates of the group convolution while optimizing the model performance in terms of complexity and latency. The approach is threefold. First, we modify and train strong baseline binary networks with a wide range of random group combinations at each convolutional layer. This set-up gives the binary neural networks a capability of preserving essential information through layers. Second, to find a good set of hyper-parameters for group convolutions we make use of the evolutionary search which leverages the exploration of efficient 1-bit models. Lastly, these binary models are trained from scratch in a usual manner to achieve the final binary model. Various experiments on ImageNet are conducted to show that following our construction guideline, the final model achieves 60.09% Top-1 accuracy and outperforms the state-of-the-art CI-BCNN with the same computational cost.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2064914939",
                        "name": "Hai T. Phan"
                    },
                    {
                        "authorId": "2109370860",
                        "name": "Zechun Liu"
                    },
                    {
                        "authorId": "51449392",
                        "name": "Dang T. Huynh"
                    },
                    {
                        "authorId": "1794486",
                        "name": "M. Savvides"
                    },
                    {
                        "authorId": "145210800",
                        "name": "Kwang-Ting Cheng"
                    },
                    {
                        "authorId": "145314568",
                        "name": "Zhiqiang Shen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "368047516fb729653a1bfafaa042e54cf00f8834",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-07592",
                    "ArXiv": "2003.03533",
                    "MAG": "3009272685",
                    "PubMedCentral": "8100137",
                    "DOI": "10.1038/s41467-021-22768-y",
                    "CorpusId": 212633550,
                    "PubMed": "33953183"
                },
                "corpusId": 212633550,
                "publicationVenue": {
                    "id": "43b3f0f9-489a-4566-8164-02fafde3cd98",
                    "name": "Nature Communications",
                    "type": "journal",
                    "alternate_names": [
                        "Nat Commun"
                    ],
                    "issn": "2041-1723",
                    "url": "https://www.nature.com/ncomms/",
                    "alternate_urls": [
                        "http://www.nature.com/ncomms/about/index.html",
                        "http://www.nature.com/ncomms/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/368047516fb729653a1bfafaa042e54cf00f8834",
                "title": "Synaptic metaplasticity in binarized neural networks",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1557381641",
                        "name": "Axel Laborieux"
                    },
                    {
                        "authorId": "5904564",
                        "name": "M. Ernoult"
                    },
                    {
                        "authorId": "22328712",
                        "name": "T. Hirtzlin"
                    },
                    {
                        "authorId": "1793153",
                        "name": "D. Querlioz"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In addition to architectural design [2,23,28], studies on 1-bit CNNs expand from training algorithms [36,1,46,3], binary optimizer design [13], regularization loss design [8,29], to better approximation of binary weights and activations [30,11,37]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1fab639219c601ddaf73430249035c827fbc1d7f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2003-03488",
                    "MAG": "3104151879",
                    "ArXiv": "2003.03488",
                    "DOI": "10.1007/978-3-030-58568-6_9",
                    "CorpusId": 212633658
                },
                "corpusId": 212633658,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/1fab639219c601ddaf73430249035c827fbc1d7f",
                "title": "ReActNet: Towards Precise Binary Neural Network with Generalized Activation Functions",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2109370860",
                        "name": "Zechun Liu"
                    },
                    {
                        "authorId": "145314568",
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "authorId": "1794486",
                        "name": "M. Savvides"
                    },
                    {
                        "authorId": "145210800",
                        "name": "Kwang-Ting Cheng"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "] 66.3 86.6 (1/1)\u00d74 CBCN [26] 61.4 82.8 (1/1)\u00d74 Ensamble [50] 61.0 - (1/1)\u00d76 BNN [10] 42.2 69.2 1/1 XNOR-Net [33] 51.2 73.2 1/1 CCNN [43] 54.2 77.9 1/1 Bi-Real Net** [28] 56.4 79.5 1/1 Rethink. BNN** [17] 56.6 79.4 1/1 XNOR-Net++ [5] 57.1 79.9 1/1 CI-Net** [40] 59.9 84.2 1/1 BATS(Ours) 60.4 83.0 1/1 BATS[2x-wider] (Ours) 66.1 87.0 1/1 7 Conclusion In this work we introduce a novel Binary Architecture "
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "51d61c2323a1435994572b14d7be5def51ad8af1",
                "externalIds": {
                    "DBLP": "conf/eccv/BulatMT20",
                    "MAG": "3010491606",
                    "ArXiv": "2003.01711",
                    "DOI": "10.1007/978-3-030-58592-1_19",
                    "CorpusId": 211818095
                },
                "corpusId": 211818095,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/51d61c2323a1435994572b14d7be5def51ad8af1",
                "title": "BATS: Binary ArchitecTure Search",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145245424",
                        "name": "Adrian Bulat"
                    },
                    {
                        "authorId": "145944235",
                        "name": "Brais Mart\u00ednez"
                    },
                    {
                        "authorId": "2610880",
                        "name": "Georgios Tzimiropoulos"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                ", 2019); latent-free Bop (Helwegen et al., 2019); and the proximal mean-field (PMF) (Ajanthan et al."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2443b743669f87f9f7a9c8e25018b05205d4eebc",
                "externalIds": {
                    "DBLP": "conf/icml/MengBK20",
                    "MAG": "3034950653",
                    "ArXiv": "2002.10778",
                    "CorpusId": 211296559
                },
                "corpusId": 211296559,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2443b743669f87f9f7a9c8e25018b05205d4eebc",
                "title": "Training Binary Neural Networks using the Bayesian Learning Rule",
                "abstract": "Neural networks with binary weights are computation-efficient and hardware-friendly, but their training is challenging because it involves a discrete optimization problem. Surprisingly, ignoring the discrete nature of the problem and using gradient-based methods, such as the Straight-Through Estimator, still works well in practice. This raises the question: are there principled approaches which justify such methods? In this paper, we propose such an approach using the Bayesian learning rule. The rule, when applied to estimate a Bernoulli distribution over the binary weights, results in an algorithm which justifies some of the algorithmic choices made by the previous approaches. The algorithm not only obtains state-of-the-art performance, but also enables uncertainty estimation for continual learning to avoid catastrophic forgetting. Our work provides a principled approach for training binary neural networks which justifies and extends existing approaches.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50123271",
                        "name": "Xiangming Meng"
                    },
                    {
                        "authorId": "153825349",
                        "name": "Roman Bachmann"
                    },
                    {
                        "authorId": "145901278",
                        "name": "M. E. Khan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                ", 2016; Zhu, Dong, & Su, 2019; Zhuang, Shen, Tan, Liu, & Reid, 2019) and optimization strategies (Alizadeh, Fern\u00e1ndezMarqu\u00e9s, Lane, & Gal, 2019; Helwegen et al., 2019), and the accuracy gap between efficient BNNs and regular DNNs is rapidly closing.",
                "Binarized Neural Networks (BNNs) represent an extreme case of quantized networks, that cannot be viewed as approximations to real-valued networks and therefore requires special tools and optimization strategies (Helwegen et al., 2019).",
                "\u2026architectures (Liu et al., 2018; Rastegari et al., 2016; Zhu, Dong, & Su, 2019; Zhuang, Shen, Tan, Liu, & Reid, 2019) and optimization strategies (Alizadeh, Fern\u00e1ndezMarqu\u00e9s, Lane, & Gal, 2019; Helwegen et al., 2019), and the accuracy gap between efficient BNNs and regular DNNs is rapidly closing."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "4118299efb2a5c32b7c1e1c32a0c4588414ccbbb",
                "externalIds": {
                    "DBLP": "journals/jossw/GeigerT20",
                    "MAG": "2999093589",
                    "DOI": "10.21105/joss.01746",
                    "CorpusId": 214102155
                },
                "corpusId": 214102155,
                "publicationVenue": {
                    "id": "1236e136-01b7-42d5-8c4a-593153a3ab37",
                    "name": "Journal of Open Source Software",
                    "type": "journal",
                    "alternate_names": [
                        "The Journal of Open Source Software",
                        "J Open Source Softw"
                    ],
                    "issn": "2475-9066",
                    "url": "https://joss.theoj.org/",
                    "alternate_urls": [
                        "https://joss.theoj.org/about"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4118299efb2a5c32b7c1e1c32a0c4588414ccbbb",
                "title": "Larq: An Open-Source Library for Training Binarized Neural Networks",
                "abstract": "Modern deep learning methods have been successfully applied to many different tasks and have the potential to revolutionize everyday lives. However, existing neural networks that use 32 bits to encode each weight and activation often have an energy budget that far exceeds the capabilities of mobile or embedded devices. One common way to improve computational efficiency is to reduce the precision of the network to 16-bit or 8-bit, also known as quantization. Binarized Neural Networks (BNNs) represent an extreme case of quantized networks, that cannot be viewed as approximations to real-valued networks and therefore requires special tools and optimization strategies (Helwegen et al., 2019). In these networks both weights and activations are restricted to {\u22121,+1} (Hubara, Courbariaux, Soudry, El-Yaniv, & Bengio, 2016). Compared to an equivalent 8-bit quantized network BNNs require 8 times smaller memory size and 8 times fewer memory accesses, which reduces energy consumption drastically when deployed on optimized hardware (Hubara et al., 2016). However, many open research questions remain until the use of BNNs and other extremely quantized neural networks becomes widespread in industry. larq is an ecosystem of Python packages for BNNs and other Quantized Neural Networks (QNNs). It is intended to facilitate researchers to resolve these outstanding questions.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "102673339",
                        "name": "Lukas Geiger"
                    },
                    {
                        "authorId": "1576731159",
                        "name": "Plumerai Team"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "For example, using learned clipping in PACT [9], double skip-connection in BiRealNet [37], parametric ReLU in [6], multi-stage knowledge distillation in [2, 6], and tailored binary optimization in [24]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b46d7d1b379155568664d64ef59baf0aa1227996",
                "externalIds": {
                    "ArXiv": "2001.02786",
                    "DBLP": "conf/cvpr/PouransariTT20",
                    "MAG": "2998895371",
                    "DOI": "10.1109/CVPRW50498.2020.00357",
                    "CorpusId": 210116550
                },
                "corpusId": 210116550,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b46d7d1b379155568664d64ef59baf0aa1227996",
                "title": "Least squares binary quantization of neural networks",
                "abstract": "Quantizing weights and activations of deep neural networks results in significant improvement in inference efficiency at the cost of lower accuracy. A source of the accuracy gap between full precision and quantized models is the quantization error. In this work, we focus on the binary quantization, in which values are mapped to -1 and 1. We provide a unified framework to analyze different scaling strategies. Inspired by the pareto-optimality of 2-bits versus 1-bit quantization, we introduce a novel 2-bits quantization with provably least squares error. Our quantization algorithms can be implemented efficiently on the hardware using bitwise operations. We present proofs to show that our proposed methods are optimal, and also provide empirical error analysis. We conduct experiments on the ImageNet dataset and show a reduced accuracy gap when using the proposed least squares quantization algorithms.1",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1842915",
                        "name": "H. Pouransari"
                    },
                    {
                        "authorId": "2577513",
                        "name": "Oncel Tuzel"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "In this report, we present a detailed study on the paper titled \"Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization\" by Helwegen et al. [2019] which proposes a new optimization method for training BNN called BOP.",
                "Authors of this paper (Helwegen et al. [2019]) claimed that using latent weights in BNN may not always result into higher accuracy.",
                "To use the dataset for training a binary network, in our work similar modifications are done to the dataset as Helwegen et al. [2019]. We use batch normalization to normalize the activations with a minibatch size of 64.",
                "The paper Helwegen et al. [2019] considers a binary convolutional architecture called BVGG net as"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "36a7a224be27a7eb4e1d89e50bc7abf1ccc89fdc",
                "externalIds": {
                    "MAG": "3042194324",
                    "CorpusId": 226784335
                },
                "corpusId": 226784335,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/36a7a224be27a7eb4e1d89e50bc7abf1ccc89fdc",
                "title": "A comprehensive study on binary optimizer and its applicability",
                "abstract": "Binarized Neural Networks are paving a way towards the deployment of deep neural networks with less memory and computation. In this report, we present a detailed study on the paper titled \"Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization\" by Helwegen et al. [2019] which proposes a new optimization method for training BNN called BOP. We \ufb01rst investigate the effect of using latent weights in BNN for analyzing prediction performance in terms of accuracy. Next, a comprehensive ablation study on hyperparameters is provided. Finally, we explore the usability of BNN in denoising autoencoders. Code for all our experiments are available at https: //github.com/nancy-nayak/rethinking-bnn/",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "151152600",
                        "name": "Nancy Nayak"
                    },
                    {
                        "authorId": "46637452",
                        "name": "Vishnu Raj"
                    },
                    {
                        "authorId": "143968719",
                        "name": "S. Kalyani"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                " been implemented in neuromorphic devices like the DynapSEL [44]. Although it may not suce to quantize previously trained high-precision weights, there are learning algorithms such as [45], [46] and [47] that are designed to nd suitable low-precision weights. Another potential solution is the application of transfer methods as suggested in [34]. Assuming similar power gures as for DYNAP-SE, we estima"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b0c3d2966fb9e135b94b929d8f4159ec74a2fb7f",
                "externalIds": {
                    "MAG": "2983115849",
                    "ArXiv": "1911.05521",
                    "DBLP": "journals/corr/abs-1911-05521",
                    "DOI": "10.1109/TBCAS.2019.2953001",
                    "CorpusId": 207930584,
                    "PubMed": "31715572"
                },
                "corpusId": 207930584,
                "publicationVenue": {
                    "id": "b705b89f-7498-41dc-960d-af625d263847",
                    "name": "IEEE Transactions on Biomedical Circuits and Systems",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Biomed Circuit Syst"
                    ],
                    "issn": "1932-4545",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=4156126"
                },
                "url": "https://www.semanticscholar.org/paper/b0c3d2966fb9e135b94b929d8f4159ec74a2fb7f",
                "title": "Real-Time Ultra-Low Power ECG Anomaly Detection Using an Event-Driven Neuromorphic Processor",
                "abstract": "Accurate detection of pathological conditions in human subjects can be achieved through off-line analysis of recorded biological signals such as electrocardiograms (ECGs). However, human diagnosis is time-consuming and expensive, as it requires the time of medical professionals. This is especially inefficient when indicative patterns in the biological signals are infrequent. Moreover, patients with suspected pathologies are often monitored for extended periods, requiring the storage and examination of large amounts of non-pathological data, and entailing a difficult visual search task for diagnosing professionals. In this work we propose a compact and sub-mW low power neural processing system that can be used to perform on-line and real-time preliminary diagnosis of pathological conditions, to raise warnings for the existence of possible pathological conditions, or to trigger an off-line data recording system for further analysis by a medical professional. We apply the system to real-time classification of ECG data for distinguishing between healthy heartbeats and pathological rhythms. Multi-channel analog ECG traces are encoded as asynchronous streams of binary events and processed using a spiking recurrent neural network operated in a reservoir computing paradigm. An event-driven neuron output layer is then trained to recognize one of several pathologies. Finally, the filtered activity of this output layer is used to generate a binary trigger signal indicating the presence or absence of a pathological pattern. We validate the approach proposed using a Dynamic Neuromorphic Asynchronous Processor (DYNAP) chip, implemented using a standard 180\u00a0nm CMOS VLSI process, and present experimental results measured from the chip.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "144664296",
                        "name": "F. Bauer"
                    },
                    {
                        "authorId": "1917808",
                        "name": "D. Muir"
                    },
                    {
                        "authorId": "1721210",
                        "name": "G. Indiveri"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In contrast to these, recently Helwegen et al. (2019) proposed Binary Optimizer (bop) to avoid using \u201clatent\u201d real-valued weights during training."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "272427c548378d28723a45a255a2d3cb1c4a0c95",
                "externalIds": {
                    "MAG": "2980767156",
                    "DBLP": "conf/aistats/AjanthanGTHD21",
                    "ArXiv": "1910.08237",
                    "CorpusId": 204788904
                },
                "corpusId": 204788904,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/272427c548378d28723a45a255a2d3cb1c4a0c95",
                "title": "Mirror Descent View for Neural Network Quantization",
                "abstract": "Quantizing large Neural Networks (NN) while maintaining the performance is highly desirable for resource-limited devices due to reduced memory and time complexity. It is usually formulated as a constrained optimization problem and optimized via a modified version of gradient descent. In this work, by interpreting the continuous parameters (unconstrained) as the dual of the quantized ones, we introduce a Mirror Descent (MD) framework for NN quantization. Specifically, we provide conditions on the projections (i.e., mapping from continuous to quantized ones) which would enable us to derive valid mirror maps and in turn the respective MD updates. Furthermore, we present a numerically stable implementation of MD that requires storing an additional set of auxiliary variables (unconstrained), and show that it is strikingly analogous to the Straight Through Estimator (STE) based method which is typically viewed as a \"trick\" to avoid vanishing gradients issue. Our experiments on CIFAR-10/100, TinyImageNet, and ImageNet classification datasets with VGG-16, ResNet-18, and MobileNetV2 architectures show that our MD variants obtain quantized networks with state-of-the-art performance.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "144722114",
                        "name": "Thalaiyasingam Ajanthan"
                    },
                    {
                        "authorId": "1491124544",
                        "name": "Kartik Gupta"
                    },
                    {
                        "authorId": "143635540",
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "authorId": "143750012",
                        "name": "R. Hartley"
                    },
                    {
                        "authorId": "144679302",
                        "name": "P. Dokania"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In the future, we will employ the latent-free optimizer Helwegen et al. (2019) for BNNs that directly update the binary weights, to reduce the memory consumption during training."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "59e692a3a6eacdafd2166570374d0d796f1ff15a",
                "externalIds": {
                    "ArXiv": "1909.09934",
                    "DBLP": "journals/corr/abs-1909-09934",
                    "MAG": "2974110043",
                    "DOI": "10.1007/s11263-022-01638-0",
                    "CorpusId": 202719575
                },
                "corpusId": 202719575,
                "publicationVenue": {
                    "id": "939ee07c-6009-43f8-b884-69238b40659e",
                    "name": "International Journal of Computer Vision",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Comput Vis"
                    ],
                    "issn": "0920-5691",
                    "url": "https://www.springer.com/computer/image+processing/journal/11263",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11263",
                        "http://link.springer.com/journal/11263"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/59e692a3a6eacdafd2166570374d0d796f1ff15a",
                "title": "Structured Binary Neural Networks for Image Recognition",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "3194022",
                        "name": "Bohan Zhuang"
                    },
                    {
                        "authorId": "12459603",
                        "name": "Chunhua Shen"
                    },
                    {
                        "authorId": "2823637",
                        "name": "Mingkui Tan"
                    },
                    {
                        "authorId": "2161037",
                        "name": "Lingqiao Liu"
                    },
                    {
                        "authorId": "145950884",
                        "name": "I. Reid"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "839fcb7c0b890fd46c36f48501b74a4ae4ea0ba7",
                "externalIds": {
                    "DBLP": "journals/access/GarciaAriasOHMY23",
                    "DOI": "10.1109/ACCESS.2023.3245808",
                    "CorpusId": 256971121
                },
                "corpusId": 256971121,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/839fcb7c0b890fd46c36f48501b74a4ae4ea0ba7",
                "title": "Recurrent Residual Networks Contain Stronger Lottery Tickets",
                "abstract": "Accurate neural networks can be found just by pruning a randomly initialized overparameterized model, leaving out the need for any weight optimization. The resulting subnetworks are small, sparse, and ternary, making excellent candidates for efficient hardware implementation. However, finding optimal connectivity patterns is an open challenge. Based on the evidence that residual networks may be approximating unrolled shallow recurrent neural networks, we conjecture that they contain better candidate subnetworks at inference time when explicitly transformed into recurrent architectures. This hypothesis is put to the test on image classification tasks, where we find subnetworks within the recurrent models that are more accurate and parameter-efficient than both the ones found within feedforward models and than the full models with learned weights. Furthermore, random recurrent subnetworks are tiny: under a simple compression scheme, ResNet-50 is compressed without a drastic loss in performance to $48.55\\times $ less memory size, fitting in under 2 megabytes. Code available at: https://github.com/Lopez-Angel/hidden-fold-networks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1409951890",
                        "name": "\u00c1ngel L\u00f3pez Garc\u00eda-Arias"
                    },
                    {
                        "authorId": "2159237466",
                        "name": "Yasuyuki Okoshi"
                    },
                    {
                        "authorId": "2067777534",
                        "name": "Masanori Hashimoto"
                    },
                    {
                        "authorId": "2159237569",
                        "name": "Masato Motomura"
                    },
                    {
                        "authorId": "2154957755",
                        "name": "Jaehoon Yu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Specifically, these networks have significantly lower memory footprint, less computational complexity, and consume less energy (Helwegen et al., 2019)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "731364f4f48d47418302122935dbb6b6654b128c",
                "externalIds": {
                    "CorpusId": 260367424
                },
                "corpusId": 260367424,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/731364f4f48d47418302122935dbb6b6654b128c",
                "title": "WORKS WITH THRESHOLD ACTIVATION FUNCTIONS",
                "abstract": "Threshold activation functions are highly preferable in neural networks due to their efficiency in hardware implementations. Moreover, their mode of operation is more interpretable and resembles that of biological neurons. However, traditional gradient based algorithms such as Gradient Descent cannot be used to train the parameters of neural networks with threshold activations since the activation function has zero gradient except at a single non-differentiable point. To this end, we study weight decay regularized training problems of deep neural networks with threshold activations. We first show that regularized deep threshold network training problems can be equivalently formulated as a standard convex optimization problem, which parallels the LASSO method, provided that the last hidden layer width exceeds a certain threshold. We also derive a simplified convex optimization formulation when the dataset can be shattered at a certain layer of the network. We corroborate our theoretical results with various numerical experiments.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "19278348",
                        "name": "Tolga Ergen"
                    },
                    {
                        "authorId": "2050160081",
                        "name": "Halil Ibrahim Gulluk"
                    },
                    {
                        "authorId": "37669328",
                        "name": "Jonathan Lacotte"
                    },
                    {
                        "authorId": "3173667",
                        "name": "Mert Pilanci"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Moreover, for its second stage of unmasking, we optimize weight signs without extra latent weights or scores (Helwegen et al., 2019).",
                "To approximately solve (4) with sign flipping transformation Us, we leverage a prior art called Bop from BNN optimization (Helwegen et al., 2019).",
                "For example, Bop (Helwegen et al., 2019) keeps a running average of historical gradients, and flips a weight\u2019s sign when its accumulated gradients surpasses a pre-defined threshold.",
                "\u2026weights using the flipping with latent weights method in (Ivan & Florian, 2020) 1; (2) PaB-Latent-PSG, the variant of PaB-Latent combined with PSG; (3) PaB-Bop, which trains the unpruned weights using Bop (Helwegen et al., 2019); and (4) PaB-BopPSG, the variant of PaB-Bop combined with PSG.",
                "For the proposed PaB methods, we compare: (1) PaB-Latent, which first prunes the network and trains the unpruned weights using the flipping with latent weights method in (Ivan & Florian, 2020) 1; (2) PaB-Latent-PSG, the variant of PaB-Latent combined with PSG; (3) PaB-Bop, which trains the unpruned weights using Bop (Helwegen et al., 2019); and (4) PaB-BopPSG, the variant of PaB-Bop combined with PSG.",
                "Bop (Helwegen et al., 2019) selects what weights and when to flip their signs by taking into account past gradient information."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "aa30ab243b2d6c2b8b4396033d0e032732adaf81",
                "externalIds": {
                    "CorpusId": 251732832
                },
                "corpusId": 251732832,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/aa30ab243b2d6c2b8b4396033d0e032732adaf81",
                "title": "P EEK - A -B OO : W HAT (M ORE ) IS D ISGUISED IN A R ANDOMLY W EIGHTED N EURAL N ETWORK , AND H OW TO F IND I T E FFICIENTLY",
                "abstract": "Sparse neural networks (NNs) are intensively investigated in literature due to their appeal in saving storage, memory, and computational costs. A recent showed that, different from conventional pruning-and-\ufb01netuning pipeline, there exist hidden subnetworks in randomly initialized NNs that have good performance without training the weights. However, such \u201chidden subnetworks\u201d have mediocre performances and require an expensive edge-popup algorithm to search for them. In this work, we de\ufb01ne an extended class of subnetworks in randomly initialized NNs called disguised subnetworks , which are not only \u201chidden\u201d in the random networks but also \u201cdisguised\u201d \u2013 hence can only be \u201cunmasked\u201d with certain transformations on weights. We argue that the unmasking process plays an important role in enlarging the capacity of the subnetworks and thus grants two major bene\ufb01ts: (i) the disguised subnetworks easily outperform the hidden counterparts; (ii) the unmasking process helps to relax the quality requirement on the sparse subnetwork mask so that the expensive edge-popup algorithm can be replaced with more ef\ufb01cient alternatives. On top of this new concept, we propose a novel two-stage algorithm that plays a P eek- a - B oo ( PaB ) game to identify the disguised subnetworks with a combination of two operations: (1) searching ef\ufb01ciently for a subnetwork at random initialization ; (2) unmasking the disguise by learning to trans-form the resulting subnetwork\u2019s remaining weights. Furthermore, we show that the unmasking",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2116298570",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "49050667",
                        "name": "Jason Zhang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "11e48e789518b8c0bd26f1d363267f6c58c8b80a",
                "externalIds": {
                    "CorpusId": 259855703
                },
                "corpusId": 259855703,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/11e48e789518b8c0bd26f1d363267f6c58c8b80a",
                "title": "Bayesian Learning for Binary Neural Networks",
                "abstract": "The project is to study Bayesian learning methods for binary neural networks. Bayesian learning paradigm averages over all models that explain the data well. Binary neural networks are computationally efficient models that can be learned by optimizing their stochastic relaxation [1].The goal is to obtain binary neural networks with improved generalization capabilities and quantified uncertainty [2].There is a successful example of Bayesian binary networks [3]",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2223049102",
                        "name": "V. Y. A. Shekhovtsov"
                    },
                    {
                        "authorId": "1691955",
                        "name": "E. H\u00fcllermeier"
                    },
                    {
                        "authorId": "3249834",
                        "name": "W. Waegeman"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bb2b5aae4455fb5f9358aa6412f4c4a7581423a2",
                "externalIds": {
                    "DBLP": "conf/ijcai/ZhaoY0GD21",
                    "DOI": "10.24963/ijcai.2021/474",
                    "CorpusId": 237101068
                },
                "corpusId": 237101068,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/bb2b5aae4455fb5f9358aa6412f4c4a7581423a2",
                "title": "Uncertainty-aware Binary Neural Networks",
                "abstract": "Binary Neural Networks (BNN) are promising machine learning solutions for deployment on resource-limited devices. Recent approaches to training BNNs have produced impressive results, but minimizing the drop in accuracy from full precision networks is still challenging. One reason is that conventional BNNs ignore the uncertainty caused by weights that are near zero, resulting in the instability or frequent flip while learning. In this work, we investigate the intrinsic uncertainty of vanishing near-zero weights, making the training vulnerable to instability. We introduce an uncertainty-aware BNN (UaBNN) by leveraging a new mapping function called certainty-sign (c-sign) to reduce these weights\u2019 uncertainties. Our c-sign function is the first to train BNNs with a decreasing uncertainty for binarization. The approach leads to a controlled learning process for BNNs. We also introduce a simple but effective method to measure the uncertainty-based on a Gaussian function. Extensive experiments demonstrate that our method improves multiple BNN methods by maintaining stability of training, and achieves a higher performance over prior arts.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "120246235",
                        "name": "Junhe Zhao"
                    },
                    {
                        "authorId": "120378243",
                        "name": "Linlin Yang"
                    },
                    {
                        "authorId": "1740430",
                        "name": "Baochang Zhang"
                    },
                    {
                        "authorId": "2067614206",
                        "name": "Guodong Guo"
                    },
                    {
                        "authorId": "48471936",
                        "name": "D. Doermann"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "94ce9170e4e7de175bab451f4baa85bac4408c35",
                "externalIds": {
                    "DBLP": "conf/bmvc/Nie0021",
                    "CorpusId": 249893168
                },
                "corpusId": 249893168,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/94ce9170e4e7de175bab451f4baa85bac4408c35",
                "title": "Multi-bit Adaptive Distillation for Binary Neural Networks",
                "abstract": "Binary neural networks (BNNs) represent weights and activations using 1-bit values, which has extremely lower memory costs and computational complexities, but usually suffer from severe accuracy degradation. Knowledge distillation is an effective way to improve the performance of BNN by inheriting the knowledge from higher-bit network. However, faced with the accuracy gap and bit gap between 1-bit network and different higher-bit networks, it is uncertain which higher-bit network is more suitable to be the teacher of a certain BNN. Therefore, we propose a novel multi-bit adaptive distilla-tion(MAD) method for maximally integrating the advantages of various bit-width teacher networks( e.g . 2-bit, 4-bit, 8-bit and 32-bit). In practice, intermediate features and output logits of teachers will be simultaneously utilized for improving the performance of BNN. Moreover, an adaptive knowledge adjusting scheme is explored to dynamically adjust the contribution of different teachers in the distillation process. Comprehensive experiments conducted on CIFAR-10/100 and ImageNet datasets with various network architectures demonstrate the superiorities of MAD over many state-of-the-arts binarization methods. For instance, without introducing any extra inference calculations, our binarized ResNet-18 achieves 1.5% improvement for BirealNet binarization method on ImageNet.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2055490179",
                        "name": "Ying Nie"
                    },
                    {
                        "authorId": "3826388",
                        "name": "Kai Han"
                    },
                    {
                        "authorId": "2108702980",
                        "name": "Yunhe Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "In this paper ([1]), the authors have developed a novel optimization method, Binary Optimizer (BOP), for training BNN.",
                "In this report, we present a detailed study on the paper titled \"Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization\" by [1] which proposes a new optimization method for training BNN called BOP.",
                "To use the dataset for training a binary network, in our work similar modifications are done to the dataset as [1].",
                "The paper [1] considers a binary convolutional architecture called BVGG net as given in [8] inspired from the architecture of ConvNet in [3] we discussed before.",
                "Authors of this paper ([1]) claimed that using latent weights in BNN may not always result into higher accuracy."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "454d698fe369b4bd54e6f947cac8e480c84959b0",
                "externalIds": {
                    "CorpusId": 250556914
                },
                "corpusId": 250556914,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/454d698fe369b4bd54e6f947cac8e480c84959b0",
                "title": "[Re] A comprehensive study on binary optimizer and its applicability",
                "abstract": "Binarized Neural Networks are paving a way towards the deployment of deep neural networks with less memory and computation. In this report, we present a detailed study on the paper titled \"Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization\" by [1] which proposes a new optimization method for training BNN called BOP. We \ufb01rst investigate the e\ufb00ect of using latent weights in BNN for analyzing prediction performance in terms of accuracy. Next, a comprehensive ablation study on hyperparameters is provided. Finally, we explore the usability of BNN in denoising autoencoders. Code for all our experiments are available at https://github.com/nancy-nayak/rethinking-bnn/",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "151152600",
                        "name": "Nancy Nayak"
                    },
                    {
                        "authorId": "46637452",
                        "name": "Vishnu Raj"
                    },
                    {
                        "authorId": "143968719",
                        "name": "S. Kalyani"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Since BNN training is not well-founded, there are still tremendous efforts on the study of BNNs\u2019 optimizations [1,5,17,29] and how to explain the effectiveness of BNNs [2]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8381e7ec8ca3b490f6514fbbf74fc3f15afbf28c",
                "externalIds": {
                    "DBLP": "conf/eccv/HeMCXHW0020",
                    "MAG": "3106579642",
                    "DOI": "10.1007/978-3-030-58580-8_14",
                    "CorpusId": 227257737
                },
                "corpusId": 227257737,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/8381e7ec8ca3b490f6514fbbf74fc3f15afbf28c",
                "title": "ProxyBNN: Learning Binarized Neural Networks via Proxy Matrices",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48535072",
                        "name": "Xiangyu He"
                    },
                    {
                        "authorId": "148432596",
                        "name": "Zitao Mo"
                    },
                    {
                        "authorId": "1998966851",
                        "name": "Ke Cheng"
                    },
                    {
                        "authorId": "7978284",
                        "name": "Weixiang Xu"
                    },
                    {
                        "authorId": "2571792",
                        "name": "Qinghao Hu"
                    },
                    {
                        "authorId": "1656803942",
                        "name": "Peisong Wang"
                    },
                    {
                        "authorId": "2145921810",
                        "name": "Qingshan Liu"
                    },
                    {
                        "authorId": "143949499",
                        "name": "Jian Cheng"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026of multiple binary models (Zhu et al., 2019), searching high-performance binary network architectures (Kim et al., 2020), and designing improved regularization objectives, optimizers and activation functions (Cai et al., 2017; Liu et al., 2018; Helwegen et al., 2019; Martinez et al., 2020).",
                ", 2020), and designing improved regularization objectives, optimizers and activation functions (Cai et al., 2017; Liu et al., 2018; Helwegen et al., 2019; Martinez et al., 2020)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4bb364227f67bca5ad12192fc4a2aa08806902f4",
                "externalIds": {
                    "CorpusId": 236910230
                },
                "corpusId": 236910230,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4bb364227f67bca5ad12192fc4a2aa08806902f4",
                "title": "WEIGHTS HAVING STABLE SIGNS ARE IMPORTANT: FINDING PRIMARY SUBNETWORKS",
                "abstract": "Binary Weight Networks (BWNs) have significantly lower computational and memory costs compared to their full-precision counterparts. To address the nondifferentiable issue of BWNs, existing methods usually use the Straight-ThroughEstimator (STE). In the optimization, they learn optimal binary weight outputs represented as a combination of scaling factors and weight signs to approximate 32-bit floating-point weight values, usually with a layer-wise quantization scheme. In this paper, we begin with an empirical study of training BWNs with STE under the settings of using common techniques and tricks. We show that in the context of using batch normalization after convolutional layers, adapting scaling factors with either hand-crafted or learnable methods brings marginal or no accuracy gain to final model, while the change of weight signs is crucial in the training of BWNs. Furthermore, we observe two astonishing training phenomena. Firstly, the training of BWNs demonstrates the process of seeking primary binary sub-networks whose weight signs are determined and fixed at the early training stage, which is akin to recent findings on the lottery ticket hypothesis for efficient learning of sparse neural networks. Secondly, we find binary kernels in the convolutional layers of final models tend to be centered on a limited number of the most frequent binary kernels, showing binary weight networks may has the potential to be further compressed, which breaks the common wisdom that representing each weight with a single bit puts the quantization to the extreme compression. To testify this hypothesis, we additionally propose a binary kernel quantization method, and we call resulting models Quantized Binary-Kernel Networks (QBNs). We hope these new experimental observations would shed new design insights to improve the training and broaden the usages of BWNs.",
                "year": 2020,
                "authors": []
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Modified Bop: BOAT uses a modified version of the Bop optimizer (Helwegen et al., 2019) to optimize the binary-valued parameter {W ,S} given gradient as learning signals.",
                "\u2026Neural DNF. BOAT utilizes two optimizers: a standard deep learning optimizer Adam (Kingma and Ba, 2014) that optimizes the continuous parameters \u03b8 of the neural network \u03c6 and a binary-parameter optimizer adopted from [Helwegen et al., 2019] that optimizes the binary parameters {W ,S} of the DNF g.",
                "1): we use standard continuous-parameter optimizer Adam (Kingma and Ba, 2014) to optimize the first-stage neural network \u03c6 and modify a binary-parameter optimizer from Helwegen et al. (2019) to optimize the parameters of the second-stage rule-based g."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b067a25be38de374047b9802e5c4d1bcd95d04dc",
                "externalIds": {
                    "CorpusId": 236922379
                },
                "corpusId": 236922379,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b067a25be38de374047b9802e5c4d1bcd95d04dc",
                "title": "NEURAL DISJUNCTIVE NORMAL FORM: VERTICALLY INTEGRATING LOGIC WITH DEEP LEARNING FOR CLASSIFICATION",
                "abstract": "We present Neural Disjunctive Normal Form (Neural DNF), a hybrid neurosymbolic classifier that vertically integrates propositional logic with a deep neural network. Here, we aim at a vertical integration of logic and deep learning: we utilize the ability of deep neural networks as feature extractors to extract intermediate representation from data, and then a Disjunctive Normal Form (DNF) module to perform logical rule-based classification; we also seek this integration to be tight that these two normally-incompatible modules can be learned in an end-to-end manner, for which we propose the BOAT algorithm. Compared with standard deep classifiers which use a linear model or variants of additive model as the classification head, Neural DNF provides a new choice of model based on logic rules. It offers interpretability via an explicit symbolic representation, strong model expressity, and a different type of model inductive bias. Neural DNF is particularly suited for certain tasks that require some logical composition and provides extra interpretability.",
                "year": 2020,
                "authors": []
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": null,
                "externalIds": null,
                "corpusId": "260367328",
                "publicationVenue": null,
                "url": null,
                "title": "G LOBALLY O PTIMAL T RAINING OF N EURAL N ET - WORKS WITH T HRESHOLD A CTIVATION F UNCTIONS",
                "abstract": null,
                "year": null,
                "authors": []
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3ee1bd47f7c8df4d6a63a66608cbbe5c4971de36",
                "externalIds": {
                    "CorpusId": 260435596
                },
                "corpusId": 260435596,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3ee1bd47f7c8df4d6a63a66608cbbe5c4971de36",
                "title": "[Re:] Training Binary Neural Networks using the Bayesian Learning Rule",
                "abstract": "(1) gives a mathematically principled approach to solve the discrete optimization problem that occurs in the case of 2 Binary Neural Networks and claims to give a similar performance on various classification benchmarks such as MNIST, 3 CIFAR-10, and CIFAR-100 as compared to their full-precision counterparts, as well as other recent algorithms to 4 train BNNs like PMF and Bop. The paper also claims that the BayesBiNN method has an application in the continual 5 learning domain as it helps in overcoming catastrophic forgetting of the past by using the posterior approximation of 6 the previous task as a prior for the upcoming task. We try to reproduce all the results presented in the original paper by 7 making a separate and independent codebase. 8",
                "year": null,
                "authors": []
            }
        }
    ]
}