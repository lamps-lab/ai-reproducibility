{
    "offset": 0,
    "data": [
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "h-detach is a stochastic algorithm specified to optimize LSTM to improve on long-term memory tasks [32]."
            ],
            "citingPaper": {
                "paperId": "a3389a2e39b08bfc983dfdb62a408c9213fd53f0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-11462",
                    "ArXiv": "2305.11462",
                    "DOI": "10.48550/arXiv.2305.11462",
                    "CorpusId": 258823199
                },
                "corpusId": 258823199,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a3389a2e39b08bfc983dfdb62a408c9213fd53f0",
                "title": "Extending Memory for Language Modelling",
                "abstract": "Breakthroughs in deep learning and memory networks have made major advances in natural language understanding. Language is sequential and information carried through the sequence can be captured through memory networks. Learning the sequence is one of the key aspects in learning the language. However, memory networks are not capable of holding infinitely long sequences in their memories and are limited by various constraints such as the vanishing or exploding gradient problem. Therefore, natural language understanding models are affected when presented with long sequential text. We introduce Long Term Memory network (LTM) to learn from infinitely long sequences. LTM gives priority to the current inputs to allow it to have a high impact. Language modeling is an important factor in natural language understanding. LTM was tested in language modeling, which requires long term memory. LTM is tested on Penn Tree bank dataset, Google Billion Word dataset and WikiText-2 dataset. We compare LTM with other language models which require long term memory.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "9416341",
                        "name": "A. Nugaliyadde"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "The CLstm and CGru based models have limitations in generating rich representation for longterm time sequence, due to the vanishing gradient problem [20]."
            ],
            "citingPaper": {
                "paperId": "99441315e2a68499d5ee41fa04006b3fe71c8998",
                "externalIds": {
                    "DBLP": "conf/isbi/OhKKJKB23",
                    "DOI": "10.1109/ISBI53787.2023.10230332",
                    "CorpusId": 261435503
                },
                "corpusId": 261435503,
                "publicationVenue": {
                    "id": "a38e0d3d-6929-4868-b4e4-af8bbacf711e",
                    "name": "IEEE International Symposium on Biomedical Imaging",
                    "type": "conference",
                    "alternate_names": [
                        "ISBI",
                        "International Symposium on Biomedical Imaging",
                        "Int Symp Biomed Imaging",
                        "IEEE Int Symp Biomed Imaging"
                    ],
                    "issn": "1945-7928",
                    "alternate_issns": [
                        "1945-8452"
                    ],
                    "url": "http://www.biomedicalimaging.org/",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/99441315e2a68499d5ee41fa04006b3fe71c8998",
                "title": "Spatio-Temporal Quantitative Ultrasound Imaging for Breast Cancer Identification",
                "abstract": "In this paper, a real-time ultrasonic attenuation coefficient imaging system identifying cancerous breast lesions is presented. In real-time quantitative medical imaging, spatio-temporal consistency is an important functional requirement, as the inconsistent frame-to-frame quantitative images can degrade diagnostic accuracy. However, the fidelity of an isolated single frame quantitative image is often degraded by the low signal-to-noise ratio due to the short detection time of the RF signal. In order to overcome such problem, the fact that sequential frame-to-frame images have high spatial correlation is utilized. Consequently, a quantitative imaging network featuring an iterative image refinement scheme is designed to inference the difference between adjacent frames images to enhance the spatio-temporal consistency. The proposed system is evaluated through numerical simulation and in-vivo breast cancer measurements. In the in-vivo study, the proposed system demonstrates statistical significance (p-value < 0.05) in the differential diagnosis of breast cancer while achieving 79% less temporal variation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1455851321",
                        "name": "SeokHwan Oh"
                    },
                    {
                        "authorId": "1455980097",
                        "name": "Myeong-Gee Kim"
                    },
                    {
                        "authorId": "2108194711",
                        "name": "Youngmin Kim"
                    },
                    {
                        "authorId": "2093297777",
                        "name": "Gui-Jae Jung"
                    },
                    {
                        "authorId": "2193110935",
                        "name": "Hyuksool Kwon"
                    },
                    {
                        "authorId": "2146977575",
                        "name": "Hyeon-Min Bae"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "The problem motivated many modifications of the RNN structure, including the use of gating mechanisms (to be discussed next), gradient clipping [239], non-saturating activation functions [68], the manipulation of the propagation path of gradients [159] and the use of orthogonal RNNs where the eigenvalues of the hiddento-hidden weight matrix are fixed to one using manifold optimization techiniques [137,181,182,211,336]."
            ],
            "citingPaper": {
                "paperId": "2d9836f272c7d39361e6edf09e35b804e29aa9a3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-12832",
                    "ArXiv": "2301.12832",
                    "DOI": "10.48550/arXiv.2301.12832",
                    "CorpusId": 256389947
                },
                "corpusId": 256389947,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2d9836f272c7d39361e6edf09e35b804e29aa9a3",
                "title": "Deep networks for system identification: a Survey",
                "abstract": "Deep learning is a topic of considerable current interest. The availability of massive data collections and powerful software resources has led to an impressive amount of results in many application areas that reveal essential but hidden properties of the observations. System identi\ufb01cation learns mathematical descriptions of dynamic systems from input-output data and can thus bene\ufb01t from the advances of deep neural networks to enrich the possible range of models to choose from. For this reason, we provide a survey of deep learning from a system identi\ufb01cation perspective. We cover a wide spectrum of topics to enable researchers to understand the methods, providing rigorous practical and theoretical insights into the bene\ufb01ts and challenges of using them. The main aim of the identi\ufb01ed model is to predict new data from previous observations. This can be achieved with different deep learning based modelling techniques and we discuss architectures commonly adopted in the literature, like feedforward, convolutional, and recurrent networks. Their parameters have to be estimated from past data trying to optimize the prediction performance. For this purpose, we discuss a speci\ufb01c set of \ufb01rst-order optimization tools that is emerged as ef\ufb01cient. The survey then draws connections to the well-studied area of kernel-based methods. They control the data \ufb01t by regularization terms that penalize models not in line with prior assumptions. We illustrate how to cast them in deep architectures to obtain deep kernel-based methods. The success of deep learning also resulted in surprising empirical observations, like the counter-intuitive behaviour of models with many parameters. We discuss the role of overparameterized models, including their connection to kernels, as well as implicit regularization mechanisms which affect generalization, speci\ufb01cally the interesting phenomena of benign over\ufb01tting and double-descent. Finally, we highlight numerical, computational and software aspects in the area with the help of applied examples.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1719301",
                        "name": "G. Pillonetto"
                    },
                    {
                        "authorId": "3386308",
                        "name": "A. Aravkin"
                    },
                    {
                        "authorId": "1397100964",
                        "name": "Daniel Gedon"
                    },
                    {
                        "authorId": "1699388",
                        "name": "L. Ljung"
                    },
                    {
                        "authorId": "19235619",
                        "name": "Ant\u00f4nio H. Ribeiro"
                    },
                    {
                        "authorId": "2608035",
                        "name": "T. Schon"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ace0e3dfe0ac715da4613936d298f0a1e979ef51",
                "externalIds": {
                    "DOI": "10.1007/s40808-022-01650-w",
                    "CorpusId": 255662940
                },
                "corpusId": 255662940,
                "publicationVenue": {
                    "id": "5f0658b1-2d16-4076-814b-da30bfe5d21e",
                    "name": "Modeling Earth Systems and Environment",
                    "type": "journal",
                    "alternate_names": [
                        "Model Earth Syst Environ"
                    ],
                    "issn": "2363-6203",
                    "alternate_issns": [
                        "2363-6211"
                    ],
                    "url": "https://www.springer.com/earth+sciences+and+geography/earth+system+sciences/journal/40808",
                    "alternate_urls": [
                        "http://www.springer.com/earth+sciences+and+geography/earth+system+sciences/journal/40808",
                        "https://link.springer.com/journal/40808"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ace0e3dfe0ac715da4613936d298f0a1e979ef51",
                "title": "Towards a better consideration of rainfall and hydrological spatial features by a deep neural network model to improve flash floods forecasting: case study on the Gardon basin, France",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2200393206",
                        "name": "Bob E. Saint-Fleur"
                    },
                    {
                        "authorId": "2200384523",
                        "name": "Sam Allier"
                    },
                    {
                        "authorId": "2200388472",
                        "name": "Emilien Lassara"
                    },
                    {
                        "authorId": "2200384521",
                        "name": "Antoine Rivet"
                    },
                    {
                        "authorId": "48150268",
                        "name": "G. Artigue"
                    },
                    {
                        "authorId": "2599190",
                        "name": "S. Pistre"
                    },
                    {
                        "authorId": "2325558",
                        "name": "A. Johannet"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                ", LSTMs [54], along with a possible modification of the gradient propagation [64], encoder-decoder approaches [28], gradient clipping [100], non-saturating functions [22], and various recurrent-weight initialization techniques via identity or orthogonal matrix initialization [71], [51]."
            ],
            "citingPaper": {
                "paperId": "10799038067c7ae48b37a8c97a89bc770d49d8f2",
                "externalIds": {
                    "ArXiv": "2207.13947",
                    "DBLP": "journals/popets/SavDPBH23",
                    "DOI": "10.48550/arXiv.2207.13947",
                    "CorpusId": 251135050
                },
                "corpusId": 251135050,
                "publicationVenue": {
                    "id": "d5dc4224-e4c3-43c9-918a-bd6326650b5b",
                    "name": "Proceedings on Privacy Enhancing Technologies",
                    "alternate_names": [
                        "Proc Priv Enhancing Technol"
                    ],
                    "issn": "2299-0984",
                    "url": "https://www.degruyter.com/view/j/popets"
                },
                "url": "https://www.semanticscholar.org/paper/10799038067c7ae48b37a8c97a89bc770d49d8f2",
                "title": "Privacy-Preserving Federated Recurrent Neural Networks",
                "abstract": "We present RHODE, a novel system that enables privacy-preserving training of and prediction on Recurrent Neural Networks (RNNs) in a cross-silo federated learning setting by relying on multiparty homomorphic encryption. RHODE preserves the confidentiality of the training data, the model, and the prediction data; and it mitigates federated learning attacks that target the gradients under a passive-adversary threat model. We propose a packing scheme, multi-dimensional packing, for a better utilization of Single Instruction, Multiple Data (SIMD) operations under encryption. With multi-dimensional packing, RHODE enables the efficient processing, in parallel, of a batch of samples. To avoid the exploding gradients problem, RHODE provides several clipping approximations for performing gradient clipping under encryption. We experimentally show that the model performance with RHODE remains similar to non-secure solutions both for homogeneous and heterogeneous data distributions among the data holders. Our experimental evaluation shows that RHODE scales linearly with the number of data holders and the number of timesteps, sub-linearly and sub-quadratically with the number of features and the number of hidden units of RNNs, respectively. To the best of our knowledge, RHODE is the first system that provides the building blocks for the training of RNNs and its variants, under encryption in a federated learning setting.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40409722",
                        "name": "Sinem Sav"
                    },
                    {
                        "authorId": "2179442643",
                        "name": "Abdulrahman Diaa"
                    },
                    {
                        "authorId": "2179442733",
                        "name": "Apostolos Pyrgelis"
                    },
                    {
                        "authorId": "1704265312",
                        "name": "Jean-Philippe Bossuat"
                    },
                    {
                        "authorId": "2121308095",
                        "name": "Jean-Pierre Hubaux"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "16b618a39f8467cfdaa450d207751efe359b4107",
                "externalIds": {
                    "DOI": "10.23919/CCC55666.2022.9902639",
                    "CorpusId": 252851755
                },
                "corpusId": 252851755,
                "publicationVenue": {
                    "id": "23f8fe4c-6537-4027-a334-6a5863115984",
                    "name": "Cybersecurity and Cyberforensics Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Chin Control Conf",
                        "Computational Complexity Conference",
                        "CCC",
                        "Comput Complex Conf",
                        "Cybersecur Cyberforensics Conf",
                        "Conference on Computational Complexity",
                        "Computing Colombian Conference",
                        "Conf Comput Complex",
                        "Comput Colomb Conf",
                        "Chinese Control Conference"
                    ],
                    "url": "http://computationalcomplexity.org/"
                },
                "url": "https://www.semanticscholar.org/paper/16b618a39f8467cfdaa450d207751efe359b4107",
                "title": "Prediction of Matte grade in Copper Flash Smelting Process based on LSTM and Mechanism Model",
                "abstract": "Copper flash smelting is the main process method for copper extracting. It is a complex and multi-phase physical and chemical change process with the characteristics of multiple variable, nonlinearity, delayed detection of product quality. Matte grade is a key indicator reflecting the stability of flash furnace smelting conditions, which directly affect the product quality and process level of the flash smelting. With the \u2018four highs\u2019 (high oxygen-enriched blast, high matte grade, high feed rate, high thermal strength) technical requirements, previous forecasting models are no longer applicable to the new conditions, so it is necessary to establish a forecasting model that can meet the requirements of the latest process. This paper uses matte grade as an example to estimate the calculation results through the compound and establish a mechanism model, analyze its feasibility. Based on the mechanism model, analyze the characteristics of long short-term memory network (LSTM) and choose to establish MSLSTM model (Semi-supervised LSTM based on mechanism model), and compare those methods, which shows that MSLSTM dynamically analyzes and predicts the non-linear relationship between the multi-parameter time series. The measured data from a smelter in Jiangxi, China are used for verification. The analysis results show that the predictive model proposed in this paper has high accuracy. MSLSTM model can be used for production operation guidance and optimal control of copper flash smelting process.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2187602160",
                        "name": "Peng Deng"
                    },
                    {
                        "authorId": "47003295",
                        "name": "Yonggang Li"
                    },
                    {
                        "authorId": "2187702621",
                        "name": "Jia Xin Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Even though there are many techniques to achieve such functionality, LSTM has been shown to achieve exceptional success due to its capability of learning both short and long term dependencies of the problem and also designed to deal with vanishing gradient problem which most of the RNN architectures suffer from [28]."
            ],
            "citingPaper": {
                "paperId": "95455e2a6b2cd280a23f08b05c3e113db5d5533a",
                "externalIds": {
                    "DOI": "10.1016/j.buildenv.2021.108327",
                    "CorpusId": 239219718
                },
                "corpusId": 239219718,
                "publicationVenue": {
                    "id": "26e5cd4a-5f0a-4ead-9653-6e161e82bb6c",
                    "name": "Building and Environment",
                    "type": "journal",
                    "alternate_names": [
                        "Build Environ"
                    ],
                    "issn": "0360-1323",
                    "url": "http://www.sciencedirect.com/science/journal/03601323"
                },
                "url": "https://www.semanticscholar.org/paper/95455e2a6b2cd280a23f08b05c3e113db5d5533a",
                "title": "CNN-LSTM architecture for predictive indoor temperature modeling",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "91021652",
                        "name": "Furkan Elmaz"
                    },
                    {
                        "authorId": "74046456",
                        "name": "Reinout Eyckerman"
                    },
                    {
                        "authorId": "6195456",
                        "name": "W. Casteels"
                    },
                    {
                        "authorId": "17314529",
                        "name": "S. Latr\u00e9"
                    },
                    {
                        "authorId": "145455328",
                        "name": "P. Hellinckx"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Future works could explore other methods for parameter regularization [11, 14, 7]."
            ],
            "citingPaper": {
                "paperId": "22b48e0ed0484b773f69f9802e1ac94f22ffb7c4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-14945",
                    "ArXiv": "2110.14945",
                    "CorpusId": 240070887
                },
                "corpusId": 240070887,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/22b48e0ed0484b773f69f9802e1ac94f22ffb7c4",
                "title": "Preventing posterior collapse in variational autoencoders for text generation via decoder regularization",
                "abstract": "Variational autoencoders trained to minimize the reconstruction error are sensitive to the posterior collapse problem, that is the proposal posterior distribution is always equal to the prior. We propose a novel regularization method based on fraternal dropout to prevent posterior collapse. We evaluate our approach using several metrics and observe improvements in all the tested configurations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2135435306",
                        "name": "Alban Petit"
                    },
                    {
                        "authorId": "3444339",
                        "name": "Caio Corro"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4453bf186c9d40242e430b7d738a9040e1377225",
                "externalIds": {
                    "PubMedCentral": "8548744",
                    "DOI": "10.3389/fgene.2021.746181",
                    "CorpusId": 238639379,
                    "PubMed": "34721533"
                },
                "corpusId": 238639379,
                "publicationVenue": {
                    "id": "9ec189b3-db0f-41d8-9c82-21ef5fa9b87e",
                    "name": "Frontiers in Genetics",
                    "type": "journal",
                    "alternate_names": [
                        "Front Genet"
                    ],
                    "issn": "1664-8021",
                    "url": "http://www.frontiersin.org/genetics/",
                    "alternate_urls": [
                        "http://journal.frontiersin.org/journal/genetics",
                        "https://www.frontiersin.org/journals/genetics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4453bf186c9d40242e430b7d738a9040e1377225",
                "title": "SS-RNN: A Strengthened Skip Algorithm for Data Classification Based on Recurrent Neural Networks",
                "abstract": "Recurrent neural networks are widely used in time series prediction and classification. However, they have problems such as insufficient memory ability and difficulty in gradient back propagation. To solve these problems, this paper proposes a new algorithm called SS-RNN, which directly uses multiple historical information to predict the current time information. It can enhance the long-term memory ability. At the same time, for the time direction, it can improve the correlation of states at different moments. To include the historical information, we design two different processing methods for the SS-RNN in continuous and discontinuous ways, respectively. For each method, there are two ways for historical information addition: 1) direct addition and 2) adding weight weighting and function mapping to activation function. It provides six pathways so as to fully and deeply explore the effect and influence of historical information on the RNNs. By comparing the average accuracy of real datasets with long short-term memory, Bi-LSTM, gated recurrent units, and MCNN and calculating the main indexes (Accuracy, Precision, Recall, and F1-score), it can be observed that our method can improve the average accuracy and optimize the structure of the recurrent neural network and effectively solve the problems of exploding and vanishing gradients.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47416151",
                        "name": "Wenjie Cao"
                    },
                    {
                        "authorId": "2119037898",
                        "name": "Yanshan Shi"
                    },
                    {
                        "authorId": "49660294",
                        "name": "Huahai Qiu"
                    },
                    {
                        "authorId": "30927871",
                        "name": "Ben-gong Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Arpit et al. (2019) propose to modify the path of the gradients in order to stabilize training with a stochastic algorithm specific to LSTM optimization.",
                "Always in Table 1, we compare with two state-of-the-art RNNs (Le et al., 2015; Arjovsky et al., 2016), and with a training algorithm for LSTM (Arpit et al., 2019).",
                "Following the setup proposed in (Arpit et al., 2019), we use 50k images for training, 10k for validation, and 10k to test our models.",
                ", 2016), and with a training algorithm for LSTM (Arpit et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "dc67bdfe3679bf2c230a22046f92602555a61900",
                "externalIds": {
                    "DBLP": "journals/nn/LandiBCC21",
                    "ArXiv": "2109.00020",
                    "DOI": "10.1016/j.neunet.2021.08.030",
                    "CorpusId": 237374551,
                    "PubMed": "34547671"
                },
                "corpusId": 237374551,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/dc67bdfe3679bf2c230a22046f92602555a61900",
                "title": "Working Memory Connections for LSTM",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "67344892",
                        "name": "Federico Landi"
                    },
                    {
                        "authorId": "1843795",
                        "name": "L. Baraldi"
                    },
                    {
                        "authorId": "3468983",
                        "name": "Marcella Cornia"
                    },
                    {
                        "authorId": "1741922",
                        "name": "R. Cucchiara"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In general LSTM model may not be properly trained by certain optimizers due to gradient vanishing problems and gradient exploding problem [11]."
            ],
            "citingPaper": {
                "paperId": "ce4a69584d6ac68447034c03d2d9d6c1705faf1c",
                "externalIds": {
                    "MAG": "3134495145",
                    "DOI": "10.7236/IJIBC.2021.13.1.100",
                    "CorpusId": 235487923
                },
                "corpusId": 235487923,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ce4a69584d6ac68447034c03d2d9d6c1705faf1c",
                "title": "Developing Sentimental Analysis System Based on Various Optimizer",
                "abstract": "Over the past few decades, natural language processing research has not made much. However, the widespread use of deep learning and neural networks attracted attention for the application of neural networks in natural language processing. Sentiment analysis is one of the challenges of natural language processing. Emotions are things that a person thinks and feels. Therefore, sentiment analysis should be able to analyze the person\u2019s attitude, opinions, and inclinations in text or actual text. In the case of emotion analysis, it is a priority to simply classify two emotions: positive and negative. In this paper we propose the deep learning based sentimental analysis system according to various optimizer that is SGD, ADAM and RMSProp. Through experimental result RMSprop optimizer shows the best performance compared to others on IMDB data set. Future work is to find more best hyper parameter for sentimental analysis system.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "70373622",
                        "name": "Seong-Hoon Eom"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "To apply the stochastic gradient truncation [Arpit et al., 2018] we select the probability of truncation p \u2208 {0, 0.1, 0.25, 0.5, 1}, which includes the full backpropagation(p = 0) and the exact truncation (p = 1).",
                "For the orthogonal LMN, we plot the gradient for different values of the probability used to truncate the gradient p [Arpit et al., 2018]."
            ],
            "citingPaper": {
                "paperId": "84bec9d0256f5e623a4741986589a82357fbee5c",
                "externalIds": {
                    "MAG": "3094674343",
                    "DBLP": "journals/corr/abs-2011-02886",
                    "ArXiv": "2011.02886",
                    "CorpusId": 226254211
                },
                "corpusId": 226254211,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/84bec9d0256f5e623a4741986589a82357fbee5c",
                "title": "Short-Term Memory Optimization in Recurrent Neural Networks by Autoencoder-based Initialization",
                "abstract": "Training RNNs to learn long-term dependencies is difficult due to vanishing gradients. We explore an alternative solution based on explicit memorization using linear autoencoders for sequences, which allows to maximize the short-term memory and that can be solved with a closed-form solution without backpropagation. We introduce an initialization schema that pretrains the weights of a recurrent neural network to approximate the linear autoencoder of the input sequences and we show how such pretraining can better support solving hard classification tasks with long sequences. We test our approach on sequential and permuted MNIST. We show that the proposed approach achieves a much lower reconstruction error for long sequences and a better gradient propagation during the finetuning phase.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144689520",
                        "name": "Antonio Carta"
                    },
                    {
                        "authorId": "1749815",
                        "name": "A. Sperduti"
                    },
                    {
                        "authorId": "3224102",
                        "name": "D. Bacciu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "03584f4ed53e6b84d928d67ee5d78f44627fc3da",
                "externalIds": {
                    "MAG": "3092205108",
                    "DBLP": "journals/ploscb/ZhaoWAMSR21",
                    "PubMedCentral": "8496832",
                    "DOI": "10.1101/2020.10.12.336271",
                    "CorpusId": 222803527,
                    "PubMed": "34550967"
                },
                "corpusId": 222803527,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/03584f4ed53e6b84d928d67ee5d78f44627fc3da",
                "title": "Learning, visualizing and exploring 16S rRNA structure using an attention-based deep neural network",
                "abstract": "Recurrent neural networks (RNNs) with memory (e.g. LSTMs) and attention mechanisms are widely used in natural language processing because they can capture short and long term sequential information for diverse tasks. We propose an integrated deep learning model for microbial DNA sequence data, which exploits convolutional networks, recurrent neural networks, and attention mechanisms to perform sample-associated attribute prediction\u2014phenotype prediction\u2014and extract interesting features, such as informative taxa and predictive k-mer context. In this paper, we develop this novel deep learning approach and evaluate its application to amplicon sequences. We focus on typically short DNA reads of 16s ribosomal RNA (rRNA) marker genes, which identify the heterogeneity of a microbial community sample. Our deep learning approach enables sample-level attribute and taxonomic prediction, with the aim of aiding biological research and supporting medical diagnosis. We demonstrate that our implementation of a novel attention-based deep network architecture, Read2Pheno, achieves read-level phenotypic prediction and, in turn, that aggregating read-level information can robustly predict microbial community properties, host phenotype, and taxonomic classification, with performance comparable to conventional approaches. Most importantly, as a further result of the training process, the network architecture will encode sequences (reads) into dense, meaningful representations: learned embedded vectors output on the intermediate layer of the network model, which can provide biological insight when visualized. Finally, we demonstrate that a model with an attention layer can automatically identify informative regions in sequences/reads which are particularly informative for classification tasks. An implementation of the attention-based deep learning network is available at https://github.com/EESI/sequence_attention.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35661444",
                        "name": "Zhengqiao Zhao"
                    },
                    {
                        "authorId": "8230326",
                        "name": "Stephen Woloszynek"
                    },
                    {
                        "authorId": "1997966680",
                        "name": "Felix Agbavor"
                    },
                    {
                        "authorId": "5333369",
                        "name": "J. Mell"
                    },
                    {
                        "authorId": "2213945",
                        "name": "B. Sokhansanj"
                    },
                    {
                        "authorId": "6997856",
                        "name": "G. Rosen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In the case of an LSTM, Kanuparthi et al. (2018) expressed the backpropagated gradient as an iterated addition of the error from each timestep, leading to a similar effect."
            ],
            "citingPaper": {
                "paperId": "a546fa91a408e80ed5895b6b07b34bd887175ba0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-04650",
                    "MAG": "3105054070",
                    "ACL": "2020.findings-emnlp.252",
                    "ArXiv": "2010.04650",
                    "DOI": "10.18653/V1/2020.FINDINGS-EMNLP.252",
                    "CorpusId": 222272198
                },
                "corpusId": 222272198,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a546fa91a408e80ed5895b6b07b34bd887175ba0",
                "title": "LSTMS Compose \u2014 and Learn \u2014 Bottom-Up",
                "abstract": "Recent work in NLP shows that LSTM language models capture compositional structure in language data. In contrast to existing work, we consider the learning process that leads to compositional behavior. For a closer look at how an LSTM\u2019s sequential representations are composed hierarchically, we present a related measure of Decompositional Interdependence (DI) between word meanings in an LSTM, based on their gate interactions. We support this measure with experiments on English language data, where DI is higher on pairs of words with lower syntactic distance. To explore the inductive biases that cause these compositional representations to arise during training, we conduct simple experiments on synthetic data. These synthetic experiments support a specific hypothesis about how hierarchical structures are discovered over the course of training: that LSTM constituent representations are learned bottom-up, relying on effective representations of their shorter children, rather than on learning the longer-range relations independently.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2362960",
                        "name": "Naomi Saphra"
                    },
                    {
                        "authorId": "144871732",
                        "name": "Adam Lopez"
                    }
                ]
            }
        },
        {
            "intents": [
                "result"
            ],
            "isInfluential": false,
            "contexts": [
                "Table 1 shows results for the models listed above, in addition to h-detach [3], an LSTM-based model with improved gradient propagation."
            ],
            "citingPaper": {
                "paperId": "5c3f78e2355b64c4ded503af766da9279380b748",
                "externalIds": {
                    "ArXiv": "2006.09471",
                    "MAG": "3036475981",
                    "DBLP": "journals/corr/abs-2006-09471",
                    "CorpusId": 219720910
                },
                "corpusId": 219720910,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5c3f78e2355b64c4ded503af766da9279380b748",
                "title": "Untangling tradeoffs between recurrence and self-attention in neural networks",
                "abstract": "Attention and self-attention mechanisms, inspired by cognitive processes, are now central to state-of-the-art deep learning on sequential tasks. However, most recent progress hinges on heuristic approaches with limited understanding of attention's role in model optimization and computation, and rely on considerable memory and computational resources that scale poorly. In this work, we present a formal analysis of how self-attention affects gradient propagation in recurrent networks, and prove that it mitigates the problem of vanishing gradients when trying to capture long-term dependencies. Building on these results, we propose a relevancy screening mechanism, inspired by the cognitive process of memory consolidation, that allows for a scalable use of sparse self-attention with recurrence. While providing guarantees to avoid vanishing gradients, we use simple numerical experiments to demonstrate the tradeoffs in performance and computational resources by efficiently balancing attention and recurrence. Based on our results, we propose a concrete direction of research to improve scalability of attentive networks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51922896",
                        "name": "Giancarlo Kerg"
                    },
                    {
                        "authorId": "40974715",
                        "name": "Bhargav Kanuparthi"
                    },
                    {
                        "authorId": "151354678",
                        "name": "Anirudh Goyal"
                    },
                    {
                        "authorId": "51008424",
                        "name": "Kyle Goyette"
                    },
                    {
                        "authorId": "1751762",
                        "name": "Yoshua Bengio"
                    },
                    {
                        "authorId": "49921594",
                        "name": "Guillaume Lajoie"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "db7b67fdcd4eb3820cb0c56bdbd40bbf881ecf58",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2004-13195",
                    "ArXiv": "2004.13195",
                    "MAG": "3023701719",
                    "CorpusId": 216562291
                },
                "corpusId": 216562291,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/db7b67fdcd4eb3820cb0c56bdbd40bbf881ecf58",
                "title": "Word Interdependence Exposes How LSTMs Compose Representations",
                "abstract": "Recent work in NLP shows that LSTM language models capture compositional structure in language data. For a closer look at how these representations are composed hierarchically, we present a novel measure of interdependence between word meanings in an LSTM, based on their interactions at the internal gates. To explore how compositional representations arise over training, we conduct simple experiments on synthetic data, which illustrate our measure by showing how high interdependence can hurt generalization. These synthetic experiments also illustrate a specific hypothesis about how hierarchical structures are discovered over the course of training: that parent constituents rely on effective representations of their children, rather than on learning long-range relations independently. We further support this measure with experiments on English language data, where interdependence is higher for more closely syntactically linked word pairs.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2362960",
                        "name": "Naomi Saphra"
                    },
                    {
                        "authorId": "144871732",
                        "name": "Adam Lopez"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026many techniques, including the use of gating mechanisms (Hochreiter and Schmidhuber, 1997; Cho et al., 2014a), gradient clipping (Pascanu et al., 2013), non-saturating activation functions (Chandar et al., 2019) and the manipulation of the propagation path of gradients (Kanuparthi et al., 2019).",
                ", 2019) and the manipulation of the propagation path of gradients (Kanuparthi et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "0a768a5dea1a796b61463fd4bb64a496a1a043c7",
                "externalIds": {
                    "MAG": "3081800699",
                    "CorpusId": 226116846
                },
                "corpusId": 226116846,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0a768a5dea1a796b61463fd4bb64a496a1a043c7",
                "title": "Learning nonlinear differentiable models for signals and systems: with applications",
                "abstract": "Building empirical models from data is of fundamental importance in engineering, and pushing the boundaries of current engineering technology requires us to model and understand nonlinear systems. In this thesis, nonlinear differentiable models and its applications are studied. This class of models has gained traction in machine learning tasks with the introduction of deep learning. Indeed, deep models of stacked differentiable components have recently achieved super-human performance on several tasks, including computer games, image classification, and medical diagnosis. The application of nonlinear differentiable models is studied for modeling signals and systems both for engineering and machine learning applications. One central question is the role of recurrence and the pros and cons of recurrent and feedforward models. The question is approached from more than one angle: 1) by studying the effect of recurrence in neural networks in terms of robustness to noise, computational cost, and convergence; 2) by analyzing the smoothness of the cost function in nonlinear system identification problems and its relation to the model internal dynamics \u2013 and proposing the use of a technique called multiple shooting for improving the cost-function smoothness; and, 3) by investigating the interplay between the internal dynamics, the attractors and the expressiveness of the model in deep recurrent neural networks. The more applied part of the thesis consists of the use of deep neural networks to solve complex tasks and to model nonlinear behavior from real data. Data from the Telehealth Center of Minas Gerais is used to train a deep neural network capable of identifying abnormalities in the electrocardiogram exam with performance superior to the medical residents in the studied scenario. Also, a deep neural network is used for modeling an electronic oscillator and an F-16 aircraft using data from ground vibration experiments, obtaining competitive results in both cases.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "146172552",
                        "name": "A. H. Ribeiro"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Although the detach-based method has been adopted in a few work [1] for better optimization on sequential tasks, our design and motivation are quite different from it."
            ],
            "citingPaper": {
                "paperId": "9fa67b1ae77844c564266c6fec0c58d170b731e7",
                "externalIds": {
                    "ArXiv": "1911.02559",
                    "MAG": "2989236540",
                    "DBLP": "journals/corr/abs-1911-02559",
                    "CorpusId": 207853328
                },
                "corpusId": 207853328,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9fa67b1ae77844c564266c6fec0c58d170b731e7",
                "title": "SCL: Towards Accurate Domain Adaptive Object Detection via Gradient Detach Based Stacked Complementary Losses",
                "abstract": "Unsupervised domain adaptive object detection aims to learn a robust detector in the domain shift circumstance, where the training (source) domain is label-rich with bounding box annotations, while the testing (target) domain is label-agnostic and the feature distributions between training and testing domains are dissimilar or even totally different. In this paper, we propose a gradient detach based stacked complementary losses (SCL) method that uses detection losses as the primary objective, and cuts in several auxiliary losses in different network stages accompanying with gradient detach training to learn more discriminative representations. We argue that the prior methods mainly leverage more loss functions for training but ignore the interaction of different losses and also the compatible training strategy (gradient detach updating in our work). Thus, our proposed method is a more syncretic adaptation learning process. We conduct comprehensive experiments on seven datasets, the results demonstrate that our method performs favorably better than the state-of-the-art methods by a significant margin. For instance, from Cityscapes to FoggyCityscapes, we achieve 37.9% mAP, outperforming the previous art Strong-Weak by 3.6%.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "145314568",
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "authorId": "70593497",
                        "name": "Harsh Maheshwari"
                    },
                    {
                        "authorId": "2114976515",
                        "name": "Weichen Yao"
                    },
                    {
                        "authorId": "1794486",
                        "name": "M. Savvides"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "18c823a3763f88f9a6f4a3ceefdcc4a74dd10ab1",
                "externalIds": {
                    "DBLP": "conf/aistats/RibeiroTAS20",
                    "ArXiv": "1906.08482",
                    "MAG": "3009122032",
                    "CorpusId": 212414868
                },
                "corpusId": 212414868,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/18c823a3763f88f9a6f4a3ceefdcc4a74dd10ab1",
                "title": "Beyond exploding and vanishing gradients: analysing RNN training using attractors and smoothness",
                "abstract": "The exploding and vanishing gradient problem has been the major conceptual principle behind most architecture and training improvements in recurrent neural networks (RNNs) during the last decade. In this paper, we argue that this principle, while powerful, might need some refinement to explain recent developments. We refine the concept of exploding gradients by reformulating the problem in terms of the cost function smoothness, which gives insight into higher-order derivatives and the existence of regions with many close local minima. We also clarify the distinction between vanishing gradients and the need for the RNN to learn attractors to fully use its expressive power. Through the lens of these refinements, we shed new light on recent developments in the RNN field, namely stable RNN and unitary (or orthogonal) RNNs.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "19235619",
                        "name": "Ant\u00f4nio H. Ribeiro"
                    },
                    {
                        "authorId": "3180466",
                        "name": "K. Tiels"
                    },
                    {
                        "authorId": "143678217",
                        "name": "L. A. Aguirre"
                    },
                    {
                        "authorId": "1802623",
                        "name": "Thomas Bo Sch\u00f6n"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "[15, 13]), purposely using non-saturating activation functions [5], and manipulating the propagation path of gradients [3]."
            ],
            "citingPaper": {
                "paperId": "66fec00077cbf271741b2eac5abf91d9a0a0a10c",
                "externalIds": {
                    "MAG": "2947963487",
                    "DBLP": "conf/nips/KergGTGVBL19",
                    "ArXiv": "1905.12080",
                    "CorpusId": 168169576
                },
                "corpusId": 168169576,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/66fec00077cbf271741b2eac5abf91d9a0a0a10c",
                "title": "Non-normal Recurrent Neural Network (nnRNN): learning long time dependencies while improving expressivity with transient dynamics",
                "abstract": "A recent strategy to circumvent the exploding and vanishing gradient problem in RNNs, and to allow the stable propagation of signals over long time scales, is to constrain recurrent connectivity matrices to be orthogonal or unitary. This ensures eigenvalues with unit norm and thus stable dynamics and training. However this comes at the cost of reduced expressivity due to the limited variety of orthogonal transformations. We propose a novel connectivity structure based on the Schur decomposition and a splitting of the Schur form into normal and non-normal parts. This allows to parametrize matrices with unit-norm eigenspectra without orthogonality constraints on eigenbases. The resulting architecture ensures access to a larger space of spectrally constrained matrices, of which orthogonal matrices are a subset. This crucial difference retains the stability advantages and training speed of orthogonal RNNs while enhancing expressivity, especially on tasks that require computations over ongoing input sequences.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "51922896",
                        "name": "Giancarlo Kerg"
                    },
                    {
                        "authorId": "51008424",
                        "name": "Kyle Goyette"
                    },
                    {
                        "authorId": "2775077",
                        "name": "M. P. Touzel"
                    },
                    {
                        "authorId": "8150760",
                        "name": "Gauthier Gidel"
                    },
                    {
                        "authorId": "37627814",
                        "name": "Eugene Vorontsov"
                    },
                    {
                        "authorId": "1751762",
                        "name": "Yoshua Bengio"
                    },
                    {
                        "authorId": "49921594",
                        "name": "Guillaume Lajoie"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Besides, to verify compatibility with other models, we re-implemented hdetach (Kanuparthi et al. 2019) and incorporate our models, bBeta-LSTM(5G+p).",
                "We compare our models and baselines, LSTM, CIFGLSTM, G2-LSTM, simple recurrent unit (SRU) (Lei et al. 2018), R-transformer (Wang et al. 2019), Batch normalized\nLSTM (BN-LSTM) (Cooijmans et al. 2017), and h-detach (Kanuparthi et al. 2019)."
            ],
            "citingPaper": {
                "paperId": "61bb0b9bf5aba11ab06e2a578ba32b21c4bd6611",
                "externalIds": {
                    "DBLP": "conf/aaai/SongJSM20",
                    "ArXiv": "1905.10521",
                    "MAG": "2997953959",
                    "DOI": "10.1609/AAAI.V34I04.6039",
                    "CorpusId": 166227783
                },
                "corpusId": 166227783,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/61bb0b9bf5aba11ab06e2a578ba32b21c4bd6611",
                "title": "Bivariate Beta LSTM",
                "abstract": "Long Short-Term Memory (LSTM) infers the long term dependency through a cell state maintained by the input and the forget gate structures, which models a gate output as a value in [0,1] through a sigmoid function. However, due to the graduality of the sigmoid function, the sigmoid gate is not flexible in representing multi-modality or skewness. Besides, the previous models lack modeling on the correlation between the gates, which would be a new method to adopt inductive bias for a relationship between previous and current input. This paper proposes a new gate structure with the bivariate Beta distribution. The proposed gate structure enables probabilistic modeling on the gates within the LSTM cell so that the modelers can customize the cell state flow with priors and distributions. Moreover, we theoretically show the higher upper bound of the gradient compared to the sigmoid function, and we empirically observed that the bivariate Beta distribution gate structure provides higher gradient values in training. We demonstrate the effectiveness of the bivariate Beta gate structure on the sentence classification, image classification, polyphonic music modeling, and image caption generation.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2490092",
                        "name": "Kyungwoo Song"
                    },
                    {
                        "authorId": "91586945",
                        "name": "Joonho Jang"
                    },
                    {
                        "authorId": "120296402",
                        "name": "Seung-Jae Shin"
                    },
                    {
                        "authorId": "1729306",
                        "name": "Il-Chul Moon"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "The inclusion of the forget gate in Longshort Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), for instance, reduces vanishing/exploding gradient issues by introducing linear temporal paths which facilitate gradient flow (Kanuparthi et al., 2019).",
                "Challenges still persist even with modern architectures which stabilise gradient flow \u2013 such as Long-short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) \u2013 with multiple lines of active research looking at both memory enhancements and training improvements to help RNNs learn longterm dependencies (Neil et al., 2016; Zhang et al., 2018; Trinh et al., 2018; Kanuparthi et al., 2019).",
                "\u2026as Long-short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) \u2013 with multiple lines of active research looking at both memory enhancements and training improvements to help RNNs learn longterm dependencies (Neil et al., 2016; Zhang et al., 2018; Trinh et al., 2018; Kanuparthi et al., 2019).",
                "Modifications to Standard RNN Training An alternative class of methods investigates the enhancement of standard training methods (Goodfellow et al., 2016), namely the augmentation of loss functions or gradient flows during training (Trinh et al., 2018; Kanuparthi et al., 2019).",
                ", 2016), namely the augmentation of loss functions or gradient flows during training (Trinh et al., 2018; Kanuparthi et al., 2019).",
                "Alternatively, Kanuparthi et al. (2019) explicitly decompose the LSTM recursion equations into a bounded linear and an unbounded polynomial gradient component, with the former being responsible for long-term dependency learn-\ning."
            ],
            "citingPaper": {
                "paperId": "b9f08bba5d77a746f8d607aa7a1032997b9de56f",
                "externalIds": {
                    "ArXiv": "1905.09691",
                    "DBLP": "journals/corr/abs-1905-09691",
                    "MAG": "2945790730",
                    "CorpusId": 162183980
                },
                "corpusId": 162183980,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b9f08bba5d77a746f8d607aa7a1032997b9de56f",
                "title": "Population-based Global Optimisation Methods for Learning Long-term Dependencies with RNNs",
                "abstract": "Despite recent innovations in network architectures and loss functions, training RNNs to learn long-term dependencies remains difficult due to challenges with gradient-based optimisation methods. Inspired by the success of Deep Neuroevolution in reinforcement learning (Such et al. 2017), we explore the use of gradient-free population-based global optimisation (PBO) techniques -- training RNNs to capture long-term dependencies in time-series data. Testing evolution strategies (ES) and particle swarm optimisation (PSO) on an application in volatility forecasting, we demonstrate that PBO methods lead to performance improvements in general, with ES exhibiting the most consistent results across a variety of architectures.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "143845299",
                        "name": "Bryan Lim"
                    },
                    {
                        "authorId": "8106073",
                        "name": "S. Zohren"
                    },
                    {
                        "authorId": "143841496",
                        "name": "Stephen J. Roberts"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "The cal-\n5reproduced from (Arpit et al., 2018) 6reproduced from (Trinh et al., 2018)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nA cc\nu ra\ncy\n1 digit\nCritical initialization Standard initialization\n4 digits 8 digits\n102 103 104\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nA cc\nu ra\ncy\n1 repetition\nCritical initialization\u2026",
                "reproduced from (Arpit et al., 2018) (6)reproduced from (Trinh et al.",
                "\u2026matrices to be exactly or approximately orthogonal (Pascanu et al., 2013; Wisdom et al., 2016; Vorontsov et al., 2017; Jose et al., 2017), or more recently by modifying some terms in the gradient (Arpit et al., 2018), while exploding gradients can be handled by clipping (Pascanu et al., 2013).",
                ", 2017), or more recently by modifying some terms in the gradient (Arpit et al., 2018), while exploding gradients can be handled by clipping (Pascanu et al."
            ],
            "citingPaper": {
                "paperId": "71c49dd770c7b40484ee31954fd450a367f5c4b2",
                "externalIds": {
                    "MAG": "2913728378",
                    "DBLP": "journals/corr/abs-1901-08987",
                    "ArXiv": "1901.08987",
                    "CorpusId": 59291977
                },
                "corpusId": 59291977,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/71c49dd770c7b40484ee31954fd450a367f5c4b2",
                "title": "Dynamical Isometry and a Mean Field Theory of LSTMs and GRUs",
                "abstract": "Training recurrent neural networks (RNNs) on long sequence tasks is plagued with difficulties arising from the exponential explosion or vanishing of signals as they propagate forward or backward through the network. Many techniques have been proposed to ameliorate these issues, including various algorithmic and architectural modifications. Two of the most successful RNN architectures, the LSTM and the GRU, do exhibit modest improvements over vanilla RNN cells, but they still suffer from instabilities when trained on very long sequences. In this work, we develop a mean field theory of signal propagation in LSTMs and GRUs that enables us to calculate the time scales for signal propagation as well as the spectral properties of the state-to-state Jacobians. By optimizing these quantities in terms of the initialization hyperparameters, we derive a novel initialization scheme that eliminates or reduces training instabilities. We demonstrate the efficacy of our initialization scheme on multiple sequence tasks, on which it enables successful training while a standard initialization either fails completely or is orders of magnitude slower. We also observe a beneficial effect on generalization performance using this new initialization.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "32994625",
                        "name": "D. Gilboa"
                    },
                    {
                        "authorId": "144757437",
                        "name": "B. Chang"
                    },
                    {
                        "authorId": "1743082",
                        "name": "Minmin Chen"
                    },
                    {
                        "authorId": "35064203",
                        "name": "Greg Yang"
                    },
                    {
                        "authorId": "2601641",
                        "name": "S. Schoenholz"
                    },
                    {
                        "authorId": "2226805",
                        "name": "Ed H. Chi"
                    },
                    {
                        "authorId": "143845796",
                        "name": "Jeffrey Pennington"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "As a consequence, a welldocumented challenge arises in the form of exploding and vanishing gradients, which has been observed\nin recursive models such as RNNs (Hochreiter and Schmidhuber, 1997), GRUs (Wolter and Yao, 2018), and even LSTMs (Kanuparthi et al., 2019).",
                "As a consequence, a welldocumented challenge arises in the form of exploding and vanishing gradients, which has been observed in recursive models such as RNNs (Hochreiter and Schmidhuber, 1997), GRUs (Wolter and Yao, 2018), and even LSTMs (Kanuparthi et al., 2019).",
                "(10) evolves through a linear recursive equation, while all other states are bounded by sigmoid and tanh activations, causing an imbalance in gradient magnitudes leading to vanishing and exploding gradients over long-term dependencies (Kanuparthi et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "a73701af64496645a33087c69bc5a06f2fa34ba0",
                "externalIds": {
                    "DBLP": "conf/aistats/SawhneyANP22",
                    "CorpusId": 248923887
                },
                "corpusId": 248923887,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a73701af64496645a33087c69bc5a06f2fa34ba0",
                "title": "Orthogonal Multi-Manifold Enriching of Directed Networks",
                "abstract": "Directed Acyclic Graphs and trees are widely prevalent in several real-world applications. These hierarchical structures show intriguing properties such as scale-free and bipartite nature, with \ufb01ne-grained temporal irregularities among nodes. Building on advances in geometrical deep learning, we explore a time-aware neural network to model trees and Directed Acyclic Graphs in multiple Riemannian manifolds of varying curvatures. To jointly utilize the strength of these manifolds, we propose M ulti-Manifold R ecursive I nteraction L earning ( MRIL ) on Directed Acyclic Graphs where we introduce an inter-manifold learning mechanism that recursively enriches each manifold with representations from sibling manifolds. We propose the integration of the Stiefel or-thogonality constraint which stabilizes the training process in Riemannian manifolds. Through a series of quantitative and exploratory experiments, we show that our method achieves competitive performance and converges much faster on data span-ning several domains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51042088",
                        "name": "Ramit Sawhney"
                    },
                    {
                        "authorId": "1923351",
                        "name": "Shivam Agarwal"
                    },
                    {
                        "authorId": "2157860264",
                        "name": "A. Neerkaje"
                    },
                    {
                        "authorId": "2066222462",
                        "name": "K. Pathak"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "They are-Data Preparation [46], Vanilla LSTM [47], Stacked LSTM [48], Bidirectional LSTM [49], CNN LSTM [50], and ConvLSTM [51]."
            ],
            "citingPaper": {
                "paperId": "e1d2f7b6a80e2e751b2a2e290706f4f1d6027126",
                "externalIds": {
                    "CorpusId": 253518969
                },
                "corpusId": 253518969,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e1d2f7b6a80e2e751b2a2e290706f4f1d6027126",
                "title": "Emotional Analysis Using Hybrid Deep Learning Models",
                "abstract": ": A person\u2019s expressions of feeling are readable from their face, which is widely regarded as the most significant feature of the human body. Detecting and recognizing a person\u2019s face is more accurate and less expensive than other types of biometrics. It is possible to infer a person\u2019s intention and state based on their emotional state, thanks to a modality known as emotion. Within the realm of computer vision research, expression analysis and recognition have emerged as one of the more exciting research topics. New HCI research considers the user\u2019s emotional state to deliver a smooth interface. This study suggests a hybrid deep learning technique for emotion analysis based on face images. The suggested system is an amalgamation of VGG16 and Bidirectional LSTM techniques to classify various emotions on the face. Binary cross-entropy was used as a loss function to optimize the model. The model was taught and tested on the KDEF dataset. Another hybrid model comprising of Conv2D, Maxpooling2D, and Bidirectional LSTM models was tested on the CK+48 dataset. Both models showed efficient performance accuracy in training and testing the face emotion classifications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2084695132",
                        "name": "G. V. Shilpa"
                    },
                    {
                        "authorId": "121091847",
                        "name": "V\u0113mana"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "6fb89abd9e3af7f40795702055e1918d1e607e03",
                "externalIds": {
                    "ACL": "2021.jeptalnrecital-taln.1",
                    "DBLP": "conf/taln/PetitC21",
                    "CorpusId": 236145081
                },
                "corpusId": 236145081,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6fb89abd9e3af7f40795702055e1918d1e607e03",
                "title": "Auto-encodeurs variationnels : contrecarrer le probl\u00e8me de posterior collapse gr\u00e2ce \u00e0 la r\u00e9gularisation du d\u00e9codeur (Variational auto-encoders : prevent posterior collapse via decoder regularization)",
                "abstract": "Les auto-encodeurs variationnels sont des mod\u00e8les g\u00e9n\u00e9ratifs utiles pour apprendre des repr\u00e9sentations latentes. En pratique, lorsqu\u2019ils sont supervis\u00e9s pour des t\u00e2ches de g\u00e9n\u00e9ration de textes, ils ont tendance \u00e0 ignorer les variables latentes lors du d\u00e9codage. Nous proposons une nouvelle m\u00e9thode de r\u00e9gularisation fond\u00e9e sur le dropout \u00ab fraternel \u00bb pour encourager l\u2019utilisation de ces variables latentes. Nous \u00e9valuons notre approche sur plusieurs jeux de donn\u00e9es et observons des am\u00e9liorations dans toutes les configurations test\u00e9es.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2059503193",
                        "name": "Alban Petit"
                    },
                    {
                        "authorId": "3444339",
                        "name": "Caio Corro"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "We first published to ICLR 2019 an empirical exploration of variants of backpropagation with better performance on LSTM models [6]."
            ],
            "citingPaper": {
                "paperId": "93a8229cb6635b855ee95ba49da1e26420767bd5",
                "externalIds": {
                    "CorpusId": 8923407
                },
                "corpusId": 8923407,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/93a8229cb6635b855ee95ba49da1e26420767bd5",
                "title": "Main research contributions",
                "abstract": "The success of GANs [14] for generative modeling have recently spurred interest in machine learning for the optimization of smooth games, where interacting agents minimize different objectives (e.g. the generator and discriminator for GAN). Smooth games also appear in various setups like domain adaptation [3] and particular formulations of reinforcement learning [22]. On the other hand, this multi-objective optimization gives rise to much different behavior than in standard objective minimization and could benefit from tailored optimization algorithms different than just simple SGD [9].",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3168518",
                        "name": "Ioannis Mitliagkas"
                    }
                ]
            }
        },
        {
            "intents": [
                "result"
            ],
            "isInfluential": false,
            "contexts": [
                "Table 1 shows results for the models listed above, in addition to h-detach (Arpit et al., 2018), an LSTM-based model with improved gradient propagation."
            ],
            "citingPaper": {
                "paperId": "e94ff03a9b247dc3d88333b5a6b73933c7d393e1",
                "externalIds": {
                    "CorpusId": 221095137
                },
                "corpusId": 221095137,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e94ff03a9b247dc3d88333b5a6b73933c7d393e1",
                "title": "Learning Long-term Dependencies Using Cognitive Inductive Biases in Self-attention RNNs",
                "abstract": "Attention and self-attention mechanisms, inspired by cognitive processes, are now central to stateof-the-art deep learning on sequential tasks. However, most recent progress hinges on heuristic approaches that rely on considerable memory and computational resources that scale poorly. In this work, we propose a relevancy screening mechanism, inspired by the cognitive process of memory consolidation, that allows for a scalable use of sparse self-attention with recurrence. We use simple numerical experiments to demonstrate that this mechanism helps enable recurrent systems on generalization and transfer learning tasks. Based on our results, we propose a concrete direction of research to improve scalability and generalization of attentive recurrent networks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51922896",
                        "name": "Giancarlo Kerg"
                    },
                    {
                        "authorId": "40974715",
                        "name": "Bhargav Kanuparthi"
                    },
                    {
                        "authorId": "151354678",
                        "name": "Anirudh Goyal"
                    },
                    {
                        "authorId": "51008424",
                        "name": "Kyle Goyette"
                    },
                    {
                        "authorId": "1865800402",
                        "name": "Y. Bengio"
                    },
                    {
                        "authorId": "49921594",
                        "name": "Guillaume Lajoie"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "There is a series of further developments following this strategy [17, 5, 26].",
                "This idea is similar to [5] that implements skip operation on conventional RNN, which can be viewed as a Bernoulli distribution sampler on UPDATE or COPY operations at each timestamp t (an analogous idea appeared in h-detach [26] which is applied on LSTM).",
                "H-detach [26] detaches the gradient flow at an arbitrary time step under a Bernoulli distribution."
            ],
            "citingPaper": {
                "paperId": "b9ff84d2d45ec704d5680119d0ea0faa01f016be",
                "externalIds": {
                    "MAG": "3096071706",
                    "DBLP": "conf/eccv/YuLL20",
                    "DOI": "10.1007/978-3-030-58607-2_8",
                    "CorpusId": 226292027
                },
                "corpusId": 226292027,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/b9ff84d2d45ec704d5680119d0ea0faa01f016be",
                "title": "RhyRNN: Rhythmic RNN for Recognizing Events in Long and Complex Videos",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144149597",
                        "name": "Tianshu Yu"
                    },
                    {
                        "authorId": "2110468365",
                        "name": "Yikang Li"
                    },
                    {
                        "authorId": "2913552",
                        "name": "Baoxin Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "To conform with Murdoch et al. (2018), our English language experiments use a one layer (400-dim) LSTM, with inputs taken from an embedding layer and outputs processed by a softmax layer.",
                "In the case of an LSTM, Kanuparthi et al. (2018) expressed the backpropagated gradient as an iterated addition of the error from each timestep, leading to a similar effect."
            ],
            "citingPaper": {
                "paperId": "75f7658200a96c9affdf749cbfac6a2bd8020a1c",
                "externalIds": {
                    "CorpusId": 251277797
                },
                "corpusId": 251277797,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/75f7658200a96c9affdf749cbfac6a2bd8020a1c",
                "title": "Edinburgh Research Explorer LSTMS Compose \u2014 and Learn \u2014 Bottom-Up",
                "abstract": "Recent work in NLP shows that LSTM language models capture hierarchical structure in language data. In contrast to existing work, we consider the learning process that leads to their compositional behavior. For a closer look at how an LSTM\u2019s sequential representations are composed hierarchically, we present a related measure of Decompositional Interdependence (DI) between word meanings in an LSTM, based on their gate interactions. We connect this measure to syntax with experiments on English language data, where DI is higher on pairs of words with lower syntactic distance. To explore the inductive biases that cause these compositional representations to arise during training, we conduct simple experiments on synthetic data. These synthetic experiments support a speci\ufb01c hypothesis about how hierarchical structures are discovered over the course of training: that LSTM constituent representations are learned bottom-up, relying on effective representations of their shorter chil-dren, rather than learning the longer-range relations independently from children.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2180148723",
                        "name": "Lstms Compose"
                    },
                    {
                        "authorId": "2180149813",
                        "name": "Learn\u2014Bottom-Up"
                    },
                    {
                        "authorId": "2362960",
                        "name": "Naomi Saphra"
                    },
                    {
                        "authorId": "144871732",
                        "name": "Adam Lopez"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Although the detach-based method has been adopted in a few work (Arpit et al., 2019) for better optimization on sequential tasks, our design and motivation are quite different from it."
            ],
            "citingPaper": {
                "paperId": "665d9fce91507b4ab440ca2611788543b0708c18",
                "externalIds": {
                    "CorpusId": 229576888
                },
                "corpusId": 229576888,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/665d9fce91507b4ab440ca2611788543b0708c18",
                "title": "SCL: TOWARDS ACCURATE DOMAIN ADAPTIVE OB-",
                "abstract": "Unsupervised domain adaptive object detection aims to learn a robust detector in the domain shift circumstance, where the training (source) domain is label-rich with bounding box annotations, while the testing (target) domain is label-agnostic and the feature distributions between training and testing domains are dissimilar or even totally different. In this paper, we propose a gradient detach based stacked complementary losses (SCL) method that uses detection objective (Ren et al., 2015) (cross entropy and smooth l1 regression) as the primary objective, and cuts in several auxiliary losses in different network stages to utilize information from the complement data (target images) that can be effective in adapting model parameters to both source and target domains. A gradient detach operation is applied between detection and context sub-networks with different objectives during training to force networks to learn discriminative representations. We argue that the conventional training with primary objective mainly leverages the information from the source-domain for maximizing likelihood and ignores the complement data in shallow layers of networks, which leads to an insufficient integration within different domains. Thus, our proposed method is a more syncretic adaptation learning process. We conduct comprehensive experiments on seven datasets, the results demonstrate that our method performs favorably better than the state-of-the-art methods by a large margin. For instance, from Cityscapes to FoggyCityscapes, we achieve 37.9% mAP, outperforming the previous art StrongWeak (Saito et al., 2019) by 3.6%1.",
                "year": 2019,
                "authors": []
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "fba92951a0a736e4678d0335d6f5fdadcfc2f282",
                "externalIds": {
                    "CorpusId": 250048006
                },
                "corpusId": 250048006,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fba92951a0a736e4678d0335d6f5fdadcfc2f282",
                "title": "[Re] h-detach: Modifying the LSTM gradient towards better optimization",
                "abstract": "Recurrent neural networks1 are widely used for processing sequences of information. Recurrent neural networks are very hard to optimize due to the exploding and vanishing gradient problem [2]. This problem becomes more evident in tasks where training data has dependencies that exist over long time scales. A number of architectures have been proposed to mitigate this problem. These architectures include Long Short Term Memory3, Gated Recurrent Unit4. These networks introduce a linear temporal path that allow gradients to flow freely across time-steps. The LSTM is the most widely used architecture. It has been shown that LSTMs have greater representational power as compared to the other architectures5. It is still hard to optimize LSTMs to learn long term dependencies. The authors say that when the weights of the LSTM are large, the gradients through the linear temporal path get suppressed. This path is important as it was introduced to carry information about long termdependencies. The authors prove this empirically. To prevent this, the authors provide a simple stochastic algorithm (h-detach). The authors experiment with this algorithm on tasks that require capturing long-term dependencies. Examples of such tasks include the copying task2,6 and the sequential-mnist task7. The authors claim that their approach results in faster convergence and stable training. In this work, we attempt to reproduce the results for the copying task and the sequential mnist task (permuted and non-permuted). We also conduct experiments to prove their claims mentioned in the ablation study.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "150010392",
                        "name": "Aniket Didolkar"
                    }
                ]
            }
        }
    ]
}