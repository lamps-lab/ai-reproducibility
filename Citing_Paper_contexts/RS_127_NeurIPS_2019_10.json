{
    "offset": 0,
    "data": [
        {
            "contexts": [
                "Further, [23] studied transfer learning with winning tickets, [24] explored the generalization of lottery ticket initializations for efficient downstream adaptation, and [25] proposed inducing adversarial robustness within the transfer learning pipeline as a means of improving transferability."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "99b0a1fb96a3dd9740d00b309cbc39d3619e3336",
                "externalIds": {
                    "ArXiv": "2308.14969",
                    "DBLP": "journals/corr/abs-2308-14969",
                    "DOI": "10.48550/arXiv.2308.14969",
                    "CorpusId": 261276429
                },
                "corpusId": 261276429,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/99b0a1fb96a3dd9740d00b309cbc39d3619e3336",
                "title": "Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets",
                "abstract": "In the era of foundation models with huge pre-training budgets, the downstream tasks have been shifted to the narrative of efficient and fast adaptation. For classification-based tasks in the domain of computer vision, the two most efficient approaches have been linear probing (LP) and visual prompting/reprogramming (VP); the former aims to learn a classifier in the form of a linear head on the features extracted by the pre-trained model, while the latter maps the input data to the domain of the source data on which the model was originally pre-trained on. Although extensive studies have demonstrated the differences between LP and VP in terms of downstream performance, we explore the capabilities of the two aforementioned methods via the sparsity axis: (a) Data sparsity: the impact of few-shot adaptation and (b) Model sparsity: the impact of lottery tickets (LT). We demonstrate that LT are not universal reprogrammers, i.e., for certain target datasets, reprogramming an LT yields significantly lower performance than the reprogrammed dense model although their corresponding upstream performance is similar. Further, we demonstrate that the calibration of dense models is always superior to that of their lottery ticket counterparts under both LP and VP regimes. Our empirical study opens a new avenue of research into VP for sparse models and encourages further understanding of the performance beyond the accuracy achieved by VP under constraints of sparsity. Code and logs can be accessed at \\url{https://github.com/landskape-ai/Reprogram_LT}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145397116",
                        "name": "Diganta Misra"
                    },
                    {
                        "authorId": "2685624",
                        "name": "Agam Goyal"
                    },
                    {
                        "authorId": "2180166734",
                        "name": "Bharat Runwal"
                    },
                    {
                        "authorId": "2158177808",
                        "name": "Pin-Yu Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[5] [33] pooled all layers together and determined pruning thresholds for different layers in an integrated fashion.",
                "Given non-standardized adoption of CNN models for experimentations in post-training pruning works, we examined as most models as appeared in various literature and add those as baselines in our comparison, including Global [33], Uniform [45], Uniform+ [13], LAMP [25], ER ker."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4dd5752eff94246005db1028f8281c2f17545f44",
                "externalIds": {
                    "ArXiv": "2308.10438",
                    "DBLP": "journals/corr/abs-2308-10438",
                    "DOI": "10.48550/arXiv.2308.10438",
                    "CorpusId": 261049780
                },
                "corpusId": 261049780,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4dd5752eff94246005db1028f8281c2f17545f44",
                "title": "Efficient Joint Optimization of Layer-Adaptive Weight Pruning in Deep Neural Networks",
                "abstract": "In this paper, we propose a novel layer-adaptive weight-pruning approach for Deep Neural Networks (DNNs) that addresses the challenge of optimizing the output distortion minimization while adhering to a target pruning ratio constraint. Our approach takes into account the collective influence of all layers to design a layer-adaptive pruning scheme. We discover and utilize a very important additivity property of output distortion caused by pruning weights on multiple layers. This property enables us to formulate the pruning as a combinatorial optimization problem and efficiently solve it through dynamic programming. By decomposing the problem into sub-problems, we achieve linear time complexity, making our optimization algorithm fast and feasible to run on CPUs. Our extensive experiments demonstrate the superiority of our approach over existing methods on the ImageNet and CIFAR-10 datasets. On CIFAR-10, our method achieves remarkable improvements, outperforming others by up to 1.0% for ResNet-32, 0.5% for VGG-16, and 0.7% for DenseNet-121 in terms of top-1 accuracy. On ImageNet, we achieve up to 4.7% and 4.6% higher top-1 accuracy compared to other methods for VGG-16 and ResNet-50, respectively. These results highlight the effectiveness and practicality of our approach for enhancing DNN performance through layer-adaptive weight pruning. Code will be available on https://github.com/Akimoto-Cris/RD_VIT_PRUNE.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109539057",
                        "name": "Kaixin Xu"
                    },
                    {
                        "authorId": "2241483692",
                        "name": "Zhe Wang"
                    },
                    {
                        "authorId": "117174641",
                        "name": "Xue Geng"
                    },
                    {
                        "authorId": "1390606776",
                        "name": "Min Wu"
                    },
                    {
                        "authorId": "2108674591",
                        "name": "Xiaoli Li"
                    },
                    {
                        "authorId": "144968898",
                        "name": "Weisi Lin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[109] find OneTicket that can generalize across a variety of datasets and optimizers within the natural image domain.",
                "[109] observe that for image classification, winning tickets generated on larger datasets (such as bigger training set size and/or more number of classes) consistently transferred better than those generated with smaller datasets.",
                "However, this kind of method is not friendly to those significant weights whose importance is not immediately apparent at the beginning [109].",
                "(2) Some literature ([67, 109, 110, 111]) studies the transferability of a winning ticket found in a source dataset to another dataset, which provides insights into the transferability of LTH."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "81b268e98042864c025b401eb6a54dcb566486d5",
                "externalIds": {
                    "ArXiv": "2308.06767",
                    "DBLP": "journals/corr/abs-2308-06767",
                    "DOI": "10.48550/arXiv.2308.06767",
                    "CorpusId": 260887757
                },
                "corpusId": 260887757,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/81b268e98042864c025b401eb6a54dcb566486d5",
                "title": "A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations",
                "abstract": "Modern deep neural networks, particularly recent large language models, come with massive model sizes that require significant computational and storage resources. To enable the deployment of modern models on resource-constrained environments and accelerate inference time, researchers have increasingly explored pruning techniques as a popular research direction in neural network compression. However, there is a dearth of up-to-date comprehensive review papers on pruning. To address this issue, in this survey, we provide a comprehensive review of existing research works on deep neural network pruning in a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to prune, and 4) fusion of pruning and other compression techniques. We then provide a thorough comparative analysis of seven pairs of contrast settings for pruning (e.g., unstructured/structured) and explore emerging topics, including post-training pruning, different levels of supervision for pruning, and broader applications (e.g., adversarial robustness) to shed light on the commonalities and differences of existing methods and lay the foundation for further method development. To facilitate future research, we build a curated collection of datasets, networks, and evaluations on different applications. Finally, we provide some valuable recommendations on selecting pruning methods and prospect promising research directions. We build a repository at https://github.com/hrcheng1066/awesome-pruning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2230189096",
                        "name": "Hongrong Cheng"
                    },
                    {
                        "authorId": "2211872272",
                        "name": "Miao Zhang"
                    },
                    {
                        "authorId": "3177281",
                        "name": "Javen Qinfeng Shi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f1636498b20cd43e9b2813d4c9428582191cad8d",
                "externalIds": {
                    "DBLP": "conf/islped/OgboguMJDHCP23",
                    "DOI": "10.1109/ISLPED58423.2023.10244258",
                    "CorpusId": 262076318
                },
                "corpusId": 262076318,
                "publicationVenue": {
                    "id": "e84fba73-73c5-46ba-96c3-a9a39ffca9be",
                    "name": "International Symposium on Low Power Electronics and Design",
                    "type": "conference",
                    "alternate_names": [
                        "ISLPED",
                        "Int Symp Low Power Electron Des"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1723"
                },
                "url": "https://www.semanticscholar.org/paper/f1636498b20cd43e9b2813d4c9428582191cad8d",
                "title": "Energy-Efficient ReRAM-Based ML Training via Mixed Pruning and Reconfigurable ADC",
                "abstract": "Machine learning (ML) models have gained prominence in solving real-world tasks. However, implementing ML models is both compute- and memory-intensive. Domain-specific architectures such as Resistive Random Access Memory (ReRAM)-based Processing-in-Memory (PIM) platforms have been proposed to efficiently accelerate ML training and inference. However, existing ML workloads require a high amount of area and power for training. A major contributor to the area and power overheads is the Analog-to-Digital Converter (ADC). In this work, we propose a mixed pruning technique along with a novel reconfigurable ADC design to improve the power consumption profile. Overall, the pruned model with the reconfigurable ADC achieves ~50% reduction in power for training compared to existing state-of-the-art ReRAM-based architectures.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2181168400",
                        "name": "Chukwufumnanya Ogbogu"
                    },
                    {
                        "authorId": "2247976777",
                        "name": "Soumen Mohapatra"
                    },
                    {
                        "authorId": "25173634",
                        "name": "Biresh Kumar Joardar"
                    },
                    {
                        "authorId": "2089333",
                        "name": "J. Doppa"
                    },
                    {
                        "authorId": "34965986",
                        "name": "D. Heo"
                    },
                    {
                        "authorId": "2242955212",
                        "name": "Krishnendu Chakrabarty"
                    },
                    {
                        "authorId": "1754491",
                        "name": "P. Pande"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0d2194a33325dffab42fb7c0994dae1539e9ac7d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-03128",
                    "ArXiv": "2308.03128",
                    "DOI": "10.48550/arXiv.2308.03128",
                    "CorpusId": 260683079
                },
                "corpusId": 260683079,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0d2194a33325dffab42fb7c0994dae1539e9ac7d",
                "title": "Iterative Magnitude Pruning as a Renormalisation Group: A Study in The Context of The Lottery Ticket Hypothesis",
                "abstract": "This thesis delves into the intricate world of Deep Neural Networks (DNNs), focusing on the exciting concept of the Lottery Ticket Hypothesis (LTH). The LTH posits that within extensive DNNs, smaller, trainable subnetworks termed\"winning tickets\", can achieve performance comparable to the full model. A key process in LTH, Iterative Magnitude Pruning (IMP), incrementally eliminates minimal weights, emulating stepwise learning in DNNs. Once we identify these winning tickets, we further investigate their\"universality\". In other words, we check if a winning ticket that works well for one specific problem could also work well for other, similar problems. We also bridge the divide between the IMP and the Renormalisation Group (RG) theory in physics, promoting a more rigorous understanding of IMP.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2227939328",
                        "name": "Abu-Al Hassan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "bbc9c7032b6468ac9eb16cd9a1b9f83297d91bae",
                "externalIds": {
                    "DBLP": "journals/tcad/OgboguAPJDCP23",
                    "DOI": "10.1109/TCAD.2022.3227879",
                    "CorpusId": 254494140
                },
                "corpusId": 254494140,
                "publicationVenue": {
                    "id": "e86c30b0-c1dd-4f0e-be5e-22af711f7d5f",
                    "name": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Comput Des Integr Circuit Syst"
                    ],
                    "issn": "0278-0070",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=43",
                    "alternate_urls": [
                        "http://ieee-ceda.org/publications/tcad",
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=43"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bbc9c7032b6468ac9eb16cd9a1b9f83297d91bae",
                "title": "Accelerating Graph Neural Network Training on ReRAM-Based PIM Architectures via Graph and Model Pruning",
                "abstract": "Graph neural networks (GNNs) are used for predictive analytics on graph-structured data, and they have become very popular in diverse real-world applications. Resistive random-access memory (ReRAM)-based PIM architectures can accelerate GNN training. However, GNN training on ReRAM-based architectures is both compute- and data intensive in nature. In this work, we propose a framework called <italic>SlimGNN</italic> that synergistically combines both graph and model pruning to accelerate GNN training on ReRAM-based architectures. The proposed framework reduces the amount of redundant information in both the GNN model and input graph(s) to streamline the overall training process. This enables fast and energy-efficient GNN training on ReRAM-based architectures. Experimental results demonstrate that using this framework, we can accelerate GNN training by up to <inline-formula> <tex-math notation=\"LaTeX\">$ {4}. {5} {\\times }$ </tex-math></inline-formula> while using <inline-formula> <tex-math notation=\"LaTeX\">$ {6}. {6} {\\times }$ </tex-math></inline-formula> less energy compared to the unpruned counterparts.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2181168400",
                        "name": "Chukwufumnanya Ogbogu"
                    },
                    {
                        "authorId": "49212589",
                        "name": "Aqeeb Iqbal Arka"
                    },
                    {
                        "authorId": "2195293869",
                        "name": "Lukas Pfromm"
                    },
                    {
                        "authorId": "25173634",
                        "name": "Biresh Kumar Joardar"
                    },
                    {
                        "authorId": "2089333",
                        "name": "J. Doppa"
                    },
                    {
                        "authorId": "143631962",
                        "name": "K. Chakrabarty"
                    },
                    {
                        "authorId": "1754491",
                        "name": "P. Pande"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Work by Morcos et al.[18] has already shown that for certain tasks such as computer vision, winning tickets can be transferred across different tasks/models.",
                "In fact, transferability of winning lottery tickets has been shown in several fields [16],[17],[18] including across networks with different architectures [19]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3ec62b4e8c32567ce6621606d2bf2d04e380e49c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-09863",
                    "ArXiv": "2306.09863",
                    "DOI": "10.48550/arXiv.2306.09863",
                    "CorpusId": 259188073
                },
                "corpusId": 259188073,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3ec62b4e8c32567ce6621606d2bf2d04e380e49c",
                "title": "Transferability of Winning Lottery Tickets in Neural Network Differential Equation Solvers",
                "abstract": "Recent work has shown that renormalisation group theory is a useful framework with which to describe the process of pruning neural networks via iterative magnitude pruning. This report formally describes the link between RG theory and IMP and extends previous results around the Lottery Ticket Hypothesis and Elastic Lottery Hypothesis to Hamiltonian Neural Networks for solving differential equations. We find lottery tickets for two Hamiltonian Neural Networks and demonstrate transferability between the two systems, with accuracy being dependent on integration times. The universality of the two systems is then analysed using tools from an RG perspective.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220218937",
                        "name": "Edward Prideaux-Ghee"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "28f1fb45610537afd2c94c28c0cf011015c795c4",
                "externalIds": {
                    "ArXiv": "2306.02415",
                    "CorpusId": 259075469
                },
                "corpusId": 259075469,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/28f1fb45610537afd2c94c28c0cf011015c795c4",
                "title": "Top-Down Network Combines Back-Propagation with Attention",
                "abstract": "Cortical processing, in vision and other domains, combines bottom-up (BU) with extensive top-down (TD) processing. Two primary goals attributed to TD processing are learning and directing attention. These two roles are accomplished in current network models through distinct mechanisms. Attention guidance is often implemented by extending the model's architecture, while learning is typically accomplished by an external learning algorithm such as back-propagation. In the current work, we present an integration of the two functions above, which appear unrelated, using a single unified mechanism inspired by the human brain. We propose a novel symmetric bottom-up top-down network structure that can integrate conventional bottom-up networks with a symmetric top-down counterpart, allowing each network to recurrently guide and influence the other. For example, during multi-task learning, the same top-down network is being used for both learning, via propagating feedback signals, and at the same time also for top-down attention, by guiding the bottom-up network to perform a selected task. In contrast with standard models, no external back-propagation is used for learning. Instead, we propose a 'Counter-Hebb' learning, which adjusts the weights of both the bottom-up and top-down networks simultaneously. We show that our method achieves competitive performance on standard multi-task learning benchmarks. Yet, unlike existing methods, we rely on single-task architectures and optimizers, without any task-specific parameters. The results, which show how attention-guided multi-tasks can be combined efficiently with internal learning in a unified TD process, suggest a possible model for combining BU and TD processing in human vision.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2070064179",
                        "name": "R. Abel"
                    },
                    {
                        "authorId": "1743045",
                        "name": "S. Ullman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Inspired by [47], we conduct experiments to study the transferability of FSTs.",
                "This is because FSTs drawn from large datasets are less likely to suffer from overfitting, which is consitent with [47]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "d36cd8385c969608282d874b1623564959a71b42",
                "externalIds": {
                    "DBLP": "conf/cvpr/TangYLL23",
                    "DOI": "10.1109/CVPR52729.2023.02338",
                    "CorpusId": 260068477
                },
                "corpusId": 260068477,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d36cd8385c969608282d874b1623564959a71b42",
                "title": "Fair Scratch Tickets: Finding Fair Sparse Networks without Weight Training",
                "abstract": "Recent studies suggest that computer vision models come at the risk of compromising fairness. There are exten-sive works to alleviate unfairness in computer vision using pre-processing, in-processing, and post-processing meth-ods. In this paper, we lead a novel fairness-aware learning paradigm for in-processing methods through the lens of the lottery ticket hypothesis (LTH) in the context of computer vision fairness. We randomly initialize a dense neural net-work and find appropriate binary masks for the weights to obtain fair sparse subnetworks without any weight training. Interestingly, to the best of our knowledge, we are the first to discover that such sparse subnetworks with inborn fair-ness exist in randomly initialized networks, achieving an accuracy-fairness trade-off comparable to that of dense neural networks trained with existing fairness-aware in-processing approaches. We term these fair subnetworks as Fair Scratch Tickets (FSTs). We also theoretically pro-vide fairness and accuracy guarantees for them. In our experiments, we investigate the existence of FSTs on var-ious datasets, target attributes, random initialization meth-ods, sparsity patterns, and fairness surrogates. We also find that FSTs can transfer across datasets and investigate other properties of FSTs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2212886261",
                        "name": "Pengwei Tang"
                    },
                    {
                        "authorId": "145889956",
                        "name": "W. Yao"
                    },
                    {
                        "authorId": "1934400475",
                        "name": "Zhicong Li"
                    },
                    {
                        "authorId": "2156610887",
                        "name": "Yong Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Given the fact that CNN models are generally overparameterized, many works [11, 37, 39, 50] have demonstrated a sparse sub-network can still reach the accuracy comparable to the original dense network and many channels in each layer can be taken away without harming the performance."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ce13c50bb6c66551806b3cd04d764990d2b7abae",
                "externalIds": {
                    "DBLP": "conf/cvpr/DingT00023",
                    "DOI": "10.1109/CVPR52729.2023.01941",
                    "CorpusId": 260084961
                },
                "corpusId": 260084961,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ce13c50bb6c66551806b3cd04d764990d2b7abae",
                "title": "Network Expansion For Practical Training Acceleration",
                "abstract": "Recently, the sizes of deep neural networks and training datasets both increase drastically to pursue better performance in a practical sense. With the prevalence of transformer-based models in vision tasks, even more pressure is laid on the GPU platforms to train these heavy models, which consumes a large amount of time and computing resources as well. Therefore, it's crucial to accelerate the training process of deep neural networks. In this paper, we propose a general network expansion method to reduce the practical time cost of the model training process. Specifically, we utilize both width- and depth-level sparsity of dense models to accelerate the training of deep neural networks. Firstly, we pick a sparse sub-network from the original dense model by reducing the number of parameters as the starting point of training. Then the sparse architecture will gradually expand during the training procedure and finally grow into a dense one. We design different expanding strategies to grow CNNs and ViTs respectively, due to the great heterogeneity in between the two architectures. Our method can be easily integrated into popular deep learning frameworks, which saves considerable training time and hardware resources. Extensive experiments show that our acceleration method can significantly speed up the training process of modern vision models on general GPU devices with negligible performance drop (e.g. 1.42\u00d7 faster for ResNet-101 and 1.34\u00d7 faster for DeiT-base on ImageNet-1k). The code is available at https://github.com/huawei-noah/Efficient-Computing/tree/master/TrainingAcceleration/NetworkExpansion and https://gitee.com/mindspore/hub/blob/master/mshub_res/assets/noah-cvlab/gpu/1.8/networkexpansion_v1.0_imagenet2012.md",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2066768061",
                        "name": "N. Ding"
                    },
                    {
                        "authorId": "103603255",
                        "name": "Yehui Tang"
                    },
                    {
                        "authorId": "3826388",
                        "name": "Kai Han"
                    },
                    {
                        "authorId": "30136198",
                        "name": "Chaoting Xu"
                    },
                    {
                        "authorId": "2108702980",
                        "name": "Yunhe Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We take inspiration by the work of Morcos et al. (2019) and re-train sparse initializations generated for one task-ES configuration on a different setting with a shared network architecture."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "261c8a036e3de4180737a28466e45d3d733d7d05",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-00045",
                    "ArXiv": "2306.00045",
                    "DOI": "10.1145/3583133.3597059",
                    "CorpusId": 258999843
                },
                "corpusId": 258999843,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/261c8a036e3de4180737a28466e45d3d733d7d05",
                "title": "Lottery tickets in evolutionary optimization: On sparse backpropagation-free trainability",
                "abstract": "Lottery tickets in Deep Learning [2] refer to highly sparse neural network initializations, which train to the performance level of their dense counterparts The existence of such sparse trainable initializations has previously been documented for a variety of gradient-based training settings. But is the lottery ticket phenomenon an idiosyncrasy of stochastic gradient descent or does it generalize to evolutionary optimization? In this paper we establish the existence of highly sparse trainable initializations for evolution strategies (ES) and characterize qualitative differences compared to gradient descent (GD)-based sparse training. We introduce a novel signal-to-noise (SNR) iterative pruning procedure, which extracts evolvable sub-networks and incorporates loss curvature information into the network pruning step. We demonstrate the existence of highly sparse evolvable initializations for a wide range of network architectures, evolution strategies and task settings. Furthermore, we find that these initializations encode an inductive bias, which transfers across different evolution strategies, related tasks and even GD-based training. Finally, we compare the local optima resulting from the different optimization paradigms and sparsity levels. In contrast to GD, ES explore diverse and flat local optima and do not preserve linear mode connectivity across sparsity levels and independent runs. The full paper was accepted at the ICML conference [4].",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2066450047",
                        "name": "R. Lange"
                    },
                    {
                        "authorId": "2593439",
                        "name": "Henning Sprekeler"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Specifically, it has been shown that the winning ticket can be transferred between datasets and tasks [19, 47, 13, 57].",
                "Other works demonstrated the generality of the winning ticket and showed that a single pruned network can be transferred across datasets and achieve good performance after fine-tuning [19, 47, 13] and even that LT can be used for non-natural datasets [57]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f71da88502c3b1ad8ac285ab0be15b423413e44c",
                "externalIds": {
                    "ArXiv": "2305.17559",
                    "DBLP": "journals/corr/abs-2305-17559",
                    "DOI": "10.48550/arXiv.2305.17559",
                    "CorpusId": 258959362
                },
                "corpusId": 258959362,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f71da88502c3b1ad8ac285ab0be15b423413e44c",
                "title": "Pruning at Initialization - A Sketching Perspective",
                "abstract": "The lottery ticket hypothesis (LTH) has increased attention to pruning neural networks at initialization. We study this problem in the linear setting. We show that finding a sparse mask at initialization is equivalent to the sketching problem introduced for efficient matrix multiplication. This gives us tools to analyze the LTH problem and gain insights into it. Specifically, using the mask found at initialization, we bound the approximation error of the pruned linear model at the end of training. We theoretically justify previous empirical evidence that the search for sparse networks may be data independent. By using the sketching perspective, we suggest a generic improvement to existing algorithms for pruning at initialization, which we show to be beneficial in the data-independent case.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2051500472",
                        "name": "Noga Bar"
                    },
                    {
                        "authorId": "2711839",
                        "name": "R. Giryes"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another study [13] conducted experiments to demonstrate"
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "2ad15361f629047fdce624f4c45ac0d4b0765be8",
                "externalIds": {
                    "DBLP": "conf/iscas/KimK23",
                    "DOI": "10.1109/ISCAS46773.2023.10181631",
                    "CorpusId": 260003616
                },
                "corpusId": 260003616,
                "publicationVenue": {
                    "id": "9bc219ae-a4dc-4241-8e1a-0552f9ee9ef7",
                    "name": "International Symposium on Circuits and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "ISCAS",
                        "Int Symp Circuit Syst"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2ad15361f629047fdce624f4c45ac0d4b0765be8",
                "title": "RepSGD: Channel Pruning Using Reparamerization for Accelerating Convolutional Neural Networks",
                "abstract": "Channel pruning is a popular method for compressing convolutional neural networks (CNNs) while maintaining acceptable accuracy. Most existing channel pruning methods use the approach of zeroing unnecessary filters and then removing them. To address the limitations of existing approaches, methods of creating forcibly filter redundancy and then removing redundant filters have been proposed without heuristic knowledge. However, these methods also use a deformed gradient to make filters identical, and performance degradation is inevitable because the parameters cannot be updated using the original gradients. To solve these problems, this study proposes RepSGD, which can compress CNNs simply and efficiently. RepSGD inserts a new point-wise convolution layer after the existing standard convolution layer. Subsequently, only new point-wise convolution layers are trained to produce filter redundancy (i.e., to make the filters identical), whereas the standard convolution layers are trained using the original gradient. After training, RepSGD merges two consecutive convolution layers into one convolution layer. Subsequently, the redundant filters in the merged convolution layer are pruned. Because RepSGD does not change the original architecture of the CNN, additional inference computation is not required, and it is possible to support training from scratch. In addition, using the original gradient in RepSGD optimizes the objective function of the CNNs better. We show that RepSGD outperforms existing pruning methods in various models and datasets through extensive experiments.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2119401673",
                        "name": "N. Kim"
                    },
                    {
                        "authorId": "2145490209",
                        "name": "Hyun Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the natural image domain, Morcos et al. (2019) finds that the winning ticket initializations generalize across a variety of vision benchmark datasets."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8cb97eb1acb9db3e683ab531bdf7b81e68a0cd46",
                "externalIds": {
                    "ArXiv": "2305.02190",
                    "DBLP": "conf/iclr/Hui0MK23",
                    "DOI": "10.48550/arXiv.2305.02190",
                    "CorpusId": 258461067
                },
                "corpusId": 258461067,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8cb97eb1acb9db3e683ab531bdf7b81e68a0cd46",
                "title": "Rethinking Graph Lottery Tickets: Graph Sparsity Matters",
                "abstract": "Lottery Ticket Hypothesis (LTH) claims the existence of a winning ticket (i.e., a properly pruned sub-network together with original weight initialization) that can achieve competitive performance to the original dense network. A recent work, called UGS, extended LTH to prune graph neural networks (GNNs) for effectively accelerating GNN inference. UGS simultaneously prunes the graph adjacency matrix and the model weights using the same masking mechanism, but since the roles of the graph adjacency matrix and the weight matrices are very different, we find that their sparsifications lead to different performance characteristics. Specifically, we find that the performance of a sparsified GNN degrades significantly when the graph sparsity goes beyond a certain extent. Therefore, we propose two techniques to improve GNN performance when the graph sparsity is high. First, UGS prunes the adjacency matrix using a loss formulation which, however, does not properly involve all elements of the adjacency matrix; in contrast, we add a new auxiliary loss head to better guide the edge pruning by involving the entire adjacency matrix. Second, by regarding unfavorable graph sparsification as adversarial data perturbations, we formulate the pruning process as a min-max optimization problem to gain the robustness of lottery tickets when the graph sparsity is high. We further investigate the question: Can the\"retrainable\"winning ticket of a GNN be also effective for graph transferring learning? We call it the transferable graph lottery ticket (GLT) hypothesis. Extensive experiments were conducted which demonstrate the superiority of our proposed sparsification method over UGS, and which empirically verified our transferable GLT hypothesis.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2064059011",
                        "name": "Bo Hui"
                    },
                    {
                        "authorId": "2149274549",
                        "name": "Da Yan"
                    },
                    {
                        "authorId": "2184257253",
                        "name": "Xiaolong Ma"
                    },
                    {
                        "authorId": "1758909",
                        "name": "Wei-Shinn Ku"
                    }
                ]
            }
        },
        {
            "contexts": [
                "1) Experiment Setup: To answer the RQ3, we select 4 state-of-the-art model compression techniques proposed in the last two years (i.e., LAMP [30], Global [31], Uniform+ [32] and ERK [33]) to see whether our DeepArc framework can speed up the model compression efficiency.",
                ", LAMP [30], Global [31], Uniform+ [32] and ERK [33]) to see whether our DeepArc framework can speed up the model compression efficiency."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "47f9e5501758763ecda7bbb7b98b0f66054ef5f0",
                "externalIds": {
                    "DBLP": "conf/icse/RenLXLSFD23",
                    "DOI": "10.1109/ICSE48619.2023.00092",
                    "CorpusId": 259860518
                },
                "corpusId": 259860518,
                "publicationVenue": {
                    "id": "a36dc29e-4ea1-4567-b0fe-1c06daf8bee8",
                    "name": "International Conference on Software Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Softw Eng",
                        "ICSE"
                    ],
                    "url": "http://www.icse-conferences.org/"
                },
                "url": "https://www.semanticscholar.org/paper/47f9e5501758763ecda7bbb7b98b0f66054ef5f0",
                "title": "DeepArc: Modularizing Neural Networks for the Model Maintenance",
                "abstract": "Neural networks are an emerging data-driven programming paradigm widely used in many areas. Unlike traditional software systems consisting of decomposable modules, a neural network is usually delivered as a monolithic package, raising challenges for some maintenance tasks such as model restructure and re-adaption. In this work, we propose DeepArc, a novel modularization method for neural networks, to reduce the cost of model maintenance tasks. Specifically, DeepArc decomposes a neural network into several consecutive modules, each of which encapsulates consecutive layers with similar semantics. The network modularization facilitates practical tasks such as refactoring the model to preserve existing features (e.g., model compression) and enhancing the model with new features (e.g., fitting new samples). The modularization and encapsulation allow us to restructure or retrain the model by only pruning and tuning a few localized neurons and layers. Our experiments show that (1) DeepArc can boost the runtime efficiency of the state-of-the-art model compression techniques by 14.8%; (2) compared to the traditional model retraining, DeepArc only needs to train less than 20% of the neurons on average to fit adversarial samples and repair under-performing models, leading to 32.85% faster training performance while achieving similar model prediction performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2217983250",
                        "name": "Xiaoning Ren"
                    },
                    {
                        "authorId": "47904366",
                        "name": "Yun Lin"
                    },
                    {
                        "authorId": "2367687",
                        "name": "Yinxing Xue"
                    },
                    {
                        "authorId": "2143183668",
                        "name": "Ruofan Liu"
                    },
                    {
                        "authorId": "2155020757",
                        "name": "Jun Sun"
                    },
                    {
                        "authorId": "2113908670",
                        "name": "Zhiyong Feng"
                    },
                    {
                        "authorId": "2152487387",
                        "name": "J. Dong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the field of transferring sparse pretrained models, several works have explored the effect of different unstructured pruning techniques during pretraining on transfer performance (Mehta, 2019; Morcos et al., 2019; Paganini & Forde, 2020b; Sabatelli et al., 2020; Sun et al., 2022; Liu et al., 2022).",
                "\u2026the field of transferring sparse pretrained models, several works have explored the effect of different unstructured pruning techniques during pretraining on transfer performance (Mehta, 2019; Morcos et al., 2019; Paganini & Forde, 2020b; Sabatelli et al., 2020; Sun et al., 2022; Liu et al., 2022)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ed302fcf4b1e6350e7d81b77417182b60c429e91",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-13164",
                    "ArXiv": "2304.13164",
                    "DOI": "10.48550/arXiv.2304.13164",
                    "CorpusId": 258331614
                },
                "corpusId": 258331614,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ed302fcf4b1e6350e7d81b77417182b60c429e91",
                "title": "Towards Compute-Optimal Transfer Learning",
                "abstract": "The field of transfer learning is undergoing a significant shift with the introduction of large pretrained models which have demonstrated strong adaptability to a variety of downstream tasks. However, the high computational and memory requirements to finetune or use these models can be a hindrance to their widespread use. In this study, we present a solution to this issue by proposing a simple yet effective way to trade computational efficiency for asymptotic performance which we define as the performance a learning algorithm achieves as compute tends to infinity. Specifically, we argue that zero-shot structured pruning of pretrained models allows them to increase compute efficiency with minimal reduction in performance. We evaluate our method on the Nevis'22 continual learning benchmark that offers a diverse set of transfer scenarios. Our results show that pruning convolutional filters of pretrained models can lead to more than 20% performance improvement in low computational regimes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1750641",
                        "name": "Massimo Caccia"
                    },
                    {
                        "authorId": "51980959",
                        "name": "Alexandre Galashov"
                    },
                    {
                        "authorId": "1660848177",
                        "name": "Arthur Douillard"
                    },
                    {
                        "authorId": "1557605861",
                        "name": "Amal Rannen-Triki"
                    },
                    {
                        "authorId": "143668237",
                        "name": "Dushyant Rao"
                    },
                    {
                        "authorId": "35550664",
                        "name": "Michela Paganini"
                    },
                    {
                        "authorId": "1778839",
                        "name": "Laurent Charlin"
                    },
                    {
                        "authorId": "1706809",
                        "name": "Marc'Aurelio Ranzato"
                    },
                    {
                        "authorId": "1996134",
                        "name": "Razvan Pascanu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, there have been many attempts at pruning convolutional neural networks (CNNs) which include the original paper on lottery tickets itself [6] (80\u201390% prune rate), a generalized approach [7] to prune across many vision datasets (such as MNIST, SVHN, CIFAR-10/100, ImageNet, and Places365), and an object detection variant [8] pruned up to 80%.",
                "Second, in the generalized work on pruning CNNs [7] they mention larger datasets tend to produce better lottery tickets."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ab5e9f33be33c6e4346cece73aea9b982fba2c53",
                "externalIds": {
                    "DBLP": "journals/make/BluteauG23",
                    "DOI": "10.3390/make5020024",
                    "CorpusId": 258250104
                },
                "corpusId": 258250104,
                "publicationVenue": {
                    "id": "472fe64a-ea91-4506-8d2d-4c9a9374e1ea",
                    "name": "Machine Learning and Knowledge Extraction",
                    "alternate_names": [
                        "Mach Learn Knowl Extr"
                    ],
                    "issn": "2504-4990",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-1327032",
                    "alternate_urls": [
                        "https://nbn-resolving.org/urn/resolver.pl?urn=urn:nbn:ch:bel-1327032",
                        "https://www.mdpi.com/journal/make"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ab5e9f33be33c6e4346cece73aea9b982fba2c53",
                "title": "Lottery Ticket Search on Untrained Models with Applied Lottery Sample Selection",
                "abstract": "In this paper, we present a new approach to improve tabular datasets by applying the lottery ticket hypothesis to tabular neural networks. Prior approaches were required to train the original large-sized model to find these lottery tickets. In this paper we eliminate the need to train the original model and discover lottery tickets using networks a fraction of the model\u2019s size. Moreover, we show that we can remove up to 95% of the training dataset to discover lottery tickets, while still maintaining similar accuracy. The approach uses a genetic algorithm (GA) to train candidate pruned models by encoding the nodes of the original model for selection measured by performance and weight metrics. We found that the search process does not require a large portion of the training data, but when the final pruned model is selected it can be retrained on the full dataset, even if it is often not required. We propose a lottery sample hypothesis similar to the lottery ticket hypotheses where a subsample of lottery samples of the training set can train a model with equivalent performance to the original dataset. We show that the combination of finding lottery samples alongside lottery tickets can allow for faster searches and greater accuracy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1417323622",
                        "name": "Ryan Bluteau"
                    },
                    {
                        "authorId": "35031080",
                        "name": "R. Gras"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "001c53724df07454a13e3a8db915707549d9e42d",
                "externalIds": {
                    "DBLP": "journals/tetc/JoardarDLCP23",
                    "DOI": "10.1109/TETC.2022.3223630",
                    "CorpusId": 256772512
                },
                "corpusId": 256772512,
                "publicationVenue": {
                    "id": "855d4173-dc30-48dc-96c2-42a5b6372940",
                    "name": "IEEE Transactions on Emerging Topics in Computing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Emerg Top Comput"
                    ],
                    "issn": "2168-6750",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6245516",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6245516"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/001c53724df07454a13e3a8db915707549d9e42d",
                "title": "ReaLPrune: ReRAM Crossbar-Aware Lottery Ticket Pruning for CNNs",
                "abstract": "Training machine learning (ML) models at the edge (on-chip training on end user devices) can address many pressing challenges including data privacy/security, increase the accessibility of ML applications to different parts of the world by reducing the dependence on the communication fabric and the cloud infrastructure, and meet the real-time requirements of AR/VR applications. However, existing edge platforms do not have sufficient computing capabilities to support complex ML tasks such as training large CNNs. ReRAM-based architectures offer high-performance yet energy efficient computing platforms for on-chip CNN training/inferencing. However, ReRAM-based architectures are not scalable with the size of the CNN. Larger CNNs have more weights, which requires more ReRAM cells that cannot be integrated in a single chip. Moreover, training larger CNNs on-chip will require higher power, which cannot be afforded by these smaller devices. Pruning is an effective way to solve this problem. However, existing pruning techniques are either targeted for inferencing only, or they are not crossbar-aware. This leads to sub-optimal hardware savings and performance benefits for CNN training on ReRAM-based architectures. In this paper, we address this problem by proposing a novel crossbar-aware pruning strategy, referred as ReaLPrune, which can prune more than 90% of CNN weights. The pruned model can be trained from scratch without any accuracy loss. Experimental results indicate that ReaLPrune reduces hardware requirements by 77.2% and accelerates CNN training by \u223c20\u00d7 compared to unpruned CNNs. ReaLPrune also outperforms other crossbar-aware pruning techniques in terms of both performance and hardware savings. In addition, ReaLPrune is equally effective for diverse datasets and more complex CNNs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "25173634",
                        "name": "Biresh Kumar Joardar"
                    },
                    {
                        "authorId": "2089333",
                        "name": "J. Doppa"
                    },
                    {
                        "authorId": "40348219",
                        "name": "Hai Helen Li"
                    },
                    {
                        "authorId": "143631962",
                        "name": "K. Chakrabarty"
                    },
                    {
                        "authorId": "1754491",
                        "name": "P. Pande"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Presented at the Sparsity Workshop at the 11 th International Conference on Learning Representations , Kigali, Rwanda, 2023.\ncost (Morcos et al., 2019) including memory consumption and inference time, and additionally enable wide-spread democratization of DNN\u2019s with a low carbon footprint.",
                "cost (Morcos et al., 2019) including memory consumption and inference time, and additionally enable wide-spread democratization of DNN\u2019s with a low carbon footprint."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f5075c3bc11587bb85c2b7afb33c7e490a40ba50",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-15953",
                    "ArXiv": "2303.15953",
                    "DOI": "10.48550/arXiv.2303.15953",
                    "CorpusId": 257771904
                },
                "corpusId": 257771904,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f5075c3bc11587bb85c2b7afb33c7e490a40ba50",
                "title": "Randomly Initialized Subnetworks with Iterative Weight Recycling",
                "abstract": "The Multi-Prize Lottery Ticket Hypothesis posits that randomly initialized neural networks contain several subnetworks that achieve comparable accuracy to fully trained models of the same architecture. However, current methods require that the network is sufficiently overparameterized. In this work, we propose a modification to two state-of-the-art algorithms (Edge-Popup and Biprop) that finds high-accuracy subnetworks with no additional storage cost or scaling. The algorithm, Iterative Weight Recycling, identifies subsets of important weights within a randomly initialized network for intra-layer reuse. Empirically we show improvements on smaller network architectures and higher prune rates, finding that model sparsity can be increased through the\"recycling\"of existing weights. In addition to Iterative Weight Recycling, we complement the Multi-Prize Lottery Ticket Hypothesis with a reciprocal finding: high-accuracy, randomly initialized subnetwork's produce diverse masks, despite being generated with the same hyperparameter's and pruning strategy. We explore the landscapes of these masks, which show high variability.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1512264892",
                        "name": "Matt Gorbett"
                    },
                    {
                        "authorId": "145285040",
                        "name": "L. D. Whitley"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "859e50ae31de320990d92f59b9cad48f68683649",
                "externalIds": {
                    "DBLP": "conf/ofc/TanimuraT23",
                    "DOI": "10.23919/OFC49934.2023.10117269",
                    "CorpusId": 258788860
                },
                "corpusId": 258788860,
                "publicationVenue": {
                    "id": "3cf29406-26d3-4e57-920e-379597d4bbcb",
                    "name": "Optical Fiber Communications Conference and Exhibition",
                    "type": "conference",
                    "alternate_names": [
                        "Opt Fiber Commun Conf Exhib",
                        "Opt Fiber Commun Conf",
                        "OFC",
                        "Optical Fiber Communication Conference"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/859e50ae31de320990d92f59b9cad48f68683649",
                "title": "Upgrade of Deep Neural Network-based Optical Monitors by Communication-Efficient Federated Learning",
                "abstract": "We present an efficient scheme to upgrade DNN-based optical monitors collaboratively trained through multiple network operators without revealing each confidential data, applying federated learning with pre-model size reduction based on transferable lottery ticket hypothesis.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145077197",
                        "name": "T. Tanimura"
                    },
                    {
                        "authorId": "49410325",
                        "name": "Masayuki Takase"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This includes the transferability of lottery tickets between different datasets [37, 38, 39, 40, 41, 42] and even between different models [43]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "de643458575961f99923953f6727809f74451b1b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-11244",
                    "ArXiv": "2302.11244",
                    "DOI": "10.48550/arXiv.2302.11244",
                    "CorpusId": 257079070
                },
                "corpusId": 257079070,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/de643458575961f99923953f6727809f74451b1b",
                "title": "Considering Layerwise Importance in the Lottery Ticket Hypothesis",
                "abstract": "The Lottery Ticket Hypothesis (LTH) showed that by iteratively training a model, removing connections with the lowest global weight magnitude and rewinding the remaining connections, sparse networks can be extracted. This global comparison removes context information between connections within a layer. Here we study means for recovering some of this layer distributional context and generalise the LTH to consider weight importance values rather than global weight magnitudes. We find that given a repeatable training procedure, applying different importance metrics leads to distinct performant lottery tickets with little overlapping connections. This strongly suggests that lottery tickets are not unique",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2064051623",
                        "name": "Benjamin Vandersmissen"
                    },
                    {
                        "authorId": "108666950",
                        "name": "Jos\u00e9 Oramas"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[110] illustrated the computational expense of determining winning tickets in the lottery ticket hypothesis."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e854ddc09f1dd7ce30d22dbbd542a60cb363d58f",
                "externalIds": {
                    "DBLP": "journals/csur/MishraG23",
                    "DOI": "10.1145/3570955",
                    "CorpusId": 253399078
                },
                "corpusId": 253399078,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e854ddc09f1dd7ce30d22dbbd542a60cb363d58f",
                "title": "Transforming Large-Size to Lightweight Deep Neural Networks for IoT Applications",
                "abstract": "Deep Neural Networks (DNNs) have gained unprecedented popularity due to their high-order performance and automated feature extraction capability. This has encouraged researchers to incorporate DNN in different Internet of Things (IoT) applications in recent years. However, the colossal requirement of computation, energy, and storage of DNNs make their deployment prohibitive on resource-constrained IoT devices. Therefore, several compression techniques have been proposed in recent years to reduce the energy, storage, and computation requirements of the DNN. These techniques have utilized a different perspective for compressing a DNN with minimal accuracy compromise. This encourages us to comprehensively overview DNN compression techniques for the IoT. This article presents a comprehensive overview of existing literature on compressing the DNN that reduces energy consumption, storage, and computation requirements for IoT applications. We divide the existing approaches into five broad categories\u2014network pruning, sparse representation, bits precision, knowledge distillation, and miscellaneous\u2014based upon the mechanism incorporated for compressing the DNN. The article discusses the challenges associated with each category of DNN compression techniques and presents some prominent applications using IoT in conjunction with a compressed DNN. Finally, we provide a quick summary of existing work under each category with the future direction in DNN compression.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1845888952",
                        "name": "Rahul Mishra"
                    },
                    {
                        "authorId": "15966255",
                        "name": "Hari Prabhat Gupta"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Finally, the resulting \u2299 is the winning ticket of the LTH by using the surrogate dataset.",
                "transferable to the target task on the basis of the observations in a previous study [7].",
                "The LTH [3, 7, 16] claims that randomly initialized overparametrized DNNs can contain certain sub-networks called winning tickets that can achieve comparable test accuracy of the original DNNs, even with smaller parameter size of the networks.",
                "The pre-training stage finds the winning ticket of the LTH through pruning the original model after training using the surrogate dataset.",
                "The server has a surrogate dataset to pre-train a model for finding the winning ticket of the LTH.",
                "We propose an FL method that initializes a model at the server by using the winning ticket of the LTH found with a surrogate dataset, as shown in Fig.",
                "Although realizations of the winning ticket depend on both the model and dataset used for the training, research showed that the winning tickets obtained from large datasets could be transferable to tasks on other datasets [7] in several cases.",
                "We proposed and evaluated a communication-efficient FL method by imposing the LTH-based initialization with a surrogate dataset before the FL process.",
                "The LTH claims that randomly initialized DNNs contain smaller \u201cgood\u201d sub-networks, called \u201cwinning tickets\u201d that can attain similar performance as the original DNNs even with their smaller model sizes.",
                "Li et al. proposed LotteryFL that integrates FL and the LTH [4].",
                "At the pre-training stage, a surrogate dataset is used to extract the sub-network of the original model, i.e., the winning ticket of the LTH, at the server.",
                "To address the issue, using sub-networks extracted from the original DNNs on the basis of the lottery ticket hypothesis (LTH) [3] has been proposed and investigated [4\u20136].",
                "from the winning ticket for the target task in general, findings on the winning tickets [7] suggest that the resulting ticket by a different dataset can be transferable to other tasks."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "6eb92ca40c25d88b6c5ec61b52e5a7abdc937683",
                "externalIds": {
                    "DBLP": "conf/ccnc/TanimuraT23",
                    "DOI": "10.1109/CCNC51644.2023.10060578",
                    "CorpusId": 257587635
                },
                "corpusId": 257587635,
                "publicationVenue": {
                    "id": "7cd7bf33-3c9a-4b3f-8b1a-177673c6ca73",
                    "name": "Consumer Communications and Networking Conference",
                    "type": "conference",
                    "alternate_names": [
                        "CCNC",
                        "Consum Commun Netw Conf"
                    ],
                    "url": "http://ieee-ccnc.org/"
                },
                "url": "https://www.semanticscholar.org/paper/6eb92ca40c25d88b6c5ec61b52e5a7abdc937683",
                "title": "Compressing Model before Federated Learning by Transferrable Surrogate Lottery Ticket",
                "abstract": "Federated learning (FL) is a promising technique to collaboratively train a model with distributed users and datasets. To develop communication-efficient FL systems, model-size reduction by using a winning ticket of the lottery ticket hypothesis has been proposed and investigated; however, it is still an issue how to discover winning tickets in FL systems, which incur communication costs for model training to find the winning ticket. To address this issue, we propose a method of using a surrogate dataset that can be synthetically generated at an FL server, instead of the distributed target datasets found at the clients, for finding the winning ticket. The method is based on observations that winning tickets obtained from a large dataset could be transferable to other tasks. The performance evaluations using a subset of the FEMNIST (as target) and Chars 74K (as emulated surrogate) datasets show that the proposed method reduces communication cost in FL by about 80%.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2211925863",
                        "name": "Takahito Tanimura"
                    },
                    {
                        "authorId": "49410325",
                        "name": "Masayuki Takase"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[22] elaborately discussed the generalizing and transferring capability of lottery tickets.",
                "Later [22], [39] improved the LTH idea.",
                "However, the transfer of weight initializations across datasets was first analyzed by [22].",
                "[22] observed an intriguing fact about LTH regarding the transferability of lottery tickets.",
                "[22]: Winning tickets are computationally expensive to generate because of the repetitive train-prune-rewind cycle.",
                "[21] and [22] have shown that random tickets with and without masking preserved results in lower performance than winning tickets with equal parameters.",
                "Later works like [22] demonstrate that these tickets generated from one dataset can even be transferred to another and achieve accuracies comparable to the original network.",
                "Pruning heuristics of our proposed method is motivated by previous work [21], [22].",
                "Such division is different from previous work done by [22], because they performed experiments by dividing Cifar-10 into parts where each part consisted of all ten classes.",
                "According to [22], both tickets from two separate partitions are transferable to the entire Cifar-100 dataset."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "bb2f59bcf38b7b0cfb5b73483ddb10d597c839de",
                "externalIds": {
                    "ArXiv": "2212.12770",
                    "DBLP": "journals/corr/abs-2212-12770",
                    "DOI": "10.48550/arXiv.2212.12770",
                    "CorpusId": 255125079
                },
                "corpusId": 255125079,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bb2f59bcf38b7b0cfb5b73483ddb10d597c839de",
                "title": "COLT: Cyclic Overlapping Lottery Tickets for Faster Pruning of Convolutional Neural Networks",
                "abstract": "\u2014Pruning refers to the elimination of trivial weights from neural networks. The sub-networks within an overparameterized model produced after pruning are often called Lottery tickets. This research aims to generate winning lottery tickets from a set of lottery tickets that can achieve similar accuracy to the original unpruned network. We introduce a novel winning ticket called Cyclic Overlapping Lottery Ticket (COLT) by data splitting and cyclic retraining of the pruned network from scratch. We apply a cyclic pruning algorithm that keeps only the overlapping weights of different pruned models trained on different data segments. Our results demonstrate that COLT can achieve similar accuracies (obtained by the unpruned model) while maintaining high sparsities. We show that the accuracy of COLT is on par with the winning tickets of Lottery Ticket Hypothesis (LTH) and, at times, is better. Moreover, COLTs can be generated using fewer iterations than tickets generated by the popular Iterative Magnitude Pruning (IMP) method. In addition, we also notice COLTs generated on large datasets can be transferred to small ones without compromising performance, demonstrating its generalizing capability. We conduct all our experiments on Cifar-10, Cifar-100 & TinyImageNet datasets and report superior performance than the state-of-the-art methods. Codes are available at: https: //github.com/ismail31416/COLT",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110864495",
                        "name": "Md. Ismail Hossain"
                    },
                    {
                        "authorId": "2075536921",
                        "name": "Mohammed Rakib"
                    },
                    {
                        "authorId": "32580323",
                        "name": "M. M. L. Elahi"
                    },
                    {
                        "authorId": "2146777033",
                        "name": "Nabeel Mohammed"
                    },
                    {
                        "authorId": "51245102",
                        "name": "Shafin Rahman"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "fd529dc803b31987f543a5d6de389e52315abaeb",
                "externalIds": {
                    "DBLP": "conf/bigdataconf/HarnYHZSSK22",
                    "DOI": "10.1109/BigData55660.2022.10020964",
                    "CorpusId": 256322331
                },
                "corpusId": 256322331,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fd529dc803b31987f543a5d6de389e52315abaeb",
                "title": "IGRP: Iterative Gradient Rank Pruning for Finding Graph Lottery Ticket",
                "abstract": "Graph Neural Networks (GNNs) have shown promising performance in many applications, yet remain extremely difficult to train over large-scale graph datasets. Existing weight pruning techniques can prune out the layer weights; however, they cannot fully address the high computation complexity of GNN inference, caused by large graph size and complicated node connections. In this paper, we propose an Iterative Gradient Rank Pruning (IGRP) algorithm to find graph lottery tickets (GLT) of GNNs where each GLT includes a pruned adjacency matrix and a sub-network. Our IGRP can avoid layer collapse and the winning ticket achieves Maximal critical compression. We evaluate the proposed method on small-scale (Cora and Citeseer), medium-scale (PubMed and Wiki-CS), and large-scale (Ogbn-ArXiv and Ogbn-Products) graph datasets. We demonstrate that both Single-shot and Multi-shot of IGRP outperform the state-of-the-art unified GNN sparsification (UGS) framework on node classification. The source code can be found in https://github.com/poweiharn/IGRP_GNN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7444800",
                        "name": "P. Harn"
                    },
                    {
                        "authorId": "2203118892",
                        "name": "Sai Deepthi Yeddula"
                    },
                    {
                        "authorId": "2064059011",
                        "name": "Bo Hui"
                    },
                    {
                        "authorId": "40539618",
                        "name": "J. Zhang"
                    },
                    {
                        "authorId": "2203154091",
                        "name": "Libo Sun"
                    },
                    {
                        "authorId": "1717206",
                        "name": "Min-Te Sun"
                    },
                    {
                        "authorId": "1758909",
                        "name": "Wei-Shinn Ku"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d7b38953d6a5774026cca2075e6a5650b8ed7c79",
                "externalIds": {
                    "ArXiv": "2211.13935",
                    "DBLP": "journals/corr/abs-2211-13935",
                    "DOI": "10.48550/arXiv.2211.13935",
                    "CorpusId": 254018109
                },
                "corpusId": 254018109,
                "publicationVenue": {
                    "id": "75d7a8c1-d871-42db-a8e4-7cf5146fdb62",
                    "name": "Social Science Research Network",
                    "type": "journal",
                    "alternate_names": [
                        "SSRN, Social Science Research Network (SSRN) home page",
                        "SSRN Electronic Journal",
                        "Soc Sci Res Netw",
                        "SSRN",
                        "SSRN Home Page",
                        "SSRN Electron J",
                        "Social Science Electronic Publishing presents Social Science Research Network"
                    ],
                    "issn": "1556-5068",
                    "url": "http://www.ssrn.com/",
                    "alternate_urls": [
                        "www.ssrn.com/",
                        "https://fatcat.wiki/container/tol7woxlqjeg5bmzadeg6qrg3e",
                        "https://www.wikidata.org/wiki/Q53949192",
                        "www.ssrn.com/en",
                        "http://www.ssrn.com/en/",
                        "http://umlib.nl/ssrn",
                        "umlib.nl/ssrn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d7b38953d6a5774026cca2075e6a5650b8ed7c79",
                "title": "LU decomposition and Toeplitz decomposition of a neural network",
                "abstract": "It is well-known that any matrix $A$ has an LU decomposition. Less well-known is the fact that it has a 'Toeplitz decomposition' $A = T_1 T_2 \\cdots T_r$ where $T_i$'s are Toeplitz matrices. We will prove that any continuous function $f : \\mathbb{R}^n \\to \\mathbb{R}^m$ has an approximation to arbitrary accuracy by a neural network that takes the form $L_1 \\sigma_1 U_1 \\sigma_2 L_2 \\sigma_3 U_2 \\cdots L_r \\sigma_{2r-1} U_r$, i.e., where the weight matrices alternate between lower and upper triangular matrices, $\\sigma_i(x) := \\sigma(x - b_i)$ for some bias vector $b_i$, and the activation $\\sigma$ may be chosen to be essentially any uniformly continuous nonpolynomial function. The same result also holds with Toeplitz matrices, i.e., $f \\approx T_1 \\sigma_1 T_2 \\sigma_2 \\cdots \\sigma_{r-1} T_r$ to arbitrary accuracy, and likewise for Hankel matrices. A consequence of our Toeplitz result is a fixed-width universal approximation theorem for convolutional neural networks, which so far have only arbitrary width versions. Since our results apply in particular to the case when $f$ is a general neural network, we may regard them as LU and Toeplitz decompositions of a neural network. The practical implication of our results is that one may vastly reduce the number of weight parameters in a neural network without sacrificing its power of universal approximation. We will present several experiments on real data sets to show that imposing such structures on the weight matrices sharply reduces the number of training parameters with almost no noticeable effect on test accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2116487311",
                        "name": "Yucong Liu"
                    },
                    {
                        "authorId": "1490484221",
                        "name": "Simiao Jiao"
                    },
                    {
                        "authorId": "1784453",
                        "name": "Lek-Heng Lim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent work has shown that LTP pruned models can generalize across a variety of datasets within an application domain as well as with different optimizers [12], [30].",
                "ity property [30]; 2) The transferred tickets act as a regularizer and prevents overfitting while training [30]; and 3) winning tickets learn generic inductive biases which improve training."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "32b407b06d0c32f238aa3e11733f8d291ecd5b68",
                "externalIds": {
                    "DBLP": "journals/tcad/OgboguAJDLCP22",
                    "DOI": "10.1109/TCAD.2022.3197342",
                    "CorpusId": 251481177
                },
                "corpusId": 251481177,
                "publicationVenue": {
                    "id": "e86c30b0-c1dd-4f0e-be5e-22af711f7d5f",
                    "name": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Comput Des Integr Circuit Syst"
                    ],
                    "issn": "0278-0070",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=43",
                    "alternate_urls": [
                        "http://ieee-ceda.org/publications/tcad",
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=43"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/32b407b06d0c32f238aa3e11733f8d291ecd5b68",
                "title": "Accelerating Large-Scale Graph Neural Network Training on Crossbar Diet",
                "abstract": "Resistive random-access memory (ReRAM)-based manycore architectures enable acceleration of graph neural network (GNN) inference and training. GNNs exhibit characteristics of both DNNs and graph analytics. Hence, GNN training/inferencing on ReRAM-based manycore architectures give rise to both computation and on-chip communication challenges. In this work, we leverage model pruning and efficient graph storage to reduce the computation and communication bottlenecks associated with GNN training on ReRAM-based manycore accelerators. However, traditional pruning techniques are either targeted for inferencing only, or they are not crossbar-aware. In this work, we propose a GNN pruning technique called DietGNN. DietGNN is a crossbar-aware pruning technique that achieves high accuracy training and enables energy, area, and storage efficient computing on ReRAM-based manycore platforms. The DietGNN pruned model can be trained from scratch without any noticeable accuracy loss. Our experimental results show that when mapped on to a ReRAM-based manycore architecture, DietGNN can reduce the number of crossbars by over 90% and accelerate GNN training by ${\\sim }{2}.{7}{\\times }$ compared to its unpruned counterpart. In addition, DietGNN reduces energy consumption by more than ${\\sim }{3}.{5}{\\times }$ compared to the unpruned counterpart.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2181168400",
                        "name": "Chukwufumnanya Ogbogu"
                    },
                    {
                        "authorId": "49212589",
                        "name": "Aqeeb Iqbal Arka"
                    },
                    {
                        "authorId": "25173634",
                        "name": "Biresh Kumar Joardar"
                    },
                    {
                        "authorId": "2089333",
                        "name": "J. Doppa"
                    },
                    {
                        "authorId": "40348219",
                        "name": "Hai Helen Li"
                    },
                    {
                        "authorId": "143631962",
                        "name": "K. Chakrabarty"
                    },
                    {
                        "authorId": "1754491",
                        "name": "P. Pande"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Empirical studies based on this statement show that NNs could potentially be significantly smaller without sacrificing accuracy (Chen et al., 2020; Frankle et al., 2019; Gale et al., 2019; Liu et al., 2018; Morcos et al., 2019; Zhou et al., 2019; Zhu and Gupta, 2017)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f2b9ca17a4e4a54753a8ac841efd0ece5f6c97f7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-16169",
                    "ArXiv": "2210.16169",
                    "DOI": "10.48550/arXiv.2210.16169",
                    "CorpusId": 253165051
                },
                "corpusId": 253165051,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f2b9ca17a4e4a54753a8ac841efd0ece5f6c97f7",
                "title": "LOFT: Finding Lottery Tickets through Filter-wise Training",
                "abstract": "Recent work on the Lottery Ticket Hypothesis (LTH) shows that there exist ``\\textit{winning tickets}'' in large neural networks. These tickets represent ``sparse'' versions of the full model that can be trained independently to achieve comparable accuracy with respect to the full model. However, finding the winning tickets requires one to \\emph{pretrain} the large model for at least a number of epochs, which can be a burdensome task, especially when the original neural network gets larger. In this paper, we explore how one can efficiently identify the emergence of such winning tickets, and use this observation to design efficient pretraining algorithms. For clarity of exposition, our focus is on convolutional neural networks (CNNs). To identify good filters, we propose a novel filter distance metric that well-represents the model convergence. As our theory dictates, our filter analysis behaves consistently with recent findings of neural network learning dynamics. Motivated by these observations, we present the \\emph{LOttery ticket through Filter-wise Training} algorithm, dubbed as \\textsc{LoFT}. \\textsc{LoFT} is a model-parallel pretraining algorithm that partitions convolutional layers by filters to train them independently in a distributed setting, resulting in reduced memory and communication costs during pretraining. Experiments show that \\textsc{LoFT} $i)$ preserves and finds good lottery tickets, while $ii)$ it achieves non-trivial computation and communication savings, and maintains comparable or even better accuracy than other pretraining methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2154497049",
                        "name": "Qihan Wang"
                    },
                    {
                        "authorId": "2064156092",
                        "name": "Chen Dun"
                    },
                    {
                        "authorId": "2144957237",
                        "name": "Fangshuo Liao"
                    },
                    {
                        "authorId": "1741680",
                        "name": "C. Jermaine"
                    },
                    {
                        "authorId": "2126894228",
                        "name": "Anastasios Kyrillidis"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "faef4d06af957830992d3a4189caf7e3b910be62",
                "externalIds": {
                    "DBLP": "journals/make/BluteauGIP22",
                    "DOI": "10.3390/make4040048",
                    "CorpusId": 253278902
                },
                "corpusId": 253278902,
                "publicationVenue": {
                    "id": "472fe64a-ea91-4506-8d2d-4c9a9374e1ea",
                    "name": "Machine Learning and Knowledge Extraction",
                    "alternate_names": [
                        "Mach Learn Knowl Extr"
                    ],
                    "issn": "2504-4990",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-1327032",
                    "alternate_urls": [
                        "https://nbn-resolving.org/urn/resolver.pl?urn=urn:nbn:ch:bel-1327032",
                        "https://www.mdpi.com/journal/make"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/faef4d06af957830992d3a4189caf7e3b910be62",
                "title": "Lottery Ticket Structured Node Pruning for Tabular Datasets",
                "abstract": "This paper experiments with well known pruning approaches, iterative and one-shot, and presents a new approach to lottery ticket pruning applied to tabular neural networks based on iterative pruning. Our contribution is a standard model for comparison in terms of speed and performance for tabular datasets that often do not get optimized through research. We show leading results in several tabular datasets that can compete with ensemble approaches. We tested on a wide range of datasets with a general improvement over the original (already leading) model in 6 of 8 datasets tested in terms of F1/RMSE. This includes a total reduction of over 85% of nodes with the additional ability to prune over 98% of nodes with minimal affect to accuracy. The new iterative approach we present will first optimize for lottery ticket quality by selecting an optimal architecture size and weights, then apply the iterative pruning strategy. The new iterative approach shows minimal degradation in accuracy compared to the original iterative approach, but it is capable of pruning models much smaller due to optimal weight pre-selection. Training and inference time improved over 50% and 10%, respectively, and up to 90% and 35%, respectively, for large datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1417323622",
                        "name": "Ryan Bluteau"
                    },
                    {
                        "authorId": "35031080",
                        "name": "R. Gras"
                    },
                    {
                        "authorId": "2189626897",
                        "name": "Zachary Innes"
                    },
                    {
                        "authorId": "2189626557",
                        "name": "Mitchel Paulin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such winning tickets have demonstrated their abilities to transfer across tasks and datasets (Morcos et al., 2019; Yu et al., 2020; Desai et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "40e49ba41eca31f9c2661cc65f2c13dc4f2c7859",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-04457",
                    "ACL": "2022.emnlp-main.758",
                    "ArXiv": "2210.04457",
                    "DOI": "10.48550/arXiv.2210.04457",
                    "CorpusId": 252780166
                },
                "corpusId": 252780166,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/40e49ba41eca31f9c2661cc65f2c13dc4f2c7859",
                "title": "XPrompt: Exploring the Extreme of Prompt Tuning",
                "abstract": "Prompt tuning learns soft prompts to condition the frozen Pre-trained Language Models (PLMs) for performing downstream tasks in a parameter-efficient manner. While prompt tuning has gradually reached the performance level of fine-tuning as the model scale increases, there is still a large performance gap between prompt tuning and fine-tuning for models of moderate and small scales (typically less than 11B parameters). In this paper, we empirically show that the trained prompt tokens can have a negative impact on a downstream task and thus degrade its performance. To bridge the gap, we propose a novel Prompt tuning model with an eXtremely small scale (XPrompt) under the regime of lottery tickets hypothesis. Specifically, XPrompt eliminates the negative prompt tokens at different granularity levels through a hierarchical structured pruning, yielding a more parameter-efficient prompt yet with a competitive performance. Comprehensive experiments are carried out on the SuperGLUE tasks, and the results indicate that XPrompt is able to close the performance gap at smaller model scales.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2163587600",
                        "name": "Fang Ma"
                    },
                    {
                        "authorId": "145107889",
                        "name": "Chen Zhang"
                    },
                    {
                        "authorId": "2152318755",
                        "name": "Lei Ren"
                    },
                    {
                        "authorId": "2109593338",
                        "name": "Jingang Wang"
                    },
                    {
                        "authorId": "2145778781",
                        "name": "Qifan Wang"
                    },
                    {
                        "authorId": "50224935",
                        "name": "Wei Yu Wu"
                    },
                    {
                        "authorId": "38472218",
                        "name": "Xiaojun Quan"
                    },
                    {
                        "authorId": "2151679983",
                        "name": "Dawei Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "They have also not included a comparison of Global MP to SOTA algorithms [12], [13], [14], [15], [16], [17].",
                "prior works have used Global MP as a baseline [12], [13],"
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b6408c7dc8ce8c386197990d90ccb528419db25b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-14624",
                    "ArXiv": "2209.14624",
                    "DOI": "10.48550/arXiv.2209.14624",
                    "CorpusId": 252595918
                },
                "corpusId": 252595918,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b6408c7dc8ce8c386197990d90ccb528419db25b",
                "title": "Is Complexity Required for Neural Network Pruning? A Case Study on Global Magnitude Pruning",
                "abstract": "Pruning neural networks has become popular in the last decade when it was shown that a large number of weights can be safely removed from modern neural networks without compromising accuracy. Numerous pruning methods have been proposed since then, each claiming to be better than the previous. Many state-of-the-art (SOTA) techniques today rely on complex pruning methodologies utilizing importance scores, getting feedback through back-propagation or having heuristics-based pruning rules amongst others. In this work, we question whether this pattern of introducing complexity is really necessary to achieve better pruning results. We benchmark these SOTA techniques against a naive pruning baseline, namely, Global Magnitude Pruning (Global MP). Global MP ranks weights in order of their magnitudes and prunes the smallest ones. Hence, in its vanilla form, it is one of the simplest pruning techniques. Surprisingly, we find that vanilla Global MP outperforms all the other SOTA techniques and achieves a new SOTA result. It also achieves promising performance on FLOPs sparsification, which we find is enhanced, when pruning is conducted in a gradual fashion. We also find that Global MP is generalizable across tasks, datasets, and models with superior performance. Moreover, a common issue that many pruning algorithms run into at high sparsity rates, namely, layer-collapse, can be easily fixed in Global MP by setting a minimum threshold of weights to be retained in each layer. Lastly, unlike many other SOTA techniques, Global MP does not require any additional algorithm specific hyper-parameters and is very straightforward to tune and implement. We showcase our findings on various models (WRN-28-8, ResNet-32, ResNet-50, MobileNet-V1 and FastGRNN) and multiple datasets (CIFAR-10, ImageNet and HAR-2). Code is available at https://github.com/manasgupta-1/GlobalMP.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109835363",
                        "name": "Manas Gupta"
                    },
                    {
                        "authorId": "7197691",
                        "name": "Efe Camci"
                    },
                    {
                        "authorId": "2186404613",
                        "name": "Vishandi Rudy Keneta"
                    },
                    {
                        "authorId": "2186403073",
                        "name": "Abhishek Vaidyanathan"
                    },
                    {
                        "authorId": "2186404868",
                        "name": "Ritwik Kanodia"
                    },
                    {
                        "authorId": "2121484",
                        "name": "Chuan-Sheng Foo"
                    },
                    {
                        "authorId": "2186403431",
                        "name": "Wu Min"
                    },
                    {
                        "authorId": "2056595772",
                        "name": "Lin Jie"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Since the lottery hypothesis was put forward, much work has been done on expansion[3,6,28] and theoretical research[25,29]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c70e95e78a513946ac3e85b86339bf3ec095be10",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-13378",
                    "ArXiv": "2209.13378",
                    "DOI": "10.48550/arXiv.2209.13378",
                    "CorpusId": 252545154
                },
                "corpusId": 252545154,
                "publicationVenue": {
                    "id": "a8f26d13-e373-4e48-b57b-ef89bf48f4db",
                    "name": "Asian Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Asian Conf Comput Vis",
                        "ACCV"
                    ],
                    "url": "http://www.cvl.iis.u-tokyo.ac.jp/afcv/"
                },
                "url": "https://www.semanticscholar.org/paper/c70e95e78a513946ac3e85b86339bf3ec095be10",
                "title": "Neural Network Panning: Screening the Optimal Sparse Network Before Training",
                "abstract": "Pruning on neural networks before training not only compresses the original models, but also accelerates the network training phase, which has substantial application value. The current work focuses on fine-grained pruning, which uses metrics to calculate weight scores for weight screening, and extends from the initial single-order pruning to iterative pruning. Through these works, we argue that network pruning can be summarized as an expressive force transfer process of weights, where the reserved weights will take on the expressive force from the removed ones for the purpose of maintaining the performance of original networks. In order to achieve optimal expressive force scheduling, we propose a pruning scheme before training called Neural Network Panning which guides expressive force transfer through multi-index and multi-process steps, and designs a kind of panning agent based on reinforcement learning to automate processes. Experimental results show that Panning performs better than various available pruning before training methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2179588435",
                        "name": "Xiatao Kang"
                    },
                    {
                        "authorId": "2420746",
                        "name": "P. Li"
                    },
                    {
                        "authorId": "2179641804",
                        "name": "Jiayi Yao"
                    },
                    {
                        "authorId": "153228216",
                        "name": "Chengxi Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This outstanding work sparks an interest in the deep learning community, and numerous related PaI researches have emerged [4, 24, 26, 36, 39, 41]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1f92a63cd5d2e1b1dbf335784372d6da9c5e3f46",
                "externalIds": {
                    "ArXiv": "2209.05683",
                    "DBLP": "conf/bmvc/YangWJQK22",
                    "DOI": "10.48550/arXiv.2209.05683",
                    "CorpusId": 252212168
                },
                "corpusId": 252212168,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/1f92a63cd5d2e1b1dbf335784372d6da9c5e3f46",
                "title": "One-shot Network Pruning at Initialization with Discriminative Image Patches",
                "abstract": "One-shot Network Pruning at Initialization (OPaI) is an effective method to decrease network pruning costs. Recently, there is a growing belief that data is unnecessary in OPaI. However, we obtain an opposite conclusion by ablation experiments in two representative OPaI methods, SNIP and GraSP. Specifically, we find that informative data is crucial to enhancing pruning performance. In this paper, we propose two novel methods, Discriminative One-shot Network Pruning (DOP) and Super Stitching, to prune the network by high-level visual discriminative image patches. Our contributions are as follows. (1) Extensive experiments reveal that OPaI is data-dependent. (2) Super Stitching performs significantly better than the original OPaI method on benchmark ImageNet, especially in a highly compressed model.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152747397",
                        "name": "Yinan Yang"
                    },
                    {
                        "authorId": "144602988",
                        "name": "Yi Ji"
                    },
                    {
                        "authorId": "40349048",
                        "name": "Yu Wang"
                    },
                    {
                        "authorId": "2072590761",
                        "name": "Heng Qi"
                    },
                    {
                        "authorId": "1718829",
                        "name": "Jien Kato"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Morcos et al. (2019) unveiled that the winning tickets discovered using larger datasets consistently transferred better than those generated using smaller datasets."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "173a41e7f0eebca7deadaa623d36f83ad93c9bf5",
                "externalIds": {
                    "DBLP": "conf/aaai/0006LFHMP23",
                    "ArXiv": "2208.10842",
                    "DOI": "10.48550/arXiv.2208.10842",
                    "CorpusId": 251741428
                },
                "corpusId": 251741428,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/173a41e7f0eebca7deadaa623d36f83ad93c9bf5",
                "title": "Lottery Pools: Winning More by Interpolating Tickets without Increasing Training or Inference Cost",
                "abstract": "Lottery tickets (LTs) is able to discover accurate and sparse subnetworks that could be trained in isolation to match the performance of dense networks. Ensemble, in parallel, is one of the oldest time-proven tricks in machine learning to improve performance by combining the output of multiple independent models. However, the benefits of ensemble in the context of LTs will be diluted since ensemble does not directly lead to stronger sparse subnetworks, but leverages their predictions for a better decision. In this work, we first observe that directly averaging the weights of the adjacent learned subnetworks significantly boosts the performance of LTs. Encouraged by this observation, we further propose an alternative way to perform an \"ensemble'' over the subnetworks identified by iterative magnitude pruning via a simple interpolating strategy. We call our method Lottery Pools. In contrast to the naive ensemble which brings no performance gains to each single subnetwork, Lottery Pools yields much stronger sparse subnetworks than the original LTs without requiring any extra training or inference cost. Across various modern architectures on CIFAR-10/100 and ImageNet, we show that our method achieves significant performance gains in both, in-distribution and out-of-distribution scenarios. Impressively, evaluated with VGG-16 and ResNet-18, the produced sparse subnetworks outperform the original LTs by up to 1.88% on CIFAR-100 and 2.36% on CIFAR-100-C; the resulting dense network surpasses the pre-trained dense-model up to \n 2.22% on CIFAR-100 and 2.38% on CIFAR-100-C. Our source code can be found at https://github.com/luuyin/Lottery-pools.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1410465360",
                        "name": "Lu Yin"
                    },
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2182421180",
                        "name": "Fang Meng"
                    },
                    {
                        "authorId": "8242939",
                        "name": "Tianjin Huang"
                    },
                    {
                        "authorId": "49917515",
                        "name": "V. Menkovski"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "723fe6b3d6b8489486cba3fad94efdd25bdba4ac",
                "externalIds": {
                    "DBLP": "journals/ijautcomp/WuWLYYDSL22",
                    "DOI": "10.1007/s11633-022-1340-5",
                    "CorpusId": 251707034
                },
                "corpusId": 251707034,
                "publicationVenue": {
                    "id": "1caabc5e-b06a-4ba8-bccd-8d3b71322232",
                    "name": "Machine Intelligence Research",
                    "alternate_names": [
                        "Mach Intell Res"
                    ],
                    "issn": "2731-538X"
                },
                "url": "https://www.semanticscholar.org/paper/723fe6b3d6b8489486cba3fad94efdd25bdba4ac",
                "title": "Efficient Visual Recognition: A Survey on Recent Advances and Brain-inspired Methodologies",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2038030133",
                        "name": "Yang Wu"
                    },
                    {
                        "authorId": "1452735766",
                        "name": "Dingheng Wang"
                    },
                    {
                        "authorId": "7829127",
                        "name": "Xiaotong Lu"
                    },
                    {
                        "authorId": "2158028718",
                        "name": "Fan Yang"
                    },
                    {
                        "authorId": "2144049133",
                        "name": "Man Yao"
                    },
                    {
                        "authorId": "50702526",
                        "name": "Weifeng Dong"
                    },
                    {
                        "authorId": "2182163977",
                        "name": "Jianbo Shi"
                    },
                    {
                        "authorId": "2190108823",
                        "name": "Guoqi Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such cost poses several challenges for the research community: the training of a network model is associated to large carbon footprints and the commercialization of of AI research (especially for edge devices) is hindered by the resource requirements of the models Strubell et al. [2019]. For several years now, many works in literature have shown that is possible to shrink both the size and resource requirements, mainly via quantization Yang et al.",
                "Such cost poses several challenges for the research community: the training of a network model is associated to large carbon footprints and the commercialization of of AI research (especially for edge devices) is hindered by the resource requirements of the models Strubell et al. [2019]. For several years now, many works in literature have shown that is possible to shrink both the size and resource requirements, mainly via quantization Yang et al. [2020], Jin et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "99f001eeb7528b19ba9a480f2fbd758921ceec9d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-09455",
                    "ArXiv": "2207.09455",
                    "DOI": "10.48550/arXiv.2207.09455",
                    "CorpusId": 250698914
                },
                "corpusId": 250698914,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/99f001eeb7528b19ba9a480f2fbd758921ceec9d",
                "title": "To update or not to update? Neurons at equilibrium in deep models",
                "abstract": "Recent advances in deep learning optimization showed that, with some a-posteriori information on fully-trained models, it is possible to match the same performance by simply training a subset of their parameters. Such a discovery has a broad impact from theory to applications, driving the research towards methods to identify the minimum subset of parameters to train without look-ahead information exploitation. However, the methods proposed do not match the state-of-the-art performance, and rely on unstructured sparsely connected models. In this work we shift our focus from the single parameters to the behavior of the whole neuron, exploiting the concept of neuronal equilibrium (NEq). When a neuron is in a configuration at equilibrium (meaning that it has learned a specific input-output relationship), we can halt its update; on the contrary, when a neuron is at non-equilibrium, we let its state evolve towards an equilibrium state, updating its parameters. The proposed approach has been tested on different state-of-the-art learning strategies and tasks, validating NEq and observing that the neuronal equilibrium depends on the specific learning setup.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "16248506",
                        "name": "Andrea Bragagnolo"
                    },
                    {
                        "authorId": "47376816",
                        "name": "Enzo Tartaglione"
                    },
                    {
                        "authorId": "1691141",
                        "name": "Marco Grangetto"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Pruning can be performed either before training a model Lee et al. (2018); Wang et al. (2020), early in training Rachwan et al. (2022); You et al. (2020), or after the model has been fully trained Frankle et al. (2020a,b); LeCun et al. (1990); Morcos et al. (2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "592b71ed7ee121e299455494a8523098f0bb1b12",
                "externalIds": {
                    "ArXiv": "2207.04227",
                    "DBLP": "journals/corr/abs-2207-04227",
                    "DOI": "10.48550/arXiv.2207.04227",
                    "CorpusId": 250426523
                },
                "corpusId": 250426523,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/592b71ed7ee121e299455494a8523098f0bb1b12",
                "title": "On the Robustness and Anomaly Detection of Sparse Neural Networks",
                "abstract": "The robustness and anomaly detection capability of neural networks are crucial topics for their safe adoption in the real-world. Moreover, the over-parameterization of recent networks comes with high computational costs and raises questions about its in\ufb02uence on robustness and anomaly detection. In this work, we show that sparsity can make networks more robust and better anomaly detectors. To motivate this even further, we show that a pre-trained neural network contains, within its parameter space, sparse subnetworks that are better at these tasks without any further training. We also show that structured sparsity greatly helps in reducing the complexity of expensive robustness and detection methods, while maintaining or even improving their results on these tasks. Finally, we introduce a new method, SensNorm, which uses the sensitivity of weights derived from an appropriate pruning method to detect anomalous samples in the input.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1580152221",
                        "name": "Morgane Ayle"
                    },
                    {
                        "authorId": "50997190",
                        "name": "Bertrand Charpentier"
                    },
                    {
                        "authorId": "2122080487",
                        "name": "John Rachwan"
                    },
                    {
                        "authorId": "73775589",
                        "name": "D. Zugner"
                    },
                    {
                        "authorId": "79462643",
                        "name": "Simon Geisler"
                    },
                    {
                        "authorId": "51249380",
                        "name": "Stephan Gunnemann"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A line of work [51,52,16,7] discover the existence of transferable winning tickets from source dataset and successfully transfer it to target dataset.",
                "To address this, a line of work [51,52,16,7] discover the existence of transferable winning tickets from source dataset and successfully transfer it to target dataset, thus eliminating search cost.",
                "Unfortunately, such techniques do not show comparable performance with the original IMP methods, thus mainstream LTH leverages IMP as a pruning scheme [25,83,52]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "910698a9f88e04ce63e1107342a2126d3f119b9b",
                "externalIds": {
                    "ArXiv": "2207.01382",
                    "DBLP": "journals/corr/abs-2207-01382",
                    "DOI": "10.48550/arXiv.2207.01382",
                    "CorpusId": 250264586
                },
                "corpusId": 250264586,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/910698a9f88e04ce63e1107342a2126d3f119b9b",
                "title": "Exploring Lottery Ticket Hypothesis in Spiking Neural Networks",
                "abstract": "Spiking Neural Networks (SNNs) have recently emerged as a new generation of low-power deep neural networks, which is suitable to be implemented on low-power mobile/edge devices. As such devices have limited memory storage, neural pruning on SNNs has been widely explored in recent years. Most existing SNN pruning works focus on shallow SNNs (2~6 layers), however, deeper SNNs (>16 layers) are proposed by state-of-the-art SNN works, which is difficult to be compatible with the current SNN pruning work. To scale up a pruning technique towards deep SNNs, we investigate Lottery Ticket Hypothesis (LTH) which states that dense networks contain smaller subnetworks (i.e., winning tickets) that achieve comparable performance to the dense networks. Our studies on LTH reveal that the winning tickets consistently exist in deep SNNs across various datasets and architectures, providing up to 97% sparsity without huge performance degradation. However, the iterative searching process of LTH brings a huge training computational cost when combined with the multiple timesteps of SNNs. To alleviate such heavy searching cost, we propose Early-Time (ET) ticket where we find the important weight connectivity from a smaller number of timesteps. The proposed ET ticket can be seamlessly combined with a common pruning techniques for finding winning tickets, such as Iterative Magnitude Pruning (IMP) and Early-Bird (EB) tickets. Our experiment results show that the proposed ET ticket reduces search time by up to 38% compared to IMP or EB methods. Code is available at Github.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1992946926",
                        "name": "Youngeun Kim"
                    },
                    {
                        "authorId": "1527101214",
                        "name": "Yuhang Li"
                    },
                    {
                        "authorId": "2116194543",
                        "name": "Hyoungseob Park"
                    },
                    {
                        "authorId": "2088932121",
                        "name": "Yeshwanth Venkatesha"
                    },
                    {
                        "authorId": "1820826857",
                        "name": "Ruokai Yin"
                    },
                    {
                        "authorId": "9352814",
                        "name": "P. Panda"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Most of the work investigating the use of sparse sparse training in RL are in the context of the lottery ticket hypothesis; Morcos et al. (2019) studied the existence of lucky sparse initializations using pruning and late-rewinding; Hasani et al. (2020) proposed an interesting approach by\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7c337018654121e85ac3159a5217d663a8222064",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-10369",
                    "ArXiv": "2206.10369",
                    "DOI": "10.48550/arXiv.2206.10369",
                    "CorpusId": 249890186
                },
                "corpusId": 249890186,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7c337018654121e85ac3159a5217d663a8222064",
                "title": "The State of Sparse Training in Deep Reinforcement Learning",
                "abstract": "The use of sparse neural networks has seen rapid growth in recent years, particularly in computer vision. Their appeal stems largely from the reduced number of parameters required to train and store, as well as in an increase in learning efficiency. Somewhat surprisingly, there have been very few efforts exploring their use in Deep Reinforcement Learning (DRL). In this work we perform a systematic investigation into applying a number of existing sparse training techniques on a variety of DRL agents and environments. Our results corroborate the findings from sparse training in the computer vision domain - sparse networks perform better than dense networks for the same parameter count - in the DRL domain. We provide detailed analyses on how the various components in DRL are affected by the use of sparse networks and conclude by suggesting promising avenues for improving the effectiveness of sparse training methods, as well as for advancing their use in DRL.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "30131402",
                        "name": "L. Graesser"
                    },
                    {
                        "authorId": "3399348",
                        "name": "Utku Evci"
                    },
                    {
                        "authorId": "152585800",
                        "name": "Erich Elsen"
                    },
                    {
                        "authorId": "39163115",
                        "name": "P. S. Castro"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "fead7c8911d4177a90b66193191bdf940443b526",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-06563",
                    "ArXiv": "2206.06563",
                    "DOI": "10.48550/arXiv.2206.06563",
                    "CorpusId": 249642387
                },
                "corpusId": 249642387,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fead7c8911d4177a90b66193191bdf940443b526",
                "title": "Zeroth-Order Topological Insights into Iterative Magnitude Pruning",
                "abstract": "Modern-day neural networks are famously large, yet also highly redundant and compressible; there exist numerous pruning strategies in the deep learning literature that yield over 90% sparser sub-networks of fully-trained, dense architectures while still maintaining their original accuracies. Amongst these many methods though -- thanks to its conceptual simplicity, ease of implementation, and efficacy -- Iterative Magnitude Pruning (IMP) dominates in practice and is the de facto baseline to beat in the pruning community. However, theoretical explanations as to why a simplistic method such as IMP works at all are few and limited. In this work, we leverage the notion of persistent homology to gain insights into the workings of IMP and show that it inherently encourages retention of those weights which preserve topological information in a trained network. Subsequently, we also provide bounds on how much different networks can be pruned while perfectly preserving their zeroth order topological features, and present a modified version of IMP to do the same.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1398245246",
                        "name": "Aishwarya H. Balwani"
                    },
                    {
                        "authorId": "82280359",
                        "name": "J. Krzyston"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(Chen et al., 2020b; Desai et al., 2019; Morcos et al., 2019; Mehta, 2019) investigate the transferability across different datasets (i.",
                "(Chen et al., 2020b; Desai et al., 2019; Morcos et al., 2019; Mehta, 2019) investigate the transferability across different datasets (i.e., dataset transfer), while other pioneers study the transferability of pre-trained tickets from supervised and self-supervised vision pre-training (Chen et al.,\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "637e093fb97863fc9bd4b1b69722bbe70804e4e0",
                "externalIds": {
                    "ArXiv": "2206.04762",
                    "DBLP": "journals/corr/abs-2206-04762",
                    "DOI": "10.48550/arXiv.2206.04762",
                    "CorpusId": 249605768
                },
                "corpusId": 249605768,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/637e093fb97863fc9bd4b1b69722bbe70804e4e0",
                "title": "Data-Efficient Double-Win Lottery Tickets from Robust Pre-training",
                "abstract": "Pre-training serves as a broadly adopted starting point for transfer learning on various downstream tasks. Recent investigations of lottery tickets hypothesis (LTH) demonstrate such enormous pre-trained models can be replaced by extremely sparse subnetworks (a.k.a. matching subnetworks) without sacrificing transferability. However, practical security-crucial applications usually pose more challenging requirements beyond standard transfer, which also demand these subnetworks to overcome adversarial vulnerability. In this paper, we formulate a more rigorous concept, Double-Win Lottery Tickets, in which a located subnetwork from a pre-trained model can be independently transferred on diverse downstream tasks, to reach BOTH the same standard and robust generalization, under BOTH standard and adversarial training regimes, as the full pre-trained model can do. We comprehensively examine various pre-training mechanisms and find that robust pre-training tends to craft sparser double-win lottery tickets with superior performance over the standard counterparts. For example, on downstream CIFAR-10/100 datasets, we identify double-win matching subnetworks with the standard, fast adversarial, and adversarial pre-training from ImageNet, at 89.26%/73.79%, 89.26%/79.03%, and 91.41%/83.22% sparsity, respectively. Furthermore, we observe the obtained double-win lottery tickets can be more data-efficient to transfer, under practical data-limited (e.g., 1% and 10%) downstream schemes. Our results show that the benefits from robust pre-training are amplified by the lottery ticket scheme, as well as the data-limited transfer setting. Codes are available at https://github.com/VITA-Group/Double-Win-LTH.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2109338656",
                        "name": "Zhenyu (Allen) Zhang"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "37873860",
                        "name": "Yang Zhang"
                    },
                    {
                        "authorId": "2122374354",
                        "name": "Shiyu Chang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In LTH, first a large network is trained and pruned to be a small subnetwork S, then retraining S using its original initialization yields comparable or even better performance, while retraining S with a different initialization performs much worse.",
                "Other empirical observations like lottery ticket hypothesis (LTH) (Frankle & Carbin, 2019; Morcos et al., 2019; Tian et al., 2019; Yu et al., 2020), recently also verified in CL (Chen et al.",
                "Other empirical observations like lottery ticket hypothesis (LTH) (Frankle & Carbin, 2019; Morcos et al., 2019; Tian et al., 2019; Yu et al., 2020), recently also verified in CL (Chen et al., 2021), may also be explained similarly.",
                "For LTH, our explanation is that S contains weights that are initialized luckily, i.e., close to useful local optima and converge to them during training."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f5db3b0a99e9ab7777b2fecf8b5d237715a3464d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-01342",
                    "ArXiv": "2206.01342",
                    "DOI": "10.48550/arXiv.2206.01342",
                    "CorpusId": 249375359
                },
                "corpusId": 249375359,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f5db3b0a99e9ab7777b2fecf8b5d237715a3464d",
                "title": "Understanding the Role of Nonlinearity in Training Dynamics of Contrastive Learning",
                "abstract": "While the empirical success of self-supervised learning (SSL) heavily relies on the usage of deep nonlinear models, existing theoretical works on SSL understanding still focus on linear ones. In this paper, we study the role of nonlinearity in the training dynamics of contrastive learning (CL) on one and two-layer nonlinear networks with homogeneous activation $h(x) = h'(x)x$. We have two major theoretical discoveries. First, the presence of nonlinearity can lead to many local optima even in 1-layer setting, each corresponding to certain patterns from the data distribution, while with linear activation, only one major pattern can be learned. This suggests that models with lots of parameters can be regarded as a \\emph{brute-force} way to find these local optima induced by nonlinearity. Second, in the 2-layer case, linear activation is proven not capable of learning specialized weights into diverse patterns, demonstrating the importance of nonlinearity. In addition, for 2-layer setting, we also discover \\emph{global modulation}: those local patterns discriminative from the perspective of global-level patterns are prioritized to learn, further characterizing the learning process. Simulation verifies our theoretical findings.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1932187449",
                        "name": "Yuandong Tian"
                    }
                ]
            }
        },
        {
            "contexts": [
                "LTH [17] inspired many to study the transferability of the \"winning ticket\", or sparse subnetwork architecture in general, across domains [45, 47, 51].",
                "[45] showed that winning ticket initializations generalized across a variety of natural image datasets, suggesting that different tasks seem to enjoy the same sparse subnetwork structure."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d6e8c9425e9a376a47e6ca30c60a66d1227b9c1b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-04662",
                    "ArXiv": "2206.04662",
                    "DOI": "10.1109/CVPR52688.2022.01206",
                    "CorpusId": 249538140
                },
                "corpusId": 249538140,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d6e8c9425e9a376a47e6ca30c60a66d1227b9c1b",
                "title": "DiSparse: Disentangled Sparsification for Multitask Model Compression",
                "abstract": "Despite the popularity of Model Compression and Mul-titask Learning, how to effectively compress a multitask model has been less thoroughly analyzed due to the chal-lenging entanglement of tasks in the parameter space. In this paper, we propose DiSparse, a simple, effective, and first-of-its-kind multitask pruning and sparse training scheme. We consider each task independently by disentangling the importance measurement and take the unani-mous decisions among all tasks when performing parame-ter pruning and selection. Our experimental results demon-strate superior performance on various configurations and settings compared to popular sparse training and pruning methods. Besides the effectiveness in compression, DiS-parse also provides a powerful tool to the multitask learning community. Surprisingly, we even observed better per-formance than some dedicated multitask learning methods in several cases despite the high model sparsity enforced by DiSparse. We analyzed the pruning masks generated with DiSparse and observed strikingly similar sparse net-work architecture identified by each task even before the training starts. We also observe the existence of a \u201cwater-shed\u201d layer where the task relatedness sharply drops, implying no benefits in continued parameters sharing. Our code and models will be available at: https://github.com/SHI-Labs/DiSparse-Multitask-Model-Compression.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143900241",
                        "name": "Xing Sun"
                    },
                    {
                        "authorId": "2855934",
                        "name": "Ali Hassani"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "143983679",
                        "name": "Gao Huang"
                    },
                    {
                        "authorId": "48667025",
                        "name": "Humphrey Shi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This phenomenon has been observed in multiple applications, including computer vision (Morcos et al., 2019; Frankle et al., 2020) and natural language processing (Gale et al.",
                "This phenomenon has been observed in multiple applications, including computer vision (Morcos et al., 2019; Frankle et al., 2020) and natural language processing (Gale et al., 2019; Yu et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d66dcad5d9f0564a96c035e0f6c64fe046738885",
                "externalIds": {
                    "DBLP": "conf/emnlp/ForoutanBLBA22",
                    "ArXiv": "2205.12672",
                    "ACL": "2022.emnlp-main.513",
                    "DOI": "10.48550/arXiv.2205.12672",
                    "CorpusId": 249062609
                },
                "corpusId": 249062609,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/d66dcad5d9f0564a96c035e0f6c64fe046738885",
                "title": "Discovering Language-neutral Sub-networks in Multilingual Language Models",
                "abstract": "Multilingual pre-trained language models transfer remarkably well on cross-lingual downstream tasks. However, the extent to which they learn language-neutral representations (i.e., shared representations that encode similar phenomena across languages), and the effect of such representations on cross-lingual transfer performance, remain open questions.In this work, we conceptualize language neutrality of multilingual models as a function of the overlap between language-encoding sub-networks of these models. We employ the lottery ticket hypothesis to discover sub-networks that are individually optimized for various languages and tasks. Our evaluation across three distinct tasks and eleven typologically-diverse languages demonstrates that sub-networks for different languages are topologically similar (i.e., language-neutral), making them effective initializations for cross-lingual transfer with limited performance degradation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9737058",
                        "name": "Negar Foroutan"
                    },
                    {
                        "authorId": "1739201368",
                        "name": "M. Banaei"
                    },
                    {
                        "authorId": "2875254",
                        "name": "R. Lebret"
                    },
                    {
                        "authorId": "2691021",
                        "name": "Antoine Bosselut"
                    },
                    {
                        "authorId": "1751802",
                        "name": "K. Aberer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[144] |\u0398 i | \u2713 \u2717 \u2717 Datasets/Optimizers \u2717 Zhang et al.",
                "resetting the weights to values reached early in the first, dense training [120, 144, 149].",
                "Costs for finding LTs via iterative magnitude pruning can be reduced by using early stopping and low precision training for each pre-training iteration [146], sharing LTs for different datasets and optimizers [144] or iteratively reducing the dataset together with the number of non-zero parameters [145]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "de3ccbae89aa1462b207888f232ba82b8398f6e7",
                "externalIds": {
                    "ArXiv": "2205.08099",
                    "DBLP": "journals/corr/abs-2205-08099",
                    "DOI": "10.1007/s10462-023-10489-1",
                    "CorpusId": 248834207
                },
                "corpusId": 248834207,
                "publicationVenue": {
                    "id": "ea8553fe-2467-4367-afee-c4deb3754820",
                    "name": "Artificial Intelligence Review",
                    "type": "journal",
                    "alternate_names": [
                        "Artif Intell Rev"
                    ],
                    "issn": "0269-2821",
                    "url": "https://link.springer.com/journal/10462"
                },
                "url": "https://www.semanticscholar.org/paper/de3ccbae89aa1462b207888f232ba82b8398f6e7",
                "title": "Dimensionality reduced training by pruning and freezing parts of a deep neural network: a survey",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2029488873",
                        "name": "Paul Wimmer"
                    },
                    {
                        "authorId": "144442281",
                        "name": "Jens Mehnert"
                    },
                    {
                        "authorId": "2063161",
                        "name": "A. Condurache"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To overcome 143 this problem, Morcos et al. (2019) proposed to 144 transfer the WT structure from source tasks to re- 145 lated tasks in the computer vision (CV) field."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3ce1d29cd78310d0f47aab3eb98497fefce1e96a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-11218",
                    "ArXiv": "2204.11218",
                    "ACL": "2022.naacl-main.428",
                    "DOI": "10.48550/arXiv.2204.11218",
                    "CorpusId": 248377421
                },
                "corpusId": 248377421,
                "publicationVenue": {
                    "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
                    "name": "North American Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "North Am Chapter Assoc Comput Linguistics",
                        "NAACL"
                    ],
                    "url": "https://www.aclweb.org/portal/naacl"
                },
                "url": "https://www.semanticscholar.org/paper/3ce1d29cd78310d0f47aab3eb98497fefce1e96a",
                "title": "Learning to Win Lottery Tickets in BERT Transfer via Task-agnostic Mask Training",
                "abstract": "Recent studies on the lottery ticket hypothesis (LTH) show that pre-trained language models (PLMs) like BERT contain matching subnetworks that have similar transfer learning performance as the original PLM. These subnetworks are found using magnitude-based pruning. In this paper, we find that the BERT subnetworks have even more potential than these studies have shown. Firstly, we discover that the success of magnitude pruning can be attributed to the preserved pre-training performance, which correlates with the downstream transferability. Inspired by this, we propose to directly optimize the subnetwork structure towards the pre-training objectives, which can better preserve the pre-training performance. Specifically, we train binary masks over model weights on the pre-training tasks, with the aim of preserving the universal transferability of the subnetwork, which is agnostic to any specific downstream tasks. We then fine-tune the subnetworks on the GLUE benchmark and the SQuAD dataset. The results show that, compared with magnitude pruning, mask training can effectively find BERT subnetworks with improved overall performance on downstream tasks. Moreover, our method is also more efficient in searching subnetworks and more advantageous when fine-tuning within a certain range of data scarcity. Our code is available at https://github.com/llyx97/TAMT.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7896029",
                        "name": "Yuanxin Liu"
                    },
                    {
                        "authorId": "33427918",
                        "name": "Fandong Meng"
                    },
                    {
                        "authorId": "1390641501",
                        "name": "Zheng Lin"
                    },
                    {
                        "authorId": "143655088",
                        "name": "Peng Fu"
                    },
                    {
                        "authorId": "9310727",
                        "name": "Yanan Cao"
                    },
                    {
                        "authorId": "2154491752",
                        "name": "Weiping Wang"
                    },
                    {
                        "authorId": "48128428",
                        "name": "Jie Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[75] proposed to use one generalized lottery tickets for all vision benchmarks and got comparable results with the specialized lottery tickets; Frankle et al."
            ],
            "isInfluential": false,
            "intents": [
                "result"
            ],
            "citingPaper": {
                "paperId": "8326dba15f6b8ee6e43c23eea3265a05e59e8135",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-00595",
                    "ArXiv": "2204.00595",
                    "DOI": "10.48550/arXiv.2204.00595",
                    "CorpusId": 247922732
                },
                "corpusId": 247922732,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8326dba15f6b8ee6e43c23eea3265a05e59e8135",
                "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training",
                "abstract": "Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute or memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency--quality tradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable algorithms to approximate a given dense weight matrix. To address these issues, we propose a class of matrices (Monarch) that is hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms). Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution. These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2x with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique called\"reverse sparsification,\"Monarch matrices serve as a useful intermediate representation to speed up GPT-2 pretraining on OpenWebText by 2x without quality drop. The same technique brings 23% faster BERT pretraining than even the very optimized implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds up BERT fine-tuning on GLUE by 1.7x with comparable accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "24593911",
                        "name": "Tri Dao"
                    },
                    {
                        "authorId": "4319427",
                        "name": "Beidi Chen"
                    },
                    {
                        "authorId": "145193121",
                        "name": "N. Sohoni"
                    },
                    {
                        "authorId": "150898486",
                        "name": "Arjun D Desai"
                    },
                    {
                        "authorId": "40585370",
                        "name": "Michael Poli"
                    },
                    {
                        "authorId": "2161243147",
                        "name": "Jessica Grogan"
                    },
                    {
                        "authorId": "2161308739",
                        "name": "Alexander Liu"
                    },
                    {
                        "authorId": null,
                        "name": "Aniruddh Rao"
                    },
                    {
                        "authorId": "1755572",
                        "name": "A. Rudra"
                    },
                    {
                        "authorId": "1803218",
                        "name": "Christopher R\u00e9"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other ways of reducing the computational cost include finding a good subnetwork and then fine-tuning (Sreenivasan et al., 2022b), and transferring lottery tickets (Morcos et al., 2019; Chen et al., 2021c).",
                ", 2022b), and transferring lottery tickets (Morcos et al., 2019; Chen et al., 2021c)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0dd693206e94be5d10305d4b9e73ab68024a499a",
                "externalIds": {
                    "DBLP": "conf/aistats/YangW23",
                    "ArXiv": "2203.14328",
                    "CorpusId": 257631949
                },
                "corpusId": 257631949,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0dd693206e94be5d10305d4b9e73ab68024a499a",
                "title": "On the Neural Tangent Kernel Analysis of Randomly Pruned Neural Networks",
                "abstract": "Motivated by both theory and practice, we study how random pruning of the weights affects a neural network's neural tangent kernel (NTK). In particular, this work establishes an equivalence of the NTKs between a fully-connected neural network and its randomly pruned version. The equivalence is established under two cases. The first main result studies the infinite-width asymptotic. It is shown that given a pruning probability, for fully-connected neural networks with the weights randomly pruned at the initialization, as the width of each layer grows to infinity sequentially, the NTK of the pruned neural network converges to the limiting NTK of the original network with some extra scaling. If the network weights are rescaled appropriately after pruning, this extra scaling can be removed. The second main result considers the finite-width case. It is shown that to ensure the NTK's closeness to the limit, the dependence of width on the sparsity parameter is asymptotically linear, as the NTK's gap to its limit goes down to zero. Moreover, if the pruning probability is set to zero (i.e., no pruning), the bound on the required width matches the bound for fully-connected neural networks in previous works up to logarithmic factors. The proof of this result requires developing a novel analysis of a network structure which we called \\textit{mask-induced pseudo-networks}. Experiments are provided to evaluate our results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118571035",
                        "name": "Hongru Yang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "21fadcd3c13d122ac87c367faea882ff023cac31",
                "externalIds": {
                    "DBLP": "conf/icip/Tartaglione22",
                    "ArXiv": "2202.12400",
                    "DOI": "10.1109/ICIP46576.2022.9897223",
                    "CorpusId": 247154981
                },
                "corpusId": 247154981,
                "publicationVenue": {
                    "id": "b6369c33-5d70-463c-8e82-95a54efa3cc8",
                    "name": "International Conference on Information Photonics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Image Process",
                        "ICIP",
                        "Int Conf Inf Photonics",
                        "International Conference on Image Processing"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/21fadcd3c13d122ac87c367faea882ff023cac31",
                "title": "The Rise of the Lottery Heroes: Why Zero-Shot Pruning is Hard",
                "abstract": "Recent advances in deep learning optimization showed that just a subset of parameters are really necessary to successfully train a model. Potentially, such a discovery has broad impact from the theory to application; however, it is known that finding these trainable sub-network is a typically costly process. This inhibits practical applications: can the learned sub-graph structures in deep learning models be found at training time\u0192 In this work we explore such a possibility, observing and motivating why common approaches typically fail in the extreme scenarios of interest, and proposing an approach which potentially enables training with reduced computational effort. The experiments on either challenging architectures and datasets suggest the algorithmic accessibility over such a computational gain, and in particular a trade-off between accuracy achieved and training complexity deployed emerges.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47376816",
                        "name": "Enzo Tartaglione"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The transferability of winning tickets has been investigated in [23].",
                "[23] Ari Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "5f2fe6889ac91655ea49d941db6694dcdbb849bb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-11484",
                    "ArXiv": "2202.11484",
                    "CorpusId": 247058456
                },
                "corpusId": 247058456,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5f2fe6889ac91655ea49d941db6694dcdbb849bb",
                "title": "Reconstruction Task Finds Universal Winning Tickets",
                "abstract": "Pruning well-trained neural networks is effective to achieve a promising accuracy-efficiency trade-off in computer vision regimes. However, most of existing pruning algorithms only focus on the classification task defined on the source domain. Different from the strong transferability of the original model, a pruned network is hard to transfer to complicated downstream tasks such as object detection arXiv:arch-ive/2012.04643. In this paper, we show that the image-level pretrain task is not capable of pruning models for diverse downstream tasks. To mitigate this problem, we introduce image reconstruction, a pixel-level task, into the traditional pruning framework. Concretely, an autoencoder is trained based on the original model, and then the pruning process is optimized with both autoencoder and classification losses. The empirical study on benchmark downstream tasks shows that the proposed method can outperform state-of-the-art results explicitly.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Ruichen Li"
                    },
                    {
                        "authorId": "2145728702",
                        "name": "Binghui Li"
                    },
                    {
                        "authorId": "2066197601",
                        "name": "Qi Qian"
                    },
                    {
                        "authorId": "39060743",
                        "name": "Liwei Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "52e8102e070dbed745c39fd518f4f6aa3daffb3c",
                "externalIds": {
                    "DBLP": "conf/iclr/LiangJZH0GCZ22",
                    "ArXiv": "2202.02664",
                    "CorpusId": 246634100
                },
                "corpusId": 246634100,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/52e8102e070dbed745c39fd518f4f6aa3daffb3c",
                "title": "No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models",
                "abstract": "Recent research has shown the existence of significant redundancy in large Transformer models. One can prune the redundant parameters without significantly sacrificing the generalization performance. However, we question whether the redundant parameters could have contributed more if they were properly trained. To answer this question, we propose a novel training strategy that encourages all parameters to be trained sufficiently. Specifically, we adaptively adjust the learning rate for each parameter according to its sensitivity, a robust gradient-based measure reflecting this parameter's contribution to the model performance. A parameter with low sensitivity is redundant, and we improve its fitting by increasing its learning rate. In contrast, a parameter with high sensitivity is well-trained, and we regularize it by decreasing its learning rate to prevent further overfitting. We conduct extensive experiments on natural language understanding, neural machine translation, and image classification to demonstrate the effectiveness of the proposed schedule. Analysis shows that the proposed schedule indeed reduces the redundancy and improves generalization performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "98703980",
                        "name": "Chen Liang"
                    },
                    {
                        "authorId": "2152630772",
                        "name": "Haoming Jiang"
                    },
                    {
                        "authorId": "52194893",
                        "name": "Simiao Zuo"
                    },
                    {
                        "authorId": "50462546",
                        "name": "Pengcheng He"
                    },
                    {
                        "authorId": "46522098",
                        "name": "Xiaodong Liu"
                    },
                    {
                        "authorId": "48441311",
                        "name": "Jianfeng Gao"
                    },
                    {
                        "authorId": "2109136147",
                        "name": "Weizhu Chen"
                    },
                    {
                        "authorId": "36345161",
                        "name": "T. Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In [8, 30] a \u201dlottery ticket hypothesis\u201d was proposed that with an optimal substructure of the neural network acquired by weights pruning directly train a pruned model could reach similar results as pruning a pre-trained network.",
                "[30] Ari S Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian."
            ],
            "isInfluential": false,
            "intents": [
                "result",
                "background"
            ],
            "citingPaper": {
                "paperId": "1b12c780263864d44f86e708c4f511890b5f031b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-11803",
                    "ArXiv": "2201.11803",
                    "CorpusId": 246411685
                },
                "corpusId": 246411685,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1b12c780263864d44f86e708c4f511890b5f031b",
                "title": "On the Convergence of Heterogeneous Federated Learning with Arbitrary Adaptive Online Model Pruning",
                "abstract": "One of the biggest challenges in Federated Learning (FL) is that client devices often have drastically different computation and communication resources for local updates. To this end, recent research efforts have focused on training heterogeneous local models obtained by pruning a shared global model. Despite empirical success, theoretical guarantees on convergence remain an open question. In this paper, we present a unifying framework for heterogeneous FL algorithms with {\\em arbitrary} adaptive online model pruning and provide a general convergence analysis. In particular, we prove that under certain sufficient conditions and on both IID and non-IID data, these algorithms converges to a stationary point of standard FL for general smooth cost functions, with a convergence rate of $O(\\frac{1}{\\sqrt{Q}})$. Moreover, we illuminate two key factors impacting convergence: pruning-induced noise and minimum coverage index, advocating a joint design of local pruning masks for efficient training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2007579020",
                        "name": "Hanhan Zhou"
                    },
                    {
                        "authorId": "143928529",
                        "name": "Tian Lan"
                    },
                    {
                        "authorId": "2836326",
                        "name": "Guru Venkataramani"
                    },
                    {
                        "authorId": "2152130976",
                        "name": "Wenbo Ding"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "90b21dbad8969b74d704eed15a3d98722a88e464",
                "externalIds": {
                    "DBLP": "conf/iclr/ChenDLY0RR22",
                    "ArXiv": "2112.00029",
                    "CorpusId": 244773609
                },
                "corpusId": 244773609,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/90b21dbad8969b74d704eed15a3d98722a88e464",
                "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models",
                "abstract": "Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.5x faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "4319427",
                        "name": "Beidi Chen"
                    },
                    {
                        "authorId": "24593911",
                        "name": "Tri Dao"
                    },
                    {
                        "authorId": "102461072",
                        "name": "Kaizhao Liang"
                    },
                    {
                        "authorId": "2142772202",
                        "name": "Jiaming Yang"
                    },
                    {
                        "authorId": "2119235975",
                        "name": "Zhao Song"
                    },
                    {
                        "authorId": "1755572",
                        "name": "A. Rudra"
                    },
                    {
                        "authorId": "2114485554",
                        "name": "C. R\u00e9"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To evaluate our experiments, we perform three popular image classification tasks including MNIST handwritten digit recognition [6], CIFAR-10 image classification [26], and Fashion-MNIST fashion image classification [42]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "f4c361708ce3d0d3c068dd2a750b2dd8ac175af0",
                "externalIds": {
                    "DBLP": "journals/tdasci/DingCZJJ21",
                    "DOI": "10.1145/3491254",
                    "CorpusId": 246562914
                },
                "corpusId": 246562914,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f4c361708ce3d0d3c068dd2a750b2dd8ac175af0",
                "title": "Differentially Private Deep Learning with Iterative Gradient Descent Optimization",
                "abstract": "Deep learning has achieved great success in various areas and its success is closely linked to the availability of massive data. But in general, a large dataset could include sensitive data and therefore the model should have the capability to avoid privacy leakage. To achieve this aim, many works apply the famous privacy framework named differential privacy into deep learning to preserve privacy. In this article, we propose a novel perturbed iterative gradient descent optimization (PIGDO) algorithm and prove that this algorithm satisfies the differential privacy. Besides, we propose a modified moments accountant (MMA) method to conduct the privacy analysis and obtain a tighter bound of privacy loss compared with the original moments accountant method. A number of experiments demonstrate that our optimization algorithm can not only improve the model accuracy and training speed, but also achieve better privacy guarantees over the state-of-the-art algorithm while reaching the equivalent accuracy. We provide codes for all of our experiments in https://github.com/CGCL-codes/DPDLIGDO.git.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143681247",
                        "name": "Xiaofeng Ding"
                    },
                    {
                        "authorId": "2145131857",
                        "name": "Lin Chen"
                    },
                    {
                        "authorId": "33481412",
                        "name": "Pan Zhou"
                    },
                    {
                        "authorId": "2868159",
                        "name": "Wenbin Jiang"
                    },
                    {
                        "authorId": "2152883080",
                        "name": "Hai Jin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This view also provides an explanation of the empirically observed phenomenon that universal tickets achieve good performance across a number of tasks only at moderate sparsity levels and become more universal when trained on larger datasets (Morcos et al., 2019; Chen et al., 2020).",
                "The significant computational cost associated with finding good lottery tickets has motivated the quest for universal tickets that can be transferred to different tasks (Morcos et al., 2019; Chen et al., 2020).",
                "They propose an initialization scheme that extends standard approaches like He (He et al., 2015) or Glorot (Glorot & Bengio, 2010) initialization to non-zero biases and supports the existence of LTs while keeping the large mother network f0 trainable.",
                "The independence of the functions is not required and could be replaced by dictionaries, but the independence aids the compression of the bottom layers and thus our objective to find sparse LTs.",
                "Even though the function families above can be represented efficiently as LTs, we should mention that a big advantage of neural networks is that the actual function family can be learned.",
                "Before we can prove the existence of strong universal LTs, we have to formalize our notion of what makes a strong LT universal.",
                "To make meaningful progress in deriving a notion of universal LTs, we therefore need a stronger simplification.",
                "Independently from LTs, we discuss conditions when this is a promising approach, i.e., when the bottom layers of the deep neural network represent multivariate (basis) functions, whose linear combination can represent a large class of multivariate functions.",
                "Theorem 3 (Multivariate LTs (single layer)).",
                "In the following, we discuss the existence of polynomial LTs in more detail.",
                "Next, we can utilize our improved LT pruning results to prove the existence of universal LTs.",
                "However, Morcos et al. (2019) posited the existence of so-called universal lottery tickets that, once identified, can be effectively reused across a variety of settings.",
                "It is therefore reasonable to transfer it to LTs (Morcos et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "95b241948a67518cc83ad864ea96abaf9a473881",
                "externalIds": {
                    "ArXiv": "2111.11146",
                    "DBLP": "conf/iclr/BurkholzLMG22",
                    "CorpusId": 244478211
                },
                "corpusId": 244478211,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/95b241948a67518cc83ad864ea96abaf9a473881",
                "title": "On the Existence of Universal Lottery Tickets",
                "abstract": "The lottery ticket hypothesis conjectures the existence of sparse subnetworks of large randomly initialized deep neural networks that can be successfully trained in isolation. Recent work has experimentally observed that some of these tickets can be practically reused across a variety of tasks, hinting at some form of universality. We formalize this concept and theoretically prove that not only do such universal tickets exist but they also do not require further training. Our proofs introduce a couple of technical innovations related to pruning for strong lottery tickets, including extensions of subset sum results and a strategy to leverage higher amounts of depth. Our explicit sparse constructions of universal function families might be of independent interest, as they highlight representational benefits induced by univariate convolutional architectures.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "8813467",
                        "name": "R. Burkholz"
                    },
                    {
                        "authorId": "90226108",
                        "name": "Nilanjana Laha"
                    },
                    {
                        "authorId": "152113419",
                        "name": "R. Mukherjee"
                    },
                    {
                        "authorId": "1709051",
                        "name": "Alkis Gotovos"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "371e56674c336a621f12127c1769c9b1efd418db",
                "externalIds": {
                    "ArXiv": "2111.09272",
                    "DBLP": "journals/corr/abs-2111-09272",
                    "CorpusId": 244270051
                },
                "corpusId": 244270051,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/371e56674c336a621f12127c1769c9b1efd418db",
                "title": "ReaLPrune: ReRAM Crossbar-aware Lottery Ticket Pruned CNNs",
                "abstract": "\u2014Training machine learning (ML) models at the edge (on-chip training on end user devices) can address many pressing challenges including data privacy/security, increase the accessibility of ML applications to different parts of the world by reducing the dependence on the communication fabric and the cloud infrastructure, and meet the real-time requirements of AR/VR applications. However, existing edge platforms do not have sufficient computing capabilities to support complex ML tasks such as training large CNNs. ReRAM-based architectures offer high-performance yet energy efficient computing platforms for on-chip CNN training/inferencing. However, ReRAM-based architectures are not scalable with the size of the CNN. Larger CNNs have more weights, which requires more ReRAM cells that cannot be integrated in a single chip. Moreover, training larger CNNs on-chip will require higher power, which cannot be afforded by these smaller devices. Pruning is an effective way to solve this problem. However, existing pruning techniques are either targeted for inferencing only, or they are not crossbar-aware. This leads to sub-optimal hardware savings and performance benefits for CNN training on ReRAM-based architectures. In this paper, we address this problem by proposing a novel crossbar-aware pruning strategy, referred as ReaLPrune, which can prune more than 90% of CNN weights. The pruned model can be trained from scratch without any accuracy loss. Experimental results indicate that ReaLPrune reduces hardware requirements by 77.2% and accelerates CNN training by ~20 \u00d7 compared to unpruned CNNs. ReaLPrune also outperforms other crossbar-aware pruning techniques in terms of both performance and hardware savings. In addition, ReaLPrune is equally effective for diverse datasets and more complex CNNs.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "25173634",
                        "name": "Biresh Kumar Joardar"
                    },
                    {
                        "authorId": "2089333",
                        "name": "J. Doppa"
                    },
                    {
                        "authorId": "2115210516",
                        "name": "Hai Li"
                    },
                    {
                        "authorId": "143631962",
                        "name": "K. Chakrabarty"
                    },
                    {
                        "authorId": "1754491",
                        "name": "P. Pande"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[28], which shows that the winning ticket discovered from a dataset can be transferred to a highly relevant dataset."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ddee218577bea06d4ad0e0a2070e91f0e4768906",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-14678",
                    "ArXiv": "2110.14678",
                    "CorpusId": 240070452
                },
                "corpusId": 240070452,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ddee218577bea06d4ad0e0a2070e91f0e4768906",
                "title": "Meta-Learning Sparse Implicit Neural Representations",
                "abstract": "Implicit neural representations are a promising new avenue of representing general signals by learning a continuous function that, parameterized as a neural network, maps the domain of a signal to its codomain; the mapping from spatial coordinates of an image to its pixel values, for example. Being capable of conveying fine details in a high dimensional signal, unboundedly of its domain, implicit neural representations ensure many advantages over conventional discrete representations. However, the current approach is difficult to scale for a large number of signals or a data set, since learning a neural representation -- which is parameter heavy by itself -- for each signal individually requires a lot of memory and computations. To address this issue, we propose to leverage a meta-learning approach in combination with network compression under a sparsity constraint, such that it renders a well-initialized sparse parameterization that evolves quickly to represent a set of unseen signals in the subsequent training. We empirically demonstrate that meta-learned sparse neural representations achieve a much smaller loss than dense meta-learned models with the same number of parameters, when trained to fit each signal using the same number of optimization steps.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2127384453",
                        "name": "Jaehoon Lee"
                    },
                    {
                        "authorId": "1750599181",
                        "name": "Jihoon Tack"
                    },
                    {
                        "authorId": "2702448",
                        "name": "Namhoon Lee"
                    },
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Ensuing work confirmed this, showing that winning tickets are able to perform well on a variety of tasks beyond the one that they were originally discovered on [8, 17, 29, 30, 37, 39].",
                "Given that the development of the RG led to a first principled understanding of universal behavior near phase transitions, as well as a way in which to characterize materials by such behavior, we reasoned that viewing IMP from an RG perspective may prove useful when studying the universality of winning tickets [4, 5, 8, 17, 29, 30, 37, 39] and the general success IMP has found as a tool for interrogating DNNs [15].",
                "In recent years, researchers have found an intriguing corollary: winning tickets found in the context of one task can be transferred to related tasks [4, 5, 8, 17, 29, 30, 37, 39], possibly even across architectures [7]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c3a99e978d806d0426ae0b25f245c66c07e94cb1",
                "externalIds": {
                    "ArXiv": "2110.03210",
                    "DBLP": "conf/icml/RedmanCWD22",
                    "CorpusId": 246430593
                },
                "corpusId": 246430593,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c3a99e978d806d0426ae0b25f245c66c07e94cb1",
                "title": "Universality of Winning Tickets: A Renormalization Group Perspective",
                "abstract": "Foundational work on the Lottery Ticket Hypothesis has suggested an exciting corollary: winning tickets found in the context of one task can be transferred to similar tasks, possibly even across different architectures. This has generated broad interest, but methods to study this universality are lacking. We make use of renormalization group theory, a powerful tool from theoretical physics, to address this need. We \ufb01nd that iterative magnitude pruning, the principal algorithm used for discovering winning tickets, is a renormalization group scheme, and can be viewed as inducing a \ufb02ow in parameter space. We demonstrate that ResNet-50 models with transferable winning tickets have \ufb02ows with common properties, as would be expected from the theory. Similar observations are made for BERT models, with evidence that their \ufb02ows are near \ufb01xed points. Additionally, we leverage our framework to study winning tickets transferred across ResNet architectures, observ-ing that smaller models have \ufb02ows with more uniform properties than larger models, complicating transfer between them.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145036964",
                        "name": "William T. Redman"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "50173356",
                        "name": "Akshunna S. Dogra"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "62764bb2728d95805c93daef5f7a3a9debcd6417",
                "externalIds": {
                    "DBLP": "journals/tecs/ZhangWZTH21",
                    "DOI": "10.1145/3477002",
                    "CorpusId": 237596895
                },
                "corpusId": 237596895,
                "publicationVenue": {
                    "id": "13925159-5423-4d57-9ac9-3a05a713e0ef",
                    "name": "ACM Transactions on Embedded Computing Systems",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Transactions in Embedded Computing Systems",
                        "ACM Trans Embed Comput Syst"
                    ],
                    "issn": "1539-9087",
                    "url": "http://www.acm.org/pubs/contents/journals/tecs/",
                    "alternate_urls": [
                        "http://www.acm.org/tecs/",
                        "http://portal.acm.org/tecs"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/62764bb2728d95805c93daef5f7a3a9debcd6417",
                "title": "Algorithm-hardware Co-design of Attention Mechanism on FPGA Devices",
                "abstract": "Multi-head self-attention (attention mechanism) has been employed in a variety of fields such as machine translation, language modeling, and image processing due to its superiority in feature extraction and sequential data analysis. This is benefited from a large number of parameters and sophisticated model architecture behind the attention mechanism. To efficiently deploy attention mechanism on resource-constrained devices, existing works propose to reduce the model size by building a customized smaller model or compressing a big standard model. A customized smaller model is usually optimized for the specific task and needs effort in model parameters exploration. Model compression reduces model size without hurting the model architecture robustness, which can be efficiently applied to different tasks. The compressed weights in the model are usually regularly shaped (e.g. rectangle) but the dimension sizes vary (e.g. differs in rectangle height and width). Such compressed attention mechanism can be efficiently deployed on CPU/GPU platforms as their memory and computing resources can be flexibly assigned with demand. However, for Field Programmable Gate Arrays (FPGAs), the data buffer allocation and computing kernel are fixed at run time to achieve maximum energy efficiency. After compression, weights are much smaller and different in size, which leads to inefficient utilization of FPGA on-chip buffer. Moreover, the different weight heights and widths may lead to inefficient FPGA computing kernel execution. Due to the large number of weights in the attention mechanism, building a unique buffer and computing kernel for each compressed weight on FPGA is not feasible. In this work, we jointly consider the compression impact on buffer allocation and the required computing kernel during the attention mechanism compressing. A novel structural pruning method with memory footprint awareness is proposed and the associated accelerator on FPGA is designed. The experimental results show that our work can compress Transformer (an attention mechanism based model) by 95x. The developed accelerator can fully utilize the FPGA resource, processing the sparse attention mechanism with the run-time throughput performance of 1.87 Tops in ZCU102 FPGA.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108263472",
                        "name": "Xinyi Zhang"
                    },
                    {
                        "authorId": "2107887906",
                        "name": "Yawen Wu"
                    },
                    {
                        "authorId": "39399815",
                        "name": "Peipei Zhou"
                    },
                    {
                        "authorId": "8573809",
                        "name": "Xulong Tang"
                    },
                    {
                        "authorId": "2118517757",
                        "name": "Jingtong Hu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026performance for different winning ticket sub-networks within the same dataset. on the other hand, the work of Ibrahim et al. (Alabdulmohsin et al. 2021; Morcos et al. 2019) shown the existence of winning ticket sub-networks with the ability to generalize across datasets and training policies.",
                "(Alabdulmohsin et al. 2021; Morcos et al. 2019) shown the existence of winning ticket sub-networks with the ability to generalize across datasets and training policies."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "11ceb3e87711f0341345884cc0f9c4706e0f1069",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-03862",
                    "ArXiv": "2109.03862",
                    "CorpusId": 237452483
                },
                "corpusId": 237452483,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/11ceb3e87711f0341345884cc0f9c4706e0f1069",
                "title": "Juvenile state hypothesis: What we can learn from lottery ticket hypothesis researches?",
                "abstract": "The proposition of lottery ticket hypothesis revealed the relationship between network structure and initialization parameters and the learning potential of neural networks. The original lottery ticket hypothesis performs pruning and weight resetting after training convergence, exposing it to the problem of forgotten learning knowledge and potential high cost of training. Therefore, we propose a strategy that combines the idea of neural network structure search with a pruning algorithm to alleviate this problem. This algorithm searches and extends the network structure on existing winning ticket sub-network to producing new winning ticket recursively. This allows the training and pruning process to continue without compromising performance. A new winning ticket sub-network with deeper network structure, better generalization ability and better test performance can be obtained in this recursive manner. This method can solve: the difficulty of training or performance degradation of the sub-networks after pruning, the forgetting of the weights of the original lottery ticket hypothesis and the difficulty of generating winning ticket sub-network when the final network structure is not given. We validate this strategy on the MNIST and CIFAR-10 datasets. And after relating it to similar biological phenomena and relevant lottery ticket hypothesis studies in recent years, we will further propose a new hypothesis to discuss which factors that can keep a network juvenile, i.e., those possible factors that influence the learning potential or generalization performance of a neural network during training.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2141082713",
                        "name": "Di Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "83b4b42358936b0f6dfd8b432cd11e1454030892",
                "externalIds": {
                    "ArXiv": "2108.03506",
                    "DBLP": "journals/corr/abs-2108-03506",
                    "CorpusId": 236956533
                },
                "corpusId": 236956533,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/83b4b42358936b0f6dfd8b432cd11e1454030892",
                "title": "Membership Inference Attacks on Lottery Ticket Networks",
                "abstract": "The vulnerability of the Lottery Ticket Hypothesis has not been studied from the purview of Membership Inference Attacks. Through this work, we are the first to empirically show that the lottery ticket networks are equally vulnerable to membership inference attacks. A Membership Inference Attack (MIA) is the process of determining whether a data sample belongs to a training set of a trained model or not. Membership Inference Attacks could leak critical information about the training data that can be used for targeted attacks. Recent deep learning models often have very large memory footprints and a high computational cost associated with training and drawing inferences. Lottery Ticket Hypothesis is used to prune the networks to find smaller sub-networks that at least match the performance of the original model in terms of test accuracy in a similar number of iterations. We used CIFAR-10, CIFAR-100, and ImageNet datasets to perform image classification tasks and observe that the attack accuracies are similar. We also see that the attack accuracy varies directly according to the number of classes in the dataset and the sparsity of the network. We demonstrate that these attacks are transferable across models with high accuracy.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1693718757",
                        "name": "Aadesh Bagmar"
                    },
                    {
                        "authorId": "51469126",
                        "name": "Shishira R. Maiya"
                    },
                    {
                        "authorId": "2122929492",
                        "name": "Shruti Bidwalka"
                    },
                    {
                        "authorId": "144520191",
                        "name": "A. Deshpande"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u20262017) \u2022 A simple MLP model, created by removing layers from\nthe TabTransformer model (see (Huang et al. 2020), \u00a73.1, paragraph 1)\n\u2022 A sparse MLP, based on (Morcos et al. 2019) \u2022 TabTransformer (Huang et al. 2020) \u2022 TabNet (Arik and Pfister 2020) \u2022 Variational Information Bottleneck (VIB) (Alemi\u2026"
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "8f702ee9b92d1de27d19a6019106300a5b99089b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-03214",
                    "ArXiv": "2108.03214",
                    "CorpusId": 236950484
                },
                "corpusId": 236950484,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8f702ee9b92d1de27d19a6019106300a5b99089b",
                "title": "Simple Modifications to Improve Tabular Neural Networks",
                "abstract": "There is growing interest in neural network architectures for tabular data. Many general-purpose tabular deep learning models have been introduced recently, with performance sometimes rivaling gradient boosted decision trees (GBDTs). These recent models draw inspiration from various sources, including GBDTs, factorization machines, and neural networks from other application domains. Previous tabular neural networks are also drawn upon, but are possibly under-considered, especially models associated with specific tabular problems. This paper focuses on several such models, and proposes modifications for improving their performance. When modified, these models are shown to be competitive with leading general-purpose tabular models, including GBDTs.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50463361",
                        "name": "J. Fiedler"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "8a19acc6e026a58f56f7a78ef02f1fc97a6eb25c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-00944",
                    "ArXiv": "2108.00944",
                    "DOI": "10.1002/int.22827",
                    "CorpusId": 236777012
                },
                "corpusId": 236777012,
                "publicationVenue": {
                    "id": "05528bac-d212-46a6-9c84-314d4bd77368",
                    "name": "International Journal of Intelligent Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Intell Syst"
                    ],
                    "issn": "0884-8173",
                    "url": "http://www3.interscience.wiley.com/cgi-bin/jhome/36062",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/1098111X"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8a19acc6e026a58f56f7a78ef02f1fc97a6eb25c",
                "title": "Exploring lottery ticket hypothesis in media recommender systems",
                "abstract": "Media recommender systems aim to capture users\u2019 preferences and provide precise personalized recommendation of media content. There are two critical components in the common paradigm of modern recommender models: (1) representation learning, which generates an embedding for each user and item; and (2) interaction modeling, which fits user preferences toward items based on their representations. In spite of great success, when a great amount of users and items exist, it usually needs to create, store, and optimize a huge embedding table, where the scale of model parameters easily reach millions or even larger. Hence, it naturally raises questions about the heavy recommender models: Do we really need such large\u2010scale parameters? We get inspirations from the recently proposed lottery ticket hypothesis (LTH), which argues that the dense and over\u2010parameterized model contains a much smaller and sparser sub\u2010model that can reach comparable performance to the full model. In this paper, we extend LTH to media recommender systems, aiming to find the winning tickets in deep recommender models. To the best of our knowledge, this is the first work to study LTH in media recommender systems. With Matrix Factorization and Light Graph Convolution Networks as the backbone models, we found that there widely exist winning tickets in recommender models. On three media convergence data sets\u2014Yelp2018, TikTok and Kwai, the winning tickets can achieve comparable recommendation performance with only 29 % ~ 48 % , 7 % ~ 10 % , and 3 % ~ 17 % of parameters, respectively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108846876",
                        "name": "Yanfang Wang"
                    },
                    {
                        "authorId": "2003767516",
                        "name": "Yongduo Sui"
                    },
                    {
                        "authorId": "2144796537",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "2145312301",
                        "name": "Zhenguang Liu"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The above three-step procedure mostly originates from the prominent line of work of the Lottery Ticket Hypothesis [16, 8, 17, 19, 38, 44, 66, 67] (or LTH): i.",
                "The above three-step procedure mostly originates from the prominent line of work of the Lottery Ticket Hypothesis [16, 8, 17, 19, 38, 44, 66, 67] (or LTH): i.e., the idea that a pre-trained model contains \u201clottery tickets\u201d (i.e., smaller subnetworks) such that if we select those \u201ctickets\u201d cleverly, those submodels do not lose much in accuracy while reducing significantly the size of the model."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "2e1b34719554d130cfe7f5e6f2352cadec7b60a3",
                "externalIds": {
                    "ArXiv": "2108.00259",
                    "CorpusId": 244921154
                },
                "corpusId": 244921154,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2e1b34719554d130cfe7f5e6f2352cadec7b60a3",
                "title": "How much pre-training is enough to discover a good subnetwork?",
                "abstract": "Neural network pruning is useful for discovering efficient, high-performing subnetworks within pre-trained, dense network architectures. More often than not, it involves a three-step process -- pre-training, pruning, and re-training -- that is computationally expensive, as the dense model must be fully pre-trained. While previous work has revealed through experiments the relationship between the amount of pre-training and the performance of the pruned network, a theoretical characterization of such dependency is still missing. Aiming to mathematically analyze the amount of dense network pre-training needed for a pruned network to perform well, we discover a simple theoretical bound in the number of gradient descent pre-training iterations on a two-layer, fully-connected network, beyond which pruning via greedy forward selection [61] yields a subnetwork that achieves good training error. Interestingly, this threshold is shown to be logarithmically dependent upon the size of the dataset, meaning that experiments with larger datasets require more pre-training for subnetworks obtained via pruning to perform well. Lastly, we empirically validate our theoretical results on a multi-layer perceptron trained on MNIST.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34210029",
                        "name": "Cameron R. Wolfe"
                    },
                    {
                        "authorId": "2154497049",
                        "name": "Qihan Wang"
                    },
                    {
                        "authorId": "2120215686",
                        "name": "J. Kim"
                    },
                    {
                        "authorId": "2126894228",
                        "name": "Anastasios Kyrillidis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, the existing pruning approaches usually perform the iterations of parameter removal and accuracy recovery at each layer, making it hard to achieve a global optimum [14], [17] with few retraining iterations.",
                "Consequently, the pruned network can not achieve the optimal accuracy [17]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7e87d752992023d645ad3975b7207cb760b8af25",
                "externalIds": {
                    "DBLP": "conf/ijcnn/HuZXWGCC21",
                    "DOI": "10.1109/IJCNN52387.2021.9533522",
                    "CorpusId": 237597996
                },
                "corpusId": 237597996,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/7e87d752992023d645ad3975b7207cb760b8af25",
                "title": "MIXP: Efficient Deep Neural Networks Pruning for Further FLOPs Compression via Neuron Bond",
                "abstract": "Neuron networks pruning is effective in compressing pre-trained CNNs for their deployment on low-end edge devices. However, few works have focused on reducing the computational cost of pruning and inference. We find that existing pruning methods usually remove parameters without fine-grained impact analysis, making it hard to achieve an optimal solution. This work develops a novel mixture pruning mechanism, MIXP, which can effectively reduce the computational cost of CNNs while maintaining a high weight compression ratio and model accuracy. We propose to remove neuron bond that can effectively reduce convolution computations and weight size in CNNs. We also design an influence factor to analyze the importance of neuron bonds and weights in a fine-grained way so that MIXP could achieve precise pruning with few retraining iterations. Experiments with MNIST, CIFAR-10, and ImageNet datasets demonstrate that MIXP could achieve significantly fewer FLOPs and retraining iterations on four widely-used CNNs than existing pruning methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118095281",
                        "name": "Bin Hu"
                    },
                    {
                        "authorId": "145908210",
                        "name": "Tianming Zhao"
                    },
                    {
                        "authorId": "1865455943",
                        "name": "Yucheng Xie"
                    },
                    {
                        "authorId": "2152540298",
                        "name": "Yan Wang"
                    },
                    {
                        "authorId": "1713390",
                        "name": "Xiaonan Guo"
                    },
                    {
                        "authorId": "49776862",
                        "name": "Jerry Q. Cheng"
                    },
                    {
                        "authorId": "49069517",
                        "name": "Yingying Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026the performance of TabTransformer against following four categories of methods: (a) Logistic regression and GBDT; (b) MLP and a sparse MLP following Morcos et al. (2019); (c) TabNet model of Arik & Pfister (2019); and (d) the Variational Information Bottleneck model (VIB) of Alemi et al. (2017)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "bdf22f6ee3ffe0169dd455cca42d99200a10076f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-03690",
                    "ArXiv": "2107.03690",
                    "CorpusId": 235765694
                },
                "corpusId": 235765694,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bdf22f6ee3ffe0169dd455cca42d99200a10076f",
                "title": "Proceedings of the First Workshop on Weakly Supervised Learning (WeaSuL)",
                "abstract": "The paradigm of pre-training followed by finetuning has become a standard procedure for NLP tasks, with a known problem of domain shift between the pre-training and downstream corpus. Previous works have tried to mitigate this problem with additional pre-training, either on the downstream corpus itself when it is large enough, or on a manually curated unlabeled corpus of a similar domain. In this paper, we address the problem for the case when the downstream corpus is too small for additional pre-training. We propose TADPOLE, a task adapted pre-training framework based on data selection techniques adapted from Domain Adaptation. We formulate the data selection as an anomaly detection problem that unlike existing methods works well when the downstream corpus is limited in size. It results in a scalable and efficient unsupervised technique that eliminates the need for any manual data curation. We evaluate our framework on eight tasks across four different domains: Biomedical, Computer Science, News, and Movie reviews, and compare its performance against competitive baseline techniques from the area of Domain Adaptation. Our framework outperforms all the baseline methods. On large datasets we get an average gain of 0.3% in performance but on small datasets with less than 5K training examples, we get a much higher gain of 1.8%. This shows the efficacy of domain adapted finetuning when the task dataset is small.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51133383",
                        "name": "Michael A. Hedderich"
                    },
                    {
                        "authorId": "143862204",
                        "name": "Benjamin Roth"
                    },
                    {
                        "authorId": "3422953",
                        "name": "Katharina Kann"
                    },
                    {
                        "authorId": "2022124",
                        "name": "Barbara Plank"
                    },
                    {
                        "authorId": "143711421",
                        "name": "Alexander J. Ratner"
                    },
                    {
                        "authorId": "2561225",
                        "name": "D. Klakow"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[34, 35] have studied the lottery ticket hypothesis in unsupervised learning to reveal how well the tickets are transformed between different datasets."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2dbd1fc62f13cceb223f1caa70424cc3ccaeea4f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-00166",
                    "ArXiv": "2107.00166",
                    "CorpusId": 235694458
                },
                "corpusId": 235694458,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2dbd1fc62f13cceb223f1caa70424cc3ccaeea4f",
                "title": "Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?",
                "abstract": "There have been long-standing controversies and inconsistencies over the experiment setup and criteria for identifying the\"winning ticket\"in literature. To reconcile such, we revisit the definition of lottery ticket hypothesis, with comprehensive and more rigorous conditions. Under our new definition, we show concrete evidence to clarify whether the winning ticket exists across the major DNN architectures and/or applications. Through extensive experiments, we perform quantitative analysis on the correlations between winning tickets and various experimental factors, and empirically study the patterns of our observations. We find that the key training hyperparameters, such as learning rate and training epochs, as well as the architecture characteristics such as capacities and residual connections, are all highly correlated with whether and when the winning tickets can be identified. Based on our analysis, we summarize a guideline for parameter settings in regards of specific architecture characteristics, which we hope to catalyze the research progress on the topic of lottery ticket hypothesis. Our codes are publicly available at: https://github.com/boone891214/sanity-check-LTH.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "151480882",
                        "name": "Xiaolong Ma"
                    },
                    {
                        "authorId": "9347641",
                        "name": "Geng Yuan"
                    },
                    {
                        "authorId": "2007668856",
                        "name": "Xuan Shen"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "1753619492",
                        "name": "Xuxi Chen"
                    },
                    {
                        "authorId": "2116298570",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "2152354569",
                        "name": "Ning Liu"
                    },
                    {
                        "authorId": "39449475",
                        "name": "Minghai Qin"
                    },
                    {
                        "authorId": "143743061",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "46393431",
                        "name": "Yanzhi Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent studies of deep learning have demonstrated that redundancy of parameters (deep layers of hidden variables) reduces generalization error, contrary to the implications of Occam\u2019s razor (He et al. 2019; Morcos et al. 2019; Nagarajan and Kolter 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "result"
            ],
            "citingPaper": {
                "paperId": "15ac40ee6aed5677d2ffa2911be807584bb5f38d",
                "externalIds": {
                    "MAG": "3194953800",
                    "DOI": "10.1007/s41237-021-00143-x",
                    "CorpusId": 238804245
                },
                "corpusId": 238804245,
                "publicationVenue": {
                    "id": "486b61f4-bd3b-41cc-827d-5bd1c0b07592",
                    "name": "Behaviormetrika",
                    "type": "journal",
                    "issn": "0385-7417",
                    "url": "https://link.springer.com/journal/41237",
                    "alternate_urls": [
                        "https://www.jstage.jst.go.jp/browse/bhmk/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/15ac40ee6aed5677d2ffa2911be807584bb5f38d",
                "title": "e-Testing from artificial intelligence approach",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1924970",
                        "name": "M. Ueno"
                    },
                    {
                        "authorId": "2045072133",
                        "name": "Kazuma Fuchimoto"
                    },
                    {
                        "authorId": "52017989",
                        "name": "E. Tsutsumi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "If the hypothesis holds for a given network, then we can reduce the computational cost by using the sparse subnetwork instead of the entire network while maintaining the accuracy [14, 19].",
                "Many properties and applications have been studied in subsequent works, such as transferability of winning subnetworks [19], sparsification before training [14, 27, 26] and further ablation studies on the hypothesis [29]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "14eb496d5ad95f707e5ae9101bfe7bc602f88508",
                "externalIds": {
                    "ArXiv": "2106.09269",
                    "DBLP": "conf/nips/ChijiwaYIUI21",
                    "CorpusId": 235458499
                },
                "corpusId": 235458499,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/14eb496d5ad95f707e5ae9101bfe7bc602f88508",
                "title": "Pruning Randomly Initialized Neural Networks with Iterative Randomization",
                "abstract": "Pruning the weights of randomly initialized neural networks plays an important role in the context of lottery ticket hypothesis. Ramanujan et al. (2020) empirically showed that only pruning the weights can achieve remarkable performance instead of optimizing the weight values. However, to achieve the same level of performance as the weight optimization, the pruning approach requires more parameters in the networks before pruning and thus more memory space. To overcome this parameter inefficiency, we introduce a novel framework to prune randomly initialized neural networks with iteratively randomizing weight values (IteRand). Theoretically, we prove an approximation theorem in our framework, which indicates that the randomizing operations are provably effective to reduce the required number of the parameters. We also empirically demonstrate the parameter efficiency in multiple experiments on CIFAR-10 and ImageNet. The code is available at: https://github.com/dchiji-ntt/iterand",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2113252066",
                        "name": "Daiki Chijiwa"
                    },
                    {
                        "authorId": "36351779",
                        "name": "Shin'ya Yamaguchi"
                    },
                    {
                        "authorId": "1719865",
                        "name": "Yasutoshi Ida"
                    },
                    {
                        "authorId": "33595646",
                        "name": "Kenji Umakoshi"
                    },
                    {
                        "authorId": "2116994373",
                        "name": "T. Inoue"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026there are still many other promising avenues that could lead to efficient sparse training, for example, transferring existing lottery tickets Morcos et al. (2019); Mehta (2019), pruning weights during training (You et al., 2019), or dynamically changing the mask during training (Evci et\u2026",
                "For example, Morcos et al. (2019) hint at \"the possibility of parameterizing the distribution of such tickets, allowing us to sample generic, dataset-independent winning tickets.\""
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1f31db2bf95133655c29fd3bedfc2912bc76eb85",
                "externalIds": {
                    "ArXiv": "2106.06955",
                    "DBLP": "journals/corr/abs-2106-06955",
                    "CorpusId": 235421894
                },
                "corpusId": 235421894,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1f31db2bf95133655c29fd3bedfc2912bc76eb85",
                "title": "Towards Understanding Iterative Magnitude Pruning: Why Lottery Tickets Win",
                "abstract": "The lottery ticket hypothesis states that sparse subnetworks exist in randomly initialized dense networks that can be trained to the same accuracy as the dense network they reside in. However, the subsequent work has failed to replicate this on large-scale models and required rewinding to an early stable state instead of initialization. We show that by using a training method that is stable with respect to linear mode connectivity, large networks can also be entirely rewound to initialization. Our subsequent experiments on common vision tasks give strong credence to the hypothesis in Evci et al. (2020b) that lottery tickets simply retrain to the same regions (although not necessarily to the same basin). These results imply that existing lottery tickets could not have been found without the preceding dense training by iterative magnitude pruning, raising doubts about the use of the lottery ticket hypothesis.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2051865997",
                        "name": "Jaron Maene"
                    },
                    {
                        "authorId": "2112132080",
                        "name": "Mingxiao Li"
                    },
                    {
                        "authorId": "100781843",
                        "name": "Marie-Francine Moens"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another intriguing property of lottery tickets, the transferability, has also been thoroughly examined (Mehta, 2019; Morcos et al., 2019; Desai et al., 2019; Chen et al., 2020b;a)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6bc4681828143f5ecc49b7ecd388a86c70c7237a",
                "externalIds": {
                    "DBLP": "conf/icml/ZhangCCW21",
                    "ArXiv": "2106.03225",
                    "CorpusId": 235358439
                },
                "corpusId": 235358439,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6bc4681828143f5ecc49b7ecd388a86c70c7237a",
                "title": "Efficient Lottery Ticket Finding: Less Data is More",
                "abstract": "The lottery ticket hypothesis (LTH) reveals the existence of winning tickets (sparse but critical subnetworks) for dense networks, that can be trained in isolation from random initialization to match the latter's accuracies. However, finding winning tickets requires burdensome computations in the train-prune-retrain process, especially on large-scale datasets (e.g., ImageNet), restricting their practical benefits. This paper explores a new perspective on finding lottery tickets more efficiently, by doing so only with a specially selected subset of data, called Pruning-Aware Critical set (PrAC set), rather than using the full training set. The concept of PrAC set was inspired by the recent observation, that deep networks have samples that are either hard to memorize during training, or easy to forget during pruning. A PrAC set is thus hypothesized to capture those most challenging and informative examples for the dense model. We observe that a high-quality winning ticket can be found with training and pruning the dense network on the very compact PrAC set, which can substantially save training iterations for the ticket finding process. Extensive experiments validate our proposal across diverse datasets and network architectures. Specifically, on CIFAR-10, CIFAR-100, and Tiny ImageNet, we locate effective PrAC sets at 35.32%~78.19% of their training set sizes. On top of them, we can obtain the same competitive winning tickets for the corresponding dense networks, yet saving up to 82.85%~92.77%, 63.54%~74.92%, and 76.14%~86.56% training iterations, respectively. Crucially, we show that a PrAC set found is reusable across different network architectures, which can amortize the extra cost of finding PrAC sets, yielding a practical regime for efficient lottery ticket finding.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109338656",
                        "name": "Zhenyu (Allen) Zhang"
                    },
                    {
                        "authorId": "1753619492",
                        "name": "Xuxi Chen"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9efa13bc02bed2e41b3e425dd2ce15d7f2336714",
                "externalIds": {
                    "DBLP": "conf/cvpr/GaoH0H21",
                    "DOI": "10.1109/CVPR46437.2021.00915",
                    "CorpusId": 235703417
                },
                "corpusId": 235703417,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9efa13bc02bed2e41b3e425dd2ce15d7f2336714",
                "title": "Network Pruning via Performance Maximization",
                "abstract": "Channel pruning is a class of powerful methods for model compression. When pruning a neural network, it's ideal to obtain a sub-network with higher accuracy. However, a sub-network does not necessarily have high accuracy with low classification loss (loss-metric mismatch). In the paper, we first consider the loss-metric mismatch problem for pruning and propose a novel channel pruning method for Convolutional Neural Networks (CNNs) by directly maximizing the performance (i.e., accuracy) of sub-networks. Specifically, we train a stand-alone neural network to predict sub-networks' performance and then maximize the output of the network as a proxy of accuracy to guide pruning. Training such a performance prediction network efficiently is not an easy task, and it may potentially suffer from the problem of catastrophic forgetting and the imbalance distribution of sub-networks. To deal with this challenge, we introduce a corresponding episodic memory to update and collect sub-networks during the pruning process. In the experiment section, we further demonstrate that the gradients from the performance prediction network and the classification loss have different directions. Extensive experimental results show that the proposed method can achieve state-of-the-art performance with ResNet, MobileNetV2, and ShuffleNetV2+ on ImageNet and CIFAR-10.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9355577",
                        "name": "Shangqian Gao"
                    },
                    {
                        "authorId": "3057688",
                        "name": "Feihu Huang"
                    },
                    {
                        "authorId": "122905659",
                        "name": "Weidong (Tom) Cai"
                    },
                    {
                        "authorId": "145114933",
                        "name": "Heng Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although it has been well studied in discriminative models (Mehta, 2019; Morcos et al., 2019; Chen et al., 2020b), an in-depth understanding of transfer learning in GAN tickets is still missing.",
                "Mehta (2019); Morcos et al. (2019); Desai et al. (2019) are the pioneers to study the transferability of found subnetworks."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "23fa2f604f73785b638eb49df4c1bbf293e16cd5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-00134",
                    "ArXiv": "2106.00134",
                    "CorpusId": 231800078
                },
                "corpusId": 231800078,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/23fa2f604f73785b638eb49df4c1bbf293e16cd5",
                "title": "GANs Can Play Lottery Tickets Too",
                "abstract": "Deep generative adversarial networks (GANs) have gained growing popularity in numerous scenarios, while usually suffer from high parameter complexities for resource-constrained real-world applications. However, the compression of GANs has less been explored. A few works show that heuristically applying compression techniques normally leads to unsatisfactory results, due to the notorious training instability of GANs. In parallel, the lottery ticket hypothesis shows prevailing success on discriminative models, in locating sparse matching subnetworks capable of training in isolation to full model performance. In this work, we for the first time study the existence of such trainable matching subnetworks in deep GANs. For a range of GANs, we certainly find matching subnetworks at 67%-74% sparsity. We observe that with or without pruning discriminator has a minor effect on the existence and quality of matching subnetworks, while the initialization weights used in the discriminator play a significant role. We then show the powerful transferability of these subnetworks to unseen tasks. Furthermore, extensive experimental results demonstrate that our found subnetworks substantially outperform previous state-of-the-art GAN compression approaches in both image generation (e.g. SNGAN) and image-to-image translation GANs (e.g. CycleGAN). Codes available at https://github.com/VITA-Group/GAN-LTH.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1753619492",
                        "name": "Xuxi Chen"
                    },
                    {
                        "authorId": "2109338656",
                        "name": "Zhenyu (Allen) Zhang"
                    },
                    {
                        "authorId": "2003767516",
                        "name": "Yongduo Sui"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Aside from training from scratch, such winning tickets have demonstrated their abilities to transfer across tasks and datasets (Morcos et al., 2019; Yu et al., 2019; Desai et al., 2019; Chen et al., 2020a).",
                "Besides training from scratch, researchers also explore the existence of winning tickets under transfer learning regimes for over-parametrized pre-trained models across various tasks and datasets (Morcos et al., 2019; Yu et al., 2019; Desai et al., 2019; Chen et al., 2020a)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e638b9e6ee09ab4fa748b748099e0f03d471d803",
                "externalIds": {
                    "ACL": "2021.acl-long.510",
                    "DBLP": "conf/acl/LiangZCJLHZC20",
                    "ArXiv": "2105.12002",
                    "DOI": "10.18653/v1/2021.acl-long.510",
                    "CorpusId": 235186841
                },
                "corpusId": 235186841,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/e638b9e6ee09ab4fa748b748099e0f03d471d803",
                "title": "Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization",
                "abstract": "The Lottery Ticket Hypothesis suggests that an over-parametrized network consists of \u201dlottery tickets\u201d, and training a certain collection of them (i.e., a subnetwork) can match the performance of the full model. In this paper, we study such a collection of tickets, which is referred to as \u201dwinning tickets\u201d, in extremely over-parametrized models, e.g., pre-trained language models. We observe that at certain compression ratios, the generalization performance of the winning tickets can not only match but also exceed that of the full model. In particular, we observe a phase transition phenomenon: As the compression ratio increases, generalization performance of the winning tickets first improves then deteriorates after a certain threshold. We refer to the tickets on the threshold as \u201dsuper tickets\u201d. We further show that the phase transition is task and model dependent \u2014 as the model size becomes larger and the training data set becomes smaller, the transition becomes more pronounced. Our experiments on the GLUE benchmark show that the super tickets improve single task fine-tuning by 0.9 points on BERT-base and 1.0 points on BERT-large, in terms of task-average score. We also demonstrate that adaptively sharing the super tickets across tasks benefits multi-task learning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "98703980",
                        "name": "Chen Liang"
                    },
                    {
                        "authorId": "52194893",
                        "name": "Simiao Zuo"
                    },
                    {
                        "authorId": "2108809403",
                        "name": "Minshuo Chen"
                    },
                    {
                        "authorId": "2152630772",
                        "name": "Haoming Jiang"
                    },
                    {
                        "authorId": "46522098",
                        "name": "Xiaodong Liu"
                    },
                    {
                        "authorId": "50462546",
                        "name": "Pengcheng He"
                    },
                    {
                        "authorId": "36345161",
                        "name": "T. Zhao"
                    },
                    {
                        "authorId": "2109136147",
                        "name": "Weizhu Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similar ticket transfer results were reported by Morcos et al. (2019) and Mehta (2019) in the context of optimizers and vision datasets."
            ],
            "isInfluential": false,
            "intents": [
                "result"
            ],
            "citingPaper": {
                "paperId": "054756e00b52512b432c1aae9b847f84aa6b64c9",
                "externalIds": {
                    "DBLP": "conf/iclr/VischerLS22",
                    "ArXiv": "2105.01648",
                    "CorpusId": 233714921
                },
                "corpusId": 233714921,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/054756e00b52512b432c1aae9b847f84aa6b64c9",
                "title": "On Lottery Tickets and Minimal Task Representations in Deep Reinforcement Learning",
                "abstract": "The lottery ticket hypothesis questions the role of overparameterization in supervised deep learning. But how is the performance of winning lottery tickets affected by the distributional shift inherent to reinforcement learning problems? In this work, we address this question by comparing sparse agents who have to address the non-stationarity of the exploration-exploitation problem with supervised agents trained to imitate an expert. We show that feed-forward networks trained with behavioural cloning compared to reinforcement learning can be pruned to higher levels of sparsity without performance degradation. This suggests that in order to solve the RL-specific distributional shift agents require more degrees of freedom. Using a set of carefully designed baseline conditions, we find that the majority of the lottery ticket effect in both learning paradigms can be attributed to the identified mask rather than the weight initialization. The input layer mask selectively prunes entire input dimensions that turn out to be irrelevant for the task at hand. At a moderate level of sparsity the mask identified by iterative magnitude pruning yields minimal task-relevant representations, i.e., an interpretable inductive bias. Finally, we propose a simple initialization rescaling which promotes the robust identification of sparse task representations in low-dimensional control tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2087694608",
                        "name": "Marc Aurel Vischer"
                    },
                    {
                        "authorId": "2066450047",
                        "name": "R. Lange"
                    },
                    {
                        "authorId": "2593439",
                        "name": "Henning Sprekeler"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other studies concerned hyperparameters modifications [24, 25], and concentrated on the transferability [16, 26] of the pruned networks.",
                "It has been actually shown that the same winning lottery ticket generalizes across training conditions and similar datasets [16].",
                "This directly relates to the good transfer properties of the subnetworks corresponding to the winning tickets [16, 26]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "eca3791904a4b5d0f74cfe2300f2be0900ebeff6",
                "externalIds": {
                    "ArXiv": "2104.13343",
                    "DBLP": "journals/corr/abs-2104-13343",
                    "CorpusId": 233407723
                },
                "corpusId": 233407723,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/eca3791904a4b5d0f74cfe2300f2be0900ebeff6",
                "title": "Sifting out the features by pruning: Are convolutional networks the winning lottery ticket of fully connected ones?",
                "abstract": "Pruning methods can considerably reduce the size of artificial neural networks without harming their performance. In some cases, they can even uncover sub-networks that, when trained in isolation, match or surpass the test accuracy of their dense counterparts. Here we study the inductive bias that pruning imprints in such\"winning lottery tickets\". Focusing on visual tasks, we analyze the architecture resulting from iterative magnitude pruning of a simple fully connected network (FCN). We show that the surviving node connectivity is local in input space, and organized in patterns reminiscent of the ones found in convolutional networks (CNN). We investigate the role played by data and tasks in shaping the architecture of pruned sub-networks. Our results show that the winning lottery tickets of FCNs display the key features of CNNs. The ability of such automatic network-simplifying procedure to recover the key features\"hand-crafted\"in the design of CNNs suggests interesting applications to other datasets and tasks, in order to discover new and efficient architectural inductive biases.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "39324024",
                        "name": "F. Pellegrini"
                    },
                    {
                        "authorId": "2188423",
                        "name": "G. Biroli"
                    }
                ]
            }
        },
        {
            "contexts": [
                "formance as the original network, when trained in isolation [16, 32].",
                "neural network parameters space properties, gives us insights on the possible existence of low-dimensional manifolds in the network parameter space [14, 16, 20, 30, 32, 37]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c1d7caee5cbd8bb4b5e85141400941dc48db71ac",
                "externalIds": {
                    "ArXiv": "2104.13424",
                    "DBLP": "journals/corr/abs-2104-13424",
                    "DOI": "10.1145/3449639.3459320",
                    "CorpusId": 233423513
                },
                "corpusId": 233423513,
                "publicationVenue": {
                    "id": "d732841e-83f9-49ec-95ca-389e5568634b",
                    "name": "Annual Conference on Genetic and Evolutionary Computation",
                    "type": "conference",
                    "alternate_names": [
                        "GECCO",
                        "Annu Conf Genet Evol Comput",
                        "Genet Evol Comput Conf",
                        "Genetic and Evolutionary Computation Conference"
                    ],
                    "url": "http://www.sigevo.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c1d7caee5cbd8bb4b5e85141400941dc48db71ac",
                "title": "Policy manifold search: exploring the manifold hypothesis for diversity-based neuroevolution",
                "abstract": "Neuroevolution is an alternative to gradient-based optimisation that has the potential to avoid local minima and allows parallelisation. The main limiting factor is that usually it does not scale well with parameter space dimensionality. Inspired by recent work examining neural network intrinsic dimension and loss landscapes, we hypothesise that there exists a low-dimensional manifold, embedded in the policy network parameter space, around which a high-density of diverse and useful policies are located. This paper proposes a novel method for diversity-based policy search via Neuroevolution, that leverages learned representations of the policy network parameters, by performing policy search in this learned representation space. Our method relies on the Quality-Diversity (QD) framework which provides a principled approach to policy search, and maintains a collection of diverse policies, used as a dataset for learning policy representations. Further, we use the Jacobian of the inverse-mapping function to guide the search in the representation space. This ensures that the generated samples remain in the high-density regions, after mapping back to the original space. Finally, we evaluate our contributions on four continuous-control tasks in simulated environments, and compare to diversity-based baselines.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3287144",
                        "name": "Nemanja Rakicevic"
                    },
                    {
                        "authorId": "1934171",
                        "name": "Antoine Cully"
                    },
                    {
                        "authorId": "2037170549",
                        "name": "Petar Kormushev"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5fab9531ee9af67297548b07194f8b9815964f31",
                "externalIds": {
                    "MAG": "3158717321",
                    "DOI": "10.3390/ELECTRONICS10091020",
                    "CorpusId": 235510618
                },
                "corpusId": 235510618,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5fab9531ee9af67297548b07194f8b9815964f31",
                "title": "Deep Item Response Theory as a Novel Test Theory Based on Deep Learning",
                "abstract": "Item Response Theory (IRT) evaluates, on the same scale, examinees who take different tests. It requires the linkage of examinees\u2019 ability scores as estimated from different tests. However, the IRT linkage techniques assume independently random sampling of examinees\u2019 abilities from a standard normal distribution. Because of this assumption, the linkage not only requires much labor to design, but it also has no guarantee of optimality. To resolve that shortcoming, this study proposes a novel IRT based on deep learning, Deep-IRT, which requires no assumption of randomly sampled examinees\u2019 abilities from a distribution. Experiment results demonstrate that Deep-IRT estimates examinees\u2019 abilities more accurately than the traditional IRT does. Moreover, Deep-IRT can express actual examinees\u2019 ability distributions flexibly, not merely following the standard normal distribution assumed for traditional IRT. Furthermore, the results show that Deep-IRT more accurately predicts examinee responses to unknown items from the examinee\u2019s own past response histories than IRT does.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "52017989",
                        "name": "E. Tsutsumi"
                    },
                    {
                        "authorId": "47099956",
                        "name": "Ryo Kinoshita"
                    },
                    {
                        "authorId": "1924970",
                        "name": "M. Ueno"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f44b6e48969e8204e46860d42686fff1feacdd61",
                "externalIds": {
                    "DBLP": "journals/mta/HuiGZS21",
                    "MAG": "3154203592",
                    "DOI": "10.1007/s11042-021-10736-z",
                    "CorpusId": 234864714
                },
                "corpusId": 234864714,
                "publicationVenue": {
                    "id": "477368e9-7a8e-475a-8c93-6d623797fd06",
                    "name": "Multimedia tools and applications",
                    "type": "journal",
                    "alternate_names": [
                        "Multimedia Tools and Applications",
                        "Multimedia Tool Appl",
                        "Multimedia tool appl"
                    ],
                    "issn": "1380-7501",
                    "url": "https://www.springer.com/computer/information+systems+and+applications/journal/11042",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11042"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f44b6e48969e8204e46860d42686fff1feacdd61",
                "title": "Robust deflated canonical correlation analysis via feature factoring for multi-view image classification",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3491896",
                        "name": "Kaifa Hui"
                    },
                    {
                        "authorId": "91978033",
                        "name": "E. D. Ganaa"
                    },
                    {
                        "authorId": "144754529",
                        "name": "Yongzhao Zhan"
                    },
                    {
                        "authorId": "1697341",
                        "name": "Xiang-jun Shen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The inaccessibility of W\u2217 has also be noticed in [56]\u2013[58], proving the importance of our NTAA in synchronously learning the network weights and the architecture in the same way as our NTAA.",
                "The inaccessibility of W\u2217 has also been discussed in the theory of lottery ticket hypothesis [56]\u2013[58]."
            ],
            "isInfluential": false,
            "intents": [
                "result",
                "background"
            ],
            "citingPaper": {
                "paperId": "68191ba0666b9a8a0a9dfb00628279b6eaf34d71",
                "externalIds": {
                    "ArXiv": "2103.16889",
                    "DBLP": "journals/tnn/WangLCWZ22",
                    "DOI": "10.1109/TNNLS.2021.3070605",
                    "CorpusId": 232428364,
                    "PubMed": "33872158"
                },
                "corpusId": 232428364,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/68191ba0666b9a8a0a9dfb00628279b6eaf34d71",
                "title": "Joint Learning of Neural Transfer and Architecture Adaptation for Image Recognition",
                "abstract": "Current state-of-the-art visual recognition systems usually rely on the following pipeline: 1) pretraining a neural network on a large-scale data set (e.g., ImageNet) and 2) finetuning the network weights on a smaller, task-specific data set. Such a pipeline assumes that the sole weight adaptation is able to transfer the network capability from one domain to another domain based on a strong assumption that a fixed architecture is appropriate for all domains. However, each domain with a distinct recognition target may need different levels/paths of feature hierarchy, where some neurons may become redundant, and some others are reactivated to form new network structures. In this work, we prove that dynamically adapting network architectures tailored for each domain task along with weight finetuning benefits in both efficiency and effectiveness, compared to the existing image recognition pipeline that only tunes the weights regardless of the architecture. Our method can be easily generalized to an unsupervised paradigm by replacing supernet training with self-supervised learning in the source domain tasks and performing linear evaluation in the downstream tasks. This further improves the search efficiency of our method. Moreover, we also provide principled and empirical analysis to explain why our approach works by investigating the ineffectiveness of existing neural architecture search. We find that preserving the joint distribution of the network architecture and weights is of importance. This analysis not only benefits image recognition but also provides insights for crafting neural networks. Experiments on five representative image recognition tasks, such as person re-identification, age estimation, gender recognition, image classification, and unsupervised domain adaptation, demonstrate the effectiveness of our method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2749191",
                        "name": "Guangrun Wang"
                    },
                    {
                        "authorId": "1737218",
                        "name": "Liang Lin"
                    },
                    {
                        "authorId": "52146427",
                        "name": "Rongcong Chen"
                    },
                    {
                        "authorId": "32317186",
                        "name": "Guangcong Wang"
                    },
                    {
                        "authorId": "3304367",
                        "name": "Jiqi Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Prior work [33] found that a winning ticket of one dense network can generalize across datasets and optimizers, beyond the original training setting where it was identified.",
                "[33, 3] studied the transferability of winning tickets between datasets; the former focuses on showing one winning ticket to generalize across datasets and optimizers; and the latter investigates LTH in large pre-trained NLP models, and demonstrates the winning ticket transferability across downstream tasks."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "5c0705d856eb18666db4318cf76416560764a856",
                "externalIds": {
                    "DBLP": "conf/nips/ChenCWGLW21",
                    "ArXiv": "2103.16547",
                    "CorpusId": 232417266
                },
                "corpusId": 232417266,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5c0705d856eb18666db4318cf76416560764a856",
                "title": "The Elastic Lottery Ticket Hypothesis",
                "abstract": "Lottery Ticket Hypothesis (LTH) raises keen attention to identifying sparse trainable subnetworks, or winning tickets, which can be trained in isolation to achieve similar or even better performance compared to the full models. Despite many efforts being made, the most effective method to identify such winning tickets is still Iterative Magnitude-based Pruning (IMP), which is computationally expensive and has to be run thoroughly for every different network. A natural question that comes in is: can we\"transform\"the winning ticket found in one network to another with a different architecture, yielding a winning ticket for the latter at the beginning, without re-doing the expensive IMP? Answering this question is not only practically relevant for efficient\"once-for-all\"winning ticket finding, but also theoretically appealing for uncovering inherently scalable sparse patterns in networks. We conduct extensive experiments on CIFAR-10 and ImageNet, and propose a variety of strategies to tweak the winning tickets found from different networks of the same model family (e.g., ResNets). Based on these results, we articulate the Elastic Lottery Ticket Hypothesis (E-LTH): by mindfully replicating (or dropping) and re-ordering layers for one network, its corresponding winning ticket could be stretched (or squeezed) into a subnetwork for another deeper (or shallower) network from the same family, whose performance is nearly the same competitive as the latter's winning ticket directly found by IMP. We have also extensively compared E-LTH with pruning-at-initialization and dynamic sparse training methods, as well as discussed the generalizability of E-LTH to different model families, layer types, and across datasets. Code is available at https://github.com/VITA-Group/ElasticLTH.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "66273769",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "153655416",
                        "name": "Yu Cheng"
                    },
                    {
                        "authorId": "2992833",
                        "name": "Shuohang Wang"
                    },
                    {
                        "authorId": "144702900",
                        "name": "Zhe Gan"
                    },
                    {
                        "authorId": "46700348",
                        "name": "Jingjing Liu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "fa3bc410e0dd057642ecc484645acdbacfbe7d2e",
                "externalIds": {
                    "DBLP": "conf/ijcai/WangQBZF22",
                    "ArXiv": "2103.06460",
                    "DOI": "10.24963/ijcai.2022/786",
                    "CorpusId": 247012121
                },
                "corpusId": 247012121,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/fa3bc410e0dd057642ecc484645acdbacfbe7d2e",
                "title": "Recent Advances on Neural Network Pruning at Initialization",
                "abstract": "Neural network pruning typically removes connections or neurons from a pretrained converged model; while a new pruning paradigm, pruning at initialization (PaI), attempts to prune a randomly initialized network. This paper offers the first survey concentrated on this emerging pruning fashion. We first introduce a generic formulation of neural network pruning, followed by the major classic pruning topics. Then, as the main body of this paper, a thorough and structured literature review of PaI methods is presented, consisting of two major tracks (sparse training and sparse selection). Finally, we summarize the surge of PaI compared to PaT and discuss the open problems. Apart from the dedicated literature review, this paper also offers a code base for easy sanity-checking and benchmarking of different PaI methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2113269100",
                        "name": "Huan Wang"
                    },
                    {
                        "authorId": "12282768",
                        "name": "Can Qin"
                    },
                    {
                        "authorId": "153802755",
                        "name": "Yue Bai"
                    },
                    {
                        "authorId": "2129519081",
                        "name": "Yulun Zhang"
                    },
                    {
                        "authorId": "2156255943",
                        "name": "Yun Fu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The lottery ticket literature [8, 62, 34, 42, 10] regards a dense network as a set of hypotheses (subnetworks)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6c315ee8546702037bef3276529838858c7a00ad",
                "externalIds": {
                    "DBLP": "conf/cvpr/0001SD21",
                    "ArXiv": "2103.05152",
                    "DOI": "10.1109/CVPR46437.2021.01265",
                    "CorpusId": 232168799
                },
                "corpusId": 232168799,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6c315ee8546702037bef3276529838858c7a00ad",
                "title": "Knowledge Evolution in Neural Networks",
                "abstract": "Deep learning relies on the availability of a large corpus of data (labeled or unlabeled). Thus, one challenging unsettled question is: how to train a deep network on a relatively small dataset? To tackle this question, we propose an evolution-inspired training approach to boost performance on relatively small datasets. The knowledge evolution (KE) approach splits a deep network into two hypotheses: the fit-hypothesis and the reset-hypothesis. We iteratively evolve the knowledge inside the fit-hypothesis by perturbing the reset-hypothesis for multiple generations. This approach not only boosts performance, but also learns a slim network with a smaller inference cost. KE integrates seamlessly with both vanilla and residual convolutional networks. KE reduces both overfitting and the burden for data collection.We evaluate KE on various network architectures and loss functions. We evaluate KE using relatively small datasets (e.g., CUB-200) and randomly initialized deep net-works. KE achieves an absolute 21% improvement margin on a state-of-the-art baseline. This performance improvement is accompanied by a relative 73% reduction in inference cost. KE achieves state-of-the-art results on classification and metric learning benchmarks. Code available at http://bit.ly/3ulgwyb",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1685519",
                        "name": "Ahmed Taha"
                    },
                    {
                        "authorId": "1781242",
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "authorId": "1693428",
                        "name": "L. Davis"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6c48ed6999b3415e2a92386658bf2f74ddabc458",
                "externalIds": {
                    "ArXiv": "2102.11068",
                    "DBLP": "conf/icml/0007YCSMJRT0W21",
                    "CorpusId": 235826360
                },
                "corpusId": 235826360,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6c48ed6999b3415e2a92386658bf2f74ddabc458",
                "title": "Lottery Ticket Preserves Weight Correlation: Is It Desirable or Not?",
                "abstract": "In deep model compression, the recent finding\"Lottery Ticket Hypothesis\"(LTH) (Frankle&Carbin, 2018) pointed out that there could exist a winning ticket (i.e., a properly pruned sub-network together with original weight initialization) that can achieve competitive performance than the original dense network. However, it is not easy to observe such winning property in many scenarios, where for example, a relatively large learning rate is used even if it benefits training the original dense model. In this work, we investigate the underlying condition and rationale behind the winning property, and find that the underlying reason is largely attributed to the correlation between initialized weights and final-trained weights when the learning rate is not sufficiently large. Thus, the existence of winning property is correlated with an insufficient DNN pretraining, and is unlikely to occur for a well-trained DNN. To overcome this limitation, we propose the\"pruning&fine-tuning\"method that consistently outperforms lottery ticket sparse training under the same pruning algorithm and the same total training epochs. Extensive experiments over multiple deep models (VGG, ResNet, MobileNet-v2) on different datasets have been conducted to justify our proposals.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2152354569",
                        "name": "Ning Liu"
                    },
                    {
                        "authorId": "9347641",
                        "name": "Geng Yuan"
                    },
                    {
                        "authorId": "1939695",
                        "name": "Zhengping Che"
                    },
                    {
                        "authorId": "2007668856",
                        "name": "Xuan Shen"
                    },
                    {
                        "authorId": "151480882",
                        "name": "Xiaolong Ma"
                    },
                    {
                        "authorId": "153792333",
                        "name": "Qing Jin"
                    },
                    {
                        "authorId": "2111473627",
                        "name": "Jian Ren"
                    },
                    {
                        "authorId": "2115854503",
                        "name": "Jian Tang"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "46393431",
                        "name": "Yanzhi Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As baselines, we consider Global (Morcos et al., 2019), Uniform (Zhu and Gupta, 2017), and Adaptive (Gale et al., 2019) pruning techniques and LAMP (Lee et al., 2021).",
                "Layerwise pruning ratio has also been investigated for NNs pruned at initialization since the explosion of the Lottery Ticket Hypothesis (Frankle and Carbin, 2019; Morcos et al., 2019).",
                "As baselines, we consider Global (Morcos et al., 2019), Uniform (Zhu and Gupta, 2017), and Adaptive (Gale et al."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "08df7bc07135cc58c1f2614c5cd2cbb312915a18",
                "externalIds": {
                    "DBLP": "conf/aistats/IsikWN22",
                    "ArXiv": "2102.08329",
                    "CorpusId": 246705938
                },
                "corpusId": 246705938,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/08df7bc07135cc58c1f2614c5cd2cbb312915a18",
                "title": "An Information-Theoretic Justification for Model Pruning",
                "abstract": "We study the neural network (NN) compression problem, viewing the tension between the compression ratio and NN performance through the lens of rate-distortion theory. We choose a distortion metric that reflects the effect of NN compression on the model output and derive the tradeoff between rate (compression) and distortion. In addition to characterizing theoretical limits of NN compression, this formulation shows that \\emph{pruning}, implicitly or explicitly, must be a part of a good compression algorithm. This observation bridges a gap between parts of the literature pertaining to NN and data compression, respectively, providing insight into the empirical success of model pruning. Finally, we propose a novel pruning strategy derived from our information-theoretic formulation and show that it outperforms the relevant baselines on CIFAR-10 and ImageNet datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1707440322",
                        "name": "Berivan Isik"
                    },
                    {
                        "authorId": "4820756",
                        "name": "T. Weissman"
                    },
                    {
                        "authorId": "3268846",
                        "name": "Albert No"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(Morcos et al. 2019) showed successful results of model compression by generalized lottery ticket hypothesis across different benchmark datasets and popular DNN architectures.",
                "Morcos et al. (Morcos et al. 2019) showed successful results of model compression by generalized lottery ticket hypothesis across different benchmark datasets and popular DNN architectures."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "93c6fd09050833625801c8b7f85d32f2ebb0d1fb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-04073",
                    "ArXiv": "2101.04073",
                    "DOI": "10.1609/aaai.v35i17.17780",
                    "CorpusId": 231572859
                },
                "corpusId": 231572859,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/93c6fd09050833625801c8b7f85d32f2ebb0d1fb",
                "title": "Deeplite Neutrino: An End-to-End Framework for Constrained Deep Learning Model Optimization",
                "abstract": "Designing deep learning-based solutions is becoming a race for training deeper models with a greater number of layers.\nWhile a large-size deeper model could provide competitive accuracy, it creates a lot of logistical challenges and unreasonable resource requirements during development and deployment. This has been one of the key reasons for deep learning models not being excessively used in various production environments, especially in edge devices. There is an immediate requirement for optimizing and compressing these deep learning models, to enable on-device intelligence.\nIn this research, we introduce a black-box framework, Deeplite Neutrino^{TM} for production-ready optimization of deep learning models. The framework provides an easy mechanism for the end-users to provide constraints such as a tolerable drop in accuracy or target size of the optimized models, to guide the whole optimization process. The framework is easy to include in an existing production pipeline and is available as a Python Package, supporting PyTorch and Tensorflow libraries. The optimization performance of the framework is shown across multiple benchmark datasets and popular deep learning models. Further, the framework is currently used in production and the results and testimonials from several clients are summarized.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2473935",
                        "name": "A. Sankaran"
                    },
                    {
                        "authorId": "3422889",
                        "name": "Olivier Mastropietro"
                    },
                    {
                        "authorId": "1682819",
                        "name": "Ehsan Saboori"
                    },
                    {
                        "authorId": "3416143",
                        "name": "Yasser Idris"
                    },
                    {
                        "authorId": "2046243926",
                        "name": "Davis Sawyer"
                    },
                    {
                        "authorId": "1610857529",
                        "name": "Mohammadhossein Askarihemmat"
                    },
                    {
                        "authorId": "25161413",
                        "name": "G. B. Hacene"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Morcos et al. (2019) studies the transferability of winning tickets between datasets and optimizers."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0c9d97d2ba489256d4f1760598dc2c7be6d90d96",
                "externalIds": {
                    "ACL": "2021.acl-long.171",
                    "DBLP": "journals/corr/abs-2101-00063",
                    "ArXiv": "2101.00063",
                    "DOI": "10.18653/v1/2021.acl-long.171",
                    "CorpusId": 230438816
                },
                "corpusId": 230438816,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/0c9d97d2ba489256d4f1760598dc2c7be6d90d96",
                "title": "EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets",
                "abstract": "Heavily overparameterized language models such as BERT, XLNet and T5 have achieved impressive success in many NLP tasks. However, their high model complexity requires enormous computation resources and extremely long training time for both pre-training and fine-tuning. Many works have studied model compression on large NLP models, but only focusing on reducing inference time while still requiring an expensive training process. Other works use extremely large batch sizes to shorten the pre-training time, at the expense of higher computational resource demands. In this paper, inspired by the Early-Bird Lottery Tickets recently studied for computer vision tasks, we propose EarlyBERT, a general computationally-efficient training algorithm applicable to both pre-training and fine-tuning of large-scale language models. By slimming the self-attention and fully-connected sub-layers inside a transformer, we are the first to identify structured winning tickets in the early stage of BERT training. We apply those tickets towards efficient BERT training, and conduct comprehensive pre-training and fine-tuning experiments on GLUE and SQuAD downstream tasks. Our results show that EarlyBERT achieves comparable performance to standard BERT, with 35 45% less training time. Code is available at https://github.com/VITA-Group/EarlyBERT.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "66273769",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "153655416",
                        "name": "Yu Cheng"
                    },
                    {
                        "authorId": "2992833",
                        "name": "Shuohang Wang"
                    },
                    {
                        "authorId": "144702900",
                        "name": "Zhe Gan"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "46700348",
                        "name": "Jingjing Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[51, 50] showed those matching subnetworks to transfer between related classification tasks."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "5f6fccc32953f57fe29b2316eb8351e84b0179dc",
                "externalIds": {
                    "MAG": "3111921445",
                    "DBLP": "journals/corr/abs-2012-06908",
                    "ArXiv": "2012.06908",
                    "DOI": "10.1109/CVPR46437.2021.01604",
                    "CorpusId": 229152261
                },
                "corpusId": 229152261,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5f6fccc32953f57fe29b2316eb8351e84b0179dc",
                "title": "The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models",
                "abstract": "The computer vision world has been re-gaining enthusiasm in various pre-trained models, including both classical ImageNet supervised pre-training and recently emerged self-supervised pre-training such as simCLR [10] and MoCo [40]. Pre-trained weights often boost a wide range of downstream tasks including classification, detection, and segmentation. Latest studies suggest that pre-training benefits from gigantic model capacity [11]. We are hereby curious and ask: after pre-training, does a pre-trained model indeed have to stay large for its downstream transferability? In this paper, we examine supervised and self-supervised pre-trained models through the lens of the lottery ticket hypothesis (LTH) [31]. LTH identifies highly sparse matching subnetworks that can be trained in isolation from (nearly) scratch yet still reach the full models' performance. We extend the scope of LTH and question whether matching subnetworks still exist in pre-trained computer vision models, that enjoy the same downstream transfer performance. Our extensive experiments convey an overall positive message: from all pre-trained weights obtained by ImageNet classification, simCLR, and MoCo, we are consistently able to locate such matching subnetworks at 59.04% to 96.48% sparsity that transfer universally to multiple downstream tasks, whose performance see no degradation compared to using full pre-trained weights. Further analyses reveal that subnetworks found from different pre-training tend to yield diverse mask structures and perturbation sensitivities. We conclude that the core LTH observations remain generally relevant in the pre-training paradigm of computer vision, but more delicate discussions are needed in some cases. Codes and pre-trained models will be made available at: https://github.com/VITA-Group/CV_LTH_Pre-training.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2648459",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "3307026",
                        "name": "Shiyu Chang"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "37873860",
                        "name": "Yang Zhang"
                    },
                    {
                        "authorId": "1701041",
                        "name": "Michael Carbin"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026the performance of TabTransformer against following four categories of methods: (a) Logistic\nregression and GBDT (b) MLP and a sparse MLP following (Morcos et al. 2019) (c) TabNet model of Arik and Pfister (2019) (d) and the Variational Information Bottleneck model (VIB) of Alemi et al. (2017).",
                "regression and GBDT (b) MLP and a sparse MLP following (Morcos et al. 2019) (c) TabNet model of Arik and Pfister (2019) (d) and the Variational Information Bottleneck model (VIB) of Alemi et al."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a2ec47b9bcc95d2456a8a42199233e5d9129ef18",
                "externalIds": {
                    "MAG": "3111356309",
                    "DBLP": "journals/corr/abs-2012-06678",
                    "ArXiv": "2012.06678",
                    "CorpusId": 229156048
                },
                "corpusId": 229156048,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a2ec47b9bcc95d2456a8a42199233e5d9129ef18",
                "title": "TabTransformer: Tabular Data Modeling Using Contextual Embeddings",
                "abstract": "We propose TabTransformer, a novel deep tabular data modeling architecture for supervised and semi-supervised learning. The TabTransformer is built upon self-attention based Transformers. The Transformer layers transform the embeddings of categorical features into robust contextual embeddings to achieve higher prediction accuracy. Through extensive experiments on fifteen publicly available datasets, we show that the TabTransformer outperforms the state-of-the-art deep learning methods for tabular data by at least 1.0% on mean AUC, and matches the performance of tree-based ensemble models. Furthermore, we demonstrate that the contextual embeddings learned from TabTransformer are highly robust against both missing and noisy data features, and provide better interpretability. Lastly, for the semi-supervised setting we develop an unsupervised pre-training procedure to learn data-driven contextual embeddings, resulting in an average 2.1% AUC lift over the state-of-the-art methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2152663837",
                        "name": "Xin Huang"
                    },
                    {
                        "authorId": "3083159",
                        "name": "A. Khetan"
                    },
                    {
                        "authorId": "2008176653",
                        "name": "Milan Cvitkovic"
                    },
                    {
                        "authorId": "3386660",
                        "name": "Zohar S. Karnin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although some of these recent works have tried to answer the question \u2013 how well do the tickets transfer across domains [38, 37], when it comes to vision tasks \u2013 the buck stops at image classification.",
                "[37, 38] show that winning tickets transfer well across datasets.",
                "However, the study in [37] was limited to smaller datasets, like CIFAR-10 and FashionMNIST, and both [37, 38] are limited to classification tasks.",
                "We also investigate the efficacy of methods introduced by [12, 38, 13, 42] such as iterative magnitude pruning, late resetting, early bird training, and layerwise pruning in the context of object recognition.",
                "This is in contrast with previous works related to ticket transfer in vision models [37, 38].",
                "[38] scrutinize the generalization properties of winning tickets and offer empirical evidence that winning tickets can be transferred across datasets and optimizers, in the realm of image classification."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "65c826271c6d352d92d0519a569763d6ff913c27",
                "externalIds": {
                    "DBLP": "conf/cvpr/GirishMGCDS21",
                    "MAG": "3111265704",
                    "ArXiv": "2012.04643",
                    "DOI": "10.1109/CVPR46437.2021.00082",
                    "CorpusId": 227739001
                },
                "corpusId": 227739001,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/65c826271c6d352d92d0519a569763d6ff913c27",
                "title": "The Lottery Ticket Hypothesis for Object Recognition",
                "abstract": "Recognition tasks, such as object recognition and key-point estimation, have seen widespread adoption in recent years. Most state-of-the-art methods for these tasks use deep networks that are computationally expensive and have huge memory footprints. This makes it exceedingly difficult to deploy these systems on low power embedded devices. Hence, the importance of decreasing the storage requirements and the amount of computation in such models is paramount. The recently proposed Lottery Ticket Hypothesis (LTH) states that deep neural networks trained on large datasets contain smaller subnetworks that achieve on par performance as the dense networks. In this work, we perform the first empirical study investigating LTH for model pruning in the context of object detection, instance segmentation, and keypoint estimation. Our studies reveal that lottery tickets obtained from Imagenet pretraining do not transfer well to the downstream tasks. We provide guidance on how to find lottery tickets with up to 80% overall sparsity on different sub-tasks without incurring any drop in the performance. Finally, we analyse the behavior of trained tickets with respect to various task attributes such as object size, frequency, and difficulty of detection.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "143720888",
                        "name": "Sharath Girish"
                    },
                    {
                        "authorId": "51469126",
                        "name": "Shishira R. Maiya"
                    },
                    {
                        "authorId": "145428082",
                        "name": "Kamal Gupta"
                    },
                    {
                        "authorId": "50688939",
                        "name": "Hao Chen"
                    },
                    {
                        "authorId": "1693428",
                        "name": "L. Davis"
                    },
                    {
                        "authorId": "1781242",
                        "name": "Abhinav Shrivastava"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[28] observed that an existing set of winning tickets from a dataset can be applied to a different dataset and with good performance."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "bc0943b40274267482c9581bcd4f132fc26ced7c",
                "externalIds": {
                    "DBLP": "conf/intcompsymp/LiaoLW20",
                    "DOI": "10.1109/ICS51289.2020.00025",
                    "CorpusId": 232062392
                },
                "corpusId": 232062392,
                "publicationVenue": {
                    "id": "914d4734-285a-4cd8-bba1-eec0db26a512",
                    "name": "International Conference on Supercomputing",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference Computer, Communication and Computational Sciences",
                        "ICS",
                        "INFORMS Comput Soc",
                        "Int Conf Supercomput",
                        "INFORMS Computing Society",
                        "Int Conf Comput Commun Comput Sci"
                    ],
                    "url": "http://portal.acm.org/proceedings/ics/"
                },
                "url": "https://www.semanticscholar.org/paper/bc0943b40274267482c9581bcd4f132fc26ced7c",
                "title": "Convolution Filter Pruning for Transfer Learning on Small Dataset",
                "abstract": "In this paper, we propose a scheme to reduce the size of a pre-trained full-scale model with a domain-specific dataset. This scheme combines model compression and transfer learning. First, it identifies the sensitive parts of a full model using the target dataset. Then it applies transfer learning on the identified part of the network to construct a reduced and customized model. Our scheme can correct structure and parameters to prune for a target dataset, which makes the following transfer learning more efficient.We apply our scheme on image classification applications using convolutional neural networks. We observe that different image categories activate different filters, so we can identify sensitive parts of the model for different categories of images.We use VGG-16 (pre-trained by ImageNet dataset) as the full model and we apply transfer learning on two different datasets (Flowers-102 and Cats vs. Dogs). We apply three pruning criteria in our scheme. The accuracy of Flowers-102 dataset by the best criterion drops less than 2% while pruning 60% of the network. We also observe the effects of different settings of our scheme and examine their feasibility by experiments.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2051652474",
                        "name": "Ching Ya Liao"
                    },
                    {
                        "authorId": "38244340",
                        "name": "Pangfeng Liu"
                    },
                    {
                        "authorId": "1726584",
                        "name": "Jan-Jan Wu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "85a13929d9919ddcf2d133520d560aca1036e943",
                "externalIds": {
                    "ArXiv": "2011.14439",
                    "DBLP": "journals/corr/abs-2011-14439",
                    "MAG": "3109872205",
                    "CorpusId": 227228923
                },
                "corpusId": 227228923,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/85a13929d9919ddcf2d133520d560aca1036e943",
                "title": "Scaling *down* Deep Learning",
                "abstract": "Though deep learning models have taken on commercial and political relevance, many aspects of their training and operation remain poorly understood. This has sparked interest in \"science of deep learning\" projects, many of which are run at scale and require enormous amounts of time, money, and electricity. But how much of this research really needs to occur at scale? In this paper, we introduce MNIST-1D: a minimalist, low-memory, and low-compute alternative to classic deep learning benchmarks. The training examples are 20 times smaller than MNIST examples yet they differentiate more clearly between linear, nonlinear, and convolutional models which attain 32, 68, and 94% accuracy respectively (these models obtain 94, 99+, and 99+% on MNIST). Then we present example use cases which include measuring the spatial inductive biases of lottery tickets, observing deep double descent, and metalearning an activation function.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "14851288",
                        "name": "S. Greydanus"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e4ca4f0cf2d8dc5327c04e193064297fd63f1f60",
                "externalIds": {
                    "DBLP": "journals/jimaging/TessierGLAHB22",
                    "PubMedCentral": "8950981",
                    "ArXiv": "2011.10520",
                    "DOI": "10.3390/jimaging8030064",
                    "CorpusId": 247303139,
                    "PubMed": "35324619"
                },
                "corpusId": 247303139,
                "publicationVenue": {
                    "id": "c0fc53c7-b0ed-487d-9191-1262c8322621",
                    "name": "Journal of Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "J Imaging"
                    ],
                    "issn": "2313-433X",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-556372",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/jimaging",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-556372"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e4ca4f0cf2d8dc5327c04e193064297fd63f1f60",
                "title": "Rethinking Weight Decay for Efficient Neural Network Pruning",
                "abstract": "Introduced in the late 1980s for generalization purposes, pruning has now become a staple for compressing deep neural networks. Despite many innovations in recent decades, pruning approaches still face core issues that hinder their performance or scalability. Drawing inspiration from early work in the field, and especially the use of weight decay to achieve sparsity, we introduce Selective Weight Decay (SWD), which carries out efficient, continuous pruning throughout training. Our approach, theoretically grounded on Lagrangian smoothing, is versatile and can be applied to multiple tasks, networks, and pruning structures. We show that SWD compares favorably to state-of-the-art approaches, in terms of performance-to-parameters ratio, on the CIFAR-10, Cora, and ImageNet ILSVRC2012 datasets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "146940144",
                        "name": "Hugo Tessier"
                    },
                    {
                        "authorId": "144916029",
                        "name": "Vincent Gripon"
                    },
                    {
                        "authorId": "27532368",
                        "name": "Mathieu L\u00e9onardon"
                    },
                    {
                        "authorId": "2409852",
                        "name": "M. Arzel"
                    },
                    {
                        "authorId": "3312711",
                        "name": "T. Hannagan"
                    },
                    {
                        "authorId": "2060222860",
                        "name": "David Bertrand"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(2019) explore a number of other interesting questions [69]:",
                "Morcos et al.(2019) carry out experiments in which lottery tickets are identified using one optimizer, ADAM (adaptive moment estimation), and then utilise a different optimizer, SGD (Stochastic Gradient Descent) with momentum, and vice versa on the CIFAR10 data.",
                "in general, winning tickets are optimizer independent [69].",
                "Morcos et al.(2019) explore a number of other interesting questions [69]: \u2022 Are the lotteries found for one image classification task\ntransferable to other tasks?"
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "0fa13a5ef36168ff3fd08b03fd30f1f935d6a18a",
                "externalIds": {
                    "ArXiv": "2011.00241",
                    "DBLP": "journals/access/VaderaA22",
                    "MAG": "3096215947",
                    "DOI": "10.1109/ACCESS.2022.3182659",
                    "CorpusId": 226226764
                },
                "corpusId": 226226764,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0fa13a5ef36168ff3fd08b03fd30f1f935d6a18a",
                "title": "Methods for Pruning Deep Neural Networks",
                "abstract": "This paper presents a survey of methods for pruning deep neural networks. It begins by categorising over 150 studies based on the underlying approach used and then focuses on three categories: methods that use magnitude based pruning, methods that utilise clustering to identify redundancy, and methods that use sensitivity analysis to assess the effect of pruning. Some of the key influencing studies within these categories are presented to highlight the underlying approaches and results achieved. Most studies present results which are distributed in the literature as new architectures, algorithms and data sets have developed with time, making comparison across different studied difficult. The paper therefore provides a resource for the community that can be used to quickly compare the results from many different methods on a variety of data sets, and a range of architectures, including AlexNet, ResNet, DenseNet and VGG. The resource is illustrated by comparing the results published for pruning AlexNet and ResNet50 on ImageNet and ResNet56 and VGG16 on the CIFAR10 data to reveal which pruning methods work well in terms of retaining accuracy whilst achieving good compression rates. The paper concludes by identifying some research gaps and promising directions for future research.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3197685",
                        "name": "S. Vadera"
                    },
                    {
                        "authorId": "32051512",
                        "name": "Salem Ameen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, other work has shown that such subnetworks generalize well across datasets and tasks (Morcos et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2bb359e9c10612c525e85c98f8e1f193204c0fe5",
                "externalIds": {
                    "ArXiv": "2010.11354",
                    "DBLP": "conf/icml/PatilD21",
                    "CorpusId": 235620958
                },
                "corpusId": 235620958,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2bb359e9c10612c525e85c98f8e1f193204c0fe5",
                "title": "PHEW : Constructing Sparse Networks that Learn Fast and Generalize Well without Training Data",
                "abstract": "Methods that sparsify a network at initialization are important in practice because they greatly improve the efficiency of both learning and inference. Our work is based on a recently proposed decomposition of the Neural Tangent Kernel (NTK) that has decoupled the dynamics of the training process into a data-dependent component and an architecture-dependent kernel - the latter referred to as Path Kernel. That work has shown how to design sparse neural networks for faster convergence, without any training data, using the Synflow-L2 algorithm. We first show that even though Synflow-L2 is optimal in terms of convergence, for a given network density, it results in sub-networks with\"bottleneck\"(narrow) layers - leading to poor performance as compared to other data-agnostic methods that use the same number of parameters. Then we propose a new method to construct sparse networks, without any training data, referred to as Paths with Higher-Edge Weights (PHEW). PHEW is a probabilistic network formation method based on biased random walks that only depends on the initial weights. It has similar path kernel properties as Synflow-L2 but it generates much wider layers, resulting in better generalization and performance. PHEW achieves significant improvements over the data-independent SynFlow and SynFlow-L2 methods at a wide range of network densities.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "21819184",
                        "name": "S. M. Patil"
                    },
                    {
                        "authorId": "144734756",
                        "name": "C. Dovrolis"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "545cb955cb1a39e9e7615885fde5b2e4d88bb9dc",
                "externalIds": {
                    "MAG": "3106038021",
                    "DBLP": "journals/ijis/XiangWSLLL21",
                    "DOI": "10.1002/int.22302",
                    "CorpusId": 227249517
                },
                "corpusId": 227249517,
                "publicationVenue": {
                    "id": "05528bac-d212-46a6-9c84-314d4bd77368",
                    "name": "International Journal of Intelligent Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Intell Syst"
                    ],
                    "issn": "0884-8173",
                    "url": "http://www3.interscience.wiley.com/cgi-bin/jhome/36062",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/1098111X"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/545cb955cb1a39e9e7615885fde5b2e4d88bb9dc",
                "title": "One\u2010dimensional convolutional neural networks for high\u2010resolution range profile recognition via adaptively feature recalibrating and automatically channel pruning",
                "abstract": "High\u2010resolution range profile (HRRP) has obtained intensive attention in radar target recognition and convolutional neural networks (CNNs) are among predominant approaches to deal with HRRP recognition problems. However, most CNNs are designed by the rule\u2010of\u2010thumb and suffer from much more computational complexity. Aiming at enhancing the channels of one\u2010dimensional CNN (1D\u2010CNN) for extracting efficient structural information oftargets form HRRP and reducing the computation complexity, we propose a novel framework for HRRP\u2010based target recognition based on 1D\u2010CNN with channel attention and channel pruning. By introducing an aggregation\u2010perception\u2010recalibration (APR) block for channel attention to the 1D\u2010CNN backbone, channels in each 1D convolutional layer can adaptively learn to recalibrate the extracted features for enhancing the structural information captured from HRRP. To avoid rule\u2010of\u2010thumb design and reduce the computation complexity of 1D\u2010CNN, we proposed a new method incorporated withthe global best leading artificial bee colony (GBL\u2010ABC) to prune the original network based on the lottery ticket hypothesis in an automatic and heuristic manner. The extensive experimental results on the measured data illustrate that the proposed algorithm achievesthe superiorrecognition rate by combing APR and GBL\u2010ABC simultaneously.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144578027",
                        "name": "Qian Xiang"
                    },
                    {
                        "authorId": "145095349",
                        "name": "Xiaodan Wang"
                    },
                    {
                        "authorId": "2839196",
                        "name": "Yafei Song"
                    },
                    {
                        "authorId": "143846034",
                        "name": "Lei Lei"
                    },
                    {
                        "authorId": null,
                        "name": "Rui Li"
                    },
                    {
                        "authorId": "1500523550",
                        "name": "Jie Lai"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As an additional experiment, we test whether LAMP can provide a general-purpose layerwise sparsity for pruning schemes other than MP.",
                "With the effectiveness of LAMP confirmed, we take a closer look at the layerwise sparsity discovered by LAMP.",
                "One common method is the global MP criteria (see, e.g., Morcos et al. (2019)), \u2217Work done at KAIST 1i.e., simultaneously training and pruning\nar X\niv :2\n01 0.",
                "Lastly, we observe that the heuristic of Gale et al. (2019) seems to provide an improvement over the Uniform MP.",
                "5, we plot the layerwise survival rates and the number of nonzero weights discovered by iteratively pruning VGG-16 (trained on CIFAR-10), by Global MP, Erdo\u030bs-Re\u0301nyi kernel, and LAMP.",
                "In Section 4, we empirically validate the effectiveness and versatility of LAMP.",
                "We note that the gap between one-shot pruning and iterative pruning is quite small for LAMP; when 1.15% of all prunable weights survive, iterative LAMP brings only 1.09% accuracy gain over one-shot LAMP.",
                "First, the right-hand side is free of any activation function, and is equivalent to the layerwise MP.",
                "Frankle & Carbin (2019) combine MP with weight rewinding to discover efficiently trainable subnetworks: for small nets, the authors employ uniform layerwise sparsity, but use different rates for convolutional layers and fully-connected layers (with an added heuristic on the last fully-connected layer); for larger nets, authors use global MP. Morcos et al. (2019) consider transferring the \u201cwinning ticket\u201d initializations, using the global MP. Evci et al. (2020) proposes a training scheme for sparsely initialized neural networks, where the layerwise sparsity is given by the Erdo\u030bs-Re\u0301nyi kernel method; the method generalizes the scheme initially proposed by Mocanu et al. (2018) to convolutional neural networks.",
                "A global threshold on the weight magnitudes is imposed on every layer to meet the global sparsity constraint, and the layerwise sparsity is automatically determined according to this threshold; see, e.g., Morcos et al. (2019).",
                "In search of a \u201cgo-to\u201d layerwise sparsity for MP, we take a model-level distortion minimization perspective towards MP.",
                "We then observe that the problem can be relaxed to the minimization of Frobenius distortion in the weight tensor, whose solution coincides with the layerwise MP. Formally, let \u03be \u2208 Rn be an input vector to a fully-connected layer with the weight tensor W \u2208 Rm\u00d7n.",
                "We sort and prune as in global MP, taking O(n log n) steps.7\nWe observe that step 4 is the dominant term, which is shared by the global MP.",
                "\u2026convolutional layers and fully-connected layers (with an added heuristic on the last fully-connected layer); for larger nets, authors use global MP. Morcos et al. (2019) consider transferring the \u201cwinning ticket\u201d initializations, using the global MP. Evci et al. (2020) proposes a training scheme\u2026"
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "9227d5897abbf297a34d447e94a802a714b8eab2",
                "externalIds": {
                    "DBLP": "conf/iclr/LeePMAS21",
                    "ArXiv": "2010.07611",
                    "CorpusId": 234358843
                },
                "corpusId": 234358843,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9227d5897abbf297a34d447e94a802a714b8eab2",
                "title": "Layer-adaptive Sparsity for the Magnitude-based Pruning",
                "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on\"how to choose,\"the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation. Under various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection. Furthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48173961",
                        "name": "Jaeho Lee"
                    },
                    {
                        "authorId": "2115258625",
                        "name": "Sejun Park"
                    },
                    {
                        "authorId": "9962692",
                        "name": "Sangwoo Mo"
                    },
                    {
                        "authorId": "70560338",
                        "name": "Sungsoo Ahn"
                    },
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Morcos et al. (2019) consider transferring the \u201cwinning ticket\u201d initializations, and observe that global MP tend to work better with weight rewinding.",
                "A global threshold on the weight magnitudes is imposed on every layers to meet the global sparsity constraint, and the layerwise sparsity is automatically determined according to this threshold; see, e.g., Han et al. (2015); Morcos et al. (2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "04341104814cf8b99ed75391e7f7e0a2773208ba",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-07611",
                    "MAG": "3092972975",
                    "CorpusId": 222378502
                },
                "corpusId": 222378502,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/04341104814cf8b99ed75391e7f7e0a2773208ba",
                "title": "A Deeper Look at the Layerwise Sparsity of Magnitude-based Pruning",
                "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on \"how to choose,\" the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation. Under diverse datasets and models, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection. Furthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48173961",
                        "name": "Jaeho Lee"
                    },
                    {
                        "authorId": "2115258625",
                        "name": "Sejun Park"
                    },
                    {
                        "authorId": "9962692",
                        "name": "Sangwoo Mo"
                    },
                    {
                        "authorId": "70560338",
                        "name": "Sungsoo Ahn"
                    },
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Morcos et al. (2019); Sabatelli, Kestemont, and Geurts (2020) showed that LTs trained on large datasets transfer to smaller ones, but not vice versa.",
                "Given the close relationship between LTs and pruning solutions, the observation that LTs trained on large datasets transfer to smaller ones, but not vice versa (Morcos et al. 2019; Sabatelli, Kestemont, and Geurts 2020) can be explained by a common observation in transfer learning: networks trained in large datasets transfer to smaller ones.",
                "\u2026relationship between LTs and pruning solutions, the observation that LTs trained on large datasets transfer to smaller ones, but not vice versa (Morcos et al. 2019; Sabatelli, Kestemont, and Geurts 2020) can be explained by a common observation in transfer learning: networks trained in large\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "917b18b8dad23284c0a42f665f2ba1984fa360de",
                "externalIds": {
                    "DBLP": "conf/aaai/EvciIKD22",
                    "MAG": "3092446983",
                    "ArXiv": "2010.03533",
                    "DOI": "10.1609/aaai.v36i6.20611",
                    "CorpusId": 222177996
                },
                "corpusId": 222177996,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/917b18b8dad23284c0a42f665f2ba1984fa360de",
                "title": "Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win",
                "abstract": "Sparse Neural Networks (NNs) can match the generalization of dense NNs using a fraction of the compute/storage for inference, and have the potential to enable efficient training. However, naively training unstructured sparse NNs from random initialization results in significantly worse generalization, with the notable exceptions of Lottery Tickets (LTs) and Dynamic Sparse Training (DST). In this work, we attempt to answer: (1) why training unstructured sparse networks from random initialization performs poorly and; (2) what makes LTs and DST the exceptions? We show that sparse NNs have poor gradient flow at initialization and propose a modified initialization for unstructured connectivity. Furthermore, we find that DST methods significantly improve gradient flow during training over traditional sparse training methods. Finally, we show that LTs do not improve gradient flow, rather their success lies in re-learning the pruning solution they are derived from \u2014 however, this comes at the cost of learning novel solutions.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3399348",
                        "name": "Utku Evci"
                    },
                    {
                        "authorId": "3067465",
                        "name": "Yani Andrew Ioannou"
                    },
                    {
                        "authorId": "3860190",
                        "name": "Cem Keskin"
                    },
                    {
                        "authorId": "2921469",
                        "name": "Yann Dauphin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "CNN-based architectures such as VGG and ResNet) (Morcos et al. 2019), Reinforcement Learning and Natural Language Processing tasks (Yu et al. 2020).",
                "CNN-based architectures such as VGG and ResNet) (Morcos et al. 2019), Reinforcement Learning and Natural Language Processing tasks (Yu et al.",
                "In deeper networks, it\nhas been shown that IMP with late-rewinding (Frankle et al. 2019; Morcos et al. 2019) is more beneficial than rewinding to initialization.",
                "In practice however, pruning a large fraction of weights through one-shot pruning might null the weights that are actually important to the model leading to a significant drop in the performance (Morcos et al. 2019).",
                "has been shown that IMP with late-rewinding (Frankle et al. 2019; Morcos et al. 2019) is more beneficial than rewinding to initialization."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "12ff508431c627fa01aaaf5779d68b6336dec5d3",
                "externalIds": {
                    "ArXiv": "2010.02350",
                    "MAG": "3092574632",
                    "DBLP": "conf/aaai/KalibhatBF21",
                    "DOI": "10.1609/aaai.v35i9.16980",
                    "CorpusId": 222142784
                },
                "corpusId": 222142784,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/12ff508431c627fa01aaaf5779d68b6336dec5d3",
                "title": "Winning Lottery Tickets in Deep Generative Models",
                "abstract": "The lottery ticket hypothesis suggests that sparse, sub-networks of a given neural network, if initialized properly, can be trained to reach comparable or even better performance to that of the original network. Prior works in lottery tickets have primarily focused on the supervised learning setup, with several papers proposing effective ways of finding winning tickets in classification problems. In this paper, we confirm the existence of winning tickets in deep generative models such as GANs and VAEs. We show that the popular iterative magnitude pruning approach (with late resetting) can be used with generative losses to find the winning tickets. This approach effectively yields tickets with sparsity up to 99% for AutoEncoders, 93% for VAEs and 89% for GANs on CIFAR and Celeb-A datasets. We also demonstrate the transferability of winning tickets across different generative models (GANs and VAEs) sharing the same architecture, suggesting that winning tickets have inductive biases that could help train a wide range of deep generative models. Furthermore, we show the practical benefits of lottery tickets in generative models by detecting tickets at very early stages in training called early-bird tickets. Through early-bird tickets, we can achieve up to 88% reduction in floating-point operations (FLOPs) and 54% reduction in training time, making it possible to train large-scale generative models over tight resource constraints. These results out-perform existing early pruning methods like SNIP (Lee, Ajanthan, and Torr 2019) and GraSP(Wang, Zhang, and Grosse 2020). Our findings shed light towards existence of proper network initializations that could improve convergence and stability of generative models.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35359875",
                        "name": "N. Kalibhat"
                    },
                    {
                        "authorId": "3458479",
                        "name": "Y. Balaji"
                    },
                    {
                        "authorId": "34389431",
                        "name": "S. Feizi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, other work has shown that such subnetworks generalize well across datasets and tasks (Morcos et al. (2019); Tanaka et al. (2020))."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "582f567f69fac60a42b8798b8af020a346409f38",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-11354",
                    "MAG": "3094546561",
                    "CorpusId": 225039799
                },
                "corpusId": 225039799,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/582f567f69fac60a42b8798b8af020a346409f38",
                "title": "PHEW: Paths with higher edge-weights give \"winning tickets\" without training data",
                "abstract": "Sparse neural networks have generated substantial interest recently because they can be more efficient in learning and inference, without any significant drop in performance. The \"lottery ticket hypothesis\" has showed the existence of such sparse subnetworks at initialization. Given a fully-connected initialized architecture, our aim is to find such \"winning ticket\" networks, without any training data. We first show the advantages of forming input-output paths, over pruning individual connections, to avoid bottlenecks in gradient propagation. Then, we show that Paths with Higher Edge-Weights (PHEW) at initialization have higher loss gradient magnitude, resulting in more efficient training. Selecting such paths can be performed without any data. We empirically validate the effectiveness of the proposed approach against pruning-before-training methods on CIFAR10, CIFAR100 and Tiny-ImageNet for VGG-Net and ResNet. PHEW achieves significant improvements on the current state-of-the-art methods at 10\\%, 5\\% and 2\\% network density. We also evaluate the structural similarity relationship between PHEW networks and pruned networks constructed through Iterated Magnitude Pruning (IMP), concluding that the former belong in the family of winning tickets networks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "21819184",
                        "name": "S. M. Patil"
                    },
                    {
                        "authorId": "1687890",
                        "name": "C. Dovrolis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "So it is not surprising to see that Figure A1 in [32] show evidence that the method may pass the layerwise rearrange check.",
                "(9)In the paper [32]\u2019s Appendix A2 part, the authors report applying layerwise rearrange can hurt the performance when the pruning ratio is high.",
                "Similar trends are also reported in Figure 3 of [41], Figure 1b of [32] and Figure 11 of [11]."
            ],
            "isInfluential": false,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "5a04c3d28ae074bf382f8d831e40b6db7c8de509",
                "externalIds": {
                    "DBLP": "conf/nips/SuCCWG0L20",
                    "ArXiv": "2009.11094",
                    "MAG": "3088750390",
                    "CorpusId": 221857593
                },
                "corpusId": 221857593,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5a04c3d28ae074bf382f8d831e40b6db7c8de509",
                "title": "Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot",
                "abstract": "Network pruning is a method for reducing test-time computational resource requirements with minimal performance degradation. Conventional wisdom of pruning algorithms suggests that: (1) Pruning methods exploit information from training data to find good subnetworks; (2) The architecture of the pruned network is crucial for good performance. In this paper, we conduct sanity checks for the above beliefs on several recent unstructured pruning methods and surprisingly find that: (1) A set of methods which aims to find good subnetworks of the randomly-initialized network (which we call \"initial tickets\"), hardly exploits any information from the training data; (2) For the pruned networks obtained by these methods, randomly changing the preserved weights in each layer, while keeping the total number of preserved weights unchanged per layer, does not affect the final performance. These findings inspire us to choose a series of simple \\emph{data-independent} prune ratios for each layer, and randomly prune each layer accordingly to get a subnetwork (which we call \"random tickets\"). Experimental results show that our zero-shot random tickets outperform or attain a similar performance compared to existing \"initial tickets\". In addition, we identify one existing pruning method that passes our sanity checks. We hybridize the ratios in our random ticket with this method and propose a new method called \"hybrid tickets\", which achieves further improvement. (Our code is publicly available at this https URL)",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1962384229",
                        "name": "Jingtong Su"
                    },
                    {
                        "authorId": "2116613902",
                        "name": "Yihang Chen"
                    },
                    {
                        "authorId": "123970124",
                        "name": "Tianle Cai"
                    },
                    {
                        "authorId": "47353876",
                        "name": "Tianhao Wu"
                    },
                    {
                        "authorId": "9659905",
                        "name": "Ruiqi Gao"
                    },
                    {
                        "authorId": "24952249",
                        "name": "Liwei Wang"
                    },
                    {
                        "authorId": "2421201",
                        "name": "J. Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In practice, sparse networks are mainly produced by the network pruning technique [13, 11, 21, 24, 9, 45, 16, 35, 34, 28], to name a few."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "411076fd5d5b4b68822b87b0d67a6fee1d2da44c",
                "externalIds": {
                    "DBLP": "journals/ai/LinSZ22",
                    "ArXiv": "2009.07439",
                    "DOI": "10.1016/j.artint.2022.103739",
                    "CorpusId": 235683627
                },
                "corpusId": 235683627,
                "publicationVenue": {
                    "id": "96018464-22dc-4b5c-a172-c2f4a30ce131",
                    "name": "Artificial Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "Artif Intell"
                    ],
                    "issn": "0004-3702",
                    "alternate_issns": [
                        "2633-1403",
                        "2710-1673",
                        "2710-1681"
                    ],
                    "url": "http://www.elsevier.com/locate/artint",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00043702",
                        "https://www.journals.elsevier.com/artificial-intelligence"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/411076fd5d5b4b68822b87b0d67a6fee1d2da44c",
                "title": "On the landscape of one-hidden-layer sparse networks and beyond",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1812897",
                        "name": "Dachao Lin"
                    },
                    {
                        "authorId": "39447572",
                        "name": "Ruoyu Sun"
                    },
                    {
                        "authorId": "47294286",
                        "name": "Zhihua Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1a0cb11d0d4636766110e71f4825d8c025000c22",
                "externalIds": {
                    "DBLP": "journals/nn/GarciarenaMS20",
                    "MAG": "3087524380",
                    "DOI": "10.1016/j.neunet.2020.09.003",
                    "CorpusId": 221865425,
                    "PubMed": "32961432"
                },
                "corpusId": 221865425,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1a0cb11d0d4636766110e71f4825d8c025000c22",
                "title": "Analysis of the transferability and robustness of GANs evolved for Pareto set approximations",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3440974",
                        "name": "Unai Garciarena"
                    },
                    {
                        "authorId": "2970986",
                        "name": "A. Mendiburu"
                    },
                    {
                        "authorId": "145585784",
                        "name": "Roberto Santana"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Also, the relation of robustness, generalization, and pruning is a popular topic in many studies (Xu and Mannor 2012; Morcos et al. 2019; Arora et al. 2018).",
                "2020), and imposing weights sparsity (Arora et al. 2018; Morcos et al. 2019; Bartoldson et al. 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4fbadebf56f87f6fdbff33cd67800b8c2b6ffdcd",
                "externalIds": {
                    "ArXiv": "2009.05423",
                    "MAG": "3086940325",
                    "DBLP": "journals/corr/abs-2009-05423",
                    "DOI": "10.1007/s10994-021-06049-9",
                    "CorpusId": 221640623
                },
                "corpusId": 221640623,
                "publicationVenue": {
                    "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
                    "name": "Machine-mediated learning",
                    "type": "journal",
                    "alternate_names": [
                        "Mach learn",
                        "Machine Learning",
                        "Mach Learn"
                    ],
                    "issn": "0732-6718",
                    "alternate_issns": [
                        "0885-6125"
                    ],
                    "url": "http://www.springer.com/computer/artificial/journal/10994",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10994",
                        "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4fbadebf56f87f6fdbff33cd67800b8c2b6ffdcd",
                "title": "Achieving adversarial robustness via sparsity",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1991860",
                        "name": "Shu-Fan Wang"
                    },
                    {
                        "authorId": "1940000983",
                        "name": "Ningyi Liao"
                    },
                    {
                        "authorId": "1850278",
                        "name": "Liyao Xiang"
                    },
                    {
                        "authorId": "145163914",
                        "name": "Nanyang Ye"
                    },
                    {
                        "authorId": "22063226",
                        "name": "Quanshi Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note that, in addition to the polynomial terms, the relationship between accuracy and model sparsity is further modeled through an additional exponential term \u2013 a reasonable modeling assumption supported by prior knowledge of accuracy-sparsity curves in the pruning literature [9, 12, 25, 38, 31, 1, 10]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1b24ff8a707bba953a23d85c4971c8925405a180",
                "externalIds": {
                    "ArXiv": "2009.09936",
                    "DBLP": "journals/corr/abs-2009-09936",
                    "MAG": "3088702560",
                    "CorpusId": 221818901
                },
                "corpusId": 221818901,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1b24ff8a707bba953a23d85c4971c8925405a180",
                "title": "Prune Responsibly",
                "abstract": "Irrespective of the speci\ufb01c de\ufb01nition of fairness in a machine learning application, pruning the underlying model affects it. We investigate and document the emer-gence and exacerbation of undesirable per-class performance imbalances, across tasks and architectures, for almost one million categories considered across over 100K image classi\ufb01cation models that undergo a pruning process. We demonstrate the need for transparent reporting, inclusive of bias, fairness, and inclusion metrics, in real-life engineering decision-making around neural network pruning. In response to the calls for quantitative evaluation of AI models to be population-aware, we present neural network pruning as a tangible application domain where the ways in which accuracy-ef\ufb01ciency trade-offs disproportionately affect underrepresented or outlier groups have historically been overlooked. We provide a simple, Pareto-based framework to insert fairness considerations into value-based operating point selection processes, and to re-evaluate pruning technique choices. ways by pruning interventions. We are interested in quantifying the inequality of treatment among classes, cohorts, and individuals as network capacity is reduced. Speci\ufb01cally, the hypothesis we seek to test is that class imbalance and class complexity affect the per-class performance of pruned models. We model and measure the contribution of these factors to the observed changes in performance in pruned models, while controlling for other factors of variations, such as model type, dataset, pruning technique, and choice of rewinding or \ufb01netuning weights after pruning. We demonstrate that, as networks are pruned and sparsity increases, model performance deteriorates more substantially on underrepresented and complex classes than it does on others, thus exacerbating pre-existing disparities among classes. In addition, as model capacity is reduced, networks progressively lose the ability to remain invariant to example properties such as angular orientation in images, which results in higher performance losses for groups, sub-groups, and individuals whose examples exhibit these properties.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35550664",
                        "name": "Michela Paganini"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Thus, for example, neural networks (NN) [33], which require many training samples, are not used in our solution, because the cost of collecting so many samples is prohibitive for HPC workflows."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e4ea92bd1bc15005d772ebacdde909da35c531d0",
                "externalIds": {
                    "DBLP": "conf/sc/ShuGWDFK21",
                    "MAG": "3049146139",
                    "ArXiv": "2008.06991",
                    "DOI": "10.1145/3458817.3476197",
                    "CorpusId": 221139765
                },
                "corpusId": 221139765,
                "publicationVenue": {
                    "id": "0c1420a4-aa9b-44f9-b264-ba3ac5c37050",
                    "name": "International Conference for High Performance Computing, Networking, Storage and Analysis",
                    "alternate_names": [
                        "Int Conf High Perform Comput Netw Storage Anal"
                    ],
                    "issn": "2167-4337",
                    "alternate_issns": [
                        "2167-4329"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000729"
                },
                "url": "https://www.semanticscholar.org/paper/e4ea92bd1bc15005d772ebacdde909da35c531d0",
                "title": "Bootstrapping In-Situ Workflow Auto-Tuning via Combining Performance Models of Component Applications",
                "abstract": "In an in-situ workflow, multiple components such as simulation and analysis applications are coupled with streaming data transfers. The multiplicity of possible configurations necessitates an auto-tuner for workflow optimization. Existing auto-tuning approaches are computationally expensive because many configurations must be sampled by running the whole workflow repeatedly in order to train the auto-tuner surrogate model or otherwise explore the configuration space. To reduce these costs, we instead combine the performance models of component applications by exploiting the analytical workflow structure, selectively generating test configurations to measure and guide the training of a machine learning workflow surrogate model. Because the training can focus on well-performing configurations, the resulting surrogate model can achieve high prediction accuracy for good configurations despite training with fewer total configurations. Experiments with real applications demonstrate that our approach can identify significantly better configurations than other approaches for a fixed computer time budget.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1912873",
                        "name": "Tong Shu"
                    },
                    {
                        "authorId": "1794267",
                        "name": "Yanfei Guo"
                    },
                    {
                        "authorId": "8962431",
                        "name": "J. Wozniak"
                    },
                    {
                        "authorId": "34645435",
                        "name": "Xiaoning Ding"
                    },
                    {
                        "authorId": "1698701",
                        "name": "Ian T Foster"
                    },
                    {
                        "authorId": "1753288",
                        "name": "T. Kur\u00e7"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, the intriguing prospect of ticket transfer [14] could provide such initializations right at the onset of training.",
                "In that case, winning tickets could be transferred and trained on new tasks, even directly from their extremely lightweight versions [14]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ede4279199e7037b2587dd2c1cc83177b0c553c3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-16187",
                    "MAG": "3046412268",
                    "ArXiv": "2007.16187",
                    "DOI": "10.5281/ZENODO.4245492",
                    "CorpusId": 220919670
                },
                "corpusId": 220919670,
                "publicationVenue": {
                    "id": "cfc287e4-4c04-4848-ab16-633b33a61a09",
                    "name": "International Society for Music Information Retrieval Conference",
                    "type": "conference",
                    "alternate_names": [
                        "International Symposium/Conference on Music Information Retrieval",
                        "ISMIR",
                        "Int Soc Music Inf Retr Conf",
                        "Int Symp Music Inf Retr"
                    ],
                    "url": "http://www.ismir.net/"
                },
                "url": "https://www.semanticscholar.org/paper/ede4279199e7037b2587dd2c1cc83177b0c553c3",
                "title": "Ultra-light deep MIR by trimming lottery tickets",
                "abstract": "Current state-of-the-art results in Music Information Retrieval are largely dominated by deep learning approaches. These provide unprecedented accuracy across all tasks. However, the consistently overlooked downside of these models is their stunningly massive complexity, which seems concomitantly crucial to their success. In this paper, we address this issue by proposing a model pruning method based on the lottery ticket hypothesis. We modify the original approach to allow for explicitly removing parameters, through structured trimming of entire units, instead of simply masking individual weights. This leads to models which are effectively lighter in terms of size, memory and number of operations. We show that our proposal can remove up to 90% of the model parameters without loss of accuracy, leading to ultra-light deep MIR models. We confirm the surprising result that, at smaller compression ratios (removing up to 85% of a network), lighter models consistently outperform their heavier counterparts. We exhibit these results on a large array of MIR tasks including audio classification, pitch recognition, chord extraction, drum transcription and onset estimation. The resulting ultra-light deep learning models for MIR can run on CPU, and can even fit on embedded devices with minimal degradation of accuracy.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2952234",
                        "name": "P. Esling"
                    },
                    {
                        "authorId": "151228548",
                        "name": "Th'eis Bazin"
                    },
                    {
                        "authorId": "46211531",
                        "name": "Adrien Bitton"
                    },
                    {
                        "authorId": "51957030",
                        "name": "Tristan Carsault"
                    },
                    {
                        "authorId": "1846756773",
                        "name": "Ninon Devis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[20] evaluated the possibility to transfer the found tickets across optimizers or datasets."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "042237236110b42e5670bff75ec85cfd3c5bf4a6",
                "externalIds": {
                    "ArXiv": "2007.16170",
                    "DBLP": "journals/corr/abs-2007-16170",
                    "MAG": "3046330735",
                    "CorpusId": 220919894
                },
                "corpusId": 220919894,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/042237236110b42e5670bff75ec85cfd3c5bf4a6",
                "title": "Diet deep generative audio models with structured lottery",
                "abstract": "Deep learning models have provided extremely successful solutions in most audio application fields. However, the high accuracy of these models comes at the expense of a tremendous computation cost. This aspect is almost always overlooked in evaluating the quality of proposed models. However, models should not be evaluated without taking into account their complexity. This aspect is especially critical in audio applications, which heavily relies on specialized embedded hardware with real-time constraints. In this paper, we build on recent observations that deep models are highly overparameterized, by studying the lottery ticket hypothesis on deep generative audio models. This hypothesis states that extremely efficient small sub-networks exist in deep models and would provide higher accuracy than larger models if trained in isolation. However, lottery tickets are found by relying on unstructured masking, which means that resulting models do not provide any gain in either disk size or inference time. Instead, we develop here a method aimed at performing structured trimming. We show that this requires to rely on global selection and introduce a specific criterion based on mutual information. First, we confirm the surprising result that smaller models provide higher accuracy than their large counterparts. We further show that we can remove up to 95% of the model weights without significant degradation in accuracy. Hence, we can obtain very light models for generative audio across popular methods such as Wavenet, SING or DDSP, that are up to 100 times smaller with commensurate accuracy. We study the theoretical bounds for embedding these models on Raspberry Pi and Arduino, and show that we can obtain generative models on CPU with equivalent quality as large GPU models. Finally, we discuss the possibility of implementing deep generative audio models on embedded platforms.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2952234",
                        "name": "P. Esling"
                    },
                    {
                        "authorId": "1846756773",
                        "name": "Ninon Devis"
                    },
                    {
                        "authorId": "46211531",
                        "name": "Adrien Bitton"
                    },
                    {
                        "authorId": "6018858",
                        "name": "Antoine Caillon"
                    },
                    {
                        "authorId": "1403278041",
                        "name": "Axel Chemla-Romeu-Santos"
                    },
                    {
                        "authorId": "1846790027",
                        "name": "Constance Douwes"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Question 2: Are there patterns in the transferability of winning tickets? For example, do winning tickets transfer better from tasks with larger training sets [21] or tasks that are similar?",
                "However, the resulting subnetworks transfer between related tasks [21, 22].",
                "However, these subnetworks must be initialized to the state of the network \u03b8i after i steps of training [16, 21, 17] rather than to initialization \u03b80.",
                "[21] and Mehta [22] show that matching subnetworks transfer between vision tasks."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "389036b1366b64579725457993c1f63a4f3370ba",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-12223",
                    "MAG": "3104263050",
                    "ArXiv": "2007.12223",
                    "CorpusId": 220768628
                },
                "corpusId": 220768628,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/389036b1366b64579725457993c1f63a4f3370ba",
                "title": "The Lottery Ticket Hypothesis for Pre-trained BERT Networks",
                "abstract": "In natural language processing (NLP), enormous pre-trained models like BERT have become the standard starting point for training on a range of downstream tasks, and similar trends are emerging in other areas of deep learning. In parallel, work on the lottery ticket hypothesis has shown that models for NLP and computer vision contain smaller matching subnetworks capable of training in isolation to full accuracy and transferring to other tasks. In this work, we combine these observations to assess whether such trainable, transferrable subnetworks exist in pre-trained BERT models. For a range of downstream tasks, we indeed find matching subnetworks at 40% to 90% sparsity. We find these subnetworks at (pre-trained) initialization, a deviation from prior NLP research where they emerge only after some amount of training. Subnetworks found on the masked language modeling task (the same task used to pre-train the model) transfer universally; those found on other tasks transfer in a limited fashion if at all. As large-scale pre-training becomes an increasingly central paradigm in deep learning, our results demonstrate that the main lottery ticket observations remain relevant in this context. Codes available at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2648459",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "3307026",
                        "name": "Shiyu Chang"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "37873860",
                        "name": "Yang Zhang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "1701041",
                        "name": "Michael Carbin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In a similar setting, as demonstrated in the model pruning literature [7, 10, 26, 33, 38], having different pruning ratios for different layers of the network can further improve results over a single ratio across layers."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a07fc48b62c098977af6988aecf046de73712850",
                "externalIds": {
                    "MAG": "3045204521",
                    "DBLP": "journals/corr/abs-2007-11752",
                    "CorpusId": 220713375
                },
                "corpusId": 220713375,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a07fc48b62c098977af6988aecf046de73712850",
                "title": "PareCO: Pareto-aware Channel Optimization for Slimmable Neural Networks",
                "abstract": "Slimmable neural networks provide a flexible trade-off front between prediction error and computational cost (such as the number of floating-point operations or FLOPs) with the same storage cost as a single model. They have been proposed recently for resource-constrained settings such as mobile devices. However, current slimmable neural networks use a single width-multiplier for all the layers to arrive at sub-networks with different performance profiles, which neglects that different layers affect the network's prediction accuracy differently and have different FLOP requirements. Hence, developing a principled approach for deciding width-multipliers across different layers could potentially improve the performance of slimmable networks. To allow for heterogeneous width-multipliers across different layers, we formulate the problem of optimizing slimmable networks from a multi-objective optimization lens, which leads to a novel algorithm for optimizing both the shared weights and the width-multipliers for the sub-networks. We perform extensive empirical analysis with 14 network and dataset combinations and find that less over-parameterized networks benefit more from a joint channel and weight optimization than extremely over-parameterized networks. Quantitatively, improvements up to 1.7% and 1% in top-1 accuracy on the ImageNet dataset can be attained for MobileNetV2 and MobileNetV3, respectively. Our results highlight the potential of optimizing the channel counts for different layers jointly with the weights for slimmable networks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144774026",
                        "name": "Ting-Wu Chin"
                    },
                    {
                        "authorId": "4690624",
                        "name": "Ari S. Morcos"
                    },
                    {
                        "authorId": "1704073",
                        "name": "Diana Marculescu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "13884eaeac2782c29f5b011d3474be05bb0a4d9c",
                "externalIds": {
                    "ArXiv": "2007.08243",
                    "DBLP": "journals/corr/abs-2007-08243",
                    "MAG": "3042900599",
                    "CorpusId": 220546260
                },
                "corpusId": 220546260,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/13884eaeac2782c29f5b011d3474be05bb0a4d9c",
                "title": "Lottery Tickets in Linear Models: An Analysis of Iterative Magnitude Pruning",
                "abstract": "We analyse the pruning procedure behind the lottery ticket hypothesis arXiv:1803.03635v5, iterative magnitude pruning (IMP), when applied to linear models trained by gradient flow. We begin by presenting sufficient conditions on the statistical structure of the features, under which IMP prunes those features that have smallest projection onto the data. Following this, we explore IMP as a method for sparse estimation and sparse prediction in noisy settings, with minimal assumptions on the design matrix. The same techniques are then applied to derive corresponding results for threshold pruning. Finally, we present experimental evidence of the regularising effect of IMP. We hope that our work will contribute to a theoretically grounded understanding of lottery tickets and how they emerge from IMP.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1818667977",
                        "name": "Bryn Elesedy"
                    },
                    {
                        "authorId": "2080218",
                        "name": "Varun Kanade"
                    },
                    {
                        "authorId": "1725303",
                        "name": "Y. Teh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026algorithm to find sparse subnetworks (winning tickets) within a dense, randomly initialized feedforward neural network, which can achieve comparable (and sometimes greater) performance to the original network, when trained separately (Frankle & Carbin, 2018; Zhou et al., 2019; Morcos et al., 2019).",
                "In this regard, the lottery ticket hypothesis (Frankle & Carbin, 2018), suggested an algorithm to find sparse subnetworks (winning tickets) within a dense, randomly initialized feedforward neural network, which can achieve comparable (and sometimes greater) performance to the original network, when trained separately (Frankle & Carbin, 2018; Zhou et al., 2019; Morcos et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4fa84a5bfcb2cc2f98d839840aa1c949c23344b4",
                "externalIds": {
                    "DBLP": "conf/icml/HasaniLARG20",
                    "MAG": "3034500979",
                    "CorpusId": 225563441
                },
                "corpusId": 225563441,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4fa84a5bfcb2cc2f98d839840aa1c949c23344b4",
                "title": "A Natural Lottery Ticket Winner: Reinforcement Learning with Ordinary Neural Circuits",
                "abstract": "We propose a neural information processing system obtained by re-purposing the function of a biological neural circuit model to govern simulated and real-world control tasks. Inspired by the structure of the nervous system of the soilworm, C. elegans, we introduce ordinary neural circuits (ONCs), defined as the model of biological neural circuits reparameterized for the control of alternative tasks. We first demonstrate that ONCs realize networks with higher maximum flow compared to arbitrary wired networks. We then learn instances of ONCs to control a series of robotic tasks, including the autonomous parking of a real-world rover robot. For reconfiguration of the purpose of the neural circuit, we adopt a search-based optimization algorithm. Ordinary neural circuits perform on par and, in some cases, significantly surpass the performance of contemporary deep learning models. ONC networks are compact, 77% sparser than their counterpart neural controllers, and their neural dynamics are fully interpretable at the cell-level.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "8252176",
                        "name": "Ramin M. Hasani"
                    },
                    {
                        "authorId": "39083616",
                        "name": "Mathias Lechner"
                    },
                    {
                        "authorId": "2056330",
                        "name": "Alexander Amini"
                    },
                    {
                        "authorId": "145944286",
                        "name": "D. Rus"
                    },
                    {
                        "authorId": "1787208",
                        "name": "R. Grosu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is potentially transferable across vision tasks (Morcos et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3a44e4bd53ffd045b425b569fab43d7c72f2f70f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-03774",
                    "ArXiv": "2007.03774",
                    "MAG": "3040867995",
                    "CorpusId": 220403336
                },
                "corpusId": 220403336,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3a44e4bd53ffd045b425b569fab43d7c72f2f70f",
                "title": "The curious case of developmental BERTology: On sparsity, transfer learning, generalization and the brain",
                "abstract": "In this essay, we explore a point of intersection between deep learning and neuroscience, through the lens of large language models, transfer learning and network compression. Just like perceptual and cognitive neurophysiology has inspired effective deep neural network architectures which in turn make a useful model for understanding the brain, here we explore how biological neural development might inspire efficient and robust optimization procedures which in turn serve as a useful model for the maturation and aging of the brain.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2153688131",
                        "name": "Xin Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These include challenging the need for unstructured, magnitude-based pruning [59], using learning rate warmup [16], selecting unimportant weights globally instead of layer by layer [16], and late resetting unpruned parameters to values achieved early in training, as opposed to rewinding the weights all the way to their initial values [17, 38].",
                "Additionally, preliminary evidence for LT transferability has been provided in both natural and non-natural image domains [38, 45].",
                "On the other hand, lottery tickets have been shown to possess generalization properties that allow for their reuse across similar tasks, thus reducing the computational cost of finding dataset-dependent sparse sub-networks [38, 45]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "77b0f6e9717c537a30334d24f03109b80c688b4a",
                "externalIds": {
                    "ArXiv": "2007.04091",
                    "DBLP": "journals/corr/abs-2007-04091",
                    "MAG": "3042177413",
                    "CorpusId": 220403561
                },
                "corpusId": 220403561,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/77b0f6e9717c537a30334d24f03109b80c688b4a",
                "title": "Bespoke vs. Pr\u00eat-\u00e0-Porter Lottery Tickets: Exploiting Mask Similarity for Trainable Sub-Network Finding",
                "abstract": "The observation of sparse trainable sub-networks within over-parametrized networks - also known as Lottery Tickets (LTs) - has prompted inquiries around their trainability, scaling, uniqueness, and generalization properties. Across 28 combinations of image classification tasks and architectures, we discover differences in the connectivity structure of LTs found through different iterative pruning techniques, thus disproving their uniqueness and connecting emergent mask structure to the choice of pruning. In addition, we propose a consensus-based method for generating refined lottery tickets. This lottery ticket denoising procedure, based on the principle that parameters that always go unpruned across different tasks more reliably identify important sub-networks, is capable of selecting a meaningful portion of the architecture in an embarrassingly parallel way, while quickly discarding extra parameters without the need for further pruning iterations. We successfully train these sub-networks to performance comparable to that of ordinary lottery tickets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35550664",
                        "name": "Michela Paganini"
                    },
                    {
                        "authorId": "39774809",
                        "name": "J. Forde"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Morcos et al. (2019) even showed that to some extent winning tickets can be transferred across different tasks and optimizers."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "15ee10ffa90a4b354928d3e3e5fa18a9a9aff1a9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-14350",
                    "ArXiv": "2006.14350",
                    "MAG": "3037230048",
                    "CorpusId": 220055987
                },
                "corpusId": 220055987,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/15ee10ffa90a4b354928d3e3e5fa18a9a9aff1a9",
                "title": "Data-dependent Pruning to find the Winning Lottery Ticket",
                "abstract": "The Lottery Ticket Hypothesis postulates that a freshly initialized neural network contains a small subnetwork that can be trained in isolation to achieve similar performance as the full network. Our paper examines several alternatives to search for such subnetworks. We conclude that incorporating a data dependent component into the pruning criterion in the form of the gradient of the training loss -- as done in the SNIP method -- consistently improves the performance of existing pruning algorithms.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1768149295",
                        "name": "D'aniel L'evai"
                    },
                    {
                        "authorId": "3285258",
                        "name": "Zsolt Zombori"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In part, foresight pruning is motivated by evidence that specific sparse networks can be trained to yield comparable performance to the corresponding dense model (Liu et al., 2019; Frankle & Carbin, 2019; Morcos et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "021739c140c4eefaf2bbb65658e119e5251576a3",
                "externalIds": {
                    "MAG": "3035178967",
                    "DBLP": "journals/corr/abs-2006-08228",
                    "ArXiv": "2006.08228",
                    "CorpusId": 219687376
                },
                "corpusId": 219687376,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/021739c140c4eefaf2bbb65658e119e5251576a3",
                "title": "Finding trainable sparse networks through Neural Tangent Transfer",
                "abstract": "Deep neural networks have dramatically transformed machine learning, but their memory and energy demands are substantial. The requirements of real biological neural networks are rather modest in comparison, and one feature that might underlie this austerity is their sparse connectivity. In deep learning, trainable sparse networks that perform well on a specific task are usually constructed using label-dependent pruning criteria. In this article, we introduce Neural Tangent Transfer, a method that instead finds trainable sparse networks in a label-free manner. Specifically, we find sparse networks whose training dynamics, as characterized by the neural tangent kernel, mimic those of dense networks in function space. Finally, we evaluate our label-agnostic approach on several standard classification tasks and show that the resulting sparse networks achieve higher classification performance while converging faster.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "121698110",
                        "name": "Tianlin Liu"
                    },
                    {
                        "authorId": "39979946",
                        "name": "Friedemann Zenke"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A recent trend proposed to prune during training time (Frankle and Carbin, 2019; Gomez et al.; Desai et al., 2019; Morcos et al., 2019), yielding an iterative process.",
                "Generally, research sped up the process of finding winning tickets (Morcos et al., 2019; Yin et al., 2020).",
                "Orthogonally, Morcos et al. (2019) or Desai et al. (2019) study whether winning tickets transfer across tasks or datasets."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "fefcf38c2ab8abdbe766a8fbccd76af36430c31a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-07014",
                    "ArXiv": "2006.07014",
                    "MAG": "3034254993",
                    "CorpusId": 219635949
                },
                "corpusId": 219635949,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fefcf38c2ab8abdbe766a8fbccd76af36430c31a",
                "title": "How many winning tickets are there in one DNN?",
                "abstract": "The recent lottery ticket hypothesis proposes that there is one sub-network that matches the accuracy of the original network when trained in isolation. We show that instead each network contains several winning tickets, even if the initial weights are fixed. The resulting winning sub-networks are not instances of the same network under weight space symmetry, and show no overlap or correlation significantly larger than expected by chance. If randomness during training is decreased, overlaps higher than chance occur, even if the networks are trained on different tasks. We conclude that there is rather a distribution over capable sub-networks, as opposed to a single winning ticket.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "39221858",
                        "name": "Kathrin Grosse"
                    },
                    {
                        "authorId": "144588806",
                        "name": "M. Backes"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Iterative Magnitude Pruning (IMP) is a recently proposed pruning algorithm that has proven to be successful in finding extremely sparse trainable neural networks at initialization (winning lottery tickets) [10, 11, 12, 44, 45, 46, 47].",
                "Moreover, its been shown that some of these winning ticket subnetworks can generalize across datasets and optimizers [12]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3b0fb765716ef6861a84abffcbe40643857c613b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-05467",
                    "MAG": "3034877463",
                    "ArXiv": "2006.05467",
                    "CorpusId": 219558821
                },
                "corpusId": 219558821,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3b0fb765716ef6861a84abffcbe40643857c613b",
                "title": "Pruning neural networks without any data by iteratively conserving synaptic flow",
                "abstract": "Pruning the parameters of deep neural networks has generated intense interest due to potential savings in time, memory and energy both during training and at test time. Recent works have identified, through an expensive sequence of training and pruning cycles, the existence of winning lottery tickets or sparse trainable subnetworks at initialization. This raises a foundational question: can we identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? We provide an affirmative answer to this question through theory driven algorithm design. We first mathematically formulate and experimentally verify a conservation law that explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse, the premature pruning of an entire layer rendering a network untrainable. This theory also elucidates how layer-collapse can be entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow Pruning (SynFlow). This algorithm can be interpreted as preserving the total flow of synaptic strengths through the network at initialization subject to a sparsity constraint. Notably, this algorithm makes no reference to the training data and consistently competes with or outperforms existing state-of-the-art pruning algorithms at initialization over a range of models (VGG and ResNet), datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to 99.99 percent). Thus our data-agnostic pruning algorithm challenges the existing paradigm that, at initialization, data must be used to quantify which synapses are important.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1912151014",
                        "name": "Hidenori Tanaka"
                    },
                    {
                        "authorId": "145616412",
                        "name": "D. Kunin"
                    },
                    {
                        "authorId": "40657572",
                        "name": "Daniel L. K. Yamins"
                    },
                    {
                        "authorId": "25769960",
                        "name": "S. Ganguli"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This theory sparked a body of research examining the behaviour and obtainment of these winning tickets [18, 76, 54, 13, 15, 79, 17, 50], as well as some criticism [47, 19]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ead63b8f9d335ef3f5aa7ac1f86864347d894e4c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-00896",
                    "ArXiv": "2006.00896",
                    "MAG": "3031139557",
                    "CorpusId": 219177398
                },
                "corpusId": 219177398,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ead63b8f9d335ef3f5aa7ac1f86864347d894e4c",
                "title": "Pruning via Iterative Ranking of Sensitivity Statistics",
                "abstract": "With the introduction of SNIP [arXiv:1810.02340v2], it has been demonstrated that modern neural networks can effectively be pruned before training. Yet, its sensitivity criterion has since been criticized for not propagating training signal properly or even disconnecting layers. As a remedy, GraSP [arXiv:2002.07376v1] was introduced, compromising on simplicity. However, in this work we show that by applying the sensitivity criterion iteratively in smaller steps - still before training - we can improve its performance without difficult implementation. As such, we introduce 'SNIP-it'. We then demonstrate how it can be applied for both structured and unstructured pruning, before and/or during training, therewith achieving state-of-the-art sparsity-performance trade-offs. That is, while already providing the computational benefits of pruning in the training process from the start. Furthermore, we evaluate our methods on robustness to overfitting, disconnection and adversarial attacks as well.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2067199219",
                        "name": "Stijn Verdenius"
                    },
                    {
                        "authorId": "34216125",
                        "name": "M. Stol"
                    },
                    {
                        "authorId": "51131843",
                        "name": "Patrick Forr'e"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6d682bbf0dc19493a7695b9b17b0d78751e70fd3",
                "externalIds": {
                    "MAG": "3034315405",
                    "DOI": "10.1007/s40305-020-00309-6",
                    "CorpusId": 220511793
                },
                "corpusId": 220511793,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6d682bbf0dc19493a7695b9b17b0d78751e70fd3",
                "title": "Optimization for Deep Learning: An Overview",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "153899948",
                        "name": "Ruoyu Sun"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "987d20d113f09f3ab3981ba38dacd864bd9b23ce",
                "externalIds": {
                    "DOI": "10.1007/s40305-020-00309-6",
                    "CorpusId": 256376168
                },
                "corpusId": 256376168,
                "publicationVenue": {
                    "id": "efd96da9-0740-430a-a859-ad7ca0f88d82",
                    "name": "Journal of the Operations Research Society of China",
                    "type": "journal",
                    "alternate_names": [
                        "J Oper Res Soc China"
                    ],
                    "issn": "2194-668X",
                    "url": "http://www.springer.com/mathematics/applications/journal/40305",
                    "alternate_urls": [
                        "https://link.springer.com/journal/40305"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/987d20d113f09f3ab3981ba38dacd864bd9b23ce",
                "title": "Optimization for Deep Learning: An Overview",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "153899948",
                        "name": "Ruoyu Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[15] focused on the natural image domain, we investigate the possibility of transferring winning tickets obtained from the natural image domain to datasets in non natural image domains.",
                "However, it also appears that there are stronger limitations to the transferability of winning initializations which were not observed by [15].",
                "The results obtained on the artistic datasets seem to suggest that winning initializations contain inductive biases that are strong enough to get at least successfully transferred to the artistic domain, therefore confirming some of the claims that were made in [15].",
                "Besides the work presented in [15], there have been other attempts that aimed to better understand the LTH after studying it from a transfer-learning perspective.",
                "However, just as the study presented in [15], all this research limited its analysis to natural images.",
                "5, one research direction in particular has looked into how well winning ticket initializations can be transferred among different training settings (datasets and optimizers), an approach which aims at characterizing the winners of the LTH by studying to what extent their inductive biases are generic [15].",
                "We follow an experimental set-up which is similar to the one that was introduced in [15] (and that has been validated by [5]).",
                "Following [6, 15], 31 winning tickets f(x;m \u03b8k) of increasing sparsity are obtained from each of these three datasets by repeating 31 iterations of network",
                "The closest approach to what has been presented in this work is certainly [15], which shows that winning models can generalize across datasets of natural images and across different optimizers.",
                "Constructing a winning ticket with parameters \u03b8k, instead of \u03b80, is a procedure which is known as late-resetting [4], and is a simple but effective trick that makes it possible to stably find winning-initializations in deep convolutional neural networks [4, 15]."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "ad2eaf820cf65b441c576fbaa6086d32fbee0cb3",
                "externalIds": {
                    "ArXiv": "2005.05232",
                    "DBLP": "journals/corr/abs-2005-05232",
                    "MAG": "3023169080",
                    "DOI": "10.5220/0010196300590069",
                    "CorpusId": 218581948
                },
                "corpusId": 218581948,
                "publicationVenue": {
                    "id": "3e53351c-7355-4159-a3f2-f3b03a3aa989",
                    "name": "VISIGRAPP",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Comput Vis Imaging Comput Graph Theory Appl",
                        "International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications"
                    ],
                    "issn": "2184-4321",
                    "alternate_issns": [
                        "2184-5921"
                    ],
                    "url": "http://www.visigrapp.org/"
                },
                "url": "https://www.semanticscholar.org/paper/ad2eaf820cf65b441c576fbaa6086d32fbee0cb3",
                "title": "On the Transferability of Winning Tickets in Non-Natural Image Datasets",
                "abstract": "We study the generalization properties of pruned neural networks that are the winners of the lottery ticket hypothesis on datasets of natural images. We analyse their potential under conditions in which training data is scarce and comes from a non-natural domain. Specifically, we investigate whether pruned models that are found on the popular CIFAR-10/100 and Fashion-MNIST datasets, generalize to seven different datasets that come from the fields of digital pathology and digital heritage. Our results show that there are significant benefits in transferring and training sparse architectures over larger parametrized models, since in all of our experiments pruned networks, winners of the lottery ticket hypothesis, significantly outperform their larger unpruned counterparts. These results suggest that winning initializations do contain inductive biases that are generic to some extent, although, as reported by our experiments on the biomedical datasets, their generalization properties can be more limiting than what has been so far observed in the literature.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "34583651",
                        "name": "M. Sabatelli"
                    },
                    {
                        "authorId": "1890151",
                        "name": "M. Kestemont"
                    },
                    {
                        "authorId": "50206577",
                        "name": "P. Geurts"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The \u201cwinning\u201d initializations were shown to generalize across computer vision datasets (Morcos et al., 2019), and to exist both in LSTM and Transformer models for NLP tasks (Yu et al., 2020).",
                "The \u201cwinning\u201d initializations were shown to generalize across computer vision datasets (Morcos et al., 2019), and to exist both in LSTM and Transformer models for NLP tasks (Yu et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "91ac65431b2dc46919e1673fde67671c29446812",
                "externalIds": {
                    "MAG": "3022969335",
                    "ArXiv": "2005.00561",
                    "ACL": "2020.emnlp-main.259",
                    "DBLP": "conf/emnlp/PrasannaRR20",
                    "DOI": "10.18653/v1/2020.emnlp-main.259",
                    "CorpusId": 218487454
                },
                "corpusId": 218487454,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/91ac65431b2dc46919e1673fde67671c29446812",
                "title": "When BERT Plays the Lottery, All Tickets Are Winning",
                "abstract": "Much of the recent success in NLP is due to the large Transformer-based models such as BERT (Devlin et al, 2019). However, these models have been shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis. For fine-tuned BERT, we show that (a) it is possible to find a subnetwork of elements that achieves performance comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. However, the \"bad\" subnetworks can be fine-tuned separately to achieve only slightly worse performance than the \"good\" ones, indicating that most weights in the pre-trained BERT are potentially useful. We also show that the \"good\" subnetworks vary considerably across GLUE tasks, opening up the possibilities to learn what knowledge BERT actually uses at inference time.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145816991",
                        "name": "Sai Prasanna"
                    },
                    {
                        "authorId": "145046059",
                        "name": "Anna Rogers"
                    },
                    {
                        "authorId": "1681193",
                        "name": "Anna Rumshisky"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, several works have suggested the use of rewound weights from the original network as initializations [9,34,55]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6cf32ac2248bb9102743ed2bba3a088866a44dc7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2004-14584",
                    "ArXiv": "2004.14584",
                    "MAG": "3022299358",
                    "CorpusId": 216868158
                },
                "corpusId": 216868158,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6cf32ac2248bb9102743ed2bba3a088866a44dc7",
                "title": "Out-of-the-box channel pruned networks",
                "abstract": "In the last decade convolutional neural networks have become gargantuan. Pre-trained models, when used as initializers are able to fine-tune ever larger networks on small datasets. Consequently, not all the convolutional features that these fine-tuned models detect are requisite for the end-task. Several works of channel pruning have been proposed to prune away compute and memory from models that were trained already. Typically, these involve policies that decide which and how many channels to remove from each layer leading to channel-wise and/or layer-wise pruning profiles, respectively. In this paper, we conduct several baseline experiments and establish that profiles from random channel-wise pruning policies are as good as metric-based ones. We also establish that there may exist profiles from some layer-wise pruning policies that are measurably better than common baselines. We then demonstrate that the top layer-wise pruning profiles found using an exhaustive random search from one datatset are also among the top profiles for other datasets. This implies that we could identify out-of-the-box layer-wise pruning profiles using benchmark datasets and use these directly for new datasets. Furthermore, we develop a Reinforcement Learning (RL) policy-based search algorithm with a direct objective of finding transferable layer-wise pruning profiles using many models for the same architecture. We use a novel reward formulation that drives this RL search towards an expected compression while maximizing accuracy. Our results show that our transferred RL-based profiles are as good or better than best profiles found on the original dataset via exhaustive search. We then demonstrate that if we found the profiles using a mid-sized dataset such as Cifar10/100, we are able to transfer them to even a large dataset such as Imagenet.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2890758",
                        "name": "Ragav Venkatesan"
                    },
                    {
                        "authorId": "2065207183",
                        "name": "Gurumurthy Swaminathan"
                    },
                    {
                        "authorId": "2109520799",
                        "name": "Xiong Zhou"
                    },
                    {
                        "authorId": "2080133694",
                        "name": "Anna Luo"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5f0322dd869f533f49d374da15d27d835a3d1881",
                "externalIds": {
                    "MAG": "3014421314",
                    "DBLP": "journals/corr/abs-2004-01077",
                    "ArXiv": "2004.01077",
                    "DOI": "10.1109/CVPRW50498.2020.00369",
                    "CorpusId": 214774910
                },
                "corpusId": 214774910,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5f0322dd869f533f49d374da15d27d835a3d1881",
                "title": "Learning Sparse & Ternary Neural Networks with Entropy-Constrained Trained Ternarization (EC2T)",
                "abstract": "Deep neural networks (DNN) have shown remarkable success in a variety of machine learning applications. The capacity of these models (i.e., number of parameters), endows them with expressive power and allows them to reach the desired performance. In recent years, there is an increasing interest in deploying DNNs to resource- constrained devices (i.e., mobile devices) with limited energy, memory, and computational budget. To address this problem, we propose Entropy-Constrained Trained Ternarization (EC2T), a general framework to create sparse and ternary neural networks which are efficient in terms of storage (e.g., at most two binary-masks and two full-precision values are required to save a weight matrix) and computation (e.g., MAC operations are reduced to a few accumulations plus two multiplications). This approach consists of two steps. First, a super-network is created by scaling the dimensions of a pre-trained model (i.e., its width and depth). Subsequently, this super-network is simultaneously pruned (using an entropy constraint) and quantized (that is, ternary values are assigned layer-wise) in a training process, resulting in a sparse and ternary network representation. We validate the proposed approach in CIFAR-10, CIFAR-100, and ImageNet datasets, showing its effectiveness in image classification tasks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3116195",
                        "name": "Arturo Marb\u00e1n"
                    },
                    {
                        "authorId": "1607128888",
                        "name": "Daniel Becking"
                    },
                    {
                        "authorId": "2000355038",
                        "name": "Simon Wiedemann"
                    },
                    {
                        "authorId": "1699054",
                        "name": "W. Samek"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[37] proposed a generalization method which allows reusing the same wining tickets across various datasets."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "459315e4f0f3e632639e93d9f5f209d209662694",
                "externalIds": {
                    "MAG": "3012022301",
                    "DBLP": "journals/tcsv/ShiXTC21",
                    "ArXiv": "2003.05891",
                    "DOI": "10.1109/TCSVT.2020.3013170",
                    "CorpusId": 212675847
                },
                "corpusId": 212675847,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/459315e4f0f3e632639e93d9f5f209d209662694",
                "title": "SASL: Saliency-Adaptive Sparsity Learning for Neural Network Acceleration",
                "abstract": "Accelerating the inference of CNNs is critical to their deployment in real-world applications. Among all pruning approaches, the methods of implementing a sparsity learning framework have shown effectiveness as they learn and prune the models in an end-to-end data-driven manner. However, these works impose the same sparsity regularization on all filters indiscriminately, which can hardly result in an optimal structure-sparse network. In this paper, we propose a Saliency-Adaptive Sparsity Learning (SASL) approach for further optimization. A novel and effective estimation of each filter, i.e., saliency, is designed, which is measured from two aspects: the importance for prediction performance and the consumed computational resources. During sparsity learning, the regularization strength is adjusted according to the saliency, so our optimized format can better preserve the prediction performance while zeroing out more computation-heavy filters. The calculation for saliency introduces minimum overhead to the training process, which means our SASL is very efficient. During the pruning phase, in order to optimize the proposed data-dependent criterion, a hard sample mining strategy is utilized, which shows higher effectiveness and efficiency. Extensive experiments demonstrate the superior performance of our method. Notably, on ILSVRC-2012 dataset, our approach can reduce 49.7% FLOPs of ResNet-50 with very negligible 0.39% top-1 and 0.05% top-5 accuracy degradation.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2113235030",
                        "name": "Jun Shi"
                    },
                    {
                        "authorId": "2116854913",
                        "name": "Jianfeng Xu"
                    },
                    {
                        "authorId": "2764854",
                        "name": "K. Tasaka"
                    },
                    {
                        "authorId": "31482866",
                        "name": "Zhibo Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "de66ada65cd9d36e46f1f8dd2c8be480180038ec",
                "externalIds": {
                    "DBLP": "conf/mlsys/BlalockOFG20",
                    "MAG": "3010156474",
                    "ArXiv": "2003.03033",
                    "CorpusId": 212628335
                },
                "corpusId": 212628335,
                "publicationVenue": {
                    "id": "3bcf77b3-860b-4dd7-84ae-9fe9414c6c6a",
                    "name": "Conference on Machine Learning and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "MLSys",
                        "Conf Mach Learn Syst"
                    ],
                    "url": "https://mlsys.org/"
                },
                "url": "https://www.semanticscholar.org/paper/de66ada65cd9d36e46f1f8dd2c8be480180038ec",
                "title": "What is the State of Neural Network Pruning?",
                "abstract": "Neural network pruning---the task of reducing the size of a network by removing parameters---has been the subject of a great deal of work in recent years. We provide a meta-analysis of the literature, including an overview of approaches to pruning and consistent findings in the literature. After aggregating results across 81 papers and pruning hundreds of models in controlled conditions, our clearest finding is that the community suffers from a lack of standardized benchmarks and metrics. This deficiency is substantial enough that it is hard to compare pruning techniques to one another or determine how much progress the field has made over the past three decades. To address this situation, we identify issues with current practices, suggest concrete remedies, and introduce ShrinkBench, an open-source framework to facilitate standardized evaluations of pruning methods. We use ShrinkBench to compare various pruning techniques and show that its comprehensive evaluation can prevent common pitfalls when comparing pruning methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2969944",
                        "name": "Davis W. Blalock"
                    },
                    {
                        "authorId": "32446924",
                        "name": "Jose Javier Gonzalez Ortiz"
                    },
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "1724429",
                        "name": "J. Guttag"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "95133180792648387b65fa69c23537cd6ebad02d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2002-12287",
                    "MAG": "3008311476",
                    "ArXiv": "2002.12287",
                    "DOI": "10.1007/978-3-030-43883-8_3",
                    "CorpusId": 211532393
                },
                "corpusId": 211532393,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/95133180792648387b65fa69c23537cd6ebad02d",
                "title": "Deep Randomized Neural Networks",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2158038",
                        "name": "C. Gallicchio"
                    },
                    {
                        "authorId": "1752983",
                        "name": "Simone Scardapane"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Morcos et al. (2019) show that these subnetworks transfer across tasks and datasets."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8362dffc9849a76f5ea73fc03d4c8b9fd10351d2",
                "externalIds": {
                    "MAG": "3007625939",
                    "DBLP": "journals/corr/abs-2002-11448",
                    "ArXiv": "2002.11448",
                    "CorpusId": 211506753
                },
                "corpusId": 211506753,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8362dffc9849a76f5ea73fc03d4c8b9fd10351d2",
                "title": "Predicting Neural Network Accuracy from Weights",
                "abstract": "We show experimentally that the accuracy of a trained neural network can be predicted surprisingly well by looking only at its weights, without evaluating it on input data. We motivate this task and introduce a formal setting for it. Even when using simple statistics of the weights, the predictors are able to rank neural networks by their performance with very high accuracy (R2 score more than 0.98). Furthermore, the predictors are able to rank networks trained on different, unobserved datasets and with different architectures. We release a collection of 120k convolutional neural networks trained on four different datasets to encourage further research in this area, with the goal of understanding network training and performance better.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2465270",
                        "name": "Thomas Unterthiner"
                    },
                    {
                        "authorId": "51027911",
                        "name": "Daniel Keysers"
                    },
                    {
                        "authorId": "1802148",
                        "name": "S. Gelly"
                    },
                    {
                        "authorId": "1698617",
                        "name": "O. Bousquet"
                    },
                    {
                        "authorId": "3121264",
                        "name": "I. Tolstikhin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The lottery ticket hypothesis was latter analyzed in several contexts [36], [37], [38]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "bf57d48ef7032f36a8c9c29d87f0ece3710573ef",
                "externalIds": {
                    "DBLP": "journals/pami/AlfarraBHGG23",
                    "ArXiv": "2002.08838",
                    "DOI": "10.1109/TPAMI.2022.3201490",
                    "CorpusId": 251741576,
                    "PubMed": "36001517"
                },
                "corpusId": 251741576,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bf57d48ef7032f36a8c9c29d87f0ece3710573ef",
                "title": "On the Decision Boundaries of Neural Networks: A Tropical Geometry Perspective",
                "abstract": "This work tackles the problem of characterizing and understanding the decision boundaries of neural networks with piecewise linear non-linearity activations. We use tropical geometry, a new development in the area of algebraic geometry, to characterize the decision boundaries of a simple network of the form (Affine, ReLU, Affine). Our main finding is that the decision boundaries are a subset of a tropical hypersurface, which is intimately related to a polytope formed by the convex hull of two zonotopes. The generators of these zonotopes are functions of the network parameters. This geometric characterization provides new perspectives to three tasks. (i) We propose a new tropical perspective to the lottery ticket hypothesis, where we view the effect of different initializations on the tropical geometric representation of a network's decision boundaries. (ii) Moreover, we propose new tropical based optimization reformulations that directly influence the decision boundaries of the network for the task of network pruning. (iii) At last, we discuss the reformulation of the generation of adversarial attacks in a tropical sense. We demonstrate that one can construct adversaries in a new tropical setting by perturbing a specific set of decision boundaries by perturbing a set of parameters in the network.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "92400655",
                        "name": "Motasem Alfarra"
                    },
                    {
                        "authorId": "2314778",
                        "name": "Adel Bibi"
                    },
                    {
                        "authorId": "1500194870",
                        "name": "H. Hammoud"
                    },
                    {
                        "authorId": "37973115",
                        "name": "M. Gaafar"
                    },
                    {
                        "authorId": "2931652",
                        "name": "Bernard Ghanem"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(14)We might consider finding a lottery ticket for BERT, which we would expect to fit the GLUE training data just as well as pre-trained BERT (Morcos et al., 2019; Yu et al., 2019).",
                "14We might consider finding a lottery ticket for BERT, which we would expect to fit the GLUE training data just as well as pre-trained BERT (Morcos et al., 2019; Yu et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d9b824dbecbe3a1f0b1489f9e4521a532a63818d",
                "externalIds": {
                    "ACL": "2020.repl4nlp-1.18",
                    "DBLP": "journals/corr/abs-2002-08307",
                    "ArXiv": "2002.08307",
                    "MAG": "2997710335",
                    "DOI": "10.18653/v1/2020.repl4nlp-1.18",
                    "CorpusId": 211171709
                },
                "corpusId": 211171709,
                "publicationVenue": {
                    "id": "8b169440-4c13-4cf4-b3f9-1dc7c39dc888",
                    "name": "Workshop on Representation Learning for NLP",
                    "type": "conference",
                    "alternate_names": [
                        "RepL4NLP",
                        "Workshop Represent Learn NLP"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d9b824dbecbe3a1f0b1489f9e4521a532a63818d",
                "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning",
                "abstract": "Pre-trained universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1452684657",
                        "name": "Mitchell A. Gordon"
                    },
                    {
                        "authorId": "1800354",
                        "name": "Kevin Duh"
                    },
                    {
                        "authorId": "145580321",
                        "name": "Nicholas Andrews"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[33] proposed a technique for sparsifying n over-parameterized trained neural model based on the lottery hypothesis."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b5fc7d08812e7bf246800eb59202fe7ced7010e7",
                "externalIds": {
                    "ArXiv": "2002.07259",
                    "DBLP": "journals/corr/abs-2002-07259",
                    "MAG": "3005647689",
                    "DOI": "10.1007/978-3-031-33271-5_15",
                    "CorpusId": 211146529
                },
                "corpusId": 211146529,
                "publicationVenue": {
                    "id": "ad649a48-7b8a-4f02-bf0b-64d9a6f73105",
                    "name": "Integration of AI and OR Techniques in Constraint Programming",
                    "type": "conference",
                    "alternate_names": [
                        "Integr AI Tech Constraint Program",
                        "CPAIOR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=574"
                },
                "url": "https://www.semanticscholar.org/paper/b5fc7d08812e7bf246800eb59202fe7ced7010e7",
                "title": "Identifying Critical Neurons in ANN Architectures using Mixed Integer Programming",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3444427",
                        "name": "M. Elaraby"
                    },
                    {
                        "authorId": "2683398",
                        "name": "Guy Wolf"
                    },
                    {
                        "authorId": "48075546",
                        "name": "Margarida Carvalho"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Lottery ticket mechanism since has been further explored, it has been shown that the winning tickets can be used across datasets [11], and that the tickets occur in other domains as well, such as in NLP [12].",
                "Mainly, we answer the question: As is recently discovered for generic lottery ticket mechanism [11], that a winning ticket can generalize across datasets, can we extend this notion to DPLTM? Where we can use a publicly available dataset to get a winning ticket in a non-private setting, and then use that winning ticket to train a differentially private model on our sensitive dataset."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0aa1bae64e44739dbcc18f49fc2ad572c54acd70",
                "externalIds": {
                    "ArXiv": "2002.11613",
                    "DBLP": "journals/corr/abs-2002-11613",
                    "MAG": "3007414102",
                    "CorpusId": 211505784
                },
                "corpusId": 211505784,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0aa1bae64e44739dbcc18f49fc2ad572c54acd70",
                "title": "The Differentially Private Lottery Ticket Mechanism",
                "abstract": "We propose the differentially private lottery ticket mechanism (DPLTM). An end-to-end differentially private training paradigm based on the lottery ticket hypothesis. Using \"high-quality winners\", selected via our custom score function, DPLTM significantly improves the privacy-utility trade-off over the state-of-the-art. We show that DPLTM converges faster, allowing for early stopping with reduced privacy budget consumption. We further show that the tickets from DPLTM are transferable across datasets, domains, and architectures. Our extensive evaluation on several public datasets provides evidence to our claims.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3361384",
                        "name": "Lovedeep Gondara"
                    },
                    {
                        "authorId": "1751643",
                        "name": "Ke Wang"
                    },
                    {
                        "authorId": "145262240",
                        "name": "Ricardo Silva Carvalho"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similar to the experimental setup in [5], we divide the Cifar-10 dataset into two equal training splits namely Cifar-10a and Cifar-10b with 25k training samples in each,",
                "It was found in [5] that global pruning outperforms local pruning when the larger networks are considered, and hence we adopt global pruning for this case as well.",
                "Similarly, in [5], the authors reported that the winning tickets generalized reasonably across changes in the training con guration.",
                "In a recent study [5], it was found through extensive experimentation that the winning ticket initializations could be reused and are generalizable across models trained on di erent datasets and optimizers.",
                "Following this pivotal work, several studies have been carried to understand the role of initialization, the e ect of the pruning criterion used and the importance of retraining the sub-networks [3, 4, 5, 6, 7, 8] for the success of lottery tickets.",
                "To this end, we adopt a setup similar to [5]: we investigate the transferability of tickets to di erent datasets from the same distribution.",
                "For experiments with VGG-19 [33], we use its modi ed variant as in [5, 2], i.",
                "Note, this observation is consistent with results from earlier works on LTH such as [2, 5]."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "a41183d756e2078b61903b70cd606f4c0b13e2fe",
                "externalIds": {
                    "MAG": "3005249894",
                    "ArXiv": "2002.03875",
                    "DBLP": "journals/corr/abs-2002-03875",
                    "CorpusId": 211069074
                },
                "corpusId": 211069074,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a41183d756e2078b61903b70cd606f4c0b13e2fe",
                "title": "Calibrate and Prune: Improving Reliability of Lottery Tickets Through Prediction Calibration",
                "abstract": "The hypothesis that sub-network initializations (lottery) exist within the initializations of over-parameterized networks, which when trained in isolation produce highly generalizable models, has led to crucial insights into network initialization and has enabled efficient inferencing. Supervised models with uncalibrated confidences tend to be overconfident even when making wrong prediction. In this paper, for the first time, we study how explicit confidence calibration in the over-parameterized network impacts the quality of the resulting lottery tickets. More specifically, we incorporate a suite of calibration strategies, ranging from mixup regularization, variance-weighted confidence calibration to the newly proposed likelihood-based calibration and normalized bin assignment strategies. Furthermore, we explore different combinations of architectures and datasets, and make a number of key findings about the role of confidence calibration. Our empirical studies reveal that including calibration mechanisms consistently lead to more effective lottery tickets, in terms of accuracy as well as empirical calibration metrics, even when retrained using data with challenging distribution shifts with respect to the source dataset.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "153441560",
                        "name": "Bindya Venkatesh"
                    },
                    {
                        "authorId": "1744175",
                        "name": "Jayaraman J. Thiagarajan"
                    },
                    {
                        "authorId": "51149615",
                        "name": "Kowshik Thopalli"
                    },
                    {
                        "authorId": "1706272",
                        "name": "P. Sattigeri"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[14] (multilayer perceptron (MLP) is replaced by a fully connected layer).",
                "[14] show that winning tickets initializations transfer across different image classification datasets, thus suggesting that winning tickets do not entirely overfit to the particular data distribution on which they are found.",
                "[14] have shown that winning tickets initializations can be re-used across different datasets with a common domain (natural images) trained on the same task (labels classification)."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d52e41a597e22d9b9a3f57e8e4832eb689b45ecb",
                "externalIds": {
                    "ArXiv": "2001.03554",
                    "DBLP": "journals/corr/abs-2001-03554",
                    "MAG": "2999514753",
                    "CorpusId": 210157192
                },
                "corpusId": 210157192,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d52e41a597e22d9b9a3f57e8e4832eb689b45ecb",
                "title": "Pruning Convolutional Neural Networks with Self-Supervision",
                "abstract": "Convolutional neural networks trained without supervision come close to matching performance with supervised pre-training, but sometimes at the cost of an even higher number of parameters. Extracting subnetworks from these large unsupervised convnets with preserved performance is of particular interest to make them less computationally intensive. Typical pruning methods operate during training on a task while trying to maintain the performance of the pruned network on the same task. However, in self-supervised feature learning, the training objective is agnostic on the representation transferability to downstream tasks. Thus, preserving performance for this objective does not ensure that the pruned subnetwork remains effective for solving downstream tasks. In this work, we investigate the use of standard pruning methods, developed primarily for supervised learning, for networks trained without labels (i.e. on self-supervised tasks). We show that pruned masks obtained with or without labels reach comparable performance when retrained on labels, suggesting that pruning operates similarly for self-supervised and supervised learning. Interestingly, we also find that pruning preserves the transfer performance of self-supervised subnetwork representations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2062862676",
                        "name": "Mathilde Caron"
                    },
                    {
                        "authorId": "4690624",
                        "name": "Ari S. Morcos"
                    },
                    {
                        "authorId": "2329288",
                        "name": "Piotr Bojanowski"
                    },
                    {
                        "authorId": "2599292",
                        "name": "J. Mairal"
                    },
                    {
                        "authorId": "2319608",
                        "name": "Armand Joulin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a46e92bdf51e713d2286af53486e1ec0eca8df68",
                "externalIds": {
                    "DBLP": "conf/appis/HubensMDPZGD20",
                    "ArXiv": "2112.08227",
                    "MAG": "3008447163",
                    "DOI": "10.1145/3378184.3378224",
                    "CorpusId": 211104776
                },
                "corpusId": 211104776,
                "publicationVenue": {
                    "id": "47bb3da8-7725-4d6c-8353-250bebd34fde",
                    "name": "Applications of Intelligent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Appl Intell Syst",
                        "APPIS",
                        "Applications Intelligent Systems"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a46e92bdf51e713d2286af53486e1ec0eca8df68",
                "title": "An Experimental Study of the Impact of Pre-Training on the Pruning of a Convolutional Neural Network",
                "abstract": "In recent years, deep neural networks have known a wide success in various application domains. However, they require important computational and memory resources, which severely hinders their deployment, notably on mobile devices or for real-time applications. Neural networks usually involve a large number of parameters, which correspond to the weights of the network. Such parameters, obtained with the help of a training process, are determinant for the performance of the network. However, they are also highly redundant. The pruning methods notably attempt to reduce the size of the parameter set, by identifying and removing the irrelevant weights. In this paper, we examine the impact of the training strategy on the pruning efficiency. Two training modalities are considered and compared: (1) fine-tuned and (2) from scratch. The experimental results obtained on four datasets (CIFAR10, CIFAR100, SVHN and Caltech101) and for two different CNNs (VGG16 and MobileNet) demonstrate that a network that has been pre-trained on a large corpus (e.g. ImageNet) and then fine-tuned on a particular dataset can be pruned much more efficiently (up to 80% of parameter reduction) than the same network trained from scratch.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1388031811",
                        "name": "Nathan Hubens"
                    },
                    {
                        "authorId": "1681157",
                        "name": "M. Mancas"
                    },
                    {
                        "authorId": "3265104",
                        "name": "M. Decombas"
                    },
                    {
                        "authorId": "1806163",
                        "name": "M. Preda"
                    },
                    {
                        "authorId": "47651863",
                        "name": "T. Zaharia"
                    },
                    {
                        "authorId": "50276543",
                        "name": "B. Gosselin"
                    },
                    {
                        "authorId": "49164810",
                        "name": "T. Dutoit"
                    }
                ]
            }
        },
        {
            "contexts": [
                "1 of [151]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c23173e93f1db79a422e2af881a40afb96b8cb92",
                "externalIds": {
                    "MAG": "2994927236",
                    "DBLP": "journals/corr/abs-1912-08957",
                    "ArXiv": "1912.08957",
                    "CorpusId": 209414601
                },
                "corpusId": 209414601,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c23173e93f1db79a422e2af881a40afb96b8cb92",
                "title": "Optimization for deep learning: theory and algorithms",
                "abstract": "When and why can a neural network be successfully trained? This article provides an overview of optimization algorithms and theory for training neural networks. First, we discuss the issue of gradient explosion/vanishing and the more general issue of undesirable spectrum, and then discuss practical solutions including careful initialization and normalization methods. Second, we review generic optimization methods used in training neural networks, such as SGD, adaptive gradient methods and distributed methods, and theoretical results for these algorithms. Third, we review existing research on the global issues of neural network training, including results on bad local minima, mode connectivity, lottery ticket hypothesis and infinite-width analysis.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "39447572",
                        "name": "Ruoyu Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Morcos et al. (2019) show that subnetworks found by IMP with rewinding transfer between vision tasks, meaning the effort of finding a subnetworks can be amortized by reusing it many times."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3f06d02513a2763e472d2b5d5db08e9061081b9e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1912-05671",
                    "ArXiv": "1912.05671",
                    "MAG": "3035081900",
                    "CorpusId": 209324341
                },
                "corpusId": 209324341,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3f06d02513a2763e472d2b5d5db08e9061081b9e",
                "title": "Linear Mode Connectivity and the Lottery Ticket Hypothesis",
                "abstract": "We study whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise (e.g., random data order and augmentation). We find that standard vision models become stable to SGD noise in this way early in training. From then on, the outcome of optimization is determined to a linearly connected region. We use this technique to study iterative magnitude pruning (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained in isolation to full accuracy. We find that these subnetworks only reach full accuracy when they are stable to SGD noise, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (ResNet-50 and Inception-v3 on ImageNet).",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "2533850",
                        "name": "G. Dziugaite"
                    },
                    {
                        "authorId": "39331522",
                        "name": "Daniel M. Roy"
                    },
                    {
                        "authorId": "1701041",
                        "name": "Michael Carbin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In our work we use global pruning as used in the paper we are reproducing [4].",
                "The authors [4] empirically demonstrate that the structure of the subnetwork contains significant information.",
                "The authors of the original paper [4] did not release their code.",
                "We use hyperparameters provided by authors to maintain consistency with the paper we are reproducing [4].",
                "We employ late resetting of 1 epoch in all the experiments as used by the authors [4].",
                "As a part of the NeurIPS Reproducibility Challenge\u2019s Replication Track, we replicate the work done by [4] and investigate if the winning ticket initializations are generalizable across datasets and optimizers.",
                "The paper we reproduce, \"One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers\" [4] provides empirical evidence that these winning ticket initializations generalize across multiple datasets as well as optimizers1.",
                "For implementing the random ticket baseline, we generate random masks by globally permuting the winning masks as mentioned in [4].",
                "The initializations, number of epochs, learning rate annealing schedules are in accordance to [4] to maintain consistency of experiments.",
                "1Authors used anywhere in this paper refers to the authors of the paper that we reproduce [4] 2The code base can be found at github."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "5b24064a672c51113820c4b92dda9b65e7976a38",
                "externalIds": {
                    "MAG": "3081549103",
                    "CorpusId": 226814520
                },
                "corpusId": 226814520,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5b24064a672c51113820c4b92dda9b65e7976a38",
                "title": "One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers",
                "abstract": "The lottery ticket hypothesis states that smaller subnetworks within a larger deep network can be trained in isolation to achieve accuracy similar to that of original network, as long as they are initialized appropriately. However, whether these subnetworks or winning tickets are transferable across datasets and optimizers remains unclear. The paper \"One ticket to win them all:generalizing lottery ticket initializations across datasets and optimizers\" empirically shows that these winning tickets are transferable. We reproduce the results in the paper from scratch by implementing all the experiments. Our results support the original paper\u2019s claim of the winning ticket initializations being transferable. While the paper is replicable, we \ufb01nd that reproducing the paper requires access to large amount of computing resources for generating the winning tickets. Hence we also open-source the winning tickets we \ufb01nd, so others can avoid the compute-intensive procedure of generating them.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "81546141",
                        "name": "Varun Gohil"
                    },
                    {
                        "authorId": "31713526",
                        "name": "S. Narayanan"
                    },
                    {
                        "authorId": "1819271266",
                        "name": "Atishay Jain"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2a), which helps to decide which \u2018tickets\u2019 (components) generalize and transfer model-knowledge [25] or specialize it."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8d3ab7196706c8b19a8e2e71e77a4a57c12414e4",
                "externalIds": {
                    "MAG": "2991027961",
                    "ArXiv": "1912.00982",
                    "DBLP": "journals/corr/abs-1912-00982",
                    "CorpusId": 263755992
                },
                "corpusId": 263755992,
                "publicationVenue": {
                    "id": "f9af8000-42f8-410d-a622-e8811e41660a",
                    "name": "Conference on Uncertainty in Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Uncertainty in Artificial Intelligence",
                        "UAI",
                        "Conf Uncertain Artif Intell",
                        "Uncertain Artif Intell"
                    ],
                    "url": "http://www.auai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/8d3ab7196706c8b19a8e2e71e77a4a57c12414e4",
                "title": "TX-Ray: Quantifying and Explaining Model-Knowledge Transfer in (Un-)Supervised NLP",
                "abstract": "While state-of-the-art NLP explainability (XAI) methods focus on explaining per-sample decisions in supervised end or probing tasks, this is insufficient to explain and quantify model knowledge transfer during (un-)supervised training. Thus, for TX-Ray, we modify the established computer vision explainability principle of 'visualizing preferred inputs of neurons' to make it usable transfer analysis and NLP. This allows one to analyze, track and quantify how self- or supervised NLP models first build knowledge abstractions in pretraining (1), and then transfer these abstractions to a new domain (2), or adapt them during supervised fine-tuning (3). TX-Ray expresses neurons as feature preference distributions to quantify fine-grained knowledge transfer or adaptation and guide human analysis. We find that, similar to Lottery Ticket based pruning, TX-Ray based pruning can improve test set generalization and that it can reveal how early stages of self-supervision automatically learn linguistic abstractions like parts-of-speech.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "50747700",
                        "name": "Nils Rethmeier"
                    },
                    {
                        "authorId": "150333998",
                        "name": "V. Saxena"
                    },
                    {
                        "authorId": "2256281059",
                        "name": "Isabelle Augenstein"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Lottery tickets (LTs) have been show to generalize well to new tasks [44]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0de97a5ac2327fd06ffaa51eaf9251bcca5ab81c",
                "externalIds": {
                    "MAG": "3034352268",
                    "CorpusId": 219558860
                },
                "corpusId": 219558860,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0de97a5ac2327fd06ffaa51eaf9251bcca5ab81c",
                "title": "A Conceptual Framework for Lifelong Learning",
                "abstract": "Humans can learn a variety of concepts and skills incrementally over the course of their lives while exhibiting many desirable properties, such as continual learning without forgetting, forward transfer and backward transfer of knowledge, and learning a new concept or task with only a few examples. Several lines of machine learning research, such as lifelong learning, few-shot learning, and transfer learning, attempt to capture these properties. However, most previous approaches can only demonstrate subsets of these properties, often by different complex mechanisms. In this work, we propose a simple yet powerful unified framework that supports almost all of these properties and approaches through one central mechanism. We also draw connections between many peculiarities of human learning (such as memory loss and \"rain man\") and our framework. While we do not present any state-of-the-art results, we hope that this conceptual framework provides a novel perspective on existing work and proposes many new research directions.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1688204",
                        "name": "C. Ling"
                    },
                    {
                        "authorId": "41018496",
                        "name": "Tanner A. Bohn"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Morcos et al. (2019) investigate the transferability of lottery tickets across multiple optimizers and datasets for supervised image classification, showing that tickets can indeed generalize (Morcos et al., 2019).",
                "\u2026of the lottery ticket hypothesis\u2014has shown that sparse networks can be, under certain conditions, easier to optimize (Frankle and Carbin, 2019; Morcos et al., 2019; Gale et al., 2019); and (2) sparser subnetworks have significantly less capacity than their large, over-parameterized\u2026",
                "In contrast, Morcos et al. (2019) transfers the entire ticket (sparse masks and initial values) to the target domain.",
                "(2019) investigate the transferability of lottery tickets across multiple optimizers and datasets for supervised image classification, showing that tickets can indeed generalize (Morcos et al., 2019).",
                "In Morcos et al. (2019), the authors refer to the transfer of initialization as both the transfer of the sparse topologies and the transfer of the initial values of the subnetworks.",
                ", 2018b); and (2) if tickets can generalize across multiple datasets (Morcos et al., 2019).",
                "In particular, (1) whether keeping the same initialization (e.g., \u03b80) is crucial for acquiring tickets (Liu et al., 2018b); and (2) if tickets can generalize across multiple datasets (Morcos et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "22137a602b2afe18a361a03e624a7e8fe66546ea",
                "externalIds": {
                    "ACL": "D19-6117",
                    "DBLP": "conf/acl-deeplo/DesaiZA19",
                    "MAG": "2984218712",
                    "ArXiv": "1910.12708",
                    "DOI": "10.18653/v1/D19-6117",
                    "CorpusId": 202735254
                },
                "corpusId": 202735254,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/22137a602b2afe18a361a03e624a7e8fe66546ea",
                "title": "Evaluating Lottery Tickets Under Distributional Shifts",
                "abstract": "The Lottery Ticket Hypothesis suggests large, over-parameterized neural networks consist of small, sparse subnetworks that can be trained in isolation to reach a similar (or better) test accuracy. However, the initialization and generalizability of the obtained sparse subnetworks have been recently called into question. Our work focuses on evaluating the initialization of sparse subnetworks under distributional shifts. Specifically, we investigate the extent to which a sparse subnetwork obtained in a source domain can be re-trained in isolation in a dissimilar, target domain. In addition, we examine the effects of different initialization strategies at transfer-time. Our experiments show that sparse subnetworks obtained through lottery ticket training do not simply overfit to particular domains, but rather reflect an inductive bias of deep neural networks that can be exploited in multiple domains.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "120777041",
                        "name": "Shrey Desai"
                    },
                    {
                        "authorId": "19277351",
                        "name": "Hongyuan Zhan"
                    },
                    {
                        "authorId": "2057450366",
                        "name": "Ahmed Aly"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026are a lot of parameter redundancy in trained deep model and recent lottery ticket hypothesis (Frankle & Carbin, 2019; Frankle et al., 2019; Zhou et al., 2019; Morcos et al., 2019) shows that a subnetwork in the trained model, if trained alone, could achieve comparable or even better generalization.",
                ", 2015) shows there are a lot of parameter redundancy in trained deep model and recent lottery ticket hypothesis (Frankle & Carbin, 2019; Frankle et al., 2019; Zhou et al., 2019; Morcos et al., 2019) shows that a subnetwork in the trained model, if trained alone, could achieve comparable or even better generalization."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "fd68c59e58ae71e489cb8f517d22ec0832fc2abe",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1909-13458",
                    "MAG": "2975480662",
                    "ArXiv": "1909.13458",
                    "CorpusId": 203593172
                },
                "corpusId": 203593172,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fd68c59e58ae71e489cb8f517d22ec0832fc2abe",
                "title": "Over-parameterization as a Catalyst for Better Generalization of Deep ReLU network",
                "abstract": "To analyze deep ReLU network, we adopt a student-teacher setting in which an over-parameterized student network learns from the output of a fixed teacher network of the same depth, with Stochastic Gradient Descent (SGD). Our contributions are two-fold. First, we prove that when the gradient is zero (or bounded above by a small constant) at every data point in training, a situation called \\emph{interpolation setting}, there exists many-to-one \\emph{alignment} between student and teacher nodes in the lowest layer under mild conditions. This suggests that generalization in unseen dataset is achievable, even the same condition often leads to zero training error. Second, analysis of noisy recovery and training dynamics in 2-layer network shows that strong teacher nodes (with large fan-out weights) are learned first and subtle teacher nodes are left unlearned until late stage of training. As a result, it could take a long time to converge into these small-gradient critical points. Our analysis shows that over-parameterization plays two roles: (1) it is a necessary condition for alignment to happen at the critical points, and (2) in training dynamics, it helps student nodes cover more teacher nodes with fewer iterations. Both improve generalization. Experiments justify our finding. The code is released in this https URL.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "39402399",
                        "name": "Yuandong Tian"
                    }
                ]
            }
        },
        {
            "contexts": [
                " 2016; Han et al., 2015) shows there are a lot of parameter redundancy in trained deep model and recent lottery ticket hypothesis (Frankle &amp; Carbin, 2019; Frankle et al., 2019; Zhou et al., 2019; Morcos et al., 2019) shows that a subnetwork in the trained model, if trained alone, could achieve comparable or even better generalization. Based on our analysis, the intuition to explain both sides of the empirical evi"
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7db3d756dab942145a91598c572c93c7ffee669a",
                "externalIds": {
                    "MAG": "2988618093",
                    "CorpusId": 207863698
                },
                "corpusId": 207863698,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7db3d756dab942145a91598c572c93c7ffee669a",
                "title": "Student Specialization in Deep ReLU Networks With Finite Width and Input Dimension",
                "abstract": "We consider a deep ReLU / Leaky ReLU student network trained from the output of a fixed teacher network of the same depth, with Stochastic Gradient Descent (SGD). The student network is \\emph{over-realized}: at each layer $l$, the number $n_l$ of student nodes is more than that ($m_l$) of teacher. Under mild conditions on dataset and teacher network, we prove that when the gradient is small at every data sample, each teacher node is \\emph{specialized} by at least one student node \\emph{at the lowest layer}. For two-layer network, such specialization can be achieved by training on any dataset of \\emph{polynomial} size $\\mathcal{O}( K^{5/2} d^3 \\epsilon^{-1})$. until the gradient magnitude drops to $\\mathcal{O}(\\epsilon/K^{3/2}\\sqrt{d})$. Here $d$ is the input dimension, $K = m_1 + n_1$ is the total number of neurons in the lowest layer of teacher and student. Note that we require a specific form of data augmentation and the sample complexity includes the additional data generated from augmentation. To our best knowledge, we are the first to give polynomial sample complexity for student specialization of training two-layer (Leaky) ReLU networks with finite depth and width in teacher-student setting, and finite complexity for the lowest layer specialization in multi-layer case, without parametric assumption of the input (like Gaussian). Our theory suggests that teacher nodes with large fan-out weights get specialized first when the gradient is still large, while others are specialized with small gradient, which suggests inductive bias in training. This shapes the stage of training as empirically observed in multiple previous works. Experiments on synthetic and CIFAR10 verify our findings. The code is released in this https URL.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "39402399",
                        "name": "Yuandong Tian"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A finding that suggests a network that is pruned by magnitude consists of an optimal substructure of the original network, known as \u201clottery ticket hypothesis\u201d, was presented in [11], [14].",
                "The ability that we can find lottery tickets is useful when we need to retrain a pruned model on slightly different but similar datasets [14].",
                "To verify whether the final model from adaptive pruning is a lottery ticket [11], [14], we reinitialize this converged model using the original random seed, and compare its accuracy vs.",
                "The lottery ticket is useful for retraining a pruned model on a different yet similar dataset [14]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7638e6f7f379ccf49dacd97e24063a6d664e18b8",
                "externalIds": {
                    "MAG": "2977090839",
                    "DBLP": "journals/corr/abs-1909-12326",
                    "ArXiv": "1909.12326",
                    "DOI": "10.1109/TNNLS.2022.3166101",
                    "CorpusId": 203592134,
                    "PubMed": "35468066"
                },
                "corpusId": 203592134,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7638e6f7f379ccf49dacd97e24063a6d664e18b8",
                "title": "Model Pruning Enables Efficient Federated Learning on Edge Devices",
                "abstract": "Federated learning (FL) allows model training from local data collected by edge/mobile devices while preserving data privacy, which has wide applicability to image and vision applications. A challenge is that client devices in FL usually have much more limited computation and communication resources compared to servers in a data center. To overcome this challenge, we propose PruneFL--a novel FL approach with adaptive and distributed parameter pruning, which adapts the model size during FL to reduce both communication and computation overhead and minimize the overall training time, while maintaining a similar accuracy as the original model. PruneFL includes initial pruning at a selected client and further pruning as part of the FL process. The model size is adapted during this process, which includes maximizing the approximate empirical risk reduction divided by the time of one FL round. Our experiments with various datasets on edge devices (e.g., Raspberry Pi) show that: 1) we significantly reduce the training time compared to conventional FL and various other pruning-based methods and 2) the pruned model with automatically determined size converges to an accuracy that is very similar to the original model, and it is also a lottery ticket of the original model.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1390650607",
                        "name": "Yuang Jiang"
                    },
                    {
                        "authorId": "50695457",
                        "name": "Shiqiang Wang"
                    },
                    {
                        "authorId": "37428000",
                        "name": "Bongjun Ko"
                    },
                    {
                        "authorId": "3140500",
                        "name": "Wei-Han Lee"
                    },
                    {
                        "authorId": "1705536",
                        "name": "L. Tassiulas"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, finding winning tickets hinged on costly (iterative) pruning and retraining (Morcos et al., 2019) studies the reuse of winning tickets, transferable across different datasets.",
                "(Morcos et al., 2019) studies the reuse of winning tickets, transferable across different datasets."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "336868be817536e7c7fc88c391a2860cd869ea2b",
                "externalIds": {
                    "DBLP": "conf/iclr/YouL0FWCBWL20",
                    "MAG": "2995197005",
                    "ArXiv": "1909.11957",
                    "CorpusId": 202888885
                },
                "corpusId": 202888885,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/336868be817536e7c7fc88c391a2860cd869ea2b",
                "title": "Drawing early-bird tickets: Towards more efficient training of deep networks",
                "abstract": "(Frankle & Carbin, 2019) shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve comparable accuracies to the latter in a similar number of iterations. However, the identification of these winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits. In this paper, we discover for the first time that the winning tickets can be identified at the very early training stage, which we term as early-bird (EB) tickets, via low-cost training schemes (e.g., early stopping and low-precision training) at large learning rates. Our finding of EB tickets is consistent with recently reported observations that the key connectivity patterns of neural networks emerge early. Furthermore, we propose a mask distance metric that can be used to identify EB tickets with low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, we leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low-cost schemes, and then continuing to train merely the EB tickets towards the target accuracy. Experiments based on various deep networks and datasets validate: 1) the existence of EB tickets, and the effectiveness of mask distance in efficiently identifying them; and 2) that the proposed efficient training via EB tickets can achieve up to 4.7x energy savings while maintaining comparable or even better accuracy, demonstrating a promising and easily adopted method for tackling cost-prohibitive deep network training.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "47113848",
                        "name": "Haoran You"
                    },
                    {
                        "authorId": "28987646",
                        "name": "Chaojian Li"
                    },
                    {
                        "authorId": "2153916160",
                        "name": "Pengfei Xu"
                    },
                    {
                        "authorId": "108145103",
                        "name": "Y. Fu"
                    },
                    {
                        "authorId": "2118461722",
                        "name": "Yue Wang"
                    },
                    {
                        "authorId": "66273769",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "3138925",
                        "name": "Yingyan Lin"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "144908066",
                        "name": "Richard Baraniuk"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, they can be transferred between datasets [16, 17] and training methods [15].",
                "These have already found applications in, for example, transfer learning [15, 16, 17], making ticket search a problem of independent interest."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "54b8fcf4cc95c0eee93910052018d6286dc78ad9",
                "externalIds": {
                    "MAG": "2995107071",
                    "DBLP": "conf/nips/SavareseSM20",
                    "ArXiv": "1912.04427",
                    "CorpusId": 209140629
                },
                "corpusId": 209140629,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/54b8fcf4cc95c0eee93910052018d6286dc78ad9",
                "title": "Winning the Lottery with Continuous Sparsification",
                "abstract": "The Lottery Ticket Hypothesis conjectures that, for a typically-sized neural network, it is possible to find small sub-networks that, when trained from scratch, match the performance of the dense counterpart given a comparable training budget. The proposed algorithm to search for winning tickets, Iterative Magnitude Pruning, consistently finds sparse sub-networks which train faster and better than the overparameterized models they were extracted from, creating potential applications to problems such as transfer learning. In this paper, we propose Continuous Sparsification, a new algorithm to search for winning tickets which continuously removes parameters from a network during training, and learns the sub-network's structure with gradient-based methods instead of relying on pruning strategies. We show empirically that our method is capable of finding tickets that are sparser than the ones found by Iterative Magnitude Pruning, while achieving higher performance when trained from scratch. Moreover, our method can be efficiently parallelized, decreasing the ticket search cost measured in wall-clock time significantly given enough parallel computing resources.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1387999547",
                        "name": "Pedro H. P. Savarese"
                    },
                    {
                        "authorId": "2064829114",
                        "name": "Hugo Silva"
                    },
                    {
                        "authorId": "145854440",
                        "name": "M. Maire"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Morcos et al. (2019) confirm the findings on the efficacy of global pruning and late resetting.",
                "Although hard to find in larger regimes, when found, lottery tickets have been shown to possess generalization properties that allow for their reuse in similar tasks, thus reducing the computational cost of finding task- and dataset-dependent sparse sub-networks (Morcos et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "result",
                "background"
            ],
            "citingPaper": {
                "paperId": "d3b05583c5b619521975611e3766d69b80a45e0c",
                "externalIds": {
                    "MAG": "3000649712",
                    "DBLP": "journals/corr/abs-2001-05050",
                    "ArXiv": "2001.05050",
                    "CorpusId": 210701547
                },
                "corpusId": 210701547,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d3b05583c5b619521975611e3766d69b80a45e0c",
                "title": "On Iterative Neural Network Pruning, Reinitialization, and the Similarity of Masks",
                "abstract": "We examine how recently documented, fundamental phenomena in deep learning models subject to pruning are affected by changes in the pruning procedure. Specifically, we analyze differences in the connectivity structure and learning dynamics of pruned models found through a set of common iterative pruning techniques, to address questions of uniqueness of trainable, high-sparsity sub-networks, and their dependence on the chosen pruning method. In convolutional layers, we document the emergence of structure induced by magnitude-based unstructured pruning in conjunction with weight rewinding that resembles the effects of structured pruning. We also show empirical evidence that weight stability can be automatically achieved through apposite pruning techniques.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "35550664",
                        "name": "Michela Paganini"
                    },
                    {
                        "authorId": "39774809",
                        "name": "J. Forde"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a07c731e5579ab045a0a51379142a06830ba575b",
                "externalIds": {
                    "MAG": "2994612276",
                    "CorpusId": 210906475
                },
                "corpusId": 210906475,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a07c731e5579ab045a0a51379142a06830ba575b",
                "title": "Winning Privately: The Differentially Private Lottery Ticket Mechanism",
                "abstract": "We propose the differentially private lottery ticket mechanism (DPLTM). An end-to-end differentially private training paradigm based on the lottery ticket hypothesis. Using \u201chigh-quality winners\u201d, selected via our custom score function, DPLTM significantly outperforms state-of-the-art. We show that DPLTM converges faster, allowing for early stopping with reduced privacy budget consumption. We further show that the tickets from DPLTM are transferable across datasets, domains, and architectures. Our extensive evaluation on several public datasets provides evidence to our claims.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "3361384",
                        "name": "Lovedeep Gondara"
                    },
                    {
                        "authorId": "1751643",
                        "name": "Ke Wang"
                    },
                    {
                        "authorId": "145262240",
                        "name": "Ricardo Silva Carvalho"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b72ebbe2156f8fa6a13b33b5a508b621126bf968",
                "externalIds": {
                    "CorpusId": 259935058
                },
                "corpusId": 259935058,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b72ebbe2156f8fa6a13b33b5a508b621126bf968",
                "title": "ML-GUIDED OPTIMIZATION",
                "abstract": "Jan.2015-present Research Scientist and Senior Manager, Meta AI (FAIR) Research Lead in: Long-form Story Generation (2022-) AI-guided Optimization (2019-) Self-supervised and Representation Learning (2020-) Lead Scientist and Engineer of Facebook Go engine. DarkForestGo (2015): Strong CNN-based Go AI before AlphaGo. ELF OpenGo (2018): Superhuman and best open source Go AI that beats professional Go players with 20-0. Replication of AlphaZero with 2k GPUs. Sep.2013-Dec.2014 Researcher / Software Engineer Vision and Learning Group, Driverless Car Team (Waymo), Google X Real-time object recognition for autonomous driving car.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "39402399",
                        "name": "Yuandong Tian"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The Lottery Ticket Hypothesis (LTH) proposed in (Frankle & Carbin, 2019) also shows that a sparse subnetwork with comparable performance to the dense model can be found via a combination of IMP and weight resetting (Frankle & Carbin, 2019; Morcos et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "fc30ff7dd97f5e5f50d714936937cf8bbb49fd1e",
                "externalIds": {
                    "DBLP": "conf/icml/MoSP23",
                    "CorpusId": 260779607
                },
                "corpusId": 260779607,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/fc30ff7dd97f5e5f50d714936937cf8bbb49fd1e",
                "title": "Pruning via Sparsity-indexed ODE: a Continuous Sparsity Viewpoint",
                "abstract": "Neural pruning, which involves identifying the optimal sparse subnetwork, is a key technique for reducing the complexity and improving the efficiency of deep neural networks. To address the challenge of solving neural pruning at a specific sparsity level directly, we investigate the evolution of optimal subnetworks with continuously increasing sparsity, which can provide insight into how to transform an unpruned dense model into an optimal subnetwork with any desired level of sparsity. In this paper, we proposed a novel pruning framework, coined Sparsity-indexed ODE (SpODE) that provides explicit guidance on how to best preserve model performance while ensuring an infinitesimal increase in model sparsity. On top of this, we develop a pruning algorithm, termed Pruning via Sparsity-indexed ODE (PSO), that enables effective pruning via traveling along the SpODE path. Empirical experiments show that PSO achieves either better or comparable performance compared to state-of-the-art baselines across various pruning settings. Our implementations are now available on GitHub 1 .",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2159249903",
                        "name": "Zhanfeng Mo"
                    },
                    {
                        "authorId": "2108661275",
                        "name": "Haosen Shi"
                    },
                    {
                        "authorId": "1746914",
                        "name": "Sinno Jialin Pan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e747dce851ce8f302814eaf1d65ac99aa82963ff",
                "externalIds": {
                    "CorpusId": 261257952
                },
                "corpusId": 261257952,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e747dce851ce8f302814eaf1d65ac99aa82963ff",
                "title": "Chasing Better Deep Image Priors between Over-and Under-parameterization",
                "abstract": ".",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2650694",
                        "name": "Qiming Wu"
                    },
                    {
                        "authorId": "2116298570",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "2235293441",
                        "name": "Yifan Jiang"
                    },
                    {
                        "authorId": "2227945855",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "def0ece578fd4b9ad3c682e8b979c5ac4d47e936",
                "externalIds": {
                    "DOI": "10.1162/isal_a_00664",
                    "CorpusId": 262197442
                },
                "corpusId": 262197442,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/def0ece578fd4b9ad3c682e8b979c5ac4d47e936",
                "title": "Finding Sparse Initialisations using Neuroevolutionary Ticket Search (NeTS)",
                "abstract": "The explosion of interest in deep learning over the last decade has been driven by scaling mathematical models of brains referred to as Deep Neural Networks (DNNs). Common deep learning architectures have millions of parameters that require optimisation (training) for the network to learn tasks; and empirical evidence suggests training an overparame-terised network (with more parameters than data points in the training data) is necessary for the network to learn. Despite this, it has been shown that overparameterised DNNs can typically be compressed to a fraction of their original size once trained (frequently by over 90%). Further, the Lottery Ticket Hypothesis (LTH) asserts that there exists a much sparser but equally trainable subnetwork within any sufficiently overpa-rameterised DNN initialisation . In other words, the number of parameters could be significantly reduced from the outset if we could find these smaller initialisations (referred to as winning tickets ). In this paper, we introduce a new evolutionary algorithm for finding winning tickets given a feed-forward or convolutional DNN architecture, and compare our approach to the current state-of-the-art. We refer to our algorithm as Neuroevolutionary Ticket Search (NeTS) and find it discovers competitive winning tickets for a variety of architectures and two common training datasets (MNIST and CIFAR-10). We show that NeTS can be applied to pruning DNNs before substantial training with gradient descent by genetically optimising a genome consisting of a set of initial weights and a binary pruning mask; this appears to offer a significant performance benefit.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2244312780",
                        "name": "Alex Jackson"
                    },
                    {
                        "authorId": "1485377354",
                        "name": "Nandi Schoots"
                    },
                    {
                        "authorId": "2244497162",
                        "name": "Amin Ahantab"
                    },
                    {
                        "authorId": "1721762",
                        "name": "Michael Luck"
                    },
                    {
                        "authorId": "2244493660",
                        "name": "Elizabeth Black"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "090d2541909e9717dd4b607fdf13d27b59c09da2",
                "externalIds": {
                    "CorpusId": 263630488
                },
                "corpusId": 263630488,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/090d2541909e9717dd4b607fdf13d27b59c09da2",
                "title": "N EUROEVOLUTION T ICKET S EARCH \u2014 F INDING S PARSE , T RAINABLE DNN I NITIALISATIONS",
                "abstract": "The Lottery Ticket Hypothesis (LTH) asserts that a randomly initialised over-parameterised Deep Neural Network (DNN) contains a sparse subnetwork that, when trained (up to) the same amount as the original network, performs just as well. So-called winning tickets are vastly more efficient to train than dense networks; however, finding such tickets currently relies on pre-training an overpa-rameterised network via Iterative Magnitude Pruning (IMP). Due to the increasing demand for computing resources, there are major incentives to find good search procedures for winning tickets (which can drastically reduce the training time required to solve a range of problems). In this paper, we propose a new method for the evolution of sparse DNN initialisations that generates results commensurate with winning ticket search procedures: we refer to the method as Neuroevolution Ticket Search (NeTS). By training an overparameterised network using gradient descent for use as a baseline only, we show that NeTS quickly converges to near state-of-the-art performance in terms of network size and trainability. Additionally, we find that NeTS appears to be a competitive alternative to IMP in terms of wall clock time.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2244312780",
                        "name": "Alex Jackson"
                    },
                    {
                        "authorId": "1485377354",
                        "name": "Nandi Schoots"
                    },
                    {
                        "authorId": "2253792649",
                        "name": "Michael Luck"
                    },
                    {
                        "authorId": "2244493660",
                        "name": "Elizabeth Black"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In LTH, first a large network is trained and pruned to be a small subnetwork S, then retraining S using its original initialization yields comparable or even better performance, while retraining S with a different initialization performs much worse.",
                "Other empirical observations like lottery ticket hypothesis (LTH) [11; 24; 30; 35], recently also verified in CL [5], may also be explained similarly.",
                "For LTH, our explanation is that S contains weights that are initialized luckily, i.e., close to useful local optima and converge to them during training.",
                "[24] Ari Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "31b2a1aeaa6d3e4832b3545fb08c4b2353356f78",
                "externalIds": {
                    "CorpusId": 259143622
                },
                "corpusId": 259143622,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/31b2a1aeaa6d3e4832b3545fb08c4b2353356f78",
                "title": "On the Role of Nonlinearity in Training Dynamics of Contrastive Learning on One-layer Network",
                "abstract": "While the empirical success of self-supervised learning (SSL) heavily relies on the usage of deep nonlinear models, existing theoretical works on SSL understanding still focus on linear ones. In this paper, we study the role of nonlinearity in the training dynamics of contrastive learning (CL) on 1-layer nonlinear networks with homogeneous activation h ( x ) = h \u2032 ( x ) x , by extending recent \u03b1 -CL framework [29] and linking it to kernels [26]. We find that the presence of nonlinearity can lead to many local optima even in 1-layer setting, each corresponding to certain patterns from the data distribution, while with linear activation, only one major pattern can be learned. This suggests that models with lots of parameters can be regarded as a brute-force way to find these local optima induced by nonlinearity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39402399",
                        "name": "Yuandong Tian"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6291e9976067bbed346b9e50dac17225e8d1b6f1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-14328",
                    "DOI": "10.48550/arXiv.2203.14328",
                    "CorpusId": 247763112
                },
                "corpusId": 247763112,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6291e9976067bbed346b9e50dac17225e8d1b6f1",
                "title": "On the Neural Tangent Kernel Analysis of Randomly Pruned Wide Neural Networks",
                "abstract": "We study the behavior of ultra-wide neural networks when their weights are randomly pruned at the initialization, through the lens of neural tangent kernels (NTKs). We show that for fullyconnected neural networks when the network is pruned randomly at the initialization, as the width of each layer grows to infinity, the empirical NTK of the pruned neural network converges to that of the original (unpruned) network with some extra scaling factor. Further, if we apply some appropriate scaling after pruning at the initialization, the empirical NTK of the pruned network converges to the exact NTK of the original network, and we provide a non-asymptotic bound on the approximation error in terms of pruning probability. Moreover, when we apply our result to an unpruned network (i.e., we set the probability of pruning a given weight to be zero), our analysis is optimal up to a logarithmic factor in width compared with the result in (Arora et al., 2019). We conduct experiments to validate our theoretical results. We further test our theory by evaluating random pruning across different architectures via image classification on MNIST and CIFAR-10 and compare its performance with other pruning strategies.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118571035",
                        "name": "Hongru Yang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Then, each layer density is automatically generated based on this ratio through the algorithm described in the research (Morcos et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "082b0eb811d9b60f968ca8c784d0879f8afbf660",
                "externalIds": {
                    "CorpusId": 249455412
                },
                "corpusId": 249455412,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/082b0eb811d9b60f968ca8c784d0879f8afbf660",
                "title": "LSOP: Layer-Scaled One-shot Pruning A Simple and Effective Deep Pruning Framework for Neural Networks",
                "abstract": "Neural network pruning is a technique that removes unnecessary weight parameters from a network to decrease its memory and computational requirements. Many different pruning techniques have been proposed to reduce networks with over 90% shrinkage in size while minimizing accuracy loss. This paper aims to establish a framework that can generalize the mechanism among various pruning techniques, which can be used to guide users to design better deep pruning methods in the future. With some basic concepts and findings from data matrix approximation, the framework can explain the success of the state-of-the-art methods as well as a more generalized pruning design (Layer-Scaled One-shot Pruning, LSOP) proposed in this work. After pruning with different algorithms and measur-ing their accuracies, the researcher also found that those methods or algorithms aligned with the proposed framework were more accurate at sparser networks (density < 10%) than the methods that did not. This suggests that future research into neural network pruning can focus on the proposed framework, which has the potential to acceler-ate the development of pruning technology and adoption of more efficient neural networks. The LSOP framework\u2019s capability to explain strong pruning performances implies that polynomial decay and Low-Rank Matrix Approximation techniques from the field of data science can provide support for neural network pruning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39231399",
                        "name": "Oliver Wang"
                    },
                    {
                        "authorId": "144734756",
                        "name": "C. Dovrolis"
                    },
                    {
                        "authorId": null,
                        "name": "Jaeho Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(Morcos et al., 2019; Chen et al., 2020; 2021a;b) studied the transferability of winning tickets between datasets, tasks and architectures."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "aa30ab243b2d6c2b8b4396033d0e032732adaf81",
                "externalIds": {
                    "CorpusId": 251732832
                },
                "corpusId": 251732832,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/aa30ab243b2d6c2b8b4396033d0e032732adaf81",
                "title": "P EEK - A -B OO : W HAT (M ORE ) IS D ISGUISED IN A R ANDOMLY W EIGHTED N EURAL N ETWORK , AND H OW TO F IND I T E FFICIENTLY",
                "abstract": "Sparse neural networks (NNs) are intensively investigated in literature due to their appeal in saving storage, memory, and computational costs. A recent showed that, different from conventional pruning-and-\ufb01netuning pipeline, there exist hidden subnetworks in randomly initialized NNs that have good performance without training the weights. However, such \u201chidden subnetworks\u201d have mediocre performances and require an expensive edge-popup algorithm to search for them. In this work, we de\ufb01ne an extended class of subnetworks in randomly initialized NNs called disguised subnetworks , which are not only \u201chidden\u201d in the random networks but also \u201cdisguised\u201d \u2013 hence can only be \u201cunmasked\u201d with certain transformations on weights. We argue that the unmasking process plays an important role in enlarging the capacity of the subnetworks and thus grants two major bene\ufb01ts: (i) the disguised subnetworks easily outperform the hidden counterparts; (ii) the unmasking process helps to relax the quality requirement on the sparse subnetwork mask so that the expensive edge-popup algorithm can be replaced with more ef\ufb01cient alternatives. On top of this new concept, we propose a novel two-stage algorithm that plays a P eek- a - B oo ( PaB ) game to identify the disguised subnetworks with a combination of two operations: (1) searching ef\ufb01ciently for a subnetwork at random initialization ; (2) unmasking the disguise by learning to trans-form the resulting subnetwork\u2019s remaining weights. Furthermore, we show that the unmasking",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2116298570",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "49050667",
                        "name": "Jason Zhang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also noticed is that the winning tickets identified from a larger dataset usually have better transferability, which is in accordance with (Morcos et al., 2019).",
                "Several pioneer works (Mehta, 2019; Morcos et al., 2019; Desai et al., 2019; Chen et al., 2020b;a) also investigate LTH transferability across datasets and downstream tasks.",
                "However, at extremely high sparsity, the performance of transferring tickets degrades faster than the winning tickets identified on the target datasets, corresponding to the observations from previous studies (Chen et al., 2020c; Morcos et al., 2019; Chen et al., 2021e)."
            ],
            "isInfluential": false,
            "intents": [
                "result",
                "background"
            ],
            "citingPaper": {
                "paperId": "b30673f3d48e3b2106158ff7a3b3fff9b748cf3d",
                "externalIds": {
                    "CorpusId": 251734698
                },
                "corpusId": 251734698,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b30673f3d48e3b2106158ff7a3b3fff9b748cf3d",
                "title": "A UDIO L OTTERY : S PEECH R ECOGNITION M ADE U LTRA -L IGHTWEIGHT , T RANSFERABLE , AND N OISE R OBUST",
                "abstract": "Lightweight speech recognition models have seen explosive demands owing to a growing amount of speech-interactive features on mobile devices. Since designing such systems from scratch is non-trivial, practitioners typically choose to compress large (pre-trained) speech models. Recently, lottery ticket hypothesis reveals the existence of highly sparse subnetworks that can be trained in isolation without sacrificing the performance of the full models. In this paper, we investigate the tantalizing possibility of using lottery ticket hypothesis to discover lightweight speech recognition models, that are (1) robust to various noise existing in speech; (2) transferable to fit the open-world personalization; and 3) compatible with structured sparsity. We conducted extensive experiments on CNN-LSTM, RNNTransducer, and Transformer models, and verified the existence of highly sparse \u201cwinning tickets\u201d that can match the full model performance across those backbones. We obtained winning tickets that have less than 20% of full model weights on all backbones, while the most lightweight one only keeps 4.4% weights. Those winning tickets generalize to structured sparsity with no performance loss, and transfer exceptionally from large source datasets to various target datasets. Perhaps most surprisingly, when the training utterances have high background noises, the winning tickets even substantially outperform the full models, showing the extra bonus of noise robustness by inducing sparsity. Codes are available at https://github.com/VITA-Group/Audio-Lottery.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51267247",
                        "name": "Shaojin Ding"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The kind of tolerance, combined with the proven generalizability of winning tickets across different tasks, models, and datasets (Morcos et al., 2019), with more efficient methods of finding the winning tickets become available (Tanaka et al., 2020; You et al., 2020).",
                "The kind of tolerance, combined with the proven generalizability of winning tickets across different tasks, models, and datasets (Morcos et al., 2019), with more efficient methods of finding the winning tickets become available (Tanaka et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "786bb76f24c470b004b5f227abda6936dda4dd71",
                "externalIds": {
                    "CorpusId": 251736011
                },
                "corpusId": 251736011,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/786bb76f24c470b004b5f227abda6936dda4dd71",
                "title": "R EVISIT K ERNEL P RUNING WITH L OTTERY R EGU LATED G ROUPED C ONVOLUTIONS",
                "abstract": "Structured pruning methods which are capable of delivering a densely pruned network are among the most popular techniques in the realm of neural network pruning, where most methods prune the original network at a filter or layer level. Although such methods may provide immediate compression and acceleration benefits, we argue that the blanket removal of an entire filter or layer may result in undesired accuracy loss. In this paper, we revisit the idea of kernel pruning (to only prune one or several k \u00d7 k kernels out of a 3D-filter), a heavily overlooked approach under the context of structured pruning. This is because kernel pruning will naturally introduce sparsity to filters within the same convolutional layer \u2014 thus, making the remaining network no longer dense. We address this problem by proposing a versatile grouped pruning framework where we first cluster filters from each convolutional layer into equal-sized groups, prune the grouped kernels we deem unimportant from each filter group, then permute the remaining filters to form a densely grouped convolutional architecture (which also enables the parallel computing capability) for fine-tuning. Specifically, we consult empirical findings from a series of literature regarding Lottery Ticket Hypothesis to determine the optimal clustering scheme per layer, and develop a simple yet cost-efficient greedy approximation algorithm to determine which group kernels to keep within each filter group. Extensive experiments also demonstrate our method often outperforms comparable SOTA methods with lesser data augmentation needed, smaller finetuning budget required, and sometimes even much simpler procedure executed (e.g., one-shot v. iterative). Please refer to our GitHub repository for code.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2182378625",
                        "name": "Henry Zhong"
                    },
                    {
                        "authorId": "2119056364",
                        "name": "Guanqun Zhang"
                    },
                    {
                        "authorId": "2181913772",
                        "name": "Ningjia Huang"
                    },
                    {
                        "authorId": "2149230615",
                        "name": "Shuai Xu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1de4d197c23f6adea4df27977a6c8be2f7928962",
                "externalIds": {
                    "CorpusId": 252991214
                },
                "corpusId": 252991214,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1de4d197c23f6adea4df27977a6c8be2f7928962",
                "title": "Improved Sparse Optimization for Machine Learning",
                "abstract": "We consider the fundamental task of recovering sparse minimizers of a (convex) real function. Instances of this problem are often encountered in machine learning, where the goal is to find small models that can fit the training data. This is especially relevant today, when the mere size of models employed becomes so prohibitive that even transfering them across devices becomes an important challenge [HABN+21]. Previously, sparse minimization had found a lot of traction in the field of compressed sensing [MZ93, CT05, C+06, CRT06, BD09, JTK14]. A series of exciting works have shown that it is possible to recover sparse signals from a small number of measurements. This was surprising as it turned out that recovering an s-sparse signal x1 can be done efficiently using e O (s)2 linear measurements of x. While the underlying problem of finding a sparse solution to an underdetermined linear system is NP-hard in general, it turns out that structure can be leveraged to obtain beyond the worst case guarantees. In this case, the structure of the measurements was crucial to show that recovery can be performed in polynomial time. Furthermore, sparse optimization has been succesfully applied in multiple areas of machine learning and signal processing, including recommender systems, matrix factorization, or robust principal components analysis. While this line of work has been extremely successful, the specific limits of these methods are far from being understood. For example, the current state of the art shows that one can obtain a minimizer as good as the best s-sparse one, while sacrificing a factor of O (\uf8ff) in sparsity, where \uf8ff is a certain condition number of the underlying function [AS20, AS22]. Classical works in compressed sensing considered only the case of constant \uf8ff, but in many situations this can be quite large, thus giving a prohibitive blow up in sparsity. It is therefore important to ask",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2869958",
                        "name": "Adrian Vladu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "331 [35] Ari S Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian.",
                "In [4, 35] 61 a \"lottery ticket hypothesis\" was proposed that with an optimal substructure of the neural network 62 acquired by weights pruning, directly training a pruned model could reach similar results as pruning 63 a pre-trained network."
            ],
            "isInfluential": false,
            "intents": [
                "result",
                "background"
            ],
            "citingPaper": {
                "paperId": "ee4961f5c882aa63d48fcc732c66ee5b9ad56d0f",
                "externalIds": {
                    "CorpusId": 253162408
                },
                "corpusId": 253162408,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ee4961f5c882aa63d48fcc732c66ee5b9ad56d0f",
                "title": "Federated Learning with Online Adaptive Heterogeneous Local Models",
                "abstract": "In Federated Learning, one of the biggest challenges is that client devices often have 1 drastically different computation and communication resources for local updates. 2 To this end, recent research efforts have focused on training heterogeneous local 3 models that are obtained by adaptively pruning a shared global model. Despite the 4 empirical success, theoretical analysis of the convergence of these heterogeneous 5 FL algorithms remains an open question. In this paper, we establish sufficient 6 conditions for any FL algorithms with heterogeneous local models to converge to a 7 neighborhood of a stationary point of standard FL at a rate of O ( 1 \u221a Q ) . For general 8 smooth cost functions and under standard assumptions, our analysis illuminates 9 two key factors impacting the optimality gap between heterogeneous and standard 10 FL: pruning-induced noise and minimum coverage index, advocating a joint design 11 strategy of local models\u2019 pruning masks in heterogeneous FL algorithms. The 12 results are numerically validated on MNIST and CIFAR-10 datasets. 13 stationary point of standard FL (with a small optimality gap that is characterized in our analysis), 36 at a rate of O ( 1 \u221a Q ) in Q communication rounds. We prove a new upperbound and show that the 37 optimality gap (between heterogeneous and standard FL) is affected by both pruning-induced noise (as identified in single-model pruning) and a new notion of minimum coverage index in FL (i.e., any 39 parameters in the global model are included in at least \u0393 min local models). Our results also motivate 40 a joint design of efficient local-model pruning strategies (e.g., leveraging [10\u201312]) for heterogeneous 41 FL to have comparable convergence with standard FL. It captures a number of existing FL algorithms 42 and provides a general convergence guarantee. We perform extensive experiments on MNIST and 43 CIFAR10 datasets. Our numerical evaluations validate the sufficient conditions established in our 44 convergence analysis. 45",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2007579020",
                        "name": "Hanhan Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c8432138866db832d44f51952da3d49633c2ea30",
                "externalIds": {
                    "DBLP": "conf/icml/PellegriniB22",
                    "CorpusId": 250340597
                },
                "corpusId": 250340597,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c8432138866db832d44f51952da3d49633c2ea30",
                "title": "Neural Network Pruning Denoises the Features and Makes Local Connectivity Emerge in Visual Tasks",
                "abstract": "Pruning methods can considerably reduce the size of arti\ufb01cial neural networks without harming their performance and in some cases they can even uncover sub-networks that, when trained in isolation, match or surpass the test accuracy of their dense counterparts. Here, we characterize the inductive bias that pruning imprints in such \u201cwinning lottery tickets\u201d: focusing on visual tasks, we analyze the architecture resulting from iterative magnitude pruning of a simple fully connected network. We show that the surviving node connectivity is local in input space, and organized in patterns reminiscent of the ones found in convolutional networks. We investigate the role played by data and tasks in shaping the architecture of the pruned sub-network. We \ufb01nd that pruning performances, and the ability to sift out the noise and make local features emerge improve by increasing the size of the training set, and the semantic value of the data. We also study different pruning procedures, and \ufb01nd that iterative magnitude pruning is particularly effective in distilling meaningful connectivity out of features present in the original task. Our results suggest the possibility to auto-matically discover new and ef\ufb01cient architectural inductive biases in other datasets and tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39324024",
                        "name": "F. Pellegrini"
                    },
                    {
                        "authorId": "2188423",
                        "name": "G. Biroli"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Jump-Start is motivated by the work of [22], which found that an LTN trained on one image classification dataset can be finetuned to other datasets without much loss in accuracy."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2e5abc48b026bb989b2bdf0e9c977912aed6e206",
                "externalIds": {
                    "DBLP": "conf/eccv/MugunthanLGLKP22",
                    "DOI": "10.1007/978-3-031-19775-8_5",
                    "CorpusId": 253121182
                },
                "corpusId": 253121182,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/2e5abc48b026bb989b2bdf0e9c977912aed6e206",
                "title": "FedLTN: Federated Learning for Sparse and Personalized Lottery Ticket Networks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35400446",
                        "name": "Vaikkunth Mugunthan"
                    },
                    {
                        "authorId": "2062311951",
                        "name": "Eric Lin"
                    },
                    {
                        "authorId": "9929834",
                        "name": "V. Gokul"
                    },
                    {
                        "authorId": "2188861871",
                        "name": "Christian Lau"
                    },
                    {
                        "authorId": "1735243",
                        "name": "Lalana Kagal"
                    },
                    {
                        "authorId": "2066087424",
                        "name": "Steven D. Pieper"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ee00f3247492d7b8d1cea951b91a9bebc1ba9377",
                "externalIds": {
                    "CorpusId": 263672989
                },
                "corpusId": 263672989,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ee00f3247492d7b8d1cea951b91a9bebc1ba9377",
                "title": "Transforming Large-size to Lightweight Deep Neural Networks for IoT Applications: A Review",
                "abstract": "Deep Neural Networks (DNN) have gained unprecedented popularity due to their high-order performance and automated feature extraction capability. It encourages researchers to incorporate DNN in diferent Internet of Things (IoT) applications in recent years. However, the colossal requirement of computation, energy, and storage of DNN make their deployment prohibitive on resource constraint IoT devices. Therefore, several compression techniques have been proposed in recent years to reduce the energy, storage, and computation requirements of the DNN. These techniques have utilized a diferent perspective for compressing DNN with minimal accuracy compromise. It encourages us to comprehensively overview the DNN compression techniques for the IoT. This paper presents a comprehensive overview of existing literature on compressing the DNN that reduces energy consumption, storage, and computation requirements for IoT applications. We divide the existing approaches into ive broad categories, i.e., network pruning, sparse representation, bits precision, knowledge distillation, and miscellaneous, based upon the mechanism incorporated for compressing the DNN. The paper discusses the challenges associated with each category of DNN compression techniques and presents some prominent applications using IoT in conjunction with compressed DNN. Finally, we provide a quick summary of existing work under each category with the future direction in DNN compression.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2254307546",
                        "name": "Rahul Mishra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition to supervised image classification, LTH has been explored widely in numerous contexts, such as natural language processing [35, 36], reinforcement learning [37, 38], pre-training/transfer learning [39, 40, 41], and efficient training [42, 43]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e21cb9cc7470f3c94c44ac58c6de22303add3bf8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-03255",
                    "CorpusId": 231572951
                },
                "corpusId": 231572951,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e21cb9cc7470f3c94c44ac58c6de22303add3bf8",
                "title": "Good Students Play Big Lottery Better",
                "abstract": "Lottery ticket hypothesis [1] suggests that a dense neural network contains a sparse sub-network that can match the test accuracy of the original dense net when trained in isolation from (the same) random initialization. However, the hypothesis failed to generalize to larger dense networks such as ResNet-50. As a remedy, recent studies [2, 3] demonstrate that a sparse sub-network can still be obtained by using a rewinding technique, which is to re-train it from early-phase training weights or learning rates of the dense model, rather than from random initialization. Is rewinding the only or the best way to scale up lottery tickets ? This paper proposes a new, simpler and yet powerful technique for re-training the sub-network, called \"Knowledge Distillation ticket\" ( KD ticket ). Rewinding exploits the value of inheriting knowledge from the early training phase to improve lottery tickets in large networks. In comparison, KD ticket addresses a complementary possibility - inheriting useful knowledge from the late training phase of the dense model. It is achieved by leveraging the soft labels generated by the trained dense model to re-train the sub-network, instead of the hard labels. Extensive experiments are conducted using several large deep networks (e.g ResNet-50 and ResNet-110) on CIFAR-10 and ImageNet datasets. Without bells and whistles, when applied by itself, KD ticket performs on par or better than rewinding, while being nearly free of hyperparameters or ad-hoc selection. KD ticket can be further applied together with rewinding, yielding state-of-the-art results for large-scale lottery tickets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2126795",
                        "name": "Haoyu Ma"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "3236115",
                        "name": "Ting-Kuei Hu"
                    },
                    {
                        "authorId": "2061592207",
                        "name": "Chenyu You"
                    },
                    {
                        "authorId": "2111366691",
                        "name": "Xiaohui Xie"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "dfbafb4bf4e972faf38af944b37828f3865f9af9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-11068",
                    "CorpusId": 231986132
                },
                "corpusId": 231986132,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dfbafb4bf4e972faf38af944b37828f3865f9af9",
                "title": "Lottery Ticket Implies Accuracy Degradation, Is It a Desirable Phenomenon?",
                "abstract": "In deep model compression, the recent finding \u201cLottery Ticket Hypothesis\u201d (LTH) (Frankle & Carbin, 2018) pointed out that there could exist a winning ticket (i.e., a properly pruned subnetwork together with original weight initialization) that can achieve competitive performance than the original dense network. However, it is not easy to observe such winning property in many scenarios, where for example, a relatively large learning rate is used even if it benefits training the original dense model. In this work, we investigate the underlying condition and rationale behind the winning property, and find that the underlying reason is largely attributed to the correlation between initialized weights and final-trained weights when the learning rate is not sufficiently large. Thus, the existence of winning property is correlated with an insufficient DNN pretraining, and is unlikely to occur for a well-trained DNN. To overcome this limitation, we propose the \u201cpruning & fine-tuning\u201d method that consistently outperforms lottery ticket sparse training under the same pruning algorithm and the same total training epochs. Extensive experiments over multiple deep models (VGG, ResNet, MobileNetv2) on different datasets have been conducted to justify our proposals.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145915285",
                        "name": "Ning Liu"
                    },
                    {
                        "authorId": "9347641",
                        "name": "Geng Yuan"
                    },
                    {
                        "authorId": "1939695",
                        "name": "Zhengping Che"
                    },
                    {
                        "authorId": "2007668856",
                        "name": "Xuan Shen"
                    },
                    {
                        "authorId": "151480882",
                        "name": "Xiaolong Ma"
                    },
                    {
                        "authorId": "153792333",
                        "name": "Qing Jin"
                    },
                    {
                        "authorId": "144139198",
                        "name": "Jian Ren"
                    },
                    {
                        "authorId": "2115854503",
                        "name": "Jian Tang"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "46393431",
                        "name": "Yanzhi Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "bb51d822f06a320066d32f11e8c910ff2ad63db5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-06460",
                    "CorpusId": 232185631
                },
                "corpusId": 232185631,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bb51d822f06a320066d32f11e8c910ff2ad63db5",
                "title": "Emerging Paradigms of Neural Network Pruning",
                "abstract": "Over-parameterization of neural networks benefits the optimization and generalization yet brings cost in practice. Pruning is adopted as a post-processing solution to this problem, which aims to remove unnecessary parameters in a neural network with little performance compromised. It has been broadly believed the resulted sparse neural network cannot be trained from scratch to comparable accuracy. However, several recent works (e.g., [Frankle and Carbin, 2019a]) challenge this belief by discovering random sparse networks which can be trained to match the performance with their dense counterpart. This new pruning paradigm later inspires more new methods of pruning at initialization. In spite of the encouraging progress, how to coordinate these new pruning fashions with the traditional pruning has not been explored yet. This survey seeks to bridge the gap by proposing a general pruning framework so that the emerging pruning paradigms can be accommodated well with the traditional one. With it, we systematically reflect the major differences and new insights brought by these new pruning fashions, with representative works discussed at length. Finally, we summarize the open questions as worthy future directions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46507194",
                        "name": "Haiquan Wang"
                    },
                    {
                        "authorId": "12282768",
                        "name": "Can Qin"
                    },
                    {
                        "authorId": "2129519081",
                        "name": "Yulun Zhang"
                    },
                    {
                        "authorId": "46956675",
                        "name": "Y. Fu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "a small subnetwork, by studying stabilization [9] and generalization [23]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2c443cc87a1bd847fe38905840f09228cf08f1f2",
                "externalIds": {
                    "DBLP": "journals/spl/WangPL21",
                    "DOI": "10.1109/LSP.2021.3082036",
                    "CorpusId": 235308713
                },
                "corpusId": 235308713,
                "publicationVenue": {
                    "id": "d5da7004-7b61-450a-9c7d-a39500de7acf",
                    "name": "IEEE Signal Processing Letters",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Signal Process Lett"
                    ],
                    "issn": "1070-9908",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=97"
                },
                "url": "https://www.semanticscholar.org/paper/2c443cc87a1bd847fe38905840f09228cf08f1f2",
                "title": "Memory-Free Stochastic Weight Averaging by One-Way Variational Pruning",
                "abstract": "Recent works on convolutional neural networks (CNN) have attempted to find the local optima with ensemble-based approaches. Fast Geometric Ensemble (FGE) showed that captured weight points at the end of training time circulate local optima. This led to the Stochastic Weight Averaging (SWA) approach, which averages multiple model weights to find the local optima. However, they are limited by their output of fully-parameterized models, including needless parameters, after the training procedure. To solve this problem, we propose a novel training procedure: Stochastic Weight Averaging by One-way Variational Pruning (SWA-OVP). SWA-OVP reduces the number of model parameters by variationally updating the mask of weights for pruning. SWA-OVP variationally generates a mask for pruned weights in each iteration while recent pruning approaches produce the mask at the end of each training. In addition, our SWA-OVP prunes the model in a one-way training procedure, while other recent approaches prune the model weights in iterative training or require additional computation. Our experiment shows that SWA-OVP using only a 0.5x%$\\sim$0.7x% parameter size achieves even higher accuracy than SWA and FGE on several networks, such as Pre-ResNet110, Pre-ResNet164 and WideResNet28x10 on CIFAR10 and CIFAR100 datasets. SWA-OVP also achieves better performance compared to state-of-the-art pruning approaches.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108776112",
                        "name": "Yooseung Wang"
                    },
                    {
                        "authorId": "2110354994",
                        "name": "Hyunseong Park"
                    },
                    {
                        "authorId": "2108287028",
                        "name": "Jwajin Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Meanwhile, [7, 8, 12, 49, 54] thoroughly investigate the transferability of such intriguing matching subnetworks."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "cf1b59d11acce66875b59902688e3b1b8b0764f9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-14568",
                    "CorpusId": 235658057
                },
                "corpusId": 235658057,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cf1b59d11acce66875b59902688e3b1b8b0764f9",
                "title": "FreeTickets: Accurate, Robust and Efficient Deep Ensemble by Training with Dynamic Sparsity",
                "abstract": "Recent works on sparse neural networks have demonstrated that it is possible to train a sparse network in isolation to match the performance of the corresponding dense networks with a fraction of parameters. However, the iden-ti\ufb01cation of these performant sparse neural networks (win-ning tickets) either involves a costly iterative train-prune-retrain process (e.g., Lottery Ticket Hypothesis) or an over-extended sparse training time (e.g., Training with Dynamic Sparsity), both of which would raise \ufb01nancial and environmental concerns. In this work, we attempt to address this cost-reducing problem by introducing the FreeTickets concept, as the \ufb01rst solution which can boost the performance of sparse convolutional neural networks over their dense network equivalents by a large margin, while using for complete training only a fraction of the computational resources required by the latter. Concretely, we instantiate the FreeTickets concept, by proposing two novel ef\ufb01cient ensemble methods with dynamic sparsity, which yield in one shot many diverse and accurate tickets \u201cfor free\u201d during the sparse training process. The combination of these free tickets into an ensemble demonstrates a signi\ufb01cant improvement in accuracy, uncertainty estimation, robustness, and ef\ufb01ciency over the corresponding dense (ensemble) networks. Our results provide new",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "1768130522",
                        "name": "Zahra Atashgahi"
                    },
                    {
                        "authorId": "2116298570",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "67102129",
                        "name": "Ghada Sokar"
                    },
                    {
                        "authorId": "146634864",
                        "name": "Elena Mocanu"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "LTH has been explored extensively in practice and was shown to be applicable in even large-scale settings [8, 21, 22, 42, 48, 70, 71].",
                "[48] Ari S Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "45b31d499ed297927bebcd22fa7a6e2bbe191c48",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-00259",
                    "CorpusId": 236772801
                },
                "corpusId": 236772801,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/45b31d499ed297927bebcd22fa7a6e2bbe191c48",
                "title": "Provably Efficient Lottery Ticket Discovery",
                "abstract": "The lottery ticket hypothesis (LTH) [19] claims that randomly-initialized, dense neural networks contain (sparse) subnetworks that, when trained an equal amount in isolation, can match the dense network\u2019s performance. Although LTH is useful for discovering ef\ufb01cient network architectures, its three-step process\u2014pre-training, pruning, and re-training\u2014is computationally expensive, as the dense model must be fully pre-trained. Luckily, \u201cearly-bird\u201d tickets can be discovered within neural networks that are minimally pre-trained [67], allowing for the creation of ef\ufb01cient, LTH-inspired training procedures. Yet, no theoretical foundation of this phenomenon exists. We derive an analytical bound for the number of pre-training iterations that must be performed for a winning ticket to be discovered, thus providing a theoretical understanding of when and why such early-bird tickets exist. By adopting a greedy forward selection pruning strategy [65], we directly connect the pruned network\u2019s performance to the loss of the dense network from which it was derived, revealing a threshold in the number of pre-training iterations beyond which high-performing subnetworks are guaranteed to exist. We demonstrate the validity of our theoretical results across a variety of architectures and datasets, including multi-layer perceptrons (MLPs) trained on MNIST and several deep convolutional neural network (CNN) architectures trained on CIFAR10 and ImageNet.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34210029",
                        "name": "Cameron R. Wolfe"
                    },
                    {
                        "authorId": "2154497049",
                        "name": "Qihan Wang"
                    },
                    {
                        "authorId": "2120215686",
                        "name": "J. Kim"
                    },
                    {
                        "authorId": "3393746",
                        "name": "Anastasios Kyrillidis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In recent years, researchers have found an intriguing corollary: winning tickets found in the context of one task can be transferred to related tasks [3, 4, 8, 18, 28, 29, 37, 39], possibly even across different architectures [7].",
                "understanding of the universality in behavior near phase transitions, as well as a way in which to characterize materials by such behavior, we reasoned that viewing the IMP from an RG perspective may lead to new insight on the universality of winning tickets [3, 4, 8, 18, 28, 29, 37, 39] and the general success it has found in the study of DNNs [16]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6ee98d9b218fcace923fe5fef742cee54ebd32f3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-03210",
                    "CorpusId": 238419190
                },
                "corpusId": 238419190,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6ee98d9b218fcace923fe5fef742cee54ebd32f3",
                "title": "Universality of Deep Neural Network Lottery Tickets: A Renormalization Group Perspective",
                "abstract": "Foundational work on the Lottery Ticket Hypothesis has suggested an exciting corollary: winning tickets found in the context of one task can be transferred to similar tasks, possibly even across different architectures. While this has become of broad practical and theoretical interest, to date, there exists no detailed understanding of why winning ticket universality exists, or any way of knowing a priori whether a given ticket can be transferred to a given task. To address these outstanding open questions, we make use of renormalization group theory, one of the most successful tools in theoretical physics. We find that iterative magnitude pruning, the method used for discovering winning tickets, is a renormalization group scheme. This opens the door to a wealth of existing numerical and theoretical tools, some of which we leverage here to examine winning ticket universality in large scale lottery ticket experiments, as well as sheds new light on the success iterative magnitude pruning has found in the field of sparse machine learning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145036964",
                        "name": "William T. Redman"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": null,
                        "name": "Akshunna S. Dogra"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d5513752425a079195656580e15857d34e4d6941",
                "externalIds": {
                    "DBLP": "conf/nips/ZhangJZZZRLWJD21",
                    "CorpusId": 244958525
                },
                "corpusId": 244958525,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d5513752425a079195656580e15857d34e4d6941",
                "title": "Validating the Lottery Ticket Hypothesis with Inertial Manifold Theory",
                "abstract": "Despite achieving remarkable ef\ufb01ciency, traditional network pruning techniques often follow manually-crafted heuristics to generate pruned sparse networks. Such heuristic pruning strategies are hard to guarantee that the pruned networks achieve test accuracy comparable to the original dense ones. Recent works have empirically identi\ufb01ed and veri\ufb01ed the Lottery Ticket Hypothesis (LTH): a randomly-initialized dense neural network contains an extremely sparse subnetwork, which can be trained to achieve similar accuracy to the former. Due to the lack of theoretical evidence, they often need to run multiple rounds of expensive training and pruning over the original large networks to discover the sparse subnetworks with low accuracy loss. By leveraging dynamical systems theory and inertial manifold theory, this work theoretically veri\ufb01es the validity of the LTH. We explore the possibility of theoretically lossless pruning as well as one-time pruning, compared with existing neural network pruning and LTH techniques. We reformulate the neural network optimization problem as a gradient dynamical system and reduce this high-dimensional system onto inertial manifolds to obtain a low-dimensional system regarding pruned subnetworks. We demonstrate the precondition and existence of pruned subnetworks and prune the original networks in",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118690556",
                        "name": "Zeru Zhang"
                    },
                    {
                        "authorId": "103340106",
                        "name": "Jiayin Jin"
                    },
                    {
                        "authorId": "48806049",
                        "name": "Zijie Zhang"
                    },
                    {
                        "authorId": "2145499198",
                        "name": "Yang Zhou"
                    },
                    {
                        "authorId": "2145735267",
                        "name": "Xin Zhao"
                    },
                    {
                        "authorId": "48115953",
                        "name": "Jiaxiang Ren"
                    },
                    {
                        "authorId": "2118971193",
                        "name": "Ji Liu"
                    },
                    {
                        "authorId": "3008832",
                        "name": "Lingfei Wu"
                    },
                    {
                        "authorId": "1740308",
                        "name": "R. Jin"
                    },
                    {
                        "authorId": "1721158",
                        "name": "D. Dou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It requires several tricks to find lottery tickets for complicated architectures (Morcos et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "900be4c03e6103b9f94e8dfb2f5b3cf5ad830b6a",
                "externalIds": {
                    "DBLP": "conf/emnlp/ZhuWZLZ021",
                    "DOI": "10.18653/v1/2021.findings-emnlp.95",
                    "CorpusId": 244119766
                },
                "corpusId": 244119766,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/900be4c03e6103b9f94e8dfb2f5b3cf5ad830b6a",
                "title": "Less Is More: Domain Adaptation with Lottery Ticket for Reading Comprehension",
                "abstract": "In this paper, we propose a simple few-shot domain adaptation paradigm for reading comprehension. We \ufb01rst identify the lottery subnet-work structure within the Transformer-based source domain model via gradual magnitude pruning. Then, we only \ufb01ne-tune the lottery subnetwork, a small fraction of the whole parameters, on the annotated target domain data for adaptation. To obtain more adaptable sub-networks, we introduce self-attention attribution to weigh parameters, beyond simply pruning the smallest magnitude parameters, which can be seen as combining structured pruning and unstructured magnitude pruning softly. Experimental results show that our method outperforms the full model \ufb01ne-tuning adaptation on four out of \ufb01ve domains when only a small amount of annotated data available for adaptation. Moreover, introducing self-attention attribution reserves more parameters for important attention heads in the lottery subnetwork and improves the target domain model performance. Our further analyses reveal that, besides exploiting fewer parameters, the choice of subnetworks is critical to the effectiveness. 1",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2387872",
                        "name": "Haichao Zhu"
                    },
                    {
                        "authorId": "2108727290",
                        "name": "Zekun Wang"
                    },
                    {
                        "authorId": "2153525636",
                        "name": "Heng Zhang"
                    },
                    {
                        "authorId": "2112748107",
                        "name": "Ming Liu"
                    },
                    {
                        "authorId": "1571183602",
                        "name": "Sendong Zhao"
                    },
                    {
                        "authorId": "152277111",
                        "name": "Bing Qin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6bdc43be03a6a36e0e314cd84acc86856d9e7b8d",
                "externalIds": {
                    "CorpusId": 250305581
                },
                "corpusId": 250305581,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6bdc43be03a6a36e0e314cd84acc86856d9e7b8d",
                "title": "T ABULAR D ATA M ODELING VIA C ONTEXTUAL E M BEDDINGS",
                "abstract": "We introduce TabTransformer, a new tabular data modeling architecture based on deep self-attention Transformers. Our model works by embedding categorical features in a robust and contextual manual, resulting in better prediction performance. We evaluate TabTransformer for supervised setting through extensive experiments on \ufb01fteen publicly available datasets, and conclude that it outperforms the state-of-the-art deep learning methods for tabular data by at least 1 . 0% on mean AUC. Fur-thermore, for the semi-supervised setting we develop an unsupervised pre-training and \ufb01ne-tuning paradigm to learn data-driven contextual embeddings, resulting in an average 2 . 1% AUC lift over the state-of-the-art methods. Lastly, we demonstrate that the contextual embeddings learned from TabTransformer provide better interpretability, and are highly robust against both missing and noisy data features.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2152664315",
                        "name": "Xin Huang"
                    },
                    {
                        "authorId": "3083159",
                        "name": "A. Khetan"
                    },
                    {
                        "authorId": "39269062",
                        "name": "Milan Cvitkovic"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following that, a few works (Morcos et al., 2019; Mehta, 2019) have explored LTH in transfer learning.",
                "Inspired by the recent observation that training with more classes helps consolidate a more robust sparse model (Morcos et al., 2019), we propose a curriculum pruning schedule, in which IMP is conducted more aggressively for new tasks arriving later, with n(i) \u2265 n(i\u22121), until reaching the desired\u2026",
                "(Mehta, 2019; Morcos et al., 2019; Desai et al., 2019) pioneer to study the transferability of the ticket identified on one source task to another target task, which delivers insights on one-shot transferability of LTH.",
                "Inspired by the recent observation that training with more classes helps consolidate a more robust sparse model (Morcos et al., 2019), we propose a curriculum pruning schedule, in which IMP is conducted more aggressively for new tasks arriving later, with n \u2265 n(i\u22121), until reaching the desired sparsity."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e7a09a66f74a5eb0125ca96cc49623786156c99b",
                "externalIds": {
                    "CorpusId": 250075828
                },
                "corpusId": 250075828,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e7a09a66f74a5eb0125ca96cc49623786156c99b",
                "title": "L ONG L IVE THE L OTTERY : T HE E XISTENCE OF W IN NING T ICKETS IN L IFELONG L EARNING",
                "abstract": "The lottery ticket hypothesis states that a highly sparsified sub-network can be trained in isolation, given the appropriate weight initialization. This paper extends that hypothesis from one-shot task leaning, and demonstrates for the first time that such extremely compact and independently trainable sub-networks can be also identified in the lifelong learning scenario, which we call lifelong tickets. We show that the resulting lifelong ticket can further be leveraged to improve the performance of learning over continual tasks. However, it is highly non-trivial to conduct network pruning in the lifelong setting. Two critical roadblocks arise: i) As many tasks now arrive sequentially, finding tickets in a greedy weight pruning fashion will inevitably suffer from the intrinsic bias, that the earlier emerging tasks impact more; ii) As lifelong learning is consistently challenged by catastrophic forgetting, the compact network capacity of tickets might amplify the risk of forgetting. In view of those, we introduce two pruning options, e.g., top-down and bottom-up, for finding lifelong tickets. Compared to the top-down pruning that extends vanilla (iterative) pruning over sequential tasks, we show that the bottomup one, which can dynamically shrink and (re-)expand model capacity, effectively avoids the undesirable excessive pruning in the early stage. We additionally introduce lottery teaching that further overcomes forgetting via knowledge distillation aided by external unlabeled data. Unifying those ingredients, we demonstrate the existence of very competitive lifelong tickets, e.g., achieving 3\u2212 8% of the dense model size with even higher accuracy, compared to strong class-incremental learning baselines on CIFAR-10/CIFAR-100/Tiny-ImageNet datasets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2094291466",
                        "name": "Long Live"
                    },
                    {
                        "authorId": "1403334137",
                        "name": "lifeloNg leaRNiNg"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Since their introduction, a number of works have attempted to understand various aspects of this phenomenon, including their transferability (Morcos et al., 2019), the factors which make particular winning tickets good (Zhou et al.",
                "Should all layers be pruned equally? Or rather, should some layers be pruned more than others? Previous studies have shown that global pruning results in better compression and performance than layerwise (or uniform) pruning (Frankle & Carbin, 2018; Morcos et al., 2019).",
                "We used global iterative magnitude pruning (IMP) for LTs because it has been shown to perform better than one shot pruning, where the pruning procedure is only done once rather than iteratively, and local (or uniform layerwise) pruning, where each layer has the same pruning ratio (Morcos et al., 2019; Frankle & Carbin, 2018).",
                "Since their introduction, a number of works have attempted to understand various aspects of this phenomenon, including their transferability (Morcos et al., 2019), the factors which make particular winning tickets good (Zhou et al., 2019), and linear mode connectivity (Frankle et al., 2019b).",
                "\u2026pruning (IMP) for LTs because it has been shown to perform better than one shot pruning, where the pruning procedure is only done once rather than iteratively, and local (or uniform layerwise) pruning, where each layer has the same pruning ratio (Morcos et al., 2019; Frankle & Carbin, 2018).",
                "Epoch 2 was chosen to be roughly similar to experiments in Morcos et al. (2019), which showed strong winning ticket performance across a number of datasets.",
                "Previous studies have shown that global pruning results in better compression and performance than layerwise (or uniform) pruning (Frankle & Carbin, 2018; Morcos et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "af49b9fd741c16964193e7be780ca34b4e2a637d",
                "externalIds": {
                    "CorpusId": 251795137
                },
                "corpusId": 251795137,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/af49b9fd741c16964193e7be780ca34b4e2a637d",
                "title": "U NCOVERING THE IMPACT OF HYPERPARAMETERS FOR GLOBAL MAGNITUDE PRUNING",
                "abstract": "A common paradigm in model pruning is to train a model, prune, and then either fine-tune or, in the lottery ticket framework, reinitialize and retrain. Prior work has implicitly assumed that the best training configuration for model evaluation is also the best configuration for mask discovery. However, what if a training configuration which yields worse performance actually yields a mask which trains to higher performance? To test this, we decoupled the hyperparameters for mask discovery (Hfind) and mask evaluation (Heval). Using unstructured magnitude pruning on vision classification tasks, we discovered the \u201cdecoupled find-eval phenomenon,\u201d in which certain Hfind values lead to models which have lower performance, but generate masks with substantially higher eventual performance compared to using the same hyperparameters for both stages. We show that this phenomenon holds across a number of models, datasets, configurations, and also for one-shot structured pruning. Finally, we demonstrate that different Hfind values yield masks with materially different layerwise pruning ratios and that the decoupled find-eval phenomenon is causally mediated by these ratios. Our results demonstrate the practical utility of decoupling hyperparameters and provide clear insights into the mechanisms underlying this counterintuitive effect.",
                "year": 2020,
                "authors": []
            }
        },
        {
            "contexts": [
                "\uc774 \ub7f0 \ubb38\uc81c\ub97c \uadf9\ubcf5\ud558\uace0\uc790 \ud504\ub8e8\ub2dd\uc744 \ud559\uc2b5\uc774 \ub2e4 \ub05d\ub098\uc9c0 \uc54a\uc740 \uc0c1\ud0dc\uc5d0\uc11c \uc57d\uac04\uc758 \uc5d0\ud3ed(Epoch) \uc774\ud6c4 \ud504\ub8e8\ub2dd\uc744 \uc77c\uc815\ubd80\ubd84 \ud574\uc8fc\uace0, \ub2e4\uc2dc \uba87 \ubc88\uc758 \uc5d0\ud3ed\uc744 \ubc18\ubcf5\ud558\ub294 \ubc29\ubc95[23]\uacfc \uc218\uc2dd \uc744 \ud1b5\ud55c \uc911\uc694 \ub274\ub7f0\uc744 \uadfc\uc0ac \uacc4\uc0b0\ud558\uc5ec \ud574\ub2f9\uc911\uc694 \ub274\ub7f0\ub9cc \ub0a8\uae30\uace0 \ud504\ub8e8\ub2dd \ud558\ub294 \ubc29\ubc95[16]\uc774 \uc788\ub2e4."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3d0605c014d7549283ae67d4db95e1c04dee8c95",
                "externalIds": {
                    "MAG": "3084307375",
                    "DOI": "10.22156/CS4SMB.2020.10.08.023",
                    "CorpusId": 221710339
                },
                "corpusId": 221710339,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3d0605c014d7549283ae67d4db95e1c04dee8c95",
                "title": "Application and Performance Analysis of Double Pruning Method for Deep Neural Networks",
                "abstract": "Recently, the artificial intelligence deep learning field has been hard to commercialize due to the high computing power and the price problem of computing resources. In this paper, we apply a double pruning techniques to evaluate the performance of the in-depth neural network and various datasets. Double pruning combines basic Network-slimming and Parameter-prunning. Our proposed technique has the advantage of reducing the parameters that are not important to the existing learning and improving the speed without compromising the learning accuracy. After training various datasets, the pruning ratio was increased to reduce the size of the model.We confirmed that MobileNet-V3 showed the highest performance as a result of NetScore performance analysis. We confirmed that the performance after pruning was the highest in MobileNet-V3 consisting of depthwise seperable convolution neural networks in the Cifar 10 dataset, and VGGNet and ResNet in traditional convolutional neural networks also increased significantly.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2095663332",
                        "name": "Lee-Sunwoo"
                    },
                    {
                        "authorId": "150129160",
                        "name": "HoJun Yang"
                    },
                    {
                        "authorId": "2121943624",
                        "name": "Seunghwan Oh"
                    },
                    {
                        "authorId": "2012780108",
                        "name": "Mun-Hyung Lee"
                    },
                    {
                        "authorId": "1664227791",
                        "name": "Kwon Jangwoo"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6adb6c9115433ee96f57a3a4a5a325a985887acd",
                "externalIds": {
                    "MAG": "3083458047",
                    "CorpusId": 226517499
                },
                "corpusId": 226517499,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6adb6c9115433ee96f57a3a4a5a325a985887acd",
                "title": "Interpretable Recurrent Neural Networks in Continuous-time Control Environments",
                "abstract": "vii Relationship to published Work xv",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "8252176",
                        "name": "Ramin M. Hasani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A great body of work [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16] has been dedicated to this task."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0c7b76ef2e7353d348aa8e2c784bd3dd24a66d1b",
                "externalIds": {
                    "CorpusId": 227125605
                },
                "corpusId": 227125605,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0c7b76ef2e7353d348aa8e2c784bd3dd24a66d1b",
                "title": "A Principled Investigation of the Lottery Ticket Hypothesis for Deep Neural Networks",
                "abstract": "Sparsifying deep neural networks is a fundamental challenge in deep learning. Deploying massive trained models into the wild faces significant obstacles, since even for basic tasks like image classification, the size of the model can easily become prohibitively large. As the model size directly affects the time required to perform inference, methods for reducing network size by removing connections and neurons have become highly important. The Lottery Ticket Hypothesis (LTH) defined a suprising phenomenon that was noticed when attempting to obtain such sparse models. In a recent paper, Frankle and Carbin [3] advanced the hypothesis that dense, randomly initialized neural networks contain small subnetworks which, when trained in isolation, reach training accuracy comparable to the original network in the same number of passes. Hence finding such a small subnetwork (i.e. winning lottery ticket) would enable one to increase efficiency for both training and inference. While this highly intriguing result posits the existence of good sparse subnetworks, it is unclear how to find them. A great body of work [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16] has been dedicated to this task. However, current results are mostly empirical and present understanding of why is sparsification even possible is far from being complete.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2869958",
                        "name": "Adrian Vladu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5417610276ec0bff46e0ea05ef1ecf5db137dba4",
                "externalIds": {
                    "DBLP": "conf/icpram/2020s",
                    "DOI": "10.1007/978-3-030-66125-0",
                    "CorpusId": 229346229
                },
                "corpusId": 229346229,
                "publicationVenue": {
                    "id": "8ef5945c-5b25-4774-b55a-15cd5450f6e4",
                    "name": "International Conference on Pattern Recognition Applications and Methods",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Pattern Recognit Appl Method",
                        "ICPRAM"
                    ],
                    "url": "http://icpram.org/"
                },
                "url": "https://www.semanticscholar.org/paper/5417610276ec0bff46e0ea05ef1ecf5db137dba4",
                "title": "Pattern Recognition Applications and Methods: 9th International Conference, ICPRAM 2020, Valletta, Malta, February 22\u201324, 2020, Revised Selected Papers",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1486458932",
                        "name": "K. Kechagias"
                    },
                    {
                        "authorId": "2023738951",
                        "name": "Theodoros Psallidas"
                    },
                    {
                        "authorId": "1438310376",
                        "name": "Georgios Smyrnis"
                    },
                    {
                        "authorId": "144116678",
                        "name": "E. Spyrou"
                    },
                    {
                        "authorId": "2397702",
                        "name": "S. Perantonis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, other work has shown that such subnetworks generalize well across datasets and tasks (Morcos et al. (2019); Tanaka et al. (2020))."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c72097eafec0c76dee5e5f3983300312fa4cfd4a",
                "externalIds": {
                    "CorpusId": 229377474
                },
                "corpusId": 229377474,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c72097eafec0c76dee5e5f3983300312fa4cfd4a",
                "title": "\u201cWINNING TICKETS\u201d WITHOUT TRAINING DATA",
                "abstract": "Sparse neural networks have generated substantial interest recently because they can be more efficient in learning and inference, without any significant drop in performance. The \u201clottery ticket hypothesis\u201d has showed the existence of such sparse subnetworks at initialization. Given a fully-connected initialized architecture, our aim is to find such \u201cwinning ticket\u201d networks, without any training data. We first show the advantages of forming input-output paths, over pruning individual connections, to avoid bottlenecks in gradient propagation. Then, we show that Paths with Higher Edge-Weights (PHEW) at initialization have higher loss gradient magnitude, resulting in more efficient training. Selecting such paths can be performed without any data. We empirically validate the effectiveness of the proposed approach against pruning-before-training methods on CIFAR10, CIFAR100 and Tiny-ImageNet for VGG-Net and ResNet. PHEW achieves significant improvements on the current state-of-the-art methods at 10%, 5% and 2% network density. We also evaluate the structural similarity relationship between PHEW networks and pruned networks constructed through Iterated Magnitude Pruning (IMP), concluding that the former belong in the family of winning tickets networks.",
                "year": 2020,
                "authors": []
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "480760fd33f45222163022da3fa952cd9695a960",
                "externalIds": {
                    "CorpusId": 234100714
                },
                "corpusId": 234100714,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/480760fd33f45222163022da3fa952cd9695a960",
                "title": "Optimizing ANN Architecture using Mixed-Integer Programming",
                "abstract": "Over-parameterized networks, where the number of parameters surpass the number of training samples, generalize well on various tasks. However, large networks are computationally expensive in terms of the training and inference time. Furthermore, the lottery ticket hypothesis states that a subnetwork of a randomly initialized network can achieve marginal loss after training on a specific task compared to the original network. Therefore, there is a need to optimize the inference and training time, and a potential for more compact neural architectures. We introduce a novel approach \u201cOptimizing ANN Architectures using Mixed-Integer Programming\u201d (OAMIP) to find these subnetworks by identifying critical neurons and removing non-critical ones, resulting in a faster inference time. The proposed OAMIP utilizes a Mixed-Integer Program (MIP) for assigning importance scores to each neuron in deep neural network architectures. Our MIP is guided by the impact on the main learning task of the network when simultaneously pruning subsets of neurons. In concrete, the optimization of the objective function drives the solver to minimize the number of neurons, to limit the network to critical neurons, i.e., with high importance score, that need to be kept for maintaining the overall accuracy of the trained neural network. Further, the proposed formulation generalizes the recently considered lottery ticket hypothesis by identifying multiple \u201clucky\u201d subnetworks, resulting in optimized architectures, that not only perform well on a single dataset, but also generalize across multiple ones upon retraining of network weights. Finally, we present a scalable implementation of our method by decoupling the importance scores across layers using auxiliary networks and across different classes. We demonstrate the ability of OAMIP to prune neural networks with marginal loss in accuracy and generalizability on popular datasets and architectures.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3444427",
                        "name": "M. Elaraby"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In a similar setting, as demonstrated in the model pruning literature (Gordon et al., 2018; Liu et al., 2019b; Morcos et al., 2019; Renda et al., 2020), having different pruning ratios for different layers of",
                "In a similar setting, as demonstrated in the model pruning literature (Gordon et al., 2018; Liu et al., 2019b; Morcos et al., 2019; Renda et al., 2020), having different pruning ratios for different layers of\nthe network can further improve results over a single ratio across layers."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "2b39434f84fed4d7187d8edb92d37c6538c5ed9f",
                "externalIds": {
                    "CorpusId": 236783748
                },
                "corpusId": 236783748,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2b39434f84fed4d7187d8edb92d37c6538c5ed9f",
                "title": "PARECO: PARETO-AWARE CHANNEL OPTIMIZATION",
                "abstract": "Slimmable neural networks provide a flexible trade-off front between prediction error and computational cost (such as the number of floating-point operations or FLOPs) with the same storage cost as a single model. They have been proposed recently for resource-constrained settings such as mobile devices. However, current slimmable neural networks use a single width-multiplier for all the layers to arrive at sub-networks with different performance profiles, which neglects that different layers affect the network\u2019s prediction accuracy differently and have different FLOP requirements. Hence, developing a principled approach for deciding widthmultipliers across different layers could potentially improve the performance of slimmable networks. To allow for heterogeneous width-multipliers across different layers, we formulate the problem of optimizing slimmable networks from a multiobjective optimization lens, which leads to a novel algorithm for optimizing both the shared weights and the width-multipliers for the sub-networks. We perform extensive empirical analysis with 15 network and dataset combinations and two types of cost objectives, i.e., FLOPs and memory footprint, to demonstrate the effectiveness of the proposed method compared to existing alternatives. Quantitatively, improvements up to 1.7% and 8% in top-1 accuracy on the ImageNet dataset can be attained for MobileNetV2 considering FLOPs and memory footprint, respectively. Our results highlight the potential of optimizing the channel counts for different layers jointly with the weights for slimmable networks.",
                "year": 2020,
                "authors": []
            }
        },
        {
            "contexts": [
                "The success of the LT algorithm also spawned research in the area of better initialization methods based on determining winning tickets across di erent data sets [36]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "81fba049b5ca469886a6051b2105a29733fc5823",
                "externalIds": {
                    "CorpusId": 238995750
                },
                "corpusId": 238995750,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/81fba049b5ca469886a6051b2105a29733fc5823",
                "title": "Comparison and Extension of CNNs for Analysis of Salt Marsh Images",
                "abstract": "Recent advances in computer vision, most notably deep convolutional neural networks (CNNs), are exploited to identify and localize various plant species in salt marsh images. Three distinct approaches are explored that provide estimations of abundance and spatial distribution at varying levels of granularity in terms of spatial resolution. Overall, a clear trade-o is observed between the CNN estimation quality and the spatial resolution of the underlying estimation thereby o ering guidance for ecological applications of CNN-based approaches to automated plant identi cation and localization in salt marsh images. A novel way to train neural networks for semantic image segmentation, termed as Compositional Sparse Network (CSN), is also conceptualized and tested. By leveraging the properties of dynamic expansion, interconnection richness, and sparsity, a CSN is used as the backbone for the DeepLab-V3 architecture. Since CSN is analogous to Neural Architecture Search (NAS), it is also compared to a NAS-based semantic image segmentation approach. Index words: Deep Learning, Semantic Segmentation, Image classi cation, Pruning, Dynamic Expansion, Ecological monitoring, Salt marsh monitoring. Comparison and Extension of CNNs for Analysis of Salt Marsh Images",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "73675649",
                        "name": "J. Parashar"
                    },
                    {
                        "authorId": "2132928080",
                        "name": "Suchi M. Bhandarkar"
                    },
                    {
                        "authorId": "143834460",
                        "name": "B. Hopkinson"
                    },
                    {
                        "authorId": "3422895",
                        "name": "S. Bhandarkar"
                    },
                    {
                        "authorId": "2153701736",
                        "name": "Sheng Li"
                    },
                    {
                        "authorId": "7151388",
                        "name": "Frederick W. Maier"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026lottery ticket hypothesis (LTH) (Frankle & Carbin, 2019) advocates that dense models contain highly sparse subnetworks, i.e., winning tickets, with the same good trainability, expressiveness, and transferability (Morcos et al., 2019a; Chen et al., 2020b;a) compared to their dense counterpart.",
                "Similar observations also presented in Morcos et al. (2019a).",
                "\u2026advocated the existence of sparse subnetworks (winning tickets) with comparable transferability to the full dense models (Chen et al., 2020b;a; Koohpayegani et al., 2020; Morcos et al., 2019b; Chen et al., 2022a) and adversarial robustness (Chen et al., 2022b; Gui et al., 2019; Ye et al., 2019).",
                "(4) Compression w.r.t. transferability: extensive investigations (Chen et al., 2020b;a; Koohpayegani et al., 2020; Morcos et al., 2019b; Iofinova et al., 2022) indicate that there exist high quality subnetworks with competitive or even enhanced transferability across diverse datasets.",
                "\u2026et al., 2019; 2020b; Gui et al., 2019; Ye et al., 2019; Wang et al., 2018; Zhou et al., 2009; Venkatesh et al., 2020; Chen et al., 2020b;a; Koohpayegani et al., 2020; Morcos et al., 2019b; Zhang et al., 2021a; Sakamoto & Sato, 2022; Chen et al., 2022c) trying to address some part of this question."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7c0b26c232b47b8567be1cdf675154d271209cae",
                "externalIds": {
                    "CorpusId": 250580346
                },
                "corpusId": 250580346,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7c0b26c232b47b8567be1cdf675154d271209cae",
                "title": "Can You Win Everything with A Lottery Ticket?",
                "abstract": "Lottery ticket hypothesis (LTH) has demonstrated to yield independently trainable and highly sparse neural networks (a.k.a. winning tickets ), whose test set accuracies can be surprisingly on par or even better than dense models. However, accuracy is far from the only evaluation metric, and perhaps not always the most important one. Hence it might be myopic to conclude that a sparse subnetwork can replace its dense counterpart, even if the accuracy is preserved. Spurred by that, we perform the first comprehensive assessment of lottery tickets from diverse aspects beyond test accuracy, including (i) generalization to distribution shifts, (ii) prediction uncertainty, (iii) interpretability, and (iv) geometry of loss landscapes. With extensive experiments across datasets {CIFAR-10, CIFAR-100, and ImageNet}, model architectures, as well as seven sparsification methods, we thoroughly characterize the trade-off between model sparsity and the all-dimension model capabilities. We find that an appropriate sparsity (e.g., 20% \u223c 99 . 53%) can yield the winning ticket to perform comparably or even better in all above four aspects , although some aspects (generalization to certain distribution shifts, and uncertainty) appear more sensitive to the sparsification than others. We term it as a LTH-PASS . Overall, our results endorse choosing a good sparse subnetwork of a larger dense model, over directly training a small dense model of similar parameter counts. We hope that our study can offer more in-depth insights on pruning, for researchers and engineers who seek to incorporate sparse neural networks for user-facing deployments. Codes are available in https://github.com/VITA-Group/LTH-Pass .",
                "year": null,
                "authors": [
                    {
                        "authorId": "2109338656",
                        "name": "Zhenyu (Allen) Zhang"
                    },
                    {
                        "authorId": "2147189509",
                        "name": "Jerome Friedman"
                    }
                ]
            }
        }
    ]
}