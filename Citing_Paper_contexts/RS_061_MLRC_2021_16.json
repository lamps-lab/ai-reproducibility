{
    "offset": 0,
    "data": [
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e531451e6e2cd5d776cc0dd75a5a1068066ed2f7",
                "externalIds": {
                    "DBLP": "conf/iclr/AdebayoHYC23",
                    "ArXiv": "2310.02533",
                    "CorpusId": 259298198
                },
                "corpusId": 259298198,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e531451e6e2cd5d776cc0dd75a5a1068066ed2f7",
                "title": "Quantifying and Mitigating the Impact of Label Errors on Model Disparity Metrics",
                "abstract": "Errors in labels obtained via human annotation adversely affect a model's performance. Existing approaches propose ways to mitigate the effect of label error on a model's downstream accuracy, yet little is known about its impact on a model's disparity metrics. Here we study the effect of label error on a model's disparity metrics. We empirically characterize how varying levels of label error, in both training and test data, affect these disparity metrics. We find that group calibration and other metrics are sensitive to train-time and test-time label error -- particularly for minority groups. This disparate effect persists even for models trained with noise-aware algorithms. To mitigate the impact of training-time label error, we present an approach to estimate the influence of a training input's label on a model's group disparity metric. We empirically assess the proposed approach on a variety of datasets and find significant improvement, compared to alternative approaches, in identifying training inputs that improve a model's disparity metric. We complement the approach with an automatic relabel-and-finetune scheme that produces updated models with, provably, improved group calibration error.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "153165458",
                        "name": "J. Adebayo"
                    },
                    {
                        "authorId": "120861776",
                        "name": "Melissa Hall"
                    },
                    {
                        "authorId": "38849012",
                        "name": "Bowen Yu"
                    },
                    {
                        "authorId": "2079950350",
                        "name": "Bobbie Chern"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A growing number of works have pointed to degradations of fairness behavior in robust models [191, 273, 308] and differentially private models [13, 280]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5531dd953832d86253d08b739eaa9312c7c6f610",
                "externalIds": {
                    "ArXiv": "2309.17337",
                    "DOI": "10.1145/3617694.3623259",
                    "CorpusId": 263311003
                },
                "corpusId": 263311003,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5531dd953832d86253d08b739eaa9312c7c6f610",
                "title": "Toward Operationalizing Pipeline-aware ML Fairness: A Research Agenda for Developing Practical Guidelines and Tools",
                "abstract": "While algorithmic fairness is a thriving area of research, in practice, mitigating issues of bias often gets reduced to enforcing an arbitrarily chosen fairness metric, either by enforcing fairness constraints during the optimization step, post-processing model outputs, or by manipulating the training data. Recent work has called on the ML community to take a more holistic approach to tackle fairness issues by systematically investigating the many design choices made through the ML pipeline, and identifying interventions that target the issue's root cause, as opposed to its symptoms. While we share the conviction that this pipeline-based approach is the most appropriate for combating algorithmic unfairness on the ground, we believe there are currently very few methods of \\emph{operationalizing} this approach in practice. Drawing on our experience as educators and practitioners, we first demonstrate that without clear guidelines and toolkits, even individuals with specialized ML knowledge find it challenging to hypothesize how various design choices influence model behavior. We then consult the fair-ML literature to understand the progress to date toward operationalizing the pipeline-aware approach: we systematically collect and organize the prior work that attempts to detect, measure, and mitigate various sources of unfairness through the ML pipeline. We utilize this extensive categorization of previous contributions to sketch a research agenda for the community. We hope this work serves as the stepping stone toward a more comprehensive set of resources for ML researchers, practitioners, and students interested in exploring, designing, and testing pipeline-oriented approaches to algorithmic fairness.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2249533192",
                        "name": "Emily Black"
                    },
                    {
                        "authorId": "2249533181",
                        "name": "Rakshit Naidu"
                    },
                    {
                        "authorId": "1791498",
                        "name": "R. Ghani"
                    },
                    {
                        "authorId": "6783324",
                        "name": "Kit T. Rodolfa"
                    },
                    {
                        "authorId": "2249533466",
                        "name": "Daniel E. Ho"
                    },
                    {
                        "authorId": "2249539262",
                        "name": "Hoda Heidari"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4258ce8297c5a8cd0a6c1a3997c678fb0ac06b0e",
                "externalIds": {
                    "DOI": "10.1007/s11229-023-04334-9",
                    "CorpusId": 262897130
                },
                "corpusId": 262897130,
                "publicationVenue": {
                    "id": "cfb7bc3b-4dad-4d1f-aea6-f5d1f2499ce8",
                    "name": "Synthese",
                    "type": "journal",
                    "issn": "0039-7857",
                    "url": "http://www.springer.com/11229",
                    "alternate_urls": [
                        "http://www.jstor.org/journals/00397857.html",
                        "https://www.jstor.org/journal/synthese"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4258ce8297c5a8cd0a6c1a3997c678fb0ac06b0e",
                "title": "Beyond generalization: a theory of robustness in machine learning",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2247843982",
                        "name": "Timo Freiesleben"
                    },
                    {
                        "authorId": "1423968387",
                        "name": "Thomas Grote"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "05b680497fe06c4be25ce862ad8c6a8b943e6648",
                "externalIds": {
                    "ArXiv": "2309.15418",
                    "DOI": "10.1145/3624918.3625318",
                    "CorpusId": 263151993
                },
                "corpusId": 263151993,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/05b680497fe06c4be25ce862ad8c6a8b943e6648",
                "title": "Automatic Feature Fairness in Recommendation via Adversaries",
                "abstract": "Fairness is a widely discussed topic in recommender systems, but its practical implementation faces challenges in defining sensitive features while maintaining recommendation accuracy. We propose feature fairness as the foundation to achieve equitable treatment across diverse groups defined by various feature combinations. This improves overall accuracy through balanced feature generalizability. We introduce unbiased feature learning through adversarial training, using adversarial perturbation to enhance feature representation. The adversaries improve model generalization for under-represented features. We adapt adversaries automatically based on two forms of feature biases: frequency and combination variety of feature values. This allows us to dynamically adjust perturbation strengths and adversarial training weights. Stronger perturbations are applied to feature values with fewer combination varieties to improve generalization, while higher weights for low-frequency features address training imbalances. We leverage the Adaptive Adversarial perturbation based on the widely-applied Factorization Machine (AAFM) as our backbone model. In experiments, AAFM surpasses strong baselines in both fairness and accuracy measures. AAFM excels in providing item- and user-fairness for single- and multi-feature tasks, showcasing their versatility and scalability. To maintain good accuracy, we find that adversarial perturbation must be well-managed: during training, perturbations should not overly persist and their strengths should decay.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2142605154",
                        "name": "Hengchang Hu"
                    },
                    {
                        "authorId": "2248040742",
                        "name": "Yiming Cao"
                    },
                    {
                        "authorId": "2248051280",
                        "name": "Zhankui He"
                    },
                    {
                        "authorId": "145814654",
                        "name": "Samson Tan"
                    },
                    {
                        "authorId": "37596605",
                        "name": "Min-Yen Kan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "320df3537de1f1804a7b010c4e21a44c6e7c8061",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-09450",
                    "ArXiv": "2309.09450",
                    "DOI": "10.48550/arXiv.2309.09450",
                    "CorpusId": 261884475
                },
                "corpusId": 261884475,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/320df3537de1f1804a7b010c4e21a44c6e7c8061",
                "title": "Are You Worthy of My Trust?: A Socioethical Perspective on the Impacts of Trustworthy AI Systems on the Environment and Human Society",
                "abstract": "With ubiquitous exposure of AI systems today, we believe AI development requires crucial considerations to be deemed trustworthy. While the potential of AI systems is bountiful, though, is still unknown-as are their risks. In this work, we offer a brief, high-level overview of societal impacts of AI systems. To do so, we highlight the requirement of multi-disciplinary governance and convergence throughout its lifecycle via critical systemic examinations (e.g., energy consumption), and later discuss induced effects on the environment (i.e., carbon footprint) and its users (i.e., social development). In particular, we consider these impacts from a multi-disciplinary perspective: computer science, sociology, environmental science, and so on to discuss its inter-connected societal risks and inability to simultaneously satisfy aspects of well-being. Therefore, we accentuate the necessity of holistically addressing pressing concerns of AI systems from a socioethical impact assessment perspective to explicate its harmful societal effects to truly enable humanity-centered Trustworthy AI.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2241350304",
                        "name": "Jamell Dacon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These efforts can be divided into two main categories: Adversarial training [19, 20] and Non-adversarial training [21, 22] approach."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "db4e40fd1d2caab22d1ad94ab0496a1ab155b1e2",
                "externalIds": {
                    "ArXiv": "2309.05132",
                    "DBLP": "journals/corr/abs-2309-05132",
                    "DOI": "10.48550/arXiv.2309.05132",
                    "CorpusId": 261682350
                },
                "corpusId": 261682350,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/db4e40fd1d2caab22d1ad94ab0496a1ab155b1e2",
                "title": "DAD++: Improved Data-free Test Time Adversarial Defense",
                "abstract": "With the increasing deployment of deep neural networks in safety-critical applications such as self-driving cars, medical imaging, anomaly detection, etc., adversarial robustness has become a crucial concern in the reliability of these networks in real-world scenarios. A plethora of works based on adversarial training and regularization-based techniques have been proposed to make these deep networks robust against adversarial attacks. However, these methods require either retraining models or training them from scratch, making them infeasible to defend pre-trained models when access to training data is restricted. To address this problem, we propose a test time Data-free Adversarial Defense (DAD) containing detection and correction frameworks. Moreover, to further improve the efficacy of the correction framework in cases when the detector is under-confident, we propose a soft-detection scheme (dubbed as\"DAD++\"). We conduct a wide range of experiments and ablations on several datasets and network architectures to show the efficacy of our proposed approach. Furthermore, we demonstrate the applicability of our approach in imparting adversarial defense at test time under data-free (or data-efficient) applications/setups, such as Data-free Knowledge Distillation and Source-free Unsupervised Domain Adaptation, as well as Semi-supervised classification frameworks. We observe that in all the experiments and applications, our DAD++ gives an impressive performance against various adversarial attacks with a minimal drop in clean accuracy. The source code is available at: https://github.com/vcl-iisc/Improved-Data-free-Test-Time-Adversarial-Defense",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "143747407",
                        "name": "Gaurav Kumar Nayak"
                    },
                    {
                        "authorId": "2044355793",
                        "name": "Inder Khatri"
                    },
                    {
                        "authorId": "2189541527",
                        "name": "Shubham Randive"
                    },
                    {
                        "authorId": "1658305348",
                        "name": "Ruchit Rawal"
                    },
                    {
                        "authorId": "2238953541",
                        "name": "Anirban Chakraborty"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "35c08e1919a77a0ebfa3c1d26f4dc0a43d2ff794",
                "externalIds": {
                    "DOI": "10.1016/j.neunet.2023.08.063",
                    "CorpusId": 261675375,
                    "PubMed": "37729786"
                },
                "corpusId": 261675375,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/35c08e1919a77a0ebfa3c1d26f4dc0a43d2ff794",
                "title": "Boosting adversarial robustness via self-paced adversarial training.",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144741785",
                        "name": "Lirong He"
                    },
                    {
                        "authorId": "2120149872",
                        "name": "Qingzhong Ai"
                    },
                    {
                        "authorId": "2239033998",
                        "name": "Xincheng Yang"
                    },
                    {
                        "authorId": "2041980",
                        "name": "Yazhou Ren"
                    },
                    {
                        "authorId": "2238904920",
                        "name": "Qifan Wang"
                    },
                    {
                        "authorId": "2238898625",
                        "name": "Zenglin Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This notion of disparity has also been used in adjacent areas such as robustness [11, 116].",
                "[116] Han Xu, Xiaorui Liu, Yaxin Li, Anil K."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2810b402eefc20bafbe43940ca76dc75e2909299",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-10888",
                    "ArXiv": "2308.10888",
                    "DOI": "10.48550/arXiv.2308.10888",
                    "CorpusId": 261049182
                },
                "corpusId": 261049182,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2810b402eefc20bafbe43940ca76dc75e2909299",
                "title": "Unlocking Accuracy and Fairness in Differentially Private Image Classification",
                "abstract": "Privacy-preserving machine learning aims to train models on private data without leaking sensitive information. Differential privacy (DP) is considered the gold standard framework for privacy-preserving training, as it provides formal privacy guarantees. However, compared to their non-private counterparts, models trained with DP often have significantly reduced accuracy. Private classifiers are also believed to exhibit larger performance disparities across subpopulations, raising fairness concerns. The poor performance of classifiers trained with DP has prevented the widespread adoption of privacy preserving machine learning in industry. Here we show that pre-trained foundation models fine-tuned with DP can achieve similar accuracy to non-private classifiers, even in the presence of significant distribution shifts between pre-training data and downstream tasks. We achieve private accuracies within a few percent of the non-private state of the art across four datasets, including two medical imaging benchmarks. Furthermore, our private medical classifiers do not exhibit larger performance disparities across demographic groups than non-private models. This milestone to make DP training a practical and reliable technology has the potential to widely enable machine learning practitioners to train safely on sensitive datasets while protecting individuals' privacy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48092709",
                        "name": "Leonard Berrada"
                    },
                    {
                        "authorId": "3252772",
                        "name": "Soham De"
                    },
                    {
                        "authorId": "40385687",
                        "name": "J. Shen"
                    },
                    {
                        "authorId": "9200194",
                        "name": "Jamie Hayes"
                    },
                    {
                        "authorId": "49860489",
                        "name": "Robert Stanforth"
                    },
                    {
                        "authorId": "2133303881",
                        "name": "David Stutz"
                    },
                    {
                        "authorId": "143967473",
                        "name": "Pushmeet Kohli"
                    },
                    {
                        "authorId": "2157770601",
                        "name": "Samuel L. Smith"
                    },
                    {
                        "authorId": "1718064",
                        "name": "B. Balle"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3b9041318a3ab7457480cd73421473b7704981ff",
                "externalIds": {
                    "ArXiv": "2308.08938",
                    "DBLP": "journals/corr/abs-2308-08938",
                    "DOI": "10.48550/arXiv.2308.08938",
                    "CorpusId": 261030231
                },
                "corpusId": 261030231,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3b9041318a3ab7457480cd73421473b7704981ff",
                "title": "Causal Adversarial Perturbations for Individual Fairness and Robustness in Heterogeneous Data Spaces",
                "abstract": "As responsible AI gains importance in machine learning algorithms, properties such as fairness, adversarial robustness, and causality have received considerable attention in recent years. However, despite their individual significance, there remains a critical gap in simultaneously exploring and integrating these properties. In this paper, we propose a novel approach that examines the relationship between individual fairness, adversarial robustness, and structural causal models in heterogeneous data spaces, particularly when dealing with discrete sensitive attributes. We use causal structural models and sensitive attributes to create a fair metric and apply it to measure semantic similarity among individuals. By introducing a novel causal adversarial perturbation and applying adversarial training, we create a new regularizer that combines individual fairness, causality, and robustness in the classifier. Our method is evaluated on both real-world and synthetic datasets, demonstrating its effectiveness in achieving an accurate classifier that simultaneously exhibits fairness, adversarial robustness, and causal awareness.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47515476",
                        "name": "A. Ehyaei"
                    },
                    {
                        "authorId": "1643681507",
                        "name": "Kiarash Mohammadi"
                    },
                    {
                        "authorId": "145926563",
                        "name": "Amir-Hossein Karimi"
                    },
                    {
                        "authorId": "2397253",
                        "name": "S. Samadi"
                    },
                    {
                        "authorId": "2086602",
                        "name": "G. Farnadi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "380d5f59300a0a5f37ac6e9131a67c35d33d376b",
                "externalIds": {
                    "DBLP": "conf/aies/Mhasawade23",
                    "DOI": "10.1145/3600211.3604753",
                    "CorpusId": 261279492
                },
                "corpusId": 261279492,
                "publicationVenue": {
                    "id": "ace94611-0469-4818-ae70-43bdb8082d73",
                    "name": "AAAI/ACM Conference on AI, Ethics, and Society",
                    "type": "conference",
                    "alternate_names": [
                        "AAAI/ACM conference Artificial Intelligence, Ethics, and Society",
                        "AIES",
                        "AAAI/ACM Conf AI Ethics Soc",
                        "AAAI/ACM conf Artif Intell Ethics Soc",
                        "AIES "
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/380d5f59300a0a5f37ac6e9131a67c35d33d376b",
                "title": "Advancing Health Equity with Machine Learning",
                "abstract": "Social privilege in terms of power, wealth, and prestige is the driver of avoidable health inequities. But today, machine learning systems in healthcare are largely focused on data and systems within hospitals and clinics, ignoring the factors that lead to health disparities across communities. The primary goal of my research is to understand the drivers of population health inequity and design fair and equitable machine learning systems for mitigating health disparities. In order to do this, I mainly focus on causal inference and machine learning methods using data from multiple environments, such as geographical locations and hospitals, to identify and address inequities in health and healthcare.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51429443",
                        "name": "V. Mhasawade"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "787720147093bb468db87db6955dba9acf0035e0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-01823",
                    "ArXiv": "2308.01823",
                    "DOI": "10.48550/arXiv.2308.01823",
                    "CorpusId": 260438381
                },
                "corpusId": 260438381,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/787720147093bb468db87db6955dba9acf0035e0",
                "title": "Hard Adversarial Example Mining for Improving Robust Fairness",
                "abstract": "Adversarial training (AT) is widely considered the state-of-the-art technique for improving the robustness of deep neural networks (DNNs) against adversarial examples (AE). Nevertheless, recent studies have revealed that adversarially trained models are prone to unfairness problems, restricting their applicability. In this paper, we empirically observe that this limitation may be attributed to serious adversarial confidence overfitting, i.e., certain adversarial examples with overconfidence. To alleviate this problem, we propose HAM, a straightforward yet effective framework via adaptive Hard Adversarial example Mining.HAM concentrates on mining hard adversarial examples while discarding the easy ones in an adaptive fashion. Specifically, HAM identifies hard AEs in terms of their step sizes needed to cross the decision boundary when calculating loss value. Besides, an early-dropping mechanism is incorporated to discard the easy examples at the initial stages of AE generation, resulting in efficient AT. Extensive experimental results on CIFAR-10, SVHN, and Imagenette demonstrate that HAM achieves significant improvement in robust fairness while reducing computational cost compared to several state-of-the-art adversarial training methods. The code will be made publicly available.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "10442800",
                        "name": "Chenhao Lin"
                    },
                    {
                        "authorId": null,
                        "name": "Xiang Ji"
                    },
                    {
                        "authorId": "2216755422",
                        "name": "Yulong Yang"
                    },
                    {
                        "authorId": "46428422",
                        "name": "Q. Li"
                    },
                    {
                        "authorId": "104246822",
                        "name": "Chao-Min Shen"
                    },
                    {
                        "authorId": "2218475904",
                        "name": "Run Wang"
                    },
                    {
                        "authorId": "2153681408",
                        "name": "Liming Fang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "56c4094f9f2e746e38652c85a05c0db7ccb70d23",
                "externalIds": {
                    "DBLP": "journals/csr/BountakasZLX23",
                    "DOI": "10.1016/j.cosrev.2023.100573",
                    "CorpusId": 260846357
                },
                "corpusId": 260846357,
                "publicationVenue": {
                    "id": "3aa92b7f-af7a-4ebd-8925-1152710bfbc7",
                    "name": "Computer Science Review",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Sci Rev"
                    ],
                    "issn": "1574-0137",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/710138/description#description"
                },
                "url": "https://www.semanticscholar.org/paper/56c4094f9f2e746e38652c85a05c0db7ccb70d23",
                "title": "Defense strategies for Adversarial Machine Learning: A survey",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "150257765",
                        "name": "Panagiotis Bountakas"
                    },
                    {
                        "authorId": "2403031",
                        "name": "Apostolis Zarras"
                    },
                    {
                        "authorId": "2815110",
                        "name": "A. Lekidis"
                    },
                    {
                        "authorId": "1734906",
                        "name": "C. Xenakis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026on the CIFAR-10 dataset classifies \u201ccat\u201d and \u201cship\u201d at approximately 89% and 96% accuracy, respectively, while the robust accuracy of \u201ccat\u201d and \u201cship\u201d produced by an adversarially trained PreactResNet-18 on PGD-attacked CIFAR-10 dataset are approximately 17% and 59% respectively (Xu et al., 2021).",
                "We consider a binary classification task of natural examples sampled from a Gaussian mixture distribution, following the settings described in (Carmon et al., 2019; Schmidt et al., 2018; Xu et al., 2021; Ma et al., 2022).",
                "Based on the findings in (Xu et al., 2021; Ma et al., 2022), supported by the confusion matrices in Figure 1, it is clear that adversarial training hurts adversarial examples of certain classes more than others, especially adversarial examples crafted from natural examples which are harder to\u2026",
                "Theorem 1 ((Xu et al., 2021)) Given a Gaussian distribution D\u2217, a naturally trained classifier fnat which minimizes the expected natural risk: fnat(x) = arg minf E(x,y)\u223cD\u2217(1(f(x) 6= y).",
                "For example, a naturally trained PreactResNet-18 on the CIFAR-10 dataset classifies \u201ccat\u201d and \u201cship\u201d at approximately 89% and 96% accuracy, respectively, while the robust accuracy of \u201ccat\u201d and \u201cship\u201d produced by an adversarially trained PreactResNet-18 on PGD-attacked CIFAR-10 dataset are approximately 17% and 59% respectively (Xu et al., 2021).",
                "07 16\n7v 1\n[ cs\nDespite the generally impressive performance of AT against adversarial attacks, Xu et al. (2021) raised concerns about fairness in AT.",
                "These natural examples are characterized by their closeness to the class decision boundaries (Xu et al.,\n2021; Zhang et al., 2020).",
                "Given that adversarial training involves training on adversarial examples, and it unfairly hurts adversarial examples crafted from vulnerable examples, it is intuitive that adversarial training sets up its decision boundary to favor adversarial examples of invulnerable classes, as observed in (Xu et al., 2021).",
                "Based on the findings in (Xu et al., 2021; Ma et al., 2022), supported by the confusion matrices in Figure 1, it is clear that adversarial training hurts adversarial examples of certain classes more than others, especially adversarial examples crafted from natural examples which are harder to classify under natural training.",
                "\u2026training involves training on adversarial examples, and it unfairly hurts adversarial examples crafted from vulnerable examples, it is intuitive that adversarial training sets up its decision boundary to favor adversarial examples of invulnerable classes, as observed in (Xu et al., 2021)."
            ],
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "544a68888a72406b86732d84447b00b769994509",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-07167",
                    "ArXiv": "2307.07167",
                    "DOI": "10.48550/arXiv.2307.07167",
                    "CorpusId": 259924835
                },
                "corpusId": 259924835,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/544a68888a72406b86732d84447b00b769994509",
                "title": "Vulnerability-Aware Instance Reweighting For Adversarial Training",
                "abstract": "Adversarial Training (AT) has been found to substantially improve the robustness of deep learning classifiers against adversarial attacks. AT involves obtaining robustness by including adversarial examples in training a classifier. Most variants of AT algorithms treat every training example equally. However, recent works have shown that better performance is achievable by treating them unequally. In addition, it has been observed that AT exerts an uneven influence on different classes in a training set and unfairly hurts examples corresponding to classes that are inherently harder to classify. Consequently, various reweighting schemes have been proposed that assign unequal weights to robust losses of individual examples in a training set. In this work, we propose a novel instance-wise reweighting scheme. It considers the vulnerability of each natural example and the resulting information loss on its adversarial counterpart occasioned by adversarial attacks. Through extensive experiments, we show that our proposed method significantly improves over existing reweighting schemes, especially against strong white and black-box attacks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2146963626",
                        "name": "Olukorede Fakorede"
                    },
                    {
                        "authorId": "2211600759",
                        "name": "Ashutosh Nirala"
                    },
                    {
                        "authorId": "2146964558",
                        "name": "Modeste Atsague"
                    },
                    {
                        "authorId": "2136088824",
                        "name": "Jin Tian"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1b1d2dff465071df461dbbc1f98b8f05190b7a3e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-13730",
                    "ArXiv": "2308.13730",
                    "DOI": "10.1109/DAC56929.2023.10247765",
                    "CorpusId": 261242625
                },
                "corpusId": 261242625,
                "publicationVenue": {
                    "id": "021b37d3-cef1-4c12-a442-257f7900c23d",
                    "name": "Design Automation Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Des Autom Conf",
                        "DAC"
                    ],
                    "url": "http://www.dac.com/"
                },
                "url": "https://www.semanticscholar.org/paper/1b1d2dff465071df461dbbc1f98b8f05190b7a3e",
                "title": "Muffin: A Framework Toward Multi-Dimension AI Fairness by Uniting Off-the-Shelf Models",
                "abstract": "Model fairness (a.k.a., bias) has become one of the most critical problems in a wide range of AI applications. An unfair model in autonomous driving may cause a traffic accident if corner cases (e.g., extreme weather) cannot be fairly regarded; or it will incur healthcare disparities if the AI model misdiagnoses a certain group of people (e.g., brown and black skin). In recent years, there are emerging research works on addressing unfairness, and they mainly focus on a single unfair attribute, like skin tone; however, real-world data commonly have multiple attributes, among which unfairness can exist in more than one attribute, called \"multi-dimensional fairness\". In this paper, we first reveal a strong correlation between the different unfair attributes, i.e., optimizing fairness on one attribute will lead to the collapse of others. Then, we propose a novel Multi-Dimension Fairness framework, namely Muffin, which includes an automatic tool to unite off-the-shelf models to improve the fairness on multiple attributes simultaneously. Case studies on dermatology datasets with two unfair attributes show that the existing approach can achieve 21.05% fairness improvement on the first attribute while it makes the second attribute unfair by 1.85%. On the other hand, the proposed Muffin can unite multiple models to achieve simultaneously 26.32% and 20.37% fairness improvement on both attributes; meanwhile, it obtains 5.58% accuracy gain.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2149694304",
                        "name": "Yi Sheng"
                    },
                    {
                        "authorId": "2144566070",
                        "name": "Junhuan Yang"
                    },
                    {
                        "authorId": "2145452232",
                        "name": "Lei Yang"
                    },
                    {
                        "authorId": "1702907",
                        "name": "Yiyu Shi"
                    },
                    {
                        "authorId": "1690946",
                        "name": "J. Hu"
                    },
                    {
                        "authorId": "1937259",
                        "name": "Weiwen Jiang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Researchers have noticed a similar problem with accuracy and untargeted robustness: the performance of models varies with different class distribution [99], [100]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2c0645d5c8010b532969558d41bed4fc88ca70cf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-16614",
                    "ArXiv": "2306.16614",
                    "DOI": "10.48550/arXiv.2306.16614",
                    "CorpusId": 259287019
                },
                "corpusId": 259287019,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2c0645d5c8010b532969558d41bed4fc88ca70cf",
                "title": "Group-based Robustness: A General Framework for Customized Robustness in the Real World",
                "abstract": "Machine-learning models are known to be vulnerable to evasion attacks that perturb model inputs to induce misclassifications. In this work, we identify real-world scenarios where the true threat cannot be assessed accurately by existing attacks. Specifically, we find that conventional metrics measuring targeted and untargeted robustness do not appropriately reflect a model's ability to withstand attacks from one set of source classes to another set of target classes. To address the shortcomings of existing methods, we formally define a new metric, termed group-based robustness, that complements existing metrics and is better-suited for evaluating model performance in certain attack scenarios. We show empirically that group-based robustness allows us to distinguish between models' vulnerability against specific threat models in situations where traditional robustness metrics do not apply. Moreover, to measure group-based robustness efficiently and accurately, we 1) propose two loss functions and 2) identify three new attack strategies. We show empirically that with comparable success rates, finding evasive samples using our new loss functions saves computation by a factor as large as the number of targeted classes, and finding evasive samples using our new attack strategies saves time by up to 99\\% compared to brute-force search methods. Finally, we propose a defense method that increases group-based robustness by up to 3.52$\\times$.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2113562457",
                        "name": "Weiran Lin"
                    },
                    {
                        "authorId": "145659229",
                        "name": "Keane Lucas"
                    },
                    {
                        "authorId": "2220820404",
                        "name": "Neo Eyal"
                    },
                    {
                        "authorId": "41224057",
                        "name": "Lujo Bauer"
                    },
                    {
                        "authorId": "1746214",
                        "name": "M. Reiter"
                    },
                    {
                        "authorId": "36301492",
                        "name": "Mahmood Sharif"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The intersection between strategic classification and fairness is particularly salient to our work, and has featured studies that highlight the inequity that results from strategic behavior by individuals [21], as well as social cost disparities resulting from making classifiers robust to strategic behavior [32, 43]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "89959d1c87b83202913636ed6bd8d39fc2fc4a52",
                "externalIds": {
                    "DBLP": "conf/fat/EstornellDLV23",
                    "DOI": "10.1145/3593013.3594006",
                    "CorpusId": 259139757
                },
                "corpusId": 259139757,
                "publicationVenue": {
                    "id": "cdc83875-a82d-445c-b097-cbe91afe99a8",
                    "name": "Conference on Fairness, Accountability and Transparency",
                    "type": "conference",
                    "alternate_names": [
                        "FAccT",
                        "Conf Fairness Account Transpar"
                    ],
                    "url": "https://facctconference.org/"
                },
                "url": "https://www.semanticscholar.org/paper/89959d1c87b83202913636ed6bd8d39fc2fc4a52",
                "title": "Group-Fair Classification with Strategic Agents",
                "abstract": "The use of algorithmic decision making systems in domains which impact the financial, social, and political well-being of people has created a demand for these to be \u201cfair\u201d under some accepted notion of equity. This demand has in turn inspired a large body of work focused on the development of fair learning algorithms which are then used in lieu of their conventional counterparts. Most analysis of such fair algorithms proceeds from the assumption that the people affected by the algorithmic decisions are represented as immutable feature vectors. However, strategic agents may possess both the ability and the incentive to manipulate this observed feature vector in order to attain a more favorable outcome. We explore the impact that strategic agent behavior can have on group-fair classification. We find that in many settings strategic behavior can lead to fairness reversal, with a conventional classifier exhibiting higher fairness than a classifier trained to satisfy group fairness. Further, we show that fairness reversal occurs as a result of a group-fair classifier becoming more selective, achieving fairness largely by excluding individuals from the advantaged group. In contrast, if group fairness is achieved by the classifier becoming more inclusive, fairness reversal does not occur.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "103056375",
                        "name": "Andrew Estornell"
                    },
                    {
                        "authorId": "40583483",
                        "name": "Sanmay Das"
                    },
                    {
                        "authorId": "40457423",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "1699600",
                        "name": "Yevgeniy Vorobeychik"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "457dcf8cac3e82ac5bcb80963a8fcff0c3d90737",
                "externalIds": {
                    "DOI": "10.1109/icassp49357.2023.10094637",
                    "CorpusId": 258530412
                },
                "corpusId": 258530412,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/457dcf8cac3e82ac5bcb80963a8fcff0c3d90737",
                "title": "Adversarially Robust Fairness-Aware Regression",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "122923863",
                        "name": "Yulu Jin"
                    },
                    {
                        "authorId": "2067908545",
                        "name": "Lifeng Lai"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d85d7a205dfdde6cc41f46141756d14348557218",
                "externalIds": {
                    "DOI": "10.1038/s41551-023-01056-8",
                    "CorpusId": 259277694,
                    "PubMed": "37380750"
                },
                "corpusId": 259277694,
                "publicationVenue": {
                    "id": "5619586e-de5a-4bc3-ac80-04dd8530d80c",
                    "name": "Nature Biomedical Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Nat Biomed Eng"
                    ],
                    "issn": "2157-846X",
                    "url": "http://www.nature.com/natbiomedeng/"
                },
                "url": "https://www.semanticscholar.org/paper/d85d7a205dfdde6cc41f46141756d14348557218",
                "title": "Algorithmic fairness in artificial intelligence for medicine and healthcare",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108279369",
                        "name": "Richard J. Chen"
                    },
                    {
                        "authorId": "2109623647",
                        "name": "Judy J. Wang"
                    },
                    {
                        "authorId": "25259989",
                        "name": "Drew F. K. Williamson"
                    },
                    {
                        "authorId": "2242468870",
                        "name": "Tiffany Y. Chen"
                    },
                    {
                        "authorId": "1959705",
                        "name": "Jana Lipkov\u00e1"
                    },
                    {
                        "authorId": "16184125",
                        "name": "Ming Y. Lu"
                    },
                    {
                        "authorId": "2060422236",
                        "name": "S. Sahai"
                    },
                    {
                        "authorId": "37122655",
                        "name": "Faisal Mahmood"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Specifically, adversarial training involves training a separate (adversarial) classifier network by adding an adversarial loss so that the adversarial network cannot distinguish gender given the encoded image features (Edwards and Storkey, 2015; Berg et al., 2022; Xu et al., 2021).",
                "This is achieved by mainly two types of approaches, namely adversarial training (Edwards and Storkey, 2015; Berg et al., 2022; Xu et al., 2021) and mutual information (MI) minimization (Wang et al., 2021a, 2023).",
                "In-processing methods focus on altering the training objective by incorporating fairness constraints, regularization terms or leveraging adversarial learning to obtain representations invariant to gender/race (Berg et al., 2022; Wang et al., 2023; Xu et al., 2021; Cotter et al., 2019).",
                "This is achieved by mainly two types of approaches, namely adversarial training (Edwards and Storkey, 2015; Berg et al., 2022; Xu et al., 2021) and mutual information (MI) minimization (Wang et al."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "67b8d044919dac6139491845c625603f1ff77c0f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-19329",
                    "ArXiv": "2305.19329",
                    "DOI": "10.48550/arXiv.2305.19329",
                    "CorpusId": 258987942
                },
                "corpusId": 258987942,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/67b8d044919dac6139491845c625603f1ff77c0f",
                "title": "Mitigating Test-Time Bias for Fair Image Retrieval",
                "abstract": "We address the challenge of generating fair and unbiased image retrieval results given neutral textual queries (with no explicit gender or race connotations), while maintaining the utility (performance) of the underlying vision-language (VL) model. Previous methods aim to disentangle learned representations of images and text queries from gender and racial characteristics. However, we show these are inadequate at alleviating bias for the desired equal representation result, as there usually exists test-time bias in the target retrieval set. So motivated, we introduce a straightforward technique, Post-hoc Bias Mitigation (PBM), that post-processes the outputs from the pre-trained vision-language model. We evaluate our algorithm on real-world image search datasets, Occupation 1 and 2, as well as two large-scale image-text datasets, MS-COCO and Flickr30k. Our approach achieves the lowest bias, compared with various existing bias-mitigation methods, in text-based image retrieval result while maintaining satisfactory retrieval performance. The source code is publicly available at \\url{https://anonymous.4open.science/r/Fair_Text_based_Image_Retrieval-D8B2}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1411683893",
                        "name": "Fanjie Kong"
                    },
                    {
                        "authorId": "2113987946",
                        "name": "Shuai Yuan"
                    },
                    {
                        "authorId": "3314779",
                        "name": "Weituo Hao"
                    },
                    {
                        "authorId": "145153424",
                        "name": "Ricardo Henao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a85c6d3863e748374b631ecdd66e8e0cbf5a7440",
                "externalIds": {
                    "ArXiv": "2305.13057",
                    "DBLP": "journals/corr/abs-2305-13057",
                    "DOI": "10.48550/arXiv.2305.13057",
                    "CorpusId": 258832305
                },
                "corpusId": 258832305,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a85c6d3863e748374b631ecdd66e8e0cbf5a7440",
                "title": "Causality-Aided Trade-off Analysis for Machine Learning Fairness",
                "abstract": "There has been an increasing interest in enhancing the fairness of machine learning (ML). Despite the growing number of fairness-improving methods, we lack a systematic understanding of the trade-offs among factors considered in the ML pipeline when fairness-improving methods are applied. This understanding is essential for developers to make informed decisions regarding the provision of fair ML services. Nonetheless, it is extremely difficult to analyze the trade-offs when there are multiple fairness parameters and other crucial metrics involved, coupled, and even in conflict with one another. This paper uses causality analysis as a principled method for analyzing trade-offs between fairness parameters and other crucial metrics in ML pipelines. To ractically and effectively conduct causality analysis, we propose a set of domain-specific optimizations to facilitate accurate causal discovery and a unified, novel interface for trade-off analysis based on well-established causal inference methods. We conduct a comprehensive empirical study using three real-world datasets on a collection of widelyused fairness-improving techniques. Our study obtains actionable suggestions for users and developers of fair ML. We further demonstrate the versatile usage of our approach in selecting the optimal fairness-improving method, paving the way for more ethical and socially responsible AI technologies.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1899405856",
                        "name": "Zhenlan Ji"
                    },
                    {
                        "authorId": "1384480816",
                        "name": "Pingchuan Ma"
                    },
                    {
                        "authorId": "2118511999",
                        "name": "Shuai Wang"
                    },
                    {
                        "authorId": "2218136495",
                        "name": "Yanhui Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "And based on the effective robustness improvement and scalability, PGD-based adversarial training has been widely considered as the most effective and practical method for improving the robustness of ML models [41].",
                "And during various robustness enhancement technologies, adversarial training that trains ML models through a min\u2013max manner is thought as the most effective mechanism [41]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e16c90c86140de8689b4b9f150e333e5deaf71a2",
                "externalIds": {
                    "DBLP": "journals/iotj/ZhangTZZ23a",
                    "DOI": "10.1109/JIOT.2022.3222439",
                    "CorpusId": 253619838
                },
                "corpusId": 253619838,
                "publicationVenue": {
                    "id": "228761ec-c40a-479b-8309-9dcbe9851bcd",
                    "name": "IEEE Internet of Things Journal",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Internet Thing J"
                    ],
                    "issn": "2327-4662",
                    "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER288-ELE",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/servlet/opac?punumber=6488907",
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6488907",
                        "http://ieee-iotj.org/#"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e16c90c86140de8689b4b9f150e333e5deaf71a2",
                "title": "Robust Monitor for Industrial IoT Condition Prediction",
                "abstract": "The robustness of machine learning (ML) models has gained much attention along with their wide application on various safety-required Industrial Internet of Things (IIoT) paradigms. Researchers found that some specific attacks added on sensor measurements can maliciously disturb IIoT monitors that are designed using ML architectures. The Traditional detection methods could judge whether the measurements are attacked to prevent the failure of monitors. Unfortunately, recent works argue that the commonly used detection methods could be circumvented through adaptive attacks that could acquire the mechanism of detectors; they could not truly enhance the robustness of ML models. Instead, general robust mechanisms should be performed to authentically enhance the robustness of models against any potential attacks with specific restrictions. On the basis of the above argument, we design a robust condition monitor for predicting the fault condition of IIoT systems using the adversarial training technique called robust temporal convolutional network (RTCN). The model is designed to be formally robust to attacks with restricted magnitude. The temporal convolutional network (TCN) is employed to design the base structure of the monitor. TCN can capture temporal information from sensors to enhance the feature extraction performance of models. We also present a novel false data injection (FDI) attack-generating method that utilizes the conception of adversarial perturbations to disturb well-trained monitors. The experimental results verify the efficiency of feature extraction performance of our model from IIoT systems. Furthermore, adversarial training mechanism through a min\u2013max manner could effectively improve the reliability of ML-based IIoT monitors against strong FDI attacks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153648396",
                        "name": "Xingwei Zhang"
                    },
                    {
                        "authorId": "2069991042",
                        "name": "Hu Tian"
                    },
                    {
                        "authorId": "145091983",
                        "name": "Xiaolong Zheng"
                    },
                    {
                        "authorId": "2106064673",
                        "name": "D. Zeng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "First results have shown that this becomes more complicated if the fairness of the decision-making systems is a concern [71]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "25858d6c01f9c13bed90cfa00fce7207f3d1c967",
                "externalIds": {
                    "ArXiv": "2305.06055",
                    "DBLP": "journals/corr/abs-2305-06055",
                    "DOI": "10.48550/arXiv.2305.06055",
                    "CorpusId": 258587964
                },
                "corpusId": 258587964,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/25858d6c01f9c13bed90cfa00fce7207f3d1c967",
                "title": "A Classification of Feedback Loops and Their Relation to Biases in Automated Decision-Making Systems",
                "abstract": "Prediction-based decision-making systems are becoming increasingly prevalent in various domains. Previous studies have demonstrated that such systems are vulnerable to runaway feedback loops, e.g., when police are repeatedly sent back to the same neighborhoods regardless of the actual rate of criminal activity, which exacerbate existing biases. In practice, the automated decisions have dynamic feedback effects on the system itself that can perpetuate over time, making it difficult for short-sighted design choices to control the system's evolution. While researchers started proposing longer-term solutions to prevent adverse outcomes (such as bias towards certain groups), these interventions largely depend on ad hoc modeling assumptions and a rigorous theoretical understanding of the feedback dynamics in ML-based decision-making systems is currently missing. In this paper, we use the language of dynamical systems theory, a branch of applied mathematics that deals with the analysis of the interconnection of systems with dynamic behaviors, to rigorously classify the different types of feedback loops in the ML-based decision-making pipeline. By reviewing existing scholarly work, we show that this classification covers many examples discussed in the algorithmic fairness community, thereby providing a unifying and principled framework to study feedback loops. By qualitative analysis, and through a simulation example of recommender systems, we show which specific types of ML biases are affected by each type of feedback loop. We find that the existence of feedback loops in the ML-based decision-making pipeline can perpetuate, reinforce, or even reduce ML biases.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "103406391",
                        "name": "Nicol\u00f2 Pagan"
                    },
                    {
                        "authorId": "2058322501",
                        "name": "J. Baumann"
                    },
                    {
                        "authorId": "2086970716",
                        "name": "Ezzat Elokda"
                    },
                    {
                        "authorId": "2028069271",
                        "name": "G. Pasquale"
                    },
                    {
                        "authorId": "35863503",
                        "name": "S. Bolognani"
                    },
                    {
                        "authorId": "2216739364",
                        "name": "Anik'o Hann'ak"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2019) , class-wise fairness (Xu et al., 2021; Wei et al., 2023a) and the absence of formal guarantees (Wang et al.",
                "\u2026several defects remaining in adversarial training, such as decrease in natural accuracy (Tsipras et al., 2018), computational overhead (Shafahi et al., 2019) , class-wise fairness (Xu et al., 2021; Wei et al., 2023a) and the absence of formal guarantees (Wang et al., 2021; Zhang et al., 2023).",
                "\u2026robustness effectively, adversarial training has exposed several defects such as computational overhead (Shafahi et al., 2019), class-wise fairness (Xu et al., 2021; Wei et al., 2023a), among which the decreased natural accuracy (Tsipras et al., 2018; Wang & Wang, 2023) has become the major\u2026",
                ", 2019), class-wise fairness (Xu et al., 2021; Wei et al., 2023a), among which the decreased natural accuracy (Tsipras et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "aef972509d40670466d31b585ef856d6c897a811",
                "externalIds": {
                    "ArXiv": "2305.05392",
                    "CorpusId": 259313825
                },
                "corpusId": 259313825,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/aef972509d40670466d31b585ef856d6c897a811",
                "title": "Sharpness-Aware Minimization Alone can Improve Adversarial Robustness",
                "abstract": "Sharpness-Aware Minimization (SAM) is an effective method for improving generalization ability by regularizing loss sharpness. In this paper, we explore SAM in the context of adversarial robustness. We find that using only SAM can achieve superior adversarial robustness without sacrificing clean accuracy compared to standard training, which is an unexpected benefit. We also discuss the relation between SAM and adversarial training (AT), a popular method for improving the adversarial robustness of DNNs. In particular, we show that SAM and AT differ in terms of perturbation strength, leading to different accuracy and robustness trade-offs. We provide theoretical evidence for these claims in a simplified model. Finally, while AT suffers from decreased clean accuracy and computational overhead, we suggest that SAM can be regarded as a lightweight substitute for AT under certain requirements. Code is available at https://github.com/weizeming/SAM_AT.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2191808925",
                        "name": "Zeming Wei"
                    },
                    {
                        "authorId": "2216749387",
                        "name": "Jingyu Zhu"
                    },
                    {
                        "authorId": "1971137",
                        "name": "Yihao Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "linear classifier which minimizes the following classification error [28]2 f\u2217 = arg min f Pr(f(x) 6= y).",
                "[28], it is easy to obtain that fopt(x) = x + b (that is, w = 1).",
                "2It should be pointed out that although this theorem is presented in [28], the paper does not mention or discuss any concerns related to variance imbalances."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1e95f2a0da5bba3fe433fecffb7d698a4e3f7745",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-03900",
                    "ArXiv": "2305.03900",
                    "DOI": "10.48550/arXiv.2305.03900",
                    "CorpusId": 258557600
                },
                "corpusId": 258557600,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1e95f2a0da5bba3fe433fecffb7d698a4e3f7745",
                "title": "Rethinking Class Imbalance in Machine Learning",
                "abstract": "Imbalance learning is a subfield of machine learning that focuses on learning tasks in the presence of class imbalance. Nearly all existing studies refer to class imbalance as a proportion imbalance, where the proportion of training samples in each class is not balanced. The ignorance of the proportion imbalance will result in unfairness between/among classes and poor generalization capability. Previous literature has presented numerous methods for either theoretical/empirical analysis or new methods for imbalance learning. This study presents a new taxonomy of class imbalance in machine learning with a broader scope. Four other types of imbalance, namely, variance, distance, neighborhood, and quality imbalances between/among classes, which may exist in machine learning tasks, are summarized. Two different levels of imbalance including global and local are also presented. Theoretical analysis is used to illustrate the significant impact of the new imbalance types on learning fairness. Moreover, our taxonomy and theoretical conclusions are used to analyze the shortcomings of several classical methods. As an example, we propose a new logit perturbation-based imbalance learning loss when proportion, variance, and distance imbalances exist simultaneously. Several classical losses become the special case of our proposed method. Meta learning is utilized to infer the hyper-parameters related to the three types of imbalance. Experimental results on several benchmark corpora validate the effectiveness of the proposed method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2061463107",
                        "name": "Ou Wu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "22613c1389af9aeb9c0ce4c9bc2acd4910ad3a01",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-03355",
                    "ArXiv": "2305.03355",
                    "DOI": "10.48550/arXiv.2305.03355",
                    "CorpusId": 258547123
                },
                "corpusId": 258547123,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/22613c1389af9aeb9c0ce4c9bc2acd4910ad3a01",
                "title": "A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness",
                "abstract": "The aim of dataset distillation is to encode the rich features of an original dataset into a tiny dataset. It is a promising approach to accelerate neural network training and related studies. Different approaches have been proposed to improve the informativeness and generalization performance of distilled images. However, no work has comprehensively analyzed this technique from a security perspective and there is a lack of systematic understanding of potential risks. In this work, we conduct extensive experiments to evaluate current state-of-the-art dataset distillation methods. We successfully use membership inference attacks to show that privacy risks still remain. Our work also demonstrates that dataset distillation can cause varying degrees of impact on model robustness and amplify model unfairness across classes when making predictions. This work offers a large-scale benchmarking framework for dataset distillation evaluation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109296817",
                        "name": "Zongxiong Chen"
                    },
                    {
                        "authorId": "2093923066",
                        "name": "Jiahui Geng"
                    },
                    {
                        "authorId": "2215923457",
                        "name": "Herbert Woisetschlaeger"
                    },
                    {
                        "authorId": "3446997",
                        "name": "Sonja Schimmler"
                    },
                    {
                        "authorId": "2215923565",
                        "name": "Ruben Mayer"
                    },
                    {
                        "authorId": "2056440506",
                        "name": "Chunming Rong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[30] revealed that classes with lower compactness, indicated by large variances, are more challenging and exhibit poorer performance."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0338bf755af0cd6b585f179d4964da399fef3ba7",
                "externalIds": {
                    "ArXiv": "2304.13431",
                    "DBLP": "journals/corr/abs-2304-13431",
                    "DOI": "10.48550/arXiv.2304.13431",
                    "CorpusId": 258332092
                },
                "corpusId": 258332092,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0338bf755af0cd6b585f179d4964da399fef3ba7",
                "title": "Implicit Counterfactual Data Augmentation for Deep Neural Networks",
                "abstract": "Machine-learning models are prone to capturing the spurious correlations between non-causal attributes and classes, with counterfactual data augmentation being a promising direction for breaking these spurious associations. However, explicitly generating counterfactual data is challenging, with the training efficiency declining. Therefore, this study proposes an implicit counterfactual data augmentation (ICDA) method to remove spurious correlations and make stable predictions. Specifically, first, a novel sample-wise augmentation strategy is developed that generates semantically and counterfactually meaningful deep features with distinct augmentation strength for each sample. Second, we derive an easy-to-compute surrogate loss on the augmented feature set when the number of augmented samples becomes infinite. Third, two concrete schemes are proposed, including direct quantification and meta-learning, to derive the key parameters for the robust loss. In addition, ICDA is explained from a regularization aspect, with extensive experiments indicating that our method consistently improves the generalization performance of popular depth networks on multiple typical learning scenarios that require out-of-distribution generalization.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48667407",
                        "name": "Xiaoling Zhou"
                    },
                    {
                        "authorId": "2061463107",
                        "name": "Ou Wu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2810e9602d87f132e2b2f3eda6539d88d5330ec6",
                "externalIds": {
                    "DBLP": "conf/aaai/ZhouYW23",
                    "ArXiv": "2304.12550",
                    "DOI": "10.48550/arXiv.2304.12550",
                    "CorpusId": 258309137
                },
                "corpusId": 258309137,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/2810e9602d87f132e2b2f3eda6539d88d5330ec6",
                "title": "Combining Adversaries with Anti-adversaries in Training",
                "abstract": "Adversarial training is an effective learning technique to improve the robustness of deep neural networks. In this study, the influence of adversarial training on deep learning models in terms of fairness, robustness, and generalization is theoretically investigated under more general perturbation scope that different samples can have different perturbation directions (the adversarial and anti-adversarial directions) and varied perturbation bounds. Our theoretical explorations suggest that the combination of adversaries and anti-adversaries (samples with anti-adversarial perturbations) in training can be more effective in achieving better fairness between classes and a better tradeoff between robustness and generalization in some typical learning scenarios (e.g., noisy label learning and imbalance learning) compared with standard adversarial training. On the basis of our theoretical findings, a more general learning objective that combines adversaries and anti-adversaries with varied bounds on each training sample is presented. Meta learning is utilized to optimize the combination weights. Experiments on benchmark datasets under different learning scenarios verify our theoretical findings and the effectiveness of the proposed methodology.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48667407",
                        "name": "Xiaoling Zhou"
                    },
                    {
                        "authorId": "2153402103",
                        "name": "Nan Yang"
                    },
                    {
                        "authorId": "2061463107",
                        "name": "Ou Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[23] Han Xu, Xiaorui Liu, Yaxin Li, Anil Jain, and Jiliang Tang."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "93539d72d4a7295724bcf09f81930be4058dc778",
                "externalIds": {
                    "ArXiv": "2304.09779",
                    "DBLP": "journals/corr/abs-2304-09779",
                    "DOI": "10.48550/arXiv.2304.09779",
                    "CorpusId": 258212999
                },
                "corpusId": 258212999,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/93539d72d4a7295724bcf09f81930be4058dc778",
                "title": "Equalised Odds is not Equal Individual Odds: Post-processing for Group and Individual Fairness",
                "abstract": "Group fairness is achieved by equalising prediction distributions between protected sub-populations; individual fairness requires treating similar individuals alike. These two objectives, however, are incompatible when a scoring model is calibrated through discontinuous probability functions, where individuals can be randomly assigned an outcome determined by a fixed probability. This procedure may provide two similar individuals from the same protected group with classification odds that are disparately different -- a clear violation of individual fairness. Assigning unique odds to each protected sub-population may also prevent members of one sub-population from ever receiving equal chances of a positive outcome to another, which we argue is another type of unfairness called individual odds. We reconcile all this by constructing continuous probability functions between group thresholds that are constrained by their Lipschitz constant. Our solution preserves the model's predictive power, individual fairness and robustness while ensuring group fairness.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2214719520",
                        "name": "Edward A. Small"
                    },
                    {
                        "authorId": "32615063",
                        "name": "Kacper Sokol"
                    },
                    {
                        "authorId": "2214713837",
                        "name": "Daniel Manning"
                    },
                    {
                        "authorId": "144954586",
                        "name": "Flora D. Salim"
                    },
                    {
                        "authorId": "2151246882",
                        "name": "Jeffrey Chan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also compare our approach with FRL [29].",
                "To address this issue, Fair Robust Learning (FRL) [29] has been proposed, which adjusts the margin and weight among classes when fairness constraints are violated.",
                "This can explain why Fair Robust Learning (FRL) [29] can improve robust fairness by enlarging the margin for the hard classes, since the model reduces the over-fitting problem on these classes.",
                "Comparison with Fair Robust Learning (FRL) [29].",
                "Finally, we compare our approach with FRL [29], the only existing adversarial training algorithm that focuses on improving the fairness of classwise robustness.",
                ", the model may exhibit strong robustness on some classes while it can be highly vulnerable on others, as firstly revealed in [4, 21, 29]."
            ],
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "0f32276f7f72f0dcf58fe511583a301315878978",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-14460",
                    "ArXiv": "2303.14460",
                    "DOI": "10.1109/CVPR52729.2023.00792",
                    "CorpusId": 257767427
                },
                "corpusId": 257767427,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0f32276f7f72f0dcf58fe511583a301315878978",
                "title": "CFA: Class-Wise Calibrated Fair Adversarial Training",
                "abstract": "Adversarial training has been widely acknowledged as the most effective method to improve the adversarial robustness against adversarial examples for Deep Neural Networks (DNNs). So far, most existing works focus on en-hancing the overall model robustness, treating each class equally in both the training and testing phases. Although revealing the disparity in robustness among classes, few works try to make adversarial training fair at the class level without sacrificing overall robustness. In this paper, we are the first to theoretically and empirically investigate the preference of different classes for adversarial configu-rations, including perturbation margin, regularization, and weight averaging. Motivated by this, we further propose a Class-wise calibrated Fair Adversarial training frame-work, named CFA, which customizes specific training con-figurations for each class automatically. Experiments on benchmark datasets demonstrate that our proposed CFA can improve both overall robustness and fairness notably over other state-of-the-art methods. Code is available at https://github.com/PKU-ML/CFA.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2191808925",
                        "name": "Zeming Wei"
                    },
                    {
                        "authorId": "2115568564",
                        "name": "Yifei Wang"
                    },
                    {
                        "authorId": "2527106",
                        "name": "Yiwen Guo"
                    },
                    {
                        "authorId": "2115869684",
                        "name": "Yisen Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1cf0f1c0ee89820b7504d69d3dcfc9e6b41783b9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-02874",
                    "ArXiv": "2303.02874",
                    "DOI": "10.14569/IJACSA.2023.0140202",
                    "CorpusId": 257365311
                },
                "corpusId": 257365311,
                "publicationVenue": {
                    "id": "20a3a2f3-532a-4f04-9f3d-1e268e100872",
                    "name": "International Journal of Advanced Computer Science and Applications",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Adv Comput Sci Appl"
                    ],
                    "issn": "2156-5570",
                    "url": "http://sites.google.com/site/ijacsa2010/",
                    "alternate_urls": [
                        "http://thesai.org/Publication/Default.aspx",
                        "https://thesai.org/Publications/IJACSA"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1cf0f1c0ee89820b7504d69d3dcfc9e6b41783b9",
                "title": "Adversarial Sampling for Fairness Testing in Deep Neural Network",
                "abstract": "In this research, we focus on the usage of adversarial sampling to test for the fairness in the prediction of deep neural network model across different classes of image in a given dataset. While several framework had been proposed to ensure robustness of machine learning model against adversarial attack, some of which includes adversarial training algorithm. There is still the pitfall that adversarial training algorithm tends to cause disparity in accuracy and robustness among different group. Our research is aimed at using adversarial sampling to test for fairness in the prediction of deep neural network model across different classes or categories of image in a given dataset. We successfully demonstrated a new method of ensuring fairness across various group of input in deep neural network classifier. We trained our neural network model on the original image, and without training our model on the perturbed or attacked image. When we feed the adversarial samplings to our model, it was able to predict the original category/ class of the image the adversarial sample belongs to. We also introduced and used the separation of concern concept from software engineering whereby there is an additional standalone filter layer that filters perturbed image by heavily removing the noise or attack before automatically passing it to the network for classification, we were able to have accuracy of 93.3%. Cifar-10 dataset have ten categories of dataset, and so, in order to account for fairness, we applied our hypothesis across each categories of dataset and were able to get a consistent result and accuracy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2167644467",
                        "name": "Tosin Ige"
                    },
                    {
                        "authorId": "2175158718",
                        "name": "William Marfo"
                    },
                    {
                        "authorId": "2210796693",
                        "name": "Justin Tonkinson"
                    },
                    {
                        "authorId": "2167646789",
                        "name": "Sikiru Adewale"
                    },
                    {
                        "authorId": "2210795850",
                        "name": "Bolanle Hafiz Matti"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7d2d1e6a3cefc2a337640c710f591a0d6a038908",
                "externalIds": {
                    "ArXiv": "2303.02532",
                    "DBLP": "journals/corr/abs-2303-02532",
                    "DOI": "10.48550/arXiv.2303.02532",
                    "CorpusId": 257364933
                },
                "corpusId": 257364933,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7d2d1e6a3cefc2a337640c710f591a0d6a038908",
                "title": "PRECISION: Decentralized Constrained Min-Max Learning with Low Communication and Sample Complexities",
                "abstract": "Recently, min-max optimization problems have received increasing attention due to their wide range of applications in machine learning (ML). However, most existing min-max solution techniques are either single-machine or distributed algorithms coordinated by a central server. In this paper, we focus on the decentralized min-max optimization for learning with domain constraints, where multiple agents collectively solve a nonconvex-strongly-concave min-max saddle point problem without coordination from any server. Decentralized min-max optimization problems with domain constraints underpins many important ML applications, including multi-agent ML fairness assurance, and policy evaluations in multi-agent reinforcement learning. We propose an algorithm called PRECISION (proximal gradient-tracking and stochastic recursive variance reduction) that enjoys a convergence rate of O(1/T), where T is the maximum number of iterations. To further reduce sample complexity, we propose PRECISION+ with an adaptive batch size technique. We show that the fast O(1/T) convergence of PRECISION and PRECISION+ to an \u03b5-stationary point imply O(\u03b5-2) communication complexity and [EQUATION] sample complexity, where m is the number of agents and n is the size of dataset at each agent. To our knowledge, this is the first work that achieves O(\u03b5-2) in both sample and communication complexities in decentralized min-max learning with domain constraints. Our experiments also corroborate the theoretical results.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2164219912",
                        "name": "Zhuqing Liu"
                    },
                    {
                        "authorId": "2149174583",
                        "name": "Xin Zhang"
                    },
                    {
                        "authorId": "1606015788",
                        "name": "Songtao Lu"
                    },
                    {
                        "authorId": "2142651766",
                        "name": "Jia Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026which ensures that the standard error (which dictates the classification accuracy of the networks) and boundary error (since the inputs from class(es) closer to the decision boundary are expected to be more vulnerable under noise) (H. Xu et al., 2021) are minimal, thereby minimizing the bias."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c6f4f23dfa410206624769ba6163f912843ee044",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-12538",
                    "ArXiv": "2302.12538",
                    "DOI": "10.1007/s10994-023-06314-z",
                    "CorpusId": 257205726
                },
                "corpusId": 257205726,
                "publicationVenue": {
                    "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
                    "name": "Machine-mediated learning",
                    "type": "journal",
                    "alternate_names": [
                        "Mach learn",
                        "Machine Learning",
                        "Mach Learn"
                    ],
                    "issn": "0732-6718",
                    "alternate_issns": [
                        "0885-6125"
                    ],
                    "url": "http://www.springer.com/computer/artificial/journal/10994",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10994",
                        "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c6f4f23dfa410206624769ba6163f912843ee044",
                "title": "UnbiasedNets: A Dataset Diversification Framework for Robustness Bias Alleviation in Neural Networks",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1443434483",
                        "name": "Mahum Naseer"
                    },
                    {
                        "authorId": "10772089",
                        "name": "B. Prabakaran"
                    },
                    {
                        "authorId": "2090504235",
                        "name": "Osman Hasan"
                    },
                    {
                        "authorId": "2157519136",
                        "name": "Muhammad Shafique"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Prior works have examined adversarial hardening and fairness, defined as the vulnerability of each class within a classification task [40] in completely classbalanced datasets."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0e371022250f6229c157f5763409176ade237915",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-08973",
                    "ArXiv": "2302.08973",
                    "DOI": "10.48550/arXiv.2302.08973",
                    "CorpusId": 257019743
                },
                "corpusId": 257019743,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0e371022250f6229c157f5763409176ade237915",
                "title": "Measuring Equality in Machine Learning Security Defenses",
                "abstract": "Over the past decade, the machine learning security community has developed a myriad of defenses for evasion attacks. An understudied question in that community is: for whom do these defenses defend? This work considers common approaches to defending learned systems and how security defenses result in performance inequities across different sub-populations. We outline appropriate parity metrics for analysis and begin to answer this question through empirical results of the fairness implications of machine learning security methods. We find that many methods that have been proposed can cause direct harm, like false rejection and unequal benefits from robustness training. The framework we propose for measuring defense equality can be applied to robustly trained models, preprocessing-based defenses, and rejection methods. We identify a set of datasets with a user-centered application and a reasonable computational cost suitable for case studies in measuring the equality of defenses. In our case study of speech command recognition, we show how such adversarial training and augmentation have non-equal but complex protections for social subgroups across gender, accent, and age in relation to user coverage. We present a comparison of equality between two rejection-based defenses: randomized smoothing and neural rejection, finding randomized smoothing more equitable due to the sampling mechanism for minority groups. This represents the first work examining the disparity in the adversarial robustness in the speech domain and the fairness evaluation of rejection-based defenses.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "20566880",
                        "name": "Luke E. Richards"
                    },
                    {
                        "authorId": "34885007",
                        "name": "Edward Raff"
                    },
                    {
                        "authorId": "2127879703",
                        "name": "Cynthia Matuszek"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "18d6d87e20eeed78e1e0dbb53def7d4129852cca",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-06923",
                    "ArXiv": "2302.06923",
                    "DOI": "10.48550/arXiv.2302.06923",
                    "CorpusId": 256846797
                },
                "corpusId": 256846797,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/18d6d87e20eeed78e1e0dbb53def7d4129852cca",
                "title": "Do Neural Networks Generalize from Self-Averaging Sub-classifiers in the Same Way As Adaptive Boosting?",
                "abstract": "In recent years, neural networks (NNs) have made giant leaps in a wide variety of domains. NNs are often referred to as black box algorithms due to how little we can explain their empirical success. Our foundational research seeks to explain why neural networks generalize. A recent advancement derived a mutual information measure for explaining the performance of deep NNs through a sequence of increasingly complex functions. We show deep NNs learn a series of boosted classifiers whose generalization is popularly attributed to self-averaging over an increasing number of interpolating sub-classifiers. To our knowledge, we are the first authors to establish the connection between generalization in boosted classifiers and generalization in deep NNs. Our experimental evidence and theoretical analysis suggest NNs trained with dropout exhibit similar self-averaging behavior over interpolating sub-classifiers as cited in popular explanations for the post-interpolation generalization phenomenon in boosting.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2205713629",
                        "name": "Michael Sun"
                    },
                    {
                        "authorId": "2133041191",
                        "name": "Peter Chatain"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Class-wise variance is a common measure used in (Xu et al. 2021) and (Tian et al.",
                "For example, (Xu et al. 2021) propose the re-weight and re-margin strategies on TRADES.",
                "Xu et al. (2021) propose employing re-weight and re-margin strategies to solve this problem.",
                "Although AT obtains great average adversarial robustness performance over classes, (Benz et al. 2020; Xu et al. 2021; Tian et al. 2021) find that a robust model well-trained by AT exhibits a large robustness disparity in different classes on various balanced datasets, like the left classifier in\u2026",
                "To solve this problem, Benz et al. (2020) use a cost-sensitive learning fashion which is widely used in natural learning with imbalanced datasets; Xu et al. (2021) propose a new method to reduce the class-wise variance of robust accuracy over classes.",
                "To solve this problem, recently, various strategies (Benz et al. 2020; Xu et al. 2021) aimed at making the robust performance of the model consistent over all classes have been proposed.",
                "Following (Xu et al. 2021), we use average natural accuracy, average robust accuracy, worst-class natural accuracy and worst-class robust accuracy to evaluate the performance of all methods.",
                "Zhang et al. (2019), Xu et al. (2021) and Benz et al. (2020) are used as our baselines.",
                "Although AT obtains great average adversarial robustness performance over classes, (Benz et al. 2020; Xu et al. 2021; Tian et al. 2021) find that a robust model well-trained by AT exhibits a large robustness disparity in different classes on various balanced datasets, like the left classifier in Figure 1.",
                "To overcome the limitations of Benz et al. (2020); Xu et al. (2021), this paper proposes a novel min-max learning paradigm to optimize worst-class robust risk and leverages noregret dynamics to solve the proposed min-max problem, our goal is to achieve a classifier with great performance on\u2026",
                "Recently, some works (Benz et al. 2020; Xu et al. 2021) have attempted to solve this problem.",
                "Following (Xu et al. 2021), we set \u03c41 = \u03c42 = 0.05, \u03b11 = \u03b12 = 0.05 for FRL on CIFAR-10.",
                "FRL is presented in Xu et al. (2021).",
                "Recent works (Benz et al. 2020; Xu et al. 2021; Tian et al. 2021) have shown that a robust model well-trained by AT exhibits a remarkable robustness disparity among classes, and propose various methods to obtain consistent robust accuracy across classes.",
                "Class-wise variance is a common measure used in (Xu et al. 2021) and (Tian et al. 2021)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a9a0b9be86acb372d78f349b84a7f21fa2f53919",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-04025",
                    "ArXiv": "2302.04025",
                    "DOI": "10.48550/arXiv.2302.04025",
                    "CorpusId": 256662440
                },
                "corpusId": 256662440,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a9a0b9be86acb372d78f349b84a7f21fa2f53919",
                "title": "WAT: Improve the Worst-class Robustness in Adversarial Training",
                "abstract": "Deep Neural Networks (DNN) have been shown to be vulnerable to adversarial examples. Adversarial training (AT) is a popular and effective strategy to defend against adversarial attacks. Recent works have shown that a robust model well-trained by AT exhibits a remarkable robustness disparity among classes, and propose various methods to obtain consistent robust accuracy across classes. Unfortunately, these methods sacrifice a good deal of the average robust accuracy. Accordingly, this paper proposes a novel framework of worst-class adversarial training and leverages no-regret dynamics to solve this problem. Our goal is to obtain a classifier with great performance on worst-class and sacrifice just a little average robust accuracy at the same time. We then rigorously analyze the theoretical properties of our proposed algorithm, and the generalization error bound in terms of the worst-class robust risk. Furthermore, we propose a measurement to evaluate the proposed method in terms of both the average and worst-class accuracies. Experiments on various datasets and networks show that our proposed method outperforms the state-of-the-art approaches.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2132447506",
                        "name": "Boqi Li"
                    },
                    {
                        "authorId": "2155130354",
                        "name": "Weiwei Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d2712df0a1a224ac8955a662746a8b505682f4c2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-03465",
                    "ArXiv": "2302.03465",
                    "DOI": "10.1145/3593013.3594057",
                    "CorpusId": 256627631
                },
                "corpusId": 256627631,
                "publicationVenue": {
                    "id": "cdc83875-a82d-445c-b097-cbe91afe99a8",
                    "name": "Conference on Fairness, Accountability and Transparency",
                    "type": "conference",
                    "alternate_names": [
                        "FAccT",
                        "Conf Fairness Account Transpar"
                    ],
                    "url": "https://facctconference.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d2712df0a1a224ac8955a662746a8b505682f4c2",
                "title": "Robustness Implies Fairness in Causal Algorithmic Recourse",
                "abstract": "Algorithmic recourse discloses the internal procedures of a black-box decision process where decisions have significant consequences by providing recommendations to empower beneficiaries to achieve a more favorable outcome. To ensure an effective remedy, suggested interventions must not only be cost-effective but also robust and fair. To that end, it is essential to provide similar explanations to similar individuals. This study explores the concept of individual fairness and adversarial robustness in causal algorithmic recourse and addresses the challenge of achieving both. To resolve the challenges, we propose a new framework for defining adversarially robust recourse. That setting observes the protected feature as a pseudometric and demonstrates that individual fairness is a special case of adversarial robustness. Finally, we introduce the fair robust recourse problem and establish solutions to achieve both desirable properties both theoretically and empirically.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47515476",
                        "name": "A. Ehyaei"
                    },
                    {
                        "authorId": "145926563",
                        "name": "Amir-Hossein Karimi"
                    },
                    {
                        "authorId": "1707625",
                        "name": "B. Sch\u00f6lkopf"
                    },
                    {
                        "authorId": "1822482",
                        "name": "S. Maghsudi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "43b02bcbafab8600b40900e8882754baab85f2fa",
                "externalIds": {
                    "DBLP": "journals/kbs/Tian0Z0Y23",
                    "DOI": "10.1016/j.knosys.2023.110417",
                    "CorpusId": 257223347
                },
                "corpusId": 257223347,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/43b02bcbafab8600b40900e8882754baab85f2fa",
                "title": "CIFair: Constructing continuous domains of invariant features for image fair classifications",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51262864",
                        "name": "Huan Tian"
                    },
                    {
                        "authorId": "145306564",
                        "name": "B. Liu"
                    },
                    {
                        "authorId": "2185053609",
                        "name": "Tianqing Zhu"
                    },
                    {
                        "authorId": "2134555583",
                        "name": "Wanlei Zhou"
                    },
                    {
                        "authorId": "2721708",
                        "name": "P. Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Notably, our approach for the fair ViTs is a novel addition to the growing body of work on \u201cadversarial examples for fairness\u201d [66, 62]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "03d22caf64831d1c48a5a25b3f886196a13d1dce",
                "externalIds": {
                    "ArXiv": "2301.13803",
                    "DBLP": "journals/corr/abs-2301-13803",
                    "DOI": "10.48550/arXiv.2301.13803",
                    "CorpusId": 256416070
                },
                "corpusId": 256416070,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/03d22caf64831d1c48a5a25b3f886196a13d1dce",
                "title": "Fairness-aware Vision Transformer via Debiased Self-Attention",
                "abstract": "Vision Transformer (ViT) has recently gained significant interest in solving computer vision (CV) problems due to its capability of extracting informative features and modeling long-range dependencies through the self-attention mechanism. To fully realize the advantages of ViT in real-world applications, recent works have explored the trustworthiness of ViT, including its robustness and explainability. However, another desiderata, fairness has not yet been adequately addressed in the literature. We establish that the existing fairness-aware algorithms (primarily designed for CNNs) do not perform well on ViT. This necessitates the need for developing our novel framework via Debiased Self-Attention (DSA). DSA is a fairness-through-blindness approach that enforces ViT to eliminate spurious features correlated with the sensitive attributes for bias mitigation. Notably, adversarial examples are leveraged to locate and mask the spurious features in the input image patches. In addition, DSA utilizes an attention weights alignment regularizer in the training objective to encourage learning informative features for target prediction. Importantly, our DSA framework leads to improved fairness guarantees over prior works on multiple prediction tasks without compromising target prediction performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2062242240",
                        "name": "Yao Qiang"
                    },
                    {
                        "authorId": "46651935",
                        "name": "Chengyin Li"
                    },
                    {
                        "authorId": "4386787",
                        "name": "Prashant Khanduri"
                    },
                    {
                        "authorId": "39895985",
                        "name": "D. Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[182] found that safety-awareness learning poses a disparate impact on the fairness risk of subgroups."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "135fd639a1c4f035f1e7bf2111e42798fcdaffee",
                "externalIds": {
                    "ArXiv": "2212.09006",
                    "DBLP": "journals/corr/abs-2212-09006",
                    "DOI": "10.1561/116.00000084",
                    "CorpusId": 254853625
                },
                "corpusId": 254853625,
                "publicationVenue": {
                    "id": "d081062e-f8a7-4018-9072-ca99195f38d8",
                    "name": "APSIPA Transactions on Signal and Information Processing",
                    "alternate_names": [
                        "APSIPA Trans Signal Inf Process"
                    ],
                    "issn": "2048-7703",
                    "url": "https://www.cambridge.org/core/journals/apsipa-transactions-on-signal-and-information-processing",
                    "alternate_urls": [
                        "http://journals.cambridge.org/action/displayJournal?jid=SIP"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/135fd639a1c4f035f1e7bf2111e42798fcdaffee",
                "title": "A Review of Speech-centric Trustworthy Machine Learning: Privacy, Safety, and Fairness",
                "abstract": "Speech-centric machine learning systems have revolutionized many leading domains ranging from transportation and healthcare to education and defense, profoundly changing how people live, work, and interact with each other. However, recent studies have demonstrated that many speech-centric ML systems may need to be considered more trustworthy for broader deployment. Specifically, concerns over privacy breaches, discriminating performance, and vulnerability to adversarial attacks have all been discovered in ML research fields. In order to address the above challenges and risks, a significant number of efforts have been made to ensure these ML systems are trustworthy, especially private, safe, and fair. In this paper, we conduct the first comprehensive survey on speech-centric trustworthy ML topics related to privacy, safety, and fairness. In addition to serving as a summary report for the research community, we point out several promising future research directions to inspire the researchers who wish to explore further in this area.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50336596",
                        "name": "Tiantian Feng"
                    },
                    {
                        "authorId": "51257856",
                        "name": "Rajat Hebbar"
                    },
                    {
                        "authorId": "2118974816",
                        "name": "Nicholas Mehlman"
                    },
                    {
                        "authorId": "2196939199",
                        "name": "Xuan Shi"
                    },
                    {
                        "authorId": "2197478804",
                        "name": "Aditya Kommineni"
                    },
                    {
                        "authorId": "152434613",
                        "name": "Shrikanth S. Narayanan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Xu et al. (2021) focus on a balanced class setting but with varying \u201cdifficulty levels\u201d as measured by the magnitude of variance, whereas we address class imbalance.",
                "The notion of accuracy disparity in our context focuses on the performance gap of a model on different sub-groups of the overall population, where each group is indexed by the corresponding class label (Santurkar et al., 2021; Xu et al., 2021).",
                "Xu et al. (2021); Ma et al. (2022) identify and analyze the significant disparity of standard accuracy and robust accuracy among different classes or subgroups of data for adversarially trained models.",
                "If it is, then what are the fundamental factors that contribute to this potential drop of accuracy and the increase of accuracy disparity? To the best of our knowledge, there are only a few works (Tsipras et al., 2019; Xu et al., 2021; Ma et al., 2022) that partially attempt to approach these problems.",
                "While it has been shown in a previous work (Xu et al., 2021) that adversarial robustness does introduce severe accuracy disparity when different classes exhibit different \u201cdifficulty levels\u201d of learning (i.e., different magnitude of variance) in a toy example (as indicated by specific choices of\u2026",
                "While it has been shown in a previous work (Xu et al., 2021) that adversarial robustness does introduce severe accuracy disparity when different classes exhibit different \u201cdifficulty levels\u201d of learning (i.",
                "To the best of our knowledge, there are only a few works (Tsipras et al., 2019; Xu et al., 2021; Ma et al., 2022) that partially attempt to approach these problems.",
                "Following prior works (Tsipras et al., 2019; Xu et al., 2021), for the model, we consider a linear classifier and couple it with a sign function sgn to obtain the output f(x;w, b) := sgn(w\u22a4x+ b)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "e5418fd23e17dc014d7436a9ae5b98b52ac2c3ca",
                "externalIds": {
                    "ArXiv": "2211.15762",
                    "DBLP": "journals/corr/abs-2211-15762",
                    "DOI": "10.48550/arXiv.2211.15762",
                    "CorpusId": 254069952
                },
                "corpusId": 254069952,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e5418fd23e17dc014d7436a9ae5b98b52ac2c3ca",
                "title": "Understanding the Impact of Adversarial Robustness on Accuracy Disparity",
                "abstract": "While it has long been empirically observed that adversarial robustness may be at odds with standard accuracy and may have further disparate impacts on different classes, it remains an open question to what extent such observations hold and how the class imbalance plays a role within. In this paper, we attempt to understand this question of accuracy disparity by taking a closer look at linear classifiers under a Gaussian mixture model. We decompose the impact of adversarial robustness into two parts: an inherent effect that will degrade the standard accuracy on all classes due to the robustness constraint, and the other caused by the class imbalance ratio, which will increase the accuracy disparity compared to standard training. Furthermore, we also show that such effects extend beyond the Gaussian mixture model, by generalizing our data model to the general family of stable distributions. More specifically, we demonstrate that while the constraint of adversarial robustness consistently degrades the standard accuracy in the balanced class setting, the class imbalance ratio plays a fundamentally different role in accuracy disparity compared to the Gaussian case, due to the heavy tail of the stable distribution. We additionally perform experiments on both synthetic and real-world datasets to corroborate our theoretical findings. Our empirical results also suggest that the implications may extend to nonlinear models over real-world datasets. Our code is publicly available on GitHub at https://github.com/Accuracy-Disparity/AT-on-AD.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2134348585",
                        "name": "Yuzheng Hu"
                    },
                    {
                        "authorId": "2116491459",
                        "name": "Fan Wu"
                    },
                    {
                        "authorId": "40975176",
                        "name": "Hongyang Zhang"
                    },
                    {
                        "authorId": "2182575513",
                        "name": "Hang Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Within this context, recently Xu et al [44] has shown that adversarially robust models exhibit remarkable disparity of natural accuracy and robust accuracy metrics among different classes, compared to those exhibited by their standard counterpart.",
                "6, 15 [44] Han Xu, Xiaorui Liu, Yaxin Li, Anil Jain, and Jiliang Tang.",
                "For FMNIST and CIFAR, the experiments use their standard labels and assume that labels are also protected groups, mirroring the setting of previous work [23, 39, 44].",
                "Those trained on FMNIST and CIFAR, use a learning rate of 1e\u22121 and 200 epochs, as suggested in previous work [44].",
                "It has also been shown that constraining the model\u2019s hypothesis space to satisfy privacy [2], sparsity [14, 15], or robustness [26, 44] can result in disparate outcomes."
            ],
            "intents": [],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "939f6d5d17ec2b07602782858c47111473da8835",
                "externalIds": {
                    "ArXiv": "2211.11835",
                    "DBLP": "journals/corr/abs-2211-11835",
                    "DOI": "10.48550/arXiv.2211.11835",
                    "CorpusId": 253760940
                },
                "corpusId": 253760940,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/939f6d5d17ec2b07602782858c47111473da8835",
                "title": "Fairness Increases Adversarial Vulnerability",
                "abstract": "The remarkable performance of deep learning models and their applications in consequential domains (e.g., facial recognition) introduces important challenges at the intersection of equity and security. Fairness and robustness are two desired notions often required in learning models. Fairness ensures that models do not disproportionately harm (or ben-e\ufb01t) some groups over others, while robustness measures the models\u2019 resilience against small input perturbations. This paper shows the existence of a dichotomy between fairness and robustness, and analyzes when achieving fairness decreases the model robustness to adversarial samples. The reported analysis sheds light on the factors causing such contrasting behavior, suggesting that distance to the decision boundary across groups as a key explainer for this behavior. Extensive experiments on non-linear models and different architectures validate the theoretical \ufb01ndings in multiple vision domains. Finally, the paper proposes a simple, yet effective, solution to construct models achieving good tradeoffs between fairness and robustness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2055333757",
                        "name": "Cuong D. Tran"
                    },
                    {
                        "authorId": "40799079",
                        "name": "Keyu Zhu"
                    },
                    {
                        "authorId": "2141569789",
                        "name": "Ferdinando Fioretto"
                    },
                    {
                        "authorId": "72488634",
                        "name": "P. V. Hentenryck"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[Extended from Xu et al. (2021)] By Xu et al. (2021, Lemma 2), according to the data symmetry in (5), the optimal linear classifier has the form\n1, \u00b7 \u00b7 \u00b7 , 1, b\u03b3 = arg min w,b R\u03b3(f(\u00b7; w, b)).",
                "3This setting is also studied in Xu et al. (2021), which focuses on the robustness-fairness tradeoff, and thus is different\nto our interest.",
                "Theorem 1 (Extended from Theorem 2 in Xu et al. (2021)).",
                "Proof 5 in Xu et al. (2021) shows that db\u03b3 d\u03b3 \u2264 \u2212K\u22121K+1d < 0, thus b\u03b3 is strictly decreasing in \u03b3."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "19c8fee47f4a88fae01042f907d8030bf73f9bb0",
                "externalIds": {
                    "ArXiv": "2211.08942",
                    "DBLP": "journals/corr/abs-2211-08942",
                    "DOI": "10.48550/arXiv.2211.08942",
                    "CorpusId": 253553485
                },
                "corpusId": 253553485,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/19c8fee47f4a88fae01042f907d8030bf73f9bb0",
                "title": "Differentially Private Optimizers Can Learn Adversarially Robust Models",
                "abstract": "Machine learning models have shone in a variety of domains and attracted increasing attention from both the security and the privacy communities. One important yet worrying question is: will training models under the di\ufb00erential privacy (DP) constraint unfavorably impact on the adversarial robustness? While previous works have postulated that privacy comes at the cost of worse robustness, we give the \ufb01rst theoretical analysis to show that DP models can indeed be robust and accurate, even sometimes more robust than their naturally-trained non-private counterparts. We observe three key factors that in\ufb02uence the privacy-robustness-accuracy tradeo\ufb00: (1) hyperparameters for DP optimizers are critical; (2) pre-training on public data signi\ufb01cantly mitigates the accuracy and robustness drop; (3) choice of DP optimizers makes a di\ufb00erence. With these factors set properly, we achieve 90% natural accuracy, 72% robust accuracy (+9% than the non-private model) under l 2 (0 . 5) attack, and 69% robust accuracy (+16% than the non-private model) with pre-trained SimCLRv2 model under l \u221e (4 / 255) attack on CIFAR10 with (cid:15) = 2. In fact, we show both theoretically and empirically that DP models are Pareto optimal on the accuracy-robustness tradeo\ufb00. Empirically, the robustness of DP models is consistently observed on MNIST, Fashion MNIST and CelebA datasets, with ResNet and Vision Transformer. We believe our encouraging results are a signi\ufb01cant step towards training models that are private as well as robust.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49889831",
                        "name": "Yuan Zhang"
                    },
                    {
                        "authorId": "151267882",
                        "name": "Zhiqi Bu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Lastly, there start to be some research on the intersection of different trustworthy properties in machine learning [39, 114, 124, 227], it is worth studying on building health misinformation detectors that satisfy multiple trustworthy properties simultaneously."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d6e6b11a3658def372ed756457522c3bb762a8db",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-05289",
                    "ArXiv": "2211.05289",
                    "DOI": "10.48550/arXiv.2211.05289",
                    "CorpusId": 253447340
                },
                "corpusId": 253447340,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6e6b11a3658def372ed756457522c3bb762a8db",
                "title": "Combating Health Misinformation in Social Media: Characterization, Detection, Intervention, and Open Issues",
                "abstract": "Social media has been one of the main information consumption sources for the public, allowing people to seek and spread information more quickly and easily. However, the rise of various social media platforms also enables the proliferation of online misinformation. In particular, misinformation in the health domain has significant impacts on our society such as the COVID-19 infodemic. Therefore, health misinformation in social media has become an emerging research direction that attracts increasing attention from researchers of different disciplines. Compared to misinformation in other domains, the key differences of health misinformation include the potential of causing actual harm to humans' bodies and even lives, the hardness to identify for normal people, and the deep connection with medical science. In addition, health misinformation on social media has distinct characteristics from conventional channels such as television on multiple dimensions including the generation, dissemination, and consumption paradigms. Because of the uniqueness and importance of combating health misinformation in social media, we conduct this survey to further facilitate interdisciplinary research on this problem. In this survey, we present a comprehensive review of existing research about online health misinformation in different disciplines. Furthermore, we also systematically organize the related literature from three perspectives: characterization, detection, and intervention. Lastly, we conduct a deep discussion on the pressing open issues of combating health misinformation in social media and provide future directions for multidisciplinary researchers.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2163546329",
                        "name": "Canyu Chen"
                    },
                    {
                        "authorId": "2145335027",
                        "name": "Haoran Wang"
                    },
                    {
                        "authorId": "32057223",
                        "name": "Matthew A. Shapiro"
                    },
                    {
                        "authorId": "82554865",
                        "name": "Yunyu Xiao"
                    },
                    {
                        "authorId": "1682816",
                        "name": "Fei Wang"
                    },
                    {
                        "authorId": "145800151",
                        "name": "Kai Shu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although promising to improve the model\u2019s robustness, those adversarial training algorithms have been observed to result in a large disparity of accuracy and robustness among different classes while natural training does not present a similar issue [34]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6f9c78c61b7f6c850e1cd8a9fb5bcea75d90cd0e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-04449",
                    "ArXiv": "2211.04449",
                    "DOI": "10.48550/arXiv.2211.04449",
                    "CorpusId": 253397469
                },
                "corpusId": 253397469,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6f9c78c61b7f6c850e1cd8a9fb5bcea75d90cd0e",
                "title": "Fairness-aware Regression Robust to Adversarial Attacks",
                "abstract": "In this paper, we take a first step towards answering the question of how to design fair machine learning algorithms that are robust to adversarial attacks. Using a minimax framework, we aim to design an adversarially robust fair regression model that achieves optimal performance in the presence of an attacker who is able to add a carefully designed adversarial data point to the dataset or perform a rank-one attack on the dataset. By solving the proposed nonsmooth nonconvex-nonconcave minimax problem, the optimal adversary as well as the robust fairness-aware regression model are obtained. For both synthetic data and real-world datasets, numerical results illustrate that the proposed adversarially robust fair models have better performance on poisoned datasets than other fair machine learning models in both prediction accuracy and group-based fairness measure.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "122923863",
                        "name": "Yulu Jin"
                    },
                    {
                        "authorId": "2153755469",
                        "name": "Lifeng Lai"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2de86e17e80b1e9b45e6771c8890f42a7b772d30",
                "externalIds": {
                    "DOI": "10.1117/1.JEI.31.6.063046",
                    "CorpusId": 254668943
                },
                "corpusId": 254668943,
                "publicationVenue": {
                    "id": "c677ab24-0c04-487d-83e2-c252af9479c8",
                    "name": "Journal of Electronic Imaging (JEI)",
                    "type": "journal",
                    "alternate_names": [
                        "J Electron Imaging (JEI",
                        "Journal of Electronic Imaging",
                        "J Electron Imaging"
                    ],
                    "issn": "1017-9909",
                    "url": "https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging",
                    "alternate_urls": [
                        "http://electronicimaging.spiedigitallibrary.org/journal.aspx"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2de86e17e80b1e9b45e6771c8890f42a7b772d30",
                "title": "Toward deep neural networks robust to adversarial examples, using augmented data importance perception",
                "abstract": "Abstract. Deep neural networks (DNNs) have been shown to be susceptible to slightly perturbed adversarial examples. Even adversarial examples that are imperceptible to the human eye can easily lead to misclassification of DNNs. The most effective defense against adversarial examples is adversarial training (AT). Through AT, the robustness of the model against adversarial examples can be greatly improved. However, AT causes a decrease in the accuracy of the model\u2019s classification of natural examples and serious overfitting problems. To improve the generalization ability of the model, we propose a method for enhancing model robustness through augmented data importance perception (ADIP). By extracting offensive adversarial examples for data augmentation and considering the attack strength of new adversarial examples to design an importance measure term to improve the loss function, the results of experiments show that the model robustness enhancement method based on ADIP improves the robustness of the model and alleviates the overfitting problem of the model caused by AT. In addition, our algorithm trains a robust model with a small computational cost.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46842899",
                        "name": "Zhiming Chen"
                    },
                    {
                        "authorId": "2195745651",
                        "name": "Wei Xue"
                    },
                    {
                        "authorId": "2113787854",
                        "name": "Weiwei Tian"
                    },
                    {
                        "authorId": "47096747",
                        "name": "Yunhua Wu"
                    },
                    {
                        "authorId": "49805485",
                        "name": "Bing Hua"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Using adversarial training algorithms, bias that come with demographic attributes of the authors can be effectively mitigated and utilized for improved classification accuracy and robust feature prediction in the context of fairness [8], [9]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "689b15aeebf821e71a294dd6c6d39c2554e0b253",
                "externalIds": {
                    "DBLP": "conf/icdm/ZhaoW22",
                    "DOI": "10.1109/ICDMW58026.2022.00052",
                    "CorpusId": 256669125
                },
                "corpusId": 256669125,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/689b15aeebf821e71a294dd6c6d39c2554e0b253",
                "title": "Adversarial Removal of Population Bias in Genomics Phenotype Prediction",
                "abstract": "Many factors impact trait prediction from genotype data. One of the major confounding factors comes from the presence of population structure among sampled individuals, namely population stratification. When exists, it will lead to biased quantitative phenotype prediction, therefore hampering the unambiguous conclusions about prediction and limiting the downstream usage like disease evaluation or epidemiology survey. Population stratification is an implicit bias that can not be easily removed by data preprocessing. With the purpose of training a phenotype prediction model, we propose an adversarial training framework that ensures the genomics encoder is agnostic to sample populations. For better generalization, our adversarial training framework is orthogonal to the genomics encoder and phenotype prediction model. We experimentally ascertain our debiasing framework by testing on a real-world yield (phenotype) prediction dataset with soybean genomics. The developed frame-work is designed for general genomic data (e.g., human, livestock, and crops) while the phenotype can be either continuous or categorical variables.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2187998713",
                        "name": "Honggang Zhao"
                    },
                    {
                        "authorId": "2155510210",
                        "name": "Wenlu Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Xu et al. (Xu et al. 2021) empirically showed that even in balanced datasets, AT still suffers from fairness problem, where some classes have much higher performance than others.",
                "(Xu et al. 2021) empirically showed that even in balanced datasets, AT still suffers from fairness problem, where some classes have much higher performance than others."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cdff55a9ffadc668dd5abdfbdb35f0280e1447e9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-15068",
                    "ArXiv": "2210.15068",
                    "DOI": "10.48550/arXiv.2210.15068",
                    "CorpusId": 253157682
                },
                "corpusId": 253157682,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/cdff55a9ffadc668dd5abdfbdb35f0280e1447e9",
                "title": "Improving Adversarial Robustness with Self-Paced Hard-Class Pair Reweighting",
                "abstract": "Deep Neural Networks are vulnerable to adversarial attacks. Among many defense strategies, adversarial training with untargeted attacks is one of the most effective methods. Theoretically, adversarial perturbation in untargeted attacks can be added along arbitrary directions and the predicted labels of untargeted attacks should be unpredictable. However, we find that the naturally imbalanced inter-class semantic similarity makes those hard-class pairs become virtual targets of each other. This study investigates the impact of such closely-coupled classes on adversarial attacks and develops a self-paced reweighting strategy in adversarial training accordingly. Specifically, we propose to upweight hard-class pair losses in model optimization, which prompts learning discriminative features from hard classes. We further incorporate a term to quantify hard-class pair consistency in adversarial training, which greatly boosts model robustness. Extensive experiments show that the proposed adversarial training method achieves superior robustness performance over state-of-the-art defenses against a wide range of adversarial attacks. The code of the proposed SPAT is published at https://github.com/puerrrr/Self-Paced-Adversarial-Training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2067653277",
                        "name": "Peng-Fei Hou"
                    },
                    {
                        "authorId": "2180430584",
                        "name": "Jie Han"
                    },
                    {
                        "authorId": "2155446933",
                        "name": "Xingyu Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[252] hypothesized that adversarial training algorithms tend to introduce severe disparity in accuracy and robustness between different groups of data, and showed this phenomenon can happen under adversarial training algorithms minimizing"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6599397389c7670cd5bd98eb4472f0f40f00a48e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-08906",
                    "ArXiv": "2210.08906",
                    "DOI": "10.48550/arXiv.2210.08906",
                    "CorpusId": 252917583
                },
                "corpusId": 252917583,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6599397389c7670cd5bd98eb4472f0f40f00a48e",
                "title": "A.I. Robustness: a Human-Centered Perspective on Technological Challenges and Opportunities",
                "abstract": "Despite the impressive performance of Artificial Intelligence (AI) systems, their robustness remains elusive and constitutes a key issue that impedes large-scale adoption. Robustness has been studied in many domains of AI, yet with different interpretations across domains and contexts. In this work, we systematically survey the recent progress to provide a reconciled terminology of concepts around AI robustness. We introduce three taxonomies to organize and describe the literature both from a fundamental and applied point of view: 1) robustness by methods and approaches in different phases of the machine learning pipeline; 2) robustness for specific model architectures, tasks, and systems; and in addition, 3) robustness assessment methodologies and insights, particularly the trade-offs with other trustworthiness properties. Finally, we identify and discuss research gaps and opportunities and give an outlook on the field. We highlight the central role of humans in evaluating and enhancing AI robustness, considering the necessary knowledge humans can provide, and discuss the need for better understanding practices and developing supportive tools in the future.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "71223212",
                        "name": "Andrea Tocchetti"
                    },
                    {
                        "authorId": "81397590",
                        "name": "L. Corti"
                    },
                    {
                        "authorId": "9572457",
                        "name": "Agathe Balayn"
                    },
                    {
                        "authorId": "2115474673",
                        "name": "Mireia Yurrita"
                    },
                    {
                        "authorId": "2187932096",
                        "name": "Philip Lippmann"
                    },
                    {
                        "authorId": "40350773",
                        "name": "Marco Brambilla"
                    },
                    {
                        "authorId": "1688428",
                        "name": "Jie Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some discussed the bias between classes in adversarial training and proposed a training framework to mitigate this issue [44]; Others analyzed differences in robustness to adversarial samples between sensitive groups and developed a simple regularization method to address the problem [25].",
                "As background, using a validation set is commonly accepted in mainstream fairness studies [1, 34, 44], and identifying the least favorable group is possible in safety-critical applications such as facial recognition systems [6].",
                "First, because performance disparities between classes are amplified under adversarial training [43, 44], existing methods [1, 34] may perform poorly on low-frequency classes of disadvantaged groups, resulting in poor fair-performance and fair-robustness.",
                "Our empirical studies on three tasks show that the group with the least standard performance is of the worst adversarial robustness, consistent with prior studies on other datasets [25,44].",
                "Moreover, under adversarial training, robust models generally exhibit larger performance disparities between classes given adversarial inputs than benign inputs [43, 44].",
                "Specifically, on benign data, disparities between classes and predicted errors increase from standard to robust models [37, 43, 44].",
                "of ML models would sacrifice performance on benign data, and [43, 44] observe that the issue of class-imbalanced performance on benign data becomes more severe under adversarial training.",
                "Similarly, for the robust model, incorrect outputs and class imbalance performance grow from benign to adversarial inputs [9, 43, 44, 46]."
            ],
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ff96895f7f03cab09733d5940769ca601d2d5289",
                "externalIds": {
                    "ArXiv": "2209.10729",
                    "DBLP": "journals/corr/abs-2209-10729",
                    "DOI": "10.48550/arXiv.2209.10729",
                    "CorpusId": 252439030
                },
                "corpusId": 252439030,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ff96895f7f03cab09733d5940769ca601d2d5289",
                "title": "Fair Robust Active Learning by Joint Inconsistency",
                "abstract": "Fairness and robustness play vital roles in trustworthy machine learning. Observing safety-critical needs in various annotation-expensive vision applications, we introduce a novel learning framework, Fair Robust Active Learning (FRAL), generalizing conventional active learning to fair and adversarial robust scenarios. This framework allows us to achieve standard and robust minimax fairness with limited acquired labels. In FRAL, we then observe existing fairness-aware data selection strategies suffer from either ineffectiveness under severe data imbalance or inefficiency due to huge computations of adversarial training. To address these two problems, we develop a novel Joint INconsistency (JIN) method exploiting prediction inconsistencies between benign and adversarial inputs as well as between standard and robust models. These two inconsistencies can be used to identify potential fairness gains and data imbalance mitigations. Thus, by performing label acquisition with our inconsistency-based ranking metrics, we can alleviate the class imbalance issue and enhance minimax fairness with limited computation. Extensive experiments on diverse datasets and sensitive groups demonstrate that our method obtains the best results in standard and robust fairness under white-box PGD attacks compared with existing active data selection baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2142343820",
                        "name": "Tsung-Han Wu"
                    },
                    {
                        "authorId": "1792665",
                        "name": "Shang-Tse Chen"
                    },
                    {
                        "authorId": "1716836",
                        "name": "Winston H. Hsu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Previous research on trustworthy AI [236] shows that the robustness of such systems is positively correlated to their explainability [96, 270], while partly conflicts with their privacy [325] and fairness dimensions [399].",
                "Despite that a number of studies have investigated the interactions between dimensions of trustworthy AI [101, 236, 399], research on trustworthy recommender systems is still limited."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d20773683ddbb5c516131bdfdf674a772714de50",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-10117",
                    "ArXiv": "2209.10117",
                    "DOI": "10.48550/arXiv.2209.10117",
                    "CorpusId": 252407646
                },
                "corpusId": 252407646,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d20773683ddbb5c516131bdfdf674a772714de50",
                "title": "A Comprehensive Survey on Trustworthy Recommender Systems",
                "abstract": "As one of the most successful AI-powered applications, recommender systems aim to help people make appropriate decisions in an effective and efficient way, by providing personalized suggestions in many aspects of our lives, especially for various human-oriented online services such as e-commerce platforms and social media sites. In the past few decades, the rapid developments of recommender systems have significantly benefited human by creating economic value, saving time and effort, and promoting social good. However, recent studies have found that data-driven recommender systems can pose serious threats to users and society, such as spreading fake news to manipulate public opinion in social media sites, amplifying unfairness toward under-represented groups or individuals in job matching services, or inferring privacy information from recommendation results. Therefore, systems' trustworthiness has been attracting increasing attention from various aspects for mitigating negative impacts caused by recommender systems, so as to enhance the public's trust towards recommender systems techniques. In this survey, we provide a comprehensive overview of Trustworthy Recommender systems (TRec) with a specific focus on six of the most important aspects; namely, Safety&Robustness, Nondiscrimination&Fairness, Explainability, Privacy, Environmental Well-being, and Accountability&Auditability. For each aspect, we summarize the recent related technologies and discuss potential research directions to help achieve trustworthy recommender systems in the future.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "41031455",
                        "name": "Wenqi Fan"
                    },
                    {
                        "authorId": "2116710405",
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "authorId": "2157320978",
                        "name": "Xiao Chen"
                    },
                    {
                        "authorId": "2116965668",
                        "name": "Jingran Su"
                    },
                    {
                        "authorId": "2161309826",
                        "name": "Jingtong Gao"
                    },
                    {
                        "authorId": "2144737589",
                        "name": "Lin Wang"
                    },
                    {
                        "authorId": "2112246463",
                        "name": "Qidong Liu"
                    },
                    {
                        "authorId": "2108942088",
                        "name": "Yiqi Wang"
                    },
                    {
                        "authorId": "2116309628",
                        "name": "Hanfeng Xu"
                    },
                    {
                        "authorId": "2146071731",
                        "name": "Lei Chen"
                    },
                    {
                        "authorId": "2117897052",
                        "name": "Qing Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This includes, Generative Adversarial Networks (or GANS) [5, 10], to robust machine learning of different kinds [8, 12, 27, 31]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f0885659be9fd04c4a2f919a2f527f64919bbb5c",
                "externalIds": {
                    "DBLP": "conf/recsys/ShivaswamyG22",
                    "DOI": "10.1145/3523227.3546784",
                    "CorpusId": 252216595
                },
                "corpusId": 252216595,
                "publicationVenue": {
                    "id": "61275a16-1e0d-479f-ac4e-f295310761f0",
                    "name": "ACM Conference on Recommender Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Recomm Syst",
                        "RecSys",
                        "ACM Conf Recomm Syst",
                        "Conference on Recommender Systems"
                    ],
                    "url": "http://recsys.acm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/f0885659be9fd04c4a2f919a2f527f64919bbb5c",
                "title": "Adversary or Friend? An adversarial Approach to Improving Recommender Systems",
                "abstract": "Typical recommender systems models are trained to have good average performance across all users or items. In practice, this results in model performance that is good for some users but sub-optimal for many users. In this work, we consider adversarially trained machine learning models and extend them to recommender systems problems. The adversarial models are trained with no additional demographic or other information than already available to the learning algorithm. We show that adversarially reweighted learning models give more emphasis to dense areas of the feature-space that incur high loss during training. We show that a straightforward adversarial model adapted to recommender systems can fail to perform well and that a carefully designed adversarial model can perform much better. The proposed models are trained using a standard gradient descent/ascent approach that can be easily adapted to many recommender problems. We compare our results with an inverse propensity weighting based baseline that also works well in practice. We delve deep into the underlying experimental results and show that, for the users who are under-served by the baseline model, the adversarial models can achieve significantly better results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2248631",
                        "name": "Pannagadatta K. Shivaswamy"
                    },
                    {
                        "authorId": "1401736799",
                        "name": "Dario Garc\u00eda-Garc\u00eda"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use the published codes for TRADES (Zhang et al. 2019)3, FRL (Xu et al. 2021)4.",
                "For fair comparisons, we follow (Xu et al. 2021) and use the average and worst-class error rate of\n2We use the official data https://github.com/fastai/imagenette 3https://github.com/yaodongyu/TRADES 4https://github.com/hannxu123/fair robust\nstandard (Avg.",
                "However, adversarial training suffers from the robust fairness problem, where the adversarially trained models make a severe disparity in accuracy and robustness among different classes (Xu et al. 2021).",
                "In this paper, we follow (Xu et al. 2021) and use PGD attacks regarding cross entropy loss with 20 steps and step size of 2/255 to evaluate the robust fairness in our main experiment.",
                "For fair comparisons, we follow (Xu et al. 2021) and use the average and worst-class error rate of",
                "This phenomenon is firstly defined by (Xu et al. 2021) and further theoretically justified by studying a binary classification task under a Gaussian mixture distribution.",
                "We compare the previously proposed method FRL (Xu et al. 2021) which is the only method that address robust fairness problem to the best of our knowledge."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a1b66604fc3464ad4041480ac93a3a40f3fff71e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-07534",
                    "ArXiv": "2209.07534",
                    "DOI": "10.48550/arXiv.2209.07534",
                    "CorpusId": 252355018
                },
                "corpusId": 252355018,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a1b66604fc3464ad4041480ac93a3a40f3fff71e",
                "title": "Improving Robust Fairness via Balance Adversarial Training",
                "abstract": "Adversarial training (AT) methods are effective against adversarial attacks, yet they introduce severe disparity of accuracy and robustness between different classes, known as the robust fairness problem. Previously proposed Fair Robust Learning (FRL) adaptively reweights different classes to improve fairness. However, the performance of the better-performed classes decreases, leading to a strong performance drop. In this paper, we observed two unfair phenomena during adversarial training: different difficulties in generating adversarial examples from each class (source-class fairness) and disparate target class tendencies when generating adversarial examples (target-class fairness). From the observations, we propose Balance Adversarial Training (BAT) to address the robust fairness problem. Regarding source-class fairness, we adjust the attack strength and difficulties of each class to generate samples near the decision boundary for easier and fairer model learning; considering target-class fairness, by introducing a uniform distribution constraint, we encourage the adversarial example generation process for each class with a fair tendency. Extensive experiments conducted on multiple datasets (CIFAR-10, CIFAR-100, and ImageNette) demonstrate that our method can significantly outperform other baselines in mitigating the robust fairness problem (+5-10\\% on the worst class accuracy)",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2185396018",
                        "name": "Chunyu Sun"
                    },
                    {
                        "authorId": "2167458315",
                        "name": "Chenye Xu"
                    },
                    {
                        "authorId": "2093201471",
                        "name": "Chengyuan Yao"
                    },
                    {
                        "authorId": "2114786732",
                        "name": "Siyuan Liang"
                    },
                    {
                        "authorId": "2107976307",
                        "name": "Yichao Wu"
                    },
                    {
                        "authorId": "152335674",
                        "name": "Ding Liang"
                    },
                    {
                        "authorId": "6820648",
                        "name": "Xianglong Liu"
                    },
                    {
                        "authorId": "153152072",
                        "name": "Aishan Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The class \u201c+1\u201d is harder because an optimal linear classifier will give a larger error for the class \u201c+1\u201d than that for the class \u201c1\u201d when \u03c3(2) + > \u03c3 2 \u2212 [31]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "58269236081c18d575df688e39be4bb66cd00695",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-05668",
                    "ArXiv": "2209.05668",
                    "DOI": "10.48550/arXiv.2209.05668",
                    "CorpusId": 252211777,
                    "PubMed": "37220052"
                },
                "corpusId": 252211777,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/58269236081c18d575df688e39be4bb66cd00695",
                "title": "Class-Level Logit Perturbation",
                "abstract": "Features, logits, and labels are the three primary data when a sample passes through a deep neural network (DNN). Feature perturbation and label perturbation receive increasing attention in recent years. They have been proven to be useful in various deep learning approaches. For example, (adversarial) feature perturbation can improve the robustness or even generalization capability of learned models. However, limited studies have explicitly explored for the perturbation of logit vectors. This work discusses several existing methods related to class-level logit perturbation. A unified viewpoint between regular/irregular data augmentation and loss variations incurred by logit perturbation is established. A theoretical analysis is provided to illuminate why class-level logit perturbation is useful. Accordingly, new methodologies are proposed to explicitly learn to perturb logits for both the single-label and multilabel classification tasks. Meta-learning is also leveraged to determine the regular or irregular augmentation for each class. Extensive experiments on benchmark image classification datasets and their long-tail versions indicated the competitive performance of our learning method. As it only perturbs on logit, it can be used as a plug-in to fuse with any existing classification algorithms. All the codes are available at https://github.com/limengyang1992/lpl.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2175308149",
                        "name": "Mengyang Li"
                    },
                    {
                        "authorId": "2174907569",
                        "name": "Fengguang Su"
                    },
                    {
                        "authorId": "2174907958",
                        "name": "O. Wu"
                    },
                    {
                        "authorId": "2184722593",
                        "name": "Ji Zhang National Center for Applied Mathematics"
                    },
                    {
                        "authorId": "2184722484",
                        "name": "Tianjin University"
                    },
                    {
                        "authorId": "2184722306",
                        "name": "University of Southern Queensland"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It\u2019s found in [24] that adversarial training algorithms often derive serious unbalance in accuracy and robustness between different categories, and they propose Fair-Robust-Learning (FRL) framework to mitigate this unfairness problem."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1a4df400b525ef3eacfee0df3506b09aa42f1b4b",
                "externalIds": {
                    "DBLP": "journals/apin/YuWY23",
                    "DOI": "10.1007/s10489-022-03480-w",
                    "CorpusId": 252151901
                },
                "corpusId": 252151901,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1a4df400b525ef3eacfee0df3506b09aa42f1b4b",
                "title": "Global Wasserstein Margin maximization for boosting generalization in adversarial training",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2184456985",
                        "name": "Tingyue Yu"
                    },
                    {
                        "authorId": "2151226391",
                        "name": "Shen Wang"
                    },
                    {
                        "authorId": "91527102",
                        "name": "Xiangzhan Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[52] identify inherent bias amplification as a result of adversarial training and propose a framework to mitigate these biases."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "086a47fe51a9f3c3cfd07e16c39a6a1ae73e6d22",
                "externalIds": {
                    "ArXiv": "2209.00746",
                    "DBLP": "conf/eccv/ChariBAK22",
                    "DOI": "10.48550/arXiv.2209.00746",
                    "CorpusId": 252070466
                },
                "corpusId": 252070466,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/086a47fe51a9f3c3cfd07e16c39a6a1ae73e6d22",
                "title": "MIME: Minority Inclusion for Majority Group Enhancement of AI Performance",
                "abstract": "Several papers have rightly included minority groups in artificial intelligence (AI) training data to improve test inference for minority groups and/or society-at-large. A society-at-large consists of both minority and majority stakeholders. A common misconception is that minority inclusion does not increase performance for majority groups alone. In this paper, we make the surprising finding that including minority samples can improve test error for the majority group. In other words, minority group inclusion leads to majority group enhancements (MIME) in performance. A theoretical existence proof of the MIME effect is presented and found to be consistent with experimental results on six different datasets. Project webpage: https://visual.ee.ucla.edu/mime.htm/",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1432234369",
                        "name": "Pradyumna Chari"
                    },
                    {
                        "authorId": "88727401",
                        "name": "Yunhao Ba"
                    },
                    {
                        "authorId": "2184030735",
                        "name": "Shreeram S. Athreya"
                    },
                    {
                        "authorId": "2425405",
                        "name": "A. Kadambi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Thirdly, robust optimization, this type of defensive techniques aims to eliminate the existence of adversarial examples in the first place by training a robust DL model [8] [9] [18] [22] [25]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c040c20a7e0ca9afcfc1727a211330de0b2e7962",
                "externalIds": {
                    "DOI": "10.1109/BigDIA56350.2022.9874139",
                    "CorpusId": 252165946
                },
                "corpusId": 252165946,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c040c20a7e0ca9afcfc1727a211330de0b2e7962",
                "title": "RegWeight: Adversarial Training based on Dynamically-Adjusted Regularization Weights",
                "abstract": "The recent studies have evidenced that, for a target deep learning model, the accuracy of classification on adversarial examples will tend to decline while the performance on natural (original) examples becomes better. We find out that the behind reasons of such problem stems from the overfitting phenomenon. To mitigate this predicament, we in this paper propose a regularization-weight learning method RegWeight to dynamically adjust the classification boundaries over different classes. A set of experiments using two commonly-used datasets also validate the effectiveness of our proposed method, the experimental results show that our proposed RegWeight can validly promote the accuracy of classification on adversarial examples while at the same time retaining a little decreasing-level on the accuracy of classification on natural examples.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "92353131",
                        "name": "Yunfeng Lu"
                    },
                    {
                        "authorId": "2184499695",
                        "name": "Samuel Thien"
                    },
                    {
                        "authorId": "2148432293",
                        "name": "Xinxin Fan"
                    },
                    {
                        "authorId": "2108356711",
                        "name": "Yanfang Liu"
                    },
                    {
                        "authorId": "48265485",
                        "name": "B. Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "PGD/IFGSM/BIM [13,17,28\u201331,33,39\u201370] Multi-step gradient-based white-box attack High precision attack; provides more generalization than the FGSM; uses random initialization to avoid local minima Higher computational complexity; may also have an overfitting problem to some extent CNN: [13,33,41,47,49,56,64] AllCNN: [59] LeNet: [31,43,48,50,51,59,67] VGG: [44,65] ResNet: [29,30,39,42,46,48,49,51,57,60,63,66,67,70] WideResNet: [13,28,30,42\u201347,49,50,52\u201355,58,63,64,66,68,70] PreActResNet: [28,56,68] RevNet: [48] Inception: [17,48,69] Inception ResNet: [17] DenseNet: [39] IPMI2019-AttnMel: [69] CheXNet: [69] Transferred VGGFace: [61] LISA-CNN: [61] GANs: [62]"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5410ab50a5501d191b6ac02aab33cea303286c27",
                "externalIds": {
                    "DBLP": "journals/algorithms/ZhaoAM22",
                    "DOI": "10.3390/a15080283",
                    "CorpusId": 251610175
                },
                "corpusId": 251610175,
                "publicationVenue": {
                    "id": "e95c8d18-09be-464f-a3cf-5b2637f0eff6",
                    "name": "Algorithms",
                    "type": "journal",
                    "issn": "1999-4893",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-150910",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-150910",
                        "http://www.mdpi.com/journal/algorithms",
                        "http://www.mdpi.com/journal/algorithms/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5410ab50a5501d191b6ac02aab33cea303286c27",
                "title": "Adversarial Training Methods for Deep Learning: A Systematic Review",
                "abstract": "Deep neural networks are exposed to the risk of adversarial attacks via the fast gradient sign method (FGSM), projected gradient descent (PGD) attacks, and other attack algorithms. Adversarial training is one of the methods used to defend against the threat of adversarial attacks. It is a training schema that utilizes an alternative objective function to provide model generalization for both adversarial data and clean data. In this systematic review, we focus particularly on adversarial training as a method of improving the defensive capacities and robustness of machine learning models. Specifically, we focus on adversarial sample accessibility through adversarial sample generation methods. The purpose of this systematic review is to survey state-of-the-art adversarial training and robust optimization methods to identify the research gaps within this field of applications. The literature search was conducted using Engineering Village (Engineering Village is an engineering literature search tool, which provides access to 14 engineering literature and patent databases), where we collected 238 related papers. The papers were filtered according to defined inclusion and exclusion criteria, and information was extracted from these papers according to a defined strategy. A total of 78 papers published between 2016 and 2021 were selected. Data were extracted and categorized using a defined strategy, and bar plots and comparison tables were used to show the data distribution. The findings of this review indicate that there are limitations to adversarial training methods and robust optimization. The most common problems are related to data generalization and overfitting.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2181790192",
                        "name": "Weimin Zhao"
                    },
                    {
                        "authorId": "2558873",
                        "name": "Sanaa A. Alwidian"
                    },
                    {
                        "authorId": "1718287",
                        "name": "Q. Mahmoud"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We see developing robust mechanisms for fairness-aware algorithms as a crucial step towards fighting bias (Xu et al, 2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4a1a64c91c58d414bf207bca67aa8d9792680950",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-06084",
                    "ArXiv": "2207.06084",
                    "DOI": "10.48550/arXiv.2207.06084",
                    "CorpusId": 250491317
                },
                "corpusId": 250491317,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4a1a64c91c58d414bf207bca67aa8d9792680950",
                "title": "Towards A Holistic View of Bias in Machine Learning: Bridging Algorithmic Fairness and Imbalanced Learning",
                "abstract": "Machine learning (ML) is playing an increasingly important role in rendering decisions that affect a broad range of groups in society. ML models inform decisions in criminal justice, the extension of credit in banking, and the hiring practices of corporations. This posits the requirement of model fairness, which holds that automated decisions should be equitable with respect to protected features (e.g., gender, race, or age) that are often under-represented in the data. We postulate that this problem of under-representation has a corollary to the problem of imbalanced data learning. This class imbalance is often reflected in both classes and protected features. For example, one class (those receiving credit) may be over-represented with respect to another class (those not receiving credit) and a particular group (females) may be under-represented with respect to another group (males). A key element in achieving algorithmic fairness with respect to protected groups is the simultaneous reduction of class and protected group imbalance in the underlying training data, which facilitates increases in both model accuracy and fairness. We discuss the importance of bridging imbalanced learning and group fairness by showing how key concepts in these fields overlap and complement each other; and propose a novel oversampling algorithm, Fair Oversampling, that addresses both skewed class distributions and protected features. Our method: (i) can be used as an efficient pre-processing algorithm for standard ML algorithms to jointly address imbalance and group equity; and (ii) can be combined with fairness-aware learning algorithms to improve their robustness to varying levels of class imbalance. Additionally, we take a step toward bridging the gap between fairness and imbalanced learning with a new metric, Fair Utility, that combines balanced accuracy with fairness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2088594752",
                        "name": "Damien A. Dablain"
                    },
                    {
                        "authorId": "3022672",
                        "name": "B. Krawczyk"
                    },
                    {
                        "authorId": "144539424",
                        "name": "N. Chawla"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The authors in [46] showed that adversarial learning might worsen classification accuracy and fairness performance."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7f67c26c25dd266ff9158b6524667b5ca42437c8",
                "externalIds": {
                    "DBLP": "conf/accv/HanelKSLUEG22",
                    "ArXiv": "2207.05727",
                    "DOI": "10.48550/arXiv.2207.05727",
                    "CorpusId": 250451227
                },
                "corpusId": 250451227,
                "publicationVenue": {
                    "id": "a8f26d13-e373-4e48-b57b-ef89bf48f4db",
                    "name": "Asian Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Asian Conf Comput Vis",
                        "ACCV"
                    ],
                    "url": "http://www.cvl.iis.u-tokyo.ac.jp/afcv/"
                },
                "url": "https://www.semanticscholar.org/paper/7f67c26c25dd266ff9158b6524667b5ca42437c8",
                "title": "Enhancing Fairness of Visual Attribute Predictors",
                "abstract": "The performance of deep neural networks for image recognition tasks such as predicting a smiling face is known to degrade with under-represented classes of sensitive attributes. We address this problem by introducing fairness-aware regularization losses based on batch estimates of Demographic Parity, Equalized Odds, and a novel Intersection-over-Union measure. The experiments performed on facial and medical images from CelebA, UTKFace, and the SIIM-ISIC melanoma classification challenge show the effectiveness of our proposed fairness losses for bias mitigation as they improve model fairness while maintaining high classification performance. To the best of our knowledge, our work is the first attempt to incorporate these types of losses in an end-to-end training scheme for mitigating biases of visual attribute predictors. Our code is available at https://github.com/nish03/FVAP.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2175779596",
                        "name": "Tobias Hanel"
                    },
                    {
                        "authorId": "2142335267",
                        "name": "Nishant Kumar"
                    },
                    {
                        "authorId": "34033578",
                        "name": "D. Schlesinger"
                    },
                    {
                        "authorId": "31289209",
                        "name": "Meng Li"
                    },
                    {
                        "authorId": "48789179",
                        "name": "Erdem Unal"
                    },
                    {
                        "authorId": "38530236",
                        "name": "A. Eslami"
                    },
                    {
                        "authorId": "2952883",
                        "name": "S. Gumhold"
                    }
                ]
            }
        },
        {
            "contexts": [
                "uk examples learn fundamentally different representations compared to standard classifiers reducing accuracy [16]; they also can cause disparity on accuracy between classes for both clean and adversarial samples [18]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4b4ebdb52b73cd84d7656eb5f4b89ec8cff7ce37",
                "externalIds": {
                    "DBLP": "conf/embc/QendroM22",
                    "DOI": "10.1109/EMBC48229.2022.9871347",
                    "CorpusId": 249305627,
                    "PubMed": "36086386"
                },
                "corpusId": 249305627,
                "publicationVenue": {
                    "id": "d7d7587f-a1e9-46a4-9db5-54c95787c6f1",
                    "name": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf IEEE Eng Med Biology Soc",
                        "International Conference of the IEEE Engineering in Medicine and Biology Society",
                        "EMBC",
                        "Annu Int Conf IEEE Eng Med Biology Soc"
                    ],
                    "issn": "2529-7856",
                    "alternate_issns": [
                        "2375-7477"
                    ],
                    "url": "https://www.inscribepublications.com/journal/josa",
                    "alternate_urls": [
                        "http://www.sesarju.eu/sesarinnovationdays",
                        "https://embc.embs.org/2018/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4b4ebdb52b73cd84d7656eb5f4b89ec8cff7ce37",
                "title": "Towards Adversarial Robustness with Early Exit Ensembles",
                "abstract": "Deep learning techniques are increasingly used for decision-making in health applications, however, these can easily be manipulated by adversarial examples across different clinical domains. Their security and privacy vulnerabilities raise concerns about the practical deployment of these systems. The number and variety of the adversarial attacks grow continuously, making it difficult for mitigation approaches to provide effective solutions. Current mitigation techniques often rely on expensive re-training procedures as new attacks emerge. In this paper, we propose a novel adversarial mitigation technique for biosignal classification tasks. Our approach is based on recent findings interpreting early exit neural networks as an ensemble of weight sharing sub-networks. Our experiments on state-of-the-art deep learning models show that early exit ensembles can provide robustness generalizable to various white box and universal adversarial attacks. The approach increases the accuracy of vulnerable deep learning models up to 60 percentage points, while providing adversarial mitigation comparable to adversarial training. This is achieved without previous exposure to the adversarial perturbation or the computational burden of re-training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1994899",
                        "name": "Lorena Qendro"
                    },
                    {
                        "authorId": "2144247754",
                        "name": "Cecilia Mascolo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In turn, Xu et al. (2021) proposes the Fair-Robust-Learning (FRL) algorithm to alleviate this problem.",
                "Xu et al. (2021) reports that TRADES (Zhang et al., 2019) increases the variation of the per-class accuracies (accuracy in each class) which is not desirable in view of fairness."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "83285aa1a1f2d88c57b4f2d9fa970e2fb1bd0c55",
                "externalIds": {
                    "DBLP": "conf/icml/YangKK23",
                    "ArXiv": "2206.03353",
                    "CorpusId": 258426811
                },
                "corpusId": 258426811,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/83285aa1a1f2d88c57b4f2d9fa970e2fb1bd0c55",
                "title": "Improving Adversarial Robustness by Putting More Regularizations on Less Robust Samples",
                "abstract": "Adversarial training, which is to enhance robustness against adversarial attacks, has received much attention because it is easy to generate human-imperceptible perturbations of data to deceive a given deep neural network. In this paper, we propose a new adversarial training algorithm that is theoretically well motivated and empirically superior to other existing algorithms. A novel feature of the proposed algorithm is to apply more regularization to data vulnerable to adversarial attacks than other existing regularization algorithms do. Theoretically, we show that our algorithm can be understood as an algorithm of minimizing the regularized empirical risk motivated from a newly derived upper bound of the robust risk. Numerical experiments illustrate that our proposed algorithm improves the generalization (accuracy on examples) and robustness (accuracy on adversarial attacks) simultaneously to achieve the state-of-the-art performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2167315030",
                        "name": "Dong-Sheng Yang"
                    },
                    {
                        "authorId": "2153469078",
                        "name": "Insung Kong"
                    },
                    {
                        "authorId": "1763945",
                        "name": "Yongdai Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "At present, considerable efforts had been developed to improve the robustness of the DNN-implemented classifier against adversarial examples, which can be categorized as adversarial training (AT) [41, 46, 64, 73, 74, 70, 66, 72], randomization [19, 65, 12, 17, 68, 38, 58, 7] and input purification [53, 51].",
                ", Fast Adversarial Training (Fast-AT) [64], You Only Propagate Once (YOPO) [70], Adversarial Training with Hypersphere Embedding (ATHE) [46], Fair Robust Learning (FRL) [66], Friendly Adversarial Training (FAT) [73], TRADES [72] and Adversarial Training with Transferable Adversarial examples (ATTA) [74].",
                "The Fair-Robust-Learning (FRL) [66] mitigated the unfairness problem that the accuracy of some categories is much lower than the average accuracy of the DNN model.",
                "Seven adversarial training methods are evaluated, i.e., Fast Adversarial Training (Fast-AT) [64], You Only Propagate Once (YOPO) [70], Adversarial Training with Hypersphere Embedding (ATHE) [46], Fair Robust Learning (FRL) [66], Friendly Adversarial Training (FAT) [73], TRADES [72] and Adversarial Training with Transferable Adversarial examples (ATTA) [74].",
                "Several methods have been further developed to accelerate adversarial training [64, 70] and mitigate low efficiency [46, 73, 74], low generalization [72] and unfairness [66]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "47c61f82403a3d5aacc289133d4044f923926cef",
                "externalIds": {
                    "ArXiv": "2206.00924",
                    "CorpusId": 257913284
                },
                "corpusId": 257913284,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/47c61f82403a3d5aacc289133d4044f923926cef",
                "title": "FACM: Intermediate Layer Still Retain Effective Features against Adversarial Examples",
                "abstract": "In strong adversarial attacks against deep neural networks (DNN), the generated adversarial example will mislead the DNN-implemented classifier by destroying the output features of the last layer. To enhance the robustness of the classifier, in our paper, a \\textbf{F}eature \\textbf{A}nalysis and \\textbf{C}onditional \\textbf{M}atching prediction distribution (FACM) model is proposed to utilize the features of intermediate layers to correct the classification. Specifically, we first prove that the intermediate layers of the classifier can still retain effective features for the original category, which is defined as the correction property in our paper. According to this, we propose the FACM model consisting of \\textbf{F}eature \\textbf{A}nalysis (FA) correction module, \\textbf{C}onditional \\textbf{M}atching \\textbf{P}rediction \\textbf{D}istribution (CMPD) correction module and decision module. The FA correction module is the fully connected layers constructed with the output of the intermediate layers as the input to correct the classification of the classifier. The CMPD correction module is a conditional auto-encoder, which can not only use the output of intermediate layers as the condition to accelerate convergence but also mitigate the negative effect of adversarial example training with the Kullback-Leibler loss to match prediction distribution. Through the empirically verified diversity property, the correction modules can be implemented synergistically to reduce the adversarial subspace. Hence, the decision module is proposed to integrate the correction modules to enhance the DNN classifier's robustness. Specially, our model can be achieved by fine-tuning and can be combined with other model-specific defenses.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2165758695",
                        "name": "Xiangyuan Yang"
                    },
                    {
                        "authorId": "66190968",
                        "name": "Jie Lin"
                    },
                    {
                        "authorId": "120811666",
                        "name": "Han Zhang"
                    },
                    {
                        "authorId": "2595119",
                        "name": "Xinyu Yang"
                    },
                    {
                        "authorId": "2087254926",
                        "name": "Peng Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, both [10], [11] aim to realize robust fairness (i.",
                "[11] observe that robustness can impact fairness - the adversarial training algorithms tend to introduce disparity of accuracy and robustness between different groups of data.",
                "However, due to the tension between fairness and robustness [10], [11], the sequential methods (either robustness-then-fairness or fairness-thenrobustness) fail, as the fair model will become unfair after adversarial training.",
                "For example, recent works [10], [11] have observed that equipping the ML models with fairness can make these models to be more susceptible"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "fa769e1acd34979c02485a0ce5e4fb857c6c7d95",
                "externalIds": {
                    "DBLP": "conf/eurosp/SunWWW22",
                    "DOI": "10.1109/EuroSP53844.2022.00030",
                    "CorpusId": 250012106
                },
                "corpusId": 250012106,
                "publicationVenue": {
                    "id": "4c2b8cb8-e51c-4ece-9122-89595989b56f",
                    "name": "European Symposium on Security and Privacy",
                    "type": "conference",
                    "alternate_names": [
                        "EuroS&P",
                        "IEEE European Symposium on Security and Privacy",
                        "Eur Symp Secur Priv",
                        "IEEE Eur Symp Secur Priv",
                        "EUROS&P"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fa769e1acd34979c02485a0ce5e4fb857c6c7d95",
                "title": "Towards Fair and Robust Classification",
                "abstract": "Robustness and fairness are two equally important issues for machine learning systems. Despite the active research on robustness and fairness of ML recently, these efforts focus on either fairness or robustness, but not both. To bridge this gap, in this paper, we design Fair and Robust Classification (FRoC) models that equip the classification models with both fairness and robustness. Meeting both fairness and robustness constraints is not trivial due to the tension between them. The trade-off between fairness, robustness, and model accuracy also introduces additional challenge. To address these challenges, we design two FRoC methods, namely FRoC-PRE that modifies the input data before model training, and FRoC-IN that modifies the model with an adversarial objective function to address both fairness and robustness during training. FRoC-IN is suitable to the settings where the users (e.g., ML service providers) only have the access to the model but not the original data, while FRoC-PRE works for the settings where the users (e.g., data owners) have the access to both data and a surrogate model that may have similar architecture as the target model. Our extensive experiments on real-world datasets demonstrate that both FRoC-IN and FRoC-PRE can achieve both fairness and robustness with insignificant accuracy loss of the target model.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2647666",
                        "name": "Haipei Sun"
                    },
                    {
                        "authorId": "2112562708",
                        "name": "Kun Wu"
                    },
                    {
                        "authorId": "2155392029",
                        "name": "Ting Wang"
                    },
                    {
                        "authorId": "2108465033",
                        "name": "Wendy Hui Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Specifically (Xu et al., 2021; Nanda et al., 2021; Yurochkin & Sun, 2020) propose adversarial training-based algorithms for fairness."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a2a6a164f7b15c6996eb868e0ac609ecf87b11e9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-00553",
                    "ArXiv": "2206.00553",
                    "DOI": "10.48550/arXiv.2206.00553",
                    "CorpusId": 249240643
                },
                "corpusId": 249240643,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a2a6a164f7b15c6996eb868e0ac609ecf87b11e9",
                "title": "FETA: Fairness Enforced Verifying, Training, and Predicting Algorithms for Neural Networks",
                "abstract": "Algorithmic decision making driven by neural networks has become very prominent in applications that directly affect people's quality of life. In this paper, we study the problem of verifying, training, and guaranteeing individual fairness of neural network models. A popular approach for enforcing fairness is to translate a fairness notion into constraints over the parameters of the model. However, such a translation does not always guarantee fair predictions of the trained neural network model. To address this challenge, we develop a counterexample-guided post-processing technique to provably enforce fairness constraints at prediction time. Contrary to prior work that enforces fairness only on points around test or train data, we are able to enforce and guarantee fairness on all points in the input domain. Additionally, we propose an in-processing technique to use fairness as an inductive bias by iteratively incorporating fairness counterexamples in the learning process. We have implemented these techniques in a tool called FETA. Empirical evaluation on real-world datasets indicates that FETA is not only able to guarantee fairness on-the-fly at prediction time but also is able to train accurate models exhibiting a much higher degree of individual fairness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1643681507",
                        "name": "Kiarash Mohammadi"
                    },
                    {
                        "authorId": "2363557",
                        "name": "Aishwarya Sivaraman"
                    },
                    {
                        "authorId": "2086602",
                        "name": "G. Farnadi"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", [34] have showed that adversarially trained models introduce severe performance disparity across different classes."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2e196cbf324c4578426a89880ff6dae09ab16a3a",
                "externalIds": {
                    "ArXiv": "2205.14926",
                    "DBLP": "journals/corr/abs-2205-14926",
                    "DOI": "10.48550/arXiv.2205.14926",
                    "CorpusId": 249191876
                },
                "corpusId": 249191876,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2e196cbf324c4578426a89880ff6dae09ab16a3a",
                "title": "CalFAT: Calibrated Federated Adversarial Training with Label Skewness",
                "abstract": "Recent studies have shown that, like traditional machine learning, federated learning (FL) is also vulnerable to adversarial attacks. To improve the adversarial robustness of FL, federated adversarial training (FAT) methods have been proposed to apply adversarial training locally before global aggregation. Although these methods demonstrate promising results on independent identically distributed (IID) data, they suffer from training instability on non-IID data with label skewness, resulting in degraded natural accuracy. This tends to hinder the application of FAT in real-world applications where the label distribution across the clients is often skewed. In this paper, we study the problem of FAT under label skewness, and reveal one root cause of the training instability and natural accuracy degradation issues: skewed labels lead to non-identical class probabilities and heterogeneous local models. We then propose a Calibrated FAT (CalFAT) approach to tackle the instability issue by calibrating the logits adaptively to balance the classes. We show both theoretically and empirically that the optimization of CalFAT leads to homogeneous local models across the clients and better convergence points.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Chen Chen"
                    },
                    {
                        "authorId": null,
                        "name": "Yuchen Liu"
                    },
                    {
                        "authorId": "2115763552",
                        "name": "Xingjun Ma"
                    },
                    {
                        "authorId": "3366777",
                        "name": "L. Lyu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[39] studies the setting of adversarial robustness and show that adversarial training introduces unfair outcomes in term of accuracy parity [42]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0c7400b8c2e58aa68b06b98dc4f13ff15c3e1c49",
                "externalIds": {
                    "ArXiv": "2205.13574",
                    "DBLP": "journals/corr/abs-2205-13574",
                    "DOI": "10.48550/arXiv.2205.13574",
                    "CorpusId": 249152254
                },
                "corpusId": 249152254,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0c7400b8c2e58aa68b06b98dc4f13ff15c3e1c49",
                "title": "Pruning has a disparate impact on model accuracy",
                "abstract": "Network pruning is a widely-used compression technique that is able to significantly scale down overparameterized models with minimal loss of accuracy. This paper shows that pruning may create or exacerbate disparate impacts. The paper sheds light on the factors to cause such disparities, suggesting differences in gradient norms and distance to decision boundary across groups to be responsible for this critical issue. It analyzes these factors in detail, providing both theoretical and empirical support, and proposes a simple, yet effective, solution that mitigates the disparate impacts caused by pruning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2055333757",
                        "name": "Cuong D. Tran"
                    },
                    {
                        "authorId": "2141569789",
                        "name": "Ferdinando Fioretto"
                    },
                    {
                        "authorId": "2109169708",
                        "name": "Jung-Eun Kim"
                    },
                    {
                        "authorId": "1767752585",
                        "name": "Rakshit Naidu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, robustness bias is present (and sometimes even amplified) [2, 34] after supposedly making the model \u2018robust\u2019 using state-of-the-art adversarial defense methods like adversarial training.",
                "[34] demonstrated that the vulnerability of certain classes (that are inherently more-vulnerable/less-robust to adversarial perturbations) is amplified after adversarial training.",
                "Furthermore, robustness bias persists [2, 34] even after making the model robust using adversarial defenses like adversarial training."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bde77ef5d9df59fde7a15a6d2d3933e833d2520d",
                "externalIds": {
                    "DBLP": "conf/cvpr/NayakRLPC22",
                    "ArXiv": "2205.02604",
                    "DOI": "10.1109/CVPRW56347.2022.00479",
                    "CorpusId": 248524703
                },
                "corpusId": 248524703,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bde77ef5d9df59fde7a15a6d2d3933e833d2520d",
                "title": "Holistic Approach to Measure Sample-level Adversarial Vulnerability and its Utility in Building Trustworthy Systems",
                "abstract": "Adversarial attack perturbs an image with an imperceptible noise, leading to incorrect model prediction. Recently, a few works showed inherent bias associated with such attack (robustness bias), where certain subgroups in a dataset (e.g. based on class, gender, etc.) are less robust than others. This bias not only persists even after adversarial training, but often results in severe performance discrepancies across these subgroups. Existing works characterize the subgroup\u2019s robustness bias by only checking individual sample\u2019s proximity to the decision boundary. In this work, we argue that this measure alone is not sufficient and validate our argument via extensive experimental analysis. It has been observed that adversarial attacks often corrupt the high-frequency components of the input image. We, therefore, propose a holistic approach for quantifying adversarial vulnerability of a sample by combining these different perspectives, i.e., degree of model\u2019s reliance on high-frequency features and the (conventional) sample-distance to the decision boundary. We demonstrate that by reliably estimating adversarial vulnerability at the sample level using the proposed holistic metric, it is possible to develop a trustworthy system where humans can be alerted about the incoming samples that are highly likely to be misclassified at test time. This is achieved with better precision when our holistic metric is used over individual measures. To further corroborate the utility of the proposed holistic approach, we perform knowledge distillation in a limited-sample setting. We observe that the student network trained with the subset of samples selected using our combined metric performs better than both the competing baselines, viz., where samples are selected randomly or based on their distances to the decision boundary.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143747407",
                        "name": "Gaurav Kumar Nayak"
                    },
                    {
                        "authorId": "1658305348",
                        "name": "Ruchit Rawal"
                    },
                    {
                        "authorId": "1474565160",
                        "name": "Rohit Lal"
                    },
                    {
                        "authorId": "2126912981",
                        "name": "Himanshu Patil"
                    },
                    {
                        "authorId": "1429640900",
                        "name": "Anirban Chakraborty"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9212c2c87177a9a634957e1448c38f50c7959657",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-04358",
                    "ArXiv": "2204.04358",
                    "DOI": "10.1109/CVPRW56347.2022.00329",
                    "CorpusId": 248085565
                },
                "corpusId": 248085565,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9212c2c87177a9a634957e1448c38f50c7959657",
                "title": "Segmenting across places: The need for fair transfer learning with satellite imagery",
                "abstract": "The increasing availability of high-resolution satellite imagery has enabled the use of machine learning to support land-cover measurement and inform policy-making. However, labelling satellite images is expensive and is available for only some locations. This prompts the use of transfer learning to adapt models from data-rich locations to others. Given the potential for high-impact applications of satellite imagery across geographies, a systematic assessment of transfer learning implications is warranted. In this work, we consider the task of land-cover segmentation and study the fairness implications of transferring models across locations. We leverage a large satellite image segmentation benchmark with 5987 images from 18 districts (9 urban and 9 rural). Via fairness metrics we quantify disparities in model performance along two axes \u2013 across urban-rural locations and across land-cover classes. Findings show that state-of-the-art models have better overall accuracy in rural areas compared to urban areas, through unsupervised domain adaptation methods transfer learning better to urban versus rural areas and enlarge fairness gaps. In analysis of reasons for these findings, we show that raw satellite images are overall more dissimilar between source and target districts for rural than for urban locations. This work highlights the need to conduct fairness analysis for satellite imagery segmentation models and motivates the development of methods for fair transfer learning in order not to introduce disparities between places, particularly urban and rural locations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "150238772",
                        "name": "Miaohui Zhang"
                    },
                    {
                        "authorId": "20400898",
                        "name": "Harvineet Singh"
                    },
                    {
                        "authorId": "2241884886",
                        "name": "Lazarus Chok"
                    },
                    {
                        "authorId": "3144230",
                        "name": "R. Chunara"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Problem settings introduced by deep models Fair for different deep models [29, 69, 74, 75, 75, 76]; Utility studies for fair enforced models [71, 77\u201379] Neural Computing and Applications (2022) 34:12875\u201312893 12881",
                "[66] and continuous [72] sensitive features; and (2) examining some newly emerging deep learning applications, such as deep clustering [74], adversarial training [75], and attacks [77].",
                "[75] observe unfair results in adversarial training, while Chen et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6bdebf88b07e22f9955f9b06590e8ff030697299",
                "externalIds": {
                    "DBLP": "journals/nca/TianZLZ22",
                    "DOI": "10.1007/s00521-022-07136-1",
                    "CorpusId": 247723355
                },
                "corpusId": 247723355,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6bdebf88b07e22f9955f9b06590e8ff030697299",
                "title": "Image fairness in deep learning: problems, models, and challenges",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51262864",
                        "name": "Huan Tian"
                    },
                    {
                        "authorId": "32620196",
                        "name": "Tianqing Zhu"
                    },
                    {
                        "authorId": null,
                        "name": "Wei Liu"
                    },
                    {
                        "authorId": "2134555583",
                        "name": "Wanlei Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We follow a common approach in bias mitigation [18, 19, 65, 57] and employ an adversarial classifier, \u03b8adv, whose aim is to predict the attribute label A of image I given only its similarity logits from the set of sensitive text queries T"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "efabdd27929796b712cb1b3a3051ea5358dc1200",
                "externalIds": {
                    "ArXiv": "2203.11933",
                    "DBLP": "conf/ijcnlp/BergHBKSB22",
                    "ACL": "2022.aacl-main.61",
                    "DOI": "10.48550/arXiv.2203.11933",
                    "CorpusId": 247596835
                },
                "corpusId": 247596835,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/efabdd27929796b712cb1b3a3051ea5358dc1200",
                "title": "A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning",
                "abstract": "Vision-language models can encode societal biases and stereotypes, but there are challenges to measuring and mitigating these multimodal harms due to lacking measurement robustness and feature degradation. To address these challenges, we investigate bias measures and apply ranking metrics for image-text representations. We then investigate debiasing methods and show that prepending learned embeddings to text queries that are jointly trained with adversarial debiasing and a contrastive loss, reduces various bias measures with minimal degradation to the image-text representation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2068113615",
                        "name": "Hugo Elias Berg"
                    },
                    {
                        "authorId": "1939997577",
                        "name": "S. Hall"
                    },
                    {
                        "authorId": "3469024",
                        "name": "Yash Bhalgat"
                    },
                    {
                        "authorId": "2134908414",
                        "name": "Wonsuk Yang"
                    },
                    {
                        "authorId": "90729626",
                        "name": "Hannah Rose Kirk"
                    },
                    {
                        "authorId": "2048000614",
                        "name": "Aleksandar Shtedritski"
                    },
                    {
                        "authorId": "153000035",
                        "name": "Max Bain"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e0fbcb244fa9fae287184eae79f22c5b01a01ff6",
                "externalIds": {
                    "DOI": "10.1016/j.ijcce.2022.03.002",
                    "CorpusId": 247356524
                },
                "corpusId": 247356524,
                "publicationVenue": {
                    "id": "c5927bb8-7a75-403d-bc94-59064b35b031",
                    "name": "International Journal of Cognitive Computing in Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Cogn Comput Eng"
                    ],
                    "issn": "2666-3074",
                    "url": "https://www.keaipublishing.com/en/journals/international-journal-of-cognitive-computing-in-engineering/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/international-journal-of-cognitive-computing-in-engineering"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e0fbcb244fa9fae287184eae79f22c5b01a01ff6",
                "title": "Exploring Generative Adversarial Networks and Adversarial Training",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "93135024",
                        "name": "A. Sajeeda"
                    },
                    {
                        "authorId": "2064343180",
                        "name": "B. M. M. Hossain"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Therefore, privacy-preservation as a process of protecting sensitive information against being revealed or misused by unauthorized users has been studied extensively [Xu et al., 2021c].",
                "For example, [Bagdasaryan and Shmatikov, 2019] shows that differential privacy can lead to disparate impact against minority groups and [Xu et al., 2021a] studies how to mitigate the disparate impact.",
                "[Xu et al., 2021b] shows adversarial learning based defense techniques do not provide sufficient protection to minority groups, which incurs unfairness."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "79cbac6f9502480e4a1657e4375b78ff5bcea8e0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-07787",
                    "ArXiv": "2202.07787",
                    "CorpusId": 246867002
                },
                "corpusId": 246867002,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/79cbac6f9502480e4a1657e4375b78ff5bcea8e0",
                "title": "Trustworthy Anomaly Detection: A Survey",
                "abstract": "Anomaly detection has a wide range of real-world applications, such as bank fraud detection and cyber intrusion detection. In the past decade, a variety of anomaly detection models have been developed, which lead to big progress towards accurately detecting various anomalies. Despite the successes, anomaly detection models still face many limitations. The most significant one is whether we can trust the detection results from the models. In recent years, the research community has spent a great effort to design trustworthy machine learning models, such as developing trustworthy classification models. However, the attention to anomaly detection tasks is far from sufficient. Considering that many anomaly detection tasks are life-changing tasks involving human beings, labeling someone as anomalies or fraudsters should be extremely cautious. Hence, ensuring the anomaly detection models conducted in a trustworthy fashion is an essential requirement to deploy the models to conduct automatic decisions in the real world. In this brief survey, we summarize the existing efforts and discuss open problems towards trustworthy anomaly detection from the perspectives of interpretability, fairness, robustness, and privacy-preservation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2241342380",
                        "name": "Shuhan Yuan"
                    },
                    {
                        "authorId": "7916525",
                        "name": "Xintao Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A Kernel Extreme Learning Machine (KELM) is used as a regressor; two KELM models were trained, for both the face and scene modalities.",
                "Adversarial Learning (Xu et al., 2021) works by training two models; the first is a predictor model P which predicts the desired label, and the second is a discriminator model D which predicts the protected variable.",
                "Feature bias has been encountered by using adversarial learning (Xu et al., 2021), where an adversarial model is trained to re-represent the input feature in a manner agnostic w.",
                "Algorithm 1 Adversarial Learning Input: Ground truth labels y, protected variable c, input features X Models: Filter E, Predictor P , Discriminator D for e epochs do\nTrain the models P,E one step by minimising: MSE(y, P (E(X)))\u2212 \u03bb1CE(c, D(E(X))) Train the model D one step by minimising: \u03b1\u03bb2CE(c, D(E(X)))\nend for\nThis mechanism ensures that the embedding is representative enough for the predictor model to perform well, while not being representative enough for the discriminator to identify the protected variable.",
                "KELM (Huang et al., 2011) is a method which improves over Extreme Learning Machine (ELM) (Huang et al., 2004).",
                "The impact of Algorithms, Artificial Intelligence (AI), and Machine Learning (ML) on our daily lives is increasing day\n1Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Germany 2SYNCPILOT GmbH, Augsburg, Germany 3Imperial College London, UK.",
                "Feature bias has been encountered by using adversarial learning (Xu et al., 2021), where an adversarial model is trained to re-represent the input feature in a manner agnostic w. r. t. to the protected variable, hence acquiring features that do not leak information about the protected variable,\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "4eb8f3064073e70c6fae740be025f680d901ea5b",
                "externalIds": {
                    "ArXiv": "2202.00993",
                    "DBLP": "journals/corr/abs-2202-00993",
                    "CorpusId": 246473427
                },
                "corpusId": 246473427,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4eb8f3064073e70c6fae740be025f680d901ea5b",
                "title": "Normalise for Fairness: A Simple Normalisation Technique for Fairness in Regression Machine Learning Problems",
                "abstract": "Algorithms and Machine Learning (ML) are increasingly affecting everyday life and several decision-making processes, where ML has an advantage due to scalability or superior performance. Fairness in such applications is crucial, where models should not discriminate their results based on race, gender, or other protected groups. This is especially crucial for models affecting very sensitive topics, like interview hiring or recidivism prediction. Fairness is not commonly studied for regression problems compared to binary classification problems; hence, we present a simple, yet effective method based on normalisation (FaiReg), which minimises the impact of unfairness in regression problems, especially due to labelling bias. We present a theoretical analysis of the method, in addition to an empirical comparison against two standard methods for fairness, namely data balancing and adversarial training. We also include a hybrid formulation (FaiRegH), merging the presented method with data balancing, in an attempt to face labelling and sample biases simultaneously. The experiments are conducted on the multimodal dataset First Impressions (FI) with various labels, namely personality prediction and interview screening score. The results show the superior performance of diminishing the effects of unfairness better than data balancing, also without deteriorating the performance of the original problem as much as adversarial training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152322",
                        "name": "Mostafa M. Mohamed"
                    },
                    {
                        "authorId": "145411696",
                        "name": "Bj\u00f6rn Schuller"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[83] Han Xu, Xiaorui Liu, Yaxin Li, Anil Jain, and Jiliang Tang.",
                "Robustness mitigation: Relations between robustness guarantees and individual and group fairness have been studied in [85, 46, 83, 84] and [75] respectively."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1ff6750a158a9debfa5f26347c4252819ddd97a2",
                "externalIds": {
                    "DBLP": "conf/nips/SchrouffHKASOBR22",
                    "ArXiv": "2202.01034",
                    "CorpusId": 253463948
                },
                "corpusId": 253463948,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1ff6750a158a9debfa5f26347c4252819ddd97a2",
                "title": "Diagnosing failures of fairness transfer across distribution shift in real-world medical settings",
                "abstract": "Diagnosing and mitigating changes in model fairness under distribution shift is an important component of the safe deployment of machine learning in healthcare settings. Importantly, the success of any mitigation strategy strongly depends on the structure of the shift. Despite this, there has been little discussion of how to empirically assess the structure of a distribution shift that one is encountering in practice. In this work, we adopt a causal framing to motivate conditional independence tests as a key tool for characterizing distribution shifts. Using our approach in two medical applications, we show that this knowledge can help diagnose failures of fairness transfer, including cases where real-world shifts are more complex than is often assumed in the literature. Based on these results, we discuss potential remedies at each step of the machine learning pipeline.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3212089",
                        "name": "J. Schrouff"
                    },
                    {
                        "authorId": "2088732955",
                        "name": "Natalie Harris"
                    },
                    {
                        "authorId": "143812875",
                        "name": "O. Koyejo"
                    },
                    {
                        "authorId": "2922782",
                        "name": "Ibrahim M. Alabdulmohsin"
                    },
                    {
                        "authorId": "1988972565",
                        "name": "Eva Schnider"
                    },
                    {
                        "authorId": "1398933345",
                        "name": "Krista Opsahl-Ong"
                    },
                    {
                        "authorId": "2152100384",
                        "name": "Alex Brown"
                    },
                    {
                        "authorId": "2109961729",
                        "name": "Subhrajit Roy"
                    },
                    {
                        "authorId": "2007712128",
                        "name": "Diana Mincu"
                    },
                    {
                        "authorId": "2110195795",
                        "name": "Christina Chen"
                    },
                    {
                        "authorId": "2064110017",
                        "name": "Awa Dieng"
                    },
                    {
                        "authorId": "2143862099",
                        "name": "Yuan Liu"
                    },
                    {
                        "authorId": "144223091",
                        "name": "Vivek Natarajan"
                    },
                    {
                        "authorId": "6413143",
                        "name": "A. Karthikesalingam"
                    },
                    {
                        "authorId": "145993598",
                        "name": "K. Heller"
                    },
                    {
                        "authorId": "48880818",
                        "name": "S. Chiappa"
                    },
                    {
                        "authorId": "1396841807",
                        "name": "A. D'Amour"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "63b6f7c31cee7bca2a05ef77b4eb34a798c946be",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-13329",
                    "ArXiv": "2201.13329",
                    "CorpusId": 246430374
                },
                "corpusId": 246430374,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/63b6f7c31cee7bca2a05ef77b4eb34a798c946be",
                "title": "Can Adversarial Training Be Manipulated By Non-Robust Features?",
                "abstract": "Adversarial training, originally designed to resist test-time adversarial examples, has shown to be promising in mitigating training-time availability attacks. This defense ability, however, is challenged in this paper. We identify a novel threat model named stability attack, which aims to hinder robust availability by slightly manipulating the training data. Under this threat, we show that adversarial training using a conventional defense budget $\\epsilon$ provably fails to provide test robustness in a simple statistical setting, where the non-robust features of the training data can be reinforced by $\\epsilon$-bounded perturbation. Further, we analyze the necessity of enlarging the defense budget to counter stability attacks. Finally, comprehensive experiments demonstrate that stability attacks are harmful on benchmark datasets, and thus the adaptive defense is necessary to maintain robustness. Our code is available at https://github.com/TLMichael/Hypocritical-Perturbation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3023914",
                        "name": "Lue Tao"
                    },
                    {
                        "authorId": "2117930471",
                        "name": "Lei Feng"
                    },
                    {
                        "authorId": "2115312735",
                        "name": "Hongxin Wei"
                    },
                    {
                        "authorId": "2882166",
                        "name": "Jinfeng Yi"
                    },
                    {
                        "authorId": "7649626",
                        "name": "Sheng-Jun Huang"
                    },
                    {
                        "authorId": "48848338",
                        "name": "Songcan Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9a1f352ef21044700c180882038c28c3b2361914",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-06409",
                    "ArXiv": "2112.06409",
                    "DOI": "10.1007/s00778-022-00775-9",
                    "CorpusId": 245123976
                },
                "corpusId": 245123976,
                "publicationVenue": {
                    "id": "e6bfdcb4-2a51-40d0-bba8-df275f223711",
                    "name": "The VLDB journal",
                    "type": "journal",
                    "alternate_names": [
                        "Vldb J",
                        "VLDB j",
                        "The Vldb Journal"
                    ],
                    "issn": "1066-8888",
                    "url": "http://dl.acm.org/citation.cfm?CFID=753506132&CFTOKEN=90626957&id=J869"
                },
                "url": "https://www.semanticscholar.org/paper/9a1f352ef21044700c180882038c28c3b2361914",
                "title": "Data collection and quality challenges in deep learning: a data-centric AI perspective",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3288247",
                        "name": "Steven Euijong Whang"
                    },
                    {
                        "authorId": "30840932",
                        "name": "Yuji Roh"
                    },
                    {
                        "authorId": "22656934",
                        "name": "Hwanjun Song"
                    },
                    {
                        "authorId": "2143422191",
                        "name": "Jae-Gil Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026is particularly salient to our work, and has featured studies that highlight the inequity that results from strategic behavior by individuals [Hu et al., 2019], as well as inequity (social cost) resulting from making classifiers robust to strategic behavior [Milli et al., 2019, Xu et al., 2021].",
                "\u2026approaches have been introduced, particularly in machine learning, that investigate how to balance fairness and task-related efficacy, such as accurate [Agarwal et al., 2018, Feldman et al., 2015, Kearns et al., 2018, Xu et al., 2021, Zafar et al., 2019, Zemel et al., 2013, Hardt et al., 2016b]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9912b74c5c27df9d2a1574528368582ff9add2b4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-02746",
                    "ArXiv": "2112.02746",
                    "CorpusId": 244908955
                },
                "corpusId": 244908955,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9912b74c5c27df9d2a1574528368582ff9add2b4",
                "title": "Unfairness Despite Awareness: Group-Fair Classification with Strategic Agents",
                "abstract": "The use of algorithmic decision making systems in domains which impact the financial, social, and political well-being of people has created a demand for these decision making systems to be\"fair\"under some accepted notion of equity. This demand has in turn inspired a large body of work focused on the development of fair learning algorithms which are then used in lieu of their conventional counterparts. Most analysis of such fair algorithms proceeds from the assumption that the people affected by the algorithmic decisions are represented as immutable feature vectors. However, strategic agents may possess both the ability and the incentive to manipulate this observed feature vector in order to attain a more favorable outcome. We explore the impact that strategic agent behavior could have on fair classifiers and derive conditions under which this behavior leads to fair classifiers becoming less fair than their conventional counterparts under the same measure of fairness that the fair classifier takes into account. These conditions are related to the the way in which the fair classifier remedies unfairness on the original unmanipulated data: fair classifiers which remedy unfairness by becoming more selective than their conventional counterparts are the ones that become less fair than their counterparts when agents are strategic. We further demonstrate that both the increased selectiveness of the fair classifier, and consequently the loss of fairness, arises when performing fair learning on domains in which the advantaged group is overrepresented in the region near (and on the beneficial side of) the decision boundary of conventional classifiers. Finally, we observe experimentally, using several datasets and learning methods, that this fairness reversal is common, and that our theoretical characterization of the fairness reversal conditions indeed holds in most such cases.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "103056375",
                        "name": "Andrew Estornell"
                    },
                    {
                        "authorId": "40583483",
                        "name": "Sanmay Das"
                    },
                    {
                        "authorId": "2152797134",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "1699600",
                        "name": "Yevgeniy Vorobeychik"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "496f656164db40a3a51c6b214828effdbd00f0f7",
                "externalIds": {
                    "DOI": "10.1109/CSCI54926.2021.00096",
                    "CorpusId": 249929704
                },
                "corpusId": 249929704,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/496f656164db40a3a51c6b214828effdbd00f0f7",
                "title": "Adversarial Training Negatively Affects Fairness",
                "abstract": "With the increasing presence of deep learning models, many applications have had significant improvements; however, they face a new vulnerability known as adversarial examples. Adversarial examples can mislead deep learning models to predict the wrong classes without human actors noticing. Recently, many works have tried to improve adversarial examples to make them stronger and more effective. However, although some researchers have invented mechanisms to defend deep learning models against adversarial examples, those mechanisms may negatively affect different measures of fairness, which are critical in practice. This work mathematically defines four fairness scores to show that training adversarially robust models can harm fairness scores. Furthermore, we empirically show that adversarial training, one of the most potent defensive mechanisms against adversarial examples, can harm them.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46195273",
                        "name": "Korn Sooksatra"
                    },
                    {
                        "authorId": "2075541096",
                        "name": "P. Rivas"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent studies [361, 380] provide a good reference to start with.",
                "For instance, the need for data privacy might interfere with the desire to explain the system output in detail, and the pursuit of algorithmic fairness may be detrimental to the accuracy and robustness experienced by some groups [284, 361].",
                "Instead, elaborated joint optimization and tradeoffs between multiple aspects of trustworthiness are necessary [47, 158, 331, 361, 380].",
                "For example, adversarial robustness and fairness can negatively affect each other during training [284, 361]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "70043a0b612b6253b37df7d363b3bf2ec3d581c7",
                "externalIds": {
                    "ArXiv": "2110.01167",
                    "DBLP": "journals/csur/LiQLDLPYZ23",
                    "DOI": "10.1145/3555803",
                    "CorpusId": 238259667
                },
                "corpusId": 238259667,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/70043a0b612b6253b37df7d363b3bf2ec3d581c7",
                "title": "Trustworthy AI: From Principles to Practices",
                "abstract": "The rapid development of Artificial Intelligence (AI) technology has enabled the deployment of various systems based on it. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection. These shortcomings degrade user experience and erode people\u2019s trust in all AI systems. In this review, we provide AI practitioners with a comprehensive guide for building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, and accountability. To unify currently available but fragmented approaches toward trustworthy AI, we organize them in a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to system development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items for practitioners and societal stakeholders (e.g., researchers, engineers, and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges for the future development of trustworthy AI systems, where we identify the need for a paradigm shift toward comprehensively trustworthy AI systems.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "71788673",
                        "name": "Bo Li"
                    },
                    {
                        "authorId": "2063066946",
                        "name": "Peng Qi"
                    },
                    {
                        "authorId": "2156642988",
                        "name": "Bo Liu"
                    },
                    {
                        "authorId": "2066704988",
                        "name": "Shuai Di"
                    },
                    {
                        "authorId": "2130572701",
                        "name": "Jingen Liu"
                    },
                    {
                        "authorId": "2143385094",
                        "name": "Jiquan Pei"
                    },
                    {
                        "authorId": "2882166",
                        "name": "Jinfeng Yi"
                    },
                    {
                        "authorId": "150048906",
                        "name": "Bowen Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f37ce524fd285280b3a65c2ab67d1808da84920c",
                "externalIds": {
                    "ArXiv": "2110.00603",
                    "DBLP": "journals/corr/abs-2110-00603",
                    "CorpusId": 238259016
                },
                "corpusId": 238259016,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f37ce524fd285280b3a65c2ab67d1808da84920c",
                "title": "Algorithm Fairness in AI for Medicine and Healthcare",
                "abstract": "In the current development and deployment of many arti\ufb01cial intelligence (AI) systems in healthcare, algorithm fairness is a challenging problem in delivering equitable care. Recent evaluation of AI models strati\ufb01ed across race sub-populations have revealed inequalities in how patients are diagnosed, given treatments, and billed for healthcare costs. In this perspective article, we summarize the intersectional \ufb01eld of fairness in machine learning through the context of current issues in healthcare, outline how algorithmic biases ( e.g. - image acquisition, genetic variation, intra-observer labeling variability) arise in current clinical work\ufb02ows and their resulting healthcare disparities. Lastly, we also review emerging technology for mitigating bias via federated learning, disentanglement, and model explainability, and their role in AI-SaMD development.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108279369",
                        "name": "Richard J. Chen"
                    },
                    {
                        "authorId": "2242468870",
                        "name": "Tiffany Y. Chen"
                    },
                    {
                        "authorId": "1959705",
                        "name": "Jana Lipkov\u00e1"
                    },
                    {
                        "authorId": "2109623647",
                        "name": "Judy J. Wang"
                    },
                    {
                        "authorId": "25259989",
                        "name": "Drew F. K. Williamson"
                    },
                    {
                        "authorId": "16184125",
                        "name": "Ming Y. Lu"
                    },
                    {
                        "authorId": "2060422236",
                        "name": "S. Sahai"
                    },
                    {
                        "authorId": "37122655",
                        "name": "Faisal Mahmood"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, we will also discuss several other intriguing properties of robust DNNs [6], from the perspectives such as interpretability, fairness and so on.",
                "It is also essential to understand the reasons for the existence of adversarial examples, and the intrinsic natures of robust DNNs.",
                "By comparing these properties with traditional DNNs, the audience can grip a deep insight into the working mechanism of both robust DNNs and traditional DNNs.",
                "In this section, we draw some theoretical understandings, mainly from the optimization and generalization properties of robust DNNs.",
                "We first give our audience a brief introduction about what is the phenomenon of adversarial examples [3, 7], and why it can be a huge concern for the applications of DNNs."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6b92aa960722e83a878ce442485ba0fef1359456",
                "externalIds": {
                    "DBLP": "conf/kdd/XuLLWT21",
                    "DOI": "10.1145/3447548.3470812",
                    "CorpusId": 236980008
                },
                "corpusId": 236980008,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/6b92aa960722e83a878ce442485ba0fef1359456",
                "title": "Adversarial Robustness in Deep Learning: From Practices to Theories",
                "abstract": "Deep neural networks (DNNs) have achieved unprecedented accomplishments in various machine learning tasks. However, recent studies demonstrate that DNNs are extremely vulnerable to adversarial examples. They are manually synthesized input samples which look benign but can severely fool the prediction of DNN models. For machine learning practitioners who are applying DNNs, understanding the behavior of adversarial examples will not only help them improve the safety of their models, but also can help them have deeper insights into the working mechanism of the DNNs. In this tutorial, we provide a comprehensive overview on the recent advances of adversarial examples and their countermeasures, from both practical and theoretical perspectives. From the practical aspect, we give a detailed introduction of the popular algorithms to generate adversarial examples under different adversary's goals. We also discuss how the defending strategies are developed to resist these attacks, and how new attacks come out to break these defenses. From the theoretical aspect, we discuss a series of intrinsic behaviors of robust DNNs which are different from traditional DNNs, especially about their optimization and generalization properties. Finally, we introduce DeepRobust, a Pytorch adversarial learning library which aims to build a comprehensive and easy-to-use platform to foster this research field. Via our tutorial, the audience can grip the main ideas of adversarial attacks and defenses and gain a deep insight of DNN's robustness. The tutorial official website is at https://sites.google.com/view/kdd21-tutorial-adv-robust.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2018756699",
                        "name": "Han Xu"
                    },
                    {
                        "authorId": "1527096073",
                        "name": "Yaxin Li"
                    },
                    {
                        "authorId": "2124928119",
                        "name": "Xiaorui Liu"
                    },
                    {
                        "authorId": "2108329255",
                        "name": "Wentao Wang"
                    },
                    {
                        "authorId": "1736632",
                        "name": "Jiliang Tang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1e556f0d7ca319e3106383a8aca04a6ca33da46b",
                "externalIds": {
                    "ArXiv": "2108.02707",
                    "DBLP": "conf/uss/RosenbergTFJ23",
                    "CorpusId": 236924628
                },
                "corpusId": 236924628,
                "publicationVenue": {
                    "id": "54649c1d-6bcc-4232-9cd1-aa446867b8d0",
                    "name": "USENIX Security Symposium",
                    "type": "conference",
                    "alternate_names": [
                        "USENIX Secur Symp"
                    ],
                    "url": "http://www.usenix.org/events/bytopic/security.html"
                },
                "url": "https://www.semanticscholar.org/paper/1e556f0d7ca319e3106383a8aca04a6ca33da46b",
                "title": "Fairness Properties of Face Recognition and Obfuscation Systems",
                "abstract": "The proliferation of automated face recognition in the commercial and government sectors has caused significant privacy concerns for individuals. One approach to address these privacy concerns is to employ evasion attacks against the metric embedding networks powering face recognition systems: Face obfuscation systems generate imperceptibly perturbed images that cause face recognition systems to misidentify the user. Perturbed faces are generated on metric embedding networks, which are known to be unfair in the context of face recognition. A question of demographic fairness naturally follows: are there demographic disparities in face obfuscation system performance? We answer this question with an analytical and empirical exploration of recent face obfuscation systems. Metric embedding networks are found to be demographically aware: face embeddings are clustered by demographic. We show how this clustering behavior leads to reduced face obfuscation utility for faces in minority groups. An intuitive analytical model yields insight into these phenomena.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51917955",
                        "name": "Harrison Rosenberg"
                    },
                    {
                        "authorId": "2065096310",
                        "name": "Brian Tang"
                    },
                    {
                        "authorId": "1910642",
                        "name": "Kassem Fawaz"
                    },
                    {
                        "authorId": "1680133",
                        "name": "S. Jha"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "534dc84001e8b861f6699e191676fe79e1535706",
                "externalIds": {
                    "DBLP": "conf/icdm/WangXL0TT22",
                    "ArXiv": "2107.13639",
                    "DOI": "10.1109/ICDM54844.2022.00156",
                    "CorpusId": 236493567
                },
                "corpusId": 236493567,
                "publicationVenue": {
                    "id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
                    "name": "Industrial Conference on Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Ind Conf Data Min",
                        "ICDM"
                    ],
                    "url": "http://www.data-mining-forum.de/"
                },
                "url": "https://www.semanticscholar.org/paper/534dc84001e8b861f6699e191676fe79e1535706",
                "title": "Imbalanced Adversarial Training with Reweighting",
                "abstract": "Adversarial training has been empirically proven to be one of the most effective and reliable defense methods against adversarial attacks. However, the majority of existing studies are focused on balanced datasets, where each class has a similar amount of training examples. Research on adversarial training with imbalanced training datasets is rather limited. As the initial effort to investigate this problem, we reveal the facts that adversarially trained models present two distinguished behaviors from naturally trained models in imbalanced datasets: (1) Compared to natural training, adversarially trained models can suffer much worse performance on under-represented classes, when the training dataset is extremely imbalanced. (2) Traditional reweighting strategies which assign large weights to underrepresented classes will drastically hurt the model\u2019s performance on well-represented classes. In this paper, to further understand our observations, we theoretically show that the poor data separability is one key reason causing this strong tension between under-represented and well-represented classes. Motivated by this finding, we propose the Separable Reweighted Adversarial Training (SRAT) framework to facilitate adversarial training under imbalanced scenarios, by learning more separable features for different classes. Extensive experiments on various datasets verify the effectiveness of the proposed framework.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108329255",
                        "name": "Wentao Wang"
                    },
                    {
                        "authorId": "2018756699",
                        "name": "Han Xu"
                    },
                    {
                        "authorId": "1390612725",
                        "name": "Xiaorui Liu"
                    },
                    {
                        "authorId": "1527096073",
                        "name": "Yaxin Li"
                    },
                    {
                        "authorId": "72558235",
                        "name": "B. Thuraisingham"
                    },
                    {
                        "authorId": "1736632",
                        "name": "Jiliang Tang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Accordingly, the work [363] proposes a framework called Fair-Robust-Learning (FRL) to ensure fairness while improving a model\u2019s robustness.",
                "First, a direct problem to be solved is how to alleviate the conlict so as to meet both sides to the greatest extent [363].",
                "Recent research [363] indicates that adversarial training can introduce a signiicant disparity of performance and robustness among diferent groups, even if the datasets are balanced."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "8b365890c0224f17fffb90bf33da46fccacd9331",
                "externalIds": {
                    "DBLP": "journals/tist/LiuWFLLJLJT23",
                    "ArXiv": "2107.06641",
                    "DOI": "10.1145/3546872",
                    "CorpusId": 235829506
                },
                "corpusId": 235829506,
                "publicationVenue": {
                    "id": "0d993d4a-09ba-4df8-90a4-7dfe25f0cb9e",
                    "name": "ACM Transactions on Intelligent Systems and Technology",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Intell Syst Technol"
                    ],
                    "issn": "2157-6904",
                    "url": "http://portal.acm.org/tist",
                    "alternate_urls": [
                        "https://tist.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8b365890c0224f17fffb90bf33da46fccacd9331",
                "title": "Trustworthy AI: A Computational Perspective",
                "abstract": "In the past few decades, artificial intelligence (AI) technology has experienced swift developments, changing everyone\u2019s daily life and profoundly altering the course of human society. The intention behind developing AI was and is to benefit humans by reducing labor, increasing everyday conveniences, and promoting social good. However, recent research and AI applications indicate that AI can cause unintentional harm to humans by, for example, making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against a group or groups. Consequently, trustworthy AI has recently garnered increased attention regarding the need to avoid the adverse effects that AI could bring to people, so people can fully trust and live in harmony with AI technologies. A tremendous amount of research on trustworthy AI has been conducted and witnessed in recent years. In this survey, we present a comprehensive appraisal of trustworthy AI from a computational perspective to help readers understand the latest technologies for achieving trustworthy AI. Trustworthy AI is a large and complex subject, involving various dimensions. In this work, we focus on six of the most crucial dimensions in achieving trustworthy AI: (i) Safety & Robustness, (ii) Nondiscrimination & Fairness, (iii) Explainability, (iv) Privacy, (v) Accountability & Auditability, and (vi) Environmental Well-being. For each dimension, we review the recent related technologies according to a taxonomy and summarize their applications in real-world systems. We also discuss the accordant and conflicting interactions among different dimensions and discuss potential aspects for trustworthy AI to investigate in the future.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2143856455",
                        "name": "Haochen Liu"
                    },
                    {
                        "authorId": "2108941389",
                        "name": "Yiqi Wang"
                    },
                    {
                        "authorId": "41031455",
                        "name": "Wenqi Fan"
                    },
                    {
                        "authorId": "1390612725",
                        "name": "Xiaorui Liu"
                    },
                    {
                        "authorId": "1527096073",
                        "name": "Yaxin Li"
                    },
                    {
                        "authorId": "39720946",
                        "name": "Shaili Jain"
                    },
                    {
                        "authorId": "1739705",
                        "name": "Anil K. Jain"
                    },
                    {
                        "authorId": "1736632",
                        "name": "Jiliang Tang"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2021), interpretability (Ross & Doshi-Velez, 2018), fairness (Xu et al., 2021) and so on.",
                "Besides, recent studies showed AT could benefit other domains such as pre-training(Chen et al., 2020; Jiang et al., 2020), out-of-distribution generalization (Yi et al., 2021), inpainting (Khachaturov et al., 2021), interpretability (Ross & Doshi-Velez, 2018), fairness (Xu et al., 2021) and so on."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c66eb4446673199f9a7c95d8a9e6f2caad7cf585",
                "externalIds": {
                    "DBLP": "journals/tmlr/ZhangXHLCNS22",
                    "ArXiv": "2105.14676",
                    "CorpusId": 250209640
                },
                "corpusId": 250209640,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c66eb4446673199f9a7c95d8a9e6f2caad7cf585",
                "title": "NoiLin: Improving adversarial training and correcting stereotype of noisy labels",
                "abstract": "Adversarial training (AT) formulated as the minimax optimization problem can effectively enhance the model's robustness against adversarial attacks. The existing AT methods mainly focused on manipulating the inner maximization for generating quality adversarial variants or manipulating the outer minimization for designing effective learning objectives. However, empirical results of AT always exhibit the robustness at odds with accuracy and the existence of the cross-over mixture problem, which motivates us to study some label randomness for benefiting the AT. First, we thoroughly investigate noisy labels (NLs) injection into AT's inner maximization and outer minimization, respectively and obtain the observations on when NL injection benefits AT. Second, based on the observations, we propose a simple but effective method -- NoiLIn that randomly injects NLs into training data at each training epoch and dynamically increases the NL injection rate once robust overfitting occurs. Empirically, NoiLIn can significantly mitigate the AT's undesirable issue of robust overfitting and even further improve the generalization of the state-of-the-art AT methods. Philosophically, NoiLIn sheds light on a new perspective of learning with NLs: NLs should not always be deemed detrimental, and even in the absence of NLs in the training set, we may consider injecting them deliberately. Codes are available in https://github.com/zjfheart/NoiLIn.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47539929",
                        "name": "Jingfeng Zhang"
                    },
                    {
                        "authorId": "1507120550",
                        "name": "Xilie Xu"
                    },
                    {
                        "authorId": "2087238859",
                        "name": "Bo Han"
                    },
                    {
                        "authorId": "121698214",
                        "name": "Tongliang Liu"
                    },
                    {
                        "authorId": "47537639",
                        "name": "Gang Niu"
                    },
                    {
                        "authorId": "101457473",
                        "name": "Li-zhen Cui"
                    },
                    {
                        "authorId": "67154907",
                        "name": "Masashi Sugiyama"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Classifiers trainedwith adversarial examples learn fundamentally different representations compared to standard classifiers reducing accuracy [25] or they can cause disparity on accuracy for both clean and adversarial samples between different classes [26]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "376dc044cfa79f4b9dd34907d1d2ef657b949144",
                "externalIds": {
                    "ArXiv": "2105.06512",
                    "DBLP": "journals/corr/abs-2105-06512",
                    "DOI": "10.1145/3469261.3469404",
                    "CorpusId": 234682420
                },
                "corpusId": 234682420,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/376dc044cfa79f4b9dd34907d1d2ef657b949144",
                "title": "Stochastic-Shield: A Probabilistic Approach Towards Training-Free Adversarial Defense in Quantized CNNs",
                "abstract": "Quantized neural networks (NN) are the common standard to efficiently deploy deep learning models on tiny hardware platforms. However, we notice that quantized NNs are as vulnerable to adversarial attacks as the full-precision models. With the proliferation of neural networks on small devices that we carry or surround us, there is a need for efficient models without sacrificing trust in the prediction in presence of malign perturbations. Current mitigation approaches often need adversarial training or are bypassed when the strength of adversarial examples is increased. In this work, we investigate how a probabilistic framework would assist in overcoming the aforementioned limitations for quantized deep learning models. We explore Stochastic-Shield: a flexible defense mechanism that leverages an input filtering layer and a probabilistic deep learning approach materialized via Monte Carlo dropout. We show that it is possible to jointly achieve efficiency and robustness by accurately enabling each module without the burden of re-retraining or ad hoc fine-tuning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1994899",
                        "name": "Lorena Qendro"
                    },
                    {
                        "authorId": "8055527",
                        "name": "Sangwon Ha"
                    },
                    {
                        "authorId": "3002132",
                        "name": "R. D. Jong"
                    },
                    {
                        "authorId": "48818906",
                        "name": "Partha P. Maji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, some recent studies also explore the relationship between the fairness of an ML model and its other properties, such as robustness (Xu et al., 2020; Nanda et al., 2021) and privacy (Cummings et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "07f4920a4a120a4a8644804904d0f5cc215b8ff8",
                "externalIds": {
                    "DBLP": "conf/acl/LiuJKLT21",
                    "ArXiv": "2105.02778",
                    "ACL": "2021.findings-acl.7",
                    "DOI": "10.18653/v1/2021.findings-acl.7",
                    "CorpusId": 233864681
                },
                "corpusId": 233864681,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/07f4920a4a120a4a8644804904d0f5cc215b8ff8",
                "title": "The Authors Matter: Understanding and Mitigating Implicit Bias in Deep Text Classification",
                "abstract": "It is evident that deep text classification models trained on human data could be biased. In particular, they produce biased outcomes for texts that explicitly include identity terms of certain demographic groups. We refer to this type of bias as explicit bias, which has been extensively studied. However, deep text classification models can also produce biased outcomes for texts written by authors of certain demographic groups. We refer to such bias as implicit bias of which we still have a rather limited understanding. In this paper, we first demonstrate that implicit bias exists in different text classification tasks for different demographic groups. Then, we build a learning-based interpretation method to deepen our knowledge of implicit bias. Specifically, we verify that classifiers learn to make predictions based on language features that are related to the demographic attributes of the authors. Next, we propose a framework Debiased-TC to train deep text classifiers to make predictions on the right features and consequently mitigate implicit bias. We conduct extensive experiments on three real-world datasets. The results show that the text classification models trained under our proposed framework outperform traditional models significantly in terms of fairness, and also slightly in terms of classification performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2143856455",
                        "name": "Haochen Liu"
                    },
                    {
                        "authorId": "144767914",
                        "name": "Wei Jin"
                    },
                    {
                        "authorId": "1596725015",
                        "name": "Hamid Karimi"
                    },
                    {
                        "authorId": "2117940912",
                        "name": "Zitao Liu"
                    },
                    {
                        "authorId": "1736632",
                        "name": "Jiliang Tang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2aab97e35c43d961d645e650808d5b052ec180ab",
                "externalIds": {
                    "DBLP": "conf/nips/CroceASDFCM021",
                    "ArXiv": "2010.09670",
                    "MAG": "3093235747",
                    "CorpusId": 224705419
                },
                "corpusId": 224705419,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2aab97e35c43d961d645e650808d5b052ec180ab",
                "title": "RobustBench: a standardized adversarial robustness benchmark",
                "abstract": "Evaluation of adversarial robustness is often error-prone leading to overestimation of the true robustness of models. While adaptive attacks designed for a particular defense are a way out of this, there are only approximate guidelines on how to perform them. Moreover, adaptive evaluations are highly customized for particular models, which makes it difficult to compare different defenses. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. This requires to impose some restrictions on the admitted models to rule out defenses that only make gradient-based attacks ineffective without improving actual robustness. We evaluate robustness of models for our benchmark with AutoAttack, an ensemble of white- and black-box attacks which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. Our leaderboard, hosted at this http URL, aims at reflecting the current state of the art on a set of well-defined tasks in $\\ell_\\infty$- and $\\ell_2$-threat models with possible extensions in the future. Additionally, we open-source the library this http URL that provides unified access to state-of-the-art robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze general trends in $\\ell_p$-robustness and its impact on other tasks such as robustness to various distribution shifts and out-of-distribution detection.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "39171784",
                        "name": "Francesco Croce"
                    },
                    {
                        "authorId": "47669224",
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "authorId": "3482535",
                        "name": "Vikash Sehwag"
                    },
                    {
                        "authorId": "2175939276",
                        "name": "Edoardo Debenedetti"
                    },
                    {
                        "authorId": "2175939276",
                        "name": "Edoardo Debenedetti"
                    },
                    {
                        "authorId": "116491218",
                        "name": "M. Chiang"
                    },
                    {
                        "authorId": "143615345",
                        "name": "Prateek Mittal"
                    },
                    {
                        "authorId": "143610806",
                        "name": "Matthias Hein"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This type of unfairness happens in balanced datasets and does not exist in clean data trained models [1, 14, 19].",
                "This unfairness can occur even in balanced datasets but is absent in models trained on clean data [1, 19]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "adb762a645b72fc4605e6fb512ef2684db37cc93",
                "externalIds": {
                    "DBLP": "conf/aidm/GongW23",
                    "ArXiv": "1704.04960",
                    "MAG": "2605631833",
                    "DOI": "10.1145/3593078.3593935",
                    "CorpusId": 18709105
                },
                "corpusId": 18709105,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/adb762a645b72fc4605e6fb512ef2684db37cc93",
                "title": "Adversarial and Clean Data Are Not Twins",
                "abstract": "Adversarial attack has cast a shadow on the massive success of deep neural networks. Despite being almost visually identical to the clean data, the adversarial images can fool deep neural networks into the wrong predictions with very high confidence. Adversarial training, as the most prevailing defense technique, suffers from class-wise unfairness and model-dependent challenges. In this paper, we propose to detect and eliminate adversarial data in databases prior to data processing in supporting robust and secure AI workloads. We empirically show that we can build a binary classifier separating the adversarial apart from the clean data with high accuracy. We also show that the binary classifier is robust to a second-round adversarial attack. In other words, it is difficult to disguise adversarial samples to bypass the binary classifier. Furthermore, we empirically investigate the generalization limitation which lingers on all current defensive methods, including the binary classifier approach. And we hypothesize that this is the result of the intrinsic property of adversarial crafting algorithms. Our experiments ascertain that adversarial and clean data are two different datasets that can be separated with a binary classifier, which can serve as a portable component to detect and eliminate adversarial data in an end-to-end data management pipeline.",
                "year": 2017,
                "authors": [
                    {
                        "authorId": "48398849",
                        "name": "Zhitao Gong"
                    },
                    {
                        "authorId": "9590047",
                        "name": "Wenlu Wang"
                    },
                    {
                        "authorId": "1758909",
                        "name": "Wei-Shinn Ku"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Though improving adversarial robustness effectively, adversarial training has exposed several defects such as computational overhead [25], class-wise fairness [33, 30], among which the decreased clean accuracy [27, 28] have become the major concern.",
                "robust accuracy trade-off [27, 28], computational overhead [25], class-wise fairness [33, 30] and the absence of formal guarantees [29, 35]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f2779ec5f717ee1194fcef565aafef33abf9c11f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-05392",
                    "DOI": "10.48550/arXiv.2305.05392",
                    "CorpusId": 258564602
                },
                "corpusId": 258564602,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f2779ec5f717ee1194fcef565aafef33abf9c11f",
                "title": "On the Relation between Sharpness-Aware Minimization and Adversarial Robustness",
                "abstract": ". We propose a novel understanding of Sharpness-Aware Minimization (SAM) in the context of adversarial robustness. In this paper, we point out that both SAM and adversarial training (AT) can be viewed as speci\ufb01c feature perturbations, which improve adversarial robustness. However, we note that SAM and AT are distinct in terms of perturbation strength, leading to different accuracy and robustness trade-offs. We provide theoretical evidence for these claims in a simpli\ufb01ed model with rigorous mathematical proofs. Furthermore, we conduct experiment to demonstrate that only utilizing SAM can achieve superior adversarial robustness compared to standard training, which is an unexpected bene\ufb01t. As adversarial training can suffer from a decrease in clean accuracy, we show that using SAM alone can improve robustness without sacri\ufb01cing clean accuracy. Code is available at https://github.com/weizeming/SAM_AT.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2191808925",
                        "name": "Zeming Wei"
                    },
                    {
                        "authorId": "2216749387",
                        "name": "Jingyu Zhu"
                    },
                    {
                        "authorId": "1971137",
                        "name": "Yihao Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "adversarial robustness enhancement [19], [20], we propose a method to represent the decision boundary in the sample space using adversarial attacks."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ad0c2852aa67dbcdc6a1fed4f1ba567c08cb4033",
                "externalIds": {
                    "DBLP": "journals/tifs/TianWALW23",
                    "DOI": "10.1109/TIFS.2023.3293418",
                    "CorpusId": 259531771
                },
                "corpusId": 259531771,
                "publicationVenue": {
                    "id": "d406a3f4-dc05-43be-b1f6-812f29de9c0e",
                    "name": "IEEE Transactions on Information Forensics and Security",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Inf Forensics Secur"
                    ],
                    "issn": "1556-6013",
                    "url": "http://www.ieee.org/organizations/society/sp/tifs.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=10206",
                        "http://www.signalprocessingsociety.org/publications/periodicals/forensics/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ad0c2852aa67dbcdc6a1fed4f1ba567c08cb4033",
                "title": "Knowledge Representation of Training Data With Adversarial Examples Supporting Decision Boundary",
                "abstract": "Deep learning (DL) has achieved tremendous success in recent years in many fields. The success of DL typically relies on a considerable amount of training data and the expensive model optimization process. Therefore, a trained DL model and its corresponding training data have become valuable assets whose intellectual property (IP) needs to be protected. Once a DL model or its training dataset is released, there is currently no mechanism for the entity that owns one part to establish a clear relationship with the other. In this paper, we aim to reveal the integrated relationship between a given DL model and the corresponding training dataset, by framing the problem of knowledge representation of a dataset with respect to DL models trained on it: how to effectively represent the knowledge transferred from a training dataset to a DL model? Our basic idea is that the knowledge transferred from a training dataset to a DL model can be uniquely represented by the model\u2019s decision boundary. Therefore, we design a novel generation method that utilizes geometric consistency to find the samples supporting the decision boundary, which can serve as the proxy for the knowledge representation. We evaluate our method in three different cases: IP audit of training data, IP audit of DL models, and adversarial knowledge distillation. The experimental results show that our method can improve the performance of existing works in all cases, which confirm that our method can effectively represent the knowledge transferred from a training dataset to a DL model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220835716",
                        "name": "Zehao Tian"
                    },
                    {
                        "authorId": null,
                        "name": "Zixiong Wang"
                    },
                    {
                        "authorId": "2575987",
                        "name": "A. Abdelmoniem"
                    },
                    {
                        "authorId": "2019143562",
                        "name": "Gaoyang Liu"
                    },
                    {
                        "authorId": "40614774",
                        "name": "Chen Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, they demonstrate that adversarial training tends to introduce severe disparity of accuracy and robustness between different sub-partitions of data [40].",
                "Recently, the concept of fairness has been integrated with robustness by using robust accuracy measurement [2,40,36].",
                "Authors in [40] have shown that adversarial training can cause a serious disparity in both standard accuracy and adversarial robustness between different classes of data."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "40dbd97cfba2d2d819ed3194d6a3b5717cfbe44d",
                "externalIds": {
                    "DBLP": "conf/adbis/MousaviMD23",
                    "DOI": "10.1007/978-3-031-42914-9_10",
                    "CorpusId": 261344220
                },
                "corpusId": 261344220,
                "publicationVenue": {
                    "id": "23b8d7ac-6e6f-4fe1-9f7e-dc7042815a44",
                    "name": "Symposium on Advances in Databases and Information Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Symp Adv Database Inf Syst",
                        "Advances in Databases and Information Systems",
                        "ADBIS",
                        "Adv Database Inf Syst"
                    ],
                    "url": "http://www.ipi.ac.ru/sigmod/"
                },
                "url": "https://www.semanticscholar.org/paper/40dbd97cfba2d2d819ed3194d6a3b5717cfbe44d",
                "title": "FARMUR: Fair Adversarial Retraining to Mitigate Unfairness in Robustness",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2111714087",
                        "name": "S. A. Mousavi"
                    },
                    {
                        "authorId": "2086872050",
                        "name": "H. Mousavi"
                    },
                    {
                        "authorId": "2066349677",
                        "name": "M. Daneshtalab"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a755dec30bfc81183b2063bd9d7e63b942b0ca6f",
                "externalIds": {
                    "DOI": "10.14428/esann/2023.es2023-30",
                    "CorpusId": 262065223
                },
                "corpusId": 262065223,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a755dec30bfc81183b2063bd9d7e63b942b0ca6f",
                "title": "Mitigating Robustness Bias: Theoretical Results and Empirical Evidences",
                "abstract": ". Recent research has shown that some learned classifiers can be more easily fooled by an adversary who carefully crafts imperceptible or physically plausible modifications of the input data regarding particular subgroups of the population (e.g., people with particular gender, ethnicity, or skin color). This form of unfairness has been just recently studied, noting the fact that classical fairness metrics, which only observe the model outputs, are not enough but robustness biases need to be measured and mitigated as well. For this reason, in this paper, we will first develop a new metric of fairness which generalizes the current ones and degenerates in the classical ones and then we will develop a theoretical mitigation framework with consistency results able to generate a new empirical mitigation strategy and explain why the current ones actually work.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2125236051",
                        "name": "Danilo Franco"
                    },
                    {
                        "authorId": "1682762",
                        "name": "L. Oneto"
                    },
                    {
                        "authorId": "1686031",
                        "name": "D. Anguita"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Fairness and robustness: Nevertheless, relations between robustness guarantees and individual and group fairness have been studied in [44, 81, 83, 84] and in [72] respectively."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "449f9165dcdc4d6f3e911f3700c6a4c090f43855",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-01034",
                    "CorpusId": 246473096
                },
                "corpusId": 246473096,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/449f9165dcdc4d6f3e911f3700c6a4c090f43855",
                "title": "Maintaining fairness across distribution shift: do we have viable solutions for real-world applications?",
                "abstract": "Fairness and robustness are often considered as orthogonal dimensions when evaluating machine learning models. However, recent work has revealed interactions between fairness and robustness, showing that fairness properties are not necessarily maintained under distribution shift. In healthcare settings, this can result in e.g. a model that performs fairly according to a selected metric in \u201chospital A\u201d showing unfairness when deployed in \u201chospital B\u201d. While a nascent field has emerged to develop provable fair and robust models, it typically relies on strong assumptions about the shift, limiting its impact for real-world applications. In this work, we explore the settings in which recently proposed mitigation strategies are applicable by referring to a causal framing. Using examples of predictive models in dermatology and electronic health records, we show that real-world applications are complex and often invalidate the assumptions of such methods. Our work hence highlights technical, practical, and engineering gaps that prevent the development of robustly fair machine learning models for real-world applications. Finally, we discuss potential remedies at each step of the machine learning pipeline.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3212089",
                        "name": "J. Schrouff"
                    },
                    {
                        "authorId": "2088732955",
                        "name": "Natalie Harris"
                    },
                    {
                        "authorId": "143812875",
                        "name": "O. Koyejo"
                    },
                    {
                        "authorId": "2922782",
                        "name": "Ibrahim M. Alabdulmohsin"
                    },
                    {
                        "authorId": "1988972565",
                        "name": "Eva Schnider"
                    },
                    {
                        "authorId": "1398933345",
                        "name": "Krista Opsahl-Ong"
                    },
                    {
                        "authorId": "2152100384",
                        "name": "Alex Brown"
                    },
                    {
                        "authorId": "2109961729",
                        "name": "Subhrajit Roy"
                    },
                    {
                        "authorId": "2007712128",
                        "name": "Diana Mincu"
                    },
                    {
                        "authorId": "2110195795",
                        "name": "Christina Chen"
                    },
                    {
                        "authorId": "2064110017",
                        "name": "Awa Dieng"
                    },
                    {
                        "authorId": "2143862099",
                        "name": "Yuan Liu"
                    },
                    {
                        "authorId": "144223091",
                        "name": "Vivek Natarajan"
                    },
                    {
                        "authorId": "6413143",
                        "name": "A. Karthikesalingam"
                    },
                    {
                        "authorId": "49626272",
                        "name": "Katherine C. Heller"
                    },
                    {
                        "authorId": "48880818",
                        "name": "S. Chiappa"
                    },
                    {
                        "authorId": "1396841807",
                        "name": "A. D'Amour"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As baselines, 7 adversarial training methods are chosen, i.e., Fast Adversarial Training (Fast-AT) [5], You Only Propagate Once (YOPO) [8], Adversarial Training with Hypersphere Embedding (ATHE) [4], Fair Robust Learning (FRL) [9], Friendly Adversarial Training (FAT) [6], TRADES [10] and Adversarial Training with Transferable Adversarial examples (ATTA) [7], and 2 channel-wise activation suppressing methods, i.e., Channelwise Activation Suppressing (CAS) [11] and Channel-wise Importance-based Feature Selection (CIFS) [12].",
                ", Fast Adversarial Training (Fast-AT) [5], You Only Propagate Once (YOPO) [8], Adversarial Training with Hypersphere Embedding (ATHE) [4], Fair Robust Learning (FRL) [9], Friendly Adversarial Training (FAT) [6], TRADES [10] and Adversarial Training with Transferable Adversarial examples (ATTA) [7], and 2 channel-wise activation suppressing methods, i.",
                "For example, Adversarial Training with Hypersphere Embedding (ATHE) [4], Fast Adversarial Training (Fast-AT) [5], Friendly Adversarial Training (FAT) [6], Adversarial Training with Transferable Adversarial examples (ATTA) [7], You Only Propagate Once (YOPO) [8], FairRobust-Learning (FRL) [9], TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization (TRADES) [10] were proposed to enhance the robustness of DNN."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "233de57c82ffc74e65b529fc19dab55196fc1287",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-00924",
                    "DOI": "10.48550/arXiv.2206.00924",
                    "CorpusId": 249282251
                },
                "corpusId": 249282251,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/233de57c82ffc74e65b529fc19dab55196fc1287",
                "title": "FACM: Correct the Output of Deep Neural Network with Middle Layers Features against Adversarial Samples",
                "abstract": "In the strong adversarial attacks against deep neural network (DNN), the output of DNN will be misclassified if and only if the last feature layer of the DNN is completely destroyed by adversarial samples, while our studies found that the middle feature layers of the DNN can still extract the effective features of the original normal category in these adversarial attacks. To this end, in this paper, a middle Feature layer Analysis and Conditional Matching prediction distribution (FACM) model is proposed to increase the robustness of the DNN against adversarial samples through correcting the output of DNN with the features extracted by the middle layers of DNN. In particular, the middle Feature layer Analysis (FA) module, the conditional matching prediction distribution (CMPD) module and the output decision module are included in our FACM model to collaboratively correct the classification of adversarial samples. The experiments results show that, our FACM model can significantly improve the robustness of the naturally trained model against various attacks, and our FA model can significantly improve the robustness of the adversarially trained model against white-box attacks with weak transferability and black box attacks where FA model includes the FA module and the output decision module, not the CMPD module.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2165758695",
                        "name": "Xiangyuan Yang"
                    },
                    {
                        "authorId": "66190968",
                        "name": "Jie Lin"
                    },
                    {
                        "authorId": "120811666",
                        "name": "Han Zhang"
                    },
                    {
                        "authorId": "2595119",
                        "name": "Xinyu Yang"
                    },
                    {
                        "authorId": "2087254926",
                        "name": "Peng Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[34] proposes the Fair-Robust-Learning (FRL) algorithm to alleviate this problem.",
                "[34] reports that TRADES [36] increases the variation of the per-class accuracies (accuracy in each class) which is not desirable in view of fairness."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "491902c2041677d97b90153f6da99c0a24ef0010",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-03353",
                    "DOI": "10.48550/arXiv.2206.03353",
                    "CorpusId": 249431879
                },
                "corpusId": 249431879,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/491902c2041677d97b90153f6da99c0a24ef0010",
                "title": "Adaptive Regularization for Adversarial Training",
                "abstract": "Adversarial training, which is to enhance robustness against adversarial attacks, has received much attention because it is easy to generate human-imperceptible perturbations of data to deceive a given deep neural network. In this paper, we propose a new adversarial training algorithm that is theoretically well motivated and empirically superior to other existing algorithms. A novel feature of the proposed algorithm is to use a data-adaptive regularization for robustifying a prediction model. We apply more regularization to data which are more vulnerable to adversarial attacks and vice versa. Even though the idea of data-adaptive regularization is not new, our data-adaptive regularization has a \ufb01rm theoretical base of reducing an upper bound of the robust risk. Numerical experiments illustrate that our proposed algorithm improves the generalization (accuracy on clean samples) and robustness (accuracy on adversarial attacks) simultaneously to achieve the state-of-the-art performance. minimizing proposed data-adaptive regularized empirical risk. By analyzing benchmark data sets, simultaneously to in the compared to a training",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2167315030",
                        "name": "Dong-Sheng Yang"
                    },
                    {
                        "authorId": "2153469078",
                        "name": "Insung Kong"
                    },
                    {
                        "authorId": "1763945",
                        "name": "Yongdai Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[33] and [34] confirmed this finding and introduced algorithms that decrease the robustness bias by introducing class-weighted losses in the popular PGD-AT [35] and TRADES [36] algorithms for adversarial robust machine learning."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d487d4e3cf6b2e321b29dfdcefc3e49c483c4065",
                "externalIds": {
                    "DBLP": "journals/access/GittensYY22",
                    "DOI": "10.1109/ACCESS.2022.3218715",
                    "CorpusId": 253368350
                },
                "corpusId": 253368350,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d487d4e3cf6b2e321b29dfdcefc3e49c483c4065",
                "title": "An Adversarial Perspective on Accuracy, Robustness, Fairness, and Privacy: Multilateral-Tradeoffs in Trustworthy ML",
                "abstract": "Model accuracy is the traditional metric employed in machine learning (ML) applications. However, privacy, fairness, and robustness guarantees are crucial as ML algorithms increasingly pervade our lives and play central roles in socially important systems. These four desiderata constitute the pillars of Trustworthy ML (TML) and may mutually inhibit or reinforce each other. It is necessary to understand and clearly delineate the trade-offs among these desiderata in the presence of adversarial attacks. However, threat models for the desiderata are different and the defenses introduced for each leads to further trade-offs in a multilateral adversarial setting (i.e., a setting attacking several pillars simultaneously). The first half of the paper reviews the state of the art in TML research, articulates known multilateral trade-offs, and identifies open problems and challenges in the presence of an adversary that may take advantage of such multilateral trade-offs. The fundamental shortcomings of statistical association-based TML are discussed, to motivate the use of causal methods to achieve TML. The second half of the paper, in turn, advocates the use of causal modeling in TML. Evidence is collected from across the literature that causal ML is well-suited to provide a unified approach to TML. Causal discovery and causal representation learning are introduced as essential stages of causal modeling, and a new threat model for causal ML is introduced to quantify the vulnerabilities introduced through the use of causal methods. The paper concludes with pointers to possible next steps in the development of a causal TML pipeline.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9545803",
                        "name": "Alex Gittens"
                    },
                    {
                        "authorId": "7834060",
                        "name": "B. Yener"
                    },
                    {
                        "authorId": "144068857",
                        "name": "M. Yung"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[41] that adversarial defences may induce a large discrepancy of robustness among different classes."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9d4b940845a4694d48e5d5cc970947c85182b010",
                "externalIds": {
                    "CorpusId": 254610626
                },
                "corpusId": 254610626,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9d4b940845a4694d48e5d5cc970947c85182b010",
                "title": "Certified defences can hurt generalisation",
                "abstract": "In recent years, much work has been devoted to designing certified defences for neural networks, i.e., methods for learning neural networks that are provably robust to certain adversarial perturbations. Due to the non-convexity of the problem, dominant approaches in this area rely on convex approximations, which are inherently loose. In this paper, we question the effectiveness of such approaches for realistic computer vision tasks. First, we provide extensive empirical evidence to show that certified defences suffer not only worse accuracy but also worse robustness and fairness than empirical defences. We hypothesise that the reason for why certified defences suffer in generalisation is (i) the large number of relaxed non-convex constraints and (ii) the strong alignment between the adversarial perturbations and the \u201csignal\u201d direction. We provide a combination of theoretical and experimental evidence to support these hypotheses.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152233139",
                        "name": "Piersilvio De Bartolomeis"
                    },
                    {
                        "authorId": "2157427905",
                        "name": "Jacob Clarysse"
                    },
                    {
                        "authorId": "2151252957",
                        "name": "Fanny Yang"
                    },
                    {
                        "authorId": "3494481",
                        "name": "Amartya Sanyal"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The phenomenon of class-wise accuracy has also been discovered by other works concurrently to ours [43, 36, 3].",
                "Similar to our approach the fair robust learning framework [43] also attempts to train robust models with a balanced accuracy and robustness performance."
            ],
            "intents": [
                "result"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "765ceae83fbb2411141a38caaef9a2318aa81c96",
                "externalIds": {
                    "CorpusId": 235804750
                },
                "corpusId": 235804750,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/765ceae83fbb2411141a38caaef9a2318aa81c96",
                "title": "Trade-off Between Accuracy, Robustness, and Fairness of Deep Classifiers",
                "abstract": "Deep Neural Networks (DNNs) have achieved great success, however, their vulnerability to adversarial examples remains an open issue. Among numerous attempts to increase the robustness of deep classifiers, mainly adversarial training has stood the test of time as a useful defense technique. It has been shown that the increased model robustness comes at the cost of decreased accuracy. At the same time, deep classifiers trained on balanced datasets exhibit a class-wise imbalance, which is even more severe for adversarially trained models. This work aims to highlight that the fairness of classifiers should not be neglected when evaluating DNNs. To this end, we propose a class-wise loss re-weighting to obtain more fair standard and robust classifiers. The final results suggest, that fairness as well comes at the cost of accuracy and robustness, suggesting that there exists a triangular trade-off between accuracy, robustness, and fairness.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47374754",
                        "name": "Philipp Benz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For instance, parallels can be shown between individual fairness and local robustness guarantees (Yurochkin et al., 2019; Nanda et al., 2021; Xu et al., 2021; Yeom and Fredrikson, 2020) or between group fairness metrics and robustness to distribution shift (Veitch et al.",
                "For instance, parallels can be shown between individual fairness and local robustness guarantees (Yurochkin et al., 2019; Nanda et al., 2021; Xu et al., 2021; Yeom and Fredrikson, 2020) or between group fairness metrics and robustness to distribution shift (Veitch et al., 2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fbaf3a98c9e244aadb6c0a0d9680a2a00bf719ab",
                "externalIds": {
                    "DBLP": "conf/afci/SchrouffDRKF21",
                    "CorpusId": 250534119
                },
                "corpusId": 250534119,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fbaf3a98c9e244aadb6c0a0d9680a2a00bf719ab",
                "title": "Algorithmic Fairness through the Lens of Causality and Robustness (AFCR) 2021",
                "abstract": "Trustworthy machine learning (ML) encompasses multiple fields of research, including (but not limited to) robustness, algorithmic fairness, interpretability and privacy. Recently, relationships between techniques and metrics used across different fields of trustworthy ML have emerged, leading to interesting work at the intersection of algorithmic fairness, robustness, and causality. On one hand, causality has been proposed as a powerful tool to address the limitations of initial statistical definitions of fairness (Kusner et al., 2017; Chiappa, 2019; Khademi et al., 2019; Wu et al., 2019). However, questions have emerged regarding 1) the applicability of such approaches due to strong assumptions inherent to causal questions (Kilbertus et al., 2019) and 2) the suitability of a causal framing for studies of bias and discrimination (Kohler-Hausmann, 2019; Hu and Kohler-Hausmann, 2020; Kasirzadeh and Smart, 2021). On the other hand, the robustness literature has surfaced promising approaches to improve fairness in ML models. For instance, parallels can be shown between individual fairness and local robustness guarantees (Yurochkin et al., 2019; Nanda et al., 2021; Xu et al., 2021; Yeom and Fredrikson, 2020) or between group fairness metrics and robustness to distribution shift (Veitch et al., 2021). Beyond similarities, the interactions between fairness and robustness can help us understand how fairness guarantees hold under distribution shift (Singh et al., 2021; Subbaswamy and Saria, 2020) or adversarial/poisoning attacks (Solans et al., 2020; Liu et al., 2021), leading to fair and robust ML models. To encourage further work at the intersection of these fields, we organized The Algorithmic Fairness through the Lens of Causality and Robustness workshop (AFCR\u2217) as part of https://www.afciworkshop.org/",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3212089",
                        "name": "J. Schrouff"
                    },
                    {
                        "authorId": "2064110017",
                        "name": "Awa Dieng"
                    },
                    {
                        "authorId": "2135375871",
                        "name": "Miriam Rateike"
                    },
                    {
                        "authorId": "2051333263",
                        "name": "K. Kwegyir-Aggrey"
                    },
                    {
                        "authorId": "2086602",
                        "name": "G. Farnadi"
                    }
                ]
            }
        }
    ]
}