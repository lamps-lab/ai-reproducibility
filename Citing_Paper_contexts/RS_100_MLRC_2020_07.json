{
    "offset": 0,
    "data": [
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "GNNExplainer [57] Continuous relaxation Mutual Information Size Yes GC+NC Transductive PGExplainer [30] Parameterized edge selection Mutual Information Size and/or connectivity No GC+NC Inductive TAGExplainer [51] Sampling Mutual Information Size, Entropy No GC+NC Inductive GEM [27] Granger Causality+Autoencoder Causal Contribution Size, Connectivity No GC+NC Inductive SubgraphX [62] Monte Carlo Tree Search Shapley Value Size, connectivity No GC Transductive GstarX [63] Monte Carlo sampling HN-value Size No GC Inductive \u2022 Empirical investigations: How susceptible are the explanations to topological noise, variations in G NN architectures, or optimization stochasticity?",
                "\u2022 Perturbation-based: These methods [59, 30, 62, 18, 29, 43, 27, 6, 31, 1, 50] utilize perturbations of the input to identify important subgraphs that serve as factual or counterfactual explanations.",
                "However, GraphFrameX and GraphXAI collectively assess only GnnExplainer [59], PGExplainer [30], and SubgraphX [62].",
                "In a follow-up work, PGExplainer [30] extends the same idea with an additional assumption of the graph to be a random Gilbert graph.",
                "Gradient: SA [7] , Guided-BP [7] , Grad-CAM [33]; Decomposition: Excitation-BP [33], GNN-LRP [38], CAM [33]; Perturbation: GNNExplainer [59], PGExplainer [30], SubgraphX [62], GEM [27], TAGExplainer [51], CF(2) [43], RCExplainer [6],CF-GNNexplainer [29], CLEAR [31]; Surrogate: GraphLime [18], Relex [64], PGM-Explainer [47]; Global: XGNN [60], GLG-Explainer [5], Xuanyuan et al.",
                "Explanations can be broadly classified into two categories: factual reasoning [59, 30, 40, 62, 18] and counterfactual reasoning [29, 43, 31, 6, 1, 50].",
                "Gradient: SA [7] , Guided-BP [7] , Grad-CAM [33]; Decomposition: Excitation-BP [33], GNN-LRP [38], CAM [33]; Perturbation: GNNExplainer [59], PGExplainer [30], SubgraphX [62], GEM [27], TAGExplainer [51], CF 2 [43], RCExplainer [6],CF-GNNexplainer [29], CLEAR [31]; Surro-gate: GraphLime [18], Relex [64], PGM-Explainer [47]; Global: XGNN [60], GLG-Explainer [5], Xuanyuan et al. [54], GCFExplainer [19].",
                "\u2022 Instance-level: Instance-level or local explainers [59, 30, 40, 62, 18, 61, 29, 43, 27, 6, 1, 50] provide explanations for specific predictions made by a model.",
                "GNNExplainer [57] Continuous relaxation Mutual Information Size Yes GC+NC Transductive PGExplainer [30] Parameterized edge selection Mutual Information Size and/or connectivity No GC+NC Inductive TAGExplainer [51] Sampling Mutual Information Size, Entropy No GC+NC Inductive GEM [27] Granger Causality+Autoencoder Causal Contribution Size, Connectivity No GC+NC Inductive SubgraphX [62] Monte Carlo Tree Search Shapley Value Size, connectivity No GC Transductive GstarX [63] Monte Carlo sampling HN-value Size No GC Inductive",
                "[30] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang."
            ],
            "citingPaper": {
                "paperId": "8733ec39a7c9d19fcc8fea902cae12b31268fafa",
                "externalIds": {
                    "ArXiv": "2310.01794",
                    "CorpusId": 263608316
                },
                "corpusId": 263608316,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8733ec39a7c9d19fcc8fea902cae12b31268fafa",
                "title": "GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking",
                "abstract": "Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. Nonetheless, our study reveals that all algorithms are affected by stability issues when faced with noisy data. Furthermore, we have established that the current generation of counterfactual explainers often fails to provide feasible recourses due to violations of topological constraints encoded by domain-specific considerations. Overall, this benchmarking study empowers stakeholders in the field of GNNs with a comprehensive understanding of the state-of-the-art explainability methods, potential research problems for further enhancement, and the implications of their application in real-world scenarios.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2134769449",
                        "name": "Mert Kosan"
                    },
                    {
                        "authorId": "143665702",
                        "name": "S. Verma"
                    },
                    {
                        "authorId": "2219690090",
                        "name": "Burouj Armgaan"
                    },
                    {
                        "authorId": "1491635783",
                        "name": "Khushbu Pahwa"
                    },
                    {
                        "authorId": "2238534218",
                        "name": "Ambuj Singh"
                    },
                    {
                        "authorId": "3390598",
                        "name": "Sourav Medya"
                    },
                    {
                        "authorId": "2253455409",
                        "name": "Sayan Ranu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "cc92ae2ce3caf2142a92bed5957a8d5519215eb7",
                "externalIds": {
                    "ArXiv": "2310.01820",
                    "CorpusId": 263609186
                },
                "corpusId": 263609186,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cc92ae2ce3caf2142a92bed5957a8d5519215eb7",
                "title": "Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) are neural models that leverage the dependency structure in graphical data via message passing among the graph nodes. GNNs have emerged as pivotal architectures in analyzing graph-structured data, and their expansive application in sensitive domains requires a comprehensive understanding of their decision-making processes -- necessitating a framework for GNN explainability. An explanation function for GNNs takes a pre-trained GNN along with a graph as input, to produce a `sufficient statistic' subgraph with respect to the graph label. A main challenge in studying GNN explainability is to provide fidelity measures that evaluate the performance of these explanation functions. This paper studies this foundational challenge, spotlighting the inherent limitations of prevailing fidelity metrics, including $Fid_+$, $Fid_-$, and $Fid_\\Delta$. Specifically, a formal, information-theoretic definition of explainability is introduced and it is shown that existing metrics often fail to align with this definition across various statistical scenarios. The reason is due to potential distribution shifts when subgraphs are removed in computing these fidelity measures. Subsequently, a robust class of fidelity measures are introduced, and it is shown analytically that they are resilient to distribution shift issues and are applicable in a wide range of scenarios. Extensive empirical analysis on both synthetic and real datasets are provided to illustrate that the proposed metrics are more coherent with gold standard metrics.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2226672788",
                        "name": "Xu Zheng"
                    },
                    {
                        "authorId": "2253469454",
                        "name": "Farhad Shirani"
                    },
                    {
                        "authorId": "2253432775",
                        "name": "Tianchun Wang"
                    },
                    {
                        "authorId": "2249879747",
                        "name": "Wei Cheng"
                    },
                    {
                        "authorId": "2253867973",
                        "name": "Zhuomin Chen"
                    },
                    {
                        "authorId": "2256237444",
                        "name": "Haifeng Chen"
                    },
                    {
                        "authorId": "2253655246",
                        "name": "Hua Wei"
                    },
                    {
                        "authorId": "2226519756",
                        "name": "Dongsheng Luo"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "e050afb5adba1f2aa191060c4ca111e1c59f0e0d",
                "externalIds": {
                    "DBLP": "journals/isci/GarciaSiguenzaLTV23",
                    "DOI": "10.1016/j.ins.2023.119320",
                    "CorpusId": 259477587
                },
                "corpusId": 259477587,
                "publicationVenue": {
                    "id": "e46002a1-d7a6-4681-aae9-36bc3a6a1f93",
                    "name": "Information Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Information Scientist",
                        "Inf Sci"
                    ],
                    "issn": "0020-0255",
                    "alternate_issns": [
                        "0020-0263"
                    ],
                    "url": "http://www.sciencedirect.com/science/journal/00200255"
                },
                "url": "https://www.semanticscholar.org/paper/e050afb5adba1f2aa191060c4ca111e1c59f0e0d",
                "title": "Explainability techniques applied to road traffic forecasting using Graph Neural Network models",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220836224",
                        "name": "Javier Garc\u00eda-Sig\u00fcenza"
                    },
                    {
                        "authorId": "1397303114",
                        "name": "F. Llorens-Largo"
                    },
                    {
                        "authorId": "2917011",
                        "name": "L. Tortosa"
                    },
                    {
                        "authorId": "31971532",
                        "name": "Jos\u00e9-Francisco Vicent"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Specifically, in the CFAD framework, we first devise a Granger causality-based [19] causal explanation method to extract the core subgraph for a given node without requiring annotation information, according to the idea that causal explanations can extract a subgraph which is considered the main cause of the corresponding prediction [17], [20]."
            ],
            "citingPaper": {
                "paperId": "42bd7c26f22e888393c5fd3ed2d31d109e0dcdd6",
                "externalIds": {
                    "DBLP": "journals/tkde/XiaoXLZLZ23",
                    "DOI": "10.1109/TKDE.2023.3250523",
                    "CorpusId": 257276566
                },
                "corpusId": 257276566,
                "publicationVenue": {
                    "id": "c6840156-ee10-4d78-8832-7f8909811576",
                    "name": "IEEE Transactions on Knowledge and Data Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Knowl Data Eng"
                    ],
                    "issn": "1041-4347",
                    "url": "https://www.computer.org/web/tkde",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=69"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/42bd7c26f22e888393c5fd3ed2d31d109e0dcdd6",
                "title": "Counterfactual Graph Learning for Anomaly Detection on Attributed Networks",
                "abstract": "Graph anomaly detection is attracting remarkable multidisciplinary research interests ranging from finance, healthcare, and social network analysis. Recent advances on graph neural networks have substantially improved the detection performance via semi-supervised representation learning. However, prior work suggests that deep graph-based methods tend to learn spurious correlations. As a result, they fail to generalize beyond training data distribution. In this article, we aim to identify structural and contextual anomaly nodes in an attributed graph. Based on our preliminary data analyses, spurious correlations can be eliminated with causal subgraph interventions. Therefore, we propose a new graph-based anomaly detection model that can learn causal relations for anomaly detection while generalizing to new environments. To handle situations with varying environments, we steer the generative model to manufacture synthetic environment features, which are exerted on realistic subgraphs to generate counterfactual subgraphs. Further, these counterfactual subgraphs help a few-shot anomaly detection model learn transferable and causal relations across different environments. The experiments on three real-world attributed graphs show that the proposed approach achieves the best performance compared to the state-of-the-art baselines and learns robust causal representations resistant to noises and spurious correlations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2119300162",
                        "name": "Chunjing Xiao"
                    },
                    {
                        "authorId": "1593903641",
                        "name": "Xovee Xu"
                    },
                    {
                        "authorId": "2113652437",
                        "name": "Yue Lei"
                    },
                    {
                        "authorId": "2737830",
                        "name": "Kunpeng Zhang"
                    },
                    {
                        "authorId": "2210327727",
                        "name": "Siyuan Liu"
                    },
                    {
                        "authorId": "144315735",
                        "name": "Fan Zhou"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "GNN explainers such as GNNExplainer [25], XGNN [26], and PGExplainer [13] have gained increasing attention in the field of explainable artificial intelligence (XAI), which attempts to identify the most important graph structures and/or features that contribute to GNNs\u2019 predictions.",
                "GNNexplainers such as GNNExplainer [25], XGNN [26], and PGExplainer [13] have gained increasing attention in the field of ex-plainable artificial intelligence (XAI), which attempts to identify the most important graph structures and/or features that contribute to GNNs\u2019 predictions.",
                "PGExplainer [13] generates explanations for GNN models by using a probabilistic graph."
            ],
            "citingPaper": {
                "paperId": "5bbd1b9f44d320f1c6fa84aec9a48add0b1a0294",
                "externalIds": {
                    "ArXiv": "2309.16918",
                    "DOI": "10.1145/3583780.3614772",
                    "CorpusId": 263310884
                },
                "corpusId": 263310884,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5bbd1b9f44d320f1c6fa84aec9a48add0b1a0294",
                "title": "ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks",
                "abstract": "Graph neural networks (GNNs) have proven their efficacy in a variety of real-world applications, but their underlying mechanisms remain a mystery. To address this challenge and enable reliable decision-making, many GNN explainers have been proposed in recent years. However, these methods often encounter limitations, including their dependence on specific instances, lack of generalizability to unseen graphs, producing potentially invalid explanations, and yielding inadequate fidelity. To overcome these limitations, we, in this paper, introduce the Auxiliary Classifier Generative Adversarial Network (ACGAN) into the field of GNN explanation and propose a new GNN explainer dubbed~\\emph{ACGAN-GNNExplainer}. Our approach leverages a generator to produce explanations for the original input graphs while incorporating a discriminator to oversee the generation process, ensuring explanation fidelity and improving accuracy. Experimental evaluations conducted on both synthetic and real-world graph datasets demonstrate the superiority of our proposed method compared to other existing GNN explainers.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2179302425",
                        "name": "Yiqiao Li"
                    },
                    {
                        "authorId": "51239629",
                        "name": "Jianlong Zhou"
                    },
                    {
                        "authorId": "2249599312",
                        "name": "Yifei Dong"
                    },
                    {
                        "authorId": "2716743",
                        "name": "N. Shafiabady"
                    },
                    {
                        "authorId": "2249757048",
                        "name": "Fang Chen"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Like most GNN-interpretability methods (Luo et al., 2020; Ying et al., 2019; Yuan et al., 2020), we limit our interpretability analysis to topology.",
                "Interestingly, this objective correlates with the aim of GNN interpretability, which is to identify highly influential sparse subsets of nodes and edges with the largest impact on the model\u2019s behaviour (Luo et al., 2020).",
                ", 2014), perturbations of the input graph (Ying et al., 2019; Luo et al., 2020), and the training of a surrogate model (Vu and Thai, 2020) are examples of such techniques.",
                "Then, different motifs are added to each random graph (Wu et al., 2022; Ying et al., 2019; Luo et al., 2020) depending on the dataset.",
                "Datasets For graph classification, we use two synthetic datasets (BA2Motifs (Luo et al., 2020), SPMotifs."
            ],
            "citingPaper": {
                "paperId": "0e4ee9e1445d7359de7b78cc02b99b78e41eb7f2",
                "externalIds": {
                    "ArXiv": "2309.16564",
                    "CorpusId": 263131428
                },
                "corpusId": 263131428,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0e4ee9e1445d7359de7b78cc02b99b78e41eb7f2",
                "title": "Augment to Interpret: Unsupervised and Inherently Interpretable Graph Embeddings",
                "abstract": "Unsupervised learning allows us to leverage unlabelled data, which has become abundantly available, and to create embeddings that are usable on a variety of downstream tasks. However, the typical lack of interpretability of unsupervised representation learning has become a limiting factor with regard to recent transparent-AI regulations. In this paper, we study graph representation learning and we show that data augmentation that preserves semantics can be learned and used to produce interpretations. Our framework, which we named INGENIOUS, creates inherently interpretable embeddings and eliminates the need for costly additional post-hoc analysis. We also introduce additional metrics addressing the lack of formalism and metrics in the understudied area of unsupervised-representation learning interpretability. Our results are supported by an experimental study applied to both graph-level and node-level tasks and show that interpretable embeddings provide state-of-the-art performance on subsequent downstream tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2175251459",
                        "name": "Gregory Scafarto"
                    },
                    {
                        "authorId": "2079393523",
                        "name": "Madalina Ciortan"
                    },
                    {
                        "authorId": "2248163246",
                        "name": "Simon Tihon"
                    },
                    {
                        "authorId": "2248167843",
                        "name": "Quentin Ferre"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "66ce2f0bf2123f9e4965d0b244d435d0140c933c",
                "externalIds": {
                    "ArXiv": "2309.16223",
                    "CorpusId": 263136272
                },
                "corpusId": 263136272,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/66ce2f0bf2123f9e4965d0b244d435d0140c933c",
                "title": "GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations",
                "abstract": "Diverse explainability methods of graph neural networks (GNN) have recently been developed to highlight the edges and nodes in the graph that contribute the most to the model predictions. However, it is not clear yet how to evaluate the correctness of those explanations, whether it is from a human or a model perspective. One unaddressed bottleneck in the current evaluation procedure is the problem of out-of-distribution explanations, whose distribution differs from those of the training data. This important issue affects existing evaluation metrics such as the popular faithfulness or fidelity score. In this paper, we show the limitations of faithfulness metrics. We propose GInX-Eval (Graph In-distribution eXplanation Evaluation), an evaluation procedure of graph explanations that overcomes the pitfalls of faithfulness and offers new insights on explainability methods. Using a retraining strategy, the GInX score measures how informative removed edges are for the model and the EdgeRank score evaluates if explanatory edges are correctly ordered by their importance. GInX-Eval verifies if ground-truth explanations are instructive to the GNN model. In addition, it shows that many popular methods, including gradient-based methods, produce explanations that are not better than a random designation of edges as important subgraphs, challenging the findings of current works in the area. Results with GInX-Eval are consistent across multiple datasets and align with human evaluation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2146257620",
                        "name": "Kenza Amara"
                    },
                    {
                        "authorId": "1401917601",
                        "name": "Mennatallah El-Assady"
                    },
                    {
                        "authorId": "2248206514",
                        "name": "Rex Ying"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "9291ff5674f9ab7ddff964f71285491cb5065d05",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-13391",
                    "ArXiv": "2309.13391",
                    "DOI": "10.48550/arXiv.2309.13391",
                    "CorpusId": 262459256
                },
                "corpusId": 262459256,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9291ff5674f9ab7ddff964f71285491cb5065d05",
                "title": "D-Separation for Causal Self-Explanation",
                "abstract": "Rationalization is a self-explaining framework for NLP models. Conventional work typically uses the maximum mutual information (MMI) criterion to find the rationale that is most indicative of the target label. However, this criterion can be influenced by spurious features that correlate with the causal rationale or the target label. Instead of attempting to rectify the issues of the MMI criterion, we propose a novel criterion to uncover the causal rationale, termed the Minimum Conditional Dependence (MCD) criterion, which is grounded on our finding that the non-causal features and the target label are \\emph{d-separated} by the causal rationale. By minimizing the dependence between the unselected parts of the input and the target label conditioned on the selected rationale candidate, all the causes of the label are compelled to be selected. In this study, we employ a simple and practical measure of dependence, specifically the KL-divergence, to validate our proposed MCD criterion. Empirically, we demonstrate that MCD improves the F1 score by up to $13.7\\%$ compared to previous state-of-the-art MMI-based methods. Our code is available at: \\url{https://github.com/jugechengzi/Rationalization-MCD}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2157221142",
                        "name": "Wei Liu"
                    },
                    {
                        "authorId": "2152813098",
                        "name": "Jun Wang"
                    },
                    {
                        "authorId": "51175126",
                        "name": "Haozhao Wang"
                    },
                    {
                        "authorId": "145765726",
                        "name": "Rui Li"
                    },
                    {
                        "authorId": "2246203688",
                        "name": "Zhiying Deng"
                    },
                    {
                        "authorId": "1703465",
                        "name": "Yuan Zhang"
                    },
                    {
                        "authorId": "2114966139",
                        "name": "Yang Qiu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Following works [99, 184] further introduce novel combination strategies and motifs for graph construction."
            ],
            "citingPaper": {
                "paperId": "911247dddb66d1f3ef3ba3235861bd707d15b9ad",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-10979",
                    "ArXiv": "2309.10979",
                    "DOI": "10.48550/arXiv.2309.10979",
                    "CorpusId": 262067202
                },
                "corpusId": 262067202,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/911247dddb66d1f3ef3ba3235861bd707d15b9ad",
                "title": "Towards Data-centric Graph Machine Learning: Review and Outlook",
                "abstract": "Data-centric AI, with its primary focus on the collection, management, and utilization of data to drive AI models and applications, has attracted increasing attention in recent years. In this article, we conduct an in-depth and comprehensive review, offering a forward-looking outlook on the current efforts in data-centric AI pertaining to graph data-the fundamental data structure for representing and capturing intricate dependencies among massive and diverse real-life entities. We introduce a systematic framework, Data-centric Graph Machine Learning (DC-GML), that encompasses all stages of the graph data lifecycle, including graph data collection, exploration, improvement, exploitation, and maintenance. A thorough taxonomy of each stage is presented to answer three critical graph-centric questions: (1) how to enhance graph data availability and quality; (2) how to learn from graph data with limited-availability and low-quality; (3) how to build graph MLOps systems from the graph data-centric view. Lastly, we pinpoint the future prospects of the DC-GML domain, providing insights to navigate its advancements and applications.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1430762233",
                        "name": "Xin Zheng"
                    },
                    {
                        "authorId": "2242962967",
                        "name": "Yixin Liu"
                    },
                    {
                        "authorId": "2242943639",
                        "name": "Zhifeng Bao"
                    },
                    {
                        "authorId": "2242950900",
                        "name": "Meng Fang"
                    },
                    {
                        "authorId": "2243358879",
                        "name": "Xia Hu"
                    },
                    {
                        "authorId": "2243282363",
                        "name": "Alan Wee-Chung Liew"
                    },
                    {
                        "authorId": "2585415",
                        "name": "Shirui Pan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "To tackle these challenges, PGExplainer [29] aims to address the drawbacks associated with GNNExplainer.",
                "Various recent research endeavors that have devised explanation methods for GNNs, consistently emphasize the interpretability aspects at the levels of nodes, edges, or node features [29, 44, 53].",
                "(2) There is a lack of research on developing efficient algorithms for subgraph extraction that is one of the leading explanation methods for GNNs [29, 53, 56]."
            ],
            "citingPaper": {
                "paperId": "1a8de3ce09316fa190b949540a756943f02851f7",
                "externalIds": {
                    "DBLP": "conf/recsys/Mohammadi23",
                    "DOI": "10.1145/3604915.3608875",
                    "CorpusId": 261823754
                },
                "corpusId": 261823754,
                "publicationVenue": {
                    "id": "61275a16-1e0d-479f-ac4e-f295310761f0",
                    "name": "ACM Conference on Recommender Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Recomm Syst",
                        "RecSys",
                        "ACM Conf Recomm Syst",
                        "Conference on Recommender Systems"
                    ],
                    "url": "http://recsys.acm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1a8de3ce09316fa190b949540a756943f02851f7",
                "title": "Explainable Graph Neural Network Recommenders; Challenges and Opportunities",
                "abstract": "Graph Neural Networks (GNNs) have demonstrated significant potential in recommendation tasks by effectively capturing intricate connections among users, items, and their associated features. Given the escalating demand for interpretability, current research endeavors in the domain of GNNs for Recommender Systems (RecSys) necessitate the development of explainer methodologies to elucidate the decision-making process underlying GNN-based recommendations. In this work, we aim to present our research focused on techniques to extend beyond the existing approaches for addressing interpretability in GNN-based RecSys.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2240537973",
                        "name": "Amir Reza Mohammadi"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "c52c7c0d72d882f9c7f7659e9e77b78e6093fdfb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-04332",
                    "ArXiv": "2309.04332",
                    "DOI": "10.48550/arXiv.2309.04332",
                    "CorpusId": 261660243
                },
                "corpusId": 261660243,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c52c7c0d72d882f9c7f7659e9e77b78e6093fdfb",
                "title": "Graph Neural Networks Use Graphs When They Shouldn't",
                "abstract": "Predictions over graphs play a crucial role in various domains, including social networks, molecular biology, medicine, and more. Graph Neural Networks (GNNs) have emerged as the dominant approach for learning on graph data. Instances of graph labeling problems consist of the graph-structure (i.e., the adjacency matrix), along with node-specific feature vectors. In some cases, this graph-structure is non-informative for the predictive task. For instance, molecular properties such as molar mass depend solely on the constituent atoms (node features), and not on the molecular structure. While GNNs have the ability to ignore the graph-structure in such cases, it is not clear that they will. In this work, we show that GNNs actually tend to overfit the graph-structure in the sense that they use it even when a better solution can be obtained by ignoring it. We examine this phenomenon with respect to different graph distributions and find that regular graphs are more robust to this overfitting. We then provide a theoretical explanation for this phenomenon, via analyzing the implicit bias of gradient-descent-based learning of GNNs in this setting. Finally, based on our empirical and theoretical findings, we propose a graph-editing method to mitigate the tendency of GNNs to overfit graph-structures that should be ignored. We show that this method indeed improves the accuracy of GNNs across multiple benchmarks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1419980979",
                        "name": "Maya Bechler-Speicher"
                    },
                    {
                        "authorId": "2238621063",
                        "name": "Ido Amos"
                    },
                    {
                        "authorId": "2238621488",
                        "name": "Ran Gilad-Bachrach"
                    },
                    {
                        "authorId": "2238622411",
                        "name": "Amir Globerson"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Our implementations mainly follows the settings of officially public Pytorch code of PGExplainer [10] (https://github.com/divelab/DIG/tree/main/dig/ xgraph/PGExplainer).",
                "At \u03bb =50%, PGExplainer fails to identity key motifs (NO2), yet sucessfully does so at \u03bb =100%.",
                "We integrate two benchmark post-hoc explainers (GNNEXPLAINER [9] and PGEXPLAINER [10]) into a unified evaluation framework and carefully evaluate the effectiveness of explanations across four graph datasets, including two real-world datasets of different topics and two synthetic datasets.",
                "In response to the black-box limitations of GNNs, a range of GNN explainers have been introduced [9, 10, 17, 18].",
                "3 Empirical Study To address the aforementioned research questions, we evaluate the explanation quality in terms of Fid and Fid\u2212 of two benchmark post-hoc GNN explainers, GNNExplainer [9] and PGExplainer [10], on two GNN models, GCN [19] and GIN [20].",
                "We integrate the implementations of GCN and GIN from PyG [21] and GNNExplainer and PGExplainer from the original papers into a unified framework built with DIG [22].",
                "To address the aforementioned research questions, we evaluate the explanation quality in terms of Fid+ and Fid\u2212 of two benchmark post-hoc GNN explainers, GNNExplainer [9] and PGExplainer [10], on two GNN models, GCN [19] and GIN [20]."
            ],
            "citingPaper": {
                "paperId": "298bd76a12fefb8bf766d45121d7ebebe6346794",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-01706",
                    "ArXiv": "2309.01706",
                    "DOI": "10.48550/arXiv.2309.01706",
                    "CorpusId": 261530806
                },
                "corpusId": 261530806,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/298bd76a12fefb8bf766d45121d7ebebe6346794",
                "title": "On the Robustness of Post-hoc GNN Explainers to Label Noise",
                "abstract": "Proposed as a solution to the inherent black-box limitations of graph neural networks (GNNs), post-hoc GNN explainers aim to provide precise and insightful explanations of the behaviours exhibited by trained GNNs. Despite their recent notable advancements in academic and industrial contexts, the robustness of post-hoc GNN explainers remains unexplored when confronted with label noise. To bridge this gap, we conduct a systematic empirical investigation to evaluate the efficacy of diverse post-hoc GNN explainers under varying degrees of label noise. Our results reveal several key insights: Firstly, post-hoc GNN explainers are susceptible to label perturbations. Secondly, even minor levels of label noise, inconsequential to GNN performance, harm the quality of generated explanations substantially. Lastly, we engage in a discourse regarding the progressive recovery of explanation effectiveness with escalating noise levels.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2237806525",
                        "name": "Zhiqiang Zhong"
                    },
                    {
                        "authorId": "2237902963",
                        "name": "Yangqianzi Jiang"
                    },
                    {
                        "authorId": "2237805689",
                        "name": "Davide Mottin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "9e6ddeeac1924de902bec156969e76c701ac0cfa",
                "externalIds": {
                    "DOI": "10.1016/j.compchemeng.2023.108403",
                    "CorpusId": 261598084
                },
                "corpusId": 261598084,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9e6ddeeac1924de902bec156969e76c701ac0cfa",
                "title": "Graph neural networks with molecular segmentation for property prediction and structure\u2013property relationship discovery",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1430724399",
                        "name": "Zhudan Chen"
                    },
                    {
                        "authorId": "2093239",
                        "name": "Dazi Li"
                    },
                    {
                        "authorId": "2128166709",
                        "name": "Minghui Liu"
                    },
                    {
                        "authorId": "2157177153",
                        "name": "Jun Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "fff48be5e6ef0cab7e45198a768fc62b9e626be2",
                "externalIds": {
                    "DOI": "10.1016/j.websem.2023.100806",
                    "CorpusId": 261984082
                },
                "corpusId": 261984082,
                "publicationVenue": {
                    "id": "78d61825-c1d6-4e7f-ab9d-1525db71105c",
                    "name": "Journal of Web Semantics",
                    "type": "journal",
                    "alternate_names": [
                        "J Web Semant"
                    ],
                    "issn": "1570-8268",
                    "url": "http://www.elsevier.com/locate/websem",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/15708268",
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/671322/description"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fff48be5e6ef0cab7e45198a768fc62b9e626be2",
                "title": "Comprehensible Artificial Intelligence on Knowledge Graphs: A survey",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2166863949",
                        "name": "Simon Schramm"
                    },
                    {
                        "authorId": "2242636125",
                        "name": "Christoph Wehner"
                    },
                    {
                        "authorId": "1727734",
                        "name": "Ute Schmid"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "c35e0a63016016f7ccdc523536ce7ef2d7ab759e",
                "externalIds": {
                    "DOI": "10.1016/j.patcog.2023.109991",
                    "CorpusId": 263099976
                },
                "corpusId": 263099976,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c35e0a63016016f7ccdc523536ce7ef2d7ab759e",
                "title": "Towards self-explainable graph convolutional neural network with frequency adaptive inception",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2072970049",
                        "name": "Feifei Wei"
                    },
                    {
                        "authorId": "2247976984",
                        "name": "KuiZhi Mei"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Ba2Motif [34] and BAMultiShapes (BaMS for short) [35] are synthetic data sets."
            ],
            "citingPaper": {
                "paperId": "a62d4c1b2e4e59f1ef29230d78e5ba5a2571ca0b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-15096",
                    "ArXiv": "2308.15096",
                    "DOI": "10.48550/arXiv.2308.15096",
                    "CorpusId": 261276650
                },
                "corpusId": 261276650,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a62d4c1b2e4e59f1ef29230d78e5ba5a2571ca0b",
                "title": "How Faithful are Self-Explainable GNNs?",
                "abstract": "Self-explainable deep neural networks are a recent class of models that can output ante-hoc local explanations that are faithful to the model's reasoning, and as such represent a step forward toward filling the gap between expressiveness and interpretability. Self-explainable graph neural networks (GNNs) aim at achieving the same in the context of graph data. This begs the question: do these models fulfill their implicit guarantees in terms of faithfulness? In this extended abstract, we analyze the faithfulness of several self-explainable GNNs using different measures of faithfulness, identify several limitations -- both in the models themselves and in the evaluation metrics -- and outline possible ways forward.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2235062659",
                        "name": "Marc Christiansen"
                    },
                    {
                        "authorId": "2235064047",
                        "name": "Lea Villadsen"
                    },
                    {
                        "authorId": "47653501",
                        "name": "Zhiqiang Zhong"
                    },
                    {
                        "authorId": "2580338",
                        "name": "Stefano Teso"
                    },
                    {
                        "authorId": "3094226",
                        "name": "D. Mottin"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "They use different techniques to weigh the importance of the node (AttentiveFP [33]) or the edge (GNNExplainer [35] and PGExplainer [13]) in a graph.",
                "However, AttentiveFP [33], GNNExplainer [35] and PGExplainer [13] only generate the rank of the atoms in a drug.",
                "For AttentiveFP [33], GNNExplainer [35] and PGExplainer [13], we use the hyper-parameters suggested in the orginal papers.",
                "To make sure the key structures identified by different methods are comparable, for each drug, we count the number of atoms in the output of SLGNN and use the ranks generated in AttentiveFP [33], GNNExplainer [35] and PGExplainer [13] to pick the same number of atoms in the same drug.",
                "In order to provide such information, many explainable GNN models have been designed [13, 33, 35, 36].",
                "First of all, all the current explainable GNN models [13, 33, 35, 37] use atoms-based graphs, where nodes are atoms and edges are bonds between atoms.",
                "For GNNExplainer [35] and PGExplainer [13], they become applying the basic message passing algorithm on the atom-based graphs and we name them GNNExplainer(Base) and PGExplainer(Base), respectively.",
                "The competing methods we consider include our SLGNNand three state-of-the-artmethods: AttentiveFP [33], GNNExplainer [35] and PGExplainer [13].",
                "GNNExplainer [35] and PGExplainer [13] are also developed to capture the critical substructures in drug molecules, their fundamental ideas for interpretation are similar by utilizing mask matrix which is learned by optimizing the mutual information of prediction and subgraphs\u2019 distribution, where entries in the matrix represent nodes\u2019 importance.",
                "They [13, 33, 35] have been tested on BBBP, MUTAG, Tox21, synthetic datasets, etc."
            ],
            "citingPaper": {
                "paperId": "753d87c648221aa5f1d5b6afe79170290f2f931a",
                "externalIds": {
                    "ArXiv": "2309.12906",
                    "DOI": "10.1101/2023.08.28.555203",
                    "CorpusId": 261431680
                },
                "corpusId": 261431680,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/753d87c648221aa5f1d5b6afe79170290f2f931a",
                "title": "Building explainable graph neural network by sparse learning for the drug-protein binding prediction",
                "abstract": "Explainable Graph Neural Networks (GNNs) have been developed and applied to drug-protein binding prediction to identify the key chemical structures in a drug that have active interactions with the target proteins. However, the key structures identified by the current explainable GNN models are typically chemically invalid. Furthermore, a threshold needs to be manually selected to pinpoint the key structures from the rest. To overcome the limitations of the current explainable GNN models, we propose our SLGNN, which stands for using Sparse Learning to Graph Neural Networks. Our SLGNN relies on using a chemical-substructure-based graph (where nodes are chemical substructures) to represent a drug molecule. Furthermore, SLGNN incorporates generalized fussed lasso with message-passing algorithms to identify connected subgraphs that are critical for the drug-protein binding prediction. Due to the use of the chemical-substructure-based graph, it is guaranteed that any subgraphs in a drug identified by our SLGNN are chemically valid structures. These structures can be further interpreted as the key chemical structures for the drug to bind to the target protein. We demonstrate the explanatory power of our SLGNN by first showing all the key structures identified by our SLGNN are chemically valid. In addition, we illustrate that the key structures identified by our SLGNN have more predictive power than the key structures identified by the competing methods. At last, we use known drug-protein binding data to show the key structures identified by our SLGNN contain most of the binding sites. CCS CONCEPTS \u2022 Computing methodologies \u2192 Neural networks; Supervised learning by classification; Neural networks. ACM Reference Format Yang Wang, Zanyu Shi, Timothy Richardson, Kun Huang, Pathum Weerawarna, and Yijie Wang. 2023. Building explainable graph neural network by sparse learning for the drug-protein binding prediction. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (CNB MAC 2023). ACM, New York, NY, USA, 10 pages. https://doi.org/XXXXXXX.XXXXXXX",
                "year": 2023,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Yang Wang"
                    },
                    {
                        "authorId": "2236964580",
                        "name": "Zanyu Shi"
                    },
                    {
                        "authorId": "2056772682",
                        "name": "Timothy W. Richardson"
                    },
                    {
                        "authorId": "2237099019",
                        "name": "Kun Huang"
                    },
                    {
                        "authorId": "6040921",
                        "name": "P. Weerawarna"
                    },
                    {
                        "authorId": "2237105782",
                        "name": "Yijie Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "PGExplainer [53] improves on GNNExplainer by selectively choosing the sub-graph for the candidate explanation instead of trying all permutations of the sub-graph."
            ],
            "citingPaper": {
                "paperId": "0b4f8317e311808c02f18ce31ce90f1a06142cf6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-13755",
                    "ArXiv": "2308.13755",
                    "DOI": "10.1007/s10618-023-00963-3",
                    "CorpusId": 260786320
                },
                "corpusId": 260786320,
                "publicationVenue": {
                    "id": "d263025a-9eaf-443f-9bbf-72377e8d22a6",
                    "name": "Data mining and knowledge discovery",
                    "type": "journal",
                    "alternate_names": [
                        "Data Mining and Knowledge Discovery",
                        "Data Min Knowl Discov",
                        "Data min knowl discov"
                    ],
                    "issn": "1384-5810",
                    "url": "https://www.springer.com/computer/database+management+&+information+retrieval/journal/10618",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10618",
                        "http://www.springer.com/computer/database+management+&+information+retrieval/journal/10618"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0b4f8317e311808c02f18ce31ce90f1a06142cf6",
                "title": "i-Align: an interpretable knowledge graph alignment model",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3370335",
                        "name": "Bayu Distiawan Trisedya"
                    },
                    {
                        "authorId": "144954586",
                        "name": "Flora D. Salim"
                    },
                    {
                        "authorId": "123784590",
                        "name": "Jeffrey Chan"
                    },
                    {
                        "authorId": "48702898",
                        "name": "Damiano Spina"
                    },
                    {
                        "authorId": "1732541",
                        "name": "Falk Scholer"
                    },
                    {
                        "authorId": "144721996",
                        "name": "M. Sanderson"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "048d0d7cfed76aeb37726ddf5e205a4fdfdf460a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-04406",
                    "ArXiv": "2308.04406",
                    "DOI": "10.48550/arXiv.2308.04406",
                    "CorpusId": 260704408
                },
                "corpusId": 260704408,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/048d0d7cfed76aeb37726ddf5e205a4fdfdf460a",
                "title": "XGBD: Explanation-Guided Graph Backdoor Detection",
                "abstract": "Backdoor attacks pose a significant security risk to graph learning models. Backdoors can be embedded into the target model by inserting backdoor triggers into the training dataset, causing the model to make incorrect predictions when the trigger is present. To counter backdoor attacks, backdoor detection has been proposed. An emerging detection strategy in the vision and NLP domains is based on an intriguing phenomenon: when training models on a mixture of backdoor and clean samples, the loss on backdoor samples drops significantly faster than on clean samples, allowing backdoor samples to be easily detected by selecting samples with the lowest loss values. However, the ignorance of topological feature information on graph data limits its detection effectiveness when applied directly to the graph domain. To this end, we propose an explanation-guided backdoor detection method to take advantage of the topological information. Specifically, we train a helper model on the graph dataset, feed graph samples into the model, and then adopt explanation methods to attribute model prediction to an important subgraph. We observe that backdoor samples have distinct attribution distribution than clean samples, so the explanatory subgraph could serve as more discriminative features for detecting backdoor samples. Comprehensive experiments on multiple popular datasets and attack methods demonstrate the effectiveness and explainability of our method. Our code is available: https://github.com/GuanZihan/GNN_backdoor_detection.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "95405590",
                        "name": "Zihan Guan"
                    },
                    {
                        "authorId": "3432460",
                        "name": "Mengnan Du"
                    },
                    {
                        "authorId": "47717322",
                        "name": "Ninghao Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "Previous work [39] shows that task-specific explainers like PGExplainer cannot transfer to other tasks.",
                "Despite their strengths, GNNs are usually treated as black box models and thus cannot provide human-intelligible explanations [18, 42].",
                "2) PGExplainer [18] is an inductive explanation method.",
                "Here, carbon rings with chemical groups NO2 groups and NH2 are widely known to be mutagenic [18, 20].",
                "3Note that on BA-2MOTIFS and MUTAG, GNNExplainer and PGExplainer work worse than results reported in previous work [18] as we do not cherry pick the target model.",
                "According to the taxonomy provided in a recent survey [45], these methods can be subsumed into four technical route lines: the gradient-based [2, 22], perturbation-based [18, 24, 27, 33, 42], decomposition-based [2, 23], and surrogate modelbased [11, 32] methods.",
                "BA-2MOTIFS is a synthetic dataset where house motifs and cycle motifs give class labels and thus are regarded as ground-truth explanations.",
                "We observe that 1) Our proposed methods supervised by the representation space (i.e., EGIB and EGIB-TA) outperform task-specific explanation methods (i.e., GNNExplainer, PGExplainer, and Refine).",
                "Typical task-specific explanation methods [18, 33, 42] usually supervise the explanations in the label space by employing Mutual Information (MI) as a relevance metric to measure the performance of explanations.",
                ", MUTAG [6] and BA-2MOTIFS [18] as well as quantitative results.",
                "The former term can be addressed following previous works, e.g., PGExplainer [18]:\nL\ud835\udc61\ud835\udc60 (\ud835\udc4c ; \ud835\udc46 ;\u03a6) = \u2212 \ud835\udc41\u2211\ufe01 \ud835\udc56=1 \ud835\udc36\u2211\ufe01 \ud835\udc50=1 \ud835\udc43 (\ud835\udc53\ud835\udc61 (\ud835\udc54\ud835\udc56 ) = \ud835\udc50) log \ud835\udc43 (\ud835\udc53\ud835\udc61 (\ud835\udc60\ud835\udc56 ) = \ud835\udc50), (12)\nwhere \ud835\udc53\ud835\udc61 = \ud835\udc53\ud835\udc51 \u25e6 \ud835\udc53\ud835\udc52 denotes the composition of GNN-based encoder \ud835\udc53\ud835\udc52 and downstream model \ud835\udc53\ud835\udc51 .",
                "(4) Perturbation-based methods [18, 24, 27, 33, 42] generate masks with a parametrized explainer model.",
                "To tackle this problem, existing graph explanation methods [18, 33, 39] usually assume ei j follows the Bernoulli distribution.",
                "\u25a1\nB IMPLEMENTATION DETAILS B.1 Implementations of Explainers We adopt the multilayer perceptron (MLP) as the attributor T\u03a6 to calculate the logits\ud835\udc64\ud835\udc56 \ud835\udc57 following PGExplainer [18].",
                "As there are no ground-truth explanations for our above-used muti-task datasets, we provide more visualization results on two additional single-task datasets with explanation ground-truths, i.e., MUTAG [6] and BA-2MOTIFS [18] as well as quantitative results.",
                "5, we evaluate the efficiency of three different explanation strategies: the task-specific transductive explanation strategy (e.g., GNNExplainer), the task-specific inductive explanation strategy (e.g., PGExplainer) and our proposed twostage task-agnostic explanation strategy in a muti-task setting.",
                "Compared with GNNExplainer, PGExplainer provides a global understanding of predictions made by GNNs. 3) Refine [33] is an inductive explanation method that learns the multi-grained explanations with class-wise attributors and contrastive learning.",
                "There are mainly three 3Note that on BA-2MOTIFS and MUTAG, GNNExplainer and PGExplainer work worse than results reported in previous work [18] as we do not cherry pick the target model.",
                "2) We adopt the typical Bernoulli distribution assumption [18] with GumbelSoftmax reparameterization trick instead of our Categorical assumption and refer to this variant as EGIB /wo Cat."
            ],
            "citingPaper": {
                "paperId": "2150e336eee8a886e4a661169b60bfbccd323d51",
                "externalIds": {
                    "DBLP": "conf/kdd/WangLL0DDZ23",
                    "DOI": "10.1145/3580305.3599330",
                    "CorpusId": 260500183
                },
                "corpusId": 260500183,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/2150e336eee8a886e4a661169b60bfbccd323d51",
                "title": "Empower Post-hoc Graph Explanations with Information Bottleneck: A Pre-training and Fine-tuning Perspective",
                "abstract": "Researchers recently investigated to explain Graph Neural Networks (GNNs) on the access to a task-specific GNN, which may hinder their wide applications in practice. Specifically, task-specific explanation methods are incapable of explaining pretrained GNNs whose downstream tasks are usually inaccessible, not to mention giving explanations for the transferable knowledge in pretrained GNNs. Additionally, task-specific methods only consider target models' output in the label space, which are coarse-grained and insufficient to reflect the model's internal logic. To address these limitations, we consider a two-stage explanation strategy, i.e., explainers are first pretrained in a task-agnostic fashion in the representation space and then further fine-tuned in the task-specific label space and representation space jointly if downstream tasks are accessible. The two-stage explanation strategy endows post-hoc graph explanations with the applicability to pretrained GNNs where downstream tasks are inaccessible and the capacity to explain the transferable knowledge in the pretrained GNNs. Moreover, as the two-stage explanation strategy explains the GNNs in the representation space, the fine-grained information in the representation space also empowers the explanations. Furthermore, to achieve a trade-off between the fidelity and intelligibility of explanations, we propose an explanation framework based on the Information Bottleneck principle, named Explainable Graph Information Bottleneck (EGIB). EGIB subsumes the task-specific explanation and task-agnostic explanation into a unified framework. To optimize EGIB objective, we derive a tractable bound and adopt a simple yet effective explanation generation architecture. Based on the unified framework, we further theoretically prove that task-agnostic explanation is a relaxed sufficient condition of task-specific explanation, which indicates the transferability of task-agnostic explanations. Extensive experimental results demonstrate the effectiveness of our proposed explanation method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109656535",
                        "name": "Jihong Wang"
                    },
                    {
                        "authorId": "3326677",
                        "name": "Minnan Luo"
                    },
                    {
                        "authorId": "1737121128",
                        "name": "Jundong Li"
                    },
                    {
                        "authorId": "47904366",
                        "name": "Yun Lin"
                    },
                    {
                        "authorId": "123918726",
                        "name": "Yushun Dong"
                    },
                    {
                        "authorId": "2152487387",
                        "name": "J. Dong"
                    },
                    {
                        "authorId": "2152099796",
                        "name": "Qinghua Zheng"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "\u2022 PGExplainer adopts a deep neural network to parameterize the generation process of explanations [31], PGExp for short.",
                "So as to illustrate the effectiveness of our model, we compare our proposed method with interpretable graph learning methods including GRAD [51], ATT [40], GNNExplainer [51], PGExplainer [31], RCExplainer [1], and CF-GNNExplainer [30].",
                "proposed a deep neural network-based method to parameterize the generation process of explanations [31].",
                "PGExplainer, on the other hand, generates explanations using a deep neural network, also maximizing mutual information [31]."
            ],
            "citingPaper": {
                "paperId": "34213c6ba759e2b7ac5ea0cdafbdf78448a0fdfd",
                "externalIds": {
                    "DBLP": "conf/kdd/0015MZ0Z023",
                    "DOI": "10.1145/3580305.3599289",
                    "CorpusId": 260499632
                },
                "corpusId": 260499632,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/34213c6ba759e2b7ac5ea0cdafbdf78448a0fdfd",
                "title": "Counterfactual Learning on Heterogeneous Graphs with Greedy Perturbation",
                "abstract": "Due to the growing importance of using graph neural networks in high-stakes applications, there is a pressing need to interpret the predicted results of these models. Existing methods for explanation have mainly focused on generating sub-graphs comprising important edges for a specific prediction. However, these methods face two issues. Firstly, they lack counterfactual validity as removing the subgraph may not affect the prediction, and generating plausible counterfactual examples has not been adequately explored. Secondly, they cannot be extended to heterogeneous graphs as the complex information involved in such graphs increases the difficulty of generating interpretations. This paper proposes a novel counterfactual learning method, named CF-HGExplainer, for heterogeneous graphs. The method incorporates a semantic-aware attentive pooling strategy for the heterogeneous graph classifier and designs a heterogeneous decision boundaries extraction module to find the common logic for similar graphs based on the extracted graph embeddings from the classifier. Additionally, we propose to greedily perturb nodes and edges based on the distribution of node features and edge plausibility to train a neural network for heterogeneous edge weight learning. Extensive experiments on two public academic datasets demonstrate the effectiveness of CF-HGExplainer compared to state-of-the-art methods on the graph classification task and graph interpretation task.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1998961496",
                        "name": "Qiang Yang"
                    },
                    {
                        "authorId": "2109467053",
                        "name": "Changsheng Ma"
                    },
                    {
                        "authorId": "119718473",
                        "name": "Qiannan Zhang"
                    },
                    {
                        "authorId": "2118502950",
                        "name": "Xin Gao"
                    },
                    {
                        "authorId": "2117879943",
                        "name": "Chuxu Zhang"
                    },
                    {
                        "authorId": "2928371",
                        "name": "Xiangliang Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "d6f907bd780b866cf5c836c2e18ddca61165fa2d",
                "externalIds": {
                    "DBLP": "conf/kdd/YangZSWWW23",
                    "DOI": "10.1145/3580305.3599339",
                    "CorpusId": 260499730
                },
                "corpusId": 260499730,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/d6f907bd780b866cf5c836c2e18ddca61165fa2d",
                "title": "EXTRACT and REFINE: Finding a Support Subgraph Set for Graph Representation",
                "abstract": "Subgraph learning has received considerable attention in its capacity of interpreting important structural information for predictions. Existing subgraph learning usually exploits statistics on predefined structures e.g., node degrees, occurrence frequency, to extract subgraphs, or refine the contents via only capturing label-relevant information with node-level sampling. Given diverse subgraph patterns, and mutual independence with local correlations on graphs, current solutions on subgraph learning still have two limitations in extraction and refinement stages. 1) The universality of extracting substructure patterns across domains is still lacking, 2) node-level sampling in refinement will distort the original local topology and none explicit guidance eliminating redundant information contribute to inefficiency issue. In this paper, we propose a unified subgraph learning scheme, Poly-Pivot Graph Neural Network (P2GNN) where we designate the centric node of each subgraph as the pivot. In the extraction stage, we present a general subgraph extraction principle, i.e., Local; Asymmetry between the centric and affiliated nodes. To this end, we asymmetrically model the similarity between each pair of nodes with random walk and quantify mutual affiliations in Affinity Propagation architecture, to extract subgraph structures. In the refinement, we devise a subgraph-level exclusion regularization to squash the target-independent information by considering mutual relations across subgraphs, cooperatively preserving a support set of subgraphs and facilitating the refinement process for graph representation. Empirical experiments on diverse web and biological graphs reveal 1.1%~7.3% improvements against best baselines, and visualized case studies prove the universality and interpretability of our P2GNN.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2221673134",
                        "name": "Kuo Yang"
                    },
                    {
                        "authorId": "6231985",
                        "name": "Zhengyang Zhou"
                    },
                    {
                        "authorId": "2153199166",
                        "name": "Wei Sun"
                    },
                    {
                        "authorId": "2108814780",
                        "name": "Pengkun Wang"
                    },
                    {
                        "authorId": "2108599981",
                        "name": "Xu Wang"
                    },
                    {
                        "authorId": null,
                        "name": "Yang Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Brought to GML, perturbation-based explainers learn masks that assign an importance to edges and/or features of the graph [24, 29, 48]."
            ],
            "citingPaper": {
                "paperId": "913e43a412cd411434d84772c1035e81cb29d383",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-01682",
                    "ArXiv": "2308.01682",
                    "DOI": "10.48550/arXiv.2308.01682",
                    "CorpusId": 260438795
                },
                "corpusId": 260438795,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/913e43a412cd411434d84772c1035e81cb29d383",
                "title": "Evaluating Link Prediction Explanations for Graph Neural Networks",
                "abstract": "Graph Machine Learning (GML) has numerous applications, such as node/graph classification and link prediction, in real-world domains. Providing human-understandable explanations for GML models is a challenging yet fundamental task to foster their adoption, but validating explanations for link prediction models has received little attention. In this paper, we provide quantitative metrics to assess the quality of link prediction explanations, with or without ground-truth. State-of-the-art explainability methods for Graph Neural Networks are evaluated using these metrics. We discuss how underlying assumptions and technical details specific to the link prediction task, such as the choice of distance between node embeddings, can influence the quality of the explanations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2709930",
                        "name": "Claudio Borile"
                    },
                    {
                        "authorId": "26582424",
                        "name": "A. Perotti"
                    },
                    {
                        "authorId": "2735649",
                        "name": "A. Panisson"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Many existing works [30] aim to identify a subgraph that is highly correlated with the classification prediction result which is likely to get misleading explanations."
            ],
            "citingPaper": {
                "paperId": "db9876e6fe40be5b74730f25bd2d54b20b841b90",
                "externalIds": {
                    "ArXiv": "2308.00391",
                    "DBLP": "journals/corr/abs-2308-00391",
                    "DOI": "10.48550/arXiv.2308.00391",
                    "CorpusId": 260351429
                },
                "corpusId": 260351429,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/db9876e6fe40be5b74730f25bd2d54b20b841b90",
                "title": "Counterfactual Graph Transformer for Traffic Flow Prediction",
                "abstract": "Traffic flow prediction (TFP) is a fundamental problem of the Intelligent Transportation System (ITS), as it models the latent spatial-temporal dependency of traffic flow for potential congestion prediction. Recent graph-based models with multiple kinds of attention mechanisms have achieved promising performance. However, existing methods for traffic flow prediction tend to inherit the bias pattern from the dataset and lack interpretability. To this end, we propose a Counterfactual Graph Transformer (CGT) model with an instance-level explainer (e.g., finding the important subgraphs) specifically designed for TFP. We design a perturbation mask generator over input sensor features at the time dimension and the graph structure on the graph transformer module to obtain spatial and temporal counterfactual explanations. By searching the optimal perturbation masks on the input data feature and graph structures, we can obtain the concise and dominant data or graph edge links for the subsequent TFP task. After re-training the utilized graph transformer model after counterfactual perturbation, we can obtain improved and interpretable traffic flow prediction. Extensive results on three real-world public datasets show that CGT can produce reliable explanations and is promising for traffic flow prediction.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118772104",
                        "name": "Yingbin Yang"
                    },
                    {
                        "authorId": "2197890027",
                        "name": "Kai Du"
                    },
                    {
                        "authorId": "22200602",
                        "name": "Xingyuan Dai"
                    },
                    {
                        "authorId": "2389269",
                        "name": "Jianwu Fang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "4257a86ca9a0b480340937884a1e2a301153cf9a",
                "externalIds": {
                    "DOI": "10.1016/j.media.2023.102932",
                    "CorpusId": 261089195,
                    "PubMed": "37657365"
                },
                "corpusId": 261089195,
                "publicationVenue": {
                    "id": "0e5a2999-db05-4ab3-83d8-b480d49b90be",
                    "name": "Medical Image Analysis",
                    "type": "journal",
                    "alternate_names": [
                        "Med Image Anal"
                    ],
                    "issn": "1361-8415",
                    "url": "https://www.journals.elsevier.com/medical-image-analysis",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/13618415"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4257a86ca9a0b480340937884a1e2a301153cf9a",
                "title": "A-GCL: Adversarial graph contrastive learning for fMRI analysis to diagnose neurodevelopmental disorders.",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2145524177",
                        "name": "Shengjie Zhang"
                    },
                    {
                        "authorId": "2192818212",
                        "name": "Xiang Chen"
                    },
                    {
                        "authorId": "2111112264",
                        "name": "Xin Shen"
                    },
                    {
                        "authorId": "1999889514",
                        "name": "Bohan Ren"
                    },
                    {
                        "authorId": "2005203450",
                        "name": "Ziqi Yu"
                    },
                    {
                        "authorId": null,
                        "name": "Haibo Yang"
                    },
                    {
                        "authorId": "2233153511",
                        "name": "Xi Jiang"
                    },
                    {
                        "authorId": "144986260",
                        "name": "D. Shen"
                    },
                    {
                        "authorId": "2116567344",
                        "name": "Yuan Zhou"
                    },
                    {
                        "authorId": "2109108365",
                        "name": "Xiao-Yong Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Perturbation-based methods, such as GNNExplainer [66], PGExplainer [35], ZORRO [15], GraphMask [52], RC-Explainer [60], SubgraphX [70], measure the impact of the perturbation of the input features on the output of the classifier, to detect the most important features."
            ],
            "citingPaper": {
                "paperId": "6a5741e39a3c0b9ba1eaab2c890bbc471e40395e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-14849",
                    "ArXiv": "2307.14849",
                    "DOI": "10.48550/arXiv.2307.14849",
                    "CorpusId": 260203044
                },
                "corpusId": 260203044,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6a5741e39a3c0b9ba1eaab2c890bbc471e40395e",
                "title": "Counterfactual Explanations for Graph Classification Through the Lenses of Density",
                "abstract": "Counterfactual examples have emerged as an effective approach to produce simple and understandable post-hoc explanations. In the context of graph classification, previous work has focused on generating counterfactual explanations by manipulating the most elementary units of a graph, i.e., removing an existing edge, or adding a non-existing one. In this paper, we claim that such language of explanation might be too fine-grained, and turn our attention to some of the main characterizing features of real-world complex networks, such as the tendency to close triangles, the existence of recurring motifs, and the organization into dense modules. We thus define a general density-based counterfactual search framework to generate instance-level counterfactual explanations for graph classifiers, which can be instantiated with different notions of dense substructures. In particular, we show two specific instantiations of this general framework: a method that searches for counterfactual graphs by opening or closing triangles, and a method driven by maximal cliques. We also discuss how the general method can be instantiated to exploit any other notion of dense substructures, including, for instance, a given taxonomy of nodes. We evaluate the effectiveness of our approaches in 7 brain network datasets and compare the counterfactual statements generated according to several widely-used metrics. Results confirm that adopting a semantic-relevant unit of change like density is essential to define versatile and interpretable counterfactual explanation methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "89449460",
                        "name": "Carlo Abrate"
                    },
                    {
                        "authorId": "39046274",
                        "name": "Giulia Preti"
                    },
                    {
                        "authorId": "2179558887",
                        "name": "Francesco Bonchi"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "68910bf55fb492ce627c0ee337351a5751219914",
                "externalIds": {
                    "DOI": "10.1002/wcms.1681",
                    "CorpusId": 260262777
                },
                "corpusId": 260262777,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/68910bf55fb492ce627c0ee337351a5751219914",
                "title": "Explainable artificial intelligence: A taxonomy and guidelines for its application to drug discovery",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1783708",
                        "name": "I. Ponzoni"
                    },
                    {
                        "authorId": "2225766019",
                        "name": "Juan Antonio P\u00e1ez Prosper"
                    },
                    {
                        "authorId": "39727540",
                        "name": "N. Campillo"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "In addition, according methods for enhancing explainability and interpretability [42], [43] are a futher interesting direction for future research."
            ],
            "citingPaper": {
                "paperId": "8da24b98b65fbfb247792e89084d55f6ce74653d",
                "externalIds": {
                    "DBLP": "conf/indin/AhmedWA23",
                    "DOI": "10.1109/INDIN51400.2023.10218172",
                    "CorpusId": 261106481
                },
                "corpusId": 261106481,
                "publicationVenue": {
                    "id": "b6744d41-3aae-472d-98cf-21f4373115b2",
                    "name": "International Conference on Industrial Informatics",
                    "type": "conference",
                    "alternate_names": [
                        "INDIN",
                        "Int Conf Ind Informatics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8da24b98b65fbfb247792e89084d55f6ce74653d",
                "title": "Graph Neural Network-Based Measurement Inference on Irregular Sensor Geometries",
                "abstract": "In general, Graph Neural Networks (GNNs) enable modeling and learning in the context of complex data like graphs and time series \u2013 which is typically difficult for standard Deep Learning approaches. This paper proposes an approach for modeling a prediction task using irregularly sampled spatio-temporal sensor data via GNNs. Specifically, we present a method for modeling spatio-temporal sensor data represented as graphs, and a more convenient image representation enabling standard convolutional deep learning. By mapping the irregularly sampled graph to a regular graph representation, we can then integrate temporal sensor information with spatial information. In our experimentation, we demonstrate the efficacy of the proposed approach in inspection contexts of oil and gas pipelines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2189543171",
                        "name": "Martin ben Ahmed"
                    },
                    {
                        "authorId": "2116531",
                        "name": "N. Wilming"
                    },
                    {
                        "authorId": "2191921580",
                        "name": "Martin Atzmueller"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [],
            "contexts": [
                "For GNNExplainer and PGExplainer, which are previously used for the classification task, we replace the Cross-Entropy loss with the MSE loss.",
                "In previous works [25, 27, 53] for graph classification, the mutual information I (G\u2217;Y ) is estimated with the Cross-Entropy between the the predictions f (G\u2217) from GNN model f and its prediction label Y from the original graph G .",
                "could also be classified into two categories based on their methodology: self-explainable GNNs [1, 8] and post-hoc explanation methods [25, 53, 58], where the former methods provide both predictions and explanations, while the latter methods use an additional model or strategy to explain the target GNN.",
                "Based on IB, a recent work unifies the most existing post-hoc explanation methods for GNN, such as GNNExplainer [53], PGExplainer [25], with the graph information bottleneck (GIB) principle [27, 47, 55].",
                "(4) PGExplainer [25]: PGExplainer adopts a deep neural network to parameterize the generation process of explanations, which facilitates a comprehensive understanding of the predictions made by GNNs.",
                "We follow [25] to make a widely-accepted assumption that a graph can be divided by G = G\u2217 + G\u0394, where G\u2217 presents the underlying sub-graph that makes important contributions to GNN\u2019s predictions, which is the expected explanatory graph, and G\u0394 consists of the remaining label-independent edges for predictions made by the GNN.",
                "In practice, we follow the previous work [25, 50, 53] to implement them.",
                "Examples of such methods include GNNExplainer [53], which determines the importance of nodes and edges through perturbation, and PGExplainer [25], which trains a graph generator to incorporate global information.",
                "GNN explainability: The explanation methods for GNN models could be categorized into two types based on their granularity: instance-level [25, 34, 53, 58] and model-level [56], where the former methods explain the prediction for each instance by identifying important sub-graphs, and the latter method aims to understand the global decision rules captured by the GNN.",
                "In this paper, we focus on discovering the important sub-graph typologies following the previous work [25, 53]."
            ],
            "citingPaper": {
                "paperId": "54019f5026c385da3f9389ee80525c973f0eb6e5",
                "externalIds": {
                    "ArXiv": "2307.07840",
                    "DBLP": "journals/corr/abs-2307-07840",
                    "DOI": "10.48550/arXiv.2307.07840",
                    "CorpusId": 259937463
                },
                "corpusId": 259937463,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/54019f5026c385da3f9389ee80525c973f0eb6e5",
                "title": "RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task",
                "abstract": "Graph regression is a fundamental task and has received increasing attention in a wide range of graph learning tasks. However, the inference process is often not interpretable. Most existing explanation techniques are limited to understanding GNN behaviors in classification tasks. In this work, we seek an explanation to interpret the graph regression models (XAIG-R). We show that existing methods overlook the distribution shifting and continuously ordered decision boundary, which hinders them away from being applied in the regression tasks. To address these challenges, we propose a novel objective based on the information bottleneck theory and introduce a new mix-up framework, which could support various GNNs in a model-agnostic manner. We further present a contrastive learning strategy to tackle the continuously ordered labels in regression task. To empirically verify the effectiveness of the proposed method, we introduce three benchmark datasets and a real-life dataset for evaluation. Extensive experiments show the effectiveness of the proposed method in interpreting GNN models in regression tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144129760",
                        "name": "Jiaxing Zhang"
                    },
                    {
                        "authorId": "153267839",
                        "name": "Zhuomin Chen"
                    },
                    {
                        "authorId": "2176403267",
                        "name": "Hao Mei"
                    },
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    },
                    {
                        "authorId": "1474226770",
                        "name": "Hua Wei"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "4936c5acfffcc6368e99cd8326fc7555c76a58f0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-07832",
                    "ArXiv": "2307.07832",
                    "DOI": "10.1145/3580305.3599435",
                    "CorpusId": 259937050
                },
                "corpusId": 259937050,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/4936c5acfffcc6368e99cd8326fc7555c76a58f0",
                "title": "MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation",
                "abstract": "Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. However, their predictions are often not interpretable. Post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we shed light on the existence of the distribution shifting issue in existing methods, which affects explanation quality, particularly in applications on real-life datasets with tight decision boundaries. To address this issue, we introduce a generalized Graph Information Bottleneck (GIB) form that includes a label-independent graph variable, which is equivalent to the vanilla GIB. Driven by the generalized GIB, we propose a graph mixup method, MixupExplainer, with a theoretical guarantee to resolve the distribution shifting issue. We conduct extensive experiments on both synthetic and real-world datasets to validate the effectiveness of our proposed mixup approach over existing approaches. We also provide a detailed analysis of how our proposed approach alleviates the distribution shifting issue.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144129760",
                        "name": "Jiaxing Zhang"
                    },
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    },
                    {
                        "authorId": "2181654032",
                        "name": "Huazhou Wei"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The subsequent work, such as PGExplainer (Luo et al., 2020), PGMExplainer (Yuan et al.",
                "The subsequent work, such as PGExplainer (Luo et al., 2020), PGMExplainer (Yuan et al., 2021), and SubgraphX (Vu & Thai, 2020), followed (Ying et al., 2019) and evaluated GNN explainers on the suggested synthetic datasets or their close variants."
            ],
            "citingPaper": {
                "paperId": "6cf743c7031ef8bb29ff192849f07c653262561f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-06963",
                    "ArXiv": "2307.06963",
                    "DOI": "10.48550/arXiv.2307.06963",
                    "CorpusId": 259924516
                },
                "corpusId": 259924516,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6cf743c7031ef8bb29ff192849f07c653262561f",
                "title": "Is Task-Agnostic Explainable AI a Myth?",
                "abstract": "Our work serves as a framework for unifying the challenges of contemporary explainable AI (XAI). We demonstrate that while XAI methods provide supplementary and potentially useful output for machine learning models, researchers and decision-makers should be mindful of their conceptual and technical limitations, which frequently result in these methods themselves becoming black boxes. We examine three XAI research avenues spanning image, textual, and graph data, covering saliency, attention, and graph-type explainers. Despite the varying contexts and timeframes of the mentioned cases, the same persistent roadblocks emerge, highlighting the need for a conceptual breakthrough in the field to address the challenge of compatibility between XAI methods and application tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2223551448",
                        "name": "Alicja Chaszczewicz"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "6bb9c86483f212a631324ba9b47c344d419a428a",
                "externalIds": {
                    "DBLP": "conf/issta/HuWLPWZ023",
                    "DOI": "10.1145/3597926.3598145",
                    "CorpusId": 259844945
                },
                "corpusId": 259844945,
                "publicationVenue": {
                    "id": "289bfdda-eab3-4c9a-97be-ef1e0f9ddfc0",
                    "name": "International Symposium on Software Testing and Analysis",
                    "type": "conference",
                    "alternate_names": [
                        "ISSTA",
                        "Int Symp Softw Test Anal"
                    ],
                    "url": "https://dl.acm.org/conference/issta"
                },
                "url": "https://www.semanticscholar.org/paper/6bb9c86483f212a631324ba9b47c344d419a428a",
                "title": "Interpreters for GNN-Based Vulnerability Detection: Are We There Yet?",
                "abstract": "Traditional vulnerability detection methods have limitations due to their need for extensive manual labor. Using automated means for vulnerability detection has attracted research interest, especially deep learning, which has achieved remarkable results. Since graphs can better convey the structural feature of code than text, graph neural network (GNN) based vulnerability detection is significantly better than text-based approaches. Therefore, GNN-based vulnerability detection approaches are becoming popular. However, GNN models are close to black boxes for security analysts, so the models cannot provide clear evidence to explain why a code sample is detected as vulnerable or secure. At this stage, many GNN interpreters have been proposed. However, the explanations provided by these interpretations for vulnerability detection models are highly inconsistent and unconvincing to security experts. To address the above issues, we propose principled guidelines to assess the quality of the interpretation approaches for GNN-based vulnerability detectors based on concerns in vulnerability detection, namely, stability, robustness, and effectiveness. We conduct extensive experiments to evaluate the interpretation performance of six famous interpreters (GNN-LRP, DeepLIFT, GradCAM, GNNExplainer, PGExplainer, and SubGraphX) on four vulnerability detectors (DeepWukong, Devign, IVDetect, and Reveal). The experimental results show that the target interpreters achieve poor performance in terms of effectiveness, stability, and robustness. For effectiveness, we find that the instance-independent methods outperform others due to their deep insight into the detection model. In terms of stability, the perturbation-based interpretation methods are more resilient to slight changes in model parameters as they are model-agnostic. For robustness, the instance-independent approaches provide more consistent interpretation results for similar vulnerabilities.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2182450357",
                        "name": "Yutao Hu"
                    },
                    {
                        "authorId": "2223247522",
                        "name": "Suyuan Wang"
                    },
                    {
                        "authorId": "2108801422",
                        "name": "Wenke Li"
                    },
                    {
                        "authorId": "2158336147",
                        "name": "Junru Peng"
                    },
                    {
                        "authorId": "2109036133",
                        "name": "Yueming Wu"
                    },
                    {
                        "authorId": "2068865",
                        "name": "Deqing Zou"
                    },
                    {
                        "authorId": "2152883080",
                        "name": "Hai Jin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Such understanding fosters trust in the system and enables the discovery of latent patterns within the data [297], [299]."
            ],
            "citingPaper": {
                "paperId": "d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9",
                "externalIds": {
                    "ArXiv": "2307.03759",
                    "DBLP": "journals/corr/abs-2307-03759",
                    "DOI": "10.48550/arXiv.2307.03759",
                    "CorpusId": 259501265
                },
                "corpusId": 259501265,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9",
                "title": "A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection",
                "abstract": "Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. These approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present and discuss representative research works and introduce mainstream applications of GNN4TS. A comprehensive discussion of potential future research directions completes the survey. This survey, for the first time, brings together a vast array of knowledge on GNN-based time series research, highlighting foundations, practical applications, and opportunities of graph neural networks for time series analysis.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2072905592",
                        "name": "Ming Jin"
                    },
                    {
                        "authorId": "2134585717",
                        "name": "Huan Yee Koh"
                    },
                    {
                        "authorId": "3308963",
                        "name": "Qingsong Wen"
                    },
                    {
                        "authorId": "48283854",
                        "name": "Daniele Zambon"
                    },
                    {
                        "authorId": "1785004",
                        "name": "C. Alippi"
                    },
                    {
                        "authorId": "2098367190",
                        "name": "G. I. Webb"
                    },
                    {
                        "authorId": "2161241744",
                        "name": "Irwin King"
                    },
                    {
                        "authorId": "2153326034",
                        "name": "Shirui Pan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Perturbation-based methods [19,20,26,41,50] estimate input scores with the assumption that removing important features will have a large impact on the"
            ],
            "citingPaper": {
                "paperId": "74b70618fe4bf5a163702581a655bd8fe3fe5ea1",
                "externalIds": {
                    "ArXiv": "2307.01053",
                    "DBLP": "journals/corr/abs-2307-01053",
                    "DOI": "10.48550/arXiv.2307.01053",
                    "CorpusId": 259316815
                },
                "corpusId": 259316815,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/74b70618fe4bf5a163702581a655bd8fe3fe5ea1",
                "title": "ENGAGE: Explanation Guided Data Augmentation for Graph Representation Learning",
                "abstract": "The recent contrastive learning methods, due to their effectiveness in representation learning, have been widely applied to modeling graph data. Random perturbation is widely used to build contrastive views for graph data, which however, could accidentally break graph structures and lead to suboptimal performance. In addition, graph data is usually highly abstract, so it is hard to extract intuitive meanings and design more informed augmentation schemes. Effective representations should preserve key characteristics in data and abandon superfluous information. In this paper, we propose ENGAGE (ExplaNation Guided data AuGmEntation), where explanation guides the contrastive augmentation process to preserve the key parts in graphs and explore removing superfluous information. Specifically, we design an efficient unsupervised explanation method called smoothed activation map as the indicator of node importance in representation learning. Then, we design two data augmentation schemes on graphs for perturbing structural and feature information, respectively. We also provide justification for the proposed method in the framework of information theories. Experiments of both graph-level and node-level tasks, on various model architectures and on different real-world graphs, are conducted to demonstrate the effectiveness and flexibility of ENGAGE. The code of ENGAGE can be found: https://github.com/sycny/ENGAGE.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46571755",
                        "name": "Yucheng Shi"
                    },
                    {
                        "authorId": "3364022",
                        "name": "Kaixiong Zhou"
                    },
                    {
                        "authorId": "47717322",
                        "name": "Ninghao Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "PGExplainer finds more crashes on ReVeal than all other methods on Devign for Libxml2 given Table 2.",
                "PGExplainer has the best MAZ for all PUTs and for both models since around 0.95\u2212 0.98% of the code lines score an accumulated relevance lower than 50%.",
                "Although their DA is superior compared to GNNExplainer and PGExplainer, their located code lines are, however, unrelated to the actual underlying vulnerability concluding from our extrinsic results.",
                "With the rise of graph neural networks, several works ported the underlying classic explanation concepts to the graph domain [4, 41], as well as completely new graphspecific algorithms have been invented [36, 44, 54].",
                "Considering the sparsity, GNNExplainer and LineVul achieve the worst MAZ results, and PGExplainer and Smoothgrad yield the best.",
                "Although the average crashes per path are best for PGExplainer, GNNExplainer has the most precise explanations.",
                "In particular, we focus on the graph-agnostic methods Smoothgrad [46] and GradCAM [45] that are widely applied in computer vision and the graph-specific methods GNNExplainer [54] and PGExplainer [36] tailored towards explaining GNNs.",
                "4) PGExplainer: This method uses a so-called explanation network on an embedding of the graph edges [36].",
                "However, it for example, beats PGExplainer and SmoothGrad on Libming using Devign, proving it to be a strong baseline.",
                "Our results are in line with Ganz et al. [21] since according to them, Smoothgrad is among the best candidates considering the DA and PGExplainer produce the most concise explanations.",
                "Both methods outperform GradCam and PGExplainer but are inferior to GNNExplainer and SmoothGrad.",
                "With the logarithmic stretch, we first assess the influence of the model on the EM\u2019s output, for instance, GradCam, SmoothGrad, GNNExplainer and PGExplainer in Figure 5."
            ],
            "citingPaper": {
                "paperId": "1aa12661100035f01104ee3b92ff11005dc1e575",
                "externalIds": {
                    "DBLP": "conf/eurosp/GanzRHR23",
                    "DOI": "10.1109/EuroSP57164.2023.00038",
                    "CorpusId": 260365162
                },
                "corpusId": 260365162,
                "publicationVenue": {
                    "id": "4c2b8cb8-e51c-4ece-9122-89595989b56f",
                    "name": "European Symposium on Security and Privacy",
                    "type": "conference",
                    "alternate_names": [
                        "EuroS&P",
                        "IEEE European Symposium on Security and Privacy",
                        "Eur Symp Secur Priv",
                        "IEEE Eur Symp Secur Priv",
                        "EUROS&P"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1aa12661100035f01104ee3b92ff11005dc1e575",
                "title": "Hunting for Truth: Analyzing Explanation Methods in Learning-based Vulnerability Discovery",
                "abstract": "Recent research has developed a series of methods for finding vulnerabilities in software using machine learning. While the proposed methods provide a remarkable performance in controlled experiments, their practical application is hampered by their black-box nature: A security practitioner cannot tell how these methods arrive at a decision and what code structures contribute to a reported security flaw. Explanation methods for machine learning may overcome this problem and guide the practitioner to relevant code. However, there exist a variety of competing explanation methods, each highlighting different code regions when given the same finding. So far, this inconsistency has made it impossible to select a suitable explanation method for practical use.In this paper, we address this problem and develop a method for analyzing and comparing explanations for learning-based vulnerability discovery. Given a predicted vulnerability, our approach uses directed fuzzing to create local ground-truth around code regions marked as relevant by an explanation method. This local ground-truth enables us to assess the veracity of the explanation. As a result, we can qualitatively compare different explanation methods and determine the most accurate one for a particular learning setup. In an empirical evaluation with different discovery and explanation methods, we demonstrate the utility of this approach and its capabilities in making learning-based vulnerability discovery more transparent.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2135783537",
                        "name": "Tom Ganz"
                    },
                    {
                        "authorId": "2226395762",
                        "name": "Philipp Rall"
                    },
                    {
                        "authorId": "2736329",
                        "name": "Martin H\u00e4rterich"
                    },
                    {
                        "authorId": "144825749",
                        "name": "Konrad Rieck"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "contexts": [
                "Renowned homogeneous GNNs include graph convolution networks (GCNs) [14], graph attention networks (GATs) [46], and graph isomorphism\nnetworks (GINs) [53].",
                "HGNNs can be grouped into meta-path-based methods and stacking-fashion models according to their way of entangling edge and node types into the network.",
                "The majority of current explainers remain topological-level [3, 21, 23, 42, 51, 52, 57], which cannot be easily extended tomulti-level as they are oriented towards the combinatorial nature of graph topology.",
                "Though an explainable fraud transaction detectionmodel was proposed [36], which contains an initial attempt to study explainability on heterogeneous graphs, as it is fully based on a variant of GNNExplainer, we do not consider it an model-agnostic explainer for HGNNs.",
                "\u2022 PGExplainer [23] is an inductive explainer, which can be directly used on new instances after training on a group of data.",
                "The stacking-fashion HGNNs adopt architectures consisting of stacking layers with the same structure, which is analog to homogeneous GNNs. Examples of these models include HGT [12], Simple-HGN [24], and the like.",
                "On the other hand, as an emerging line of works, there has not been any modelagnostic explainer for HGNNs to the best of our knowledge.",
                "The former line of works [3, 21, 23, 42, 45, 51, 52, 54, 57] adopts heuristic metrics to quantify \u201cexplainability\u201d for designing optimization objectives or loss functions.",
                "However, feature-level explanation is ignored by most existing methods [21, 23, 42, 54, 57], which can lead to unreasonable or even misleading explanations.",
                "When \u03a6t = max, the values of feature variables correspond to a factual explanation [23, 54], which can maximize the model\u2019s belief in its decision by presenting only the critical parts.",
                "The pioneering research of explaining DGNs include GNNExplainer [54] and heat-map-based methods for GCNs [35], since then intensive research efforts have been devoted to explain homogeneous GNNs [11, 21, 23, 42, 47, 50, 54, 55, 57]."
            ],
            "citingPaper": {
                "paperId": "b27a27195e288f4594bad807e9dce1dbfc735d56",
                "externalIds": {
                    "DOI": "10.14778/3611479.3611503",
                    "CorpusId": 261197657
                },
                "corpusId": 261197657,
                "publicationVenue": {
                    "id": "fcbcaf18-8ab1-43e1-a973-604bbc7e344e",
                    "name": "Proceedings of the VLDB Endowment",
                    "type": "journal",
                    "alternate_names": [
                        "Proceedings of The Vldb Endowment",
                        "Proc VLDB Endow",
                        "Proc Vldb Endow"
                    ],
                    "issn": "2150-8097",
                    "url": "http://dl.acm.org/toc.cfm?id=J1174",
                    "alternate_urls": [
                        "http://portal.acm.org/toc.cfm?CFID=21632689&CFTOKEN=99329904&WantType=Affiliated%20Organizations&coll=ACM&dl=ACM&id=J1174&idx=J1174&part=affil&title=VLDB%20Endowment&type=periodical"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b27a27195e288f4594bad807e9dce1dbfc735d56",
                "title": "HENCE-X: Toward Heterogeneity-Agnostic Multi-Level Explainability for Deep Graph Networks",
                "abstract": "Deep graph networks (DGNs) have demonstrated their outstanding effectiveness on both heterogeneous and homogeneous graphs. However their black-box nature does not allow human users to understand their working mechanisms. Recently, extensive efforts have been devoted to explaining DGNs' prediction, yet heterogeneity-agnostic multi-level explainability is still less explored. Since the two types of graphs are both irreplaceable in real-life applications, having a more general and end-to-end explainer becomes a natural and inevitable choice. In the meantime, feature-level explanation is often ignored by existing techniques, while topological-level explanation alone can be incomplete and deceptive. Thus, we propose a heterogeneity-agnostic multi-level explainer in this paper, named HENCE-X, which is a causality-guided method that can capture the non-linear dependencies of model behavior on the input using conditional probabilities. We theoretically prove that HENCE-X is guaranteed to find the Markov blanket of the explained prediction, meaning that all information that the prediction is dependent on is identified. Experiments on three real-world datasets show that HENCE-X outperforms state-of-the-art (SOTA) methods in generating faithful factual and counterfactual explanations of DGNs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2142661696",
                        "name": "Gengsi Lv"
                    },
                    {
                        "authorId": "50445897",
                        "name": "C. Zhang"
                    },
                    {
                        "authorId": "143891665",
                        "name": "Lei Chen"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "contexts": [
                "Not to mention that model-level explanations provide easy generalization to an inductive setting, which is the nature of many GNN applications [31].",
                "Given the input data, instancelevel techniques [29, 31, 52, 54, 65, 68] have been present main stream of GNN explanation, which aim to acquire explanations for a target instance.",
                "As a result, intensive research efforts have been devoted to understand how GNNs make decisions [15, 31, 43, 54, 65, 72].",
                ", instance-level explanation [29, 31, 52, 54, 56, 65, 69], which aims to find one explanation for a given input instance."
            ],
            "citingPaper": {
                "paperId": "09ebe119cc955c804e2a5f742a8483e725213eaf",
                "externalIds": {
                    "DOI": "10.14778/3611479.3611538",
                    "CorpusId": 261193294
                },
                "corpusId": 261193294,
                "publicationVenue": {
                    "id": "fcbcaf18-8ab1-43e1-a973-604bbc7e344e",
                    "name": "Proceedings of the VLDB Endowment",
                    "type": "journal",
                    "alternate_names": [
                        "Proceedings of The Vldb Endowment",
                        "Proc VLDB Endow",
                        "Proc Vldb Endow"
                    ],
                    "issn": "2150-8097",
                    "url": "http://dl.acm.org/toc.cfm?id=J1174",
                    "alternate_urls": [
                        "http://portal.acm.org/toc.cfm?CFID=21632689&CFTOKEN=99329904&WantType=Affiliated%20Organizations&coll=ACM&dl=ACM&id=J1174&idx=J1174&part=affil&title=VLDB%20Endowment&type=periodical"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/09ebe119cc955c804e2a5f742a8483e725213eaf",
                "title": "On Data-Aware Global Explainability of Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) have significantly boosted the performance of many graph-based applications, yet they serve as black-box models. To understand how GNNs make decisions, explainability techniques have been extensively studied. While the majority of existing methods focus on local explainability, we propose DAG-Explainer in this work aiming for global explainability. Specifically, we observe three properties of superior explanations for a pretrained GNN: they should be highly recognized by the model, compliant with the data distribution and discriminative among all the classes. The first property entails an explanation to be faithful to the model, as the other two require the explanation to be convincing regarding the data distribution. Guided by these properties, we design metrics to quantify the quality of each single explanation and formulate the problem of finding data-aware global explanations for a pretrained GNN as an optimizing problem. We prove that the problem is NP-hard and adopt a randomized greedy algorithm to find a near optimal solution. Furthermore, we derive an improved bound of the approximation algorithm in our problem over the state-of-the-art (SOTA) best. Experimental results show that DAG-Explainer can efficiently produce meaningful and trustworthy explanations while preserving comparable quantitative evaluation results to the SOTA methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2142661696",
                        "name": "Gengsi Lv"
                    },
                    {
                        "authorId": "143891665",
                        "name": "Lei Chen"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "75fda42967abb42a928fba395cb8e011d3beea60",
                "externalIds": {
                    "DBLP": "journals/eswa/YuLZZW23",
                    "DOI": "10.1016/j.eswa.2023.120978",
                    "CorpusId": 259890287
                },
                "corpusId": 259890287,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/75fda42967abb42a928fba395cb8e011d3beea60",
                "title": "Code classification with graph neural networks: Have you ever struggled to make it work?",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "84205136",
                        "name": "Qingchen Yu"
                    },
                    {
                        "authorId": "2120099904",
                        "name": "Xin Liu"
                    },
                    {
                        "authorId": "2118411250",
                        "name": "Qingguo Zhou"
                    },
                    {
                        "authorId": "144279461",
                        "name": "Jianwei Zhuge"
                    },
                    {
                        "authorId": "2118839825",
                        "name": "Chunming Wu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "f96b44a72089e238fc53eda17b2243a37d5b5879",
                "externalIds": {
                    "DOI": "10.1016/j.knosys.2023.110772",
                    "CorpusId": 260173519
                },
                "corpusId": 260173519,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f96b44a72089e238fc53eda17b2243a37d5b5879",
                "title": "KE-X: Towards subgraph explanations of knowledge graph embedding based on knowledge information gain",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2221145188",
                        "name": "Dong Zhao"
                    },
                    {
                        "authorId": "1380547970",
                        "name": "Guojia Wan"
                    },
                    {
                        "authorId": "1895813",
                        "name": "Yibing Zhan"
                    },
                    {
                        "authorId": "2145656",
                        "name": "Zengmao Wang"
                    },
                    {
                        "authorId": "46573238",
                        "name": "Liang Ding"
                    },
                    {
                        "authorId": "2232383487",
                        "name": "Zhigao Zheng"
                    },
                    {
                        "authorId": "2142452296",
                        "name": "Bo Du"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "These methods attribute model predictions to graph objects, such as nodes (Vu and Thai 2020), edges (Ying et al. 2019; Luo et al. 2020; Schlichtkrull, De Cao, and Titov 2020; Wang et al. 2021b; Lin, Lan, and Li 2021) and subgraphs (Yuan et al. 2021).",
                "These methods attribute model predictions to graph objects, such as nodes (Vu and Thai 2020), edges (Ying et al. 2019; Luo et al. 2020; Schlichtkrull, De Cao, and Titov 2020; Wang et al. 2021b; Lin, Lan, and Li 2021) and subgraphs (Yuan et al."
            ],
            "citingPaper": {
                "paperId": "80d07f8cf49d0fe88bedde8a578a51fc81d0ed23",
                "externalIds": {
                    "DBLP": "conf/aaai/LiDSQHC23",
                    "DOI": "10.1609/aaai.v37i7.26040",
                    "CorpusId": 259747987
                },
                "corpusId": 259747987,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/80d07f8cf49d0fe88bedde8a578a51fc81d0ed23",
                "title": "Towards Fine-Grained Explainability for Heterogeneous Graph Neural Network",
                "abstract": "Heterogeneous graph neural networks (HGNs) are prominent approaches to node classification tasks on heterogeneous graphs. Despite the superior performance, insights about the predictions made from HGNs are obscure to humans. Existing explainability techniques are mainly proposed for GNNs on homogeneous graphs. They focus on highlighting salient graph objects to the predictions whereas the problem of how these objects affect the predictions remains unsolved. Given heterogeneous graphs with complex structures and rich semantics, it is imperative that salient objects can be accompanied with their influence paths to the predictions, unveiling the reasoning process of HGNs. In this paper, we develop xPath, a new framework that provides fine-grained explanations for black-box HGNs specifying a cause node with its influence path to the target node. In xPath, we differentiate the influence of a node on the prediction w.r.t. every individual influence path, and measure the influence by perturbing graph structure via a novel graph rewiring algorithm. Furthermore, we introduce a greedy search algorithm to find the most influential fine-grained explanations efficiently. Empirical results on various HGNs and heterogeneous graphs show that xPath yields faithful explanations efficiently, outperforming the adaptations of advanced GNN explanation approaches.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "15762417",
                        "name": "Tong Li"
                    },
                    {
                        "authorId": "2028910513",
                        "name": "Jiale Deng"
                    },
                    {
                        "authorId": "2923152",
                        "name": "Yanyan Shen"
                    },
                    {
                        "authorId": "1659028311",
                        "name": "Luyu Qiu"
                    },
                    {
                        "authorId": "9164904",
                        "name": "Hu Yongxiang"
                    },
                    {
                        "authorId": "3151540",
                        "name": "Caleb Chen Cao"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "2019), PGEXplainer (Luo et al. 2020) SubgraphX (Yuan et al.",
                "In this paper, we compare our methods with state-of-the-art GNN explanation methods, including GNNExplainer (Ying et al. 2019), PGEXplainer (Luo et al. 2020) SubgraphX (Yuan et al. 2021) and ReFine (Wang et al. 2021), and inherently interpretable GNN methods, including GAT and GSAT (Miao, Liu, and Li 2022)."
            ],
            "citingPaper": {
                "paperId": "d19060b433e298d126ec287b372d4bb6a14043da",
                "externalIds": {
                    "DBLP": "conf/aaai/Chai0DTMWS23",
                    "DOI": "10.1609/aaai.v37i12.26656",
                    "CorpusId": 259748838
                },
                "corpusId": 259748838,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d19060b433e298d126ec287b372d4bb6a14043da",
                "title": "Towards Learning to Discover Money Laundering Sub-network in Massive Transaction Network",
                "abstract": "Anti-money laundering (AML) systems play a critical role in safeguarding global economy. As money laundering is considered as one of the top group crimes, there is a crucial need to discover money laundering sub-network behind a particular money laundering transaction for a robust AML system. However, existing rule-based methods for money laundering sub-network discovery is heavily based on domain knowledge and may lag behind the modus operandi of launderers. Therefore, in this work, we first address the money laundering sub-network discovery problem with a neural network based approach, and propose an AML framework AMAP equipped with an adaptive sub-network proposer. In particular, we design an adaptive sub-network proposer guided by a supervised contrastive loss to discriminate money laundering transactions from massive benign transactions. We conduct extensive experiments on real-word datasets in AliPay of Ant Group. The result demonstrates the effectiveness of our AMAP in both money laundering transaction detection and money laundering sub-network discovering. The learned framework which yields money laundering sub-network from massive transaction network leads to a more comprehensive risk coverage and a deeper insight to money laundering strategies.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2121930095",
                        "name": "Ziwei Chai"
                    },
                    {
                        "authorId": "2152918752",
                        "name": "Yang Yang"
                    },
                    {
                        "authorId": "151045932",
                        "name": "Jiawang Dan"
                    },
                    {
                        "authorId": "2671378",
                        "name": "Sheng Tian"
                    },
                    {
                        "authorId": "2114323322",
                        "name": "Changhua Meng"
                    },
                    {
                        "authorId": null,
                        "name": "Weiqiang Wang"
                    },
                    {
                        "authorId": "2125100601",
                        "name": "Yifei Sun"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "8fe18677230648c84708e66606511d60c0d22e97",
                "externalIds": {
                    "DBLP": "conf/iwqos/RenWLLX23",
                    "DOI": "10.1109/IWQoS57198.2023.10188773",
                    "CorpusId": 260254559
                },
                "corpusId": 260254559,
                "publicationVenue": {
                    "id": "a4ebd690-6408-482a-8e37-7be6749c060d",
                    "name": "International Workshop on Quality of Service",
                    "type": "conference",
                    "alternate_names": [
                        "IWQoS",
                        "Int Workshop Qual Serv"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1855"
                },
                "url": "https://www.semanticscholar.org/paper/8fe18677230648c84708e66606511d60c0d22e97",
                "title": "Triple:The Interpretable Deep Learning Anomaly Detection Framework based on Trace-Metric-Log of Microservice",
                "abstract": "Existing anomaly detection approaches based on deep-learning just could simultaneously dig out key information from two dimensions in the traces, metrics or logs. Besides, they just output simple binary result, which ignores the key artificial statement information in the log. In this paper, we propose Triple, an interpretable anomaly detection approach based on deep learning for microservice system. More importantly, Triple aims to help engineers to establish trust in the system decision from key metrics and the artificial statements in logs. Triple leverages graph representation to describe the complicated dependency relationship in the traces with the logs and metrics embedded into the node features. Based on the graph representation, Triple trains a Spatial-Temporal Graph Convolutional Network(STGCN) to capture the key information and generate decision boundary by deep SVDD, which detects the system's anomaly. In addition, we design an interpreter to transfer the simple binary result into a humanly understandable result, including log, metrics and trace, to facilitate engineers' understanding and handling of the incoming incident. Our work has four aims. First, to the best of our knowledge, we are the first to simultaneously apply three data sources to finish anomaly detection in the domain. Second, we design a new anomaly detection method that is an STGCN based on SVDD. Third, we design an interpreter that makes the decision not only a simple binary result. The interpretable result could capture the key artificial statement information in the log and assist engineers in incident troubleshooting. Finally, we design a series experiments to validate our method's effectiveness in the real-world system's dataset. Our results show that Triple consistently achieves improvements over other state-of-the-art models by 11%-65%.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2225422822",
                        "name": "Rui Ren"
                    },
                    {
                        "authorId": null,
                        "name": "Yang Wang"
                    },
                    {
                        "authorId": "2152942854",
                        "name": "Fengrui Liu"
                    },
                    {
                        "authorId": "50819554",
                        "name": "Zhenyu Li"
                    },
                    {
                        "authorId": "1744392",
                        "name": "Gaogang Xie"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [],
            "contexts": [
                "PGExplainer follows the same paradigm as GNNExplainer which maximizes the mutual information between the predictions of the input graph and that of the evidence subgraph; however, it only generates edge masks by using a deep neural network to parameterize the generation process of the evidence subgraph.",
                "Due to its parameterized generation process, PGExplainer can explain multiple instances collectively and also works in an inductive setting.",
                ", GNNExplainer [24], PGExplainer [10], GraphMask [14], and SubgraphX [27]), that is, masking some node features and/or edge features and analyzing the resulting changes when the modified graphs are passed through the GNN model.",
                "They could be perturbation-based (e.g., GNNExplainer [24], PGExplainer [10], GraphMask [14], and SubgraphX [27]), that is, masking some node features and/or edge features and analyzing the resulting changes when the modified graphs are passed through the GNN model."
            ],
            "citingPaper": {
                "paperId": "67be1c41a7ae7e212f1365f9c90b363733f45579",
                "externalIds": {
                    "DBLP": "conf/grades/Mobaraki023",
                    "DOI": "10.1145/3594778.3594880",
                    "CorpusId": 259213245
                },
                "corpusId": 259213245,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/67be1c41a7ae7e212f1365f9c90b363733f45579",
                "title": "A Demonstration of Interpretability Methods for Graph Neural Networks",
                "abstract": "Graph neural networks (GNNs) are widely used in many downstream applications, such as graphs and nodes classification, entity resolution, link prediction, and question answering. Several interpretability methods for GNNs have been proposed recently. However, since they have not been thoroughly compared with each other, their trade-offs and efficiency in the context of underlying GNNs and downstream applications are unclear. To support more research in this domain, we develop an end-to-end interactive tool, named gInterpreter, by re-implementing 15 recent GNN interpretability methods in a common environment on top of a number of state-of-the-art GNNs employed for different downstream tasks. This paper demonstrates gInterpreter with an interactive performance profiling of 15 recent GNN inter-pretability methods, aiming to explain the complex deep learning pipelines over graph-structured data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220350635",
                        "name": "Ehsan Bonabi Mobaraki"
                    },
                    {
                        "authorId": "2108514592",
                        "name": "Arijit Khan"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "Perturbations-based methods monitor the change of prediction with respect to different input perturbations, including GNNExplainer [8], PGExplainer [9], GraphMask [10], SubgraphX [24], and so on.",
                "PGExplainer [9] learns a parameterized model to predict the importance of an edge.",
                "Yuan\u2019s experimental results show that SubgraphX achieves significantly improved explanations compared with PGExplainer [9] and GNNExplainer [8].",
                "After that, instance-level methods such as PGExplainer [9] and GraphMask [10] gradually became the research focus.",
                "In recent studies [9], [41], it is known that carbon rings and NO2 groups tend to be mutagenic."
            ],
            "citingPaper": {
                "paperId": "5409798cf2d25e60bd5f2ff5cbbf8dbca2774b49",
                "externalIds": {
                    "DBLP": "conf/ijcnn/DingLYWX23",
                    "DOI": "10.1109/IJCNN54540.2023.10191684",
                    "CorpusId": 260387935
                },
                "corpusId": 260387935,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/5409798cf2d25e60bd5f2ff5cbbf8dbca2774b49",
                "title": "MEGA: Explaining Graph Neural Networks with Network Motifs",
                "abstract": "Graph Neural Networks (GNNs) are powerful tools for graph representation. However, GNNs have remained black boxes, leading to the lack of explainability. As a consequence, the application of GNNs has been severely limited. Existing methods focus on generating an explanation with important nodes and edges but pay less attention to high-order structures (e.g., network motif), which are more intuitive and important for graph data. The explanations generated by the explainer will thus ignore the high-order information contained in multi-node neighbours, resulting in a decrease in the human comprehensibility of the explanation subgraph. In this paper, we propose a motif-aware GNNs explainer (MEGA), wherein a motif-aware subgraph generation module and a counterfactual optimization layer are employed. MEGA can provide high-quality counterfactual explanations for GNNs while focusing on high-order features of graph data. We justify the effectiveness of the proposed MEGA on both synthetic and real-world datasets. Experimental results show that MEGA outperforms state-of-the-art baselines while keeping explanations at a smaller level.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2064425407",
                        "name": "Feng Ding"
                    },
                    {
                        "authorId": "2192325640",
                        "name": "Naiwen Luo"
                    },
                    {
                        "authorId": "50443544",
                        "name": "Shuo Yu"
                    },
                    {
                        "authorId": "2226804056",
                        "name": "Tingting Wang"
                    },
                    {
                        "authorId": "2143633281",
                        "name": "Feng Xia"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "4358a86a226ddab773814712f4843bed23855ab1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-10456",
                    "ArXiv": "2306.10456",
                    "DOI": "10.48550/arXiv.2306.10456",
                    "CorpusId": 259202568
                },
                "corpusId": 259202568,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4358a86a226ddab773814712f4843bed23855ab1",
                "title": "Advancing Biomedicine with Graph Representation Learning: Recent Progress, Challenges, and Future Directions",
                "abstract": "Graph representation learning (GRL) has emerged as a pivotal field that has contributed significantly to breakthroughs in various fields, including biomedicine. The objective of this survey is to review the latest advancements in GRL methods and their applications in the biomedical field. We also highlight key challenges currently faced by GRL and outline potential directions for future research.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2146327413",
                        "name": "Fang Li"
                    },
                    {
                        "authorId": "41210261",
                        "name": "Yi Nian"
                    },
                    {
                        "authorId": "2217995931",
                        "name": "Zenan Sun"
                    },
                    {
                        "authorId": "2209356438",
                        "name": "Cui Tao"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "They are gradient-based [22], perturbation-based [31, 18], decomposition-based [11], and surrogate-based methods [27].",
                "To improve transparency and understand the behavior of GNNs, most recent works focus on providing post-hoc interpretation, which aims at explaining what patterns a pre-trained GNN model uses to make decisions [31, 18, 34, 28]."
            ],
            "citingPaper": {
                "paperId": "89ed2892d71cbbf9fa3500a07f1d2360c23c9cf6",
                "externalIds": {
                    "ArXiv": "2306.10447",
                    "DBLP": "journals/corr/abs-2306-10447",
                    "DOI": "10.48550/arXiv.2306.10447",
                    "CorpusId": 259202663
                },
                "corpusId": 259202663,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/89ed2892d71cbbf9fa3500a07f1d2360c23c9cf6",
                "title": "In-Process Global Interpretation for Graph Learning via Distribution Matching",
                "abstract": "Graphs neural networks (GNNs) have emerged as a powerful graph learning model due to their superior capacity in capturing critical graph patterns. To gain insights about the model mechanism for interpretable graph learning, previous efforts focus on post-hoc local interpretation by extracting the data pattern that a pre-trained GNN model uses to make an individual prediction. However, recent works show that post-hoc methods are highly sensitive to model initialization and local interpretation can only explain the model prediction specific to a particular instance. In this work, we address these limitations by answering an important question that is not yet studied: how to provide global interpretation of the model training procedure? We formulate this problem as in-process global interpretation, which targets on distilling high-level and human-intelligible patterns that dominate the training procedure of GNNs. We further propose Graph Distribution Matching (GDM) to synthesize interpretive graphs by matching the distribution of the original and interpretive graphs in the feature space of the GNN as its training proceeds. These few interpretive graphs demonstrate the most informative patterns the model captures during training. Extensive experiments on graph classification datasets demonstrate multiple advantages of the proposed method, including high explanation accuracy, time efficiency and the ability to reveal class-relevant structure.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "41210261",
                        "name": "Yi Nian"
                    },
                    {
                        "authorId": "144767914",
                        "name": "Wei Jin"
                    },
                    {
                        "authorId": "9538465",
                        "name": "Lu Lin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Many works [29], [30], [31], [32], [33], [34], [35], [36] have been proposed to extract meaningful data patterns for prediction by post-hoc methods [31], [34] as well as inherently interpretable models [29], [32], [33]."
            ],
            "citingPaper": {
                "paperId": "b3bb92f1eae54dc56a3a6d27e2934f186db7a9ba",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-10505",
                    "ArXiv": "2306.10505",
                    "DOI": "10.48550/arXiv.2306.10505",
                    "CorpusId": 259203698
                },
                "corpusId": 259203698,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b3bb92f1eae54dc56a3a6d27e2934f186db7a9ba",
                "title": "Structure-Sensitive Graph Dictionary Embedding for Graph Classification",
                "abstract": "Graph structure expression plays a vital role in distinguishing various graphs. In this work, we propose a Structure-Sensitive Graph Dictionary Embedding (SS-GDE) framework to transform input graphs into the embedding space of a graph dictionary for the graph classification task. Instead of a plain use of a base graph dictionary, we propose the variational graph dictionary adaptation (VGDA) to generate a personalized dictionary (named adapted graph dictionary) for catering to each input graph. In particular, for the adaptation, the Bernoulli sampling is introduced to adjust substructures of base graph keys according to each input, which increases the expression capacity of the base dictionary tremendously. To make cross-graph measurement sensitive as well as stable, multi-sensitivity Wasserstein encoding is proposed to produce the embeddings by designing multi-scale attention on optimal transport. To optimize the framework, we introduce mutual information as the objective, which further deduces to variational inference of the adapted graph dictionary. We perform our SS-GDE on multiple datasets of graph classification, and the experimental results demonstrate the effectiveness and superiority over the state-of-the-art methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152756082",
                        "name": "Guangyi Liu"
                    },
                    {
                        "authorId": "2117882197",
                        "name": "Tong Zhang"
                    },
                    {
                        "authorId": "2108076606",
                        "name": "Xudong Wang"
                    },
                    {
                        "authorId": "2954369",
                        "name": "Wenting Zhao"
                    },
                    {
                        "authorId": "2110714728",
                        "name": "Chuanwei Zhou"
                    },
                    {
                        "authorId": "144801562",
                        "name": "Zhen Cui"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "2c76331ac1676ed7fdd51b8cd744765628e0a181",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-10066",
                    "ArXiv": "2306.10066",
                    "DOI": "10.48550/arXiv.2306.10066",
                    "CorpusId": 259203474
                },
                "corpusId": 259203474,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2c76331ac1676ed7fdd51b8cd744765628e0a181",
                "title": "On the Interplay of Subset Selection and Informed Graph Neural Networks",
                "abstract": "Machine learning techniques paired with the availability of massive datasets dramatically enhance our ability to explore the chemical compound space by providing fast and accurate predictions of molecular properties. However, learning on large datasets is strongly limited by the availability of computational resources and can be infeasible in some scenarios. Moreover, the instances in the datasets may not yet be labelled and generating the labels can be costly, as in the case of quantum chemistry computations. Thus, there is a need to select small training subsets from large pools of unlabelled data points and to develop reliable ML methods that can effectively learn from small training sets. This work focuses on predicting the molecules atomization energy in the QM9 dataset. We investigate the advantages of employing domain knowledge-based data sampling methods for an efficient training set selection combined with informed ML techniques. In particular, we show how maximizing molecular diversity in the training set selection process increases the robustness of linear and nonlinear regression techniques such as kernel methods and graph neural networks. We also check the reliability of the predictions made by the graph neural network with a model-agnostic explainer based on the rate distortion explanation framework.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2186116021",
                        "name": "Niklas Breustedt"
                    },
                    {
                        "authorId": "2131643917",
                        "name": "Paolo Climaco"
                    },
                    {
                        "authorId": "2279864",
                        "name": "J. Garcke"
                    },
                    {
                        "authorId": "22232249",
                        "name": "J. Hamaekers"
                    },
                    {
                        "authorId": "3125779",
                        "name": "Gitta Kutyniok"
                    },
                    {
                        "authorId": "34678892",
                        "name": "D. Lorenz"
                    },
                    {
                        "authorId": "2220304429",
                        "name": "Rick Oerder"
                    },
                    {
                        "authorId": "2192822479",
                        "name": "Chirag Varun Shukla"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Several works are proposed to ind a subgraph structure and a small subset of node features for the target nodes as the explanations for GNN\u2019s predictions [49, 52, 88].",
                "PGExplainer [52] further learns the approximated discrete masks on edges to explain the predictions with a parameterized mask predictor."
            ],
            "citingPaper": {
                "paperId": "a1a58b125433517c2cf483e4bcf4852fcce75dcf",
                "externalIds": {
                    "DOI": "10.1145/3604427",
                    "CorpusId": 259149519
                },
                "corpusId": 259149519,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a1a58b125433517c2cf483e4bcf4852fcce75dcf",
                "title": "Invariant Node Representation Learning under Distribution Shifts with Multiple Latent Environments",
                "abstract": "Node representation learning methods, such as graph neural networks, show promising results when testing and training graph data come from the same distribution. However, the existing approaches fail to generalize under distribution shifts when the nodes reside in multiple latent environments. How to learn invariant node representations to handle distribution shifts with multiple latent environments remains unexplored. In this paper, we propose a novel Invariant Node representation Learning (INL) approach capable of generating invariant node representations based on the invariant patterns under distribution shifts with multiple latent environments by leveraging the invariance principle. Specifically, we define invariant and variant patterns as ego-subgraphs of each node, and identify the invariant ego-subgraphs through jointly accounting for node features and graph structures. In order to infer the latent environments of nodes, we propose a contrastive modularity-based graph clustering method based on the variant patterns. We further propose an invariant learning module to learn node representations that can generalize to distribution shifts. We theoretically show that our proposed method can achieve guaranteed performance under distribution shifts. Extensive experiments on both synthetic and real-world node classification benchmarks demonstrate that our method greatly outperforms state-of-the-art baselines under distribution shifts.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144911687",
                        "name": "Haoyang Li"
                    },
                    {
                        "authorId": "2116460208",
                        "name": "Ziwei Zhang"
                    },
                    {
                        "authorId": "153316152",
                        "name": "Xin Wang"
                    },
                    {
                        "authorId": "2156154955",
                        "name": "Wenwu Zhu"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "The experiments are conducted on six benchmark datasets: BA-Shapes, BA-Community, Tree-Cycles [36] and ogbn-arxiv [14] for node classification, and BA-2Motifs [24] and MUTAG [7] for graph classification.",
                "Among the baseline methods, GNNExplainer, PGExplainer and OrphicX adopt the MI value for node attribution, which is less effective than LARA, especially on ogbn-arxiv.",
                "Moreover, LARA exhibits about 90% less time latency compared to the competitive baseline PGExplainer in general.",
                "We benchmark our methods with GNNExplainer, PGExplainer, and GraphSVX on synthetic datasets in Table 9.",
                "According to the ground truth explanation, the key subgraph structure for the classification is a pentagon or a house structure on the BA-2Motifs dataset.",
                "We deploy LARA to explain node classification on BA-Shapes, BA-Community, Tree-Cycles and ogbn-arxiv; and graph classification on the BA-2Motifs and MUTAG datasets.",
                "We evaluate the faithfulness and efficiency of LARA compared to state-of-the-art methods: GNNExplainer [36], PGExplainer [24], GraphSVX [9] and OrphicX [19].",
                "PGExplainer [24] learns a model using the reparameterization trick to predict edge masks indicating their importance, while OrphicX [19] identifies the causal factors by maximizing the information flow from the latent features to the model predictions, which are used to produce explanations.",
                "Node # 102 103 104 105\nGNNExplainer 15.34 119.84 1186.43 12101.1 PGExplainer 5.63 35.52 490.48 3534.65 LARA 0.54 4.23 39.92 416.50\nTable 4: Effectiveness of explainer layer number.",
                "On BA-2Motifs, the graph is classified by the type of motifs.",
                ", MI with regularization [24] or the causal counterpart of MI [19]), so that the trained explainer provides explanatory features (e.",
                "The BA-2Motifs has 1, 000 synthetic graphs with binary labels, while MUTAG contains 4, 337 molecule graphs with binary labels.",
                "D.2 Target Model Details\nFor each dataset, we use the target model under the settings as given in [24] for BA-Shapes, BACommunity, Tree-Cycles, BA-2Motifs and MUTAG 2.",
                "4 Code for GNNExplainer and PGExplainer is at https://github.com/LarsHoldijk/RE-ParameterizedExplainerForGraphNeuralNetworks; for\nOrphicX is at https://github.com/wanyugroup/cvpr2022-orphicx; for GraphSVX is at https://github.com/AlexDuvalinho/GraphSVX.",
                "Although PGExplainer and OrphicX can simultaneously generate a batch of explanations, their time complexity increases with the edge number contained in the relevant subgraph of the target node.",
                "The baseline follows existing work [36, 24] to adopt the mutual information for the supervision of training the explainer.",
                "In order to provide efficient GNN explanations, previous work [24, 18, 30, 4] attempts to train a deep neural network-based explainer, amortizing the time and resource cost of generating explanations of many samples.",
                "In particular, as the node number grows from 102 to 105, GNNexplainer and PGExplainer show approximate 12000s and 3500s growth of time latency (\u2206 time latency), respectively."
            ],
            "citingPaper": {
                "paperId": "a2284f5fbee3eb250fe69d84881680dc388de473",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-05760",
                    "ArXiv": "2306.05760",
                    "DOI": "10.48550/arXiv.2306.05760",
                    "CorpusId": 259129511
                },
                "corpusId": 259129511,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a2284f5fbee3eb250fe69d84881680dc388de473",
                "title": "Efficient GNN Explanation via Learning Removal-based Attribution",
                "abstract": "As Graph Neural Networks (GNNs) have been widely used in real-world applications, model explanations are required not only by users but also by legal regulations. However, simultaneously achieving high fidelity and low computational costs in generating explanations has been a challenge for current methods. In this work, we propose a framework of GNN explanation named LeArn Removal-based Attribution (LARA) to address this problem. Specifically, we introduce removal-based attribution and demonstrate its substantiated link to interpretability fidelity theoretically and experimentally. The explainer in LARA learns to generate removal-based attribution which enables providing explanations with high fidelity. A strategy of subgraph sampling is designed in LARA to improve the scalability of the training process. In the deployment, LARA can efficiently generate the explanation through a feed-forward pass. We benchmark our approach with other state-of-the-art GNN explanation methods on six datasets. Results highlight the effectiveness of our framework regarding both efficiency and fidelity. In particular, LARA is 3.5 times faster and achieves higher fidelity than the state-of-the-art method on the large dataset ogbn-arxiv (more than 160K nodes and 1M edges), showing its great potential in real-world applications. Our source code is available at https://anonymous.4open.science/r/LARA-10D8/README.md.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2057815422",
                        "name": "Yao Rong"
                    },
                    {
                        "authorId": "32780441",
                        "name": "Guanchu Wang"
                    },
                    {
                        "authorId": "2151233715",
                        "name": "Qizhang Feng"
                    },
                    {
                        "authorId": "47717322",
                        "name": "Ninghao Liu"
                    },
                    {
                        "authorId": "47781070",
                        "name": "Zirui Liu"
                    },
                    {
                        "authorId": "1884159",
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "authorId": "2123553641",
                        "name": "Xia Hu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Existing Works: At a high level, GNN explainers can be classified into the two groups of instancelevel [32, 14, 18, 36, 7, 35, 13, 21, 12, 4, 1, 27] or model-level explanations [33].",
                "Instance-level methods can broadly be grouped into two categories: factual reasoning [32, 14, 18, 36, 7, 35] and counterfactual reasoning [13, 21, 4, 1, 27]."
            ],
            "citingPaper": {
                "paperId": "83a2f57677ea625a80bb438f6080a5649d14b3bb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-04835",
                    "ArXiv": "2306.04835",
                    "DOI": "10.48550/arXiv.2306.04835",
                    "CorpusId": 259108268
                },
                "corpusId": 259108268,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/83a2f57677ea625a80bb438f6080a5649d14b3bb",
                "title": "Empowering Counterfactual Reasoning over Graph Neural Networks through Inductivity",
                "abstract": "Graph neural networks (GNNs) have various practical applications, such as drug discovery, recommendation engines, and chip design. However, GNNs lack transparency as they cannot provide understandable explanations for their predictions. To address this issue, counterfactual reasoning is used. The main goal is to make minimal changes to the input graph of a GNN in order to alter its prediction. While several algorithms have been proposed for counterfactual explanations of GNNs, most of them have two main drawbacks. Firstly, they only consider edge deletions as perturbations. Secondly, the counterfactual explanation models are transductive, meaning they do not generalize to unseen data. In this study, we introduce an inductive algorithm called INDUCE, which overcomes these limitations. By conducting extensive experiments on several datasets, we demonstrate that incorporating edge additions leads to better counterfactual results compared to the existing methods. Moreover, the inductive modeling approach allows INDUCE to directly predict counterfactual perturbations without requiring instance-specific training. This results in significant computational speed improvements compared to baseline methods and enables scalable counterfactual analysis for GNNs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "143665702",
                        "name": "S. Verma"
                    },
                    {
                        "authorId": "2219690090",
                        "name": "Burouj Armgaan"
                    },
                    {
                        "authorId": "3390598",
                        "name": "Sourav Medya"
                    },
                    {
                        "authorId": "1699732",
                        "name": "Sayan Ranu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Instance-level algorithms provide explanations for individual predictions of the GNN and include methods that utilize the gradients of the features to determine input importance, such as sensitivity analysis, Guided BP, and Grad-CAM [2,33,30], perturb inputs to observe changes in output as in GNNExplainer and PGExplainer [42,20], and learn relationships between the input and its neighbors using surrogate models [15,38]."
            ],
            "citingPaper": {
                "paperId": "1e3273a6e2aeab1dbb8f71ddf7731215561d6085",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-04791",
                    "ArXiv": "2306.04791",
                    "DOI": "10.48550/arXiv.2306.04791",
                    "CorpusId": 259108514
                },
                "corpusId": 259108514,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1e3273a6e2aeab1dbb8f71ddf7731215561d6085",
                "title": "XInsight: Revealing Model Insights for GNNs with Flow-based Explanations",
                "abstract": "Progress in graph neural networks has grown rapidly in recent years, with many new developments in drug discovery, medical diagnosis, and recommender systems. While this progress is significant, many networks are `black boxes' with little understanding of the `what' exactly the network is learning. Many high-stakes applications, such as drug discovery, require human-intelligible explanations from the models so that users can recognize errors and discover new knowledge. Therefore, the development of explainable AI algorithms is essential for us to reap the benefits of AI. We propose an explainability algorithm for GNNs called eXplainable Insight (XInsight) that generates a distribution of model explanations using GFlowNets. Since GFlowNets generate objects with probabilities proportional to a reward, XInsight can generate a diverse set of explanations, compared to previous methods that only learn the maximum reward sample. We demonstrate XInsight by generating explanations for GNNs trained on two graph classification tasks: classifying mutagenic compounds with the MUTAG dataset and classifying acyclic graphs with a synthetic dataset that we have open-sourced. We show the utility of XInsight's explanations by analyzing the generated compounds using QSAR modeling, and we find that XInsight generates compounds that cluster by lipophilicity, a known correlate of mutagenicity. Our results show that XInsight generates a distribution of explanations that uncovers the underlying relationships demonstrated by the model. They also highlight the importance of generating a diverse set of explanations, as it enables us to discover hidden relationships in the model and provides valuable guidance for further analysis.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2158367804",
                        "name": "Eli J. Laird"
                    },
                    {
                        "authorId": "2092454493",
                        "name": "Ayesh Madushanka"
                    },
                    {
                        "authorId": "3162215",
                        "name": "E. Kraka"
                    },
                    {
                        "authorId": "9261516",
                        "name": "Corey Clark"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "f903e5bf67161e0229aa0890a703e0d891d3a229",
                "externalIds": {
                    "DOI": "10.1109/icassp49357.2023.10094812",
                    "CorpusId": 258530589
                },
                "corpusId": 258530589,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/f903e5bf67161e0229aa0890a703e0d891d3a229",
                "title": "Towards a More Stable and General Subgraph Information Bottleneck",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2216391097",
                        "name": "Hongzhi Liu"
                    },
                    {
                        "authorId": "152804328",
                        "name": "Kaizhong Zheng"
                    },
                    {
                        "authorId": "2462771",
                        "name": "Shujian Yu"
                    },
                    {
                        "authorId": "2108424611",
                        "name": "Badong Chen"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "We compare MSInterpreter with four popular explainable methods PGExplainer (Luo et al., 2020), GNNExplainer (Ying et al.",
                "For the BA2Motifs dataset, we compute the four metrics only for correctly predicted data.",
                "\u2022 BA2Motifs is a synthetic graph motif detection dataset.",
                "BA2Motifs contains BarabasiAlbert (BA) base graphs of size 20 and each graph has five node patterns.",
                "Recently, several advanced approaches (Ying et al., 2019; Luo et al., 2020; Schlichtkrull et al., 2021; Huang et al., 2022; Vu & Thai, 2020; Gui et al., 2022; Yuan et al., 2021; Schnake et al., 2021; Yuan et al., 2020; Yu & Gao, 2022) have been proposed to explain the predictions of graph neural\u2026",
                "The statistics for the BA2Motif (Luo et al., 2020) and MUTAG0 (Tan et al., 2022) datasets are shown in Table 2.",
                "We compare MSInterpreter with four popular explainable methods PGExplainer (Luo et al., 2020), GNNExplainer (Ying et al., 2019) ,SubgraphX (Yuan et al., 2021), and GStarX (Zhang et al., 2022).",
                "Recently, several advanced approaches (Ying et al., 2019; Luo et al., 2020; Schlichtkrull et al., 2021; Huang et al., 2022; Vu & Thai, 2020; Gui et al., 2022; Yuan et al., 2021; Schnake et al., 2021; Yuan et al., 2020; Yu & Gao, 2022) have been proposed to explain the predictions of graph neural networks (GNNs), and are divided into two categories (Yuan et al.",
                "We compare two datasets with edge interpretation labels: MUTAG0 and BA2Motifs."
            ],
            "citingPaper": {
                "paperId": "617bff50277c442fa895611f8f2b97e6ab0d3f30",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-02081",
                    "ArXiv": "2306.02081",
                    "DOI": "10.48550/arXiv.2306.02081",
                    "CorpusId": 259075505
                },
                "corpusId": 259075505,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/617bff50277c442fa895611f8f2b97e6ab0d3f30",
                "title": "Message-passing selection: Towards interpretable GNNs for graph classification",
                "abstract": "In this paper, we strive to develop an interpretable GNNs' inference paradigm, termed MSInterpreter, which can serve as a plug-and-play scheme readily applicable to various GNNs' baselines. Unlike the most existing explanation methods, MSInterpreter provides a Message-passing Selection scheme(MSScheme) to select the critical paths for GNNs' message aggregations, which aims at reaching the self-explaination instead of post-hoc explanations. In detail, the elaborate MSScheme is designed to calculate weight factors of message aggregation paths by considering the vanilla structure and node embedding components, where the structure base aims at weight factors among node-induced substructures; on the other hand, the node embedding base focuses on weight factors via node embeddings obtained by one-layer GNN.Finally, we demonstrate the effectiveness of our approach on graph classification benchmarks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108635077",
                        "name": "Wen-Ding Li"
                    },
                    {
                        "authorId": "145937448",
                        "name": "Kaixuan Chen"
                    },
                    {
                        "authorId": "2128786021",
                        "name": "Shunyu Liu"
                    },
                    {
                        "authorId": null,
                        "name": "Wenjie Huang"
                    },
                    {
                        "authorId": "1739347431",
                        "name": "Haofei Zhang"
                    },
                    {
                        "authorId": "2218856462",
                        "name": "Yingjie Tian"
                    },
                    {
                        "authorId": "2218839057",
                        "name": "Yun Su"
                    },
                    {
                        "authorId": "2152127912",
                        "name": "Mingli Song"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Lastly, GLGExplainer [2] uses local explanations from PGExplainer [56] and projects them to a set of learned prototype or concepts (similar to ProtGNN [125]) to derive a concept vector.",
                "We highlight six popular synthetic datasets:\nBA-Shapes [115]: This graph is formed by randomly connecting a base graph to a set of motifs.",
                "Information constraints: GIB [118], VGIB [116], GSAT [69], LRI [70]; Structural Constraints: DIR [107], ProtGNN [125], SEGNN [12], KER-GNN [21]; Decomposition: CAM [77], Excitation-BP [77], DEGREE [22], GNN-LRP [84]; Gradient-based: SA [5] , Guided-BP [5] , GradCAM [77]; Surrogate: PGM-Ex [56], GraphLime [34], GraphSVX [17], ReLex [124], DnX [75]; Perturbationbased: GNNExplainer[115], GraphMask [82], PGExplainer [56], ReFine [100], ZORRO [23], SubgraphX [122], GstarX [123]; Generation: XGNN [119], RGExplainer [85], GNNInterpreter [101], GFlowExplainer [46], GEM [49]; (2) Counterfactual.",
                "Due to these challenges, explaining graph neural networks is non-trivial and a large variety of methods have been proposed in the literature to tackle it [115, 69, 107, 77, 5, 97, 56, 119, 93, 4, 73].",
                "For instance, they may involve identifying important substructures within the input data [56, 82, 122], providing additional examples from the training data [12], or constructing counterfactual examples by perturbing the input to produce a different prediction outcome [55, 93, 9].",
                "GNNExplainer [115] Continuous relaxation MI Size Yes SubgraphX [122] Monte Carlo Tree Search SV Size, connectivity No GraphMask [82] Layer-wise parameterized edge selection L0 norm Prediction divergence No PGexplainer [56] Parameterized edge selection MI Size and/or connectivity No Zorro [23] Greedy selection Fidelity Threshold fidelity Yes ReFine [100] Parameterized edge attribution MI Number of edges No GstarX [123] Monte Carlo sampling HN-value Size No",
                "RCExplainer [4] Instance level Graph classification Node classification Edge prediction with Neural Network Mutag [14], BA-2motifs [56], NCI1 [99] Tree-Cycles [115], Tree-Grids [115] BA-Shapes [56], BA-Community [115]",
                "BA-Community [115]: The BA-community graph is a combination of two BA-Shapes graphs.",
                "Decomposition-based: CAM [77], Excitation-BP [77], DEGREE [22], GNNLRP [84]; Gradient-based: SA [5] , Guided-BP [5] , Grad-CAM [77]; Surrogate: PGM-Ex [56], GraphLime [34], GraphSVX [17], ReLex [124], DnX [75]; Perturbation-based: GNNExplainer [115], GraphMask [82], PGExplainer [56], ReFine [100], ZORRO [23], SubgraphX [122], GstarX [123]; Generation-based: XGNN [119], RGExplainer [85], GNNInterpreter [101], GFlowExplainer [46], GEM [49].",
                "In a follow-up work, PGExplainer [56] extends the idea in GNNExplainer by assuming the graph to be a random Gilbert graph, where the probability distribution of edges is conditionally independent."
            ],
            "citingPaper": {
                "paperId": "ea3ecb8b809e7d5ae1bbc267863c0c4e72401a68",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-01958",
                    "ArXiv": "2306.01958",
                    "DOI": "10.48550/arXiv.2306.01958",
                    "CorpusId": 259075297
                },
                "corpusId": 259075297,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ea3ecb8b809e7d5ae1bbc267863c0c4e72401a68",
                "title": "A Survey on Explainability of Graph Neural Networks",
                "abstract": "Graph neural networks (GNNs) are powerful graph-based deep-learning models that have gained significant attention and demonstrated remarkable performance in various domains, including natural language processing, drug discovery, and recommendation systems. However, combining feature information and combinatorial graph structures has led to complex non-linear GNN models. Consequently, this has increased the challenges of understanding the workings of GNNs and the underlying reasons behind their predictions. To address this, numerous explainability methods have been proposed to shed light on the inner mechanism of the GNNs. Explainable GNNs improve their security and enhance trust in their recommendations. This survey aims to provide a comprehensive overview of the existing explainability techniques for GNNs. We create a novel taxonomy and hierarchy to categorize these methods based on their objective and methodology. We also discuss the strengths, limitations, and application scenarios of each category. Furthermore, we highlight the key evaluation metrics and datasets commonly used to assess the explainability of GNNs. This survey aims to assist researchers and practitioners in understanding the existing landscape of explainability methods, identifying gaps, and fostering further advancements in interpretable graph-based machine learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219548757",
                        "name": "Jaykumar Kakkad"
                    },
                    {
                        "authorId": "2219549243",
                        "name": "Jaspal Jannu"
                    },
                    {
                        "authorId": "1571168324",
                        "name": "Kartik Sharma"
                    },
                    {
                        "authorId": "1682418",
                        "name": "C. Aggarwal"
                    },
                    {
                        "authorId": "3390598",
                        "name": "Sourav Medya"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The white box methods GNNExplainer [36], PGExplainer [37], and TAGE [54], have access to the target GNN\u2019s internals."
            ],
            "citingPaper": {
                "paperId": "f6861ce39bad377f05337653eae78102f2c0a510",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-00934",
                    "ArXiv": "2306.00934",
                    "DOI": "10.48550/arXiv.2306.00934",
                    "CorpusId": 258999422
                },
                "corpusId": 258999422,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f6861ce39bad377f05337653eae78102f2c0a510",
                "title": "Interpreting GNN-based IDS Detections Using Provenance Graph Structural Features",
                "abstract": "The black-box nature of complex Neural Network (NN)-based models has hindered their widespread adoption in security domains due to the lack of logical explanations and actionable follow-ups for their predictions. To enhance the transparency and accountability of Graph Neural Network (GNN) security models used in system provenance analysis, we propose PROVEXPLAINER, a framework for projecting abstract GNN decision boundaries onto interpretable feature spaces. We first replicate the decision-making process of GNNbased security models using simpler and explainable models such as Decision Trees (DTs). To maximize the accuracy and fidelity of the surrogate models, we propose novel graph structural features founded on classical graph theory and enhanced by extensive data study with security domain knowledge. Our graph structural features are closely tied to problem-space actions in the system provenance domain, which allows the detection results to be explained in descriptive, human language. PROVEXPLAINER allowed simple DT models to achieve 95% fidelity to the GNN on program classification tasks with general graph structural features, and 99% fidelity on malware detection tasks with a task-specific feature package tailored for direct interpretation. The explanations for malware classification are demonstrated with case studies of five real-world malware samples across three malware families.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2218792397",
                        "name": "Kunal Mukherjee"
                    },
                    {
                        "authorId": "2219236005",
                        "name": "Joshua Wiedemeier"
                    },
                    {
                        "authorId": "49980880",
                        "name": "Tianhao Wang"
                    },
                    {
                        "authorId": "2169156186",
                        "name": "Muhyun Kim"
                    },
                    {
                        "authorId": "2156361901",
                        "name": "Feng Chen"
                    },
                    {
                        "authorId": "1741044",
                        "name": "Murat Kantarcioglu"
                    },
                    {
                        "authorId": "3344254",
                        "name": "Kangkook Jee"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "c491388cdbe2989c07073c92ad427154828145d4",
                "externalIds": {
                    "PubMedCentral": "10295378",
                    "DOI": "10.3390/bioengineering10060701",
                    "CorpusId": 259270228,
                    "PubMed": "37370632"
                },
                "corpusId": 259270228,
                "publicationVenue": {
                    "id": "103075b0-1b66-4b69-9c47-f54875634fba",
                    "name": "Bioengineering",
                    "type": "journal",
                    "issn": "2306-5354",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-354376",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-354376",
                        "https://www.mdpi.com/journal/bioengineering"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c491388cdbe2989c07073c92ad427154828145d4",
                "title": "Personalized Explanations for Early Diagnosis of Alzheimer\u2019s Disease Using Explainable Graph Neural Networks with Population Graphs",
                "abstract": "Leveraging recent advances in graph neural networks, our study introduces an application of graph convolutional networks (GCNs) within a correlation-based population graph, aiming to enhance Alzheimer\u2019s disease (AD) prognosis and illuminate the intricacies of AD progression. This methodological approach leverages the inherent structure and correlations in demographic and neuroimaging data to predict amyloid-beta (A\u03b2) positivity. To validate our approach, we conducted extensive performance comparisons with conventional machine learning models and a GCN model with randomly assigned edges. The results consistently highlighted the superior performance of the correlation-based GCN model across different sample groups in the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) dataset, suggesting the importance of accurately reflecting the correlation structure in population graphs for effective pattern recognition and accurate prediction. Furthermore, our exploration of the model\u2019s decision-making process using GNNExplainer identified unique sets of biomarkers indicative of A\u03b2 positivity in different groups, shedding light on the heterogeneity of AD progression. This study underscores the potential of our proposed approach for more nuanced AD prognoses, potentially informing more personalized and precise therapeutic strategies. Future research can extend these findings by integrating diverse data sources, employing longitudinal data, and refining the interpretability of the model, which potentially has broad applicability to other complex diseases.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2110044122",
                        "name": "S. Kim"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "PGExplainer [24] shares the same objective as GNNExplainer and trains a generative model to generate explanations.",
                "With the advances in network attribution methods [29], extensive attempts have been made to open such \u201cblackboxes\u201d [24,39]."
            ],
            "citingPaper": {
                "paperId": "2270ea2a710f37fcd07a0014357238f71622176b",
                "externalIds": {
                    "DBLP": "conf/cvpr/ChanCMYY23",
                    "ArXiv": "2307.04189",
                    "DOI": "10.1109/CVPR52729.2023.01503",
                    "CorpusId": 259501256
                },
                "corpusId": 259501256,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2270ea2a710f37fcd07a0014357238f71622176b",
                "title": "Histopathology Whole Slide Image Analysis with Heterogeneous Graph Representation Learning",
                "abstract": "Graph-based methods have been extensively applied to whole slide histopathology image (WSI) analysis due to the advantage of modeling the spatial relationships among different entities. However, most of the existing methods focus on modeling WSIs with homogeneous graphs (e.g., with homogeneous node type). Despite their successes, these works are incapable of mining the complex structural relations between biological entities (e.g., the diverse interaction among different cell types) in the WSI. We propose a novel heterogeneous graph-based framework to leverage the inter-relationships among different types of nuclei for WSI analysis. Specifically, we formulate the WSI as a heterogeneous graph with \u201cnucleus-type\u201d attribute to each node and a semantic similarity attribute to each edge. We then present a new heterogeneous-graph edge attribute transformer (HEAT) to take advantage of the edge and node heterogeneity during massage aggregating. Further, we design a new pseudo-label-based semantic-consistent pooling mechanism to obtain graph-level features, which can mitigate the over-parameterization issue of conventional cluster-based pooling. Additionally, observing the limitations of existing association-based localization methods, we propose a causal-driven approach attributing the contribution of each node to improve the interpretability of our framework. Extensive experiments on three public TCGA benchmark datasets demonstrate that our frame-work outperforms the state-of-the-art methods with considerable margins on various tasks. Our codes are available at https://github.com/HKU-MedAI/WSI-HGNN.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2150195625",
                        "name": "Tsai Hor Chan"
                    },
                    {
                        "authorId": "2189374272",
                        "name": "Fernando Julio Cendra"
                    },
                    {
                        "authorId": "2109721656",
                        "name": "Lan Ma"
                    },
                    {
                        "authorId": "1830454829",
                        "name": "Guosheng Yin"
                    },
                    {
                        "authorId": "2342535",
                        "name": "Lequan Yu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "fdb8f4ab2e32e6dfd66e2bc422ecf22862fac088",
                "externalIds": {
                    "DBLP": "journals/pacmmod/WangICWNY23",
                    "DOI": "10.1145/3588956",
                    "CorpusId": 259077170
                },
                "corpusId": 259077170,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fdb8f4ab2e32e6dfd66e2bc422ecf22862fac088",
                "title": "Graph Learning for Interactive Threat Detection in Heterogeneous Smart Home Rule Data",
                "abstract": "The interactions among automation configuration rule data have led to undesired and insecure issues in smart homes, which are known as interactive threats. Most existing solutions use program analysis to identify interactive threats among automation rules, which is not suitable for closed-source platforms. Meanwhile, security policy-based solutions suffer from low detection accuracy because the pre-defined security policies in a single platform can hardly cover diverse interactive threat types across heterogeneous platforms. In this paper, we propose Glint, the first graph learning-based system for interactive threat detection in smart homes. We design a multi-scale graph representation learning model, called ITGNN, for both homogeneous and heterogeneous interaction graph pattern learning. To facilitate graph learning, we build large interaction graph training datasets by multi-domain data fusion from five different platforms. Moreover, Glint detects drifting samples with contrastive learning and improves the generalization ability with transfer learning across heterogeneous platforms. Our evaluation shows that Glint achieves 95.5% accuracy in detecting interactive threats across the five platforms. Besides, we examine a set of user-designed blueprints in the Home Assistant platform and reveal four new types of real-world interactive threats, called \"action block\", \"action ablation\", \"trigger intake\", and \"condition duplicate\", which are cross-platform interactive threats captured by Glint.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3474846",
                        "name": "Guangjing Wang"
                    },
                    {
                        "authorId": "2059895293",
                        "name": "Nikolay Ivanov"
                    },
                    {
                        "authorId": "47440030",
                        "name": "Bocheng Chen"
                    },
                    {
                        "authorId": "2151571370",
                        "name": "Qi Wang"
                    },
                    {
                        "authorId": "1810147",
                        "name": "Thanhvu Nguyen"
                    },
                    {
                        "authorId": "2480351",
                        "name": "Qiben Yan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "01e7326be04eb1f6e654d86a316cfebc2985beb4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-18451",
                    "ArXiv": "2305.18451",
                    "DOI": "10.1145/3580305.3599437",
                    "CorpusId": 258967445
                },
                "corpusId": 258967445,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/01e7326be04eb1f6e654d86a316cfebc2985beb4",
                "title": "Shift-Robust Molecular Relational Learning with Causal Substructure",
                "abstract": "Recently, molecular relational learning, whose goal is to predict the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. In this work, we propose CMRL that is robust to the distributional shift in molecular relational learning by detecting the core substructure that is causally related to chemical reactions. To do so, we first assume a causal relationship based on the domain knowledge of molecular sciences and construct a structural causal model (SCM) that reveals the relationship between variables. Based on the SCM, we introduce a novel conditional intervention framework whose intervention is conditioned on the paired molecule. With the conditional intervention framework, our model successfully learns from the causal substructure and alleviates the confounding effect of shortcut substructures that are spuriously correlated to chemical reactions. Extensive experiments on various tasks with real-world and synthetic datasets demonstrate the superiority of CMRL over state-of-the-art baseline models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153998505",
                        "name": "Namkyeong Lee"
                    },
                    {
                        "authorId": "2182294722",
                        "name": "Kanghoon Yoon"
                    },
                    {
                        "authorId": "51113624",
                        "name": "Gyoung S. Na"
                    },
                    {
                        "authorId": "98057309",
                        "name": "Sein Kim"
                    },
                    {
                        "authorId": "2109120259",
                        "name": "Chanyoung Park"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "PGExplainer [2] applies a similar objective, but samples subgraphs using the reparametrization trick.",
                ", PGExplainer [2] concatenates node representations based on their index order).",
                "Several post-hoc explainers have been proposed for explaining Graph Neural Networks\u2019 predictions using subgraphs [1, 2, 30, 3, 31, 29].",
                "Finally, we consider inductive GNN explainers: PGExplainer [2], RCExplainer [3], TAGE [29].",
                "Recent papers [1, 2, 3] have proposed different alternative notions of explainability that do not take the user into consideration and instead are validated using examples."
            ],
            "citingPaper": {
                "paperId": "02bed25ef9cc83d25515216dae0d14df1892a502",
                "externalIds": {
                    "ArXiv": "2305.15745",
                    "DBLP": "journals/corr/abs-2305-15745",
                    "DOI": "10.48550/arXiv.2305.15745",
                    "CorpusId": 258888221
                },
                "corpusId": 258888221,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/02bed25ef9cc83d25515216dae0d14df1892a502",
                "title": "Robust Ante-hoc Graph Explainer using Bilevel Optimization",
                "abstract": "Explaining the decisions made by machine learning models for high-stakes applications is critical for increasing transparency and guiding improvements to these decisions. This is particularly true in the case of models for graphs, where decisions often depend on complex patterns combining rich structural and attribute data. While recent work has focused on designing so-called post-hoc explainers, the question of what constitutes a good explanation remains open. One intuitive property is that explanations should be sufficiently informative to enable humans to approximately reproduce the predictions given the data. However, we show that post-hoc explanations do not achieve this goal as their explanations are highly dependent on fixed model parameters (e.g., learned GNN weights). To address this challenge, this paper proposes RAGE (Robust Ante-hoc Graph Explainer), a novel and flexible ante-hoc explainer designed to discover explanations for a broad class of graph neural networks using bilevel optimization. RAGE is able to efficiently identify explanations that contain the full information needed for prediction while still enabling humans to rank these explanations based on their influence. Our experiments, based on graph classification and regression, show that RAGE explanations are more robust than existing post-hoc and ante-hoc approaches and often achieve similar or better accuracy than state-of-the-art models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2134769449",
                        "name": "Mert Kosan"
                    },
                    {
                        "authorId": "2110040391",
                        "name": "A. Silva"
                    },
                    {
                        "authorId": "1399890865",
                        "name": "Ambuj K. Singh"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "It is hypothesized that the nitro group (NO2) is one of the main reasons for the property of mutagenicity [15,17]."
            ],
            "citingPaper": {
                "paperId": "b664a378c26d57be820892b291b43275843b9cfc",
                "externalIds": {
                    "ArXiv": "2305.15961",
                    "DBLP": "journals/corr/abs-2305-15961",
                    "DOI": "10.48550/arXiv.2305.15961",
                    "CorpusId": 258887512
                },
                "corpusId": 258887512,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b664a378c26d57be820892b291b43275843b9cfc",
                "title": "Quantifying the Intrinsic Usefulness of Attributional Explanations for Graph Neural Networks with Artificial Simulatability Studies",
                "abstract": "Despite the increasing relevance of explainable AI, assessing the quality of explanations remains a challenging issue. Due to the high costs associated with human-subject experiments, various proxy metrics are often used to approximately quantify explanation quality. Generally, one possible interpretation of the quality of an explanation is its inherent value for teaching a related concept to a student. In this work, we extend artificial simulatability studies to the domain of graph neural networks. Instead of costly human trials, we use explanation-supervisable graph neural networks to perform simulatability studies to quantify the inherent usefulness of attributional graph explanations. We perform an extensive ablation study to investigate the conditions under which the proposed analyses are most meaningful. We additionally validate our methods applicability on real-world graph classification and regression datasets. We find that relevant explanations can significantly boost the sample efficiency of graph neural networks and analyze the robustness towards noise and bias in the explanations. We believe that the notion of usefulness obtained from our proposed simulatability analysis provides a dimension of explanation quality that is largely orthogonal to the common practice of faithfulness and has great potential to expand the toolbox of explanation quality assessments, specifically for graph explanations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2149076354",
                        "name": "Jonas Teufel"
                    },
                    {
                        "authorId": "12366131",
                        "name": "Luca Torresi"
                    },
                    {
                        "authorId": "35323511",
                        "name": "Pascal Friederich"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Table 1 shows the explanation AUC and time efficiency (the training time is shown outside the parentheses for PGExplainer).",
                "We also split data for baselines requiring additional training (e.g. PGExplainer).",
                "We compare with four baselines methods: GRAD (Ying et al., 2019), GAT (Velic\u030ckovic\u0301 et al., 2017), GNNExplainer (Ying et al., 2019) and PGExplainer (Luo et al., 2020).",
                "Specifically, GNNExplainer and PGExplainer (Ying et al., 2019) maximize the mutual information between perturbed input and original input graph to identify the important features.",
                "For baselines (e.g., PGExplainer) who need training additional modules, we also split the data.",
                "Some other methods borrow the idea from perturbation based explanation (Ying et al., 2019; Luo et al., 2020; Lucic et al., 2021), under the assumption that removing the vital information from input would significantly reduce output confidence.",
                "(4) PGExplainer learns an MLP (Multi-layer Perceptron) model to generate the mask using the reparameterization trick (Jang et al., 2017)."
            ],
            "citingPaper": {
                "paperId": "53e41177e6d0deefc3d335e886b72d60e6e37cec",
                "externalIds": {
                    "ArXiv": "2305.12895",
                    "DBLP": "conf/iclr/FengLYTDH22",
                    "DOI": "10.48550/arXiv.2305.12895",
                    "CorpusId": 251649165
                },
                "corpusId": 251649165,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/53e41177e6d0deefc3d335e886b72d60e6e37cec",
                "title": "DEGREE: Decomposition Based Explanation for Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) are gaining extensive attention for their application in graph data. However, the black-box nature of GNNs prevents users from understanding and trusting the models, thus hampering their applicability. Whereas explaining GNNs remains a challenge, most existing methods fall into approximation based and perturbation based approaches with suffer from faithfulness problems and unnatural artifacts, respectively. To tackle these problems, we propose DEGREE \\degree to provide a faithful explanation for GNN predictions. By decomposing the information generation and aggregation mechanism of GNNs, DEGREE allows tracking the contributions of specific components of the input graph to the final prediction. Based on this, we further design a subgraph level interpretation algorithm to reveal complex interactions between graph nodes that are overlooked by previous methods. The efficiency of our algorithm can be further improved by utilizing GNN characteristics. Finally, we conduct quantitative and qualitative experiments on synthetic and real-world datasets to demonstrate the effectiveness of DEGREE on node classification and graph classification tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2151233715",
                        "name": "Qizhang Feng"
                    },
                    {
                        "authorId": "47717322",
                        "name": "Ninghao Liu"
                    },
                    {
                        "authorId": "47829900",
                        "name": "Fan Yang"
                    },
                    {
                        "authorId": "2057059798",
                        "name": "Ruixiang Tang"
                    },
                    {
                        "authorId": "3432460",
                        "name": "Mengnan Du"
                    },
                    {
                        "authorId": "2123553641",
                        "name": "Xia Hu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "There are few attempts of explainers on the node classification task [6, 21, 38, 43], while the work on interpretable link prediction based on GNNs is rather limited [36].",
                "To address the problem of lacking interpretability in GNNs, extensive works have been proposed [13, 21, 38].",
                "PGExplainer [21] generates the edge masks with parameterized explainer to find the significant subgraphs."
            ],
            "citingPaper": {
                "paperId": "d0477e6be78c2fb558488c2fcb301678af570904",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-12578",
                    "ArXiv": "2305.12578",
                    "DOI": "10.48550/arXiv.2305.12578",
                    "CorpusId": 258832424
                },
                "corpusId": 258832424,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d0477e6be78c2fb558488c2fcb301678af570904",
                "title": "Self-Explainable Graph Neural Networks for Link Prediction",
                "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance for link prediction. However, GNNs suffer from poor interpretability, which limits their adoptions in critical scenarios that require knowing why certain links are predicted. Despite various methods proposed for the explainability of GNNs, most of them are post-hoc explainers developed for explaining node classification. Directly adopting existing post-hoc explainers for explaining link prediction is sub-optimal because: (i) post-hoc explainers usually adopt another strategy or model to explain a target model, which could misinterpret the target model; and (ii) GNN explainers for node classification identify crucial subgraphs around each node for the explanation; while for link prediction, one needs to explain the prediction for each pair of nodes based on graph structure and node attributes. Therefore, in this paper, we study a novel problem of self-explainable GNNs for link prediction, which can simultaneously give accurate predictions and explanations. Concretely, we propose a new framework and it can find various $K$ important neighbors of one node to learn pair-specific representations for links from this node to other nodes. These $K$ different neighbors represent important characteristics of the node and model various factors for links from it. Thus, $K$ neighbors can provide explanations for the existence of links. Experiments on both synthetic and real-world datasets verify the effectiveness of the proposed framework for link prediction and explanation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1643792176",
                        "name": "Huaisheng Zhu"
                    },
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    },
                    {
                        "authorId": "48784944",
                        "name": "Xianfeng Tang"
                    },
                    {
                        "authorId": "2150638259",
                        "name": "Junjie Xu"
                    },
                    {
                        "authorId": "2159247988",
                        "name": "Hui Liu"
                    },
                    {
                        "authorId": "2116430057",
                        "name": "Suhang Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                ", 2023a) and important vertex/edge identification (Ying et al., 2019; Luo et al., 2020; Vu and Thai, 2020; Yu et al., 2023; Kan et al., 2022c), towards tasks such as connectomebased disease prediction and multi-level neural pattern discovery."
            ],
            "citingPaper": {
                "paperId": "0d6701a2fe054dc0183d6329160fc46e7dfac980",
                "externalIds": {
                    "DBLP": "conf/chil/YangCY23",
                    "ArXiv": "2305.14376",
                    "DOI": "10.48550/arXiv.2305.14376",
                    "CorpusId": 258865195
                },
                "corpusId": 258865195,
                "publicationVenue": {
                    "id": "67d171e0-fd12-4512-a35d-c4d7af1bd5b3",
                    "name": "ACM Conference on Health, Inference, and Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CHIL",
                        "ACM Conf Health Inference Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0d6701a2fe054dc0183d6329160fc46e7dfac980",
                "title": "PTGB: Pre-Train Graph Neural Networks for Brain Network Analysis",
                "abstract": "The human brain is the central hub of the neurobiological system, controlling behavior and cognition in complex ways. Recent advances in neuroscience and neuroimaging analysis have shown a growing interest in the interactions between brain regions of interest (ROIs) and their impact on neural development and disorder diagnosis. As a powerful deep model for analyzing graph-structured data, Graph Neural Networks (GNNs) have been applied for brain network analysis. However, training deep models requires large amounts of labeled data, which is often scarce in brain network datasets due to the complexities of data acquisition and sharing restrictions. To make the most out of available training data, we propose PTGB, a GNN pre-training framework that captures intrinsic brain network structures, regardless of clinical outcomes, and is easily adaptable to various downstream tasks. PTGB comprises two key components: (1) an unsupervised pre-training technique designed specifically for brain networks, which enables learning from large-scale datasets without task-specific labels; (2) a data-driven parcellation atlas mapping pipeline that facilitates knowledge transfer across datasets with different ROI systems. Extensive evaluations using various GNN models have demonstrated the robust and superior performance of PTGB compared to baseline methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143686435",
                        "name": "Yi Yang"
                    },
                    {
                        "authorId": "2112821580",
                        "name": "Hejie Cui"
                    },
                    {
                        "authorId": "1390553618",
                        "name": "Carl Yang"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "We follow the graph pruning setup in [14, 37] and adapt it to the dynamic graph modeling context.",
                "PGExplainer [14] learns a probabilistic graph generative model to provide explanations or importance on each edge.",
                "Following the practice in [14, 16, 37], we utilize the binary concrete distribution to approximate the sampling process and obtain the sampled sub-",
                "One way to rectify the aforementioned issues is graph pruning [14, 20, 37], a new yet promising technology that has paved the way to meet the challenges of reliable graph learning at scale."
            ],
            "citingPaper": {
                "paperId": "384d2f2f5b6441e1a3f334d9649cf3f546987e28",
                "externalIds": {
                    "ArXiv": "2305.10673",
                    "DBLP": "journals/corr/abs-2305-10673",
                    "DOI": "10.48550/arXiv.2305.10673",
                    "CorpusId": 258762326
                },
                "corpusId": 258762326,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/384d2f2f5b6441e1a3f334d9649cf3f546987e28",
                "title": "Less Can Be More: Unsupervised Graph Pruning for Large-scale Dynamic Graphs",
                "abstract": "The prevalence of large-scale graphs poses great challenges in time and storage for training and deploying graph neural networks (GNNs). Several recent works have explored solutions for pruning the large original graph into a small and highly-informative one, such that training and inference on the pruned and large graphs have comparable performance. Although empirically effective, current researches focus on static or non-temporal graphs, which are not directly applicable to dynamic scenarios. In addition, they require labels as ground truth to learn the informative structure, limiting their applicability to new problem domains where labels are hard to obtain. To solve the dilemma, we propose and study the problem of unsupervised graph pruning on dynamic graphs. We approach the problem by our proposed STEP, a self-supervised temporal pruning framework that learns to remove potentially redundant edges from input dynamic graphs. From a technical and industrial viewpoint, our method overcomes the trade-offs between the performance and the time&memory overheads. Our results on three real-world datasets demonstrate the advantages on improving the efficacy, robustness, and efficiency of GNNs on dynamic node classification tasks. Most notably, STEP is able to prune more than 50% of edges on a million-scale industrial graph Alipay (7M nodes, 21M edges) while approximating up to 98% of the original performance. Code is available at https://github.com/EdisonLeeeee/STEP.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2115953679",
                        "name": "Jintang Li"
                    },
                    {
                        "authorId": "2671378",
                        "name": "Sheng Tian"
                    },
                    {
                        "authorId": "2087049620",
                        "name": "Ruofan Wu"
                    },
                    {
                        "authorId": "2143509733",
                        "name": "Liang Zhu"
                    },
                    {
                        "authorId": "2217580640",
                        "name": "Welong Zhao"
                    },
                    {
                        "authorId": "2114323322",
                        "name": "Changhua Meng"
                    },
                    {
                        "authorId": "2146035112",
                        "name": "Liang Chen"
                    },
                    {
                        "authorId": "144291579",
                        "name": "Zibin Zheng"
                    },
                    {
                        "authorId": "2416851",
                        "name": "Hongzhi Yin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Compounding the issue of GNNs\u2019 limited transparency, GNN explainability methods mostly rely on brittle and untrustworthy local/post-hoc methods (13; 25; 26; 33; 39) or pre-defined subgraphs for explanations (2; 36), which are often unknown in UA.",
                "Inspired by vision approaches (35; 32; 10), early explainability techniques focused on feature importance (30), while subsequent works aimed to extract local explanations (39; 25; 36) or global explanations using conceptual subgraphs by clustering the activation space (26; 40; 27)."
            ],
            "citingPaper": {
                "paperId": "2f5f35acb2f41ac7a04bf259ca51a2b892c790f8",
                "externalIds": {
                    "ArXiv": "2307.11688",
                    "DBLP": "journals/corr/abs-2307-11688",
                    "DOI": "10.48550/arXiv.2307.11688",
                    "CorpusId": 260091795
                },
                "corpusId": 260091795,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2f5f35acb2f41ac7a04bf259ca51a2b892c790f8",
                "title": "Interpretable Graph Networks Formulate Universal Algebra Conjectures",
                "abstract": "The rise of Artificial Intelligence (AI) recently empowered researchers to investigate hard mathematical problems which eluded traditional approaches for decades. Yet, the use of AI in Universal Algebra (UA) -- one of the fields laying the foundations of modern mathematics -- is still completely unexplored. This work proposes the first use of AI to investigate UA's conjectures with an equivalent equational and topological characterization. While topological representations would enable the analysis of such properties using graph neural networks, the limited transparency and brittle explainability of these models hinder their straightforward use to empirically validate existing conjectures or to formulate new ones. To bridge these gaps, we propose a general algorithm generating AI-ready datasets based on UA's conjectures, and introduce a novel neural layer to build fully interpretable graph networks. The results of our experiments demonstrate that interpretable graph networks: (i) enhance interpretability without sacrificing task accuracy, (ii) strongly generalize when predicting universal algebra's properties, (iii) generate simple explanations that empirically validate existing conjectures, and (iv) identify subgraphs suggesting the formulation of novel conjectures.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145514948",
                        "name": "Francesco Giannini"
                    },
                    {
                        "authorId": "35047381",
                        "name": "S. Fioravanti"
                    },
                    {
                        "authorId": "150253747",
                        "name": "O\u011fuzhan Keskin"
                    },
                    {
                        "authorId": "2223643390",
                        "name": "A. Lupidi"
                    },
                    {
                        "authorId": "2098834685",
                        "name": "Lucie Charlotte Magister"
                    },
                    {
                        "authorId": "2075355155",
                        "name": "Pietro Lio'"
                    },
                    {
                        "authorId": "2123005765",
                        "name": "Pietro Barbiero"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Due to the intended objective to learn the CFG node importance, the model is capable of addressing adversarial evasion techniques such as XOR obfuscation, Sematic NOP obfuscation, code manipulation, etc. Experiments with YANCFG dataset [20] over three state-of-the-art models (GNNExplainer [30], SubgraphX [31], and PGExplainer [32]) justify the feasibility of the approach.",
                "Experiments with YANCFG dataset [20] over three state-of-the-art models (GNNExplainer [30], SubgraphX [31], and PGExplainer [32]) justify the feasibility of the approach."
            ],
            "citingPaper": {
                "paperId": "75411bb007f63decc7b34e637282e57ebfa81335",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-08993",
                    "ArXiv": "2305.08993",
                    "DOI": "10.48550/arXiv.2305.08993",
                    "CorpusId": 258714699
                },
                "corpusId": 258714699,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/75411bb007f63decc7b34e637282e57ebfa81335",
                "title": "Survey of Malware Analysis through Control Flow Graph using Machine Learning",
                "abstract": "Malware is a significant threat to the security of computer systems and networks which requires sophisticated techniques to analyze the behavior and functionality for detection. Traditional signature-based malware detection methods have become ineffective in detecting new and unknown malware due to their rapid evolution. One of the most promising techniques that can overcome the limitations of signature-based detection is to use control flow graphs (CFGs). CFGs leverage the structural information of a program to represent the possible paths of execution as a graph, where nodes represent instructions and edges represent control flow dependencies. Machine learning (ML) algorithms are being used to extract these features from CFGs and classify them as malicious or benign. In this survey, we aim to review some state-of-the-art methods for malware detection through CFGs using ML, focusing on the different ways of extracting, representing, and classifying. Specifically, we present a comprehensive overview of different types of CFG features that have been used as well as different ML algorithms that have been applied to CFG-based malware detection. We provide an in-depth analysis of the challenges and limitations of these approaches, as well as suggest potential solutions to address some open problems and promising future directions for research in this field.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2150699855",
                        "name": "Shaswata Mitra"
                    },
                    {
                        "authorId": "40402638",
                        "name": "Stephen Torri"
                    },
                    {
                        "authorId": "2736774",
                        "name": "Sudip Mittal"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026understanding oversmoothing (Li et al., 2018; Zhao & Akoglu, 2020; Oono & Suzuki, 2020a; Rong et al., 2020), interpretability (Ying et al., 2019; Luo et al., 2020; Vu & Thai, 2020; Yuan et al., 2020; 2021), expressiveness (Xu et al., 2019; Chen et al., 2019; Maron et al., 2019; Dehmamy et al.,\u2026",
                ", 2020), interpretability (Ying et al., 2019; Luo et al., 2020; Vu & Thai, 2020; Yuan et al., 2020; 2021), expressiveness (Xu et al."
            ],
            "citingPaper": {
                "paperId": "206ee73f4e3187b5a71c3569bb1d443b8cce9351",
                "externalIds": {
                    "ArXiv": "2305.08048",
                    "DBLP": "journals/corr/abs-2305-08048",
                    "DOI": "10.48550/arXiv.2305.08048",
                    "CorpusId": 258685824
                },
                "corpusId": 258685824,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/206ee73f4e3187b5a71c3569bb1d443b8cce9351",
                "title": "Towards Understanding the Generalization of Graph Neural Networks",
                "abstract": "Graph neural networks (GNNs) are the most widely adopted model in graph-structured data oriented learning and representation. Despite their extraordinary success in real-world applications, understanding their working mechanism by theory is still on primary stage. In this paper, we move towards this goal from the perspective of generalization. To be specific, we first establish high probability bounds of generalization gap and gradients in transductive learning with consideration of stochastic optimization. After that, we provide high probability bounds of generalization gap for popular GNNs. The theoretical results reveal the architecture specific factors affecting the generalization gap. Experimental results on benchmark datasets show the consistency between theoretical results and empirical evidence. Our results provide new insights in understanding the generalization of GNNs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2112701620",
                        "name": "Huayi Tang"
                    },
                    {
                        "authorId": "93006732",
                        "name": "Y. Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "One is node classification trained on BA-Shapes and BA-Community, the other is graph classification trained on MUTAG and Circuits[10].",
                "The Circuits dataset is consisting of 500 circuit graphs."
            ],
            "citingPaper": {
                "paperId": "b01c4f2c4d54723b590a828d4e1b4cdbfea5dad4",
                "externalIds": {
                    "DOI": "10.1109/ICECAI58670.2023.10176786",
                    "CorpusId": 259861082
                },
                "corpusId": 259861082,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b01c4f2c4d54723b590a828d4e1b4cdbfea5dad4",
                "title": "Improved GraphSVX for GNN Explanations Based on Cross Entropy",
                "abstract": "Graph neural networks (GNNs) are a type of neural networks that can operate on graph data structures. And GNNs are difficult to explain. This lack of interpretability is a significant challenge in domains where transparency, accountability, and fairness are essential. This paper proposes an improved version of the GraphSVX method for explaining graph neural networks (GNNs) based cross-entropy. The proposed method combines the strengths of the existing explanation techniques and outperforms the original methods in terms of accuracy. It was evaluated on several benchmark datasets and a self-collected circuit dataset. The improved GraphSVX method provides a promising approach to explain GNNs for both graph classification tasks and node classification tasks, which can increase trust in the model\u2019s decisions and facilitate the development of more transparent and accountable AI systems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118210445",
                        "name": "Xinyao Yu"
                    },
                    {
                        "authorId": "2176030959",
                        "name": "Dong Liang"
                    },
                    {
                        "authorId": "2108049317",
                        "name": "Qinfeng Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "c67f9c6a31bb7087a86e5d58c87cea13ce1eef52",
                "externalIds": {
                    "PubMedCentral": "10423541",
                    "DOI": "10.1136/gutjnl-2023-329512",
                    "CorpusId": 258639921,
                    "PubMed": "37173125"
                },
                "corpusId": 258639921,
                "publicationVenue": {
                    "id": "99b00e0c-56b4-4079-91b7-0fa64b30b1ca",
                    "name": "Gut",
                    "type": "journal",
                    "issn": "0017-5749",
                    "url": "https://gut.bmj.com/",
                    "alternate_urls": [
                        "http://gut.bmj.com/",
                        "http://gut.bmjjournals.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c67f9c6a31bb7087a86e5d58c87cea13ce1eef52",
                "title": "Screening of normal endoscopic large bowel biopsies with interpretable graph learning: a retrospective study",
                "abstract": "Objective To develop an interpretable artificial intelligence algorithm to rule out normal large bowel endoscopic biopsies, saving pathologist resources and helping with early diagnosis. Design A graph neural network was developed incorporating pathologist domain knowledge to classify 6591 whole-slides images (WSIs) of endoscopic large bowel biopsies from 3291 patients (approximately 54% female, 46% male) as normal or abnormal (non-neoplastic and neoplastic) using clinically driven interpretable features. One UK National Health Service (NHS) site was used for model training and internal validation. External validation was conducted on data from two other NHS sites and one Portuguese site. Results Model training and internal validation were performed on 5054 WSIs of 2080 patients resulting in an area under the curve-receiver operating characteristic (AUC-ROC) of 0.98 (SD=0.004) and AUC-precision-recall (PR) of 0.98 (SD=0.003). The performance of the model, named Interpretable Gland-Graphs using a Neural Aggregator (IGUANA), was consistent in testing over 1537 WSIs of 1211 patients from three independent external datasets with mean AUC-ROC=0.97 (SD=0.007) and AUC-PR=0.97 (SD=0.005). At a high sensitivity threshold of 99%, the proposed model can reduce the number of normal slides to be reviewed by a pathologist by approximately 55%. IGUANA also provides an explainable output highlighting potential abnormalities in a WSI in the form of a heatmap as well as numerical values associating the model prediction with various histological features. Conclusion The model achieved consistently high accuracy showing its potential in optimising increasingly scarce pathologist resources. Explainable predictions can guide pathologists in their diagnostic decision-making and help boost their confidence in the algorithm, paving the way for its future clinical adoption.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1393638783",
                        "name": "S. Graham"
                    },
                    {
                        "authorId": "2144851619",
                        "name": "F. Minhas"
                    },
                    {
                        "authorId": "144769161",
                        "name": "M. Bilal"
                    },
                    {
                        "authorId": "2185196473",
                        "name": "Mahmoud Ali"
                    },
                    {
                        "authorId": "39412069",
                        "name": "Y. Tsang"
                    },
                    {
                        "authorId": "48396378",
                        "name": "M. Eastwood"
                    },
                    {
                        "authorId": "40406870",
                        "name": "N. Wahab"
                    },
                    {
                        "authorId": "9965999",
                        "name": "M. Jahanifar"
                    },
                    {
                        "authorId": "51403067",
                        "name": "E. Hero"
                    },
                    {
                        "authorId": "37241103",
                        "name": "K. Dodd"
                    },
                    {
                        "authorId": "50845995",
                        "name": "H. Sahota"
                    },
                    {
                        "authorId": "2112539068",
                        "name": "Shao-Hong Wu"
                    },
                    {
                        "authorId": "1846373",
                        "name": "Wenqi Lu"
                    },
                    {
                        "authorId": "10377090",
                        "name": "A. Azam"
                    },
                    {
                        "authorId": "1656647604",
                        "name": "K. Benes"
                    },
                    {
                        "authorId": "7702841",
                        "name": "M. Nimir"
                    },
                    {
                        "authorId": "3975977",
                        "name": "K. Hewitt"
                    },
                    {
                        "authorId": "145901092",
                        "name": "A. Bhalerao"
                    },
                    {
                        "authorId": "2080565545",
                        "name": "A. Robinson"
                    },
                    {
                        "authorId": "46805778",
                        "name": "H. Eldaly"
                    },
                    {
                        "authorId": "31752042",
                        "name": "S. Raza"
                    },
                    {
                        "authorId": "40503291",
                        "name": "K. Gopalakrishnan"
                    },
                    {
                        "authorId": "47723656",
                        "name": "D. Snead"
                    },
                    {
                        "authorId": "1580315694",
                        "name": "N. Rajpoot"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "[27] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang."
            ],
            "citingPaper": {
                "paperId": "2b333a51c829681c41f0879a3c0241ea8ff559a7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-13010",
                    "ArXiv": "2304.13010",
                    "DOI": "10.48550/arXiv.2304.13010",
                    "CorpusId": 258309271
                },
                "corpusId": 258309271,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2b333a51c829681c41f0879a3c0241ea8ff559a7",
                "title": "Unstructured and structured data: Can we have the best of both worlds with large language models?",
                "abstract": "This paper presents an opinion on the potential of using large language models to query on both unstructured and structured data. It also outlines some research challenges related to the topic of building question-answering systems for both types of data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "34582619",
                        "name": "W. Tan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Recently, the explainability of graph neural networks (GNNs) [19, 20, 21, 22, 23, 24] has been explored [25, 26, 27, 28, 29, 30, 31], but these studies are limited to understanding supervised GNNs.",
                "Moreover, [29, 30, 27, 31] have recently proposed generating explanations for supervised GNN models, but these methods cannot be applied to unsupervised node embedding."
            ],
            "citingPaper": {
                "paperId": "e433e6712ec2475f1c9ce8cb0a715edd96ed4fc1",
                "externalIds": {
                    "ArXiv": "2304.12036",
                    "DBLP": "journals/corr/abs-2304-12036",
                    "DOI": "10.1016/j.neunet.2023.04.029",
                    "CorpusId": 258298674,
                    "PubMed": "37210973"
                },
                "corpusId": 258298674,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e433e6712ec2475f1c9ce8cb0a715edd96ed4fc1",
                "title": "Generating Post-hoc Explanations for Skip-gram-based Node Embeddings by Identifying Important Nodes with Bridgeness",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1845151",
                        "name": "Hogun Park"
                    },
                    {
                        "authorId": "144050371",
                        "name": "Jennifer Neville"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "A small subset of GNN explainers, especially local, perturbation-based methods (such as Yuan et al. (2021); Duval & Malliaros (2021); Luo et al. (2020); Ying et al. (2019)) can be applied to black-box models."
            ],
            "citingPaper": {
                "paperId": "a4ab2885279de2a34e7554aff01ce4c2a8512cf3",
                "externalIds": {
                    "DBLP": "conf/iclr/MasoomiHXHSCID22",
                    "ArXiv": "2304.07670",
                    "DOI": "10.48550/arXiv.2304.07670",
                    "CorpusId": 251648935
                },
                "corpusId": 251648935,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a4ab2885279de2a34e7554aff01ce4c2a8512cf3",
                "title": "Explanations of Black-Box Models based on Directional Feature Interactions",
                "abstract": "As machine learning algorithms are deployed ubiquitously to a variety of domains, it is imperative to make these often black-box models transparent. Several recent works explain black-box models by capturing the most influential features for prediction per instance; such explanation methods are univariate, as they characterize importance per feature. We extend univariate explanation to a higher-order; this enhances explainability, as bivariate methods can capture feature interactions in black-box models, represented as a directed graph. Analyzing this graph enables us to discover groups of features that are equally important (i.e., interchangeable), while the notion of directionality allows us to identify the most influential features. We apply our bivariate method on Shapley value explanations, and experimentally demonstrate the ability of directional explanations to discover feature interactions. We show the superiority of our method against state-of-the-art on CIFAR10, IMDB, Census, Divorce, Drug, and gene data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "146126409",
                        "name": "A. Masoomi"
                    },
                    {
                        "authorId": "2164482366",
                        "name": "Davin Hill"
                    },
                    {
                        "authorId": "50070300",
                        "name": "Zhonghui Xu"
                    },
                    {
                        "authorId": "3157841",
                        "name": "C. Hersh"
                    },
                    {
                        "authorId": "9875892",
                        "name": "E. Silverman"
                    },
                    {
                        "authorId": "35190471",
                        "name": "P. Castaldi"
                    },
                    {
                        "authorId": "1776006",
                        "name": "Stratis Ioannidis"
                    },
                    {
                        "authorId": "144729179",
                        "name": "Jennifer G. Dy"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "In the second step we apply PGExplainer with a minor modification to circumvent the \u201cintroduced evidence\u201d [21] issue due to the presence of soft masks.",
                "These modifications lead to design choices for the regularization terms that are different from those originally used in PGExplainer.",
                "Of particular interest for this work is PGExplainer [6], which uses a small network to parametrize the probability of each edge \u03c9ij of being part of the explanatory subgraph, and sample from this distribution to obtain the final explanation subgraph characterized by edges eij .",
                "The vast majority of these techniques [5, 6] provide a post-hoc explanation, thus inferring the reasons that led to a specific outcome by a trained model.",
                "While PGExplainer generates a unique explanation subgraph, we are interested in collecting\nseveral subgraphs to train the subgraph-based classifier.",
                "In particular, there are gradient-based approaches [13], perturbation-based approaches [5, 6, 14], decomposition methods [15], and counterfactual explainers like [16] and GEM [17]."
            ],
            "citingPaper": {
                "paperId": "7ee52b457a53218ce063abb8284ff067ffd64841",
                "externalIds": {
                    "ArXiv": "2304.07152",
                    "DBLP": "journals/corr/abs-2304-07152",
                    "DOI": "10.48550/arXiv.2304.07152",
                    "CorpusId": 258170418
                },
                "corpusId": 258170418,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7ee52b457a53218ce063abb8284ff067ffd64841",
                "title": "Combining Stochastic Explainers and Subgraph Neural Networks can Increase Expressivity and Interpretability",
                "abstract": "Subgraph-enhanced graph neural networks (SGNN) can increase the expressive power of the standard message-passing framework. This model family represents each graph as a collection of subgraphs, generally extracted by random sampling or with hand-crafted heuristics. Our key observation is that by selecting\"meaningful\"subgraphs, besides improving the expressivity of a GNN, it is also possible to obtain interpretable results. For this purpose, we introduce a novel framework that jointly predicts the class of the graph and a set of explanatory sparse subgraphs, which can be analyzed to understand the decision process of the classifier. We compare the performance of our framework against standard subgraph extraction policies, like random node/edge deletion strategies. The subgraphs produced by our framework allow to achieve comparable performance in terms of accuracy, with the additional benefit of providing explanations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "115566972",
                        "name": "Indro Spinelli"
                    },
                    {
                        "authorId": "2185348974",
                        "name": "Michele Guerra"
                    },
                    {
                        "authorId": "14553624",
                        "name": "F. Bianchi"
                    },
                    {
                        "authorId": "1752983",
                        "name": "Simone Scardapane"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "In particular, several GNN explanation studies have been proposed [38,48,68] and claim that the behavior of GNN models is strongly related to the structure of the training graph."
            ],
            "citingPaper": {
                "paperId": "00630c05a4b2f17fb03120b0af09368291ba44e9",
                "externalIds": {
                    "DBLP": "conf/uss/WangH023",
                    "ArXiv": "2304.03093",
                    "DOI": "10.48550/arXiv.2304.03093",
                    "CorpusId": 257984974
                },
                "corpusId": 257984974,
                "publicationVenue": {
                    "id": "54649c1d-6bcc-4232-9cd1-aa446867b8d0",
                    "name": "USENIX Security Symposium",
                    "type": "conference",
                    "alternate_names": [
                        "USENIX Secur Symp"
                    ],
                    "url": "http://www.usenix.org/events/bytopic/security.html"
                },
                "url": "https://www.semanticscholar.org/paper/00630c05a4b2f17fb03120b0af09368291ba44e9",
                "title": "Inductive Graph Unlearning",
                "abstract": "As a way to implement the\"right to be forgotten\"in machine learning, \\textit{machine unlearning} aims to completely remove the contributions and information of the samples to be deleted from a trained model without affecting the contributions of other samples. Recently, many frameworks for machine unlearning have been proposed, and most of them focus on image and text data. To extend machine unlearning to graph data, \\textit{GraphEraser} has been proposed. However, a critical issue is that \\textit{GraphEraser} is specifically designed for the transductive graph setting, where the graph is static and attributes and edges of test nodes are visible during training. It is unsuitable for the inductive setting, where the graph could be dynamic and the test graph information is invisible in advance. Such inductive capability is essential for production machine learning systems with evolving graphs like social media and transaction networks. To fill this gap, we propose the \\underline{{\\bf G}}\\underline{{\\bf U}}ided \\underline{{\\bf I}}n\\underline{{\\bf D}}uctiv\\underline{{\\bf E}} Graph Unlearning framework (GUIDE). GUIDE consists of three components: guided graph partitioning with fairness and balance, efficient subgraph repair, and similarity-based aggregation. Empirically, we evaluate our method on several inductive benchmarks and evolving transaction graphs. Generally speaking, GUIDE can be efficiently implemented on the inductive graph learning tasks for its low graph partition cost, no matter on computation or structure information. The code will be available here: https://github.com/Happy2Git/GUIDE.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109451905",
                        "name": "Cheng-Long Wang"
                    },
                    {
                        "authorId": "2925985",
                        "name": "Mengdi Huai"
                    },
                    {
                        "authorId": "2119265639",
                        "name": "Di Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "9eee6d56a2815e19a63a32dddda7c3383514d1e8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-02277",
                    "ArXiv": "2304.02277",
                    "DOI": "10.1109/IJCNN54540.2023.10191949",
                    "CorpusId": 257952370
                },
                "corpusId": 257952370,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/9eee6d56a2815e19a63a32dddda7c3383514d1e8",
                "title": "Rethinking the Trigger-injecting Position in Graph Backdoor Attack",
                "abstract": "Backdoor attacks have been demonstrated as a security threat for machine learning models. Traditional backdoor attacks intend to inject backdoor functionality into the model such that the backdoored model will perform abnormally on inputs with predefined backdoor triggers and still retain state-of-the-art performance on the clean inputs. While there are already some works on backdoor attacks on Graph Neural Networks (GNNs), the backdoor trigger in the graph domain is mostly injected into random positions of the sample. There is no work analyzing and explaining the backdoor attack performance when injecting triggers into the most important or least important area in the sample, which we refer to as trigger-injecting strategies MIAS and LIAS, respectively. Our results show that, generally, LIAS performs better, and the differences between the LIAS and MIAS performance can be significant. Furthermore, we explain these two strategies\u2019 similar (better) attack performance through explanation techniques, which results in a further understanding of backdoor attacks in GNNs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2155954578",
                        "name": "Jing Xu"
                    },
                    {
                        "authorId": "2144883147",
                        "name": "Gorka Abad"
                    },
                    {
                        "authorId": "1686538",
                        "name": "S. Picek"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "\u2022 BA-2motifs [114]: It is a graph classification dataset which contains 800 graphs.",
                "Accuracy, F1 and AUC When ground-truth rationales are available for graphs, a direct evaluation method can be employed by comparing the identified explanatory components to the ground-truth explanations [114].",
                "Many existing works [114, 213, 216] aim to identify a subgraph that is highly correlated with the prediction result.",
                "Various efforts have been taken for the explainability of GNNs, which can be generally categorized into instance-level post-hoc explanations [114, 213], model-level post-hoc explanations [215] and self-explainable methods [37].",
                "Different from other explanation methods [114, 213] that only give substructures of high correlation score with the prediction, graph counterfactual explanation aims to get more actionable and useful explanations by understanding how the prediction can be changed in order to achieve an alternative outcome [173].",
                "Although the aforementioned methods [37, 114, 213, 215] can help to understand and explain GNNs, they can be easily stuck at spurious explanations [208].",
                "It should be noted that for other graph explanation methods [114, 213], we only remove edges to find the critical subgraph as the explanation for the current prediction."
            ],
            "citingPaper": {
                "paperId": "f4e18d4aefc1ee0f88f0bb09fa5c17c74aad767e",
                "externalIds": {
                    "ArXiv": "2304.01391",
                    "DBLP": "journals/corr/abs-2304-01391",
                    "DOI": "10.48550/arXiv.2304.01391",
                    "CorpusId": 257921453
                },
                "corpusId": 257921453,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f4e18d4aefc1ee0f88f0bb09fa5c17c74aad767e",
                "title": "Counterfactual Learning on Graphs: A Survey",
                "abstract": "Graph-structured data are pervasive in the real-world such as social networks, molecular graphs and transaction networks. Graph neural networks (GNNs) have achieved great success in representation learning on graphs, facilitating various downstream tasks. However, GNNs have several drawbacks such as lacking interpretability, can easily inherit the bias of the training data and cannot model the casual relations. Recently, counterfactual learning on graphs has shown promising results in alleviating these drawbacks. Various graph counterfactual learning approaches have been proposed for counterfactual fairness, explainability, link prediction and other applications on graphs. To facilitate the development of this promising direction, in this survey, we categorize and comprehensively review papers on graph counterfactual learning. We divide existing methods into four categories based on research problems studied. For each category, we provide background and motivating examples, a general framework summarizing existing works and a detailed review of these works. We point out promising future research directions at the intersection of graph-structured data, counterfactual learning, and real-world applications. To offer a comprehensive view of resources for future studies, we compile a collection of open-source implementations, public datasets, and commonly-used evaluation metrics. This survey aims to serve as a ``one-stop-shop'' for building a unified understanding of graph counterfactual learning categories and current resources. We also maintain a repository for papers and resources and will keep updating the repository https://github.com/TimeLovercc/Awesome-Graph-Causal-Learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2149465392",
                        "name": "Zhimeng Guo"
                    },
                    {
                        "authorId": "33664431",
                        "name": "Teng Xiao"
                    },
                    {
                        "authorId": "1682418",
                        "name": "C. Aggarwal"
                    },
                    {
                        "authorId": "2146672392",
                        "name": "Hui Liu"
                    },
                    {
                        "authorId": "2116430057",
                        "name": "Suhang Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "e0e831deaa9c91243e7e742783e3961cf82594d9",
                "externalIds": {
                    "DOI": "10.1016/j.yamp.2023.01.002",
                    "CorpusId": 258173068
                },
                "corpusId": 258173068,
                "publicationVenue": {
                    "id": "bdc57733-c0a1-43f7-9404-bae4003d94ca",
                    "name": "Advances in Molecular Pathology",
                    "alternate_names": [
                        "Adv Mol Pathol"
                    ],
                    "issn": "2589-4080",
                    "url": "https://www.sciencedirect.com/journal/advances-in-molecular-pathology"
                },
                "url": "https://www.semanticscholar.org/paper/e0e831deaa9c91243e7e742783e3961cf82594d9",
                "title": "Artificial Intelligence, Bioinformatics, and Pathology",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108886427",
                        "name": "Joshua Levy"
                    },
                    {
                        "authorId": "2143667930",
                        "name": "Yunrui Lu"
                    },
                    {
                        "authorId": "2214558780",
                        "name": "M. Montivero"
                    },
                    {
                        "authorId": "1642210467",
                        "name": "O. Ramwala"
                    },
                    {
                        "authorId": "2204338194",
                        "name": "Jason R. McFadden"
                    },
                    {
                        "authorId": "1785423401",
                        "name": "Carly B Miles"
                    },
                    {
                        "authorId": "2214553664",
                        "name": "Adam Gilbert Diamond"
                    },
                    {
                        "authorId": "2183500760",
                        "name": "R. Reddy"
                    },
                    {
                        "authorId": "2183500760",
                        "name": "R. Reddy"
                    },
                    {
                        "authorId": "2055462778",
                        "name": "T. Hudson"
                    },
                    {
                        "authorId": "2139436337",
                        "name": "Zarif L. Azher"
                    },
                    {
                        "authorId": "2164580456",
                        "name": "Akash Pamal"
                    },
                    {
                        "authorId": "2184792774",
                        "name": "Sameer Gabbita"
                    },
                    {
                        "authorId": "2214553656",
                        "name": "Tess Cronin"
                    },
                    {
                        "authorId": "146151625",
                        "name": "A. A. Ould Ismail"
                    },
                    {
                        "authorId": "2192712872",
                        "name": "Tarushii Goel"
                    },
                    {
                        "authorId": "2214553812",
                        "name": "Sanjay Jacob"
                    },
                    {
                        "authorId": "2192361384",
                        "name": "A. Suvarna"
                    },
                    {
                        "authorId": "2115453262",
                        "name": "Taein Kim"
                    },
                    {
                        "authorId": "120996269",
                        "name": "Edward Zhang"
                    },
                    {
                        "authorId": "89333422",
                        "name": "N. Reddy"
                    },
                    {
                        "authorId": "2132699202",
                        "name": "Sumanth Ratna"
                    },
                    {
                        "authorId": "150167464",
                        "name": "Jason Zavras"
                    },
                    {
                        "authorId": "37530422",
                        "name": "L. Vaickus"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Other graph explanation methods such as PGExplainer [54] and GNNExplainer [55] can only identify discrete edges or nodes, which cannot be used for the interaction vulnerability explanation among nodes."
            ],
            "citingPaper": {
                "paperId": "5ba51a86d48bbfb82d7493375708cc3484450fa9",
                "externalIds": {
                    "DBLP": "conf/icde/WangGLLY23",
                    "DOI": "10.1109/ICDE55515.2023.00120",
                    "CorpusId": 260171471
                },
                "corpusId": 260171471,
                "publicationVenue": {
                    "id": "764e3630-ddac-4c21-af4b-9d32ffef082e",
                    "name": "IEEE International Conference on Data Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "ICDE",
                        "Int Conf Data Eng",
                        "IEEE Int Conf Data Eng",
                        "International Conference on Data Engineering"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1331"
                },
                "url": "https://www.semanticscholar.org/paper/5ba51a86d48bbfb82d7493375708cc3484450fa9",
                "title": "Federated IoT Interaction Vulnerability Analysis",
                "abstract": "IoT devices provide users with great convenience in smart homes. However, the interdependent behaviors across devices may yield unexpected interactions. To analyze the potential IoT interaction vulnerabilities, in this paper, we propose a federated and explicable IoT interaction data management system FexIoT. To address the lack of information in the closed-source platforms, FexIoT captures causality information by fusing multi-domain data, including the descriptions of apps and real-time event logs, into interaction graphs. The interaction graph representation is encoded by graph neural networks (GNNs). To collaboratively train the GNN model without sharing the raw data, we design a layer-wise clustering-based federated GNN framework for learning intrinsic clustering relationships among GNN model weights, which copes with the statistical heterogeneity and the concept drift problem of graph data. In addition, we propose the Monte Carlo beam search with the SHAP method to search and measure the risk of subgraphs, in order to explain the potential vulnerability causes. We evaluate our prototype on datasets collected from five IoT automation platforms. The results show that FexIoT achieves more than 90% average accuracy for interaction vulnerability detection, outperforming the existing methods. Moreover, FexIoT offers an explainable result for the detected vulnerabilities.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3474846",
                        "name": "Guangjing Wang"
                    },
                    {
                        "authorId": "7014630",
                        "name": "Hanqing Guo"
                    },
                    {
                        "authorId": "2112838196",
                        "name": "Anran Li"
                    },
                    {
                        "authorId": "2225046620",
                        "name": "Xiaorui Liu"
                    },
                    {
                        "authorId": "2480351",
                        "name": "Qiben Yan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Concerning this particular task, CFGExplainer outperforms other explainability frameworks like GNNExplainer [139], SubgraphX [140] and PGExplainer [141]."
            ],
            "citingPaper": {
                "paperId": "cea911f3a903a03b6333f2211cbde1c4b9ff82e8",
                "externalIds": {
                    "ArXiv": "2303.16004",
                    "DBLP": "journals/corr/abs-2303-16004",
                    "DOI": "10.48550/arXiv.2303.16004",
                    "CorpusId": 257771694
                },
                "corpusId": 257771694,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cea911f3a903a03b6333f2211cbde1c4b9ff82e8",
                "title": "A Survey on Malware Detection with Graph Representation Learning",
                "abstract": "Malware detection has become a major concern due to the increasing number and complexity of malware. Traditional detection methods based on signatures and heuristics are used for malware detection, but unfortunately, they suffer from poor generalization to unknown attacks and can be easily circumvented using obfuscation techniques. In recent years, Machine Learning (ML) and notably Deep Learning (DL) achieved impressive results in malware detection by learning useful representations from data and have become a solution preferred over traditional methods. More recently, the application of such techniques on graph-structured data has achieved state-of-the-art performance in various domains and demonstrates promising results in learning more robust representations from malware. Yet, no literature review focusing on graph-based deep learning for malware detection exists. In this survey, we provide an in-depth literature review to summarize and unify existing works under the common approaches and architectures. We notably demonstrate that Graph Neural Networks (GNNs) reach competitive results in learning robust embeddings from malware represented as expressive graph structures, leading to an efficient detection by downstream classifiers. This paper also reviews adversarial attacks that are utilized to fool graph-based detection methods. Challenges and future research directions are discussed at the end of the paper.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2176440737",
                        "name": "Tristan Bilot"
                    },
                    {
                        "authorId": "2046785",
                        "name": "Nour El Madhoun"
                    },
                    {
                        "authorId": "1682056",
                        "name": "K. A. Agha"
                    },
                    {
                        "authorId": "2727096",
                        "name": "Anis Zouaoui"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Considering the efficient optimization, we follow (Ying et al. 2019) and (Luo et al. 2020) to approximate it with cross-entropy between y and y\u0302, where y\u0302 is the prediction with augmentation v as the input and calculated via\nv = g(x; \u03f5) z = f\u03b8(v) y\u0302 = hw(z), (1) where z is the representation and\u2026",
                "2019) and (Luo et al. 2020) to approximate it with cross-entropy between y and \u0177, where \u0177 is the prediction with augmentation v as the input and calculated via"
            ],
            "citingPaper": {
                "paperId": "6e70a2b7512fde9d25176c508f9cad35e47f66ad",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-11911",
                    "ArXiv": "2303.11911",
                    "DOI": "10.48550/arXiv.2303.11911",
                    "CorpusId": 257636777
                },
                "corpusId": 257636777,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/6e70a2b7512fde9d25176c508f9cad35e47f66ad",
                "title": "Time Series Contrastive Learning with Information-Aware Augmentations",
                "abstract": "Various contrastive learning approaches have been proposed in recent years and achieve significant empirical success. While effective and prevalent, contrastive learning has been less explored for time series data. A key component of contrastive learning is to select appropriate augmentations imposing some priors to construct feasible positive samples, such that an encoder can be trained to learn robust and discriminative representations. Unlike image and language domains where \"desired'' augmented samples can be generated with the rule of thumb guided by prefabricated human priors, the ad-hoc manual selection of time series augmentations is hindered by their diverse and human-unrecognizable temporal structures. How to find the desired augmentations of time series data that are meaningful for given contrastive learning tasks and datasets remains an open question. In this work, we address the problem by encouraging both high fidelity and variety based on information theory. A theoretical analysis leads to the criteria for selecting feasible data augmentations. On top of that, we propose a new contrastive learning approach with information-aware augmentations, InfoTS, that adaptively selects optimal augmentations for time series representation learning. Experiments on various datasets show highly competitive performance with up to a 12.0% reduction in MSE on forecasting tasks and up to 3.7% relative improvement in accuracy on classification tasks over the leading baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    },
                    {
                        "authorId": "145859270",
                        "name": "Wei Cheng"
                    },
                    {
                        "authorId": "2107962435",
                        "name": "Yingheng Wang"
                    },
                    {
                        "authorId": "2116459424",
                        "name": "Dongkuan Xu"
                    },
                    {
                        "authorId": "2090567",
                        "name": "Jingchao Ni"
                    },
                    {
                        "authorId": "3007026",
                        "name": "Wenchao Yu"
                    },
                    {
                        "authorId": "2048981220",
                        "name": "Xuchao Zhang"
                    },
                    {
                        "authorId": "3215702",
                        "name": "Yanchi Liu"
                    },
                    {
                        "authorId": "47557891",
                        "name": "Yuncong Chen"
                    },
                    {
                        "authorId": "2145225543",
                        "name": "Haifeng Chen"
                    },
                    {
                        "authorId": "2190288679",
                        "name": "Xiang Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The uncovering of such black box models in order to gain chemical insights is addressed by the development of graph-specific XAI techniques[69, 70, 71] and by the adaptation of methods from other domains[72, 73, 74]."
            ],
            "citingPaper": {
                "paperId": "294ffec7392ba42e5c8e6fde76cf796f87b58cc2",
                "externalIds": {
                    "ArXiv": "2303.12188",
                    "DOI": "10.1016/j.patter.2023.100803",
                    "CorpusId": 257663765
                },
                "corpusId": 257663765,
                "publicationVenue": {
                    "id": "17bac89e-3dba-467a-b9d4-71e3baefb08b",
                    "name": "Patterns",
                    "type": "journal",
                    "issn": "2666-3899",
                    "url": "https://www.cell.com/patterns",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/patterns"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/294ffec7392ba42e5c8e6fde76cf796f87b58cc2",
                "title": "Accurate, interpretable predictions of materials properties within transformer language models",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50069714",
                        "name": "V. Korolev"
                    },
                    {
                        "authorId": "2183911554",
                        "name": "Pavel Protsenko"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "We also note that, while DnX is often better than GNNExplainer and PGExplainer, its performance bests FastDnX only in 12.5% of cases.",
                "Building on parameterized explainers by Luo et al. (2020), Wang et al. (2021) proposed ReFine to leverage both global information (e.g., class-wise knowledge) via pre-training and local one (i.e., instance specific patterns) using a fine-tuning process.",
                "To alleviate the burden of optimizing again whenever we want to explain a different node, Luo et al. (2020, PGExplainer) propose using node embeddings to parameterize the masks, i.e., amortizing the inference.",
                "GNNand PGExplainer do not appear in the comparison for GIN since they require propagating edge masks, and Torch Geometric does not support edge features for GIN.",
                "For GCN and GATED, PGExplainer yields the best results.",
                "The same is not true for the competing methods, e.g., PGExplainer loses over 15% AUC for the BA-Community (cf., GCN and ARMA).",
                "It is worth mentioning that PGExplainer and GNNExplainer \u2014 as described in the experimental section of their respective papers \u2014 output edge-level explanations, so their results are not immediately comparable to that of our methods and PGMExplainer.",
                "For all datasets, we measure performance in terms of AUC, following Luo et al. (2020).",
                "We compare DnX against three baseline explainers: GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), and PGMExplainer (Vu and Thai, 2020).",
                "Nonetheless, GNNExplainer and PGExplainer impose strong assumptions on our access to the GNN we are trying to explain.",
                ", 2019), PGExplainer (Luo et al., 2020), and PGMExplainer (Vu and Thai, 2020)."
            ],
            "citingPaper": {
                "paperId": "f945b6788d4042c950e57e6032c0ad122566661e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-10139",
                    "ArXiv": "2303.10139",
                    "DOI": "10.48550/arXiv.2303.10139",
                    "CorpusId": 257622939
                },
                "corpusId": 257622939,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f945b6788d4042c950e57e6032c0ad122566661e",
                "title": "Distill n' Explain: explaining graph neural networks using simple surrogates",
                "abstract": "Explaining node predictions in graph neural networks (GNNs) often boils down to finding graph substructures that preserve predictions. Finding these structures usually implies back-propagating through the GNN, bonding the complexity (e.g., number of layers) of the GNN to the cost of explaining it. This naturally begs the question: Can we break this bond by explaining a simpler surrogate GNN? To answer the question, we propose Distill n' Explain (DnX). First, DnX learns a surrogate GNN via knowledge distillation. Then, DnX extracts node or edge-level explanations by solving a simple convex program. We also propose FastDnX, a faster version of DnX that leverages the linear decomposition of our surrogate model. Experiments show that DnX and FastDnX often outperform state-of-the-art GNN explainers while being orders of magnitude faster. Additionally, we support our empirical findings with theoretical results linking the quality of the surrogate model (i.e., distillation error) to the faithfulness of explanations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2061161328",
                        "name": "Tamara A. Pereira"
                    },
                    {
                        "authorId": "2211967383",
                        "name": "Erik Nasciment"
                    },
                    {
                        "authorId": "2164014168",
                        "name": "Lucas Emanuel Resck"
                    },
                    {
                        "authorId": "144128644",
                        "name": "Diego Mesquita"
                    },
                    {
                        "authorId": "3383481",
                        "name": "A. Souza"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Similarly to Luo et al. (2020), we use the cross-entropy function to replace the conditional entropy function H(Y | sf ) with N given instances, and define the reward function as follows:"
            ],
            "citingPaper": {
                "paperId": "2fd0e6aaccee819a880a55d9190700c6b754d32d",
                "externalIds": {
                    "DBLP": "conf/iclr/LiLLHP23",
                    "ArXiv": "2303.02448",
                    "DOI": "10.48550/arXiv.2303.02448",
                    "CorpusId": 257365860
                },
                "corpusId": 257365860,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2fd0e6aaccee819a880a55d9190700c6b754d32d",
                "title": "DAG Matters! GFlowNets Enhanced Explainer For Graph Neural Networks",
                "abstract": "Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over the years. Existing literature mainly focus on selecting a subgraph, through combinatorial optimization, to provide faithful explanations. However, the exponential size of candidate subgraphs limits the applicability of state-of-the-art methods to large-scale GNNs. We enhance on this through a different approach: by proposing a generative structure -- GFlowNets-based GNN Explainer (GFlowExplainer), we turn the optimization problem into a step-by-step generative problem. Our GFlowExplainer aims to learn a policy that generates a distribution of subgraphs for which the probability of a subgraph is proportional to its' reward. The proposed approach eliminates the influence of node sequence and thus does not need any pre-training strategies. We also propose a new cut vertex matrix to efficiently explore parent states for GFlowNets structure, thus making our approach applicable in a large-scale setting. We conduct extensive experiments on both synthetic and real datasets, and both qualitative and quantitative results show the superiority of our GFlowExplainer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2128179285",
                        "name": "Wenqian Li"
                    },
                    {
                        "authorId": "47002988",
                        "name": "Yinchuan Li"
                    },
                    {
                        "authorId": "46947185",
                        "name": "Zhigang Li"
                    },
                    {
                        "authorId": "2069718816",
                        "name": "Jianye Hao"
                    },
                    {
                        "authorId": "2057013801",
                        "name": "Yan Pang"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Note that different from [27], our method is suitable for the homogeneous and heterogeneous graphs simultaneously.",
                "Next, we follow [18, 27] to re-parameter the process using the uniform distribution with the parameter \u03a9 as follows:",
                "Note that it is a discrete process to select the important edges, similar to [27], we first use binary concrete distribution to calculate P (ei j ) for approximating the sampling process and then re-parameter the selection process to optimize the objective function.",
                "further improved the performance and interpretation for multiple instances [27].",
                "Similar to [27, 49], our objective is to maximize the mutual information between the detection predictions and the interpretable sub-graph Ge .",
                "Some GNN-based interpretable methods have been developed [27, 28, 33, 34, 37, 49, 61], which can be used to interpret the predictions/generations by GNNs."
            ],
            "citingPaper": {
                "paperId": "16cb1672b1b3a5829392fdff7181c1f8789f49a1",
                "externalIds": {
                    "DBLP": "conf/wsdm/YangMZGZ023",
                    "DOI": "10.1145/3539597.3570453",
                    "CorpusId": 257079754
                },
                "corpusId": 257079754,
                "publicationVenue": {
                    "id": "ea38228f-6ed3-4222-a3ce-d963d8cc9516",
                    "name": "Web Search and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Web Search Data Min",
                        "WSDM"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=3158"
                },
                "url": "https://www.semanticscholar.org/paper/16cb1672b1b3a5829392fdff7181c1f8789f49a1",
                "title": "Interpretable Research Interest Shift Detection with Temporal Heterogeneous Graphs",
                "abstract": "Researchers dedicate themselves to research problems they are interested in and often have evolving research interests in their academic careers. The study of research interest shift detection can help to find facts relevant to scientific training paths, scientific funding trends, and knowledge discovery. Existing methods define specific graph structures like author-conference-topic networks, and co-citing networks to detect research interest shift. They either ignore the temporal factor or miss heterogeneous information characterizing academic activities. More importantly, the detection results lack the interpretations of how research interests change over time, thus reducing the model's credibility. To address these issues, we propose a novel interpretable research interest shift detection model with temporal heterogeneous graphs. We first construct temporal heterogeneous graphs to represent the research interests of the target authors. To make the detection interpretable, we design a deep neural network to parameterize the generation process of interpretation on the predicted results in the form of a weighted sub-graph. Additionally, to improve the training process, we propose a semantic-aware negative data sampling strategy to generate non-interesting auxiliary shift graphs as contrastive samples. Extensive experiments demonstrate that our model outperforms the state-of-the-art baselines on two public academic graph datasets and is capable of producing interpretable results.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1998961496",
                        "name": "Qiang Yang"
                    },
                    {
                        "authorId": "2109467053",
                        "name": "Changsheng Ma"
                    },
                    {
                        "authorId": "119718473",
                        "name": "Qiannan Zhang"
                    },
                    {
                        "authorId": "2118502950",
                        "name": "Xin Gao"
                    },
                    {
                        "authorId": "2117879943",
                        "name": "Chuxu Zhang"
                    },
                    {
                        "authorId": "2928371",
                        "name": "Xiangliang Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "As the determinant subgraphs are expected to be connected, connective constraints [17, 37] are used to allocate more selective probabilities to the edges, which connect with the part selected already.",
                "Although studies [17, 29, 33] have been extensively conducted on identifying the salient subgraph, network dissection of GNNs remains largely unexplored.",
                "Following previous studies [1, 17, 29], we focus on the structural features (i.",
                "Most prior studies [1, 15, 17, 33] realize post-hoc explainability from answering \u201cWhich fractions of input graph aremost influential to the prediction?\u201d, thus generating explanations at the granularity of input features [3].",
                "PGExplainer [17] and GSAT [19] (in its post-hoc working mode) develop an additional encoder to calculate the selection probability of each edge.",
                "The scheme of feature attribution and selection [1, 17, 29, 30, 33] prevails towards post-hoc explanations of GNN models.",
                "That is, unaware of this limitation, most previous explainers [17, 19, 30, 33] simply feed subgraph f (\u00b7|\u0398) into the original network \u0398, rather than f (\u00b7|\u0398\u0302), and then use I (f (Gs |\u0398);Y) as the main part of the loss function to optimize the explainer.",
                "Attention-based line [17, 19, 30] focuses on training an attention function for edge attribution according to input features.",
                "Following previous works [17, 33], we adopt the Barabasi-Albert (BA) graphs as the base and attach each graph with one of three motif types: house, cycle, and grid."
            ],
            "citingPaper": {
                "paperId": "52c36f7f3554c497d680aa8ccb38aef6b96e26f0",
                "externalIds": {
                    "DBLP": "conf/wsdm/Fang00L0C23",
                    "DOI": "10.1145/3539597.3570378",
                    "CorpusId": 257079786
                },
                "corpusId": 257079786,
                "publicationVenue": {
                    "id": "ea38228f-6ed3-4222-a3ce-d963d8cc9516",
                    "name": "Web Search and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Web Search Data Min",
                        "WSDM"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=3158"
                },
                "url": "https://www.semanticscholar.org/paper/52c36f7f3554c497d680aa8ccb38aef6b96e26f0",
                "title": "Cooperative Explanations of Graph Neural Networks",
                "abstract": "With the growing success of graph neural networks (GNNs), the explainability of GNN is attracting considerable attention. Current explainers mostly leverage feature attribution and selection to explain a prediction. By tracing the importance of input features, they select the salient subgraph as the explanation. However, their explainability is at the granularity of input features only, and cannot reveal the usefulness of hidden neurons. This inherent limitation makes the explainers fail to scrutinize the model behavior thoroughly, resulting in unfaithful explanations. In this work, we explore the explainability of GNNs at the granularity of both input features and hidden neurons. To this end, we propose an explainer-agnostic framework, Cooperative GNN Explanation (CGE) to generate the explanatory subgraph and subnetwork simultaneously, which jointly explain how the GNN model arrived at its prediction. Specifically, it first initializes the importance scores of input features and hidden neurons with masking networks. Then it iteratively retrains the importance scores, refining the salient subgraph and subnetwork by discarding low-scored features and neurons in each iteration. Through such cooperative learning, CGE not only generates faithful and concise explanations, but also exhibits how the salient information flows by activating and deactivating neurons. We conduct extensive experiments on both synthetic and real-world datasets, validating the superiority of CGE over state-of-the-art approaches. Code is available at https://github.com/MangoKiller/CGE_demo.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2159830802",
                        "name": "Junfeng Fang"
                    },
                    {
                        "authorId": "98285513",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "2153659066",
                        "name": "An Zhang"
                    },
                    {
                        "authorId": "2163529904",
                        "name": "Zemin Liu"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    },
                    {
                        "authorId": "143779329",
                        "name": "Tat-Seng Chua"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "To make the edge dropping procedure differentiable and enable an end-toend optimization process, we relax the discrete p e to a continuous variable in [0, 1] and apply the Gumbel-Max reparameterization trick [12, 15]."
            ],
            "citingPaper": {
                "paperId": "544e8ec96f87e515ec5980c513a75fc4f4f377f0",
                "externalIds": {
                    "DBLP": "conf/wsdm/WangX0SLG023",
                    "DOI": "10.1145/3539597.3570483",
                    "CorpusId": 257079682
                },
                "corpusId": 257079682,
                "publicationVenue": {
                    "id": "ea38228f-6ed3-4222-a3ce-d963d8cc9516",
                    "name": "Web Search and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Web Search Data Min",
                        "WSDM"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=3158"
                },
                "url": "https://www.semanticscholar.org/paper/544e8ec96f87e515ec5980c513a75fc4f4f377f0",
                "title": "Knowledge-Adaptive Contrastive Learning for Recommendation",
                "abstract": "By jointly modeling user-item interactions and knowledge graph (KG) information, KG-based recommender systems have shown their superiority in alleviating data sparsity and cold start problems. Recently, graph neural networks (GNNs) have been widely used in KG-based recommendation, owing to the strong ability of capturing high-order structural information. However, we argue that existing GNN-based methods have the following two limitations. Interaction domination: the supervision signal of user-item interaction will dominate the model training, and thus the information of KG is barely encoded in learned item representations; Knowledge overload: KG contains much recommendation-irrelevant information, and such noise would be enlarged during the message aggregation of GNNs. The above limitations prevent existing methods to fully utilize the valuable information lying in KG. In this paper, we propose a novel algorithm named Knowledge-Adaptive Contrastive Learning (KACL) to address these challenges. Specifically, we first generate data augmentations from user-item interaction view and KG view separately, and perform contrastive learning across the two views. Our design of contrastive loss will force the item representations to encode information shared by both views, thereby alleviating the interaction domination issue. Moreover, we introduce two learnable view generators to adaptively remove task-irrelevant edges during data augmentation, and help tolerate the noises brought by knowledge overload. Experimental results on three public benchmarks demonstrate that KACL can significantly improve the performance on top-K recommendation compared with state-of-the-art methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144221632",
                        "name": "Hao Wang"
                    },
                    {
                        "authorId": "2200090139",
                        "name": "Yao Xu"
                    },
                    {
                        "authorId": "2154170623",
                        "name": "Cheng Yang"
                    },
                    {
                        "authorId": "2086996232",
                        "name": "Chuan Shi"
                    },
                    {
                        "authorId": "2209214173",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "2209215053",
                        "name": "Ning Guo"
                    },
                    {
                        "authorId": "49293587",
                        "name": "Zhiyuan Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "PGExplainer [25] adopts the sameMI importance andmask-learning idea, but it trains a mask predictor to generate a discrete mask.",
                "Given a data instance, most methods generate an explanation by learning a mask to select an edge-induced subgraph [25, 43] or searching over the space of subgraphs [46].",
                "Since PaGE-Link and both baselines can generate masks M, we also follow [25] to compare explainers by the masks they generated using the ROC-AUC score.",
                "Many works [25, 43] use synthetic data as benchmarks, but no benchmark datasets are available for evaluating GNN explanations for heterogeneous LP.",
                "PGExplainer has a training stage and an inference stage (separated by / in the table).",
                "Thus, we extend the popular GNNExplainer [45] and PGExplainer [25] as our baselines.",
                "We refer to these two adapted explainers as GNNExp-Link and PGExp-Link below.",
                "To get the importance of a path, we first use a mean-field approximation for the joint probability by multiplying \ud835\udc43 (\ud835\udc52) together, and we normalize each\nAlgorithm 1 PaGE-Link\nGNNExp [45] PGExp [25] SubgraphX [48] PaGE-Link (ours) \ud835\udc42 ( |E\ud835\udc50 |\ud835\udc47 ) \ud835\udc42 ( |E |\ud835\udc47 ) / \ud835\udc42 ( |E\ud835\udc50",
                "PGExplainer [25] adopts the same MI importance but trains a mask predictor to generate a discrete mask instead.",
                "We conduct a human evaluation by randomly picking 100 predicted links from the test set of AugCitation and generate explanations for each link using GNNExp-Link, PGExp-Link, and PaGE-Link.",
                "GNNExp [43] PGExp [25] SubgraphX [46] PaGE-Link (ours)",
                "Therefore we extend the widespread GNNExplainer [43] and PGExplainer [25] as our baseline models.",
                "Graph Neural Networks (GNNs) [41, 53] have recently achieved state-of-the-art performance on many graph ML tasks and attracted increased interest in studying their explainability [25, 43, 45, 50]."
            ],
            "citingPaper": {
                "paperId": "4008f607e29cfe6c0cce0b5ae119827380b99031",
                "externalIds": {
                    "DBLP": "conf/www/Zhang0SAZFS23",
                    "ArXiv": "2302.12465",
                    "DOI": "10.1145/3543507.3583511",
                    "CorpusId": 257205930
                },
                "corpusId": 257205930,
                "publicationVenue": {
                    "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
                    "name": "The Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Web Conf",
                        "WWW"
                    ],
                    "url": "http://www.iw3c2.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4008f607e29cfe6c0cce0b5ae119827380b99031",
                "title": "PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous Link Prediction",
                "abstract": "Transparency and accountability have become major concerns for black-box machine learning (ML) models. Proper explanations for the model behavior increase model transparency and help researchers develop more accountable models. Graph neural networks (GNN) have recently shown superior performance in many graph ML problems than traditional methods, and explaining them has attracted increased interest. However, GNN explanation for link prediction (LP) is lacking in the literature. LP is an essential GNN task and corresponds to web applications like recommendation and sponsored search on web. Given existing GNN explanation methods only address node/graph-level tasks, we propose Path-based GNN Explanation for heterogeneous Link prediction (PaGE-Link) that generates explanations with connection interpretability, enjoys model scalability, and handles graph heterogeneity. Qualitatively, PaGE-Link can generate explanations as paths connecting a node pair, which naturally captures connections between the two nodes and easily transfer to human-interpretable explanations. Quantitatively, explanations generated by PaGE-Link improve AUC for recommendation on citation and user-item graphs by 9 - 35% and are chosen as better by 78.79% of responses in human evaluation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2145408511",
                        "name": "Shichang Zhang"
                    },
                    {
                        "authorId": "73329314",
                        "name": "Jiani Zhang"
                    },
                    {
                        "authorId": "2118943843",
                        "name": "Xiang Song"
                    },
                    {
                        "authorId": "2121390172",
                        "name": "Soji Adeshina"
                    },
                    {
                        "authorId": "122579067",
                        "name": "Da Zheng"
                    },
                    {
                        "authorId": "1702392",
                        "name": "C. Faloutsos"
                    },
                    {
                        "authorId": "2109461904",
                        "name": "Yizhou Sun"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Nowadays, GNNs are also widely used in recommender systems.",
                "Inspired by InfoMin principle proposed by [23], AD-GCL [21] optimizes adversarial graph augmentation strategies to train GNNs to avoid capturing redundant information during the training.",
                "This style of edge learning has also been used in parameterized explanations and adversarial attacks of GNNs [16,22].",
                "GNN-based methods utilize the structural information of bipartite graphs into representations by introducing the powerful GNNs.",
                "Based on modeling such bipartite graphs, Graph Neural Networks (GNNs) [9,26,8] can learn the effective node representations of users and items for personalized recommendations."
            ],
            "citingPaper": {
                "paperId": "67fd28fbcf55173fecf40ed52ea610513923752b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-02317",
                    "ArXiv": "2302.02317",
                    "DOI": "10.48550/arXiv.2302.02317",
                    "CorpusId": 256615827
                },
                "corpusId": 256615827,
                "publicationVenue": {
                    "id": "8107ca1c-f651-4769-86dc-3d94a7b5ac26",
                    "name": "International Conference on Database Systems for Advanced Applications",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Database Syst Adv Appl",
                        "Database Syst Adv Appl",
                        "Database Systems for Advanced Applications",
                        "DASFAA"
                    ],
                    "url": "http://www.dasfaa.org/"
                },
                "url": "https://www.semanticscholar.org/paper/67fd28fbcf55173fecf40ed52ea610513923752b",
                "title": "Adversarial Learning Data Augmentation for Graph Contrastive Learning in Recommendation",
                "abstract": "Recently, Graph Neural Networks (GNNs) achieve remarkable success in Recommendation. To reduce the influence of data sparsity, Graph Contrastive Learning (GCL) is adopted in GNN-based CF methods for enhancing performance. Most GCL methods consist of data augmentation and contrastive loss (e.g., InfoNCE). GCL methods construct the contrastive pairs by hand-crafted graph augmentations and maximize the agreement between different views of the same node compared to that of other nodes, which is known as the InfoMax principle. However, improper data augmentation will hinder the performance of GCL. InfoMin principle, that the good set of views shares minimal information and gives guidelines to design better data augmentation. In this paper, we first propose a new data augmentation (i.e., edge-operating including edge-adding and edge-dropping). Then, guided by InfoMin principle, we propose a novel theoretical guiding contrastive learning framework, named Learnable Data Augmentation for Graph Contrastive Learning (LDA-GCL). Our methods include data augmentation learning and graph contrastive learning, which follow the InfoMin and InfoMax principles, respectively. In implementation, our methods optimize the adversarial loss function to learn data augmentation and effective representations of users and items. Extensive experiments on four public benchmark datasets demonstrate the effectiveness of LDA-GCL.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47513427",
                        "name": "Junjie Huang"
                    },
                    {
                        "authorId": "2057766936",
                        "name": "Qi Cao"
                    },
                    {
                        "authorId": "3360722",
                        "name": "Ruobing Xie"
                    },
                    {
                        "authorId": "2145442161",
                        "name": "Shaoliang Zhang"
                    },
                    {
                        "authorId": "2066080084",
                        "name": "Feng Xia"
                    },
                    {
                        "authorId": "2476503",
                        "name": "Huawei Shen"
                    },
                    {
                        "authorId": "2110251463",
                        "name": "Xueqi Cheng"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "PGExplainer [4] also obtains an edge-based explanation for a target graph by optimizing a neural network that estimates the edge mask.",
                "PGExplainer [4] learns a neural network that outputs an edge mask under the condition that each edge parameter follows a Bernoulli distribution independently.",
                "A random explainer (which places a random score on each node), GNNExplainer [3], PGExpaliner [4], and SubgraphX [5] were adopted as the baselines for graph classification."
            ],
            "citingPaper": {
                "paperId": "997f3147417b286293f999a781b8f5aa8be140e8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-02139",
                    "ArXiv": "2302.02139",
                    "DOI": "10.48550/arXiv.2302.02139",
                    "CorpusId": 256616269
                },
                "corpusId": 256616269,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/997f3147417b286293f999a781b8f5aa8be140e8",
                "title": "Structural Explanations for Graph Neural Networks using HSIC",
                "abstract": "Graph neural networks (GNNs) are a type of neural model that tackle graphical tasks in an end-to-end manner. Recently, GNNs have been receiving increased attention in machine learning and data mining communities because of the higher performance they achieve in various tasks, including graph classification, link prediction, and recommendation. However, the complicated dynamics of GNNs make it difficult to understand which parts of the graph features contribute more strongly to the predictions. To handle the interpretability issues, recently, various GNN explanation methods have been proposed. In this study, a flexible model agnostic explanation method is proposed to detect significant structures in graphs using the Hilbert-Schmidt independence criterion (HSIC), which captures the nonlinear dependency between two variables through kernels. More specifically, we extend the GraphLIME method for node explanation with a group lasso and a fused lasso-based node explanation method. The group and fused regularization with GraphLIME enables the interpretation of GNNs in substructure units. Then, we show that the proposed approach can be used for the explanation of sequential graph classification tasks. Through experiments, it is demonstrated that our method can identify crucial structures in a target graph in various settings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2051714212",
                        "name": "Ayato Toyokuni"
                    },
                    {
                        "authorId": "143979662",
                        "name": "Makoto Yamada"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "for standard GNNs [49, 99, 46, 2], only few works focused on explaining TGNNs [90, 82, 31]."
            ],
            "citingPaper": {
                "paperId": "b88f456daaf29860d2b59c621be3bd878a581a59",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-01018",
                    "ArXiv": "2302.01018",
                    "DOI": "10.48550/arXiv.2302.01018",
                    "CorpusId": 256503594
                },
                "corpusId": 256503594,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b88f456daaf29860d2b59c621be3bd878a581a59",
                "title": "Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities",
                "abstract": "Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2130615549",
                        "name": "Antonio Longa"
                    },
                    {
                        "authorId": "2144942797",
                        "name": "Veronica Lachi"
                    },
                    {
                        "authorId": "2042269425",
                        "name": "G. Santin"
                    },
                    {
                        "authorId": "144020416",
                        "name": "M. Bianchini"
                    },
                    {
                        "authorId": "49305855",
                        "name": "B. Lepri"
                    },
                    {
                        "authorId": "2075355139",
                        "name": "P. Li\u00f3"
                    },
                    {
                        "authorId": "47260481",
                        "name": "F. Scarselli"
                    },
                    {
                        "authorId": "1702610",
                        "name": "Andrea Passerini"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Inspired by GIN, Principal Neighbourhood Aggregation (PNA) [108] employs all three of these functions to pool the node embeddings, while TextING [109] includes both mean and maximization pooling to capture the label distribution and strengthen the keyword features."
            ],
            "citingPaper": {
                "paperId": "6c7796ba7f04c77d888bf6b65d3498abfacfe71a",
                "externalIds": {
                    "ArXiv": "2301.05860",
                    "CorpusId": 258865620
                },
                "corpusId": 258865620,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6c7796ba7f04c77d888bf6b65d3498abfacfe71a",
                "title": "State of the Art and Potentialities of Graph-level Learning",
                "abstract": "Graphs have a superior ability to represent relational data, like chemical compounds, proteins, and social networks. Hence, graph-level learning, which takes a set of graphs as input, has been applied to many tasks including comparison, regression, classification, and more. Traditional approaches to learning a set of graphs heavily rely on hand-crafted features, such as substructures. But while these methods benefit from good interpretability, they often suffer from computational bottlenecks as they cannot skirt the graph isomorphism problem. Conversely, deep learning has helped graph-level learning adapt to the growing scale of graphs by extracting features automatically and encoding graphs into low-dimensional representations. As a result, these deep graph learning methods have been responsible for many successes. Yet, there is no comprehensive survey that reviews graph-level learning starting with traditional learning and moving through to the deep learning approaches. This article fills this gap and frames the representative algorithms into a systematic taxonomy covering traditional learning, graph-level deep neural networks, graph-level graph neural networks, and graph pooling. To ensure a thoroughly comprehensive survey, the evolutions, interactions, and communications between methods from four different branches of development are also examined. This is followed by a brief review of the benchmark data sets, evaluation metrics, and common downstream applications. The survey concludes with a broad overview of 12 current and future directions in this booming field.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2149233069",
                        "name": "Zhenyu Yang"
                    },
                    {
                        "authorId": "2151251543",
                        "name": "Ge Zhang"
                    },
                    {
                        "authorId": "2142734769",
                        "name": "Jia Wu"
                    },
                    {
                        "authorId": "2118801701",
                        "name": "Jian Yang"
                    },
                    {
                        "authorId": "120607997",
                        "name": "Quan.Z Sheng"
                    },
                    {
                        "authorId": "2057237074",
                        "name": "Shan Xue"
                    },
                    {
                        "authorId": "1857210",
                        "name": "Chuan Zhou"
                    },
                    {
                        "authorId": "1682418",
                        "name": "C. Aggarwal"
                    },
                    {
                        "authorId": "2138443697",
                        "name": "Hao Peng"
                    },
                    {
                        "authorId": "2146226874",
                        "name": "Wenbin Hu"
                    },
                    {
                        "authorId": "2064408469",
                        "name": "Edwin R. Hancock"
                    },
                    {
                        "authorId": "2075355155",
                        "name": "Pietro Lio'"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "From the methodology perspective, existing methods can be categorized as (1) self-explainable GNNs [50, 51], where the GNN can simultaneously give prediction and explanations on the prediction; and (2) post-hoc explanations [19, 20, 24], which adopt another model or strategy to provide explanations of a target GNN.",
                "After incorporating Align Gaus on dataset TreeGrid, SHD distance of top-6 edges drops from 4.39 to 2.13 for GNNExplainer and from 1.38 to 0.13 for PGExplainer.",
                "Our proposed algorithms in Section 5.2 are implemented and incorporated into two representative GNN explanation frameworks, i.e., GNNExplainer [12] and PGExplainer [13].",
                "PGExplainer [13] extends it by incorporating a graph generator to utilize global information.",
                "For PGExplainer, learning rate is initialized to 0.003 and training epoch is set as 30.",
                "PGExplainer is adopted as the base method.",
                "\u2022 ATT [20]: It utilizes average attention weights inside self-attention layers to distinguish important edges.",
                "\u2022 GRAD [20]: A gradient-based method, which assigns importance weights to edges by computing gradients of GNN\u2019s prediction w.",
                "Extending this idea, PGExplainer trains a graph generator to utilize global information in explanation and enable faster inference in the inductive setting [13].",
                "R denotes regularization terms on the explanation, imposing prior knowledge into the searching process, like constraints on budgets or connectivity distributions [20].",
                "The details are given as follows: \u2022 GRAD [13]: A gradient-based method, which assigns\nimportance weights to edges by computing gradients of GNN\u2019s prediction w.r.t the adjacency matrix.",
                "GNNExplainer and PGExplainer are re-implemented, upon which four alignment strategies are instantiated and tested.",
                "\u2022 ATT [13]: It utilizes average attention weights inside selfattention layers to distinguish important edges.",
                "Following PGExplainer [13], chemical groups NH2 and NO2 are used as ground-truth explanations.",
                "\u2022 PGExplaienr [20]: A parameterized explainer that learns a GNN to predict important edges for each graph, and is trained via testing diferent perturbations; \u2022 Gem [56]: Similar to PGExplainer but from the causal view, based on the estimated individual causal efect.",
                "PGExplainer [20] extends it by incorporating a graph generator to utilize global information.",
                "(3) In summary, our proposal can provide more faithful explanations for both clean and mixed settings, while PGExplainer would suffer from spurious explanations and fail to faithfully explain GNN\u2019s predictions, especially in the existence of biases.",
                "We adopt GNNExplainer and PGExplainer as baselines.",
                "In most cases, the variant utilizing latent Gaussian distribution demonstrates stronger improvements, showing the best results on 3 out of 4 datasets; \u2022 On more complex datasets like Mutag, the benefit of introducing embedding alignment is more significant, e.g., the performance of PGExplainer improves from 83.7% to 95.9% with Align Gaus.",
                "However, PGExplainer would produce similar explanations as the clean model, still highly in accord with motif structures.",
                "\u2022 PGExplaienr [13]: A parameterized explainer that learns a GNN to predict important edges for each graph, and is trained via testing different perturbations; \u2022 Gem [45]: Similar to PGExplainer but from the causal view, based on the estimated individual causal effect.",
                "This underlying assumption is rooted in optimization objectives adopted by existing works [19, 20, 24].",
                "Extending this idea, PGExplainer trains a graph generator to utilize global information in explanation and enable faster inference in the inductive setting [20].",
                "From the results, we can make the following observations: \u2022 Across all four datasets, with both GNNExplainer or PG-\nExplainer as the base method, incorporating embedding alignment can improve the quality of obtained explanations; \u2022 Among proposed alignment strategies, those distributionaware approaches, particularly the variant based on Gaussian mixture models, achieve the best performance.",
                "After incorporating it on dataset Mutag, SHD distance of top-6 edges drops from 4.78 to 3.85 for GNNExplainer and from 3.42 to 1.15 for\nPGExplainer.",
                "Following PGExplainer [20], chemical groups \ufffd\ufffd2 and \ufffd\ufffd2 are used as ground-truth explanations.",
                "For baseline methods GRAD, ATT, Gem, and RG-Explainer, their performances reported in their original papers are presented.",
                "PGExplainer and GNNExplainer are used as the backbone method.",
                "From these two experiments, we can observe that embedding alignment can obtain explanations of better faithfulness and is flexible to be incorporated into various models such as GNNExplainer and PGExplainer, which answers RQ1.",
                "A variety of strategies for post-hoc instance-level explanations have been explored in the literature, including utilizing signals from gradients based [49, 53], perturbed predictions based [19, 20, 24, 54], and decomposition based [49, 55]."
            ],
            "citingPaper": {
                "paperId": "a69ca65d10b15716fc5da5dc830e04101b419e9e",
                "externalIds": {
                    "ArXiv": "2301.02791",
                    "DBLP": "journals/corr/abs-2301-02791",
                    "DOI": "10.1145/3616542",
                    "CorpusId": 255545933
                },
                "corpusId": 255545933,
                "publicationVenue": {
                    "id": "0d993d4a-09ba-4df8-90a4-7dfe25f0cb9e",
                    "name": "ACM Transactions on Intelligent Systems and Technology",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Intell Syst Technol"
                    ],
                    "issn": "2157-6904",
                    "url": "http://portal.acm.org/tist",
                    "alternate_urls": [
                        "https://tist.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a69ca65d10b15716fc5da5dc830e04101b419e9e",
                "title": "Faithful and Consistent Graph Neural Network Explanations with Rationale Alignment",
                "abstract": "Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over recent years. Instance-level GNN explanation aims to discover critical input elements, like nodes or edges, that the target GNN relies upon for making predictions. Though various algorithms are proposed, most of them formalize this task by searching the minimal subgraph which can preserve original predictions. However, an inductive bias is deep-rooted in this framework: several subgraphs can result in the same or similar outputs as the original graphs. Consequently, they have the danger of providing spurious explanations and failing to provide consistent explanations. Applying them to explain weakly-performed GNNs would further amplify these issues. To address this problem, we theoretically examine the predictions of GNNs from the causality perspective. Two typical reasons for spurious explanations are identified: confounding effect of latent variables like distribution shift, and causal factors distinct from the original input. Observing that both confounding effects and diverse causal rationales are encoded in internal representations, we propose a new explanation framework with an auxiliary alignment loss, which is theoretically proven to be optimizing a more faithful explanation objective intrinsically. Concretely for this alignment loss, a set of different perspectives are explored: anchor-based alignment, distributional alignment based on Gaussian mixture models, mutual-information-based alignment, etc. A comprehensive study is conducted both on the effectiveness of this new framework in terms of explanation faithfulness/consistency and on the advantages of these variants. For our codes, please refer to the following URL link: https://github.com/TianxiangZhao/GraphNNExplanation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1999191869",
                        "name": "Tianxiang Zhao"
                    },
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    },
                    {
                        "authorId": "48505793",
                        "name": "Xiang Zhang"
                    },
                    {
                        "authorId": "2116430057",
                        "name": "Suhang Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "175b99cd341c162b9f78fdfc4ccec83d58c3b87c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-02780",
                    "ArXiv": "2301.02780",
                    "DOI": "10.48550/arXiv.2301.02780",
                    "CorpusId": 255545775
                },
                "corpusId": 255545775,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/175b99cd341c162b9f78fdfc4ccec83d58c3b87c",
                "title": "Explaining Graph Neural Networks via Non-parametric Subgraph Matching",
                "abstract": "The great success in graph neural networks (GNNs) provokes the question about explainability: \u201cWhich fraction of the input graph is the most determinant to the prediction?\u201d Particularly, parametric explainers prevail in existing approaches because of their stronger capability to decipher the black-box (i.e., the target GNN). In this paper, based on the observation that graphs typically share some joint motif patterns, we propose a novel non-parametric subgraph matching framework, dubbed MatchExplainer, to explore explanatory subgraphs. It couples the target graph with other counterpart instances and identi\ufb01es the most crucial joint substructure by minimizing the node corresponding-based distance. Moreover, we note that present graph sampling or node-dropping methods usually suffer from the false positive sampling problem. To ameliorate that issue, we design a new augmentation paradigm named MatchDrop. It takes advantage of MatchExplainer to \ufb01x the most informative portion of the graph and merely operates graph augmentations on the rest less informative part. We conduct extensive experiments on both synthetic and real-world datasets and show the effectiveness of our MatchExplainer by outperforming all parametric baselines with signi\ufb01cant margins. Additional results also demonstrate that our MatchDrop is a general scheme to be equipped with GNNs for enhanced performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152167768",
                        "name": "Fang Wu"
                    },
                    {
                        "authorId": "2118155623",
                        "name": "Siyuan Li"
                    },
                    {
                        "authorId": "47767812",
                        "name": "Lirong Wu"
                    },
                    {
                        "authorId": "9215251",
                        "name": "Dragomir R. Radev"
                    },
                    {
                        "authorId": "2146420304",
                        "name": "Yinghui Jiang"
                    },
                    {
                        "authorId": "2153674069",
                        "name": "Xurui Jin"
                    },
                    {
                        "authorId": "152135528",
                        "name": "Z. Niu"
                    },
                    {
                        "authorId": "1390908654",
                        "name": "Stan Z. Li"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "For BA2Motif dataset, Cycle and House motifs are causal factors which determine the graph label, while Tree motif is non-causal factor which is spuriously associated with the true label Luo et al. (2020). In Figure 7, CI-GNN could successfully recognize the Cycle and House motifs that explain the labels, while GNNExplainer, PGExplainer and RC-Explainer assign the larger weights on edges out of Cycle and House motifs, suggesting that Tree motif (the spurious correlation) obtained by GNNExplainer, PGExplainer and RC-Explainer could lead to unreliable prediction.",
                "For example, GNNExplainer Ying et al. (2019) and PGExplainer Luo et al. (2020) employ edge masks to identify a compact subgraph structure, while ZORRO Funke et al. (2020) uses node masks and node feature masks to recognize essential input nodes and node features.",
                "\u2026et al. (2018)) and six recently proposed state-of-the-art (SOTA) graph explainers, namely subgraph information bottleneck (SIB) Yu et al. (2021), GNNExplainer Ying et al. (2019), PGExplainer Luo et al. (2020), RC-Explainer Wang et al. (2022), OrphicX Lin et al. (2022) and DIR-GNN Wu et al. (2021).",
                "For example, GNNExplainer Ying et al. (2019) and PGExplainer Luo et al. (2020) extract a compact subgraph to provide the instance-level explanations, while XGNN Yuan et al. (2020) generates the discriminative graph patterns to provide the model-level explanations.",
                "For example, in the BA-2Motif dataset Luo et al. (2020) in which graphs with House motifs are labeled with 0 and the ones with Cycle are with 1.",
                "For BA2Motif dataset, Cycle and House motifs are causal factors which determine the graph label, while Tree motif is non-causal factor which is spuriously associated with the true label Luo et al. (2020)."
            ],
            "citingPaper": {
                "paperId": "108c960f7d8dafdbc4c41ab7e23610a5959ae2fc",
                "externalIds": {
                    "ArXiv": "2301.01642",
                    "DBLP": "journals/corr/abs-2301-01642",
                    "DOI": "10.48550/arXiv.2301.01642",
                    "CorpusId": 255415863
                },
                "corpusId": 255415863,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/108c960f7d8dafdbc4c41ab7e23610a5959ae2fc",
                "title": "CI-GNN: A Granger Causality-Inspired Graph Neural Network for Interpretable Brain Network-Based Psychiatric Diagnosis",
                "abstract": "There is a recent trend to leverage the power of graph neural networks (GNNs) for brain-network based psychiatric diagnosis, which,in turn, also motivates an urgent need for psychiatrists to fully understand the decision behavior of the used GNNs. However, most of the existing GNN explainers are either post-hoc in which another interpretive model needs to be created to explain a well-trained GNN, or do not consider the causal relationship between the extracted explanation and the decision, such that the explanation itself contains spurious correlations and suffers from weak faithfulness. In this work, we propose a granger causality-inspired graph neural network (CI-GNN), a built-in interpretable model that is able to identify the most influential subgraph (i.e., functional connectivity within brain regions) that is causally related to the decision (e.g., major depressive disorder patients or healthy controls), without the training of an auxillary interpretive network. CI-GNN learns disentangled subgraph-level representations {\\alpha} and \\b{eta} that encode, respectively, the causal and noncausal aspects of original graph under a graph variational autoencoder framework, regularized by a conditional mutual information (CMI) constraint. We theoretically justify the validity of the CMI regulation in capturing the causal relationship. We also empirically evaluate the performance of CI-GNN against three baseline GNNs and four state-of-the-art GNN explainers on synthetic data and three large-scale brain disease datasets. We observe that CI-GNN achieves the best performance in a wide range of metrics and provides more reliable and concise explanations which have clinical evidence.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "152804328",
                        "name": "Kaizhong Zheng"
                    },
                    {
                        "authorId": "2462771",
                        "name": "Shujian Yu"
                    },
                    {
                        "authorId": "2199175116",
                        "name": "Ba-dong Chen"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "contexts": [
                "Specifically, a trained PGExplainer [14] and Gem [12] can be used in inductive scenarios to infer explanations for unexplained instances without retraining the explanation models.",
                "PGExplainer [14] introduces explanations for GNNs with the use of a probabilistic graph.",
                "Recently, several explainers [25,5,26,14,12] have been proposed to tackle the problem of explaining GNN models.",
                "While PGExplainer [14], Gem [12], and XGNN [26] can provide a global explanation of the model prediction."
            ],
            "citingPaper": {
                "paperId": "396b5f4ac19f1c6c16569790c14ec6013a466563",
                "externalIds": {
                    "ArXiv": "2301.00012",
                    "DBLP": "journals/corr/abs-2301-00012",
                    "DOI": "10.48550/arXiv.2301.00012",
                    "CorpusId": 255372423
                },
                "corpusId": 255372423,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/396b5f4ac19f1c6c16569790c14ec6013a466563",
                "title": "GANExplainer: GAN-based Graph Neural Networks Explainer",
                "abstract": ". With the rapid deployment of graph neural networks (GNNs) based techniques into a wide range of applications such as link prediction, node classi\ufb01cation, and graph classi\ufb01cation the explainability of GNNs has become an indispensable component for predictive and trustworthy decision-making. Thus, it is critical to explain why graph neural network (GNN) makes particular predictions for them to be believed in many applications. Some GNNs explainers have been proposed recently. However, they lack to generate accurate and real explanations. To mitigate these limitations, we propose GANExplainer, based on Generative Adversarial Network (GAN) architecture. GANExplainer is composed of a generator to create explanations and a discriminator to assist with the Generator development. We investigate the explanation accuracy of our models by comparing the performance of GANExplainer with other state-of-the-art methods. Our empirical results on synthetic datasets indicate that GANExplainer improves explanation accuracy by up to 35% compared to its alternatives.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2179302425",
                        "name": "Yiqiao Li"
                    },
                    {
                        "authorId": "51239629",
                        "name": "Jianlong Zhou"
                    },
                    {
                        "authorId": "2114719042",
                        "name": "Boyuan Zheng"
                    },
                    {
                        "authorId": "145093625",
                        "name": "Fang Chen"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Recently, new approaches to explain and interpret the predictions of GNNs have also been proposed and they include GNNExplainer [55], PGExplainer [35], and PGMExplainer [47], which focus on providing single instance or multi-instance explanations for homogeneous networks."
            ],
            "citingPaper": {
                "paperId": "950a34c7478d82a6b195ba96ccd471fb3a8c5da1",
                "externalIds": {
                    "DOI": "10.1145/3578522",
                    "CorpusId": 254975545
                },
                "corpusId": 254975545,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/950a34c7478d82a6b195ba96ccd471fb3a8c5da1",
                "title": "Learning and Understanding User Interface Semantics from Heterogeneous Networks with Multimodal and Positional Attributes",
                "abstract": "User interfaces (UI) of desktop, web, and mobile applications involve a hierarchy of objects (e.g., applications, screens, view class, and other types of design objects) with multimodal (e.g., textual and visual) and positional (e.g., spatial location, sequence order, and hierarchy level) attributes. We can therefore represent a set of application UIs as a heterogeneous network with multimodal and positional attributes. Such a network not only represents how users understand the visual layout of UIs but also influences how users would interact with applications through these UIs. To model the UI semantics well for different UI annotation, search, and evaluation tasks, this article proposes the novel Heterogeneous Attention-based Multimodal Positional (HAMP) graph neural network model. HAMP combines graph neural networks with the scaled dot-product attention used in transformers to learn the embeddings of heterogeneous nodes and associated multimodal and positional attributes in a unified manner. HAMP is evaluated with classification and regression tasks conducted on three distinct real-world datasets. Our experiments demonstrate that HAMP significantly out-performs other state-of-the-art models on such tasks. To further provide interpretations of the contribution of heterogeneous network information for understanding the relationships between the UI structure and prediction tasks, we propose Adaptive HAMP (AHAMP), which adaptively learns the importance of different edges linking different UI objects. Our experiments demonstrate AHAMP\u2019s superior performance over HAMP on a number of tasks, and its ability to provide interpretations of the contribution of multimodal and positional attributes, as well as heterogeneous network information to different tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2075155185",
                        "name": "Gary (Ming) Ang"
                    },
                    {
                        "authorId": "1709901",
                        "name": "Ee-Peng Lim"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "The instance-level category includes gradient/features-based methods [2], [23], perturbation-based methods [21], [26], [38], [42], decomposition-based methods [2], [27], [28], and surrogate-based methods [13], [34]."
            ],
            "citingPaper": {
                "paperId": "a03f43a8987a148b7f3b599e8aa70ab9feeb8b64",
                "externalIds": {
                    "DBLP": "conf/bigdataconf/LiuZX22",
                    "DOI": "10.1109/BigData55660.2022.10020318",
                    "CorpusId": 256312270
                },
                "corpusId": 256312270,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a03f43a8987a148b7f3b599e8aa70ab9feeb8b64",
                "title": "Trade less Accuracy for Fairness and Trade-off Explanation for GNN",
                "abstract": "Graphs are widely found in social network analysis and e-commerce, where Graph Neural Networks (GNNs) are the state-of the-art model. GNNs can be biased due to sensitive attributes and network topology. With existing work that learns a fair node representation or adjacency matrix, achieving a strong guarantee of group fairness while preserving prediction accuracy is still challenging, with the fairness-accuracy trade-off remaining obscure to human decision-makers. We first define and analyze a novel upper bound of group fairness to optimize the adjacency matrix for fairness without significantly h arming prediction accuracy. To understand the nuance of fairness-accuracy tradeoff, we further propose macroscopic and microscopic explanation methods to reveal the trade-offs and the space that one can exploit. The macroscopic explanation method is based on stratified sampling and linear programming to deterministically explain the dynamics of the group fairness and prediction accuracy. Driving down to the microscopic level, we propose a path-based explanation that reveals how network topology leads to the tradeoff. On seven graph datasets, we demonstrate the novel upper bound can achieve more efficient fairness-accuracy trade-offs and the intuitiveness of the explanation methods can clearly pinpoint where the trade-off is improved.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144399347",
                        "name": "Yazheng Liu"
                    },
                    {
                        "authorId": "2108286275",
                        "name": "Xi Zhang"
                    },
                    {
                        "authorId": "3131378",
                        "name": "Sihong Xie"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "58fd3609571d4c49478c9c8d260e272aab31f960",
                "externalIds": {
                    "DBLP": "conf/bigdataconf/WarmsleyWXLT22",
                    "DOI": "10.1109/BigData55660.2022.10020943",
                    "CorpusId": 256319294
                },
                "corpusId": 256319294,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/58fd3609571d4c49478c9c8d260e272aab31f960",
                "title": "A Survey of Explainable Graph Neural Networks for Cyber Malware Analysis",
                "abstract": "Malicious cybersecurity activities have become increasingly worrisome for individuals and companies alike. While machine learning methods like Graph Neural Networks (GNNs) have proven successful on the malware detection task, their output is often difficult to understand. Explainable malware detection methods are needed to automatically identify malicious programs and present results to malware analysts in a way that is human interpretable. In this survey, we outline a number of GNN explainability methods and compare their performance on a real-world malware detection dataset. Specifically, we formulated the detection problem as a graph classification problem on the malware Control Flow Graphs (CFGs). We find that gradient-based methods outperform perturbation-based methods in terms of computational expense and performance on explainer-specific metrics (e.g., Fidelity and Sparsity). Our results provide insights into designing new GNN-based models for cyber malware detection and attribution.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9950697",
                        "name": "Dana Warmsley"
                    },
                    {
                        "authorId": "2978541",
                        "name": "Alex Waagen"
                    },
                    {
                        "authorId": "1684988",
                        "name": "Jiejun Xu"
                    },
                    {
                        "authorId": "2135920458",
                        "name": "Zhining Liu"
                    },
                    {
                        "authorId": "2058143613",
                        "name": "H. Tong"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "8e55df97a7bb7f44a707d75d3e4179e0bdd1e8d2",
                "externalIds": {
                    "DBLP": "conf/bigdataconf/SarkarAL22",
                    "DOI": "10.1109/BigData55660.2022.10020969",
                    "CorpusId": 256323567
                },
                "corpusId": 256323567,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8e55df97a7bb7f44a707d75d3e4179e0bdd1e8d2",
                "title": "Explainable Prediction of the Severity of COVID-19 Outbreak for US Counties",
                "abstract": "Ever since the COVID-19 outbreak, various works have focused on using multitude of different static and dynamic features to aid the prediction of disease forecasting models. However, in the absence of historical pandemic data these models will not be able to give any meaningful insight about the areas which are most likely to be affected based on preexisting conditions. Furthermore, the black box nature of neural networks often becomes an impediment for the concerned authorities to derive any meaning from. In this paper, we propose a novel explainable Graph Neural Network (GNN) framework called Graph-COVID-19-Explainer (GC-Explainer) that gives explainable prediction for the severity of the spread during initial outbreak. We utilize a comprehensive set of static population characteristics to use as node features of Graph where each node corresponds to a geographical region. Unlike post-hoc methods of GNN explanations, we propose a framework for learning important features during the training of the model. We further apply our model on real-world early pandemic data to show the validity of our approach. Through GC-Explainer, we show that static features along with spatial dependency among regions can be used to explain the varied degree of severity in outbreak during the early part of the pandemic and provide a framework to identify the at-risk areas for any infectious disease outbreak, especially when historical data is not available.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2153583242",
                        "name": "Shailik Sarkar"
                    },
                    {
                        "authorId": "1471374166",
                        "name": "Abdulaziz Alhamadani"
                    },
                    {
                        "authorId": "1752590",
                        "name": "Chang-Tien Lu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "5c743bea9cdd2bf0a3efb603c67309ee27907aa0",
                "externalIds": {
                    "ArXiv": "2212.08966",
                    "CorpusId": 257496081
                },
                "corpusId": 257496081,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5c743bea9cdd2bf0a3efb603c67309ee27907aa0",
                "title": "Graph Learning and Its Applications: A Holistic Survey",
                "abstract": "Graph learning is a prevalent domain that endeavors to learn the intricate relationships among nodes and the topological structure of graphs. Over the years, graph learning has transcended from graph theory to graph data mining. With the advent of representation learning, it has attained remarkable performance in diverse scenarios. Owing to its extensive application prospects, graph learning attracts copious attention. While some researchers have accomplished impressive surveys on graph learning, they failed to connect related objectives, methods, and applications in a more coherent way. As a result, they did not encompass current ample scenarios and challenging problems due to the rapid expansion of graph learning. Particularly, large language models have recently had a disruptive effect on human life, but they also show relative weakness in structured scenarios. The question of how to make these models more powerful with graph learning remains open. Different from previous surveys on graph learning, we provide a holistic review that analyzes current works from the perspective of graph structure, and discusses the latest applications, trends, and challenges in graph learning. Specifically, we commence by proposing a taxonomy and then summarize the methods employed in graph learning. We then provide a detailed elucidation of mainstream applications. Finally, we propose future directions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1409866591",
                        "name": "Shaopeng Wei"
                    },
                    {
                        "authorId": "97522134",
                        "name": "Yu Zhao"
                    },
                    {
                        "authorId": "2143791763",
                        "name": "Xingyan Chen"
                    },
                    {
                        "authorId": "2117895423",
                        "name": "Qing Li"
                    },
                    {
                        "authorId": "2162961864",
                        "name": "Fuzhen Zhuang"
                    },
                    {
                        "authorId": "2155375528",
                        "name": "Ji Liu"
                    },
                    {
                        "authorId": "2147326459",
                        "name": "Gang Kou"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "8edb469ef694e6259a97e88a53440338d3865ebc",
                "externalIds": {
                    "DBLP": "journals/bib/TianWYLY23",
                    "DOI": "10.1093/bib/bbac534",
                    "CorpusId": 254850082,
                    "PubMed": "36526280"
                },
                "corpusId": 254850082,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8edb469ef694e6259a97e88a53440338d3865ebc",
                "title": "Predicting molecular properties based on the interpretable graph neural network with multistep focus mechanism",
                "abstract": "Graph neural networks based on deep learning methods have been extensively applied to the molecular property prediction because of its powerful feature learning ability and good performance. However, most of them are black boxes and cannot give the reasonable explanation about the underlying prediction mechanisms, which seriously reduce people's trust on the neural network-based prediction models. Here we proposed a novel graph neural network named iteratively focused graph network (IFGN), which can gradually identify the key atoms/groups in the molecule that are closely related to the predicted properties by the multistep focus mechanism. At the same time, the combination of the multistep focus mechanism with visualization can also generate multistep interpretations, thus allowing us to gain a deep understanding of the predictive behaviors of the model. For all studied eight datasets, the IFGN model achieved good prediction performance, indicating that the proposed multistep focus mechanism also can improve the performance of the model obviously besides increasing the interpretability of built model. For researchers to use conveniently, the corresponding website (http://graphadmet.cn/works/IFGN) was also developed and can be used free of charge.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2125433444",
                        "name": "Yanan Tian"
                    },
                    {
                        "authorId": "2108159658",
                        "name": "Xiaorui Wang"
                    },
                    {
                        "authorId": "114008058",
                        "name": "X. Yao"
                    },
                    {
                        "authorId": "46936235",
                        "name": "Huanxiang Liu"
                    },
                    {
                        "authorId": "2193639833",
                        "name": "Yingzhen Yang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Molecular graphs of the Mutagenic class have two topology groups, one with motif NO2 and another one with motif NH2 [13]."
            ],
            "citingPaper": {
                "paperId": "0376ebc5fc9ae00ea3f3f31e0a92d0c2788cf486",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-08689",
                    "ArXiv": "2212.08689",
                    "DOI": "10.48550/arXiv.2212.08689",
                    "CorpusId": 254853664
                },
                "corpusId": 254853664,
                "publicationVenue": {
                    "id": "50534c12-f4ba-4c64-806b-01647d1baacf",
                    "name": "LOG IN",
                    "type": "journal",
                    "alternate_names": [
                        "Log in",
                        "Log",
                        "LOG"
                    ],
                    "issn": "0720-8642",
                    "alternate_issns": [
                        "1547-4690",
                        "0024-5798"
                    ],
                    "url": "https://www.log-in-verlag.de/informatische_bildung/",
                    "alternate_urls": [
                        "https://www.anycorp.com/log/about",
                        "https://www.jstor.org/journal/log",
                        "https://www.anycorp.com/",
                        "http://www.jstor.org/action/showPublication?journalCode=log",
                        "http://www.anycorp.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0376ebc5fc9ae00ea3f3f31e0a92d0c2788cf486",
                "title": "TopoImb: Toward Topology-level Imbalance in Learning from Graphs",
                "abstract": "Graph serves as a powerful tool for modeling data that has an underlying structure in non-Euclidean space, by encoding relations as edges and entities as nodes. Despite developments in learning from graph-structured data over the years, one obstacle persists: graph imbalance. Although several attempts have been made to target this problem, they are limited to considering only class-level imbalance. In this work, we argue that for graphs, the imbalance is likely to exist at the sub-class topology group level. Due to the flexibility of topology structures, graphs could be highly diverse, and learning a generalizable classification boundary would be difficult. Therefore, several majority topology groups may dominate the learning process, rendering others under-represented. To address this problem, we propose a new framework {\\method} and design (1 a topology extractor, which automatically identifies the topology group for each instance with explicit memory cells, (2 a training modulator, which modulates the learning process of the target GNN model to prevent the case of topology-group-wise under-representation. {\\method} can be used as a key component in GNN models to improve their performances under the data imbalance setting. Analyses on both topology-level imbalance and the proposed {\\method} are provided theoretically, and we empirically verify its effectiveness with both node-level and graph-level classification as the target tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1999191869",
                        "name": "Tianxiang Zhao"
                    },
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    },
                    {
                        "authorId": "48505793",
                        "name": "Xiang Zhang"
                    },
                    {
                        "authorId": "2116430057",
                        "name": "Suhang Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "cb8b40570cb2b987e47052dea06979cae38fb129",
                "externalIds": {
                    "ArXiv": "2212.07056",
                    "DBLP": "journals/corr/abs-2212-07056",
                    "DOI": "10.48550/arXiv.2212.07056",
                    "CorpusId": 254636157
                },
                "corpusId": 254636157,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cb8b40570cb2b987e47052dea06979cae38fb129",
                "title": "On the Probability of Necessity and Sufficiency of Explaining Graph Neural Networks: A Lower Bound Optimization Approach",
                "abstract": "The explainability of Graph Neural Networks (GNNs) is critical to various GNN applications but remains an open challenge. A convincing explanation should be both necessary and sufficient simultaneously. However, existing GNN explaining approaches focus on only one of the two aspects, necessity or sufficiency, or a heuristic trade-off between the two. Theoretically, the Probability of Necessity and Sufficiency (PNS) can be applied to search for the most necessary and sufficient explanation since it can mathematically quantify the necessity and sufficiency of an explanation. Nevertheless, the difficulty of obtaining PNS due to non-monotonicity and the challenge of counterfactual estimation limit its wide use. To address the non-identifiability of PNS, we resort to a lower bound of PNS that can be optimized via counterfactual estimation, and propose Necessary and Sufficient Explanation for GNN (NSEG) via optimizing that lower bound. Specifically, we employ nearest neighbor matching to generate counterfactual samples and leverage continuous masks with a sampling strategy to optimize the lower bound. Empirical study shows that NSEG achieves excellent performance in generating the most necessary and sufficient explanations among a series of state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39913331",
                        "name": "Ruichu Cai"
                    },
                    {
                        "authorId": "2144317761",
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "authorId": "1598058943",
                        "name": "Xuexin Chen"
                    },
                    {
                        "authorId": "143844731",
                        "name": "Yuan Fang"
                    },
                    {
                        "authorId": "2180361439",
                        "name": "Min Wu"
                    },
                    {
                        "authorId": "2054649406",
                        "name": "Jie Qiao"
                    },
                    {
                        "authorId": "145586380",
                        "name": "Z. Hao"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "4310c6bfe66b10b078fb4d395c1b73de5234b192",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-03806",
                    "ArXiv": "2212.03806",
                    "DOI": "10.48550/arXiv.2212.03806",
                    "CorpusId": 254366361
                },
                "corpusId": 254366361,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4310c6bfe66b10b078fb4d395c1b73de5234b192",
                "title": "Towards Explainable Motion Prediction using Heterogeneous Graph Representations",
                "abstract": "Motion prediction systems aim to capture the future behavior of traffic scenarios enabling autonomous vehicles to perform safe and efficient planning. The evolution of these scenarios is highly uncertain and depends on the interactions of agents with static and dynamic objects in the scene. GNN-based approaches have recently gained attention as they are well suited to naturally model these interactions. However, one of the main challenges that remains unexplored is how to address the complexity and opacity of these models in order to deal with the transparency requirements for autonomous driving systems, which includes aspects such as interpretability and explainability. In this work, we aim to improve the explainability of motion prediction systems by using different approaches. First, we propose a new Explainable Heterogeneous Graph-based Policy (XHGP) model based on an heterograph representation of the traffic scene and lane-graph traversals, which learns interaction behaviors using object-level and type-level attention. This learned attention provides information about the most important agents and interactions in the scene. Second, we explore this same idea with the explanations provided by GNNExplainer. Third, we apply counterfactual reasoning to provide explanations of selected individual scenarios by exploring the sensitivity of the trained model to changes made to the input data, i.e., masking some elements of the scene, modifying trajectories, and adding or removing dynamic agents. The explainability analysis provided in this paper is a first step towards more transparent and reliable motion prediction systems, important from the perspective of the user, developers and regulatory agencies. The code to reproduce this work is publicly available at https://github.com/sancarlim/Explainable-MP/tree/v1.1.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1395613806",
                        "name": "Sandra Carrasco Limeros"
                    },
                    {
                        "authorId": "2088566030",
                        "name": "Sylwia Majchrowska"
                    },
                    {
                        "authorId": "19173439",
                        "name": "Joakim Johnander"
                    },
                    {
                        "authorId": "6387775",
                        "name": "Christoffer Petersson"
                    },
                    {
                        "authorId": "1729433",
                        "name": "D. F. Llorca"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The perturbation-based interpretation methods [8], [9] can only select the set of important nodes, but it is difficult to produce a fair and reasonable importance ranking for nodes."
            ],
            "citingPaper": {
                "paperId": "25b1fa9960dadba581dc4b342c61ede99a486f45",
                "externalIds": {
                    "DBLP": "conf/globecom/LiLLCY22",
                    "DOI": "10.1109/GLOBECOM48099.2022.10001460",
                    "CorpusId": 255597121
                },
                "corpusId": 255597121,
                "publicationVenue": {
                    "id": "b189dec0-41d0-4cea-a906-7c5186895904",
                    "name": "Global Communications Conference",
                    "type": "conference",
                    "alternate_names": [
                        "GLOBECOM",
                        "Glob Commun Conf"
                    ],
                    "url": "http://www.ieee-globecom.org/"
                },
                "url": "https://www.semanticscholar.org/paper/25b1fa9960dadba581dc4b342c61ede99a486f45",
                "title": "Shapley Explainer - An Interpretation Method for GNNs Used in SDN",
                "abstract": "Graph neural networks (GNNs) have been widely applied in software-defined network (SDN) for better network modeling and performance prediction. However, the black-box characteristic of deep learning makes the GNNs hard to interpret, such interpretability issue hinders the wide use of GNNs. In this paper, we propose Shapley Explainer, that provides fair importance scores to the input nodes of a GNN within an appropriate computation cost, thereby providing a valid and reasonable interpretation of graph neural network on software defined network. The proposed method derives the importance ranking of topological nodes by combining shapley values with a soft discrete mask matrix. We apply Shapley Explainer to RouteNet model, a GNN model that provides intelligent predictions of SDN network performance metrics. The experimental results show that Shapley Explainer can provide effective interpretations for RouteNet. It also verifies that the RouteNet model can correctly learn the relationship between features, which can provide a better understanding of the prediction process of RouteNet, promoting the application of GNN-based SDN systems in engineering practice.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1838103",
                        "name": "Chuanhuang Li"
                    },
                    {
                        "authorId": "2199949540",
                        "name": "Jiali Lou"
                    },
                    {
                        "authorId": "2172052034",
                        "name": "Shiyuan Liu"
                    },
                    {
                        "authorId": "49865500",
                        "name": "Zebin Chen"
                    },
                    {
                        "authorId": "152162529",
                        "name": "Xiaoyong Yuan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                ", 2019; Zhang and Zitnik, 2020), Black-box explanation (Ying et al., 2019; Luo et al., 2020; Vu and Thai, 2020), etc."
            ],
            "citingPaper": {
                "paperId": "fa1dc6a23a946ee4730f0340a709717865d72543",
                "externalIds": {
                    "DBLP": "journals/fdata/FuH22",
                    "PubMedCentral": "9755577",
                    "DOI": "10.3389/fdata.2022.1062637",
                    "CorpusId": 254129753,
                    "PubMed": "36532844"
                },
                "corpusId": 254129753,
                "publicationVenue": {
                    "id": "165fa1b5-e07f-4b6e-9203-04493f6a7c5c",
                    "name": "Frontiers in Big Data",
                    "alternate_names": [
                        "Front Big Data"
                    ],
                    "issn": "2624-909X",
                    "url": "https://www.frontiersin.org/journals/big-data"
                },
                "url": "https://www.semanticscholar.org/paper/fa1dc6a23a946ee4730f0340a709717865d72543",
                "title": "Natural and Artificial Dynamics in Graphs: Concept, Progress, and Future",
                "abstract": "Graph structures have attracted much research attention for carrying complex relational information. Based on graphs, many algorithms and tools are proposed and developed for dealing with real-world tasks such as recommendation, fraud detection, molecule design, etc. In this paper, we first discuss three topics of graph research, i.e., graph mining, graph representations, and graph neural networks (GNNs). Then, we introduce the definitions of natural dynamics and artificial dynamics in graphs, and the related works of natural and artificial dynamics about how they boost the aforementioned graph research topics, where we also discuss the current limitation and future opportunities.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1893402501",
                        "name": "Dongqi Fu"
                    },
                    {
                        "authorId": "31108652",
                        "name": "Jingrui He"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "2019), PGExplainer (Luo et al. 2020), GraphLIME (Huang et al.",
                "Our work is directly related to black-box perturbationbased explaining methods for GNNs, including GNNExplainer (Ying et al. 2019), PGExplainer (Luo et al. 2020), GraphLIME (Huang et al. 2020), PGMExplainer (Vu and Thai 2020), RelEx (Zhang, DeFazio, and Ramesh 2021), GraphSVX (Duval and Malliaros 2021) and ZORRO (Funke, Khosla, and Anand 2021).",
                "Our work is directly related to black-box perturbationbased explaining methods for GNNs, including GNNExplainer (Ying et al. 2019), PGExplainer (Luo et al. 2020), GraphLIME (Huang et al. 2020), PGMExplainer (Vu and Thai 2020), RelEx (Zhang, DeFazio, and Ramesh 2021), GraphSVX (Duval and Malliaros\u2026"
            ],
            "citingPaper": {
                "paperId": "50c9c755e423da1d4809f01d9e792911bb0c5ccd",
                "externalIds": {
                    "ArXiv": "2212.00952",
                    "DBLP": "journals/corr/abs-2212-00952",
                    "DOI": "10.48550/arXiv.2212.00952",
                    "CorpusId": 254221214
                },
                "corpusId": 254221214,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/50c9c755e423da1d4809f01d9e792911bb0c5ccd",
                "title": "On the Limit of Explaining Black-box Temporal Graph Neural Networks",
                "abstract": "Temporal Graph Neural Network (TGNN) has been receiving a lot of attention recently due to its capability in modeling time-evolving graph-related tasks. Similar to Graph Neural Networks, it is also non-trivial to interpret predictions made by a TGNN due to its black-box nature. A major approach tackling this problems in GNNs is by analyzing the model' responses on some perturbations of the model's inputs, called perturbation-based explanation methods. While these methods are convenient and flexible since they do not need internal access to the model, does this lack of internal access prevent them from revealing some important information of the predictions? Motivated by that question, this work studies the limit of some classes of perturbation-based explanation methods. Particularly, by constructing some specific instances of TGNNs, we show (i) node-perturbation cannot reliably identify the paths carrying out the prediction, (ii) edge-perturbation is not reliable in determining all nodes contributing to the prediction and (iii) perturbing both nodes and edges does not reliably help us identify the graph's components carrying out the temporal aggregation in TGNNs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32590027",
                        "name": "Minh N. Vu"
                    },
                    {
                        "authorId": "1698253",
                        "name": "M. Thai"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "f474925aea22b01e4162f2ca49cd3bccf64e2afb",
                "externalIds": {
                    "ArXiv": "2212.00342",
                    "DBLP": "journals/corr/abs-2212-00342",
                    "DOI": "10.1145/3570991.3571065",
                    "CorpusId": 254125610
                },
                "corpusId": 254125610,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f474925aea22b01e4162f2ca49cd3bccf64e2afb",
                "title": "xEM: Explainable Entity Matching in Customer 360",
                "abstract": "Entity matching in Customer 360 is the task of determining if multiple records represent the same real world entity. Entities are typically people, organizations, locations, and events represented as attributed nodes in a graph, though they can also be represented as records in relational data. While probabilistic matching engines and artificial neural network models exist for this task, explaining entity matching has received less attention. In this demo, we present our Explainable Entity Matching (xEM) system and discuss the different AI/ML considerations that went into its implementation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2088077323",
                        "name": "Sukriti Jaitly"
                    },
                    {
                        "authorId": "2192958696",
                        "name": "Deepa Mariam George"
                    },
                    {
                        "authorId": "27526892",
                        "name": "Balaji Ganesan"
                    },
                    {
                        "authorId": "2193035545",
                        "name": "Muhammad Ameen"
                    },
                    {
                        "authorId": "2193035487",
                        "name": "Srinivas Pusapati"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "More specifically, these explanation methods can be broadly categorized into i) gradient-based methods [5] that leverage the gradients of the GNN model to generate explanations; ii) perturbation-based methods [9, 11, 13] that aim to generate explanations by calculating the change in GNN predictions upon perturbations of the input graph structure (nodes, edges, or subgraphs); and iii) surrogate-based methods [7, 12] that fit a simple interpretable model to approximate the predictive behavior of the given GNN model.",
                "While several categories of GNN explanation methods have been proposed: gradient-based [5, 10, 14], perturbation-based [8, 9, 11, 13, 15], and surrogatebased [7, 12], their utility is limited to generating post hoc node- and edge-level explanations for a given pre-trained GNN model."
            ],
            "citingPaper": {
                "paperId": "242e3249bc4a22d39ad939c4445e73a0aee1eca9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-16731",
                    "ArXiv": "2211.16731",
                    "DOI": "10.48550/arXiv.2211.16731",
                    "CorpusId": 254096047
                },
                "corpusId": 254096047,
                "publicationVenue": {
                    "id": "50534c12-f4ba-4c64-806b-01647d1baacf",
                    "name": "LOG IN",
                    "type": "journal",
                    "alternate_names": [
                        "Log in",
                        "Log",
                        "LOG"
                    ],
                    "issn": "0720-8642",
                    "alternate_issns": [
                        "1547-4690",
                        "0024-5798"
                    ],
                    "url": "https://www.log-in-verlag.de/informatische_bildung/",
                    "alternate_urls": [
                        "https://www.anycorp.com/log/about",
                        "https://www.jstor.org/journal/log",
                        "https://www.anycorp.com/",
                        "http://www.jstor.org/action/showPublication?journalCode=log",
                        "http://www.anycorp.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/242e3249bc4a22d39ad939c4445e73a0aee1eca9",
                "title": "Towards Training GNNs using Explanation Directed Message Passing",
                "abstract": "With the increasing use of Graph Neural Networks (GNNs) in critical real-world applications, several post hoc explanation methods have been proposed to understand their predictions. However, there has been no work in generating explanations on the fly during model training and utilizing them to improve the expressive power of the underlying GNN models. In this work, we introduce a novel explanation-directed neural message passing framework for GNNs, EXPASS (EXplainable message PASSing), which aggregates only embeddings from nodes and edges identified as important by a GNN explanation method. EXPASS can be used with any existing GNN architecture and subgraph-optimizing explainer to learn accurate graph embeddings. We theoretically show that EXPASS alleviates the oversmoothing problem in GNNs by slowing the layer wise loss of Dirichlet energy and that the embedding difference between the vanilla message passing and EXPASS framework can be upper bounded by the difference of their respective model weights. Our empirical results show that graph embeddings learned using EXPASS improve the predictive performance and alleviate the oversmoothing problems of GNNs, opening up new frontiers in graph machine learning to develop explanation-based training frameworks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2156850569",
                        "name": "V. Giunchiglia"
                    },
                    {
                        "authorId": "2192822479",
                        "name": "Chirag Varun Shukla"
                    },
                    {
                        "authorId": "2159543073",
                        "name": "Guadalupe Gonzalez"
                    },
                    {
                        "authorId": "40228633",
                        "name": "Chirag Agarwal"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "In the realm of learning on graphs, some existing works aim to interpret GNNs (Ying et al. 2019; Luo et al. 2020; Yuan et al. 2020a), and they mainly focus on understanding the utility (e.",
                "In the realm of learning on graphs, some existing works aim to interpret GNNs (Ying et al. 2019; Luo et al. 2020; Yuan et al. 2020a), and they mainly focus on understanding the utility (e.g., node classification accuracy) of GNNs on the test set."
            ],
            "citingPaper": {
                "paperId": "ec568050b74edfdb7e463200daf42ba36663505c",
                "externalIds": {
                    "ArXiv": "2211.14383",
                    "DBLP": "conf/aaai/DongW0LL23",
                    "DOI": "10.48550/arXiv.2211.14383",
                    "CorpusId": 254044254
                },
                "corpusId": 254044254,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/ec568050b74edfdb7e463200daf42ba36663505c",
                "title": "Interpreting Unfairness in Graph Neural Networks via Training Node Attribution",
                "abstract": "Graph Neural Networks (GNNs) have emerged as the leading paradigm for solving graph analytical problems in various real-world applications. Nevertheless, GNNs could potentially render biased predictions towards certain demographic subgroups. Understanding how the bias in predictions arises is critical, as it guides the design of GNN debiasing mechanisms. However, most existing works overwhelmingly focus on GNN debiasing, but fall short on explaining how such bias is induced. In this paper, we study a novel problem of interpreting GNN unfairness through attributing it to the influence of training nodes. Specifically, we propose a novel strategy named Probabilistic Distribution Disparity (PDD) to measure the bias exhibited in GNNs, and develop an algorithm to efficiently estimate the influence of each training node on such bias. We verify the validity of PDD and the effectiveness of influence estimation through experiments on real-world datasets. Finally, we also demonstrate how the proposed framework could be used for debiasing GNNs. Open-source code can be found at https://github.com/yushundong/BIND.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "123918726",
                        "name": "Yushun Dong"
                    },
                    {
                        "authorId": "2117075272",
                        "name": "Song Wang"
                    },
                    {
                        "authorId": "2157405959",
                        "name": "Jing Ma"
                    },
                    {
                        "authorId": "47717322",
                        "name": "Ninghao Liu"
                    },
                    {
                        "authorId": "1737121128",
                        "name": "Jundong Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Notable ones include GNNExplainer [40], PGExplainer [20], and Zorro [12]."
            ],
            "citingPaper": {
                "paperId": "f1069c804886a294b1bc50a925318330dccf6d54",
                "externalIds": {
                    "ArXiv": "2211.13236",
                    "DBLP": "journals/corr/abs-2211-13236",
                    "DOI": "10.48550/arXiv.2211.13236",
                    "CorpusId": 254018143
                },
                "corpusId": 254018143,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f1069c804886a294b1bc50a925318330dccf6d54",
                "title": "MEGAN: Multi-Explanation Graph Attention Network",
                "abstract": "We propose a multi-explanation graph attention network (MEGAN). Unlike existing graph explainability methods, our network can produce node and edge attributional explanations along multiple channels, the number of which is independent of task specifications. This proves crucial to improve the interpretability of graph regression predictions, as explanations can be split into positive and negative evidence w.r.t to a reference value. Additionally, our attention-based network is fully differentiable and explanations can actively be trained in an explanation-supervised manner. We first validate our model on a synthetic graph regression dataset with known ground-truth explanations. Our network outperforms existing baseline explainability methods for the single- as well as the multi-explanation case, achieving near-perfect explanation accuracy during explanation supervision. Finally, we demonstrate our model's capabilities on multiple real-world datasets. We find that our model produces sparse high-fidelity explanations consistent with human intuition about those tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2149076354",
                        "name": "Jonas Teufel"
                    },
                    {
                        "authorId": "12366131",
                        "name": "Luca Torresi"
                    },
                    {
                        "authorId": "47591654",
                        "name": "Patrick Reiser"
                    },
                    {
                        "authorId": "35323511",
                        "name": "Pascal Friederich"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "In recent years, people have put forward several methods to explain GNNs prediction, such as XGNN [30], GNNExplainer [9], PGExplainer [26], etc.",
                "Disturbance based methods [9], [18], [25], [26] monitor the changes of predicted values under different input disturbances, to learn the importance score of input characteristics.",
                "PGExplainer uses the generation probability model of graph data, which can learn concise potential structure from the observed graph data [26].",
                "Take PGExplainer as an example [26], which provides explanation for each instance from the global perspective of GNNs model."
            ],
            "citingPaper": {
                "paperId": "e0639a141a5bca7c2100b042226a859e69370d07",
                "externalIds": {
                    "DBLP": "conf/iccpr/KangLL22",
                    "DOI": "10.1145/3581807.3581850",
                    "CorpusId": 258834549
                },
                "corpusId": 258834549,
                "publicationVenue": {
                    "id": "73e015d4-4386-4edd-b9c2-f563f870bfa2",
                    "name": "International Conferences on Computing and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "ICCPR",
                        "Int Conf Comput Pattern Recognit"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e0639a141a5bca7c2100b042226a859e69370d07",
                "title": "GANExplainer: Explainability Method for Graph Neural Network with Generative Adversarial Nets",
                "abstract": "In recent years, graph neural networks (GNNs) have achieved encouraging performance in the processing of graph data generated in non-Euclidean space. GNNs learn node features by aggregating and combining neighbor information, which is applied to many graphics tasks. However, the complex deep learning structure is still regarded as a black box, which is difficult to obtain the full trust of human beings. Due to the lack of interpretability, the application of graph neural network is greatly limited. Therefore, we propose an interpretable method, called GANExplainer, to explain GNNs at the model level. Our method can implicitly generate the characteristic subgraph of the graph without relying on specific input examples as the interpretation of the model to the data. GANExplainer relies on the framework of generative-adversarial method to train the generator and discriminator at the same time. More importantly, when constructing the discriminator, the corresponding graph rules are added to ensure the effectiveness of the generated characteristic subgraph. We carried out experiments on synthetic dataset and chemical molecules dataset and verified the effect of our method on model level interpreter from three aspects: accuracy, fidelity and sparsity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2172124591",
                        "name": "Xinrui Kang"
                    },
                    {
                        "authorId": "2176030959",
                        "name": "Dong Liang"
                    },
                    {
                        "authorId": "2108049317",
                        "name": "Qinfeng Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "63d2f88b6d3d912d5898b18e8c796c19779d4823",
                "externalIds": {
                    "PubMedCentral": "9669545",
                    "DBLP": "journals/air/AskrEEEGH23",
                    "DOI": "10.1007/s10462-022-10306-1",
                    "CorpusId": 253662441,
                    "PubMed": "36415536"
                },
                "corpusId": 253662441,
                "publicationVenue": {
                    "id": "ea8553fe-2467-4367-afee-c4deb3754820",
                    "name": "Artificial Intelligence Review",
                    "type": "journal",
                    "alternate_names": [
                        "Artif Intell Rev"
                    ],
                    "issn": "0269-2821",
                    "url": "https://link.springer.com/journal/10462"
                },
                "url": "https://www.semanticscholar.org/paper/63d2f88b6d3d912d5898b18e8c796c19779d4823",
                "title": "Deep learning in drug discovery: an integrative review and future challenges",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1641821967",
                        "name": "Heba Askr"
                    },
                    {
                        "authorId": "118275048",
                        "name": "Enas Elgeldawi"
                    },
                    {
                        "authorId": "2191345430",
                        "name": "Heba Aboul Ella"
                    },
                    {
                        "authorId": "11911513",
                        "name": "Y. Elshaier"
                    },
                    {
                        "authorId": "41229815",
                        "name": "M. Gomaa"
                    },
                    {
                        "authorId": "1697259",
                        "name": "A. Hassanien"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "291fd11f65c3b74f551bbee1c14905bab1c57de2",
                "externalIds": {
                    "ArXiv": "2211.08903",
                    "DBLP": "journals/corr/abs-2211-08903",
                    "DOI": "10.48550/arXiv.2211.08903",
                    "CorpusId": 253553656
                },
                "corpusId": 253553656,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/291fd11f65c3b74f551bbee1c14905bab1c57de2",
                "title": "Cross-Mode Knowledge Adaptation for Bike Sharing Demand Prediction using Domain-Adversarial Graph Neural Networks",
                "abstract": "For bike sharing systems, demand prediction is crucial to ensure the timely re-balancing of available bikes according to predicted demand. Existing methods for bike sharing demand prediction are mostly based on its own historical demand variation, essentially regarding it as a closed system and neglecting the interaction between different transportation modes. This is particularly important for bike sharing because it is often used to complement travel through other modes (e.g., public transit). Despite some recent progress, no existing method is capable of leveraging spatiotemporal information from multiple modes and explicitly considers the distribution discrepancy between them, which can easily lead to negative transfer. To address these challenges, this study proposes a domain-adversarial multi-relational graph neural network (DA-MRGNN) for bike sharing demand prediction with multimodal historical data as input. A temporal adversarial adaptation network is introduced to extract shareable features from demand patterns of different modes. To capture correlations between spatial units across modes, we adapt a multi-relational graph neural network (MRGNN) considering both cross-mode similarity and difference. In addition, an explainable GNN technique is developed to understand how our proposed model makes predictions. Extensive experiments are conducted using real-world bike sharing, subway and ride-hailing data from New York City. The results demonstrate the superior performance of our proposed approach compared to existing methods and the effectiveness of different model components.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2117876144",
                        "name": "Yuebing Liang"
                    },
                    {
                        "authorId": "2159540981",
                        "name": "Guan Huang"
                    },
                    {
                        "authorId": "144743176",
                        "name": "Zhan Zhao"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "A variant of this method, the PGExplainer [63] uses a deep neural network to parameterize the generation process of the masks and the explanations in general, thereby explaining multiple instances of sub-graphs at the same time."
            ],
            "citingPaper": {
                "paperId": "05eba5c8543551f090486c68bf4e9e5e2b27cb53",
                "externalIds": {
                    "PubMedCentral": "9794543",
                    "DBLP": "journals/ki/FinzelSATPH22",
                    "DOI": "10.1007/s13218-022-00781-7",
                    "CorpusId": 253405517,
                    "PubMed": "36590103"
                },
                "corpusId": 253405517,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/05eba5c8543551f090486c68bf4e9e5e2b27cb53",
                "title": "Generating Explanations for Conceptual Validation of Graph Neural Networks: An Investigation of Symbolic Predicates Learned on Relevance-Ranked Sub-Graphs",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40009599",
                        "name": "Bettina Finzel"
                    },
                    {
                        "authorId": "1947785",
                        "name": "Anna Saranti"
                    },
                    {
                        "authorId": "2162998870",
                        "name": "Alessa Angerschmid"
                    },
                    {
                        "authorId": "2130321660",
                        "name": "David E. Tafler"
                    },
                    {
                        "authorId": "2152300",
                        "name": "B. Pfeifer"
                    },
                    {
                        "authorId": "47596587",
                        "name": "Andreas Holzinger"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The last two datasets are widely used to benchmark explanation methods [15, 17] for graph classification."
            ],
            "citingPaper": {
                "paperId": "f628a458c3f71978a864f361e58cb945595c523f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-01548",
                    "ArXiv": "2211.01548",
                    "DOI": "10.48550/arXiv.2211.01548",
                    "CorpusId": 253264998
                },
                "corpusId": 253264998,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f628a458c3f71978a864f361e58cb945595c523f",
                "title": "INGREX: An Interactive Explanation Framework for Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) are widely used in many modern applications, necessitating explanations for their decisions. However, the complexity of GNNs makes it difficult to explain predictions. Even though several methods have been proposed lately, they can only provide simple and static explanations, which are difficult for users to understand in many scenarios. Therefore, we introduce INGREX, an interactive explanation framework for GNNs designed to aid users in comprehending model predictions. Our framework is implemented based on multiple explanation algorithms and advanced libraries. We demonstrate our framework in three scenarios covering common demands for GNN explanations to present its effectiveness and helpfulness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "41022579",
                        "name": "Tien-Cuong Bui"
                    },
                    {
                        "authorId": "2055470540",
                        "name": "Van-Duc Le"
                    },
                    {
                        "authorId": "2108718185",
                        "name": "Wen-Syan Li"
                    },
                    {
                        "authorId": "2237996",
                        "name": "S. Cha"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "a47b67429370638a9bfea439f28878710e8b369c",
                "externalIds": {
                    "DBLP": "journals/cbm/HuangYYPZ23",
                    "DOI": "10.1016/j.compbiomed.2022.106308",
                    "CorpusId": 253940890,
                    "PubMed": "36462371"
                },
                "corpusId": 253940890,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a47b67429370638a9bfea439f28878710e8b369c",
                "title": "MNC-Net: Multi-task graph structure learning based on node clustering for early Parkinson's disease diagnosis",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1574219909",
                        "name": "Liqin Huang"
                    },
                    {
                        "authorId": "5579854",
                        "name": "Xiaofang Ye"
                    },
                    {
                        "authorId": "2174206513",
                        "name": "Mingjing Yang"
                    },
                    {
                        "authorId": "2089093863",
                        "name": "Lin Pan"
                    },
                    {
                        "authorId": "152449801",
                        "name": "Shaohua Zheng"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "In 2020, PGExplainer [16] was proposed, providing a global understanding of predictions made by GNNs.",
                "In 2020, PGExplainer [16] was proposed, providing a global understanding of predictions made by GNNs. PGMExplainer was also introduced in 2020 [17], which is based on perturbation of the original graph to eliminate unimportant variables from input\u2013output data, employing explanation Bayesian networks as the last step."
            ],
            "citingPaper": {
                "paperId": "06789f80bb006ea5cbc2e10815be1a1a8b94fa1d",
                "externalIds": {
                    "DBLP": "journals/entropy/LysovMVGT22",
                    "PubMedCentral": "9689005",
                    "DOI": "10.3390/e24111597",
                    "CorpusId": 253305613,
                    "PubMed": "36359687"
                },
                "corpusId": 253305613,
                "publicationVenue": {
                    "id": "8270cfe1-3713-4325-a7bd-c6a87eed889e",
                    "name": "Entropy",
                    "type": "journal",
                    "issn": "1099-4300",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-155606",
                    "alternate_urls": [
                        "http://www.mdpi.com/journal/entropy/",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-155606",
                        "https://www.mdpi.com/journal/entropy"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/06789f80bb006ea5cbc2e10815be1a1a8b94fa1d",
                "title": "Entropy as a High-Level Feature for XAI-Based Early Plant Stress Detection",
                "abstract": "This article is devoted to searching for high-level explainable features that can remain explainable for a wide class of objects or phenomena and become an integral part of explainable AI (XAI). The present study involved a 25-day experiment on early diagnosis of wheat stress using drought stress as an example. The state of the plants was periodically monitored via thermal infrared (TIR) and hyperspectral image (HSI) cameras. A single-layer perceptron (SLP)-based classifier was used as the main instrument in the XAI study. To provide explainability of the SLP input, the direct HSI was replaced by images of six popular vegetation indices and three HSI channels (R630, G550, and B480; referred to as indices), along with the TIR image. Furthermore, in the explainability analysis, each of the 10 images was replaced by its 6 statistical features: min, max, mean, std, max\u2013min, and the entropy. For the SLP output explainability, seven output neurons corresponding to the key states of the plants were chosen. The inner layer of the SLP was constructed using 15 neurons, including 10 corresponding to the indices and 5 reserved neurons. The classification possibilities of all 60 features and 10 indices of the SLP classifier were studied. Study result: Entropy is the earliest high-level stress feature for all indices; entropy and an entropy-like feature (max\u2013min) paired with one of the other statistical features can provide, for most indices, 100% accuracy (or near 100%), serving as an integral part of XAI.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2159303036",
                        "name": "Maxim Lysov"
                    },
                    {
                        "authorId": "123443886",
                        "name": "Irina E. Maximova"
                    },
                    {
                        "authorId": "2058607372",
                        "name": "E. Vasiliev"
                    },
                    {
                        "authorId": "51136441",
                        "name": "A. Getmanskaya"
                    },
                    {
                        "authorId": "2864194",
                        "name": "V. Turlapov"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Recently, various GNN explanation [15], [16], [38], [39] or graph denoising [17] techniques have been developed to find the most informative structures."
            ],
            "citingPaper": {
                "paperId": "7559e646d1aa8e47dc60c513bcb9b97357e8833f",
                "externalIds": {
                    "DBLP": "conf/icdm/0002ST22",
                    "DOI": "10.1109/ICDM54844.2022.00177",
                    "CorpusId": 256463492
                },
                "corpusId": 256463492,
                "publicationVenue": {
                    "id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
                    "name": "Industrial Conference on Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Ind Conf Data Min",
                        "ICDM"
                    ],
                    "url": "http://www.data-mining-forum.de/"
                },
                "url": "https://www.semanticscholar.org/paper/7559e646d1aa8e47dc60c513bcb9b97357e8833f",
                "title": "Sparsified Subgraph Memory for Continual Graph Representation Learning",
                "abstract": "Memory replay, which stores a subset of representative historical data from previous tasks to replay while learning new tasks, exhibits state-of-the-art performance for various continual learning applications on Euclidean data. While topological information plays a critical role in characterizing graph data, existing memory replay based graph learning techniques only store individual nodes for replay and do not consider their associated edge information. To this end, we propose a sparsified subgraph memory (SSM), which sparsifies the selected computation graphs into fixed size before storing them into the memory. In this way, we can reduce the memory consumption of a computation subgraph from $\\mathcal{O}(d^{L})$ to $\\mathcal{O}(1)$, and for the first time enable GNNs to utilize the explicit topological information for memory replay. Finally, our empirical studies show that SSM outperforms state-of-the-art approaches by up to 27.8% on four different public datasets. Unlike existing methods which focus on task incremental learning (task-IL) setting, SSM succeeds in the challenging class incremental learning (class-IL) setting in which a model is required to distinguish all learned classes without task indicators, and even achieves comparable performance to joint training which is the performance upper bound for continual learning. Our code is available at https://github.com/QueuQ/SSM.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3358065",
                        "name": "Xikun Zhang"
                    },
                    {
                        "authorId": "2451800",
                        "name": "Dongjin Song"
                    },
                    {
                        "authorId": "143719920",
                        "name": "D. Tao"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "contexts": [
                "PGExplainer [9] proposed a probabilistic graph generative model to provide explanations of multiple instances collectively.",
                ") input graph that contributed most towards the underlying GNN model\u2019s prediction [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].",
                ", [5], [6], [9], [13]), model-level explanations for GNNs have been largely underexplored in the literature.",
                "On the other hand, instead of directly searching for subgraphs, several studies [9], [10], [12] were shown to learn parameterized models for explanations of GNN\u2019s predictions."
            ],
            "citingPaper": {
                "paperId": "67c0a7c64b62c2b6370c3788c685d00d610963d6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-17159",
                    "ArXiv": "2210.17159",
                    "DOI": "10.48550/arXiv.2210.17159",
                    "CorpusId": 253237095
                },
                "corpusId": 253237095,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/67c0a7c64b62c2b6370c3788c685d00d610963d6",
                "title": "PAGE: Prototype-Based Model-Level Explanations for Graph Neural Networks",
                "abstract": "\u2014Aside from graph neural networks (GNNs) catching signi\ufb01cant attention as a powerful framework revolutionizing graph representation learning, there has been an increasing demand for explaining GNN models. Although various explanation methods for GNNs have been developed, most studies have focused on instance-level explanations, which produce explanations tailored to a given graph instance. In our study, we propose Prototype-bAsed GNN-Explainer (PAGE) , a novel model-level GNN explanation method that explains what the underlying GNN model has learned for graph classi\ufb01cation by discovering human-interpretable prototype graphs . Our method produces explanations for a given class , thus being capable of offering more concise and comprehensive explanations than those of instance-level explanations. First, PAGE selects embeddings of class-discriminative input graphs on the graph-level embedding space after clustering them. Then, PAGE discovers a common subgraph pattern by iteratively searching for high matching node tuples using node-level embeddings via a prototype scoring function, thereby yielding a prototype graph as our explanation. Using \ufb01ve graph classi\ufb01cation datasets, we demonstrate that PAGE qualitatively and quantitatively outperforms the state-of-the-art model-level explanation method. We also carry out experimental studies systematically by showing the relationship between PAGE and instance-level explanation methods, the robustness of PAGE to input data scarce environments, and the computational ef\ufb01ciency of the proposed prototype scoring function in PAGE.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2087244325",
                        "name": "Yong-Min Shin"
                    },
                    {
                        "authorId": "2129906247",
                        "name": "Sun-Woo Kim"
                    },
                    {
                        "authorId": "2164501868",
                        "name": "Won-Yong Shin"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "This approach generalizes PGExplainer (Luo et al., 2020) and learns a node mask m \u2208 [0, 1]n for each sample C. Similar to BernMask, it also has mask size and entropy constraints in its loss function, and their coefficients are both tuned from {1.0, 0.1, 0.01} as well.",
                "BernMask-P is extended from Luo et al. (2020) based on the authors\u2019 code and a recent PR in PyG.",
                "Some methods to interpret graph neural networks can be applied to geometric data (Ying et al., 2019; Luo et al., 2020; Schlichtkrull et al., 2021; Yuan et al., 2021).",
                ", 2017) were widely used for inherent interpretability, multiple recent studies show that they cannot provide reliable interpretation, especially for data with irregular structures (Serrano & Smith, 2019; Jain & Wallace, 2019; Ying et al., 2019; Luo et al., 2020).",
                "This approach generalizes PGExplainer (Luo et al., 2020) and learns a node mask m \u2208 [0, 1] for each sample C.",
                "Among them, BernMask and BernMask-P are post-hoc extended from two previous methods on graph-structured data, i.e., Ying et al. (2019) and Luo et al. (2020).",
                "\u2026et al., 2015; Vaswani et al., 2017) were widely used for inherent interpretability, multiple recent studies show that they cannot provide reliable interpretation, especially for data with irregular structures (Serrano & Smith, 2019; Jain & Wallace, 2019; Ying et al., 2019; Luo et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "18ffe561eefe204c116d7f871a25c0500f5098e0",
                "externalIds": {
                    "DBLP": "conf/iclr/0001LLL23",
                    "ArXiv": "2210.16966",
                    "DOI": "10.48550/arXiv.2210.16966",
                    "CorpusId": 253238010
                },
                "corpusId": 253238010,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/18ffe561eefe204c116d7f871a25c0500f5098e0",
                "title": "Interpretable Geometric Deep Learning via Learnable Randomness Injection",
                "abstract": "Point cloud data is ubiquitous in scientific fields. Recently, geometric deep learning (GDL) has been widely applied to solve prediction tasks with such data. However, GDL models are often complicated and hardly interpretable, which poses concerns to scientists who are to deploy these models in scientific analysis and experiments. This work proposes a general mechanism, learnable randomness injection (LRI), which allows building inherently interpretable models based on general GDL backbones. LRI-induced models, once trained, can detect the points in the point cloud data that carry information indicative of the prediction label. We also propose four datasets from real scientific applications that cover the domains of high-energy physics and biochemistry to evaluate the LRI mechanism. Compared with previous post-hoc interpretation methods, the points detected by LRI align much better and stabler with the ground-truth patterns that have actual scientific meanings. LRI is grounded by the information bottleneck principle, and thus LRI-induced models are also more robust to distribution shifts between training and test scenarios. Our code and datasets are available at \\url{https://github.com/Graph-COM/LRI}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151793768",
                        "name": "Siqi Miao"
                    },
                    {
                        "authorId": "6426643",
                        "name": "Yunan Luo"
                    },
                    {
                        "authorId": "2156102035",
                        "name": "Miaoyuan Liu"
                    },
                    {
                        "authorId": "2112519768",
                        "name": "Pan Li"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "PgExpl: (Perturbation) [53]: The Parametrized Explainer for GNNs (PgExpl) [53] adopts a very similar formulation of the explanation problem as GnnExpl where the two major differences are: i) PgExpl provides solely explanations in terms of subgraph structures, neglecting explanations in terms of node features; ii) instead of directly optimizing continuous edge and features masks as done by GnnExpl, it uses Gradient Descend to train a MLP which, given the two concatenated node embeddings [X (t) i ||X (t) j ], predicts the likelihood of the edge (i, j) being a relevant edge.",
                "[95] proposed a categorization of explainers into four categories: gradient-based which exploit gradients of the input neural network [79, 80, 57]; perturbation-based where perturbations of the input graphs are aimed at obtaining explainable subgraphs [91, 53, 24, 69]; decomposition-based which try to decompose the input identifying the explanations [6, 57, 70]; and surrogate-based where a simple interpretable surrogate model is used to explain the original neural network [38, 101, 84].",
                "Independently from this categorization, a further fundamental distinction is among explainers providing explanations in terms of edge [91, 53, 69, 96] or node masks [79, 80, 57, 6, 57, 70].",
                "Roughly speaking, gradient-based explainers exploit gradients of the input neural network [79, 80, 57], perturbation-based models perturb the input aiming to obtain explainable subgraphs[91, 53, 24, 69], decomposition-based models try to decompose the input identifying the explanations [6, 57, 70], while surrogate-based models use a simple interpretable surrogate to explain the original neural network [38, 101, 84].",
                "However, their study is limited to node classification and the three explainers under analysis [91, 53, 69] are not well representative of the diversity of explanation strategies that have been proposed, as summarized in the aforementioned taxonomy [95].",
                "[91], is a widely used dataset for benchmarking GNN explainers [91, 53, 75, 95, 96, 104]."
            ],
            "citingPaper": {
                "paperId": "bfafae6c6add7f06bd212911de27f75e416c015a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-15304",
                    "ArXiv": "2210.15304",
                    "DOI": "10.48550/arXiv.2210.15304",
                    "CorpusId": 253157826
                },
                "corpusId": 253157826,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bfafae6c6add7f06bd212911de27f75e416c015a",
                "title": "Explaining the Explainers in Graph Neural Networks: a Comparative Study",
                "abstract": "Following a fast initial breakthrough in graph based learning, Graph Neural Networks (GNNs) have reached a widespread application in many science and engineering fields, prompting the need for methods to understand their decision process. GNN explainers have started to emerge in recent years, with a multitude of methods both novel or adapted from other domains. To sort out this plethora of alternative approaches, several studies have benchmarked the performance of different explainers in terms of various explainability metrics. However, these earlier works make no attempts at providing insights into why different GNN architectures are more or less explainable, or which explainer should be preferred in a given setting. In this survey, we fill these gaps by devising a systematic experimental study, which tests ten explainers on eight representative architectures trained on six carefully designed graph and node classification datasets. With our results we provide key insights on the choice and applicability of GNN explainers, we isolate key components that make them usable and successful and provide recommendations on how to avoid common interpretation pitfalls. We conclude by highlighting open questions and directions of possible future research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2130615549",
                        "name": "Antonio Longa"
                    },
                    {
                        "authorId": "2165224662",
                        "name": "Steve Azzolin"
                    },
                    {
                        "authorId": "2042269425",
                        "name": "G. Santin"
                    },
                    {
                        "authorId": "50139333",
                        "name": "G. Cencetti"
                    },
                    {
                        "authorId": "2075355155",
                        "name": "Pietro Lio'"
                    },
                    {
                        "authorId": "49305855",
                        "name": "B. Lepri"
                    },
                    {
                        "authorId": "1702610",
                        "name": "Andrea Passerini"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "211599bc961320d04067c583601c4465a73dd584",
                "externalIds": {
                    "ArXiv": "2210.11695",
                    "DBLP": "journals/corr/abs-2210-11695",
                    "DOI": "10.1145/3539597.3570376",
                    "CorpusId": 253080473
                },
                "corpusId": 253080473,
                "publicationVenue": {
                    "id": "ea38228f-6ed3-4222-a3ce-d963d8cc9516",
                    "name": "Web Search and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Web Search Data Min",
                        "WSDM"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=3158"
                },
                "url": "https://www.semanticscholar.org/paper/211599bc961320d04067c583601c4465a73dd584",
                "title": "Global Counterfactual Explainer for Graph Neural Networks",
                "abstract": "Graph neural networks (GNNs) find applications in various domains such as computational biology, natural language processing, and computer security. Owing to their popularity, there is an increasing need to explain GNN predictions since GNNs are black-box machine learning models. One way to address this is counterfactual reasoning where the objective is to change the GNN prediction by minimal changes in the input graph. Existing methods for counterfactual explanation of GNNs are limited to instance-specific local reasoning. This approach has two major limitations of not being able to offer global recourse policies and overloading human cognitive ability with too much information. In this work, we study the global explainability of GNNs through global counterfactual reasoning. Specifically, we want to find a small set of representative counterfactual graphs that explains all input graphs. Towards this goal, we propose GCFExplainer, a novel algorithm powered by vertex-reinforced random walks on an edit map of graphs with a greedy summary. Extensive experiments on real graph datasets show that the global explanation from GCFExplainer provides important high-level insights of the model behavior and achieves a 46.9% gain in recourse coverage and a 9.5% reduction in recourse cost compared to the state-of-the-art local counterfactual explainers.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2134769449",
                        "name": "Mert Kosan"
                    },
                    {
                        "authorId": "2129461984",
                        "name": "Zexi Huang"
                    },
                    {
                        "authorId": "3390598",
                        "name": "Sourav Medya"
                    },
                    {
                        "authorId": "1699732",
                        "name": "Sayan Ranu"
                    },
                    {
                        "authorId": "1399890865",
                        "name": "Ambuj K. Singh"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "601e18e5158d03d4db62c9bd85519f40f8e32fe4",
                "externalIds": {
                    "ArXiv": "2210.12089",
                    "DBLP": "journals/corr/abs-2210-12089",
                    "DOI": "10.1145/3618105",
                    "CorpusId": 253080529
                },
                "corpusId": 253080529,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/601e18e5158d03d4db62c9bd85519f40f8e32fe4",
                "title": "A Survey on Graph Counterfactual Explanations: Definitions, Methods, Evaluation, and Research Challenges",
                "abstract": "Graph Neural Networks (GNNs) perform well in community detection and molecule classification. Counterfactual Explanations (CE) provide counter-examples to overcome the transparency limitations of black-box models. Due to the growing attention in graph learning, we focus on the concepts of CE for GNNs. We analysed the SoA to provide a taxonomy, a uniform notation, and the benchmarking datasets and evaluation metrics. We discuss fourteen methods, their evaluation protocols, twenty-two datasets, and nineteen metrics. We integrated the majority of methods into the GRETEL library to conduct an empirical evaluation to understand their strengths and pitfalls. We highlight open challenges and future work.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1390033678",
                        "name": "Mario Alfonso Prado-Romero"
                    },
                    {
                        "authorId": "32208207",
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "authorId": "1765155",
                        "name": "G. Stilo"
                    },
                    {
                        "authorId": "1685102",
                        "name": "F. Giannotti"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "Following [7, 8], our explanation framework concentrates on finding important factors from graph structures, node features, and edge features that affect predictions the most.",
                "We chose one instance from each dataset and visualized explanations provided by SCALE, GNNExplainer, and PGExplainer in Figure 3.",
                "BA-2motifs (BA-2m) [8] consists of 1000 graphs with two classes constructed by adding specific motifs to BA graphs, where half contain 5-node house motifs and the other half include 5-node cycle motifs.",
                "Besides, post-hoc explanation methods [7, 8] often transform node classification problems into graph classification problems via subgraph (Khop) sampling.",
                "Its performance is also comparable to PGExplainer on the BA-2motifs dataset.",
                "Specifically, it achieves outstanding precision and recall scores in node classification datasets and outperforms state-of-the-art methods GNNExplainer and PGExplainer.",
                "(5) Inspired by [8], the mask matrix is initialized via an MLP network in which inputs are edge embedding vectors constructed by concatenating embedding vectors of source and target nodes taken from the black-box GNN model.",
                "The performance gains are up to 94x compared to GNNExplainer and 120x in comparison with PGExplainer.",
                "[7], is an active research area with several following research papers [8, 22, 23].",
                "Their proposed solutions for GNN explanations are too straightforward, making it hard to achieve significant results on other datasets, such as the ones in [7, 8].",
                "Among them, perturbation methods such as GNNExplainer [7] and PGExplainer [8] have been widely adopted since they introduced benchmark datasets for explanation tasks and achieved outstanding results.",
                "For fair comparisons, we contacted the authors of GNNExplainer, PGExplainer, and SEGNN to request evaluation scripts for all datasets.",
                "Even though GNNExplainer and PGExplainer can also present explanations on multiple levels by adjusting the visibility threshold, setting this value too low may result in outputs with multiple disconnected components due to the independence of edge selections.",
                "Second, we compared our framework with two state-of-the-art post-hoc explanation methods [7, 8] in qualitative aspects to highlight the quality of explanations provided by SCALE.",
                "\u2022 PGExplainer [8] shared the same approach with GNNExplainer [7] but initialized masks using embedding vectors from the pre-trained model.",
                "Even though GNNExplainer and PGExplainer can highlight impactful edges of target nodes in node classification explanations, they cannot differentiate the contributions of these edges since edge weights only represent selection probabilities.",
                "SCALE outperforms baselines on Mutag, wherein the precision score gains are 15.54% compared to PGExplainer and 51.52% in comparison with GNNExplainer.",
                "Other baselines except PGExplainer were also executed using the same PyTorch version.",
                "9.1 for experiments with PGExplainer.",
                "Therefore, we follow [7, 8] to formulate structural explanations as binary classification tasks, wherein influential nodes and edges of predictions are included in explanations."
            ],
            "citingPaper": {
                "paperId": "9be4ad61622a522dd044a0696afcfdb27378e8f0",
                "externalIds": {
                    "ArXiv": "2210.11094",
                    "DBLP": "journals/corr/abs-2210-11094",
                    "DOI": "10.48550/arXiv.2210.11094",
                    "CorpusId": 253018959
                },
                "corpusId": 253018959,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9be4ad61622a522dd044a0696afcfdb27378e8f0",
                "title": "Toward Multiple Specialty Learners for Explaining GNNs via Online Knowledge Distillation",
                "abstract": "Graph Neural Networks (GNNs) have become increasingly ubiquitous in numerous applications and systems, necessitating explanations of their predictions, especially when making critical decisions. However, explaining GNNs is challenging due to the complexity of graph data and model execution. Despite additional computational costs, post-hoc explanation approaches have been widely adopted due to the generality of their architectures. Intrinsically interpretable models provide instant explanations but are usually model-specific, which can only explain particular GNNs. Therefore, we propose a novel GNN explanation framework named SCALE, which is general and fast for explaining predictions. SCALE trains multiple specialty learners to explain GNNs since constructing one powerful explainer to examine attributions of interactions in input graphs is complicated. In training, a black-box GNN model guides learners based on an online knowledge distillation paradigm. In the explanation phase, explanations of predictions are provided by multiple explainers corresponding to trained learners. Specifically, edge masking and random walk with restart procedures are executed to provide structural explanations for graph-level and node-level predictions, respectively. A feature attribution module provides overall summaries and instance-level feature contributions. We compare SCALE with state-of-the-art baselines via quantitative and qualitative experiments to prove its explanation correctness and execution performance. We also conduct a series of ablation studies to understand the strengths and weaknesses of the proposed framework.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "41022579",
                        "name": "Tien-Cuong Bui"
                    },
                    {
                        "authorId": "2055470540",
                        "name": "Van-Duc Le"
                    },
                    {
                        "authorId": "2108718185",
                        "name": "Wen-Syan Li"
                    },
                    {
                        "authorId": "2237996",
                        "name": "S. Cha"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "0923ba40016c1a93e3642a7a5cd43acfa834dc0a",
                "externalIds": {
                    "DBLP": "conf/cikm/Prado-RomeroS22",
                    "DOI": "10.1145/3511808.3557608",
                    "CorpusId": 252904717
                },
                "corpusId": 252904717,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/0923ba40016c1a93e3642a7a5cd43acfa834dc0a",
                "title": "GRETEL: Graph Counterfactual Explanation Evaluation Framework",
                "abstract": "Machine Learning (ML) systems are a building part of the modern tools which impact our daily life in several application domains. Due to their black-box nature, those systems are hardly adopted in application domains (e.g. health, finance) where understanding the decision process is of paramount importance. Explanation methods were developed to explain how the ML model has taken a specific decision for a given case/instance. Graph Counterfactual Explanations (GCE) is one of the explanation techniques adopted in the Graph Learning domain. The existing works on Graph Counterfactual Explanations diverge mostly in the problem definition, application domain, test data, and evaluation metrics, and most existing works do not compare exhaustively against other counterfactual explanation techniques present in the literature. We present GRETEL, a unified framework to develop and test GCE methods in several settings. GRETEL is a highly extensible evaluation framework which promotes Open Science and the reproducibility of the evaluation by providing a set of well-defined mechanisms to integrate and manage easily: both real and synthetic datasets, ML models, state-of-the-art explanation techniques, and evaluation measures. Lastly, we also show the experiments conducted to integrate and test several existing scenarios (datasets, measures, explainers).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1390033678",
                        "name": "Mario Alfonso Prado-Romero"
                    },
                    {
                        "authorId": "1765155",
                        "name": "G. Stilo"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Typical post-hoc interpretation methods include approximation-based methods [2], relevance-propagationbased methods [34, 48], perturbation-based methods [25, 46], and decomposition methods [8]."
            ],
            "citingPaper": {
                "paperId": "02887690948c215612cf77145b3af5525f06a2e1",
                "externalIds": {
                    "DBLP": "conf/cikm/0002LHJ22",
                    "DOI": "10.1145/3511808.3557500",
                    "CorpusId": 252904836
                },
                "corpusId": 252904836,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/02887690948c215612cf77145b3af5525f06a2e1",
                "title": "Tutorial on Deep Learning Interpretation: A Data Perspective",
                "abstract": "Deep learning models have achieved exceptional predictive performance in a wide variety of tasks, ranging from computer vision, natural language processing, to graph mining. Many businesses and organizations across diverse domains are now building large-scale applications based on deep learning. However, there are growing concerns, regarding the fairness, security, and trustworthiness of these models, largely due to the opaque nature of their decision processes. Recently, there has been an increasing interest in explainable deep learning that aims to reduce the opacity of a model by explaining its behavior, its predictions, or both, thus building trust between human and complex deep learning models. A collection of explanation methods have been proposed in recent years that address the problem of low explainability and opaqueness of models. In this tutorial, we introduce recent explanation methods from a data perspective, targeting models that process image data, text data, and graph data, respectively. We will compare their strengths and limitations, and offer real-world applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "98256637",
                        "name": "Zhou Yang"
                    },
                    {
                        "authorId": "47717322",
                        "name": "Ninghao Liu"
                    },
                    {
                        "authorId": "2123553641",
                        "name": "Xia Hu"
                    },
                    {
                        "authorId": "2174345226",
                        "name": "Fang Jin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "38f513a6cefe6148e1e479312e56f3ed1deaa3a5",
                "externalIds": {
                    "DBLP": "conf/cikm/SahaDB22",
                    "DOI": "10.1145/3511808.3557535",
                    "CorpusId": 252905038
                },
                "corpusId": 252905038,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/38f513a6cefe6148e1e479312e56f3ed1deaa3a5",
                "title": "A Model-Centric Explainer for Graph Neural Network based Node Classification",
                "abstract": "Graph Neural Networks (GNNs) learn node representations by aggregating a node's feature vector with its neighbors. They perform well across a variety of graph tasks. However, to enhance the reliability and trustworthiness of these models during use in critical scenarios, it is of essence to look into the decision making mechanisms of these models rather than treating them as black boxes. Our model-centric method gives insight into the kind of information learnt by GNNs about node neighborhoods during the task of node classification. We propose a neighborhood generator as an explainer that generates optimal neighborhoods to maximize a particular class prediction of the trained GNN model. We formulate neighborhood generation as a reinforcement learning problem and use a policy gradient method to train our generator using feedback from the trained GNN-based node classifier. Our method provides intelligible explanations of learning mechanisms of GNN models on synthetic as well as real-world datasets and even highlights certain shortcomings of these models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145760267",
                        "name": "Sayan Saha"
                    },
                    {
                        "authorId": "39544273",
                        "name": "Monidipa Das"
                    },
                    {
                        "authorId": "82752795",
                        "name": "S. Bandyopadhyay"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "b5b4eff9962c1fdcdc11fc7c4861d579f4c34f3a",
                "externalIds": {
                    "DOI": "10.1101/2022.10.17.22279804",
                    "CorpusId": 252915138
                },
                "corpusId": 252915138,
                "publicationVenue": {
                    "id": "d5e5b5e7-54b1-4f53-82fc-4853f3e71c58",
                    "name": "medRxiv",
                    "type": "journal",
                    "url": "https://www.medrxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/b5b4eff9962c1fdcdc11fc7c4861d579f4c34f3a",
                "title": "Screening of normal endoscopic large bowel biopsies with artificial intelligence: a retrospective study",
                "abstract": "Objectives: Develop an interpretable AI algorithm to rule out normal large bowel endoscopic biopsies saving pathologist resources. Design: Retrospective study. Setting: One UK NHS site was used for model training and internal validation. External validation conducted on data from two other NHS sites and one site in Portugal. Participants: 6,591 whole-slides images of endoscopic large bowel biopsies from 3,291 patients (54% Female, 46% Male). Main outcome measures: Area under the receiver operating characteristic and precision recall curves (AUC-ROC and AUC-PR), measuring agreement between consensus pathologist diagnosis and AI generated classification of normal versus abnormal biopsies. Results: A graph neural network was developed incorporating pathologist domain knowledge to classify the biopsies as normal or abnormal using clinically driven interpretable features. Model training and internal validation were performed on 5,054 whole slide images of 2,080 patients from a single NHS site resulting in an AUC-ROC of 0.98 (SD=0.004) and AUC-PR of 0.98 (SD=0.003). The predictive performance of the model was consistent in testing over 1,537 whole slide images of 1,211 patients from three independent external datasets with mean AUC-ROC = 0.97 (SD=0.007) and AUC-PR = 0.97 (SD=0.005). Our analysis shows that at a high sensitivity threshold of 99%, the proposed model can, on average, reduce the number of normal slides to be reviewed by a pathologist by 55%. A key advantage of IGUANA is its ability to provide an explainable output highlighting potential abnormalities in a whole slide image as a heatmap overlay in addition to numerical values associating model prediction with various histological features. Example results with interpretable features can be viewed online at https://iguana.dcs.warwick.ac.uk/. Conclusions: An interpretable AI model was developed to screen abnormal cases for review by pathologists. The model achieved consistently high predictive accuracy on independent cohorts showing its potential in optimising increasingly scarce pathologist resources and for achieving faster time to diagnosis. Explainable predictions of IGUANA can guide pathologists in their diagnostic decision making and help boost their confidence in the algorithm, paving the way for future clinical adoption.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1393638783",
                        "name": "S. Graham"
                    },
                    {
                        "authorId": "2144851619",
                        "name": "F. Minhas"
                    },
                    {
                        "authorId": "144769161",
                        "name": "M. Bilal"
                    },
                    {
                        "authorId": "2185196473",
                        "name": "Mahmoud Ali"
                    },
                    {
                        "authorId": "39412069",
                        "name": "Y. Tsang"
                    },
                    {
                        "authorId": "2241747975",
                        "name": "Mark Eastwood"
                    },
                    {
                        "authorId": "40406870",
                        "name": "N. Wahab"
                    },
                    {
                        "authorId": "9965999",
                        "name": "M. Jahanifar"
                    },
                    {
                        "authorId": "51403067",
                        "name": "E. Hero"
                    },
                    {
                        "authorId": "2241661209",
                        "name": "Katherine Dodd"
                    },
                    {
                        "authorId": "50845995",
                        "name": "H. Sahota"
                    },
                    {
                        "authorId": "2112539068",
                        "name": "Shao-Hong Wu"
                    },
                    {
                        "authorId": "2241839850",
                        "name": "Wenqi Lu"
                    },
                    {
                        "authorId": "10377090",
                        "name": "A. Azam"
                    },
                    {
                        "authorId": "1656647604",
                        "name": "K. Benes"
                    },
                    {
                        "authorId": "7702841",
                        "name": "M. Nimir"
                    },
                    {
                        "authorId": "2241749759",
                        "name": "Katherine Hewitt"
                    },
                    {
                        "authorId": "145901092",
                        "name": "A. Bhalerao"
                    },
                    {
                        "authorId": "2228123801",
                        "name": "Andrew Robinson"
                    },
                    {
                        "authorId": "46805778",
                        "name": "H. Eldaly"
                    },
                    {
                        "authorId": "31752042",
                        "name": "S. Raza"
                    },
                    {
                        "authorId": "40503291",
                        "name": "K. Gopalakrishnan"
                    },
                    {
                        "authorId": "47723656",
                        "name": "D. Snead"
                    },
                    {
                        "authorId": "1580315694",
                        "name": "N. Rajpoot"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Other works have explored post-hoc analysis for explainable predictions in GNNs, notably, GNNExplainer Ying et al. [2019], PGExplainer Luo et al. [2020], PGM-Explainer Vu and Thai [2020], and SubgraphX Yuan et al. [2021] have all been developed for this purpose.",
                "[2019], PGExplainer Luo et al. [2020], PGM-Explainer Vu and Thai [2020], and SubgraphX Yuan et al.",
                "[2019], PGExplainer Luo et al. [2020], PGM-Explainer Vu and Thai [2020], and SubgraphX Yuan et al. [2021] have all been developed for this purpose."
            ],
            "citingPaper": {
                "paperId": "a489f6377247d7c98102e644a48de0e640c7f279",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-09475",
                    "ArXiv": "2210.09475",
                    "DOI": "10.48550/arXiv.2210.09475",
                    "CorpusId": 252968370
                },
                "corpusId": 252968370,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a489f6377247d7c98102e644a48de0e640c7f279",
                "title": "AMPNet: Attention as Message Passing for Graph Neural Networks",
                "abstract": "Feature-level interactions between nodes can carry crucial information for under-standing complex interactions in graph-structured data. Current interpretability techniques, however, are limited in their ability to capture feature-level interactions between different nodes. In this work, we propose AMPNet, a general Graph Neural Network (GNN) architecture for uncovering feature-level interactions between different nodes in a graph. Our framework applies a multiheaded attention operation during message-passing to contextualize messages based on the feature interactions between different nodes. We utilize subgraph sampling and node feature downsampling in our experiments to improve the scalability of our architecture to large networks. We evaluate AMPNet on several benchmark and real-world datasets, and develop a synthetic benchmark based on cyclic cellular automata to test the ability of our framework to recover the underlying generation rules of the cellular automata based on feature-interactions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49669190",
                        "name": "S. A. Rizvi"
                    },
                    {
                        "authorId": "120729272",
                        "name": "N. Nguyen"
                    },
                    {
                        "authorId": "8544727",
                        "name": "H. Lyu"
                    },
                    {
                        "authorId": "2188058342",
                        "name": "B. Christensen"
                    },
                    {
                        "authorId": "16217712",
                        "name": "J. O. Caro"
                    },
                    {
                        "authorId": "103284094",
                        "name": "E. Zappala"
                    },
                    {
                        "authorId": "2117751468",
                        "name": "M. Brbi\u0107"
                    },
                    {
                        "authorId": "2180409627",
                        "name": "R. M. Dhodapkar"
                    },
                    {
                        "authorId": "7385683",
                        "name": "D. V. Dijk"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "The authors in [2] limited 250 their analysis to graphs that were containing the ground truth motifs, and proposed to just keep the 251 top-k edges.",
                "1 Training the GNN f 236 For both BAMultiShapes and Mutagenicity we relied on the codebase provided by [2] for training 237 the GNN f to explain and to train the Local Explainer.",
                "For Mutagenicity we replicated the setting in the PGExplainer paper [21], while for BAMultiShapes and HIN we trained until convergence a 3-layer GCN. Details about the training of the networks and their accuracies are in the Appendix.",
                "The results for Mutagenicity are in line with the one reported in [2].",
                "1, 2, 3 144 [2] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang 145 Zhang.",
                "2 Local Explanations Processing 245 As detailed in [2], the output of PGExplainer consists in a weighted edge mask wij \u2208 V \u00d7 V where 246 each wij is the likelihood of the edge being an important edge.",
                "For Mutagenicity, we sticked to the 247 original implementation which was correctly able to reproduce the results presented in the paper [2].",
                "For Mutagenicity, over which PGExplainer was originally evaluated, we simply selected the threshold \u03b8 that maximises the F1 score of the local explainer over all graphs, including those that do not contain the ground-truth motif.",
                "Nonetheless, in this work, we relied on PGExplainer [2] since it allows 57 the extraction of arbitrary disconnected motifs as explanations and it gave excellent results in our 58 experiments.",
                "Finally, for BAMultiShapes and HIN, for which we extracted our own local explanations, we trained PGExplainer on the train split of the original dataset.",
                "01 For Mutagenicity we replicated the model accuracy and the local explanations presented in [2], while 116 for BAMultiShapes we trained until convergence a 3-layers GCN.",
                "For BAMultiShapes we trained a 3-layers 238 GCN [27] (20-20-20 hidden units) with mean graph pooling for the final prediction, whereas for 239 Mutagenicty we reproduced the results of [2].",
                "In this work we relied mainly on two off-the-shelf explainers, namely, PGExplainer [21] and XGNN [36].",
                "PGExplainer: For Mutagenicity and BAMultiShapes, we used the original implementation as provided by [21]."
            ],
            "citingPaper": {
                "paperId": "5bce30f98caa5decac63ef6cf58f5580b4c17883",
                "externalIds": {
                    "ArXiv": "2210.07147",
                    "DBLP": "journals/corr/abs-2210-07147",
                    "DOI": "10.48550/arXiv.2210.07147",
                    "CorpusId": 252873057
                },
                "corpusId": 252873057,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5bce30f98caa5decac63ef6cf58f5580b4c17883",
                "title": "Global Explainability of GNNs via Logic Combination of Learned Concepts",
                "abstract": "While instance-level explanation of GNN is a well-studied problem with plenty of approaches being developed, providing a global explanation for the behaviour of a GNN is much less explored, despite its potential in interpretability and debugging. Existing solutions either simply list local explanations for a given class, or generate a synthetic prototypical graph with maximal score for a given class, completely missing any combinatorial aspect that the GNN could have learned. In this work, we propose GLGExplainer (Global Logic-based GNN Explainer), the first Global Explainer capable of generating explanations as arbitrary Boolean combinations of learned graphical concepts. GLGExplainer is a fully differentiable architecture that takes local explanations as inputs and combines them into a logic formula over graphical concepts, represented as clusters of local explanations. Contrary to existing solutions, GLGExplainer provides accurate and human-interpretable global explanations that are perfectly aligned with ground-truth explanations (on synthetic data) or match existing domain knowledge (on real-world data). Extracted formulas are faithful to the model predictions, to the point of providing insights into some occasionally incorrect rules learned by the model, making GLGExplainer a promising diagnostic tool for learned GNNs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2165224662",
                        "name": "Steve Azzolin"
                    },
                    {
                        "authorId": "2130615549",
                        "name": "Antonio Longa"
                    },
                    {
                        "authorId": "2123005765",
                        "name": "Pietro Barbiero"
                    },
                    {
                        "authorId": "2075355155",
                        "name": "Pietro Lio'"
                    },
                    {
                        "authorId": "1702610",
                        "name": "Andrea Passerini"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "[35] introduced a prior to graph, and sampled a new graph from observed one via uniform distribution."
            ],
            "citingPaper": {
                "paperId": "eed3acfd8dff188a8d0a92a27c4ee146e86b7c7a",
                "externalIds": {
                    "ArXiv": "2210.07011",
                    "DBLP": "journals/corr/abs-2210-07011",
                    "DOI": "10.48550/arXiv.2210.07011",
                    "CorpusId": 252873303
                },
                "corpusId": 252873303,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/eed3acfd8dff188a8d0a92a27c4ee146e86b7c7a",
                "title": "Variational Graph Generator for Multi-View Graph Clustering",
                "abstract": "Multi-view graph clustering (MGC) methods are increasingly being studied due to the explosion of multi-view data with graph structural information. The critical point of MGC is to better utilize the view-specific and view-common information in features and graphs of multiple views. However, existing works have an inherent limitation that they are unable to concurrently utilize the consensus graph information across multiple graphs and the view-specific feature information. To address this issue, we propose Variational Graph Generator for Multi-View Graph Clustering (VGMGC). Specifically, a novel variational graph generator is proposed to extract common information among multiple graphs. This generator infers a reliable variational consensus graph based on a priori assumption over multiple graphs. Then a simple yet effective graph encoder in conjunction with the multi-view clustering objective is presented to learn the desired graph embeddings for clustering, which embeds the inferred view-common graph and view-specific graphs together with features. Finally, theoretical results illustrate the rationality of VGMGC by analyzing the uncertainty of the inferred consensus graph with information bottleneck principle. Extensive experiments demonstrate the superior performance of our VGMGC over SOTAs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2135294453",
                        "name": "Jianpeng Chen"
                    },
                    {
                        "authorId": "47396038",
                        "name": "Yawen Ling"
                    },
                    {
                        "authorId": "2145753839",
                        "name": "Jie Xu"
                    },
                    {
                        "authorId": "2041980",
                        "name": "Yazhou Ren"
                    },
                    {
                        "authorId": "2712233",
                        "name": "Shudong Huang"
                    },
                    {
                        "authorId": "1783239",
                        "name": "X. Pu"
                    },
                    {
                        "authorId": "40901820",
                        "name": "Lifang He"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Recently, some initial efforts [14, 22, 38, 41, 43] have been taken to address the explainability issue of GNNs.",
                "PGExplainer [22] proposes to combine the global view of GNNs to facilitate the extraction of important graphs by applying a parameterized explainer.",
                "\u2022 PGExplainer [22]: It adopts a MLP-based explainer to obtain the important subgraphs from a global view to reduce the computation cost and obtain better explanations.",
                "Recently, some works in explainability of GNNs are emerging [3, 10, 22, 25, 38, 41]."
            ],
            "citingPaper": {
                "paperId": "a0e0b032c29d71507b132caa0da77749c9b0ba01",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01974",
                    "ArXiv": "2210.01974",
                    "DOI": "10.48550/arXiv.2210.01974",
                    "CorpusId": 252715869
                },
                "corpusId": 252715869,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a0e0b032c29d71507b132caa0da77749c9b0ba01",
                "title": "Towards Prototype-Based Self-Explainable Graph Neural Network",
                "abstract": "Graph Neural Networks (GNNs) have shown great ability in modeling graph-structured data for various domains. However, GNNs are known as black-box models that lack interpretability. Without understanding their inner working, we cannot fully trust them, which largely limits their adoption in high-stake scenarios. Though some initial efforts have been taken to interpret the predictions of GNNs, they mainly focus on providing post-hoc explanations using an additional explainer, which could misrepresent the true inner working mechanism of the target GNN. The works on self-explainable GNNs are rather limited. Therefore, we study a novel problem of learning prototype-based self-explainable GNNs that can simultaneously give accurate predictions and prototype-based explanations on predictions. We design a framework which can learn prototype graphs that capture representative patterns of each class as class-level explanations. The learned prototypes are also used to simultaneously make prediction for for a test instance and provide instance-level explanation. Extensive experiments on real-world and synthetic datasets show the effectiveness of the proposed framework for both prediction accuracy and explanation quality.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152961073",
                        "name": "Enyan Dai"
                    },
                    {
                        "authorId": "2116430057",
                        "name": "Suhang Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "4fac6a8d86261484983729f3c7d466677d4cf359",
                "externalIds": {
                    "DBLP": "conf/iclr/LinCW23",
                    "ArXiv": "2210.00643",
                    "DOI": "10.48550/arXiv.2210.00643",
                    "CorpusId": 252683604
                },
                "corpusId": 252683604,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4fac6a8d86261484983729f3c7d466677d4cf359",
                "title": "Spectral Augmentation for Self-Supervised Learning on Graphs",
                "abstract": "Graph contrastive learning (GCL), as an emerging self-supervised learning technique on graphs, aims to learn representations via instance discrimination. Its performance heavily relies on graph augmentation to reflect invariant patterns that are robust to small perturbations; yet it still remains unclear about what graph invariance GCL should capture. Recent studies mainly perform topology augmentations in a uniformly random manner in the spatial domain, ignoring its influence on the intrinsic structural properties embedded in the spectral domain. In this work, we aim to find a principled way for topology augmentations by exploring the invariance of graphs from the spectral perspective. We develop spectral augmentation which guides topology augmentations by maximizing the spectral change. Extensive experiments on both graph and node classification tasks demonstrate the effectiveness of our method in self-supervised representation learning. The proposed method also brings promising generalization capability in transfer learning, and is equipped with intriguing robustness property under adversarial attacks. Our study sheds light on a general principle for graph topology augmentation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9538465",
                        "name": "Lu Lin"
                    },
                    {
                        "authorId": "7557913",
                        "name": "Jinghui Chen"
                    },
                    {
                        "authorId": "2108883660",
                        "name": "Hongning Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "24b2aed0f130e5278325b5055711de44d247460e",
                "externalIds": {
                    "DBLP": "conf/nips/Fan0MST22",
                    "ArXiv": "2209.14107",
                    "DOI": "10.48550/arXiv.2209.14107",
                    "CorpusId": 252567836
                },
                "corpusId": 252567836,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/24b2aed0f130e5278325b5055711de44d247460e",
                "title": "Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure",
                "abstract": "Most Graph Neural Networks (GNNs) predict the labels of unseen graphs by learning the correlation between the input graphs and labels. However, by presenting a graph classification investigation on the training graphs with severe bias, surprisingly, we discover that GNNs always tend to explore the spurious correlations to make decision, even if the causal correlation always exists. This implies that existing GNNs trained on such biased datasets will suffer from poor generalization capability. By analyzing this problem in a causal view, we find that disentangling and decorrelating the causal and bias latent variables from the biased graphs are both crucial for debiasing. Inspiring by this, we propose a general disentangled GNN framework to learn the causal substructure and bias substructure, respectively. Particularly, we design a parameterized edge mask generator to explicitly split the input graph into causal and bias subgraphs. Then two GNN modules supervised by causal/bias-aware loss functions respectively are trained to encode causal and bias subgraphs into their corresponding representations. With the disentangled representations, we synthesize the counterfactual unbiased training samples to further decorrelate causal and bias variables. Moreover, to better benchmark the severe bias problem, we construct three new graph datasets, which have controllable bias degrees and are easier to visualize and explain. Experimental results well demonstrate that our approach achieves superior generalization performance over existing baselines. Furthermore, owing to the learned edge mask, the proposed model has appealing interpretability and transferability. Code and data are available at: https://github.com/googlebaba/DisC.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48635390",
                        "name": "Shaohua Fan"
                    },
                    {
                        "authorId": "2118449003",
                        "name": "Xiao Wang"
                    },
                    {
                        "authorId": "2186301319",
                        "name": "Yanhu Mo"
                    },
                    {
                        "authorId": "2151458697",
                        "name": "Chuan Shi"
                    },
                    {
                        "authorId": "152226504",
                        "name": "Jian Tang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                ", 2019], PGE-Explainer [Luo et al., 2020], GradCAM [Pope et al.",
                "To this end, we use the BA2Motifs dataset [Luo et al., 2020].",
                "We compare the proposed method with popular post-hoc explanation techniques including the GNN-Explainer Ying et al. [2019], PGE-Explainer Luo et al. [2020], GradCAM Pope et al. [2019], GNN-LRP Schnake et al. [2020], and SubgraphX Yuan et al. [2021]2."
            ],
            "citingPaper": {
                "paperId": "02a6474812c0847def08d838570dcad3ef9ffdbd",
                "externalIds": {
                    "ArXiv": "2209.14402",
                    "CorpusId": 258833737
                },
                "corpusId": 258833737,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/02a6474812c0847def08d838570dcad3ef9ffdbd",
                "title": "L2XGNN: Learning to Explain Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) are a popular class of machine learning models. Inspired by the learning to explain (L2X) paradigm, we propose L2XGNN, a framework for explainable GNNs which provides faithful explanations by design. L2XGNN learns a mechanism for selecting explanatory subgraphs (motifs) which are exclusively used in the GNNs message-passing operations. L2XGNN is able to select, for each input graph, a subgraph with specific properties such as being sparse and connected. Imposing such constraints on the motifs often leads to more interpretable and effective explanations. Experiments on several datasets suggest that L2XGNN achieves the same classification accuracy as baseline methods using the entire input graph while ensuring that only the provided explanations are used to make predictions. Moreover, we show that L2XGNN is able to identify motifs responsible for the graph's properties it is intended to predict.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2163190285",
                        "name": "Giuseppe Serra"
                    },
                    {
                        "authorId": "2780262",
                        "name": "Mathias Niepert"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "1bce45f1d69873f077ec63edda38e21b51b73e8f",
                "externalIds": {
                    "DOI": "10.1155/2022/7807878",
                    "CorpusId": 252529667
                },
                "corpusId": 252529667,
                "publicationVenue": {
                    "id": "6b6df2de-21bc-4137-9859-3fcef46f6a21",
                    "name": "Mobile Information Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Mob Inf Syst"
                    ],
                    "issn": "1574-017X",
                    "url": "https://www.hindawi.com/journals/misy/",
                    "alternate_urls": [
                        "https://www.iospress.nl/journal/mobile-information-systems/",
                        "https://www.iospress.nl/html/1574017x.php",
                        "http://content.iospress.com/journals/mobile-information-systems"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1bce45f1d69873f077ec63edda38e21b51b73e8f",
                "title": "Exploration of Cross-Border Language Planning Using the Graph Neural Network for Internet of Things-Native Data",
                "abstract": "This work aims to study applying the graph neural network (GNN) in cross-border language planning (CBLP). Consequently, following a review of the connotation of GNN, it puts forward the research method for CBLP based on the Internet of Things (IoT)-native data and studies the classification of language texts utilizing different types of GNNs. Firstly, the isomorphic label-embedded graph convolution network (GCN) is proposed. Then, it proposes a scalability-enhanced heterogeneous GCN. Subsequently, the two GCN models are fused, and the research model-heterogeneous InducGCN is proposed. Finally, the model performances are comparatively analyzed. The experimental findings suggest that the classification accuracy of label-embedded GNN is higher than that of other methods, with the highest recognition accuracy of 97.37% on dataset R8. The classification accuracy of the proposed heterogeneous InducGCN fusion model has been improved by 0.09% more than the label-embedded GNN, reaching 97.46%.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2176258581",
                        "name": "Juan Long"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "are different from those reported in the original paper [6].",
                "On the other hand, a more recent method called PGExplainer [6] can learn how to sample subgraphs that highlight the most relevant parts of the input that influence the GNN output.",
                "\u2022 a synthetic dataset, BA-2motifs [6], which contains 1000 graphs divided into two classes according to the motif they contain: either a \u201chouse\u201d or a five-node cycle;"
            ],
            "citingPaper": {
                "paperId": "7731136e82db390f5bf1a978526f70b99b0b3975",
                "externalIds": {
                    "ArXiv": "2209.07926",
                    "DBLP": "journals/corr/abs-2209-07926",
                    "DOI": "10.7557/18.6796",
                    "CorpusId": 252355065
                },
                "corpusId": 252355065,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7731136e82db390f5bf1a978526f70b99b0b3975",
                "title": "Explainability in subgraphs-enhanced Graph Neural Networks",
                "abstract": "Recently, subgraphs-enhanced Graph Neural Networks (SGNNs) have been introduced to enhance the expressive power of Graph Neural Networks (GNNs), which was proved to be not higher than the 1-dimensional Weisfeiler-Leman isomorphism test. The new paradigm suggests using subgraphs extracted from the input graph to improve the model's expressiveness, but the additional complexity exacerbates an already challenging problem in GNNs: explaining their predictions. In this work, we adapt PGExplainer, one of the most recent explainers for GNNs, to SGNNs. The proposed explainer accounts for the contribution of all the different subgraphs and can produce a meaningful explanation that humans can interpret. The experiments that we performed both on real and synthetic datasets show that our framework is successful in explaining the decision process of an SGNN on graph classification tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2185348974",
                        "name": "Michele Guerra"
                    },
                    {
                        "authorId": "115566972",
                        "name": "Indro Spinelli"
                    },
                    {
                        "authorId": "1752983",
                        "name": "Simone Scardapane"
                    },
                    {
                        "authorId": "14553624",
                        "name": "F. Bianchi"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "The existing works can be classified into two categories: instance-level explanations [11, 12, 13] and model-level explanations [14]."
            ],
            "citingPaper": {
                "paperId": "192067b0d238d54480d72d751cbd005e2ad2d2e4",
                "externalIds": {
                    "ArXiv": "2209.07924",
                    "DBLP": "conf/iclr/WangS23",
                    "DOI": "10.48550/arXiv.2209.07924",
                    "CorpusId": 252355289
                },
                "corpusId": 252355289,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/192067b0d238d54480d72d751cbd005e2ad2d2e4",
                "title": "GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks",
                "abstract": "Recently, Graph Neural Networks (GNNs) have significantly advanced the performance of machine learning tasks on graphs. However, this technological breakthrough makes people wonder: how does a GNN make such decisions, and can we trust its prediction with high confidence? When it comes to some critical fields, such as biomedicine, where making wrong decisions can have severe consequences, it is crucial to interpret the inner working mechanisms of GNNs before applying them. In this paper, we propose a model-agnostic model-level explanation method for different GNNs that follow the message passing scheme, GNNInterpreter, to explain the high-level decision-making process of the GNN model. More specifically, GNNInterpreter learns a probabilistic generative graph distribution that produces the most discriminative graph pattern the GNN tries to detect when making a certain prediction by optimizing a novel objective function specifically designed for the model-level explanation for GNNs. Compared to existing works, GNNInterpreter is more flexible and computationally efficient in generating explanation graphs with different types of node and edge features, without introducing another blackbox or requiring manually specified domain-specific rules. In addition, the experimental studies conducted on four different datasets demonstrate that the explanation graphs generated by GNNInterpreter match the desired graph pattern if the model is ideal; otherwise, potential model pitfalls can be revealed by the explanation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144685915",
                        "name": "Xiaoqi Wang"
                    },
                    {
                        "authorId": "2110771216",
                        "name": "Hang Shen"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In recent years, several methods for Explainability in Graph Neural Networks have been proposed, such as XGNN [39] , gnnexplainer [38], PGExplainer [21] , etc.",
                "Perturbation-based methods [9, 21, 27, 34, 38] monitor the changes in predicted values under different input perturbations, thereby learning the importance scores of input features."
            ],
            "citingPaper": {
                "paperId": "1fc85db15b5cfc209cb9a9912d9099f533d13558",
                "externalIds": {
                    "ArXiv": "2209.02902",
                    "DBLP": "journals/corr/abs-2209-02902",
                    "DOI": "10.48550/arXiv.2209.02902",
                    "CorpusId": 252110794
                },
                "corpusId": 252110794,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1fc85db15b5cfc209cb9a9912d9099f533d13558",
                "title": "Defending Against Backdoor Attack on Graph Nerual Network by Explainability",
                "abstract": "Backdoor attack is a powerful attack algorithm to deep learning model. Recently, GNN\u2019s vulnerability to backdoor attack has been proved especially on graph classification task. In this paper, we propose the first backdoor detection and defense method on GNN. Most backdoor attack depends on injecting small but influential trigger to the clean sample. For graph data, current backdoor attack focus on manipulating the graph structure to inject the trigger. We find that there are apparent differences between benign samples and malicious samples in some explanatory evaluation metrics, such as fidelity and infidelity. After identifying the malicious sample, the explainability of the GNN model can help us capture the most significant subgraph which is probably the trigger in a trojan graph. We use various dataset and different attack settings to prove the effectiveness of our defense method. The attack success rate all turns out to decrease considerably.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118935671",
                        "name": "B. Jiang"
                    },
                    {
                        "authorId": "2184415579",
                        "name": "Zhao Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Many other works focus on providing local explanations which are more robust and faithful (Dai et al. 2022; Luo et al. 2020; Vu and Thai 2020)."
            ],
            "citingPaper": {
                "paperId": "1566fedfe843ac88fb36803368fa84bed6db2af3",
                "externalIds": {
                    "DBLP": "conf/aaai/XuanyuanBGML23",
                    "ArXiv": "2208.10609",
                    "DOI": "10.48550/arXiv.2208.10609",
                    "CorpusId": 251741343
                },
                "corpusId": 251741343,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1566fedfe843ac88fb36803368fa84bed6db2af3",
                "title": "Global Concept-Based Interpretability for Graph Neural Networks via Neuron Analysis",
                "abstract": "Graph neural networks (GNNs) are highly effective on a variety of graph-related tasks; however, they lack interpretability and transparency. Current explainability approaches are typically local and treat GNNs as black-boxes. They do not look inside the model, inhibiting human trust in the model and explanations. Motivated by the ability of neurons to detect high-level semantic concepts in vision models, we perform a novel analysis on the behaviour of individual GNN neurons to answer questions about GNN interpretability. We propose a novel approach for producing global explanations for GNNs using neuron-level concepts to enable practitioners to have a high-level view of the model. Specifically, (i) to the best of our knowledge, this is the first work which shows that GNN neurons act as concept detectors and have strong alignment with concepts formulated as logical compositions of node degree and neighbourhood properties; (ii) we quantitatively assess the importance of detected concepts, and identify a trade-off between training duration and neuron-level interpretability; (iii) we demonstrate that our global explainability approach has advantages over the current state-of-the-art -- we can disentangle the explanation into individual interpretable concepts backed by logical descriptions, which reduces potential for bias and improves user-friendliness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2230244815",
                        "name": "Xuanyuan Han"
                    },
                    {
                        "authorId": "2123005765",
                        "name": "Pietro Barbiero"
                    },
                    {
                        "authorId": "1713791843",
                        "name": "Dobrik Georgiev"
                    },
                    {
                        "authorId": "2098834685",
                        "name": "Lucie Charlotte Magister"
                    },
                    {
                        "authorId": "1401225582",
                        "name": "Pietro Li'o"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "These explanations can be given with respect to node attributes Ma \u2208 R, nodes Mn \u2208 R , or edges Me \u2208 RN\u00d7N , depending on specific GNN explainer, such as GNNExplainer [14], PGExplainer [10], and SubgraphX [31].",
                "These explanations can be given with respect to node attributes Ma d\u2208 , nodes \u2208 Mn N , or edges \u2208 \u00d7Me\nN N , depending on specific GNN explainer, such as GNNExplainer14, PGExplainer10, and SubgraphX31.",
                "We incorporate eight GNN explainability methods, including gradient-based: Grad29, GradCAM11, GuidedBP6, Integrated Gradients30; perturbation-based: GNNExplainer14, PGExplainer10, SubgraphX31; and surrogate-based methods: PGMExplainer13.",
                "Still, this faithfulness is relatively weak, only 0.001 better than\nMethod GEA (\u2191) GEF (\u2193) GES (\u2193) GECF (\u2193) GEGF (\u2193)\nRandom 0.148 \u00b1 0.002 0.579 \u00b1 0.007 0.920 \u00b1 0.002 0.763 \u00b1 0.003 0.023 \u00b1 0.002\nGrad 0.193 \u00b1 0.002 0.392 \u00b1 0.006 0.806 \u00b1 0.004 0.159 \u00b1 0.004 0.039 \u00b1 0.003\nGradCAM 0.222 \u00b1 0.002 0.452 \u00b1 0.006 0.263 \u00b1 0.004 0.010 \u00b1 0.001 0.020 \u00b1 0.002\nGuidedBP 0.194 \u00b1 0.001 0.557 \u00b1 0.007 0.432 \u00b1 0.004 0.067 \u00b1 0.002 0.021 \u00b1 0.002\nIG 0.142 \u00b1 0.002 0.545 \u00b1 0.007 0.727 \u00b1 0.005 0.110 \u00b1 0.003 0.021 \u00b1 0.002\nGNNExplainer 0.102 \u00b1 0.003 0.534 \u00b1 0.007 0.431 \u00b1 0.008 0.233 \u00b1 0.006 0.027 \u00b1 0.002\nPGMExplainer 0.133 \u00b1 0.002 0.541 \u00b1 0.007 0.984 \u00b1 0.001 0.791 \u00b1 0.003 0.096 \u00b1 0.004\nPGExplainer 0.194 \u00b1 0.002 0.557 \u00b1 0.007 0.217 \u00b1 0.004 0.009 \u00b1 0.000 0.029 \u00b1 0.002\nSubgraphX 0.324 \u00b1 0.004 0.254 \u00b1 0.006 0.745 \u00b1 0.005 0.241 \u00b1 0.006 0.035 \u00b1 0.003\nDataset Method GEA (\u2191) GEF (\u2193)\nMutag\nRandom 0.044 \u00b1 0.007 0.590 \u00b1 0.031\nGrad 0.022 \u00b1 0.006 0.598 \u00b1 0.030\nGradCAM 0.085 \u00b1 0.012 0.672 \u00b1 0.029\nGuidedBP 0.036 \u00b1 0.007 0.649 \u00b1 0.030\nIntegrated Grad (IG) 0.049 \u00b1 0.010 0.443 \u00b1 0.031\nGNNExplainer 0.031 \u00b1 0.005 0.618 \u00b1 0.030\nPGMExplainer 0.042 \u00b1 0.007 0.503 \u00b1 0.031\nPGExplainer 0.046 \u00b1 0.007 0.504 \u00b1 0.031\nSubgraphX 0.039 \u00b1 0.007 0.611 \u00b1 0.030\nBenzene\nRandom 0.108 \u00b1 0.003 0.513 \u00b1 0.012\nGrad 0.122 \u00b1 0.007 0.262 \u00b1 0.011\nGradCAM 0.291 \u00b1 0.007 0.551 \u00b1 0.012\nGuidedBP 0.205 \u00b1 0.007 0.438 \u00b1 0.012\nIntegrated Grad (IG) 0.044 \u00b1 0.003 0.182 \u00b1 0.010\nGNNExplainer 0.129 \u00b1 0.005 0.444 \u00b1 0.012\nPGMExplainer 0.154 \u00b1 0.006 0.433 \u00b1 0.012\nPGExplainer 0.169 \u00b1 0.007 0.375 \u00b1 0.012\nSubgraphX 0.371 \u00b1 0.009 0.513 \u00b1 0.012\nFl-Carbonyl\nRandom 0.087 \u00b1 0.007 0.440 \u00b1 0.26\nGrad 0.132 \u00b1 0.010 0.210 \u00b1 0.021\nGradCAM 0.005 \u00b1 0.007 0.500 \u00b1 0.026\nGuidedBP 0.089 \u00b1 0.010 0.315 \u00b1 0.024\nIntegrated Grad (IG) 0.091 \u00b1 0.007 0.174 \u00b1 0.019\nGNNExplainer 0.094 \u00b1 0.009 0.423 \u00b1 0.026\nPGMExplainer 0.078 \u00b1 0.008 0.426 \u00b1 0.026\nPGExplainer 0.079 \u00b1 0.009 0.372 \u00b1 0.025\nSubgraphX 0.008 \u00b1 0.002 0.466 \u00b1 0.026\n9Scientific Data | (2023) 10:144 | https://doi.org/10.1038/s41597-023-01974-x\nrandom explanation.",
                "PGExplainer generates the least unstable explanations\u201335.35% less unstable explanations than the average instability across other GNN explainers.",
                "We incorporate eight GNN explainability methods, including gradient-based: Grad [29], GradCAM [11], GuidedBP [6], Integrated Gradients [30]; perturbation-based: GNNExplainer [14], PGExplainer [10], SubgraphX [31]; and surrogate-based methods: PGMExplainer [13]."
            ],
            "citingPaper": {
                "paperId": "f50c92916832fba9e0e56fa781b0a03b3e07f3d4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-09339",
                    "PubMedCentral": "10024712",
                    "ArXiv": "2208.09339",
                    "DOI": "10.1038/s41597-023-01974-x",
                    "CorpusId": 251710449,
                    "PubMed": "36934095"
                },
                "corpusId": 251710449,
                "publicationVenue": {
                    "id": "62924b2a-8fb8-4b93-92c6-735516b49af0",
                    "name": "Scientific Data",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Data"
                    ],
                    "issn": "2052-4463",
                    "url": "http://www.nature.com/sdata/"
                },
                "url": "https://www.semanticscholar.org/paper/f50c92916832fba9e0e56fa781b0a03b3e07f3d4",
                "title": "Evaluating explainability for graph neural networks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40228633",
                        "name": "Chirag Agarwal"
                    },
                    {
                        "authorId": "2149932294",
                        "name": "Owen Queen"
                    },
                    {
                        "authorId": "1892673",
                        "name": "Himabindu Lakkaraju"
                    },
                    {
                        "authorId": "2095762",
                        "name": "M. Zitnik"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "(Ying et al. 2019; Luo et al. 2020; Yuan et al. 2021) propose to adopt an explanation method to figure out the causal relationship between the model\u2019s inputs and outputs."
            ],
            "citingPaper": {
                "paperId": "8a4c8b331abc0d5522fc5262595ff7d597c8a93b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-08584",
                    "ArXiv": "2208.08584",
                    "DOI": "10.48550/arXiv.2208.08584",
                    "CorpusId": 251643624
                },
                "corpusId": 251643624,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/8a4c8b331abc0d5522fc5262595ff7d597c8a93b",
                "title": "Robust Causal Graph Representation Learning against Confounding Effects",
                "abstract": "The prevailing graph neural network models have achieved significant progress in graph representation learning. However, in this paper, we uncover an ever-overlooked phenomenon: the pre-trained graph representation learning model tested with full graphs underperforms the model tested with well-pruned graphs. This observation reveals that there exist confounders in graphs, which may interfere with the model learning semantic information, and current graph representation learning methods have not eliminated their influence. To tackle this issue, we propose Robust Causal Graph Representation Learning (RCGRL) to learn robust graph representations against confounding effects. RCGRL introduces an active approach to generate instrumental variables under unconditional moment restrictions, which empowers the graph representation learning model to eliminate confounders, thereby capturing discriminative information that is causally related to downstream predictions. We offer theorems and proofs to guarantee the theoretical effectiveness of the proposed approach. Empirically, we conduct extensive experiments on a synthetic dataset and multiple benchmark datasets. Experimental results demonstrate the effectiveness and generalization ability of RCGRL. Our codes are available at https://github.com/hang53/RCGRL.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108469098",
                        "name": "Hang Gao"
                    },
                    {
                        "authorId": "2118506408",
                        "name": "Jiangmeng Li"
                    },
                    {
                        "authorId": "2059455684",
                        "name": "Wenwen Qiang"
                    },
                    {
                        "authorId": "2114860376",
                        "name": "Lingyu Si"
                    },
                    {
                        "authorId": "2113743531",
                        "name": "Bing Xu"
                    },
                    {
                        "authorId": "2153619515",
                        "name": "Changwen Zheng"
                    },
                    {
                        "authorId": "2323566",
                        "name": "Fuchun Sun"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Explanation for Graph Neural Network In this task, we follow the setting in GNNExplainer [39] and PGExplainer [17] and construct four kinds of node classification datasets.",
                "For the explanation task, we follow the quantitative evaluation settings in GNNExplainer [39] and PGExplainer [17]."
            ],
            "citingPaper": {
                "paperId": "cd677e9a70cc4a9f67ca956b171bd71d4d6d8831",
                "externalIds": {
                    "DBLP": "conf/kdd/ZhangGPH22",
                    "DOI": "10.1145/3534678.3539415",
                    "CorpusId": 251518439
                },
                "corpusId": 251518439,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/cd677e9a70cc4a9f67ca956b171bd71d4d6d8831",
                "title": "Improving Social Network Embedding via New Second-Order Continuous Graph Neural Networks",
                "abstract": "Graph neural networks (GNN) are powerful tools in many web research problems. However, existing GNNs are not fully suitable for many real-world web applications. For example, over-smoothing may affect personalized recommendations and the lack of an explanation for the GNN prediction hind the understanding of many business scenarios. To address these problems, in this paper, we propose a new second-order continuous GNN which naturally avoids over-smoothing and enjoys better interpretability. There is some research interest in continuous graph neural networks inspired by the recent success of neural ordinary differential equations (ODEs). However, there are some remaining problems w.r.t. the prevailing first-order continuous GNN frameworks. Firstly, augmenting node features is an essential, however heuristic step for the numerical stability of current frameworks; secondly, first-order methods characterize a diffusion process, in which the over-smoothing effect w.r.t. node representations are intrinsic; and thirdly, there are some difficulties to integrate the topology of graphs into the ODEs. Therefore, we propose a framework employing second-order graph neural networks, which usually learn a less stiff transformation than the first-order counterpart. Our method can also be viewed as a coupled first-order model, which is easy to implement. We propose a semi-model-agnostic method based on our model to enhance the prediction explanation using high-order information. We construct an analog between continuous GNNs and some famous partial differential equations and discuss some properties of the first and second-order models. Extensive experiments demonstrate the effectiveness of our proposed method, and the results outperform related baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49889644",
                        "name": "Yanfu Zhang"
                    },
                    {
                        "authorId": "9355577",
                        "name": "Shangqian Gao"
                    },
                    {
                        "authorId": "2143385183",
                        "name": "Jian Pei"
                    },
                    {
                        "authorId": "145114933",
                        "name": "Heng Huang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "5c9bbfa5e8fa084595c14b503cf9d1410ddfb22c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-03075",
                    "ArXiv": "2208.03075",
                    "DOI": "10.48550/arXiv.2208.03075",
                    "CorpusId": 251371662
                },
                "corpusId": 251371662,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5c9bbfa5e8fa084595c14b503cf9d1410ddfb22c",
                "title": "PGX: A Multi-level GNN Explanation Framework Based on Separate Knowledge Distillation Processes",
                "abstract": "Graph Neural Networks (GNNs) are widely adopted in advanced AI systems due to their capability of representation learning on graph data. Even though GNN explanation is crucial to increase user trust in the systems, it is challenging due to the complexity of GNN execution. Lately, many works have been proposed to address some of the issues in GNN explanation. However, they lack generalization capability or suffer from computational burden when the size of graphs is enormous. To address these challenges, we propose a multi-level GNN explanation framework based on an observation that GNN is a multimodal learning process of multiple components in graph data. The complexity of the original problem is relaxed by breaking into multiple sub-parts represented as a hierarchical structure. The top-level explanation aims at specifying the contribution of each component to the model execution and predictions, while fine-grained levels focus on feature attribution and graph structure attribution analysis based on knowledge distillation. Student models are trained in standalone modes and are responsible for capturing different teacher behaviors, later used for particular component interpretation. Besides, we also aim for personalized explanations as the framework can generate different results based on user preferences. Finally, extensive experiments demonstrate the effectiveness and fidelity of our proposed approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "41022579",
                        "name": "Tien-Cuong Bui"
                    },
                    {
                        "authorId": "2108718185",
                        "name": "Wen-Syan Li"
                    },
                    {
                        "authorId": "2237996",
                        "name": "S. Cha"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "The majority of existing methods provide a factual explanation in the form of a subgraph of the original graph that is deemed to be important for the prediction [3,9,22,27,30,36,46,51,54]."
            ],
            "citingPaper": {
                "paperId": "edbdec562cb1525eafe148b249b75480a284dc59",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-04222",
                    "ArXiv": "2208.04222",
                    "DOI": "10.48550/arXiv.2208.04222",
                    "CorpusId": 251402395
                },
                "corpusId": 251402395,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/edbdec562cb1525eafe148b249b75480a284dc59",
                "title": "GREASE: Generate Factual and Counterfactual Explanations for GNN-based Recommendations",
                "abstract": "Recently, graph neural networks (GNNs) have been widely used to develop successful recommender systems. Although powerful, it is very difficult for a GNN-based recommender system to attach tangible explanations of why a specific item ends up in the list of suggestions for a given user. Indeed, explaining GNN-based recommendations is unique, and existing GNN explanation methods are inappropriate for two reasons. First, traditional GNN explanation methods are designed for node, edge, or graph classification tasks rather than ranking, as in recommender systems. Second, standard machine learning explanations are usually intended to support skilled decision-makers. Instead, recommendations are designed for any end-user, and thus their explanations should be provided in user-understandable ways. In this work, we propose GREASE, a novel method for explaining the suggestions provided by any black-box GNN-based recommender system. Specifically, GREASE first trains a surrogate model on a target user-item pair and its $l$-hop neighborhood. Then, it generates both factual and counterfactual explanations by finding optimal adjacency matrix perturbations to capture the sufficient and necessary conditions for an item to be recommended, respectively. Experimental results conducted on real-world datasets demonstrate that GREASE can generate concise and effective explanations for popular GNN-based recommender models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2157281262",
                        "name": "Ziheng Chen"
                    },
                    {
                        "authorId": "144925193",
                        "name": "F. Silvestri"
                    },
                    {
                        "authorId": "2144547216",
                        "name": "Jia Wang"
                    },
                    {
                        "authorId": "1591136873",
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "authorId": null,
                        "name": "Zhenhua Huang"
                    },
                    {
                        "authorId": "34609799",
                        "name": "H. Ahn"
                    },
                    {
                        "authorId": "2651748",
                        "name": "Gabriele Tolomei"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Datasets We perform the experiments on the same set of datasets as GNNExplainer [12], as subsequent research established them as benchmarks [13, 14, 21].",
                "[13] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang.",
                "Notice that we do not focus on other post-hoc explainability methods, such as GNNExplainer [12], PGExplainer [13] or PGMExplainer [14], as to the best of our knowledge GCExplainer is the only explainability method providing global concept-based explanations for GNNs.",
                "In an attempt to alleviate this issue, the Parameterised Graph Explainer (PGExplainer, [13]) and the Probabilistic Graphical Model Explainer (PGM-Explainer, [14]) parametrize the process of generating explanations using deep neural networks to provide multi-instance explanations."
            ],
            "citingPaper": {
                "paperId": "1b5f8d3693ce9dfc07e7606039322b7e7b60270b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-13586",
                    "ArXiv": "2207.13586",
                    "DOI": "10.48550/arXiv.2207.13586",
                    "CorpusId": 251104693
                },
                "corpusId": 251104693,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1b5f8d3693ce9dfc07e7606039322b7e7b60270b",
                "title": "Encoding Concepts in Graph Neural Networks",
                "abstract": "The opaque reasoning of Graph Neural Networks induces a lack of human trust. Existing graph network explainers attempt to address this issue by providing post-hoc explanations, however, they fail to make the model itself more interpretable. To fill this gap, we introduce the Concept Encoder Module, the first differentiable concept-discovery approach for graph networks. The proposed approach makes graph networks explainable by design by first discovering graph concepts and then using these to solve the task. Our results demonstrate that this approach allows graph networks to: (i) attain model accuracy comparable with their equivalent vanilla versions, (ii) discover meaningful concepts that achieve high concept completeness and purity scores, (iii) provide high-quality concept-based logic explanations for their prediction, and (iv) support effective interventions at test time: these can increase human trust as well as significantly improve model performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2098834685",
                        "name": "Lucie Charlotte Magister"
                    },
                    {
                        "authorId": "2123005765",
                        "name": "Pietro Barbiero"
                    },
                    {
                        "authorId": "1641643092",
                        "name": "Dmitry Kazhdan"
                    },
                    {
                        "authorId": "1752951302",
                        "name": "F. Siciliano"
                    },
                    {
                        "authorId": "79277428",
                        "name": "Gabriele Ciravegna"
                    },
                    {
                        "authorId": "144925193",
                        "name": "F. Silvestri"
                    },
                    {
                        "authorId": "2075355155",
                        "name": "Pietro Lio'"
                    },
                    {
                        "authorId": "1708741",
                        "name": "M. Jamnik"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "For instance, the PGExplainer focuses on the explanation of the complete graph structures and provides a global understanding of predictions made by GNNs.",
                ", nodes, node features, and edges) to the true important truth [16,33,39] (see Eq.",
                "[16] proposed a model-agnostic method of explainable GNNs called PGExplainer.",
                "PGExplainer provides explanations for GNNs by generating a probabilistic graph.",
                "Then a new graph is generated that contains only the structure necessary for the decision making by GNNs.\nSimilar to the PGM-Explainer analysing the explained features from conditional probabilities, Luo et al. [16] proposed a model-agnostic method of explainable GNNs called PGExplainer.",
                "Qualitative analyses have been widely used in recent research, such as GNNExplainer [33], PGExplainer [16], GAN-GNNExplainer [14], etc.",
                "A robust interpretation method can provide similar explanations despite the presence of such attacks [16,39]."
            ],
            "citingPaper": {
                "paperId": "8f13afe3ce7391873ce92807fe3938851bafd079",
                "externalIds": {
                    "ArXiv": "2207.12599",
                    "DBLP": "journals/corr/abs-2207-12599",
                    "DOI": "10.48550/arXiv.2207.12599",
                    "CorpusId": 251067111
                },
                "corpusId": 251067111,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8f13afe3ce7391873ce92807fe3938851bafd079",
                "title": "A Survey of Explainable Graph Neural Networks: Taxonomy and Evaluation Metrics",
                "abstract": "Graph neural networks (GNNs) have demonstrated a significant boost in prediction performance on graph data. At the same time, the predictions made by these models are often hard to interpret. In that regard, many efforts have been made to explain the prediction mechanisms of these models from perspectives such as GNNExplainer, XGNN and PGExplainer. Although such works present systematic frameworks to interpret GNNs, a holistic review for explainable GNNs is unavailable. In this survey, we present a comprehensive review of explainability techniques developed for GNNs. We focus on explainable graph neural networks and categorize them based on the use of explainable methods. We further provide the common performance metrics for GNNs explanations and point out several future research directions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2179302425",
                        "name": "Yiqiao Li"
                    },
                    {
                        "authorId": "51239629",
                        "name": "Jianlong Zhou"
                    },
                    {
                        "authorId": "3455244",
                        "name": "Sunny Verma"
                    },
                    {
                        "authorId": "145093625",
                        "name": "Fang Chen"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "221ee346c96d60895eb0d446870aedcc5b0c9e81",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-12748",
                    "ArXiv": "2207.12748",
                    "DOI": "10.48550/arXiv.2207.12748",
                    "CorpusId": 251066831
                },
                "corpusId": 251066831,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/221ee346c96d60895eb0d446870aedcc5b0c9e81",
                "title": "ScoreCAM GNN: une explication optimale des r\u00e9seaux profonds sur graphes",
                "abstract": "The explainability of deep networks is becoming a central issue in the deep learning community. It is the same for learning on graphs, a data structure present in many real world problems. In this paper, we propose a method that is more optimal, lighter, consistent and better exploits the topology of the evaluated graph than the state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2179189861",
                        "name": "Adrien Raison"
                    },
                    {
                        "authorId": "2066193155",
                        "name": "Pascal Bourdon"
                    },
                    {
                        "authorId": "48552959",
                        "name": "David Helbert"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Consequently, an increasing number of works are focussing on explaining [11, 12, 13, 14] the decisions of",
                "In terms of methodologies, several techniques based on input perturbations [11, 12, 13], input gradients[36, 37], causal techniques [34, 38, 33] as well as utilizing simpler surrogate models [14] have been explored.",
                "Explanations usually include the importance scores for nodes/edges in a subgraph (or node\u2019s neighborhood in case of node-level task) and the node features [11, 12, 13]."
            ],
            "citingPaper": {
                "paperId": "af10e2205b6162ac4b76e01ba140056c9a43a32b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-10896",
                    "ArXiv": "2207.10896",
                    "DOI": "10.48550/arXiv.2207.10896",
                    "CorpusId": 251018344
                },
                "corpusId": 251018344,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/af10e2205b6162ac4b76e01ba140056c9a43a32b",
                "title": "Privacy and Transparency in Graph Machine Learning: A Unified Perspective",
                "abstract": "Graph Machine Learning (GraphML), whereby classical machine learning is generalized to irregular graph domains, has enjoyed a recent renaissance, leading to a dizzying array of models and their applications in several domains. With its growing applicability to sensitive domains and regulations by governmental agencies for trustworthy AI systems, researchers have started looking into the issues of transparency and privacy of graph learning. However, these topics have been mainly investigated independently. In this position paper, we provide a unified perspective on the interplay of privacy and transparency in GraphML. In particular, we describe the challenges and possible research directions for a formal investigation of privacy-transparency tradeoffs in GraphML.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35070805",
                        "name": "Megha Khosla"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "07cec14c0f9d0e05caf770802b6d7346085f1449",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-11175",
                    "ArXiv": "2207.11175",
                    "DOI": "10.48550/arXiv.2207.11175",
                    "CorpusId": 251018570
                },
                "corpusId": 251018570,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/07cec14c0f9d0e05caf770802b6d7346085f1449",
                "title": "Explaining Dynamic Graph Neural Networks via Relevance Back-propagation",
                "abstract": "Graph Neural Networks (GNNs) have shown remarkable effectiveness in capturing abundant information in graph-structured data. However, the black-box nature of GNNs hinders users from understanding and trusting the models, thus leading to difficulties in their applications. While recent years witness the prosperity of the studies on explaining GNNs, most of them focus on static graphs, leaving the explanation of dynamic GNNs nearly unexplored. It is challenging to explain dynamic GNNs, due to their unique characteristic of time-varying graph structures. Directly using existing models designed for static graphs on dynamic graphs is not feasible because they ignore temporal dependencies among the snapshots. In this work, we propose DGExplainer to provide reliable explanation on dynamic GNNs. DGExplainer redistributes the output activation score of a dynamic GNN to the relevances of the neurons of its previous layer, which iterates until the relevance scores of the input neuron are obtained. We conduct quantitative and qualitative experiments on real-world datasets to demonstrate the effectiveness of the proposed framework for identifying important nodes for link prediction and node regression for dynamic GNNs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "95102259",
                        "name": "Jiaxuan Xie"
                    },
                    {
                        "authorId": "9720172",
                        "name": "Yezi Liu"
                    },
                    {
                        "authorId": "1798830",
                        "name": "Yanning Shen"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Another popular perspective is from sensitivity analysis, which perturbs minor component and evaluate its influence on the global level [32, 17]."
            ],
            "citingPaper": {
                "paperId": "f5daa29099727d62b3916da1f00bf72b2bfbeffc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-12931",
                    "ArXiv": "2207.12931",
                    "DOI": "10.48550/arXiv.2207.12931",
                    "CorpusId": 251067107
                },
                "corpusId": 251067107,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f5daa29099727d62b3916da1f00bf72b2bfbeffc",
                "title": "Demystifying Graph Convolution with a Simple Concatenation",
                "abstract": "Graph convolution (GConv) is a widely used technique that has been demonstrated to be extremely effective for graph learning applications, most notably node catego-rization. On the other hand, many GConv-based models do not quantify the effect of graph topology and node features on performance, and are even surpassed by some models that do not consider graph structure or node properties. We quantify the information overlap between graph topology, node features, and labels in order to determine graph convolution\u2019s representation power in the node classi\ufb01cation task. In this work, we \ufb01rst determine the linear separability of graph convoluted features using analysis of variance. Mutual information is used to acquire a bet- ter understanding of the possible non-linear relationship between graph topology, node features, and labels. Our theoretical analysis demonstrates that a simple and ef\ufb01cient graph operation that concatenates only graph topology and node prop-erties consistently outperforms conventional graph convolution, especially in the heterophily case. Extensive empirical research utilizing a synthetic dataset and real-world benchmarks demonstrates that graph concatenation is a simple but more \ufb02exible alternative to graph convolution.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111361041",
                        "name": "Zhiqian Chen"
                    },
                    {
                        "authorId": "2174010477",
                        "name": "Zonghan Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "PGExplainer [24] adopts a deep neural network to parameterize the generation process of explanations enabling the explanation of multiple instances collectively."
            ],
            "citingPaper": {
                "paperId": "f8c25a4728b4d39ff8475358c36bc3a8c6944320",
                "externalIds": {
                    "DBLP": "conf/ijcnn/LiYPS22",
                    "DOI": "10.1109/IJCNN55064.2022.9892241",
                    "CorpusId": 252625108
                },
                "corpusId": 252625108,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/f8c25a4728b4d39ff8475358c36bc3a8c6944320",
                "title": "CoGNet: Cooperative Graph Neural Networks",
                "abstract": "Graph representation learning has received increasing attention in recent years for many real-world applications. A major challenge in graph representation learning is the lack of labeled data. To address this challenge, Graph Neural Networks (GNNs) use message passing frameworks to combine information from unlabeled data with labeled data. However, the use of unlabeled data under the message passing framework is indirect in the training process where unlabeled data does not supervise the training process. To fully exploit the potential of unlabeled data, we propose a novel dual-view cooperative training framework for graph data where unlabeled data is involved in the training process for supervision. Specifically, we regard different views as the reasoning processes of two GNN models with which the models make predictions, integrating the understanding of different models on the underlying graph. To exchange information between models, we design a pseudo-label-based approach, where the two models mutually provide pseudo labels to each other iteratively. Moreover, to ensure the quality of pseudo labels, we propose an entropy-based pseudo-labels selection procedure and we adopt GNNExplainer to visualize different views in our framework. Our comprehensive experimental evaluation shows that our methods can boost the performance of state-of-the-art models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2159314356",
                        "name": "Peibo Li"
                    },
                    {
                        "authorId": "2108586036",
                        "name": "Yixing Yang"
                    },
                    {
                        "authorId": "1783801",
                        "name": "M. Pagnucco"
                    },
                    {
                        "authorId": "2157995570",
                        "name": "Yang Song"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "b2186c86b09b0891c19650630037dc5c7ecdf601",
                "externalIds": {
                    "DBLP": "conf/ijcai/Veyrin-ForrerKD22",
                    "DOI": "10.24963/ijcai.2022/105",
                    "CorpusId": 250633808
                },
                "corpusId": 250633808,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/b2186c86b09b0891c19650630037dc5c7ecdf601",
                "title": "What Does My GNN Really Capture? On Exploring Internal GNN Representations",
                "abstract": "Graph Neural Networks (GNNs) are very efficient at classifying graphs but their internal functioning is opaque which limits their field of application. Existing methods to explain GNN focus on disclosing the relationships between input graphs and model decision. In this article, we propose a method that goes further and isolates the internal features, hidden in the network layers, that are automatically identified by the GNN and used in the decision process. We show that this method makes possible to know the parts of the input graphs used by GNN with much less bias that SOTA methods and thus to bring confidence in the decision process.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1452681405",
                        "name": "Luca Veyrin-Forrer"
                    },
                    {
                        "authorId": "2158653859",
                        "name": "Ataollah Kamal"
                    },
                    {
                        "authorId": "1762557",
                        "name": "S. Duffner"
                    },
                    {
                        "authorId": "2331463",
                        "name": "M. Plantevit"
                    },
                    {
                        "authorId": "1763302",
                        "name": "C. Robardet"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "A general paradigm to generate explanations for GNNs is to find an explanation graph G\u2032 that has the maximum agreement with the label distribution on the original graph G = (V,E,W ), where G\u2032 can be a subgraph [39] or other variations of G [24, 40].",
                "Although several methods have been proposed for GNN explanation [24, 36, 39], most of them focus on node-level prediction tasks and will produce a unique explanation for each subject when applied to graph-level tasks."
            ],
            "citingPaper": {
                "paperId": "a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-00813",
                    "ArXiv": "2207.00813",
                    "DOI": "10.48550/arXiv.2207.00813",
                    "CorpusId": 250264514
                },
                "corpusId": 250264514,
                "publicationVenue": {
                    "id": "61a709e3-2060-423c-8de5-ffd3885aa31c",
                    "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                    "type": "conference",
                    "alternate_names": [
                        "Medical Image Computing and Computer-Assisted Intervention",
                        "MICCAI",
                        "Med Image Comput Comput Interv",
                        "Int Conf Med Image Comput Comput Interv"
                    ],
                    "url": "http://www.miccai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a4240ed69c1ef93df5f5a9d7a5eea8cb45b1ffe7",
                "title": "Interpretable Graph Neural Networks for Connectome-Based Brain Disorder Analysis",
                "abstract": "Human brains lie at the core of complex neurobiological systems, where the neurons, circuits, and subsystems interact in enigmatic ways. Understanding the structural and functional mechanisms of the brain has long been an intriguing pursuit for neuroscience research and clinical disorder therapy. Mapping the connections of the human brain as a network is one of the most pervasive paradigms in neuroscience. Graph Neural Networks (GNNs) have recently emerged as a potential method for modeling complex network data. Deep models, on the other hand, have low interpretability, which prevents their usage in decision-critical contexts like healthcare. To bridge this gap, we propose an interpretable framework to analyze disorder-specific Regions of Interest (ROIs) and prominent connections. The proposed framework consists of two modules: a brain-network-oriented backbone model for disease prediction and a globally shared explanation generator that highlights disorder-specific biomarkers including salient ROIs and important connections. We conduct experiments on three real-world datasets of brain disorders. The results verify that our framework can obtain outstanding performance and also identify meaningful biomarkers. All code for this work is available at https://github.com/HennyJie/IBGNN.git.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112821580",
                        "name": "Hejie Cui"
                    },
                    {
                        "authorId": "2054963003",
                        "name": "Wei Dai"
                    },
                    {
                        "authorId": "2653121",
                        "name": "Yanqiao Zhu"
                    },
                    {
                        "authorId": "2108535977",
                        "name": "Xiaoxiao Li"
                    },
                    {
                        "authorId": "2148919780",
                        "name": "Lifang He"
                    },
                    {
                        "authorId": "1390553618",
                        "name": "Carl Yang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "The perturbation-based approaches (6; 38; 20; 41; 27) learns the important features and structural information by observing the predictive power of the model when noise is added to the input."
            ],
            "citingPaper": {
                "paperId": "0bff78ef88c6f1bbbf2fb77de4439f7d2168d8e9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-13983",
                    "ArXiv": "2206.13983",
                    "DOI": "10.48550/arXiv.2206.13983",
                    "CorpusId": 250089361
                },
                "corpusId": 250089361,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0bff78ef88c6f1bbbf2fb77de4439f7d2168d8e9",
                "title": "BAGEL: A Benchmark for Assessing Graph Neural Network Explanations",
                "abstract": "The problem of interpreting the decisions of machine learning is a well-researched and important. We are interested in a specific type of machine learning model that deals with graph data called graph neural networks. Evaluating interpretability approaches for graph neural networks (GNN) specifically are known to be challenging due to the lack of a commonly accepted benchmark. Given a GNN model, several interpretability approaches exist to explain GNN models with diverse (sometimes conflicting) evaluation methodologies. In this paper, we propose a benchmark for evaluating the explainability approaches for GNNs called Bagel. In Bagel, we firstly propose four diverse GNN explanation evaluation regimes -- 1) faithfulness, 2) sparsity, 3) correctness. and 4) plausibility. We reconcile multiple evaluation metrics in the existing literature and cover diverse notions for a holistic evaluation. Our graph datasets range from citation networks, document graphs, to graphs from molecules and proteins. We conduct an extensive empirical study on four GNN models and nine post-hoc explanation approaches for node and graph classification tasks. We open both the benchmarks and reference implementations and make them available at https://github.com/Mandeep-Rathee/Bagel-benchmark.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50447319",
                        "name": "M. Rathee"
                    },
                    {
                        "authorId": "143923185",
                        "name": "Thorben Funke"
                    },
                    {
                        "authorId": "39775488",
                        "name": "Avishek Anand"
                    },
                    {
                        "authorId": "35070805",
                        "name": "Megha Khosla"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [],
            "contexts": [
                "With the trained graph models, we quantitatively and qualitatively compare our FlowX with eight baselines, including GradCAM [34], DeepLIFT [43], GNNExplainer [27], PGExplainer [28], PGMExplainer [31], SubgraphX [29], GNNGI [37], GNN-LRP [37].",
                "Second, several existing methods, such as GNNExplainer [27], PGExplainer [28], and GraphMask [36], explain GNNs by studying the importance of different graph edges.",
                "Thus, different methods have been proposed to explain the predictions of GNNs, such as GraphLime [30], GNNExplainer [27], PGExplainer [28], PGMExplainer [31], SubgraphX [29], TAGE [32], XGNN [26], and GraphSVX [33].",
                "Recently, several techniques have been proposed to explain GNNs, such as XGNN [26], GNNExplainer [27], PGExplainer [28], and SubgraphX [29], etc."
            ],
            "citingPaper": {
                "paperId": "661986252dc2457a3ce1ed8a208ddf1dd6aa1ece",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-12987",
                    "ArXiv": "2206.12987",
                    "DOI": "10.48550/arXiv.2206.12987",
                    "CorpusId": 250072908
                },
                "corpusId": 250072908,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/661986252dc2457a3ce1ed8a208ddf1dd6aa1ece",
                "title": "FlowX: Towards Explainable Graph Neural Networks via Message Flows",
                "abstract": "We investigate the explainability of graph neural networks (GNNs) as a step towards elucidating their working mechanisms. While most current methods focus on explaining graph nodes, edges, or features, we argue that, as the inherent functional mechanism of GNNs, message flows are more natural for performing explainability. To this end, we propose a novel method here, known as FlowX, to explain GNNs by identifying important message flows. To quantify the importance of flows, we propose to follow the philosophy of Shapley values from cooperative game theory. To tackle the complexity of computing all coalitions' marginal contributions, we propose an approximation scheme to compute Shapley-like values as initial assessments of further redistribution training. We then propose a learning algorithm to train flow scores and improve explainability. Experimental studies on both synthetic and real-world datasets demonstrate that our proposed FlowX leads to improved explainability of GNNs. The code is available at https://github.com/divelab/DIG.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1914700964",
                        "name": "Shurui Gui"
                    },
                    {
                        "authorId": "1498527026",
                        "name": "Hao Yuan"
                    },
                    {
                        "authorId": "2146041754",
                        "name": "Jie Wang"
                    },
                    {
                        "authorId": "40522277",
                        "name": "Qicheng Lao"
                    },
                    {
                        "authorId": "2154305557",
                        "name": "Kang Li"
                    },
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "For GNN Explainer and PGExplainer, we modified their objective function in a similar way as the attention-based explanation.",
                "(3) Faithfulness Gap: how to obtain bias (fairness) explanations that are faithful to the GNN prediction? To ensure the obtained explanations reflect the true reasoning results based on the given GNN model, most existing works on the instance-level GNN explanation obtain explanations that encode as much critical information as possible for a given GNN prediction [22, 30, 39].",
                "prediction of a node based on its computation graph [22, 30, 39].",
                "The adopted existing GNN explanation approaches for adaptation include the attention-based GNN explanation [29], the gradient-based GNN explanation [29], and two state-of-the-art GNN explanation approaches (GNN Explainer [39] and PGExplainer [22]).",
                "\u2022 Existing GNN explanation models (e.g., GNN Explainer and PGExplainer) do not show any superior performance over other straightforward GNN explanation approaches such as Att and Grad.",
                "For the training of PGExplainer, the learning rate is set as 0.003.",
                "The basic rationale is that if small perturbations lead to dramatic changes in the GNN prediction, then what has been perturbed is regarded as critical for the GNN prediction [22, 27, 32, 39, 42].",
                "\u2022 PGExplainer.",
                "For PGExplainer, we adopt the implementations of [22].",
                "For all adopted GNN explanation baselines (i.e., Att, Grad, GNN Explainer, and PGExplainer), we also adopt their released implementations for a fair comparison.",
                "To evaluate how well the proposed framework can be generalized to different explanation backbones, we adopt GNN Explainer [39] and PGExplainer [22] as two backbones of explainers for evaluation."
            ],
            "citingPaper": {
                "paperId": "19e83a60ed705144729d312c854cd91086303243",
                "externalIds": {
                    "ArXiv": "2206.12104",
                    "DBLP": "journals/corr/abs-2206-12104",
                    "DOI": "10.1145/3534678.3539319",
                    "CorpusId": 250048541
                },
                "corpusId": 250048541,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/19e83a60ed705144729d312c854cd91086303243",
                "title": "On Structural Explanation of Bias in Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) have shown satisfying performance in various graph analytical problems. Hence, they have become the de facto solution in a variety of decision-making scenarios. However, GNNs could yield biased results against certain demographic subgroups. Some recent works have empirically shown that the biased structure of the input network is a significant source of bias for GNNs. Nevertheless, no studies have systematically scrutinized which part of the input network structure leads to biased predictions for any given node. The low transparency on how the structure of the input network influences the bias in GNN outcome largely limits the safe adoption of GNNs in various decision-critical scenarios. In this paper, we study a novel research problem of structural explanation of bias in GNNs. Specifically, we propose a novel post-hoc explanation framework to identify two edge sets that can maximally account for the exhibited bias and maximally contribute to the fairness level of the GNN prediction for any given node, respectively. Such explanations not only provide a comprehensive understanding of bias/fairness of GNN predictions but also have practical significance in building an effective yet fair GNN model. Extensive experiments on real-world datasets validate the effectiveness of the proposed framework towards delivering effective structural explanations for the bias of GNNs. Open-source code can be found at https://github.com/yushundong/REFEREE.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "123918726",
                        "name": "Yushun Dong"
                    },
                    {
                        "authorId": "2117075272",
                        "name": "Song Wang"
                    },
                    {
                        "authorId": "2153607948",
                        "name": "Yu Wang"
                    },
                    {
                        "authorId": "12524628",
                        "name": "Tyler Derr"
                    },
                    {
                        "authorId": "1737121128",
                        "name": "Jundong Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "c559dc62c0d64295f9c0dfb5f322b3f30ddf44eb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-09677",
                    "ArXiv": "2206.09677",
                    "DOI": "10.48550/arXiv.2206.09677",
                    "CorpusId": 249889598
                },
                "corpusId": 249889598,
                "publicationVenue": {
                    "id": "50534c12-f4ba-4c64-806b-01647d1baacf",
                    "name": "LOG IN",
                    "type": "journal",
                    "alternate_names": [
                        "Log in",
                        "Log",
                        "LOG"
                    ],
                    "issn": "0720-8642",
                    "alternate_issns": [
                        "1547-4690",
                        "0024-5798"
                    ],
                    "url": "https://www.log-in-verlag.de/informatische_bildung/",
                    "alternate_urls": [
                        "https://www.anycorp.com/log/about",
                        "https://www.jstor.org/journal/log",
                        "https://www.anycorp.com/",
                        "http://www.jstor.org/action/showPublication?journalCode=log",
                        "http://www.anycorp.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c559dc62c0d64295f9c0dfb5f322b3f30ddf44eb",
                "title": "GraphFramEx: Towards Systematic Evaluation of Explainability Methods for Graph Neural Networks",
                "abstract": "As one of the most popular machine learning models today, graph neural networks (GNNs) have attracted intense interest recently, and so does their explainability. Users are increasingly interested in a better understanding of GNN models and their outcomes. Unfortunately, today's evaluation frameworks for GNN explainability often rely on few inadequate synthetic datasets, leading to conclusions of limited scope due to a lack of complexity in the problem instances. As GNN models are deployed to more mission-critical applications, we are in dire need for a common evaluation protocol of explainability methods of GNNs. In this paper, we propose, to our best knowledge, the first systematic evaluation framework for GNN explainability, considering explainability on three different\"user needs\". We propose a unique metric that combines the fidelity measures and classifies explanations based on their quality of being sufficient or necessary. We scope ourselves to node classification tasks and compare the most representative techniques in the field of input-level explainability for GNNs. For the inadequate but widely used synthetic benchmarks, surprisingly shallow techniques such as personalized PageRank have the best performance for a minimum computation time. But when the graph structure is more complex and nodes have meaningful features, gradient-based methods are the best according to our evaluation criteria. However, none dominates the others on all evaluation dimensions and there is always a trade-off. We further apply our evaluation protocol in a case study for frauds explanation on eBay transaction graphs to reflect the production environment.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146257620",
                        "name": "Kenza Amara"
                    },
                    {
                        "authorId": "83539859",
                        "name": "Rex Ying"
                    },
                    {
                        "authorId": "1445089663",
                        "name": "Zitao Zhang"
                    },
                    {
                        "authorId": "2171816624",
                        "name": "Zhihao Han"
                    },
                    {
                        "authorId": "2412958",
                        "name": "Yinan Shan"
                    },
                    {
                        "authorId": "1689559",
                        "name": "U. Brandes"
                    },
                    {
                        "authorId": "50323214",
                        "name": "S. Schemm"
                    },
                    {
                        "authorId": "1776014",
                        "name": "Ce Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "Furthermore, most explanation methods for GNNs\u2019 focus on providing factual explanations [16,29,11]."
            ],
            "citingPaper": {
                "paperId": "28362783e390c02fe0ac129f0268e8fdfdd8293d",
                "externalIds": {
                    "ArXiv": "2206.02957",
                    "DBLP": "journals/corr/abs-2206-02957",
                    "DOI": "10.1145/3511808.3557608",
                    "CorpusId": 249431911
                },
                "corpusId": 249431911,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/28362783e390c02fe0ac129f0268e8fdfdd8293d",
                "title": "GRETEL: A unified framework for Graph Counterfactual Explanation Evaluation",
                "abstract": ". Nowadays, Machine Learning (ML) systems are a fundamental part of those tools with an impact on our daily life in several application domains. Unfortunately those systems, due to their black-box nature, are hardly adopted in those application domains (e.g. health, \ufb01nance) where having an understanding of the decision process is of paramount importance. For this reason, explanation methods were developed to give insight into how the ML model has taken a speci\ufb01c decision for a given case/instance. In particular, Graph Counterfactual Explanations (GCE) is one of the possible explanation techniques in the Graph Learning domain. Those techniques can be useful to discover, for example: i) molecular compounds similar in terms of speci\ufb01c desired properties, or ii) new insights into the interplay of di\ufb00erent brain regions for certain diseases. Unfortunately, the existing works of Graph Counterfactual Explanations diverge mostly in the problem de\ufb01nition, application domain, test data, and evaluation metrics, and most existing works do not compare against other counterfactual explanation techniques present in the literature. For these reasons, we present GRETEL , a uni\ufb01ed framework to develop and test GCEs\u2019. Our framework provides a set of well-de\ufb01ned mechanisms to easily integrate and manage: both real and synthetic datasets, ML models, state-of-the-art explanation techniques, and a set of evaluation measures. GRETEL is a well-organized and highly extensible platform, which promotes the Open Science and experiments reproducibility thus it can be adopted e\ufb00ortlessly by future researchers who want to create and test their new explanation methods by comparing them to existing techniques across several application domains, data and evaluation measures. To present GRETEL , we show the experiments conducted to integrate and test several synthetic and real datasets with several existing explanation techniques and base ML models. University of L\u2019Aquila.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1390033678",
                        "name": "Mario Alfonso Prado-Romero"
                    },
                    {
                        "authorId": "1765155",
                        "name": "G. Stilo"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "PGExplainer [10] is also based on maximizing mutual information between a class label and a highly contributing graph towards GNN prediction."
            ],
            "citingPaper": {
                "paperId": "249f6d16f87e509629c7944e39d7b6e8ec2d81da",
                "externalIds": {
                    "ArXiv": "2206.03491",
                    "DBLP": "journals/corr/abs-2206-03491",
                    "DOI": "10.48550/arXiv.2206.03491",
                    "CorpusId": 249461902
                },
                "corpusId": 249461902,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/249f6d16f87e509629c7944e39d7b6e8ec2d81da",
                "title": "EiX-GNN : Concept-level eigencentrality explainer for graph neural networks",
                "abstract": "Nowadays, deep prediction models, especially graph neural networks, have a majorplace in critical applications. In such context, those models need to be highlyinterpretable or being explainable by humans, and at the societal scope, this understandingmay also be feasible for humans that do not have a strong prior knowledgein models and contexts that need to be explained. In the literature, explainingis a human knowledge transfer process regarding a phenomenon between an explainerand an explainee. We propose EiX-GNN (Eigencentrality eXplainer forGraph Neural Networks) a new powerful method for explaining graph neural networksthat encodes computationally this social explainer-to-explainee dependenceunderlying in the explanation process. To handle this dependency, we introducethe notion of explainee concept assimibility which allows explainer to adapt itsexplanation to explainee background or expectation. We lead a qualitative studyto illustrate our explainee concept assimibility notion on real-world data as wellas a qualitative study that compares, according to objective metrics established inthe literature, fairness and compactness of our method with respect to performingstate-of-the-art methods. It turns out that our method achieves strong results inboth aspects.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50792035",
                        "name": "P. Bourdon"
                    },
                    {
                        "authorId": "2013308",
                        "name": "D. Helbert"
                    },
                    {
                        "authorId": "1631289689",
                        "name": "A. Raison"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "This\nextra speed in explaining graphs comes at the cost of an offline training procedure in both CFGExplainer and PGExplainer.",
                "PGExplainer [17], three state-of-the-art interpretability models that operate on graph neural networks.",
                "work [16], [17], [18], CFGExplainer explains the prediction by a particular classifier on a particular sample.",
                "While both CFGExplainer and PGExplainer require an offline training procedure, PGExplainer requires an input constructed from edge embeddings, as opposed to node embeddings that are used by CFGExplainer.",
                "In this section, we provide a brief overview of Attributed Control Flow Graphs (ACFGs) used for malware classification, Graph Neural Networks (GNNs), and three graph based interpretability models used in our evaluations, namely GNNExplainer [16], SubgraphX [18] and PGExplainer [17].",
                "This is in contrast to PGExplainer [17] that uses edge embeddings, where the",
                "This is in contrast to our solution CFGExplainer and PGExplainer [17] which leverage global information to provide instance-level explanations.",
                "such as GNNExplainer [16] and PGExplainer [17] learn to identify subgraphs by perturbing the original graphs by means of identifying a mask for edges or features and then combining it with the original graph.",
                "Recently, there have been efforts in explaining GNN-based classification results [16], [17], [18] by identifying graph structures that contribute most towards classification.",
                "In this section, we evaluate the classification accuracy of equisized subgraphs produced by four GNN-based interpretability models \u2013 CFGExplainer, GNNExplainer [16], SubgraphX [18] and PGExplainer [17].",
                ", G = Gs+\u2206G) similar to works in [16], [17], where Gs is the subgraph that makes important contribution towards malware classification and \u2206G the subgraph containing the rest of the nodes.",
                "We have compared CFGExplainer against three state-ofthe-art GNN-oriented explainers, namely GNNExplainer [16], SubgraphX [18] and PGExplainer [17], using eleven malware families (Bagle, Bifrose, Hupigon, Ldpinch, Lmir, Rbot, Sdbot, Swizzor, Vundo, Zbot and Zlob) and one benign family.",
                "In contrast, CFGExplainer and PGExplainer generate explanations in around 3.9 and 6.4 minutes, respectively."
            ],
            "citingPaper": {
                "paperId": "81724e067cbd9c5f62e86074924a1b6f3e2bfa53",
                "externalIds": {
                    "DBLP": "conf/dsn/HerathWYY22",
                    "DOI": "10.1109/dsn53405.2022.00028",
                    "CorpusId": 250122258
                },
                "corpusId": 250122258,
                "publicationVenue": {
                    "id": "7f03ba38-4bb4-41a3-a6d7-914c28f84272",
                    "name": "Dependable Systems and Networks",
                    "type": "conference",
                    "alternate_names": [
                        "DSN",
                        "Dependable Syst Netw"
                    ],
                    "url": "http://www.dsn.org/"
                },
                "url": "https://www.semanticscholar.org/paper/81724e067cbd9c5f62e86074924a1b6f3e2bfa53",
                "title": "CFGExplainer: Explaining Graph Neural Network-Based Malware Classification from Control Flow Graphs",
                "abstract": "With the ever increasing threat of malware, extensive research effort has been put on applying Deep Learning for malware classification tasks. Graph Neural Networks (GNNs) that process malware as Control Flow Graphs (CFGs) have shown great promise for malware classification. However, these models are viewed as black-boxes, which makes it hard to validate and identify malicious patterns. To that end, we propose CFG-Explainer, a deep learning based model for interpreting GNN-oriented malware classification results. CFGExplainer identifies a subgraph of the malware CFG that contributes most towards classification and provides insight into importance of the nodes (i.e., basic blocks) within it. To the best of our knowledge, CFGExplainer is the first work that explains GNN-based mal-ware classification. We compared CFGExplainer against three explainers, namely GNNExplainer, SubgraphX and PGExplainer, and showed that CFGExplainer is able to identify top equisized subgraphs with higher classification accuracy than the other three models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145021872",
                        "name": "J. D. Herath"
                    },
                    {
                        "authorId": "50847225",
                        "name": "Priti Wakodikar"
                    },
                    {
                        "authorId": "93329376",
                        "name": "Pin Yang"
                    },
                    {
                        "authorId": "144456144",
                        "name": "Guanhua Yan"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "PGExplainer [25] provides an inductive edge explanation method working on a set of graphs, by learning edge masks with a multi-layer neural network.",
                "On the other hand, several GNN explanation methods are proposed recently [23], [24], [25], [26], [27].",
                ", GNNExplainer [26], PGExplainer [25], and GraphMask [27], apply an edgecentric strategy by identifying the important edges and regarding the constructed subgraph as the explanation result.",
                "BA-2motifs [25] is a motif-based synthetic dataset, each graph from which contains a fivenode house-like motif or a cycle motif.",
                "Following the existing works [25], [26], in consideration of relaxation, we adopt Bernoulli distribution P (Gs) = \u220f (i,j)\u2208E P ((i, j)) for edge explanation, where P ((i, j)) is the probability of the edge (i, j)\u2019s existence.",
                "We compare ILLUMINATI with the following baseline GNN explanation methods, GNNExplainer [26], PGMExplainer [23], and PGExplainer [25].",
                "that contribute to the prediction, for example, by retaining important edges [25], [27]."
            ],
            "citingPaper": {
                "paperId": "26647ac28bfd1bed6cf2d81d1cdd01dc1e852b69",
                "externalIds": {
                    "ArXiv": "2303.14836",
                    "DBLP": "conf/eurosp/HeJH22",
                    "DOI": "10.1109/EuroSP53844.2022.00013",
                    "CorpusId": 249997059
                },
                "corpusId": 249997059,
                "publicationVenue": {
                    "id": "4c2b8cb8-e51c-4ece-9122-89595989b56f",
                    "name": "European Symposium on Security and Privacy",
                    "type": "conference",
                    "alternate_names": [
                        "EuroS&P",
                        "IEEE European Symposium on Security and Privacy",
                        "Eur Symp Secur Priv",
                        "IEEE Eur Symp Secur Priv",
                        "EUROS&P"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/26647ac28bfd1bed6cf2d81d1cdd01dc1e852b69",
                "title": "Illuminati: Towards Explaining Graph Neural Networks for Cybersecurity Analysis",
                "abstract": "Graph neural networks (GNNs) have been utilized to create multi-layer graph models for a number of cybersecurity applications from fraud detection to software vulnerability analysis. Unfortunately, like traditional neural networks, GNNs also suffer from a lack of transparency, that is, it is challenging to interpret the model predictions. Prior works focused on specific factor explanations for a GNN model. In this work, we have designed and implemented Illuminati, a comprehensive and accurate explanation framework for cybersecurity applications using GNN models. Given a graph and a pre-trained GNN model, Illuminati is able to identify the important nodes, edges, and attributes that are contributing to the prediction while requiring no prior knowledge of GNN models. We evaluate Illuminati in two cybersecurity applications, i.e., code vulnerability detection and smart contract vulnerability detection. The experiments show that Illuminati achieves more accurate explanation results than state-of-the-art methods, specifically, 87.6% of subgraphs identified by Illuminati are able to retain their original prediction, an improvement of 10.3% over others at 77.3%. Furthermore, the explanation of Illuminati can be easily understood by the domain experts, suggesting the significant usefulness for the development of cybersecurity applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "11270586",
                        "name": "Haoyu He"
                    },
                    {
                        "authorId": "3028841",
                        "name": "Yuede Ji"
                    },
                    {
                        "authorId": "2107466791",
                        "name": "H. H. Huang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "For simplicity, we assume that the selections of edges from the original graph G are conditionally independent to each other [Luo et al., 2020], that is Pw = \u220fM i=1 Pwi ."
            ],
            "citingPaper": {
                "paperId": "a6231129c3245e2d816dbb55217b1981c20f45da",
                "externalIds": {
                    "DBLP": "conf/uai/YuAYJP22",
                    "ArXiv": "2206.00118",
                    "DOI": "10.48550/arXiv.2206.00118",
                    "CorpusId": 249240613
                },
                "corpusId": 249240613,
                "publicationVenue": {
                    "id": "f9af8000-42f8-410d-a622-e8811e41660a",
                    "name": "Conference on Uncertainty in Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Uncertainty in Artificial Intelligence",
                        "UAI",
                        "Conf Uncertain Artif Intell",
                        "Uncertain Artif Intell"
                    ],
                    "url": "http://www.auai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a6231129c3245e2d816dbb55217b1981c20f45da",
                "title": "Principle of Relevant Information for Graph Sparsification",
                "abstract": "Graph sparsification aims to reduce the number of edges of a graph while maintaining its structural properties. In this paper, we propose the first general and effective information-theoretic formulation of graph sparsification, by taking inspiration from the Principle of Relevant Information (PRI). To this end, we extend the PRI from a standard scalar random variable setting to structured data (i.e., graphs). Our Graph-PRI objective is achieved by operating on the graph Laplacian, made possible by expressing the graph Laplacian of a subgraph in terms of a sparse edge selection vector $\\mathbf{w}$. We provide both theoretical and empirical justifications on the validity of our Graph-PRI approach. We also analyze its analytical solutions in a few special cases. We finally present three representative real-world applications, namely graph sparsification, graph regularized multi-task learning, and medical imaging-derived brain network classification, to demonstrate the effectiveness, the versatility and the enhanced interpretability of our approach over prevalent sparsification techniques. Code of Graph-PRI is available at https://github.com/SJYuCNEL/PRI-Graphs",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2462771",
                        "name": "Shujian Yu"
                    },
                    {
                        "authorId": "2819104",
                        "name": "F. Alesiani"
                    },
                    {
                        "authorId": "46764830",
                        "name": "Wenzhe Yin"
                    },
                    {
                        "authorId": "1747567",
                        "name": "R. Jenssen"
                    },
                    {
                        "authorId": "143961030",
                        "name": "J. Pr\u00edncipe"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Furthermore, [Luo et al., 2020] presented PGExplainer to explain GLNNs-GNNs collectively and inductively.",
                "These explanation models work in a post-hoc fashion and can be roughly divided into perturbation-based [Yuan et al., 2020a; Luo et al., 2020; Ying et al., 2019], surrogate modelbased [Huang et al., 2020], and gradient-based [Pope et al., 2019]."
            ],
            "citingPaper": {
                "paperId": "aaa6dcde3b3e4d1fab77631879983072f58c8844",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-15555",
                    "ArXiv": "2205.15555",
                    "DOI": "10.48550/arXiv.2205.15555",
                    "CorpusId": 249209760
                },
                "corpusId": 249209760,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/aaa6dcde3b3e4d1fab77631879983072f58c8844",
                "title": "Graph-level Neural Networks: Current Progress and Future Directions",
                "abstract": "Graph-structured data consisting of objects (i.e., nodes) and relationships among objects (i.e., edges) are ubiquitous. Graph-level learning is a matter of studying a collection of graphs instead of a single graph. Traditional graph-level learning methods used to be the mainstream. However, with the increasing scale and complexity of graphs, Graph-level Neural Networks (GLNNs, deep learning-based graph-level learning methods) have been attractive due to their superiority in modeling high-dimensional data. Thus, a survey on GLNNs is necessary. To frame this survey, we propose a systematic taxonomy covering GLNNs upon deep neural networks, graph neural networks, and graph pooling. The representative and state-of-the-art models in each category are focused on this survey. We also investigate the reproducibility, benchmarks, and new graph datasets of GLNNs. Finally, we conclude future directions to further push forward GLNNs. The repository of this survey is available at https://github.com/GeZhangMQ/Awesome-Graph-level-Neural-Networks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151251543",
                        "name": "Ge Zhang"
                    },
                    {
                        "authorId": "2142734769",
                        "name": "Jia Wu"
                    },
                    {
                        "authorId": null,
                        "name": "Jian Yang"
                    },
                    {
                        "authorId": "2057237074",
                        "name": "Shan Xue"
                    },
                    {
                        "authorId": "2110232168",
                        "name": "Wenbin Hu"
                    },
                    {
                        "authorId": "2110713858",
                        "name": "Chuan Zhou"
                    },
                    {
                        "authorId": "49349645",
                        "name": "Hao Peng"
                    },
                    {
                        "authorId": "120607997",
                        "name": "Quan.Z Sheng"
                    },
                    {
                        "authorId": "1682418",
                        "name": "C. Aggarwal"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "25fcf6c8325a408c072f18f84c21c8b946408df3",
                "externalIds": {
                    "ArXiv": "2205.13733",
                    "DBLP": "conf/wsdm/ZhaoLZW23",
                    "DOI": "10.1145/3539597.3570421",
                    "CorpusId": 254854393
                },
                "corpusId": 254854393,
                "publicationVenue": {
                    "id": "ea38228f-6ed3-4222-a3ce-d963d8cc9516",
                    "name": "Web Search and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Web Search Data Min",
                        "WSDM"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=3158"
                },
                "url": "https://www.semanticscholar.org/paper/25fcf6c8325a408c072f18f84c21c8b946408df3",
                "title": "Towards Faithful and Consistent Explanations for Graph Neural Networks",
                "abstract": "Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over recent years. Instance-level GNN explanation aims to discover critical input elements, like nodes or edges, that the target GNN relies upon for making predictions. Though various algorithms are proposed, most of them formalize this task by searching the minimal subgraph which can preserve original predictions. However, an inductive bias is deep-rooted in this framework: several subgraphs can result in the same or similar outputs as the original graphs. Consequently, they have the danger of providing spurious explanations and fail to provide consistent explanations. Applying them to explain weakly-performed GNNs would further amplify these issues. To address this problem, we theoretically examine the predictions of GNNs from the causality perspective. Two typical reasons of spurious explanations are identified: confounding effect of latent variables like distribution shift, and causal factors distinct from the original input. Observing that both confounding effects and diverse causal rationales are encoded in internal representations, we propose a simple yet effective countermeasure by aligning embeddings. Concretely, concerning potential shifts in the high-dimensional space, we design a distribution-aware alignment algorithm based on anchors. This new objective is easy to compute and can be incorporated into existing techniques with no or little effort. Theoretical analysis shows that it is in effect optimizing a more faithful explanation objective in design, which further justifies the proposed approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1999191869",
                        "name": "Tianxiang Zhao"
                    },
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    },
                    {
                        "authorId": "48505793",
                        "name": "Xiang Zhang"
                    },
                    {
                        "authorId": "2116430057",
                        "name": "Suhang Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "We discuss these for the BA-2Motifs [31], Tree-Cycle [54], and MUTAG[10] datasets in Section 4.3, and others in Appendix C.",
                "[31] follow a similar idea but emphasize finding important edges and finding explanations for many predictions at the same time.",
                "We discuss these for the BA-2Motifs [31], Tree-Cycle [54], and MUTAG[10] datasets in Section 4.",
                "We use the Infection and Negative Evidence benchmarks from Faber et al. [15], The BA-Shapes, Tree-Cycle, and Tree-Grid benchmarks from Ying et al. [54], and the BA-2Motifs dataset from Luo et al. [31].",
                "There are several works [11; 31; 54; 55; 56] that use the Mutagenicity dataset but call it MUTAG.",
                "We observe this in the BA-2Motifs dataset."
            ],
            "citingPaper": {
                "paperId": "50164ce3a7126f815d33a2b004b83d122cd6f0cd",
                "externalIds": {
                    "ArXiv": "2205.13234",
                    "DBLP": "journals/corr/abs-2205-13234",
                    "DOI": "10.48550/arXiv.2205.13234",
                    "CorpusId": 249097802
                },
                "corpusId": 249097802,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/50164ce3a7126f815d33a2b004b83d122cd6f0cd",
                "title": "DT+GNN: A Fully Explainable Graph Neural Network using Decision Trees",
                "abstract": "We propose the fully explainable Decision Tree Graph Neural Network (DT+GNN) architecture. In contrast to existing black-box GNNs and post-hoc explanation methods, the reasoning of DT+GNN can be inspected at every step. To achieve this, we first construct a differentiable GNN layer, which uses a categorical state space for nodes and messages. This allows us to convert the trained MLPs in the GNN into decision trees. These trees are pruned using our newly proposed method to ensure they are small and easy to interpret. We can also use the decision trees to compute traditional explanations. We demonstrate on both real-world datasets and synthetic GNN explainability benchmarks that this architecture works as well as traditional GNNs. Furthermore, we leverage the explainability of DT+GNNs to find interesting insights into many of these datasets, with some surprising results. We also provide an interactive web tool to inspect DT+GNN's decision making.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146980777",
                        "name": "Peter M\u00fcller"
                    },
                    {
                        "authorId": "36352356",
                        "name": "Lukas Faber"
                    },
                    {
                        "authorId": "1995092493",
                        "name": "Karolis Martinkus"
                    },
                    {
                        "authorId": "2075356250",
                        "name": "Roger Wattenhofer"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "For BA-shapes, we use node indices [400:700:5] following the choice in the previous work [37, 20, 32, 2].",
                "More details on the implementaion of GNNExpl and PGExpl can be found in Appendix B.2.",
                "For subgraph explanations, we include: 1) GNNExpl as it is the building block for many follow-up works; 2) a continuous version of GNNExpl (GNNExpl (soft)) where we do not drop edges with small importance scores and we instead consider it as providing a constinous and weighted adjacency matrix Asij \u2264 1 and 3) PGExpl.",
                "This paper mainly considers GNNExpl and PGExpl as the evaluation of SubgraphX using the default parameters from DIG [18] exceeds the timeout limit (10 minuteshttps://us06web.zoom.us/j/86767269620 per node) with our current computational resources (Titan RTX) on Cora.",
                "In the context of GNNs, recent work has proposed numerous subgraph explanation techniques [37, 20, 32, 2], which aim to specifically leverage graph structure when identifying the node and edge features that are important for a prediction.",
                "The sparsity of PGExpl shows the highest sensitivity against the model\u2019s accuracy; 2) in both datasets all explanation methods become less faithful when the model becomes more accurate.",
                "1] and can be direclty usd as \u201cprobabilities\" as the attribution scores from GNNExpl and PGExpl.",
                "Since PGExpl shares the same motivation with GNNExpl and only differs on techniques that 1) narrow down the searching space for the optimal As and 2) learn a dense layer to jointly explain a set of nodes, we therefore point the readers to Luo et al. [20] for details.",
                "B.2 Implementation GNNExpl and PGExpl\nWe use the implementation on Pytorch Geometric for GNNExpl2.",
                "3a and does not apply to GNNExpl and PGExpl as they are already in the range of [0, 1].",
                "PGExpl was originally built in TensorFlow.",
                "The output of a subgraph explanation is a perturbed matrix of node features or/and an adjacency matrix with more zero entries, which are considered as the most relevant part of the input graph towards the output [37, 20, 32, 2].",
                "Example choices of L(Xs, As, F ) are mutual information, i.e. GNNExpl [37] and PGExpl [20], and Shapley Value, i.e. SubgraphX [2].",
                "In BAshapes, we find that training PGExpl with all points provides a slightly better results on ROC-AUC score so we train all nodes together.",
                "a continuous mask [37] or `1 norm [20], for the size function s.",
                "We use the default number of epochs for GNNExpl and PGExpl in the public repository.",
                "3a, we find that the ROC-AUC scores for SoftGNNExpl and PGExpl are pretty high even when the model is in its early stage of training while the correlations of the rest methods between explanation AUC and the test accuracy are stronger.",
                "PGExpl is excluded because it does not attribute over node features.",
                "GNNExpl [37] and PGExpl [20], and Shapley Value, i.",
                "Experimental details and relevant plots are included in Appendix C.3) Our results suggest that SoftGNNExpl and PGExpl may not be sensitive enough to reflect the model\u2019s performance."
            ],
            "citingPaper": {
                "paperId": "f7240206180e30fb4e642a3afedd2046f59277f3",
                "externalIds": {
                    "ArXiv": "2205.11850",
                    "DBLP": "journals/corr/abs-2205-11850",
                    "DOI": "10.48550/arXiv.2205.11850",
                    "CorpusId": 249017774
                },
                "corpusId": 249017774,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f7240206180e30fb4e642a3afedd2046f59277f3",
                "title": "Faithful Explanations for Deep Graph Models",
                "abstract": "This paper studies faithful explanations for Graph Neural Networks (GNNs). First, we provide a new and general method for formally characterizing the faithfulness of explanations for GNNs. It applies to existing explanation methods, including feature attributions and subgraph explanations. Second, our analytical and empirical results demonstrate that feature attribution methods cannot capture the nonlinear effect of edge features, while existing subgraph explanation methods are not faithful. Third, we introduce \\emph{k-hop Explanation with a Convolutional Core} (KEC), a new explanation method that provably maximizes faithfulness to the original GNN by leveraging information about the graph structure in its adjacency matrix and its \\emph{k-th} power. Lastly, our empirical results over both synthetic and real-world datasets for classification and anomaly detection tasks with GNNs demonstrate the effectiveness of our approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50219096",
                        "name": "Zifan Wang"
                    },
                    {
                        "authorId": "1515573782",
                        "name": "Yuhang Yao"
                    },
                    {
                        "authorId": "2166234783",
                        "name": "Chaoran Zhang"
                    },
                    {
                        "authorId": "2119078431",
                        "name": "Han Zhang"
                    },
                    {
                        "authorId": "2166306755",
                        "name": "Youjie Kang"
                    },
                    {
                        "authorId": "1393650147",
                        "name": "Carlee Joe-Wong"
                    },
                    {
                        "authorId": "2623167",
                        "name": "Matt Fredrikson"
                    },
                    {
                        "authorId": "7207306",
                        "name": "Anupam Datta"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "contexts": [
                "Generally, they highlight the important patterns of the input graphs such as nodes [52, 95], edges [144, 78] and sub-graphs [147, 152] which are crucial for the model predictions.",
                "PGExplainer [78] proposes to explain the predictions via edge masks.",
                "Despite their great success, GNNs are generally treated as black-box since their decisions are less understood [144, 78], leading to the increasing concerns about the explainability of GNNs."
            ],
            "citingPaper": {
                "paperId": "d22efa7a35464ab9b40f8a4c926bbdcb91b84699",
                "externalIds": {
                    "ArXiv": "2205.10014",
                    "DBLP": "journals/corr/abs-2205-10014",
                    "DOI": "10.48550/arXiv.2205.10014",
                    "CorpusId": 248965359
                },
                "corpusId": 248965359,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d22efa7a35464ab9b40f8a4c926bbdcb91b84699",
                "title": "A Survey of Trustworthy Graph Learning: Reliability, Explainability, and Privacy Protection",
                "abstract": "Deep graph learning has achieved remarkable progresses in both business and scienti\ufb01c areas ranging from \ufb01nance and e-commerce, to drug and advanced material discovery. Despite these progresses, how to ensure various deep graph learning algorithms behave in a socially responsible manner and meet regulatory compliance requirements becomes an emerging problem, especially in risk-sensitive domains. Trustworthy graph learning (TwGL) aims to solve the above problems from a technical viewpoint. In contrast to conventional graph learning research which mainly cares about model performance, TwGL considers various reliability and safety aspects of the graph learning framework including but not limited to robustness, explainability, and privacy. In this survey, we provide a comprehensive review of recent leading approaches in the TwGL \ufb01eld from three dimensions, namely, reliability, explainability, and privacy protection. We give a general categorization for existing work and review typical work for each category. To give further insights for TwGL research, we provide a uni\ufb01ed view to inspect previous works and build the connection between them. We also point out some important open problems remaining to be solved in the future developments of TwGL.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "27055880",
                        "name": "Bingzhe Wu"
                    },
                    {
                        "authorId": "2115953679",
                        "name": "Jintang Li"
                    },
                    {
                        "authorId": "28822585",
                        "name": "Junchi Yu"
                    },
                    {
                        "authorId": "2419616",
                        "name": "Yatao Bian"
                    },
                    {
                        "authorId": "7214272",
                        "name": "Hengtong Zhang"
                    },
                    {
                        "authorId": "2145762399",
                        "name": "Chaochao Chen"
                    },
                    {
                        "authorId": "144549366",
                        "name": "Chengbin Hou"
                    },
                    {
                        "authorId": null,
                        "name": "Guoji Fu"
                    },
                    {
                        "authorId": "1853048147",
                        "name": "Liang Chen"
                    },
                    {
                        "authorId": "1754673",
                        "name": "Tingyang Xu"
                    },
                    {
                        "authorId": "48537464",
                        "name": "Yu Rong"
                    },
                    {
                        "authorId": "1687974",
                        "name": "Xiaolin Zheng"
                    },
                    {
                        "authorId": "1768190",
                        "name": "Junzhou Huang"
                    },
                    {
                        "authorId": "2053865709",
                        "name": "Ran He"
                    },
                    {
                        "authorId": "143905981",
                        "name": "Baoyuan Wu"
                    },
                    {
                        "authorId": "2113638448",
                        "name": "Guangyu Sun"
                    },
                    {
                        "authorId": "2153522384",
                        "name": "Peng Cui"
                    },
                    {
                        "authorId": "144291579",
                        "name": "Zibin Zheng"
                    },
                    {
                        "authorId": "47781621",
                        "name": "Zhe Liu"
                    },
                    {
                        "authorId": "144259957",
                        "name": "P. Zhao"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "On this account, we follow the Gumbel-Softmax reparametrization trick [35, 36] and relax the binary variables ei,j to a continuous edge weight variables \u00eai,j = \u03c3((log \u2212 log(1\u2212 ) + wi,j)/\u03c4) \u2208 [0, 1], where \u03c3(\u00b7) is the sigmoid function; \u223c Uniform(0, 1); \u03c4 is the temperature hyperparameter such that lim\u03c4\u21920 p(\u00eai,j = 1) = \u03c3(wi,j); wi,j is the latent variables which is calculated by a neural network following previous work [10],",
                "(2) Parametric explanation methods [9, 24, 10, 25, 14] generate the explanatory subgraphs with a parametrized explainer model.",
                "Following the previous explanation works [9, 10], we leverage mutual information to measure the relevance and therefore formulate the explanation problem as argmaxS I(S;Z).",
                "To provide a global understanding of the model prediction, PGExplainer [10] generates the explanatory subgraphs with a deep neural network whose parameters are shared across the explained instances.",
                "Despite their strengths, GNNs are usually treated as black boxes and thus cannot provide human-intelligible explanations [9, 10].",
                "Following this strategy, previous explanation methods for supervised setting, such as GNNExplainer [9], PGExplainer [10], PGM-Explainer [14], and Refine [25], could be adopted as baselines."
            ],
            "citingPaper": {
                "paperId": "1c13e3ce072f6eeb9f33244c4866a52f7b584ed0",
                "externalIds": {
                    "ArXiv": "2205.09934",
                    "DBLP": "journals/corr/abs-2205-09934",
                    "DOI": "10.48550/arXiv.2205.09934",
                    "CorpusId": 248965040
                },
                "corpusId": 248965040,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1c13e3ce072f6eeb9f33244c4866a52f7b584ed0",
                "title": "Towards Explanation for Unsupervised Graph-Level Representation Learning",
                "abstract": "Due to the superior performance of Graph Neural Networks (GNNs) in various domains, there is an increasing interest in the GNN explanation problem \" which fraction of the input graph is the most crucial to decide the model\u2019s decision? \" Existing explanation methods focus on the supervised settings, e.g. , node classi\ufb01cation and graph classi\ufb01cation, while the explanation for unsupervised graph-level representation learning is still unexplored. The opaqueness of the graph representations may lead to unexpected risks when deployed for high-stake decision-making scenarios. In this paper, we advance the Information Bottleneck principle (IB) to tackle the proposed explanation problem for unsupervised graph representations, which leads to a novel principle, Unsupervised Subgraph Information Bottleneck (USIB). We also theoretically analyze the connection between graph representations and explanatory subgraphs on the label space, which reveals that the expressiveness and robustness of representations bene\ufb01t the \ufb01delity of explanatory subgraphs. Experimental results on both synthetic and real-world datasets demonstrate the superiority of our developed explainer and the validity of our theoretical analysis. ABSTRACT Data artifacts incentivize machine learning models to learn non-transferable generalizations by taking advantage of shortcuts in the data, and there is growing evidence that data artifacts play a role for the strong results that deep learning models achieve in recent natural language processing benchmarks. In this paper, we focus on task-oriented dialogue and investigate whether popular datasets such as MultiWOZ contain such data artifacts. We found that by only keeping frequent phrases in the training examples, state-of-the-art models perform similarly compared to the variant trained with full data, suggesting they exploit these spurious correlations to solve the task. Motivated by this, we propose a contrastive learning based framework to encourage the model to ignore these cues and focus on learning generalisable patterns. We also experiment with adversarial filtering to remove \u201ceasy\u201d training instances so that the model would focus on learning from the \u201charder\u201d instances. We conduct a number of generalization experiments \u2014 e.g., cross-domain/dataset and adversarial tests \u2014 to assess the robustness of our approach and found that it works exceptionally well.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152099796",
                        "name": "Qinghua Zheng"
                    },
                    {
                        "authorId": "2109656535",
                        "name": "Jihong Wang"
                    },
                    {
                        "authorId": "3326677",
                        "name": "Minnan Luo"
                    },
                    {
                        "authorId": "40508553",
                        "name": "Yaoliang Yu"
                    },
                    {
                        "authorId": "1737121128",
                        "name": "Jundong Li"
                    },
                    {
                        "authorId": "145095579",
                        "name": "L. Yao"
                    },
                    {
                        "authorId": "2840330",
                        "name": "Xiao Chang"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                ", GNNExplainer [22], PGExplainer [56] XGNN [132], RG-Explainer [152], OrphicX [153]) or counterfactual reason-",
                "GNNExplainer [22] Explainability Perturbation-based Grey-box Instance/Group NC/GC Edge/Feature PGExplainer [56] Explainability Perturbation-based Grey-box Instance NC/GC Edge ZORRO [159] Explainability Perturbation-based Grey-box Instance NC Node/Feature Causal Screening [149] Explainability Perturbation-based Grey-box Instance GC Edge GraphMask [160] Explainability Perturbation-based White-box Instance SRL/MQA Edge SubgraphX [161] Explainability Perturbation-based Black-box Instance NC/GC Subgraph CF-GNNExplainer [162] Explainability Perturbation-based Grey-box Instance NC Edge RCExplainer [136] Explainability Perturbation-based Grey-box Instance NC/GC Edge ReFine [163] Explainability Perturbation-based Grey-box Instance GC Edge CF2 [164] Explainability Perturbation-based Grey-box Instance NC/GC Edge/Feature",
                ", PGExplainer [56]) provide a sample-dependant explanation for each graph sample.",
                ", subgraphs) can reveal which components in molecule graphs support the final biochemical functionality predictions of GNNs [56].",
                "embedding in PGExplainer [56]) because access to the GNN system may be limited (e."
            ],
            "citingPaper": {
                "paperId": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
                "externalIds": {
                    "ArXiv": "2205.07424",
                    "DBLP": "journals/corr/abs-2205-07424",
                    "DOI": "10.48550/arXiv.2205.07424",
                    "CorpusId": 248811191
                },
                "corpusId": 248811191,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/21913eb287f8fc33db8f6274fd2a07072c4e11eb",
                "title": "Trustworthy Graph Neural Networks: Aspects, Methods and Trends",
                "abstract": "Graph neural networks (GNNs) have emerged as a series of competent graph learning methods for diverse real-world scenarios, ranging from daily applications like recommendation systems and question answering to cutting-edge technologies such as drug discovery in life sciences and n-body simulation in astrophysics. However, task performance is not the only requirement for GNNs. Performance-oriented GNNs have exhibited potential adverse effects like vulnerability to adversarial attacks, unexplainable discrimination against disadvantaged groups, or excessive resource consumption in edge computing environments. To avoid these unintentional harms, it is necessary to build competent GNNs characterised by trustworthiness. To this end, we propose a comprehensive roadmap to build trustworthy GNNs from the view of the various computing technologies involved. In this survey, we introduce basic concepts and comprehensively summarise existing efforts for trustworthy GNNs from six aspects, including robustness, explainability, privacy, fairness, accountability, and environmental well-being. Additionally, we highlight the intricate cross-aspect relations between the above six aspects of trustworthy GNNs. Finally, we present a thorough overview of trending directions for facilitating the research and industrialisation of trustworthy GNNs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2156713249",
                        "name": "He Zhang"
                    },
                    {
                        "authorId": "2115265646",
                        "name": "Bang Wu"
                    },
                    {
                        "authorId": "3032058",
                        "name": "Xingliang Yuan"
                    },
                    {
                        "authorId": "2153326034",
                        "name": "Shirui Pan"
                    },
                    {
                        "authorId": "8163721",
                        "name": "Hanghang Tong"
                    },
                    {
                        "authorId": "2112496348",
                        "name": "Jian Pei"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "252d1a19390f773af9bbe8895360cb29efdecab3",
                "externalIds": {
                    "DOI": "10.3389/fenrg.2022.920407",
                    "CorpusId": 248563269
                },
                "corpusId": 248563269,
                "publicationVenue": {
                    "id": "757ec547-4fbf-4010-b6ae-c6b833ccd3a4",
                    "name": "Frontiers in Energy Research",
                    "type": "journal",
                    "alternate_names": [
                        "Front Energy Res"
                    ],
                    "issn": "2296-598X",
                    "url": "http://www.frontiersin.org/Energy_Research",
                    "alternate_urls": [
                        "https://www.frontiersin.org/journals/energy-research",
                        "http://www.frontiersin.org/Energy_Research/archive"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/252d1a19390f773af9bbe8895360cb29efdecab3",
                "title": "Short-Term Wind Power Prediction via Spatial Temporal Analysis and Deep Residual Networks",
                "abstract": "Wind power is a rapidly growing source of clean energy. Accurate short-term forecasting of wind power is essential for reliable energy generation. In this study, we propose a novel wind power forecasting approach using spatiotemporal analysis to enhance forecasting performance. First, the wind power time-series data from the target turbine and adjacent neighboring turbines were utilized to form a graph structure using graph neural networks (GNN). The graph structure was used to compute the spatiotemporal correlation between the target turbine and adjacent turbines. Then, the prediction models were trained using a deep residual network (DRN) for short-term wind power prediction. Considering the wind speed, the historic wind power, air density, and historic wind power in adjacent wind turbines within the supervisory control and data acquisition (SCADA) system were utilized. A comparative analysis was performed using conventional machine-learning approaches. Industrial data collected from Hami County, Xinjiang, China, were used for the case study. The computational results validate the superiority of the proposed approach for short-term wind-power forecasting.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "10664037",
                        "name": "Huajin Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "1b15d1e2cf7d879c64553dddb24901902cb7e5a0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-03612",
                    "ArXiv": "2205.03612",
                    "DOI": "10.48550/arXiv.2205.03612",
                    "CorpusId": 248572361
                },
                "corpusId": 248572361,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1b15d1e2cf7d879c64553dddb24901902cb7e5a0",
                "title": "BrainIB: Interpretable Brain Network-based Psychiatric Diagnosis with Graph Information Bottleneck",
                "abstract": "Developing a new diagnostic models based on the underlying biological mechanisms rather than subjective symptoms for psychiatric disorders is an emerging consensus. Recently, machine learning-based classifiers using functional connectivity (FC) for psychiatric disorders and healthy controls are developed to identify brain markers. However, existing machine learningbased diagnostic models are prone to over-fitting (due to insufficient training samples) and perform poorly in new test environment. Furthermore, it is difficult to obtain explainable and reliable brain biomarkers elucidating the underlying diagnostic decisions. These issues hinder their possible clinical applications. In this work, we propose BrainIB, a new graph neural network (GNN) framework to analyze functional magnetic resonance images (fMRI), by leveraging the famed Information Bottleneck (IB) principle. BrainIB is able to identify the most informative edges in the brain (i.e., subgraph) and generalizes well to unseen data. We evaluate the performance of BrainIB against 8 popular brain network classification methods on two multi-site, largescale datasets and observe that our BrainIB always achieves the highest diagnosis accuracy. It also discovers the subgraph biomarkers which are consistent to clinical and neuroimaging findings.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152804328",
                        "name": "Kaizhong Zheng"
                    },
                    {
                        "authorId": "2462771",
                        "name": "Shujian Yu"
                    },
                    {
                        "authorId": "7480500",
                        "name": "Baojuan Li"
                    },
                    {
                        "authorId": "1747567",
                        "name": "R. Jenssen"
                    },
                    {
                        "authorId": "2108424611",
                        "name": "Badong Chen"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "044c866a4ab50a096864a3507da4d893fbd6fa28",
                "externalIds": {
                    "DBLP": "conf/infocom/ZhuZZGLL22",
                    "DOI": "10.1109/infocomwkshps54753.2022.9798287",
                    "CorpusId": 249900451
                },
                "corpusId": 249900451,
                "publicationVenue": {
                    "id": "be267cb9-6411-4126-8b64-4847025171ee",
                    "name": "Conference on Computer Communications Workshops",
                    "type": "conference",
                    "alternate_names": [
                        "INFOCOM WKSHPS",
                        "Conf Comput Commun Work"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/044c866a4ab50a096864a3507da4d893fbd6fa28",
                "title": "Interpretability Evaluation of Botnet Detection Model based on Graph Neural Network",
                "abstract": "Due to the conspicuous ability to capture topology characteristics, graph neural networks (GNN) have been widely used in botnet detection and proven efficient. However, the blackbox nature of GNN models creates an obstacle for users to trust these classified instruments. In addition to high accuracy, stakeholders also hope that these models are consistent with human cognition. To cope with this problem, we propose a method to evaluate the trustworthiness of GNN-based botnet detection models, called BD-GNNExplainer. Concretely, BD-GNNExplainer extracts the data that contribute the most to GNN\u2019s decision by reducing the loss between the classification results generated by the selected subgraph as the GNN model\u2019s input and the results generated by the entire graph as input. We calculate the relevance between the model-relied data and the informative data to quantify a score expressing interpretability. For different-structure GNN models, these scores will tell us which one is more trustworthy and ultimately become an essential basis for model optimization. To the best of our knowledge, our work is the first time to discuss the interpretability of botnet detection systems and will provide a guideline for making the botnet detection methodology more understandable.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2116876486",
                        "name": "Xiaoli Zhu"
                    },
                    {
                        "authorId": "2144291097",
                        "name": "Yong Zhang"
                    },
                    {
                        "authorId": "2156120015",
                        "name": "Zhao Zhang"
                    },
                    {
                        "authorId": "2169303888",
                        "name": "Da Guo"
                    },
                    {
                        "authorId": "2118912886",
                        "name": "Qi Li"
                    },
                    {
                        "authorId": null,
                        "name": "Zhao Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "9d526d1eca2ec06b4f412b4db5e822bd7fb99982",
                "externalIds": {
                    "DBLP": "conf/icde/LiDLCC22",
                    "DOI": "10.1109/icde53745.2022.00081",
                    "CorpusId": 251291801
                },
                "corpusId": 251291801,
                "publicationVenue": {
                    "id": "764e3630-ddac-4c21-af4b-9d32ffef082e",
                    "name": "IEEE International Conference on Data Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "ICDE",
                        "Int Conf Data Eng",
                        "IEEE Int Conf Data Eng",
                        "International Conference on Data Engineering"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1331"
                },
                "url": "https://www.semanticscholar.org/paper/9d526d1eca2ec06b4f412b4db5e822bd7fb99982",
                "title": "Black-box Adversarial Attack and Defense on Graph Neural Networks",
                "abstract": "Graph neural networks (GNNs) have achieved great success on various graph tasks. However, recent studies have re-vealed that GNNs are vulnerable to adversarial attacks, including topology modifications and feature perturbations. Regardless of the fruitful progress, existing attackers require node labels and GNN parameters to optimize a bi-level problem, or cannot cover both topology modifications and feature perturbations, which are not practical, efficient, or effective. In this paper, we propose a black-box attacker PEEGA, which is restricted to access node features and graph topology for practicability. Specifically, we propose to measure the negative impact of various adversarial attacks from the perspective of node representations, thereby we formulate a single-level problem that can be efficiently solved. Furthermore, we observe that existing attackers tend to blur the context of nodes through adding edges between nodes with different labels. As a result, GNNs are unable to recognize nodes. Based on this observation, we propose a GNN defender GNAT, which incorporates three augmented graphs, i.e., a topology graph, a feature graph, and an ego graph, to make the context of nodes more distinguishable. Extensive experiments on three real-world datasets demonstrate the effectiveness and efficiency of our proposed attacker, despite the fact that we do not access node labels and GNN parameters. Moreover, the effectiveness and efficiency of our proposed defender are also validated by substantial experiments.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145537904",
                        "name": "Haoyang Li"
                    },
                    {
                        "authorId": "51129379",
                        "name": "Shimin Di"
                    },
                    {
                        "authorId": "2145274105",
                        "name": "Zijian Li"
                    },
                    {
                        "authorId": "49330176",
                        "name": "Lei Chen"
                    },
                    {
                        "authorId": "144115026",
                        "name": "Jiannong Cao"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "258bb0381c8427543c45a7e19bb5fef7ed0eea01",
                "externalIds": {
                    "DBLP": "conf/mm/ChenLZFJZ22",
                    "ArXiv": "2204.11582",
                    "DOI": "10.1145/3503161.3547859",
                    "CorpusId": 248377006
                },
                "corpusId": 248377006,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/258bb0381c8427543c45a7e19bb5fef7ed0eea01",
                "title": "Graph-DETR3D: Rethinking Overlapping Regions for Multi-View 3D Object Detection",
                "abstract": "3D object detection from multiple image views is a fundamental and challenging task for visual scene understanding. However, accurately detecting objects through perspective views in the 3D space is extremely difficult due to the lack of depth information. Recently, DETR3D introduces a novel 3D-2D query paradigm in aggregating multi-view images for 3D object detection and achieves state-of-the-art performance. In this paper, with intensive pilot experiments, we quantify the objects located at different regions and find that the \"truncated instances'' (i.e., at the border regions of each image) are the main bottleneck hindering the performance of DETR3D. Although it merges multiple features from two adjacent views in the overlapping regions, DETR3D still suffers from insufficient feature aggregation, thus missing the chance to fully boost the detection performance. In an effort to tackle the problem, we propose Graph-DETR3D to automatically aggregate multi-view imagery information through graph structure learning. It constructs a dynamic 3D graph between each object query and 2D feature maps to enhance the object representations, especially at the border regions. Besides, Graph-DETR3D benefits from a novel depth-invariant multi-scale training strategy, which maintains the visual depth consistency by simultaneously scaling the image size and the object depth. Extensive experiments on the nuScenes dataset demonstrate the effectiveness and efficiency of our Graph-DETR3D. Notably, our best model achieves 49.5 NDS on the nuScenes test leaderboard, achieving new state-of-the-art in comparison with various published image-view 3D object detectors.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48354360",
                        "name": "Zehui Chen"
                    },
                    {
                        "authorId": "2109724970",
                        "name": "Zhenyu Li"
                    },
                    {
                        "authorId": "2145404507",
                        "name": "Shiquan Zhang"
                    },
                    {
                        "authorId": "40901159",
                        "name": "Liangji Fang"
                    },
                    {
                        "authorId": "1617905616",
                        "name": "Qinhong Jiang"
                    },
                    {
                        "authorId": "2152338990",
                        "name": "Feng Zhao"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "ture; (2) masks or attention scores of edges [16], [17], [18], which are derived from the masking functions or attention networks to approximate the target prediction via the fractional (masked or attentive) graph; or (3) prediction changes on perturbed edges [4], [19], [20], which are fetched by per-",
                "cient to offer a global understanding of the GNN, such as class-wise explanations [17], [18].",
                "In this work, we follow the prior studies [14], [16], [17] and focus mainly on the structural features (i.",
                "Masks or attention scores of structural features [16], [17], [18]: The basic idea is to maximize the mutual",
                "ers, covering the gradient-based (SA [13], Grad-CAM [14]), masking-based (GNNExplainer [16]), attention-based (PGExplainer [17]), and perturbation-based (CXPlain [20], PGM-"
            ],
            "citingPaper": {
                "paperId": "6958980a4c4946d6575ee0dcbdf4cf38af59294e",
                "externalIds": {
                    "ArXiv": "2204.11028",
                    "DBLP": "journals/corr/abs-2204-11028",
                    "DOI": "10.1109/TPAMI.2022.3170302",
                    "CorpusId": 248376956,
                    "PubMed": "35471869"
                },
                "corpusId": 248376956,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6958980a4c4946d6575ee0dcbdf4cf38af59294e",
                "title": "Reinforced Causal Explainer for Graph Neural Networks",
                "abstract": "Explainability is crucial for probing graph neural networks (GNNs), answering questions like \u201cWhy the GNN model makes a certain prediction?\u201d. Feature attribution is a prevalent technique of highlighting the explanatory subgraph in the input graph, which plausibly leads the GNN model to make its prediction. Various attribution methods have been proposed to exploit gradient-like or attention scores as the attributions of edges, then select the salient edges with top attribution scores as the explanation. However, most of these works make an untenable assumption \u2014 the selected edges are linearly independent \u2014 thus leaving the dependencies among edges largely unexplored, especially their coalition effect. We demonstrate unambiguous drawbacks of this assumption \u2014 making the explanatory subgraph unfaithful and verbose. To address this challenge, we propose a reinforcement learning agent, Reinforced Causal Explainer (RC-Explainer). It frames the explanation task as a sequential decision process \u2014 an explanatory subgraph is successively constructed by adding a salient edge to connect the previously selected subgraph. Technically, its policy network predicts the action of edge addition, and gets a reward that quantifies the action\u2019s causal effect on the prediction. Such reward accounts for the dependency of the newly-added edge and the previously-added edges, thus reflecting whether they collaborate together and form a coalition to pursue better explanations. It is trained via policy gradient to optimize the reward stream of edge sequences. As such, RC-Explainer is able to generate faithful and concise explanations, and has a better generalization power to unseen graphs. When explaining different GNNs on three graph classification datasets, RC-Explainer achieves better or comparable performance to state-of-the-art approaches w.r.t. two quantitative metrics: predictive accuracy, contrastivity, and safely passes sanity checks and visual inspections. Codes and datasets are available at https://github.com/xiangwang1223/reinforced_causal_explainer.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "98285513",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "40507402",
                        "name": "Y. Wu"
                    },
                    {
                        "authorId": "2153659066",
                        "name": "An Zhang"
                    },
                    {
                        "authorId": "2163400298",
                        "name": "Fuli Feng"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    },
                    {
                        "authorId": "143779329",
                        "name": "Tat-Seng Chua"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-08570",
                    "ArXiv": "2204.08570",
                    "DOI": "10.48550/arXiv.2204.08570",
                    "CorpusId": 248239981
                },
                "corpusId": 248239981,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
                "title": "A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability",
                "abstract": "Graph Neural Networks (GNNs) have made rapid developments in the recent years. Due to their great ability in modeling graph-structured data, GNNs are vastly used in various applications, including high-stakes scenarios such as financial analysis, traffic predictions, and drug discovery. Despite their great potential in benefiting humans in the real world, recent study shows that GNNs can leak private information, are vulnerable to adversarial attacks, can inherit and magnify societal bias from training data and lack interpretability, which have risk of causing unintentional harm to the users and society. For example, existing works demonstrate that attackers can fool the GNNs to give the outcome they desire with unnoticeable perturbation on training graph. GNNs trained on social networks may embed the discrimination in their decision process, strengthening the undesirable societal bias. Consequently, trustworthy GNNs in various aspects are emerging to prevent the harm from GNN models and increase the users' trust in GNNs. In this paper, we give a comprehensive survey of GNNs in the computational aspects of privacy, robustness, fairness, and explainability. For each aspect, we give the taxonomy of the related methods and formulate the general frameworks for the multiple categories of trustworthy GNNs. We also discuss the future research directions of each aspect and connections between these aspects to help achieve trustworthiness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152961073",
                        "name": "Enyan Dai"
                    },
                    {
                        "authorId": "1999191869",
                        "name": "Tianxiang Zhao"
                    },
                    {
                        "authorId": "1643792176",
                        "name": "Huaisheng Zhu"
                    },
                    {
                        "authorId": "2150636336",
                        "name": "Jun Xu"
                    },
                    {
                        "authorId": "2149465392",
                        "name": "Zhimeng Guo"
                    },
                    {
                        "authorId": "2146672392",
                        "name": "Hui Liu"
                    },
                    {
                        "authorId": "1736632",
                        "name": "Jiliang Tang"
                    },
                    {
                        "authorId": "2893721",
                        "name": "Suhang Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "and effective description of the GNN [117], [118], [119]."
            ],
            "citingPaper": {
                "paperId": "9064845595d2fe7dd860c612050e4818a191ff62",
                "externalIds": {
                    "ArXiv": "2204.06127",
                    "DBLP": "journals/tetci/NieCW23",
                    "DOI": "10.1109/TETCI.2022.3222545",
                    "CorpusId": 248405972
                },
                "corpusId": 248405972,
                "publicationVenue": {
                    "id": "544cddb9-1149-469a-8377-d8c34f08d8b1",
                    "name": "IEEE Transactions on Emerging Topics in Computational Intelligence",
                    "alternate_names": [
                        "IEEE Trans Emerg Top Comput Intell"
                    ],
                    "issn": "2471-285X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=7433297",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7433297"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9064845595d2fe7dd860c612050e4818a191ff62",
                "title": "Reinforcement Learning on Graphs: A Survey",
                "abstract": "Graph mining tasks arise from many different application domains, including social networks, biological networks, transportation, and E-commerce, which have been receiving great attention from the theoretical and algorithmic design communities in recent years, and there has been some pioneering work employing the research-rich Reinforcement Learning (RL) techniques to address graph mining tasks. However, these fusion works are dispersed in different research domains, which makes them difficult to compare. In this survey, we provide a comprehensive overview of these fusion works and generalize these works to Graph Reinforcement Learning (GRL) as a unified formulation. We further discuss the applications of GRL methods across various domains, and simultaneously propose the key challenges and advantages of integrating graph mining and RL methods. Furthermore, we propose important directions and challenges to be solved in the future. To our knowledge, this is the latest work on a comprehensive survey of GRL, this work provides a global view and a learning resource for scholars. Based on our review, we create a collection of papers for both interested scholars who want to enter this rapidly developing domain and experts who would like to compare GRL methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2153114137",
                        "name": "Mingshuo Nie"
                    },
                    {
                        "authorId": "3252139",
                        "name": "Dongming Chen"
                    },
                    {
                        "authorId": "2111215516",
                        "name": "Dongqi Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "8d7f75e8e4c15ac402686b8e4fd153c4cc2b12c2",
                "externalIds": {
                    "DBLP": "journals/ijon/OnetoNBEMSBDBTS22",
                    "DOI": "10.1016/j.neucom.2022.04.072",
                    "CorpusId": 248146349
                },
                "corpusId": 248146349,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8d7f75e8e4c15ac402686b8e4fd153c4cc2b12c2",
                "title": "Towards learning trustworthily, automatically, and with guarantees on graphs: An overview",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1682762",
                        "name": "L. Oneto"
                    },
                    {
                        "authorId": "1715584",
                        "name": "Nicol\u00f3 Navarin"
                    },
                    {
                        "authorId": "1684175",
                        "name": "B. Biggio"
                    },
                    {
                        "authorId": "2142061424",
                        "name": "Federico Errica"
                    },
                    {
                        "authorId": "41231471",
                        "name": "A. Micheli"
                    },
                    {
                        "authorId": "47260481",
                        "name": "F. Scarselli"
                    },
                    {
                        "authorId": "144020416",
                        "name": "M. Bianchini"
                    },
                    {
                        "authorId": "65904555",
                        "name": "Luca Demetrio"
                    },
                    {
                        "authorId": "150257726",
                        "name": "P. Bongini"
                    },
                    {
                        "authorId": "1834340",
                        "name": "A. Tacchella"
                    },
                    {
                        "authorId": "1749815",
                        "name": "A. Sperduti"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "PGExplainer [14] was proposed to learn a mask predictor to obtain the edge masks for providing explanations.",
                "Consistent with prior works [11, 14, 34], we focus on explanations on graph structures.",
                "Other prior works, including GNNExplainer [32], PGExplainer [14], PGM-Explainer [25], SubgraphX [35], GraphMask [23], XGNN [34] and others [21] are provided in Appendix A.",
                "They are GNNExplainer [32], PGExplainer [14], and Gem [11]3."
            ],
            "citingPaper": {
                "paperId": "cfd1ba68e0dee2bbf86dd202e79587599939539c",
                "externalIds": {
                    "DBLP": "conf/cvpr/LinL0022",
                    "ArXiv": "2203.15209",
                    "DOI": "10.1109/CVPR52688.2022.01336",
                    "CorpusId": 247778655
                },
                "corpusId": 247778655,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cfd1ba68e0dee2bbf86dd202e79587599939539c",
                "title": "OrphicX: A Causality-Inspired Latent Variable Model for Interpreting Graph Neural Networks",
                "abstract": "This paper proposes a new eXplanation framework, called OrphicX, for generating causal explanations for any graph neural networks (GNNs) based on learned latent causal factors. Specifically, we construct a distinct generative model and design an objective function that encourages the generative model to produce causal, compact, and faithful explanations. This is achieved by isolating the causal factors in the latent space of graphs by maximizing the information flow measurements. We theoretically analyze the cause-effect relationships in the proposed causal graph, identify node attributes as confounders between graphs and GNN predictions, and circumvent such confounder effect by leveraging the backdoor adjustment formula. Our framework is compatible with any GNNs, and it does not require access to the process by which the target GNN produces its predictions. In addition, it does not rely on the linear-independence assumption of the explained features, nor require prior knowledge on the graph learning tasks. We show a proof-of-concept of OrphicX on canonical classification problems on graph data. In particular, we analyze the explanatory subgraphs obtained from explanations for molecular graphs (i.e., Mutag) and quantitatively evaluate the explanation performance with frequently occurring subgraph patterns. Empirically, we show that OrphicX can effectively identify the causal semantics for generating causal explanations, significantly outperforming its alternatives11This project is supported by the Internal Research Fund at The Hong Kong Polytechnic University P0035763. HW is partially supported by NSF Grant IIS-2127918 and an Amazon Faculty Research Award..",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2526241",
                        "name": "Wanyu Lin"
                    },
                    {
                        "authorId": "2105547066",
                        "name": "Hao Lan"
                    },
                    {
                        "authorId": "39483391",
                        "name": "Hao Wang"
                    },
                    {
                        "authorId": "2145520273",
                        "name": "Baochun Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "9757ac93ecd8b6d7c3b1609377d89000564959e3",
                "externalIds": {
                    "DBLP": "conf/isbi/ZhouHZSC22",
                    "ArXiv": "2204.13188",
                    "DOI": "10.1109/ISBI52829.2022.9761449",
                    "CorpusId": 248407604
                },
                "corpusId": 248407604,
                "publicationVenue": {
                    "id": "a38e0d3d-6929-4868-b4e4-af8bbacf711e",
                    "name": "IEEE International Symposium on Biomedical Imaging",
                    "type": "conference",
                    "alternate_names": [
                        "ISBI",
                        "International Symposium on Biomedical Imaging",
                        "Int Symp Biomed Imaging",
                        "IEEE Int Symp Biomed Imaging"
                    ],
                    "issn": "1945-7928",
                    "alternate_issns": [
                        "1945-8452"
                    ],
                    "url": "http://www.biomedicalimaging.org/",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9757ac93ecd8b6d7c3b1609377d89000564959e3",
                "title": "Interpretable Graph Convolutional Network Of Multi-Modality Brain Imaging For Alzheimer\u2019s Disease Diagnosis",
                "abstract": "Identification of brain regions related to the specific neurological disorders are of great importance for biomarker and diagnostic studies. In this paper, we propose an interpretable Graph Convolutional Network (GCN) framework for the identification and classification of Alzheimer\u2019s disease (AD) using multi-modality brain imaging data. Specifically, we extended the Gradient Class Activation Mapping (Grad-CAM) technique to quantify the most discriminative features identified by GCN from brain connectivity patterns. We then utilized them to find signature regions of interest (ROIs) by detecting the difference of features between regions in healthy control (HC), mild cognitive impairment (MCI), and AD groups. We conducted the experiments on the ADNI database with imaging data from three modalities, including VBM-MRI, FDG-PET, and AV45-PET, and showed that the ROI features learned by our method were effective for enhancing the performances of both clinical score prediction and disease status identification. It also successfully identified biomarkers associated with AD and MCI.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1914934360",
                        "name": "Houliang Zhou"
                    },
                    {
                        "authorId": "2148919788",
                        "name": "Lifang He"
                    },
                    {
                        "authorId": "115116889",
                        "name": "Yu Zhang"
                    },
                    {
                        "authorId": "38862163",
                        "name": "Li Shen"
                    },
                    {
                        "authorId": "2127888666",
                        "name": "Brian Chen"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "We employ several real-world datasets ENZYMES, Mutagenicity, PC-3, NCI109, NCI-H23H [40] and one synthesized dataset BA-2Motifs [39] for graph classification.",
                "Inspired by the recent research on GNN explainability [38], [39], we employ all perturbation operation DG 2 T1\u00f0G\u00de of the clean graph G (i.",
                "%) on BA-2Motifs\ncurrent samples unseen samples\nk 1 2 3 1 2 3\nClean 99.63 99.63 99.63 100.00 100.00 100.00 Rand 78.85 54.35 50.09 81.00 55.35 50.00 Grad 60.50 51.13 50.13 - - - RL-S2V 50.00 50.00 50.00 50.00 50.00 50.00 Ours 50.00 50.00 50.00 50.00 50.00 50.00\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE.",
                "To reveal the attack strategies of the projective ranking method, we visualize the perturbations in adversarial samples on the BA-2Motifs dataset based on the GCN model.",
                "BA-2Motifs is a synthetic dataset in which two motifs (House and Pentagon) are attached on random base graphs.",
                "nicity, PC-3, NCI109, NCI-H23H [40] and one synthesized dataset BA-2Motifs [39] for graph classification."
            ],
            "citingPaper": {
                "paperId": "257ec0c78839f08980f18559a412aa130ade617f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-12993",
                    "ArXiv": "2202.12993",
                    "DOI": "10.1109/TKDE.2022.3219209",
                    "CorpusId": 247158788
                },
                "corpusId": 247158788,
                "publicationVenue": {
                    "id": "c6840156-ee10-4d78-8832-7f8909811576",
                    "name": "IEEE Transactions on Knowledge and Data Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Knowl Data Eng"
                    ],
                    "issn": "1041-4347",
                    "url": "https://www.computer.org/web/tkde",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=69"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/257ec0c78839f08980f18559a412aa130ade617f",
                "title": "Projective Ranking-Based GNN Evasion Attacks",
                "abstract": "Graph neural networks (GNNs) offer promising learning methods for graph-related tasks. However, GNNs are at risk of adversarial attacks. Two primary limitations of the current evasion attack methods are highlighted: (1) The current GradArgmax ignores the \u201clong-term\u201d benefit of the perturbation. It is faced with zero-gradient and invalid benefit estimates in certain situations. (2) In reinforcement learning-based attack methods, the learned attack strategies might not be transferable when the attack budget changes. To this end, we first formulate the perturbation space and propose an evaluation framework and the projective ranking method. We aim to learn a powerful attack strategy then adapt it as little as possible to generate adversarial samples under dynamic budget settings. In our method, based on mutual information, we rank and assess the attack benefits of each perturbation for an effective attack strategy. By projecting the strategy, our method dramatically minimizes the cost of learning a new attack strategy when the attack budget changes. In the comparative assessment with GradArgmax and RL-S2V, the results show our method owns high attack performance and effective transferability. The visualization of our method also reveals various attack patterns in the generation of adversarial samples.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2156713249",
                        "name": "He Zhang"
                    },
                    {
                        "authorId": "3032058",
                        "name": "Xingliang Yuan"
                    },
                    {
                        "authorId": "1857210",
                        "name": "Chuan Zhou"
                    },
                    {
                        "authorId": "2153326034",
                        "name": "Shirui Pan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "based [23]\u2013[25], perturbation-based [26]\u2013[29], and surrogate methods [30]\u2013[32]."
            ],
            "citingPaper": {
                "paperId": "72e41c76bfb8b2c1e42b4a5297a84e7e259debbc",
                "externalIds": {
                    "ArXiv": "2202.08815",
                    "DBLP": "conf/ijcnn/PerottiBBP23",
                    "DOI": "10.1109/IJCNN54540.2023.10191053",
                    "CorpusId": 259375972
                },
                "corpusId": 259375972,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/72e41c76bfb8b2c1e42b4a5297a84e7e259debbc",
                "title": "Explaining Identity-aware Graph Classifiers through the Language of Motifs",
                "abstract": "Most methods for explaining black-box classifiers (e.g., on tabular data, images, or time series) rely on measuring the impact that removing/perturbing features has on the model output. This forces the explanation language to match the classifier's feature space. However, when dealing with graph data, in which the basic features correspond to the edges describing the graph structure, this matching between features space and explanation language might not be appropriate. Decoupling the feature space (edges) from a desired high-lever explanation language (such as motifs) is thus a major challenge towards developing actionable explanations for graph classification tasks. In this paper we introduce Graphshap, a Shapley-based approach able to provide motif-based explanations for identityaware graph classifiers, assuming no knowledge whatsoever about the model or its training data: the only requirement is that the classifier can be queried as a black-box at will. For the sake of computational efficiency we explore a progressive approximation strategy and show how a simple kernel can efficiently approximate explanation scores, thus allowing Graphshap to scale on scenarios with a large explanation space (i.e., large number of motifs). We showcase Graphshap on a real-world brain-network dataset consisting of patients affected by Autism Spectrum Disorder and a control group. Our experiments highlight how the classification provided by a black-box model can be effectively explained by few connectomics patterns.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "26582424",
                        "name": "A. Perotti"
                    },
                    {
                        "authorId": "46726748",
                        "name": "P. Bajardi"
                    },
                    {
                        "authorId": "2179558887",
                        "name": "Francesco Bonchi"
                    },
                    {
                        "authorId": "2735649",
                        "name": "A. Panisson"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "In a high-level view, recent state-of-the-art GNN explanation methods are based on either factual reasoning [24, 43] or counterfactual reasoning [22, 23].",
                "[24] requires explicit motif to generate explanations thus could not be applied on NCI1 and CiteSeer, which is the reason why it is not included.",
                "[24] assumed that the nitro group (NO2) and amino group (NH2) are the true reasons for mutaginicity and filtered out the mutagens that do not contain them.",
                "To solve the problem, PGExplainer [24] adopts the reparameterization trick and learns approximate discrete masks that maximizes the mutual information between key structures and predictions, and XGNN [44] generates a graph based on reinforcement learning to approximate the prediction of the original graph."
            ],
            "citingPaper": {
                "paperId": "4f0925684db82985f9c48566065d4ead5e00a16b",
                "externalIds": {
                    "ArXiv": "2202.08816",
                    "DBLP": "conf/www/TanGFGX0Z22",
                    "DOI": "10.1145/3485447.3511948",
                    "CorpusId": 246904359
                },
                "corpusId": 246904359,
                "publicationVenue": {
                    "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
                    "name": "The Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Web Conf",
                        "WWW"
                    ],
                    "url": "http://www.iw3c2.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4f0925684db82985f9c48566065d4ead5e00a16b",
                "title": "Learning and Evaluating Graph Neural Network Explanations based on Counterfactual and Factual Reasoning",
                "abstract": "Structural data well exists in Web applications, such as social networks in social media, citation networks in academic websites, and threads data in online forums. Due to the complex topology, it is difficult to process and make use of the rich information within such data. Graph Neural Networks (GNNs) have shown great advantages on learning representations for structural data. However, the non-transparency of the deep learning models makes it non-trivial to explain and interpret the predictions made by GNNs. Meanwhile, it is also a big challenge to evaluate the GNN explanations, since in many cases, the ground-truth explanations are unavailable. In this paper, we take insights of Counterfactual and Factual (CF2) reasoning from causal inference theory, to solve both the learning and evaluation problems in explainable GNNs. For generating explanations, we propose a model-agnostic framework by formulating an optimization problem based on both of the two casual perspectives. This distinguishes CF2 from previous explainable GNNs that only consider one of them. Another contribution of the work is the evaluation of GNN explanations. For quantitatively evaluating the generated explanations without the requirement of ground-truth, we design metrics based on Counterfactual and Factual reasoning to evaluate the necessity and sufficiency of the explanations. Experiments show that no matter ground-truth explanations are available or not, CF2 generates better explanations than previous state-of-the-art methods on real-world datasets. Moreover, the statistic analysis justifies the correlation between the performance on ground-truth evaluation and our proposed metrics.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110449137",
                        "name": "Juntao Tan"
                    },
                    {
                        "authorId": "1947101",
                        "name": "Shijie Geng"
                    },
                    {
                        "authorId": "2011378",
                        "name": "Zuohui Fu"
                    },
                    {
                        "authorId": "152988336",
                        "name": "Yingqiang Ge"
                    },
                    {
                        "authorId": "2111044480",
                        "name": "Shuyuan Xu"
                    },
                    {
                        "authorId": "48515097",
                        "name": "Yunqi Li"
                    },
                    {
                        "authorId": "1591136873",
                        "name": "Yongfeng Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "To restrict the size of subgraphs given by the explainer, we follow previous studies [15] to add a size regularization term R, computed as the averaged importance score, to the above objectives.",
                "In contrast, PGExplainer [15] performs inductive learning, i.",
                "Whereas existing studies focus on designing optimization approaches [34, 36] and explainer architectures [15] under the typical task-specific setting, our work focuses on an orthogonal problem to enable task-agnostic explanations with the proposed framework including the universal embedding explainer and conditioned learning objectives.",
                "Learning Inductive Task-agnostic # explainers required Gradient- & Rule-based No 1 GNNExplainer [34] Yes No No M \u2217N SubgraphX [36] Yes No No M \u2217N PGExplainer [15] Yes Yes No M Task-agnostic explainers Yes Yes Yes 1",
                "In our study, we follow [15], focusing on the importance of edges to provide explanations to GNNs.",
                "We do this by comparing TAGE with multiple baseline methods including non-learning-based methods GradCAM [20] and DeepLIFT [21], as well as learning-based methods GNNExplainer [34] and PGExplainer [15].",
                "In particular, perturbation methods involve learning or optimization [12, 13, 15, 34, 36] and, while bearing higher computational costs, generally achieve state-of-the-art performance in terms of explanation quality."
            ],
            "citingPaper": {
                "paperId": "e76317b5191b88d83758be322c026af77492de44",
                "externalIds": {
                    "ArXiv": "2202.08335",
                    "DBLP": "conf/nips/XieKTH0SJ22",
                    "CorpusId": 246904603
                },
                "corpusId": 246904603,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e76317b5191b88d83758be322c026af77492de44",
                "title": "Task-Agnostic Graph Explanations",
                "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools to encode graph-structured data. Due to their broad applications, there is an increasing need to develop tools to explain how GNNs make decisions given graph-structured data. Existing learning-based GNN explanation approaches are task-specific in training and hence suffer from crucial drawbacks. Specifically, they are incapable of producing explanations for a multitask prediction model with a single explainer. They are also unable to provide explanations in cases where the GNN is trained in a self-supervised manner, and the resulting representations are used in future downstream tasks. To address these limitations, we propose a Task-Agnostic GNN Explainer (TAGE) that is independent of downstream models and trained under self-supervision with no knowledge of downstream tasks. TAGE enables the explanation of GNN embedding models with unseen downstream tasks and allows efficient explanation of multitask models. Our extensive experiments show that TAGE can significantly speed up the explanation efficiency by using the same model to explain predictions for multiple downstream tasks while achieving explanation quality as good as or even better than current state-of-the-art GNN explanation approaches. Our code is pubicly available as part of the DIG library at https://github.com/divelab/DIG/tree/main/dig/xgraph/TAGE/.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "14629242",
                        "name": "Yaochen Xie"
                    },
                    {
                        "authorId": "47617256",
                        "name": "S. Katariya"
                    },
                    {
                        "authorId": "48784944",
                        "name": "Xianfeng Tang"
                    },
                    {
                        "authorId": "2057479333",
                        "name": "E-Wen Huang"
                    },
                    {
                        "authorId": "145850291",
                        "name": "Nikhil S. Rao"
                    },
                    {
                        "authorId": "2691095",
                        "name": "Karthik Subbian"
                    },
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Previous works on explainability in graph neural networks include gradient-based methods (Pope et al., 2019), graph decomposition (Schnake et al., 2021) graph perturbations (Ying et al., 2019; Luo et al., 2020), and a local approxima-\ntion using simpler models (Huang et al., 2020).",
                ", 2021) graph perturbations (Ying et al., 2019; Luo et al., 2020), and a local approximation using simpler models (Huang et al."
            ],
            "citingPaper": {
                "paperId": "577a350ade92578913245e2d474aeabcb576e6d6",
                "externalIds": {
                    "DBLP": "conf/icml/AliSEMMW22",
                    "ArXiv": "2202.07304",
                    "CorpusId": 246863594
                },
                "corpusId": 246863594,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/577a350ade92578913245e2d474aeabcb576e6d6",
                "title": "XAI for Transformers: Better Explanations through Conservative Propagation",
                "abstract": "Transformers have become an important workhorse of machine learning, with numerous applications. This necessitates the development of reliable methods for increasing their transparency. Multiple interpretability methods, often based on gradient information, have been proposed. We show that the gradient in a Transformer reflects the function only locally, and thus fails to reliably identify the contribution of input features to the prediction. We identify Attention Heads and LayerNorm as main reasons for such unreliable explanations and propose a more stable way for propagation through these layers. Our proposal, which can be seen as a proper extension of the well-established LRP method to Transformers, is shown both theoretically and empirically to overcome the deficiency of a simple gradient-based approach, and achieves state-of-the-art explanation performance on a broad range of Transformer models and datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112239702",
                        "name": "Ameen Ali"
                    },
                    {
                        "authorId": "90387439",
                        "name": "Thomas Schnake"
                    },
                    {
                        "authorId": "1557932201",
                        "name": "Oliver Eberle"
                    },
                    {
                        "authorId": "144535526",
                        "name": "G. Montavon"
                    },
                    {
                        "authorId": "2113612432",
                        "name": "Klaus-Robert M\u00fcller"
                    },
                    {
                        "authorId": "48519520",
                        "name": "Lior Wolf"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "02a3452a5f7fe42ba32bbf30af28b7845b2d6857",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-07082",
                    "ArXiv": "2202.07082",
                    "CorpusId": 246863422
                },
                "corpusId": 246863422,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/02a3452a5f7fe42ba32bbf30af28b7845b2d6857",
                "title": "Graph Neural Networks for Graphs with Heterophily: A Survey",
                "abstract": "Recent years have witnessed fast developments of graph neural networks (GNNs) that have benefited myriads of graph analytic tasks and applications. In general, most GNNs depend on the homophily assumption that nodes belonging to the same class are more likely to be connected. However, as a ubiquitous graph property in numerous real-world scenarios, heterophily, i.e., nodes with different labels tend to be linked, significantly limits the performance of tailor-made homophilic GNNs. Hence, \\textit{GNNs for heterophilic graphs} are gaining increasing attention in this community. To the best of our knowledge, in this paper, we provide a comprehensive review of GNNs for heterophilic graphs for the first time. Specifically, we propose a systematic taxonomy that essentially governs existing heterophilic GNN models, along with a general summary and detailed analysis. Furthermore, we summarize the mainstream heterophilic graph benchmarks to facilitate robust and fair evaluations. In the end, we point out the potential directions to advance and stimulate future research and applications on heterophilic graphs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2143713674",
                        "name": "Xin Zheng"
                    },
                    {
                        "authorId": "2116018493",
                        "name": "Yixin Liu"
                    },
                    {
                        "authorId": "2153326034",
                        "name": "Shirui Pan"
                    },
                    {
                        "authorId": "2112175772",
                        "name": "Miao Zhang"
                    },
                    {
                        "authorId": "1860892",
                        "name": "Di Jin"
                    },
                    {
                        "authorId": "152297693",
                        "name": "Philip S. Yu"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Based on the synthetic BAMotif graph classification task [58, 104] shown in Fig.",
                "To begin with, we construct 3-class synthetic datasets based on BAMotif [58] and follow Wu et al. [104] to inject spurious correlations between motif graph and base graph during the generation.",
                "To begin with, we construct 3-class synthetic datasets based on BAMotif [58] and follow Wu et al.",
                "We construct 3-class synthetic datasets based on BAMotif [116, 58] following [104], where the model needs to tell which one of three motifs (House, Cycle, Crane) that the graph contains."
            ],
            "citingPaper": {
                "paperId": "aa39c5a3080de756ad00648548e8f4faa2cfbf54",
                "externalIds": {
                    "DBLP": "conf/nips/0002ZB00XL0C22",
                    "ArXiv": "2202.05441",
                    "CorpusId": 252815553
                },
                "corpusId": 252815553,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/aa39c5a3080de756ad00648548e8f4faa2cfbf54",
                "title": "Learning Causally Invariant Representations for Out-of-Distribution Generalization on Graphs",
                "abstract": "Despite recent success in using the invariance principle for out-of-distribution (OOD) generalization on Euclidean data (e.g., images), studies on graph data are still limited. Different from images, the complex nature of graphs poses unique challenges to adopting the invariance principle. In particular, distribution shifts on graphs can appear in a variety of forms such as attributes and structures, making it difficult to identify the invariance. Moreover, domain or environment partitions, which are often required by OOD methods on Euclidean data, could be highly expensive to obtain for graphs. To bridge this gap, we propose a new framework, called Causality Inspired Invariant Graph LeArning (CIGA), to capture the invariance of graphs for guaranteed OOD generalization under various distribution shifts. Specifically, we characterize potential distribution shifts on graphs with causal models, concluding that OOD generalization on graphs is achievable when models focus only on subgraphs containing the most information about the causes of labels. Accordingly, we propose an information-theoretic objective to extract the desired subgraphs that maximally preserve the invariant intra-class information. Learning with these subgraphs is immune to distribution shifts. Extensive experiments on 16 synthetic or real-world datasets, including a challenging setting -- DrugOOD, from AI-aided drug discovery, validate the superior OOD performance of CIGA.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108962670",
                        "name": "Yongqiang Chen"
                    },
                    {
                        "authorId": "2109116068",
                        "name": "Yonggang Zhang"
                    },
                    {
                        "authorId": "2419616",
                        "name": "Yatao Bian"
                    },
                    {
                        "authorId": "50841357",
                        "name": "Han Yang"
                    },
                    {
                        "authorId": "47737190",
                        "name": "Kaili Ma"
                    },
                    {
                        "authorId": "2051756680",
                        "name": "Binghui Xie"
                    },
                    {
                        "authorId": "121698214",
                        "name": "Tongliang Liu"
                    },
                    {
                        "authorId": "2153287285",
                        "name": "Bo Han"
                    },
                    {
                        "authorId": "2116502293",
                        "name": "James Cheng"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Furthermore, we believe that a comparison of different explainability techniques like [65], [66], [67], [68] will provide even more insight into the differences between DP and non-DP training, which we also intend to investigate in future work."
            ],
            "citingPaper": {
                "paperId": "38be103cd559a4ad48ced78faa36358f9820c330",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-02575",
                    "ArXiv": "2202.02575",
                    "DOI": "10.1109/TPAMI.2022.3228315",
                    "CorpusId": 246634226,
                    "PubMed": "37015371"
                },
                "corpusId": 246634226,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/38be103cd559a4ad48ced78faa36358f9820c330",
                "title": "Differentially Private Graph Neural Networks for Whole-Graph Classification",
                "abstract": "Graph Neural Networks (GNNs) have established themselves as state-of-the-art for many machine learning applications such as the analysis of social and medical networks. Several among these datasets contain privacy-sensitive data. Machine learning with differential privacy is a promising technique to allow deriving insight from sensitive data while offering formal guarantees of privacy protection. However, the differentially private training of GNNs has so far remained under-explored due to the challenges presented by the intrinsic structural connectivity of graphs. In this work, we introduce a framework for differential private graph-level classification. Our method is applicable to graph deep learning on multi-graph datasets and relies on differentially private stochastic gradient descent (DP-SGD). We show results on a variety of datasets and evaluate the impact of different GNN architectures and training hyperparameters on model performance for differentially private graph classification, as well as the scalability of the method on a large medical dataset. Our experiments show that DP-SGD can be applied to graph classification tasks with reasonable utility losses. Furthermore, we apply explainability techniques to assess whether similar representations are learned in the private and non-private settings. Our results can also function as robust baselines for future work in this area.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2114359901",
                        "name": "Tamara T. Mueller"
                    },
                    {
                        "authorId": "1561434672",
                        "name": "J. Paetzold"
                    },
                    {
                        "authorId": "2124979072",
                        "name": "Chinmay Prabhakar"
                    },
                    {
                        "authorId": "2035505443",
                        "name": "Dmitrii Usynin"
                    },
                    {
                        "authorId": "1717710",
                        "name": "D. Rueckert"
                    },
                    {
                        "authorId": "9555451",
                        "name": "Georgios Kaissis"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "2d23a740ade86345d53bceda3326952be8659f28",
                "externalIds": {
                    "ArXiv": "2202.00519",
                    "DBLP": "journals/corr/abs-2202-00519",
                    "CorpusId": 246442194
                },
                "corpusId": 246442194,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2d23a740ade86345d53bceda3326952be8659f28",
                "title": "MotifExplainer: a Motif-based Graph Neural Network Explainer",
                "abstract": "We consider the explanation problem of Graph Neural Networks (GNNs). Most existing GNN explanation methods identify the most important edges or nodes but fail to consider substructures, which are more important for graph data. The only method that considers subgraphs tries to search all possible subgraphs and identify the most significant subgraphs. However, the subgraphs identified may not be recurrent or statistically important. In this work, we propose a novel method, known as MotifExplainer, to explain GNNs by identifying important motifs, recurrent and statistically significant patterns in graphs. Our proposed motif-based methods can provide better human-understandable explanations than methods based on nodes, edges, and regular subgraphs. Given an input graph and a pre-trained GNN model, our method first extracts motifs in the graph using well-designed motif extraction rules. Then we generate motif embedding by feeding motifs into the pre-trained GNN. Finally, we employ an attention-based method to identify the most influential motifs as explanations for the final prediction results. The empirical studies on both synthetic and real-world datasets demonstrate the effectiveness of our method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8873470",
                        "name": "Zhaoning Yu"
                    },
                    {
                        "authorId": "3920758",
                        "name": "Hongyang Gao"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Besides the models (Ying et al., 2019; Luo et al., 2020; Schlichtkrull et al., 2021; Yu et al., 2021) that we have compared with in detail in Sec.",
                "Other Related Works Besides the models (Ying et al., 2019; Luo et al., 2020; Schlichtkrull et al., 2021; Yu et al., 2021) that we have compared with in detail in Sec.",
                "For Mutag, we split it randomly into 80%/20% to train and validate models, and following (Luo et al., 2020) we use mutagen molecules with -NO2 or -NH2 as test data (because only these samples have explanation labels).",
                "Many works have been recently proposed to extract critical data patterns for the prediction by interpreting GNNs in post-hoc ways (Ying et al., 2019; Yuan et al., 2020a; Vu & Thai, 2020; Luo et al., 2020; Schlichtkrull et al., 2021; Yuan et al., 2021; Lin et al., 2021; Henderson et al., 2021).",
                "We compare interpretability with post-hoc methods GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), GraphMask (Schlichtkrull et al., 2021), and inherently interpretable models DIR (Wu et al., 2022) and IB-subgraph (Yu et al., 2021).",
                "Table 6 shows a direct comparison with PGExplainer and GNNExplainer between the interpretation ROC AUC reported in (Luo et al., 2020) and the performance of GSAT.",
                "Almost all previous GNN interpretation methods are posthoc, such as GNNExplainer (Ying et al., 2019), PGEx-\nplainer (Luo et al., 2020) and GraphMask (Schlichtkrull et al., 2021).",
                "BA-2Motifs (Luo et al., 2020) is a synthetic dataset, where the base graph is generated by Baraba\u0301si-Albert (BA) model.",
                "Over Ba-2Motifs and Mutag, GNNExplainer and PGExplainer work worse than what reported in (Luo et al., 2020) as we do not cherry pick the pre-trained model.",
                "Further Comparison on Interpretation Mechanism PGExplainer and GraphMask also have stochasticity in their models (Luo et al., 2020; Schlichtkrull et al., 2021).",
                "Following (Luo et al., 2020), -NO2 and -NH2 in mutagen graphs are labeled as ground-truth explanations.",
                "For post-hoc methods, the pre-trained models are also trained with 10 different random seeds instead of a fixed pre-trained model in (Luo et al., 2020).",
                "BA-2Motifs (Luo et al., 2020) is a synthetic dataset with binary graph labels.",
                "We use the tuned recommended settings from (Luo et al., 2020), including the temperature, the coefficient of `1-norm regularization and the coefficient of entropy regularization.",
                "PGExplainer and GraphMask also have stochasticity in their models (Luo et al., 2020; Schlichtkrull et al., 2021).",
                "GSAT is substantially different from previous methods, as we do not use any sparsity constraints such as `1-norm (Ying et al., 2019; Luo et al., 2020), `0-norm (Schlichtkrull et al.",
                "For interpretation evaluation, we report explanation ROC AUC following (Ying et al., 2019; Luo et al., 2020).",
                ", 2019), PGExplainer (Luo et al., 2020), GraphMask (Schlichtkrull et al.",
                "GSAT is substantially different from previous methods, as we do not use any sparsity constraints such as `1-norm (Ying et al., 2019; Luo et al., 2020), `0-norm (Schlichtkrull et al., 2021) or `2-regression to {0, 1} (Yu et al., 2021) to select size-constrained (or connectivity-constrained)\u2026",
                "plainer (Luo et al., 2020) and GraphMask (Schlichtkrull et al."
            ],
            "citingPaper": {
                "paperId": "68c25a2dcb4df9632996fdcb078ff3bed8300a9c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-12987",
                    "ArXiv": "2201.12987",
                    "CorpusId": 246430773
                },
                "corpusId": 246430773,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/68c25a2dcb4df9632996fdcb078ff3bed8300a9c",
                "title": "Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism",
                "abstract": "Interpretable graph learning is in need as many scientific applications depend on learning models to collect insights from graph-structured data. Previous works mostly focused on using post-hoc approaches to interpret pre-trained models (graph neural networks in particular). They argue against inherently interpretable models because the good interpretability of these models is often at the cost of their prediction accuracy. However, those post-hoc methods often fail to provide stable interpretation and may extract features that are spuriously correlated with the task. In this work, we address these issues by proposing Graph Stochastic Attention (GSAT). Derived from the information bottleneck principle, GSAT injects stochasticity to the attention weights to block the information from task-irrelevant graph components while learning stochasticity-reduced attention to select task-relevant subgraphs for interpretation. The selected subgraphs provably do not contain patterns that are spuriously correlated with the task under some assumptions. Extensive experiments on eight datasets show that GSAT outperforms the state-of-the-art methods by up to 20%$\\uparrow$ in interpretation AUC and 5%$\\uparrow$ in prediction accuracy. Our code is available at https://github.com/Graph-COM/GSAT.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151793768",
                        "name": "Siqi Miao"
                    },
                    {
                        "authorId": "2156102035",
                        "name": "Miaoyuan Liu"
                    },
                    {
                        "authorId": "1561672016",
                        "name": "Pan Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-12872",
                    "ArXiv": "2201.12872",
                    "CorpusId": 246431036
                },
                "corpusId": 246431036,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
                "title": "Discovering Invariant Rationales for Graph Neural Networks",
                "abstract": "Intrinsic interpretability of graph neural networks (GNNs) is to find a small subset of the input graph's features -- rationale -- which guides the model prediction. Unfortunately, the leading rationalization models often rely on data biases, especially shortcut features, to compose rationales and make predictions without probing the critical and causal patterns. Moreover, such data biases easily change outside the training distribution. As a result, these models suffer from a huge drop in interpretability and predictive performance on out-of-distribution data. In this work, we propose a new strategy of discovering invariant rationale (DIR) to construct intrinsically interpretable GNNs. It conducts interventions on the training distribution to create multiple interventional distributions. Then it approaches the causal rationales that are invariant across different distributions while filtering out the spurious patterns that are unstable. Experiments on both synthetic and real-world datasets validate the superiority of our DIR in terms of interpretability and generalization ability on graph classification over the leading baselines. Code and datasets are available at https://github.com/Wuyxin/DIR-GNN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "10593442",
                        "name": "Yingmin Wu"
                    },
                    {
                        "authorId": "98285513",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "2153659066",
                        "name": "An Zhang"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    },
                    {
                        "authorId": "143779329",
                        "name": "Tat-Seng Chua"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "BA2Motifs [25] contains graphs with a Barabasi-Albert (BA) base graph of size 20 and a 5-node motif in each graph.",
                "In fact, GNNExplainer, PGExplainer, and SubgraphX can never generate explanations including only disconnected -",
                "PGExplainer [25] uses the same scoring function as [41] but generates a discrete mask on edges by training an edge mask predictor.",
                "We compare with 5 strong baselines representing the SOTA methods for GNN explanation: GNNExplainer [41], PGExplainer [25], SubgraphX [44], GraphSVX [9], and OrphicX [21]."
            ],
            "citingPaper": {
                "paperId": "bb608cf0cfef9103f14f7d326be36e999dc88af5",
                "externalIds": {
                    "DBLP": "conf/nips/ZhangLSS22",
                    "ArXiv": "2201.12380",
                    "CorpusId": 248987580
                },
                "corpusId": 248987580,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/bb608cf0cfef9103f14f7d326be36e999dc88af5",
                "title": "GStarX: Explaining Graph Neural Networks with Structure-Aware Cooperative Games",
                "abstract": "Explaining machine learning models is an important and increasingly popular area of research interest. The Shapley value from game theory has been proposed as a prime approach to compute feature importance towards model predictions on images, text, tabular data, and recently graph neural networks (GNNs) on graphs. In this work, we revisit the appropriateness of the Shapley value for GNN explanation, where the task is to identify the most important subgraph and constituent nodes for GNN predictions. We claim that the Shapley value is a non-ideal choice for graph data because it is by definition not structure-aware. We propose a Graph Structure-aware eXplanation (GStarX) method to leverage the critical graph structure information to improve the explanation. Specifically, we define a scoring function based on a new structure-aware value from the cooperative game theory proposed by Hamiache and Navarro (HN). When used to score node importance, the HN value utilizes graph structures to attribute cooperation surplus between neighbor nodes, resembling message passing in GNNs, so that node importance scores reflect not only the node feature importance, but also the node structural roles. We demonstrate that GStarX produces qualitatively more intuitive explanations, and quantitatively improves explanation fidelity over strong baselines on chemical graph property prediction and text graph sentiment classification.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145408511",
                        "name": "Shichang Zhang"
                    },
                    {
                        "authorId": "152891495",
                        "name": "Yozen Liu"
                    },
                    {
                        "authorId": "145474474",
                        "name": "Neil Shah"
                    },
                    {
                        "authorId": "2109461904",
                        "name": "Yizhou Sun"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Moreover, many approaches [Ying et al., 2019, Luo et al., 2020, Schlichtkrull et al., 2021] involve the perturbation of graphs by means of atom removal, masking the graphs or edge cutting, but they do not take into account the fact that, in chemistry, removing an atom or a bond from a molecule\u2026"
            ],
            "citingPaper": {
                "paperId": "7b9bb3d50da64a0fe972b00ba759701bb218e87a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-05704",
                    "ArXiv": "2202.05704",
                    "CorpusId": 246822598
                },
                "corpusId": 246822598,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7b9bb3d50da64a0fe972b00ba759701bb218e87a",
                "title": "Semi-Supervised GCN for learning Molecular Structure-Activity Relationships",
                "abstract": "Since the introduction of artificial intelligence in medicinal chemistry, the necessity has emerged to analyse how molecular property variation is modulated by either single atoms or chemical groups. In this paper, we propose to train graph-to-graph neural network using semi-supervised learning for attributing structure-property relationships. As initial case studies we apply the method to solubility and molecular acidity while checking its consistency in comparison with known experimental chemical data. As final goal, our approach could represent a valuable tool to deal with problems such as activity cliffs, lead optimization and de-novo drug design.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2134230598",
                        "name": "Alessio Ragno"
                    },
                    {
                        "authorId": "2154412826",
                        "name": "Dylan Savoia"
                    },
                    {
                        "authorId": "2709822",
                        "name": "R. Capobianco"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Towards this end, a variety of explainer models are proposed for feature attribution (Selvaraju et al., 2017; Ying et al., 2019; Luo et al., 2020; Vu & Thai, 2020), which decomposes the predictor\u2019s prediction as contributions (i.",
                "Explainability of graph neural networks (GNNs) (Hamilton et al., 2017; Dwivedi et al., 2020) is crucial to model understanding and reliability in real-world applications, especially when about fairness and privacy (Ying et al., 2019; Luo et al., 2020).",
                "A prevalent solution is building an explainer model to conduct feature attribution (Ying et al., 2019; Luo et al., 2020; Pope et al., 2019).",
                "Specifically, Gs can be a structure-wise (Ying et al., 2019; Luo et al., 2020) or featurewise (Ying et al.",
                "Another line (Luo et al., 2020; Ying et al., 2019; Yuan et al., 2020a; Yue Zhang, 2020; Michael Sejr Schlichtkrull, 2021) learns the masks on graph features.",
                "Towards this end, a variety of explainer models are proposed for feature attribution (Selvaraju et al., 2017; Ying et al., 2019; Luo et al., 2020; Vu & Thai, 2020), which decomposes the predictor\u2019s prediction as contributions (i.e., importance) of its input features (e.g., edges, nodes).",
                "Going beyond the instance-wise explanation, PGExplainer (Luo et al., 2020) generates masks for multiple instances inductively.",
                ", 2020) is crucial to model understanding and reliability in real-world applications, especially when about fairness and privacy (Ying et al., 2019; Luo et al., 2020).",
                "By \u201cground-truth\u201d, we follow the prior studies (Ying et al., 2019; Yuan et al., 2020a; Luo et al., 2020) and treat the subgraphs coherent to the model knowledge (e.",
                "By \u201cground-truth\u201d, we follow the prior studies (Ying et al., 2019; Yuan et al., 2020a; Luo et al., 2020) and treat the subgraphs coherent to the model knowledge (e.g., the motif subgraphs in TR3) or human knowledge (e.g., the digit subgraphs in MNISTsup) as the ground-truth explanations.",
                "\u2026of the input features, which redistributes the probability of features according to their importance and sample the salient features as an explanatory subgraph Gs. Specifically, Gs can be a structure-wise (Ying et al., 2019; Luo et al., 2020) or featurewise (Ying et al., 2019) subgraph of G."
            ],
            "citingPaper": {
                "paperId": "d1be97e8d37dc9e1c1a386c6d2d2a7d5b069e28b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-08802",
                    "ArXiv": "2201.08802",
                    "CorpusId": 246210075
                },
                "corpusId": 246210075,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d1be97e8d37dc9e1c1a386c6d2d2a7d5b069e28b",
                "title": "Deconfounding to Explanation Evaluation in Graph Neural Networks",
                "abstract": "Explainability of graph neural networks (GNNs) aims to answer\"Why the GNN made a certain prediction?\", which is crucial to interpret the model prediction. The feature attribution framework distributes a GNN's prediction to its input features (e.g., edges), identifying an influential subgraph as the explanation. When evaluating the explanation (i.e., subgraph importance), a standard way is to audit the model prediction based on the subgraph solely. However, we argue that a distribution shift exists between the full graph and the subgraph, causing the out-of-distribution problem. Furthermore, with an in-depth causal analysis, we find the OOD effect acts as the confounder, which brings spurious associations between the subgraph importance and model prediction, making the evaluation less reliable. In this work, we propose Deconfounded Subgraph Evaluation (DSE) which assesses the causal effect of an explanatory subgraph on the model prediction. While the distribution shift is generally intractable, we employ the front-door adjustment and introduce a surrogate variable of the subgraphs. Specifically, we devise a generative model to generate the plausible surrogates that conform to the data distribution, thus approaching the unbiased estimation of subgraph importance. Empirical results demonstrate the effectiveness of DSE in terms of explanation fidelity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "10593442",
                        "name": "Yingmin Wu"
                    },
                    {
                        "authorId": "98285513",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "2153659066",
                        "name": "An Zhang"
                    },
                    {
                        "authorId": "2148950326",
                        "name": "Xia Hu"
                    },
                    {
                        "authorId": "2163400298",
                        "name": "Fuli Feng"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    },
                    {
                        "authorId": "143779329",
                        "name": "Tat-Seng Chua"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "7caaafd5a3ee033c98e792c7ea5b699d005753d5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-08164",
                    "ArXiv": "2201.08164",
                    "DOI": "10.1145/3583558",
                    "CorpusId": 246063780
                },
                "corpusId": 246063780,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7caaafd5a3ee033c98e792c7ea5b699d005753d5",
                "title": "From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI",
                "abstract": "The rising popularity of explainable artificial intelligence (XAI) to understand high-performing black boxes raised the question of how to evaluate explanations of machine learning (ML) models. While interpretability and explainability are often presented as a subjectively validated binary property, we consider it a multi-faceted concept. We identify 12 conceptual properties, such as Compactness and Correctness, that should be evaluated for comprehensively assessing the quality of an explanation. Our so-called Co-12 properties serve as categorization scheme for systematically reviewing the evaluation practices of more than 300 papers published in the past 7 years at major AI and ML conferences that introduce an XAI method. We find that one in three papers evaluate exclusively with anecdotal evidence, and one in five papers evaluate with users. This survey also contributes to the call for objective, quantifiable evaluation methods by presenting an extensive overview of quantitative XAI evaluation methods. Our systematic collection of evaluation methods provides researchers and practitioners with concrete tools to thoroughly validate, benchmark, and compare new and existing XAI methods. The Co-12 categorization scheme and our identified evaluation methods open up opportunities to include quantitative metrics as optimization criteria during model training to optimize for accuracy and interpretability simultaneously.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "17698891",
                        "name": "Meike Nauta"
                    },
                    {
                        "authorId": "52019849",
                        "name": "Jan Trienes"
                    },
                    {
                        "authorId": "66163851",
                        "name": "Shreyasi Pathak"
                    },
                    {
                        "authorId": "13407092",
                        "name": "Elisa Nguyen"
                    },
                    {
                        "authorId": "2066935841",
                        "name": "Michelle Peters"
                    },
                    {
                        "authorId": "2150574981",
                        "name": "Yasmin Schmitt"
                    },
                    {
                        "authorId": "3044872",
                        "name": "J\u00f6rg Schl\u00f6tterer"
                    },
                    {
                        "authorId": "1711719",
                        "name": "M. V. Keulen"
                    },
                    {
                        "authorId": "145566115",
                        "name": "C. Seifert"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "However, the PGExplainer explicitly works on edge masks and thus requires the GNN model to internally adjust edge weights, which was not applicable in our case.",
                "for relevant subgraphs and their motifs [28], [29], walks [30]",
                "In fact, the mentioned problematic was recently addressed by a method called PGExplainer [29].",
                "As a consequence, explanations may not reflect the global decisions made by the GNN classifier [29]."
            ],
            "citingPaper": {
                "paperId": "ef5bc7c78ec114c46053f168744ced4a4ee68c50",
                "externalIds": {
                    "DOI": "10.1101/2022.01.12.475995",
                    "CorpusId": 245939976,
                    "PubMed": "36124793"
                },
                "corpusId": 245939976,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/ef5bc7c78ec114c46053f168744ced4a4ee68c50",
                "title": "GNN-SubNet: disease subnetwork detection with explainable Graph Neural Networks",
                "abstract": "The tremendous success of graphical neural networks (GNNs) has already had a major impact on systems biology research. For example, GNNs are currently used for drug target recognition in protein-drug interaction networks as well as cancer gene discovery and more. Important aspects whose practical relevance is often underestimated are comprehensibility, interpretability, and explainability. In this work, we present a graph-based deep learning framework for disease subnetwork detection via explainable GNNs. In our framework, each patient is represented by the topology of a protein-protein network (PPI), and the nodes are enriched by molecular multimodal data, such as gene expression and DNA methylation. Therefore, our novel modification of the GNNexplainer for model-wide explanations can detect potential disease subnetworks, which is of high practical relevance. The proposed methods are implemented in the GNN-SubNet Python program, which we have made freely available on our GitHub for the international research community (https://github.com/pievos101/GNN-SubNet).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152300",
                        "name": "B. Pfeifer"
                    },
                    {
                        "authorId": "2149945485",
                        "name": "Afan Secic"
                    },
                    {
                        "authorId": "1947785",
                        "name": "Anna Saranti"
                    },
                    {
                        "authorId": "47596587",
                        "name": "Andreas Holzinger"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Here, also explanation techniques [49, 50] could yield interesting insights, in order to lead feature construction and modeling."
            ],
            "citingPaper": {
                "paperId": "544ff0949f3484a17276814fad226420739c0bb7",
                "externalIds": {
                    "DBLP": "journals/ijdsa/BloemheuvelHJMA23",
                    "ArXiv": "2201.00818",
                    "DOI": "10.1007/s41060-022-00349-6",
                    "CorpusId": 251969448
                },
                "corpusId": 251969448,
                "publicationVenue": {
                    "id": "c3875580-2cce-4a95-a094-fff60bb381b9",
                    "name": "International Journal of Data Science and Analysis",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Data Sci Anal",
                        "International Journal of Data Science and Analytics"
                    ],
                    "issn": "2575-1883",
                    "alternate_issns": [
                        "2364-4168"
                    ],
                    "url": "http://www.sciencepublishinggroup.com/journal/index?journalid=367",
                    "alternate_urls": [
                        "https://link.springer.com/journal/41060"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/544ff0949f3484a17276814fad226420739c0bb7",
                "title": "Graph neural networks for multivariate time series regression with application to seismic data",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "112846169",
                        "name": "Stefan Bloemheuvel"
                    },
                    {
                        "authorId": "2058938860",
                        "name": "J. V. D. Hoogen"
                    },
                    {
                        "authorId": "103938937",
                        "name": "Dario Jozinovi\u0107"
                    },
                    {
                        "authorId": "50159526",
                        "name": "A. Michelini"
                    },
                    {
                        "authorId": "2142735959",
                        "name": "Martin Atzmueller"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "We leverage existing works in network graphs and GNNs such as GNNExplainer and PGExplainer [11, 20] to mine common and influential substructures from sample input graphs.",
                "These substructures are compact and summarize important behaviors of a GNN[9, 15], and existing works such as GNNExplainer and PGExplainer [9, 15] can extract substructures that are important to each GNN prediction (e."
            ],
            "citingPaper": {
                "paperId": "380f3b660d7ce4a061ec26050876af5b221c9dce",
                "externalIds": {
                    "DBLP": "conf/icse/NguyenLNLH22",
                    "ArXiv": "2201.00115",
                    "DOI": "10.1145/3510455.3512780",
                    "CorpusId": 245650494
                },
                "corpusId": 245650494,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/380f3b660d7ce4a061ec26050876af5b221c9dce",
                "title": "Toward the Analysis of Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) have recently emerged as an effective framework for representing and analyzing graph-structured data. GNNs have been applied to many real-world problems such as knowledge graph analysis, social networks recommendation, and even COVID-19 detection and vaccine development. However, unlike other deep neural networks such as Feedforward Neural Networks (FFNNs), few verification and property inference techniques exist for GNNs. This is potentially due to dynamic behaviors of GNNs, which can take arbitrary graphs as input, whereas FFNNs which only take fixed size numerical vectors as inputs. This paper proposes GNN-Infer, an approach to analyze and infer properties of GNNs by extracting influential structures of the GNNs and then converting them into FFNNs. This allows us to leverage existing powerful FFNNs analyses to obtain results for the original GNNs. We discuss various designs of CNN-lnfer to ensure the scalability and accuracy of the conversions. We also illustrate CNN-Infer on a study case of node classification. We believe that CNN-Infer opens new research directions for understanding and analyzing GNNs. ACM Reference Format: Thanh-Dat Nguyen, Thanh Le-Cong, ThanhVu H. Nguyen, Xuan-Bach D. Le, and Quyet-Thang Huynh. 2022. Toward the Analysis of Graph Neural Networks. In New Ideas and Emerging Results (ICSE-NIER\u201922), May 21-29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3510455.3512780",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2238900980",
                        "name": "Thanh-Dat Nguyen"
                    },
                    {
                        "authorId": "1811413300",
                        "name": "Thanh Le-Cong"
                    },
                    {
                        "authorId": "2117823666",
                        "name": "Thanh-Hung Nguyen"
                    },
                    {
                        "authorId": "83263357",
                        "name": "X. Le"
                    },
                    {
                        "authorId": "2065718",
                        "name": "Quyet-Thang Huynh"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "We compare VGIB with various explanation model including GNNExplainer [49], PGExplainer [28], GraphMask [37], IGExplainer [41], GraphGrad-CAM [31] and GIB [52].",
                "In the explainability of Graph Neural Networks (GNNs), it is vital to generate the explanatory subgraph of the input, which faithfully interprets the predicted results [28, 49]."
            ],
            "citingPaper": {
                "paperId": "afe13acf0a5a5c126d0394e09a5a55616d581128",
                "externalIds": {
                    "ArXiv": "2112.09899",
                    "DBLP": "journals/corr/abs-2112-09899",
                    "DOI": "10.1109/CVPR52688.2022.01879",
                    "CorpusId": 245335028
                },
                "corpusId": 245335028,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/afe13acf0a5a5c126d0394e09a5a55616d581128",
                "title": "Improving Subgraph Recognition with Variational Graph Information Bottleneck",
                "abstract": "Subgraph recognition aims at discovering a compressed substructure of a graph that is most informative to the graph property. It can be formulated by optimizing Graph Information Bottleneck (GIB) with a mutual information estimator. However, GIB suffers from training instability and degenerated results due to its intrinsic optimization process. To tackle these issues, we reformulate the subgraph recognition problem into two steps: graph perturbation and subgraph selection, leading to a novel Variational Graph Information Bottleneck (VGIB) framework. VGIB first employs the noise injection to modulate the information flow from the input graph to the perturbed graph. Then, the perturbed graph is encouraged to be informative to the graph property. VGIB further obtains the desired subgraph by filtering out the noise in the perturbed graph. With the customized noise prior for each input, the VGIB objective is endowed with a tractable variational upper bound, leading to a superior empirical performance as well as theoretical properties. Extensive experiments on graph interpretation, explainability of Graph Neural Networks, and graph classification show that VGIB finds better subgraphs than existing methods11Code is avaliable on https://github.com/Samyu0304/VGIB.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "28822585",
                        "name": "Junchi Yu"
                    },
                    {
                        "authorId": "2109811424",
                        "name": "Jie Cao"
                    },
                    {
                        "authorId": "2053866626",
                        "name": "Ran He"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "To this end, several methods have been proposed to discover an important subgraph of the input for the explanation [35, 23, 37, 12].",
                "Note that M is a soft mask with continues values instead of a binary mask, we further employ an entropy constraint to encourage discrete values in M [23]:",
                "Furthermore, PGEXPLAINER learns a parametric model to generate explanations [23]."
            ],
            "citingPaper": {
                "paperId": "170ce0eebe1c6e65ecf70f2ded7864f6d4428f1f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-09895",
                    "ArXiv": "2112.09895",
                    "CorpusId": 245334653
                },
                "corpusId": 245334653,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/170ce0eebe1c6e65ecf70f2ded7864f6d4428f1f",
                "title": "Towards the Explanation of Graph Neural Networks in Digital Pathology with Information Flows",
                "abstract": "As Graph Neural Networks (GNNs) are widely adopted in digital pathology, there is increasing attention to developing explanation models (explainers) of GNNs for improved transparency in clinical decisions. Existing explainers discover an explanatory subgraph relevant to the prediction. However, such a subgraph is insufficient to reveal all the critical biological substructures for the prediction because the prediction will remain unchanged after removing that subgraph. Hence, an explanatory subgraph should be not only necessary for prediction, but also sufficient to uncover the most predictive regions for the explanation. Such explanation requires a measurement of information transferred from different input subgraphs to the predictive output, which we define as information flow. In this work, we address these key challenges and propose IFEXPLAINER, which generates a necessary and sufficient explanation for GNNs. To evaluate the information flow within GNN's prediction, we first propose a novel notion of predictiveness, named $f$-information, which is directional and incorporates the realistic capacity of the GNN model. Based on it, IFEXPLAINER generates the explanatory subgraph with maximal information flow to the prediction. Meanwhile, it minimizes the information flow from the input to the predictive result after removing the explanation. Thus, the produced explanation is necessarily important to the prediction and sufficient to reveal the most crucial substructures. We evaluate IFEXPLAINER to interpret GNN's predictions on breast cancer subtyping. Experimental results on the BRACS dataset show the superior performance of the proposed method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "28822585",
                        "name": "Junchi Yu"
                    },
                    {
                        "authorId": "1754673",
                        "name": "Tingyang Xu"
                    },
                    {
                        "authorId": "2053865709",
                        "name": "Ran He"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "We apply a generative probabilistic model to learn intrinsic underlying molecular graph structures as the topology-level augmentations, which are believed to make the most contribution to molecule representations readout from GNNs. Simultaneously, MolCLE also learns feature selectors that mask out unimportant atom features to generate attribute-level augmentations.",
                "Finally, we can notice that our MolCLE model shows a comparative performance to GROVER [47], which has the largest GNNs architecture tailored for molecular graph data with tens of millions of parameters.",
                "Recently, considering molecule naturally as graph-structure data, many works [27, 56] explore the flourished GNNs to encode molecular graphs into low-dimensional representations.",
                "Previous work [35] imposes an alternative solution, namely budget constraint, which applies a hard threshold B to limit the size of explanatory graphs.",
                "The intuition behind is obvious that the graph embedding with a well-augmented graph view as GNN\u2019s input should be closest to the original one.",
                "Due to the intractable number of potential subgraphs hindering the model from optimizing the objective directly [35], we follow Janson et al.",
                "Thus, we also apply the element-wise entropy [35, 68] constraint to increase the",
                "The significance of this claim has been empirically proved and explicitly introduced [35, 68].",
                "Inspired by previous works on explaining GNNs [35, 68, 70], we utilize a generative probabilistic model to obtain topological augmentations.",
                "2.2 Graph Neural Networks Recent success of GNNs [8, 12, 29] have aroused many attention to use these methods for analyzing various graph-structured data."
            ],
            "citingPaper": {
                "paperId": "0b72be2425ffb29c60cefcca0237e18b792fd558",
                "externalIds": {
                    "DBLP": "conf/bibm/WangMSW21",
                    "DOI": "10.1101/2021.12.03.471150",
                    "CorpusId": 244957640
                },
                "corpusId": 244957640,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/0b72be2425ffb29c60cefcca0237e18b792fd558",
                "title": "Molecular Graph Contrastive Learning with Parameterized Explainable Augmentations",
                "abstract": "Learning generalizable, transferable, and robust representations for molecule data has always been a challenge. The recent success of contrastive learning (CL) for self-supervised graph representation learning provides a novel perspective to learn molecule representations. The most prevailing graph CL framework is to maximize the agreement of representations in different augmented graph views. However, existing graph CL frameworks usually adopt stochastic augmentations or schemes according to pre-defined rules on the input graph to obtain different graph views in various scales (e.g. node, edge, and subgraph), which may destroy topological semantemes and domain prior in molecule data, leading to suboptimal performance. Therefore, designing parameterized, learnable, and explainable augmentation is quite necessary for molecular graph contrastive learning. A well-designed parameterized augmentation scheme can preserve chemically meaningful structural information and intrinsically essential attributes for molecule graphs, which helps to learn representations that are insensitive to perturbation on unimportant atoms and bonds. In this paper, we propose a novel Molecular Graph Contrastive Learning with Parameterized Explainable Augmentations, MolCLE for brevity, that self-adaptively incorporates chemically significative information from both topological and semantic aspects of molecular graphs. Specifically, we apply deep neural networks to parameterize the augmentation process for both the molecular graph topology and atom attributes, to highlight contributive molecular substructures and recognize underlying chemical semantemes. Comprehensive experiments on a variety of real-world datasets demonstrate that our proposed method consistently outperforms compared baselines, which verifies the effectiveness of the proposed framework. Detailedly, our self-supervised MolCLE model surpasses many supervised counterparts, and meanwhile only uses hundreds of thousands of parameters to achieve comparative results against the state-of-the-art baseline, which has tens of millions of parameters. We also provide detailed case studies to validate the explainability of augmented graph views. CCS CONCEPTS \u2022 Mathematics of computing \u2192 Graph algorithms; \u2022 Applied computing \u2192 Bioinformatics; \u2022 Computing methodologies \u2192 Neural networks; Unsupervised learning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2107962435",
                        "name": "Yingheng Wang"
                    },
                    {
                        "authorId": "66127288",
                        "name": "Yaosen Min"
                    },
                    {
                        "authorId": "2083204089",
                        "name": "Erzhuo Shao"
                    },
                    {
                        "authorId": "2115565474",
                        "name": "Ji Wu"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "For example, GNNExplainer (Ying et al. 2019) and PGExplainer (Luo et al. 2020) are proposed to select a compact subgraph structure that maximizes the mutual information with the GNN\u2019s predictions as the explanation.",
                "2019), perturbationbased methods (Ying et al. 2019; Luo et al. 2020; Yuan et al. 2021; Schlichtkrull, De Cao, and Titov 2020), decomposition methods (Schwarzenberg et al.",
                "We compare ProtGNN+ with two post-hoc methods: PGExplainer (Luo et al. 2020) and GNNExplainer (Ying et al. 2019).",
                "2019) and PGExplainer (Luo et al. 2020) are proposed to select a compact subgraph structure that maximizes the mutual information with the GNN\u2019s predictions as the explanation.",
                "\u2026several classes: gradients/features-based methods (Baldassarre and Azizpour 2019; Pope et al. 2019), perturbationbased methods (Ying et al. 2019; Luo et al. 2020; Yuan et al. 2021; Schlichtkrull, De Cao, and Titov 2020), decomposition methods (Schwarzenberg et al. 2019; Schnake et al. 2020),\u2026",
                "We compare ProtGNN+ with two post-hoc methods: PGExplainer (Luo et al. 2020) and GNNExplainer (Ying et al."
            ],
            "citingPaper": {
                "paperId": "7de413da6e0a00e14270cfaed2a31666e7c28747",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-00911",
                    "ArXiv": "2112.00911",
                    "DOI": "10.1609/aaai.v36i8.20898",
                    "CorpusId": 244798623
                },
                "corpusId": 244798623,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7de413da6e0a00e14270cfaed2a31666e7c28747",
                "title": "ProtGNN: Towards Self-Explaining Graph Neural Networks",
                "abstract": "Despite the recent progress in Graph Neural Networks (GNNs), it remains challenging to explain the predictions\n made by GNNs. Existing explanation methods mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations for a trained GNN. The fact that post-hoc methods fail to reveal the original reasoning process of GNNs raises the need of building GNNs with built-in interpretability. In this work, we propose Prototype Graph Neural Network (ProtGNN), which combines prototype learning with GNNs and provides a new perspective on the explanations of GNNs. In ProtGNN, the explanations are naturally derived from the case-based reasoning process and are actually used during classification. The prediction of ProtGNN is obtained by comparing the inputs to a few learned prototypes in the latent space.\n Furthermore, for better interpretability and higher efficiency, a novel conditional subgraph sampling module is incorporated to indicate which part of the input graph is most similar to each prototype in ProtGNN+. Finally, we evaluate our method on a wide range of datasets and perform concrete case studies. Extensive results show that ProtGNN and ProtGNN+ can provide inherent interpretability while achieving accuracy on par with the non-interpretable counterparts.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2129455190",
                        "name": "Zaixin Zhang"
                    },
                    {
                        "authorId": "2144831836",
                        "name": "Qi Liu"
                    },
                    {
                        "authorId": "2144219662",
                        "name": "Hao Wang"
                    },
                    {
                        "authorId": "46655401",
                        "name": "Chengqiang Lu"
                    },
                    {
                        "authorId": "153897134",
                        "name": "Chee-Kong Lee"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Existing methods are GNNExplainer [13], PGExplainer [14], GraphMask [15].",
                "Unfortunately, most existing methods that focus on edge-level or subgraph-level explanations such as GNNExplainer [13], PGExplainer [14],",
                "Unfortunately, most existing methods that focus on edge-level or subgraph-level explanations such as GNNExplainer [13], PGExplainer [14], and GraphMask [15] can not be used under the explanation supervision framework, as those explanations typically require additional objectives and optimization steps, making it not differentiable to the backbone model\u2019s parameters."
            ],
            "citingPaper": {
                "paperId": "a4bd7e965edb65d73f975bece566c7eb03e09f71",
                "externalIds": {
                    "DBLP": "conf/icdm/GaoSBYH021",
                    "DOI": "10.1109/ICDM51629.2021.00023",
                    "CorpusId": 244657872
                },
                "corpusId": 244657872,
                "publicationVenue": {
                    "id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
                    "name": "Industrial Conference on Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Ind Conf Data Min",
                        "ICDM"
                    ],
                    "url": "http://www.data-mining-forum.de/"
                },
                "url": "https://www.semanticscholar.org/paper/a4bd7e965edb65d73f975bece566c7eb03e09f71",
                "title": "GNES: Learning to Explain Graph Neural Networks",
                "abstract": "In recent years, graph neural networks (GNNs) and the research on their explainability are experiencing rapid developments and achieving significant progress. Many methods are proposed to explain the predictions of GNNs, focusing on \u201chow to generate explanations\u201d However, research questions like \u201cwhether the GNN explanations are inaccurate\u201d, \u201cwhat if the explanations are inaccurate\u201d, and \u201chow to adjust the model to generate more accurate explanations\u201d have not been well explored. To address the above questions, this paper proposes a GNN Explanation Supervision (GNES) 1 framework to adaptively learn how to explain GNNs more correctly. Specifically, our framework jointly optimizes both model prediction and model explanation by enforcing both whole graph regularization and weak supervision on model explanations. For the graph regularization, we propose a unified explanation formulation for both node-level and edge-level explanations by enforcing the consistency between them. The node- and edge-level explanation techniques we propose are also generic and rigorously demonstrated to cover several existing major explainers as special cases. Extensive experiments on five real-world datasets across two application domains demonstrate the effectiveness of the proposed model on improving the reasonability of the explanation while still keep or even improve the backbone GNNs model performance.1Code available at: https://github.com/YuyangGao/GNES.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110674789",
                        "name": "Yuyang Gao"
                    },
                    {
                        "authorId": "2113204748",
                        "name": "Tong Sun"
                    },
                    {
                        "authorId": "2068473088",
                        "name": "R. Bhatt"
                    },
                    {
                        "authorId": "2145103541",
                        "name": "Dazhou Yu"
                    },
                    {
                        "authorId": "2151797416",
                        "name": "S. Hong"
                    },
                    {
                        "authorId": "144000223",
                        "name": "Liang Zhao"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2022 PGExplainer (PGExp) [23] trains a deep neural network to parameterize the generation of explanations."
            ],
            "citingPaper": {
                "paperId": "50cfdcfc5b2cdf21d4e7ca9cdd9b74a426fa4671",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-14651",
                    "ArXiv": "2111.14651",
                    "DOI": "10.1109/ICDM51629.2021.00052",
                    "CorpusId": 244714677
                },
                "corpusId": 244714677,
                "publicationVenue": {
                    "id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
                    "name": "Industrial Conference on Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Ind Conf Data Min",
                        "ICDM"
                    ],
                    "url": "http://www.data-mining-forum.de/"
                },
                "url": "https://www.semanticscholar.org/paper/50cfdcfc5b2cdf21d4e7ca9cdd9b74a426fa4671",
                "title": "Multi-objective Explanations of GNN Predictions",
                "abstract": "Graph Neural Network (GNN) has achieved state-of-the-art performance in various high-stake prediction tasks, but multiple layers of aggregations on graphs with irregular structures make GNN a less interpretable model. Prior methods use simpler subgraphs to simulate the full model, or counterfactuals to identify the causes of a prediction. The two families of approaches aim at two distinct objectives, \u201csimulatability\u201d and \u201ccounterfactual relevance\u201d, but it is not clear how the objectives can jointly influence the human understanding of an explanation. We design a user-study to investigate such joint effects, and use the findings to design a multi-objective optimization (MOO) algorithm to find Pareto optimal explanations that are well-balanced in simulatability and counterfactual. Since the target model can be of any GNN variants and may not be accessible due to privacy concerns, we design a search algorithm using zero-th order information without accessing the architecture and parameters of the target model. Quantitative experiments on nine graphs from four applications demonstrate that the Pareto efficient explanations dominate single-objective baselines that use first-order continuous optimization or discrete combinatorial search. The explanations are further evaluated in robustness and sensitivity to show their capability of revealing convincing causes, while being cautious about the possible confounders. The diverse dominating counterfactuals can certify the feasibility of algorithmic recourse, that can potentially promote algorithmic fairness where humans are participating in the decision-making using GNN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108101960",
                        "name": "Yifei Liu"
                    },
                    {
                        "authorId": "2145762275",
                        "name": "Chao Chen"
                    },
                    {
                        "authorId": "2144399347",
                        "name": "Yazheng Liu"
                    },
                    {
                        "authorId": "47957054",
                        "name": "Xi Zhang"
                    },
                    {
                        "authorId": "3131378",
                        "name": "Sihong Xie"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Overall, for the BA-Shape dataset, in [10] 19% of labels flip, in [18] 39% of labels flip and in [15] 18% of labels flip.",
                "with several papers [18,13,15] using state-of-the-art explainer method [10] and its evaluation protocol as benchmark.",
                "We can observe three different types of masks that have been proposed, including soft masks (GNNExplainer [10], CF-GNNExplainer[18]), discrete masks (ZORRO [13]) and approximated discrete masks (PGExplainer [15])(3).",
                "For all 4 introduced explainer methods [10,18,13,15], the synthetic datasets BAShapes and Tree-Cycles are used for evaluation.",
                "In [13], accuracy is used, defined as the matching rate for important edges in explanations compared with those in the ground truthsd [15], an accuracy is formalized according to a binary classification task, where edges in the groundtruth explanation are treated as labels and the importance scores are viewed as prediction scores."
            ],
            "citingPaper": {
                "paperId": "a63fa4b6d85c5bb1037666a5877befb340b0591d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-12984",
                    "ArXiv": "2111.12984",
                    "DOI": "10.1007/978-3-030-93736-2_6",
                    "CorpusId": 244708879
                },
                "corpusId": 244708879,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a63fa4b6d85c5bb1037666a5877befb340b0591d",
                "title": "Demystifying Graph Neural Network Explanations",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1992914162",
                        "name": "Anna Himmelhuber"
                    },
                    {
                        "authorId": "2020545",
                        "name": "Mitchell Joblin"
                    },
                    {
                        "authorId": "2599096",
                        "name": "Martin Ringsquandl"
                    },
                    {
                        "authorId": "1727058",
                        "name": "T. Runkler"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Taking the classifying mutagenic property of a molecular graph (Luo et al., 2020) as an example, G is a molecular graph where the nodes are atoms and the edges are the chemical bonds between atoms, and Y is the class label, e.g., whether the molecule is mutagenic or not.",
                "\u2026causes Z such as the factors to generate nitrogen dioxide (NO2) group, which has an determinative effect on the mutagenicity of molecule, and the effect of irrelevant variable M , such as the carbon ring which exist more frequently in mutagenic molecule but not determinative (Luo et al., 2020).",
                "Taking the classifying mutagenic property of a molecular graph (Luo et al., 2020) as an example, G is a molecular graph where the nodes are atoms and the edges are the chemical bonds between atoms, and Y is the class label, e.",
                ", 1991), as we could explain the results based on the knowledge used in (Luo et al., 2020).",
                "StableGNN correctly identifies chemicalNO2 andNH2, which are known to be mutagenic (Luo et al., 2020) while baselines fail in.",
                "The whole molecular graph G is an effect of relevant latent causes Z such as the factors to generate nitrogen dioxide (NO2) group, which has an determinative effect on the mutagenicity of molecule, and the effect of irrelevant variable M , such as the carbon ring which exist more frequently in mutagenic molecule but not determinative (Luo et al., 2020).",
                "Moreover, we also conduct the experiments on a commonly used graph classification dataset, MUTAG (Debnath et al., 1991), as we could explain the results based on the knowledge used in (Luo et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "054f789db0e32ba2c6bda1d0029f35ea4b5bff2c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-10657",
                    "ArXiv": "2111.10657",
                    "CorpusId": 244478202
                },
                "corpusId": 244478202,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/054f789db0e32ba2c6bda1d0029f35ea4b5bff2c",
                "title": "Generalizing Graph Neural Networks on Out-Of-Distribution Graphs",
                "abstract": "Graph Neural Networks (GNNs) are proposed without considering the agnostic distribution shifts between training and testing graphs, inducing the degeneration of the generalization ability of GNNs on Out-Of-Distribution (OOD) settings. The fundamental reason for such degeneration is that most GNNs are developed based on the I.I.D hypothesis. In such a setting, GNNs tend to exploit subtle statistical correlations existing in the training set for predictions, even though it is a spurious correlation. However, such spurious correlations may change in testing environments, leading to the failure of GNNs. Therefore, eliminating the impact of spurious correlations is crucial for stable GNNs. To this end, we propose a general causal representation framework, called StableGNN. The main idea is to extract high-level representations from graph data first and resort to the distinguishing ability of causal inference to help the model get rid of spurious correlations. Particularly, we exploit a graph pooling layer to extract subgraph-based representations as high-level representations. Furthermore, we propose a causal variable distinguishing regularizer to correct the biased training distribution. Hence, GNNs would concentrate more on the stable correlations. Extensive experiments on both synthetic and real-world OOD graph datasets well verify the effectiveness, flexibility and interpretability of the proposed framework.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48635390",
                        "name": "Shaohua Fan"
                    },
                    {
                        "authorId": "2118449003",
                        "name": "Xiao Wang"
                    },
                    {
                        "authorId": "2113915032",
                        "name": "Chuan Shi"
                    },
                    {
                        "authorId": "143738684",
                        "name": "Peng Cui"
                    },
                    {
                        "authorId": "2156645170",
                        "name": "Bai Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[19]) PGExplainer uses a so-called explanation network on a universal embedding of the graph edges to obtain a transferable version of the EM.",
                "For graph-specific approaches we consider GNNExplainer [35], PGExplainer [19], and Graph-LRP [24], which all have been specifically designed to provide insights on GNNs."
            ],
            "citingPaper": {
                "paperId": "93644bbadee80e0f35f0e922d8fef87d24c4f572",
                "externalIds": {
                    "DBLP": "conf/ccs/GanzHWR21",
                    "DOI": "10.1145/3474369.3486866",
                    "CorpusId": 240001850
                },
                "corpusId": 240001850,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/93644bbadee80e0f35f0e922d8fef87d24c4f572",
                "title": "Explaining Graph Neural Networks for Vulnerability Discovery",
                "abstract": "Graph neural networks (GNNs) have proven to be an effective tool for vulnerability discovery that outperforms learning-based methods working directly on source code. Unfortunately, these neural networks are uninterpretable models, whose decision process is completely opaque to security experts, which obstructs their practical adoption. Recently, several methods have been proposed for explaining models of machine learning. However, it is unclear whether these methods are suitable for GNNs and support the task of vulnerability discovery. In this paper we present a framework for evaluating explanation methods on GNNs. We develop a set of criteria for comparing graph explanations and linking them to properties of source code. Based on these criteria, we conduct an experimental study of nine regular and three graph-specific explanation methods. Our study demonstrates that explaining GNNs is a non-trivial task and all evaluation criteria play a role in assessing their efficacy. We further show that graph-specific explanations relate better to code semantics and provide more information to a security expert than regular methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2135783537",
                        "name": "Tom Ganz"
                    },
                    {
                        "authorId": "2736329",
                        "name": "Martin H\u00e4rterich"
                    },
                    {
                        "authorId": "1750929757",
                        "name": "Alexander Warnecke"
                    },
                    {
                        "authorId": "144825749",
                        "name": "Konrad Rieck"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Following prior work [5, 22, 43], we formalize the notion of importance using mutual information (MI) and formulate our explanation module as the following optimization framework:",
                "In addition to grid-like data, interpretable graph neural networks have been studied to provide explanatory subgraphs for instances [22, 43] or classes [44].",
                "Table 4 shows the results of three explainers by formalizing quantitative interpretation evaluation as a classification problem [22, 43]."
            ],
            "citingPaper": {
                "paperId": "4de65fa36223233fccbe6846e546001d0d195095",
                "externalIds": {
                    "DBLP": "conf/cikm/DengR021",
                    "DOI": "10.1145/3459637.3482309",
                    "CorpusId": 237640235
                },
                "corpusId": 237640235,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4de65fa36223233fccbe6846e546001d0d195095",
                "title": "Understanding Event Predictions via Contextualized Multilevel Feature Learning",
                "abstract": "Deep learning models have been studied to forecast human events using vast volumes of data, yet they still cannot be trusted in certain applications such as healthcare and disaster assistance due to the lack of interpretability. Providing explanations for event predictions not only helps practitioners understand the underlying mechanism of prediction behavior but also enhances the robustness of event analysis. Improving the transparency of event prediction models is challenging given the following factors: (i) multilevel features exist in event data which creates a challenge to cross-utilize different levels of data; (ii) features across different levels and time steps are heterogeneous and dependent; and (iii) static model-level interpretations cannot be easily adapted to event forecasting given the dynamic and temporal characteristics of the data. Recent interpretation methods have proven their capabilities in tasks that deal with graph-structured or relational data. In this paper, we present a Contextualized Multilevel Feature learning framework, CMF, for interpretable temporal event prediction. It consists of a predictor for forecasting events of interest and an explanation module for interpreting model predictions. We design a new context-based feature fusion method to integrate multiple levels of heterogeneous features. We also introduce a temporal explanation module to determine sequences of text and subgraphs that have crucial roles in a prediction. We conduct extensive experiments on several real-world datasets of political and epidemic events. We demonstrate that the proposed method is competitive compared with the state-of-the-art models while possessing favorable interpretation capabilities.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "37202548",
                        "name": "Songgaojun Deng"
                    },
                    {
                        "authorId": "145344187",
                        "name": "H. Rangwala"
                    },
                    {
                        "authorId": "26426534",
                        "name": "Yue Ning"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "DatasetsWe employ one synthesized dataset BA-2Motifs [8] and five real-world datasets ENZYMES, Mutagenicity, PC-3, NCI109, NCI-H23H [11] for graph classification."
            ],
            "citingPaper": {
                "paperId": "40926357b37a37225390d1026adb734e6bf4ab69",
                "externalIds": {
                    "DBLP": "conf/cikm/ZhangWY0WYP21",
                    "DOI": "10.1145/3459637.3482161",
                    "CorpusId": 240230542
                },
                "corpusId": 240230542,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/40926357b37a37225390d1026adb734e6bf4ab69",
                "title": "Projective Ranking: A Transferable Evasion Attack Method on Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) have emerged as a series of effective learning methods for graph-related tasks. However, GNNs are shown vulnerable to adversarial attacks, where attackers can fool GNNs into making wrong predictions on adversarial samples with well-designed perturbations. Specifically, we observe that the current evasion attacks suffer from two limitations: (1) the attack strategy based on the reinforcement learning method might not be transferable when the attack budget changes; (2) the greedy mechanism in the vanilla gradient-based method ignores the long-term benefits of each perturbation operation. In this paper, we propose a new attack method named projective ranking to overcome the above limitations. Our idea is to learn a powerful attack strategy considering the long-term benefits of perturbations, then adjust it as little as possible to generate adversarial samples under different budgets. We further employ mutual information to measure the long-term benefits of each perturbation and rank them accordingly, so the learned attack strategy has better attack performance. Our method dramatically reduces the adaptation cost of learning a new attack strategy by projecting the attack strategy when the attack budget changes. Our preliminary evaluation results in synthesized and real-world datasets demonstrate that our method owns powerful attack performance and effective transferability.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1677195067",
                        "name": "He Zhang"
                    },
                    {
                        "authorId": "2115265646",
                        "name": "Bang Wu"
                    },
                    {
                        "authorId": "2112166824",
                        "name": "Xiangwen Yang"
                    },
                    {
                        "authorId": "1857210",
                        "name": "Chuan Zhou"
                    },
                    {
                        "authorId": "2117010331",
                        "name": "Shuo Wang"
                    },
                    {
                        "authorId": "3032058",
                        "name": "Xingliang Yuan"
                    },
                    {
                        "authorId": "2585415",
                        "name": "Shirui Pan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "9da452beb26e1f979032f8e9dca77eefd61202b6",
                "externalIds": {
                    "ArXiv": "2110.09823",
                    "CorpusId": 239049909
                },
                "corpusId": 239049909,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9da452beb26e1f979032f8e9dca77eefd61202b6",
                "title": "An Empirical Study: Extensive Deep Temporal Point Process",
                "abstract": "Temporal point process as the stochastic process on continuous domain of time is commonly used to model the asynchronous event sequence featuring with occurrence timestamps. Thanks to the strong expressivity of deep neural networks, they are emerging as a promising choice for capturing the patterns in asynchronous sequences, in the context of temporal point process. In this paper, we first review recent research emphasis and difficulties in modeling asynchronous event sequences with deep temporal point process, which can be concluded into four fields: encoding of history sequence, formulation of conditional intensity function, relational discovery of events and learning approaches for optimization. We introduce most of recently proposed models by dismantling them into the four parts, and conduct experiments by remodularizing the first three parts with the same learning strategy for a fair empirical evaluation. Besides, we extend the history encoders and conditional intensity function family, and propose a Granger causality discovery framework for exploiting the relations among multi-types of events. Because the Granger causality can be represented by the Granger causality graph, discrete graph structure learning in the framework of Variational Inference is employed to reveal latent structures of the graph. Further experiments show that the proposed framework with latent graph discovery can both capture the relations and achieve an improved fitting and predicting performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Haitao Lin"
                    },
                    {
                        "authorId": "2111728470",
                        "name": "Cheng Tan"
                    },
                    {
                        "authorId": "47767812",
                        "name": "Lirong Wu"
                    },
                    {
                        "authorId": "1962824654",
                        "name": "Zhangyang Gao"
                    },
                    {
                        "authorId": "1390908654",
                        "name": "Stan Z. Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "fd2c46892ed4d0a147a6f94353167238536b1290",
                "externalIds": {
                    "PubMedCentral": "8581570",
                    "DOI": "10.1016/j.xinn.2021.100176",
                    "CorpusId": 239552405,
                    "PubMed": "34806059"
                },
                "corpusId": 239552405,
                "publicationVenue": {
                    "id": "0d8eac9f-1ce5-459e-8d08-409fc8b4e36f",
                    "name": "The Innovation",
                    "type": "journal",
                    "alternate_names": [
                        "Innovation",
                        "Innov"
                    ],
                    "issn": "2666-6758",
                    "alternate_issns": [
                        "2576-2044",
                        "1025-8892"
                    ],
                    "url": "https://www.cell.com/the-innovation",
                    "alternate_urls": [
                        "https://innovation.ctor.press/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fd2c46892ed4d0a147a6f94353167238536b1290",
                "title": "Intelligent financial fraud detection practices in post-pandemic era",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1702020",
                        "name": "Xiaoqian Zhu"
                    },
                    {
                        "authorId": "2125319109",
                        "name": "Xiang Ao"
                    },
                    {
                        "authorId": "2106704739",
                        "name": "Zidi Qin"
                    },
                    {
                        "authorId": "2140408366",
                        "name": "Yanpeng Chang"
                    },
                    {
                        "authorId": "2152797691",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "2158548",
                        "name": "Qing He"
                    },
                    {
                        "authorId": "46276984",
                        "name": "Jianping Li"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                ", GNNExplainer [25], PGExplainer [17] and SubgraphX [28].",
                "Explanation methods can be broadly categorized as model-level explainers [17], [20], [26], which try to extract global explanatory patterns from the trained model, and instance-level algorithms [5], [12], [16], [25], [28], which try to explain individual predictions performed by the model.",
                "Yet, the results of PGExplainer over the model trained with MATE are comparable with the ones presented in [17].",
                "For the quantitative part, following [11], [17], [25] we compute the AUC score between the edges inside motifs, considered as positive edges, and the importance weights provided by the explanation methods.",
                "Concerning the explainers we use GNNExplainer [25], PGExplainer [17] and SubgraphX [28].",
                "PGExplainer [17] learns a parameterized model trained on the entire dataset to predict edge importance.",
                "However, the authors of [17] observed that carbon rings exist in both mutagen and nonmutagenic graphs."
            ],
            "citingPaper": {
                "paperId": "13a2ee8292908a8064abc2f43835a8dd5b11d695",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-09426",
                    "ArXiv": "2109.09426",
                    "DOI": "10.1109/TNNLS.2022.3171398",
                    "CorpusId": 237571970,
                    "PubMed": "35544494"
                },
                "corpusId": 237571970,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/13a2ee8292908a8064abc2f43835a8dd5b11d695",
                "title": "A Meta-Learning Approach for Training Explainable Graph Neural Networks",
                "abstract": "In this article, we investigate the degree of explainability of graph neural networks (GNNs). The existing explainers work by finding global/local subgraphs to explain a prediction, but they are applied after a GNN has already been trained. Here, we propose a meta-explainer for improving the level of explainability of a GNN directly at training time, by steering the optimization procedure toward minima that allow post hoc explainers to achieve better results, without sacrificing the overall accuracy of GNN. Our framework (called MATE, MetA-Train to Explain) jointly trains a model to solve the original task, e.g., node classification, and to provide easily processable outputs for downstream algorithms that explain the model's decisions in a human-friendly way. In particular, we meta-train the model's parameters to quickly minimize the error of an instance-level GNNExplainer trained on-the-fly on randomly sampled nodes. The final internal representation relies on a set of features that can be ``better'' understood by an explanation algorithm, e.g., another instance of GNNExplainer. Our model-agnostic approach can improve the explanations produced for different GNN architectures and use any instance-based explainer to drive this process. Experiments on synthetic and real-world datasets for node and graph classification show that we can produce models that are consistently easier to explain by different algorithms. Furthermore, this increase in explainability comes at no cost to the accuracy of the model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "115566972",
                        "name": "Indro Spinelli"
                    },
                    {
                        "authorId": "1752983",
                        "name": "Simone Scardapane"
                    },
                    {
                        "authorId": "1737292",
                        "name": "A. Uncini"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "4de91a958455b05f60c06bb28d3552c441e32204",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-07371",
                    "ArXiv": "2109.07371",
                    "DOI": "10.1109/ICDM51629.2021.00116",
                    "CorpusId": 237513810
                },
                "corpusId": 237513810,
                "publicationVenue": {
                    "id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
                    "name": "Industrial Conference on Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Ind Conf Data Min",
                        "ICDM"
                    ],
                    "url": "http://www.data-mining-forum.de/"
                },
                "url": "https://www.semanticscholar.org/paper/4de91a958455b05f60c06bb28d3552c441e32204",
                "title": "Self-learn to Explain Siamese Networks Robustly",
                "abstract": "Learning to compare two objects are essential in applications, especially when labeled data are scarce and imbalanced. As these applications can involve humans and make high-stake decisions, it is critical to explain the learned models. We aim to study post-hoc explanations of Siamese networks (SN) widely used in learning to compare. We characterize the instability of gradient-based explanations due to the additional compared object in SN, in contrast to architectures with a single input instance. We optimize for global invariance based on unlabeled data using self-learning to promote the stability of local explanations for individual input. The invariance leads to constrained optimization problems that can be solved using gradient descent-ascent (GDA), or KL-divergence regularized unconstrained optimization solved by SGD. We provide convergence proofs when the objective functions are nonconvex due to the Siamese architecture. Results on tabular and graph data from neuroscience and chemical engineering show that our local explanations robustly respects the self-learned invariance while optimizing the explanation faithfulness and simplicity. We further demonstrate the convergence of GDA experimentally.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145762275",
                        "name": "Chao Chen"
                    },
                    {
                        "authorId": "49746239",
                        "name": "Yifan Shen"
                    },
                    {
                        "authorId": "2068988549",
                        "name": "Guixiang Ma"
                    },
                    {
                        "authorId": "1833914",
                        "name": "Xiangnan Kong"
                    },
                    {
                        "authorId": "35065995",
                        "name": "S. Rangarajan"
                    },
                    {
                        "authorId": "2108287319",
                        "name": "Xi Zhang"
                    },
                    {
                        "authorId": "3131378",
                        "name": "Sihong Xie"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "For the graph explanation model E\u0434 with parameters \u03b8\u0434 , we use parameterized graph explainer (PGExplainer) model [19], minimizing the entropy loss:",
                "We use the recently proposed methods of concept learning [41] for explanations using textual data, and PGExplainer [19] for explanations using graphs data.",
                "We use the recently proposed methods of concept learning [40] for explanations using textual data, and PGExplainer [18] for explanations using graphs data.",
                "For the graph explanation model \ud835\udc38\ud835\udc54 with parameters \ud835\udf03\ud835\udc54 , we use parameterized graph explainer (PGExplainer) model [18], minimizing the entropy loss: L\ud835\udc38\ud835\udc54 (\ud835\udf03\ud835\udc54) = E\ud835\udc3a\ud835\udc46\u223c\ud835\udc5e (\ud835\udf03\ud835\udc54) [\ud835\udc3b (\ud835\udc74\ud835\udc90 (\u210e( \u00ae\ud835\udc65),\ud835\udc74\ud835\udc88 (\ud835\udc3a)) | \ud835\udc74\ud835\udc90 (\u210e( \u00ae\ud835\udc65),\ud835\udc74\ud835\udc88 (\ud835\udc3a\ud835\udc46 )))]\nwhere \ud835\udc3a\ud835\udc46 is explanation subgraph sampled from a distribution \ud835\udc5e parameterized by \ud835\udf03\ud835\udc54 ."
            ],
            "citingPaper": {
                "paperId": "bd30cff574fa8a8cf7c5f3f737e2e8717078babc",
                "externalIds": {
                    "DBLP": "conf/www/IslamB22",
                    "ArXiv": "2108.11656",
                    "DOI": "10.1145/3485447.3511941",
                    "CorpusId": 246824070
                },
                "corpusId": 246824070,
                "publicationVenue": {
                    "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
                    "name": "The Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Web Conf",
                        "WWW"
                    ],
                    "url": "http://www.iw3c2.org/"
                },
                "url": "https://www.semanticscholar.org/paper/bd30cff574fa8a8cf7c5f3f737e2e8717078babc",
                "title": "AR-BERT: Aspect-relation enhanced Aspect-level Sentiment Classification with Multi-modal Explanations",
                "abstract": "Aspect level sentiment classification (ALSC) is a difficult problem with state-of-the-art models showing less than 80% macro-F1 score on benchmark datasets. Existing models do not incorporate information on aspect-aspect relations in knowledge graphs (KGs), e.g. DBpedia. Two main challenges stem from inaccurate disambiguation of aspects to KG entities, and the inability to learn aspect representations from the large KGs in joint training with ALSC models. We propose AR-BERT, a novel two-level global-local entity embedding scheme that allows efficient joint training of KG-based aspect embeddings and ALSC models. A novel incorrect disambiguation detection technique addresses the problem of inaccuracy in aspect disambiguation. We also introduce the problem of determining mode significance in multi-modal explanation generation, and propose a two step solution. The proposed methods show a consistent improvement of 2.5 \u2212 4.1 percentage points, over the recent BERT-based baselines on benchmark datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2150050941",
                        "name": "Sk Mainul Islam"
                    },
                    {
                        "authorId": "1609913602",
                        "name": "Sourangshu Bhattacharya"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "PGExplainer [22] applies a parameterized explainer to generate the edge masks from a global view to identify the important subgraphs.",
                "data such as images and texts; while the work on interpretable GNNs for graph structured data are rather limited [14, 22, 45, 48].",
                "\u2022 PGExplainer [22]: It adopts a MLP-based explainer to obtain the important subgraphs from a global view to reduce the computation cost and obtain better explanations.",
                "Some initial efforts [14, 22, 45] have been taken to address this problem.",
                "There are few attempts of post-hoc explainers [14, 22, 45, 48] to provide explanations for trained GNNs; while the work on self-explainable GNNs is rather limited.",
                "BA-Shapes: To compare with the state-of-the-art GNN explainers [22, 45] which identify crucial subgraphs for predictions, we construct BA-Shapes following the setting in GNNExplainer [45]."
            ],
            "citingPaper": {
                "paperId": "a2aa6f3828c2d07381569310c70543f6624dbc7c",
                "externalIds": {
                    "ArXiv": "2108.12055",
                    "DBLP": "journals/corr/abs-2108-12055",
                    "DOI": "10.1145/3459637.3482306",
                    "CorpusId": 237347079
                },
                "corpusId": 237347079,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a2aa6f3828c2d07381569310c70543f6624dbc7c",
                "title": "Towards Self-Explainable Graph Neural Network",
                "abstract": "Graph Neural Networks (GNNs), which generalize the deep neural networks to graph-structured data, have achieved great success in modeling graphs. However, as an extension of deep learning for graphs, GNNs lack explainability, which largely limits their adoption in scenarios that demand the transparency of models. Though many efforts are taken to improve the explainability of deep learning, they mainly focus on i.i.d data, which cannot be directly applied to explain the predictions of GNNs because GNNs utilize both node features and graph topology to make predictions. There are only very few work on the explainability of GNNs and they focus on post-hoc explanations. Since post-hoc explanations are not directly obtained from the GNNs, they can be biased and misrepresent the true explanations. Therefore, in this paper, we study a novel problem of self-explainable GNNs which can simultaneously give predictions and explanations. We propose a new framework which can find K-nearest labeled nodes for each unlabeled node to give explainable node classification, where nearest labeled nodes are found by interpretable similarity module in terms of both node similarity and local structure similarity. Extensive experiments on real-world and synthetic datasets demonstrate the effectiveness of the proposed framework for explainable node classification.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "152961073",
                        "name": "Enyan Dai"
                    },
                    {
                        "authorId": "2116430057",
                        "name": "Suhang Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "result"
            ],
            "contexts": [
                "Furthermore, previous research in graphs tried attention-based explanation with weak results [18, 36] and other domains still investigate whether attention yields promising explanations without a definite answer yet.",
                "Follow-up works on GNN explanation pick up these datasets [13, 16, 18, 25, 31] or vary them slightly [7]."
            ],
            "citingPaper": {
                "paperId": "9b165b8abab60c01cd1eaabf58fd427f0e9ec97d",
                "externalIds": {
                    "DBLP": "conf/kdd/FaberMW21",
                    "DOI": "10.1145/3447548.3467283",
                    "CorpusId": 236980318
                },
                "corpusId": 236980318,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/9b165b8abab60c01cd1eaabf58fd427f0e9ec97d",
                "title": "When Comparing to Ground Truth is Wrong: On Evaluating GNN Explanation Methods",
                "abstract": "We study the evaluation of graph explanation methods. The state of the art to evaluate explanation methods is to first train a GNN, then generate explanations, and finally compare those explanations with the ground truth. We show five pitfalls that sabotage this pipeline because the GNN does not use the ground-truth edges. Thus, the explanation method cannot detect the ground truth. We propose three novel benchmarks: (i) pattern detection, (ii) community detection, and (iii) handling negative evidence and gradient saturation. In a re-evaluation of state-of-the-art explanation methods, we show paths for improving existing methods and highlight further paths for GNN explanation research.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "36352356",
                        "name": "Lukas Faber"
                    },
                    {
                        "authorId": "2123019027",
                        "name": "Amin K. Moghaddam"
                    },
                    {
                        "authorId": "1716440",
                        "name": "Roger Wattenhofer"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Note that similar observations on another representative explainer (PGExplainer [23]) can be found in Figure 7 (Refer to Appendix Section).",
                "In this section, in order to evaluate the effectiveness of our proposed attacking method on both GNNs and its explanations, we apply our proposed method to another representative explainer for the GNNs model (PGExplainer [23]), which adopts a deep model to parameterize the generation process of explanations in the inductive setting.",
                "Experimental results on two explainers (GNNEXPLAINER[20] and PGExplainer [23]) demonstrate that GEAttack achieves good performance for attacking GNN models, and adversarial edges generated by GEAttack are much harder to be detected by GNNEXPLAINER, which successfully achieve the joint attacks on a GNN model and its explanations.",
                "PGExplainer [23] is proposed to generate an explanation for each instance with a global understanding of the target GNN model in an inductive setting.",
                "Experimentally, the objective function of GNNEXPLAINER can be optimized to learn adjacency mask matrix MA and feature selection mask matrix MF in the following manner [20, 23]:",
                "In order to explain why a GNN model f\u03b8 predicts a given node vi\u2019s label as Y , the GNNEXPLAINER acts to provide a local interpretation GS = (AS ,XS) by highlighting the relevant features XS and the relevant subgraph structure AS for its prediction [20, 23].",
                "After all, the adversarial perturbations on graphs are highly correlated with the target label, since it is the perturbations that cause such malicious prediction [20, 23].",
                "In particular, given a trained GNN model and its prediction on a test node, GNNEXPLAINER [20, 23] will return a small subgraph together with a small subset of node features that are most influential for its prediction."
            ],
            "citingPaper": {
                "paperId": "e92fde4731f996d69abab06c9a2078513e4e11d0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-03388",
                    "ArXiv": "2108.03388",
                    "DOI": "10.1109/ICDE55515.2023.00056",
                    "CorpusId": 236956812
                },
                "corpusId": 236956812,
                "publicationVenue": {
                    "id": "764e3630-ddac-4c21-af4b-9d32ffef082e",
                    "name": "IEEE International Conference on Data Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "ICDE",
                        "Int Conf Data Eng",
                        "IEEE Int Conf Data Eng",
                        "International Conference on Data Engineering"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1331"
                },
                "url": "https://www.semanticscholar.org/paper/e92fde4731f996d69abab06c9a2078513e4e11d0",
                "title": "Jointly Attacking Graph Neural Network and its Explanations",
                "abstract": "Graph Neural Networks (GNNs) have boosted the performance for many graph-related tasks. Despite the great success, recent studies have shown that GNNs are still vulnerable to adversarial attacks, where adversaries can mislead the GNNs' prediction by modifying graphs. On the other hand, the explanation of GNNs (GnnExplainer for short) provides a better understanding of a trained GNN model by generating a small subgraph and features that are most influential for its prediction. In this paper, we first perform empirical studies to validate that GnnExplainer can act as an inspection tool and have the potential to detect the adversarial perturbations for graphs. This finding motivates us to further investigate a new problem: Whether a graph neural network and its explanations can be jointly attacked by modifying graphs with malicious desires? It is challenging to answer this question since the goals of adversarial attack and bypassing the GnnExplainer essentially contradict with each other. In this work, we give a confirmative answer for this question by proposing a novel attack framework (GEAttack) for graphs, which can attack both a GNN model and its explanations by exploiting their vulnerabilities simultaneously. To the best of our knowledge, this is the very first effort to attack both GNNs and explanations on graph-structured data for the trustworthiness of GNNs. Comprehensive experiments on various real-world datasets demonstrate the effectiveness of the proposed method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "41031455",
                        "name": "Wenqi Fan"
                    },
                    {
                        "authorId": "144767914",
                        "name": "Wei Jin"
                    },
                    {
                        "authorId": "2124928119",
                        "name": "Xiaorui Liu"
                    },
                    {
                        "authorId": "2018756699",
                        "name": "Han Xu"
                    },
                    {
                        "authorId": "48784944",
                        "name": "Xianfeng Tang"
                    },
                    {
                        "authorId": "2116430057",
                        "name": "Suhang Wang"
                    },
                    {
                        "authorId": "2117897052",
                        "name": "Qing Li"
                    },
                    {
                        "authorId": "1736632",
                        "name": "Jiliang Tang"
                    },
                    {
                        "authorId": "2110325165",
                        "name": "Jianping Wang"
                    },
                    {
                        "authorId": "1682418",
                        "name": "C. Aggarwal"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "PGExplainer (Luo et al., 2020) aims to address the issues of GNNExplainer.",
                "Recent research has attempted to improve the understanding of GNNs by producing various explainability techniques (Pope et al., 2019; Baldassarre & Azizpour, 2019; Ying et al., 2019; Schnake et al., 2020; Luo et al., 2020; Vu & Thai, 2020).",
                "We perform the experiments on the same set of datasets as GNNExplainer (Ying et al., 2019), as GNNExplainer has been established as a benchmark by subsequent research (Luo et al., 2020; Vu & Thai, 2020).",
                "More recent research has focused on GNNspecific explainability techniques, which take into account the graph structure (Ying et al., 2019; Luo et al., 2020; Vu & Thai, 2020)."
            ],
            "citingPaper": {
                "paperId": "61598046fe8dbdd58b40989f2eb1822e7a160c16",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-11889",
                    "ArXiv": "2107.11889",
                    "CorpusId": 236428348
                },
                "corpusId": 236428348,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/61598046fe8dbdd58b40989f2eb1822e7a160c16",
                "title": "GCExplainer: Human-in-the-Loop Concept-based Explanations for Graph Neural Networks",
                "abstract": "While graph neural networks (GNNs) have been shown to perform well on graph-based data from a variety of fields, they suffer from a lack of transparency and accountability, which hinders trust and consequently the deployment of such models in high-stake and safety-critical scenarios. Even though recent research has investigated methods for explaining GNNs, these methods are limited to single-instance explanations, also known as local explanations. Motivated by the aim of providing global explanations, we adapt the well-known Automated Concept-based Explanation approach (Ghorbani et al., 2019) to GNN node and graph classification, and propose GCExplainer. GCExplainer is an unsupervised approach for post-hoc discovery and extraction of global concept-based explanations for GNNs, which puts the human in the loop. We demonstrate the success of our technique on five node classification datasets and two graph classification datasets, showing that we are able to discover and extract high-quality concept representations by putting the human in the loop. We achieve a maximum completeness score of 1 and an average completeness score of 0.753 across the datasets. Finally, we show that the concept-based explanations provide an improved insight into the datasets and GNN models compared to the state-of-the-art explanations produced by GNNExplainer (Ying et al., 2019).",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2098834685",
                        "name": "Lucie Charlotte Magister"
                    },
                    {
                        "authorId": "1641643092",
                        "name": "Dmitry Kazhdan"
                    },
                    {
                        "authorId": "2118838286",
                        "name": "Vikash Singh"
                    },
                    {
                        "authorId": "144269589",
                        "name": "P. Lio\u2019"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "d22bfbd0c8d671567c8e3d3f21628fcf2ab8f5dd",
                "externalIds": {
                    "ArXiv": "2107.10234",
                    "CorpusId": 236924826
                },
                "corpusId": 236924826,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d22bfbd0c8d671567c8e3d3f21628fcf2ab8f5dd",
                "title": "Bridging the Gap between Spatial and Spectral Domains: A Unified Framework for Graph Neural Networks",
                "abstract": "Deep learning's performance has been extensively recognized recently. Graph neural networks (GNNs) are designed to deal with graph-structural data that classical deep learning does not easily manage. Since most GNNs were created using distinct theories, direct comparisons are impossible. Prior research has primarily concentrated on categorizing existing models, with little attention paid to their intrinsic connections. The purpose of this study is to establish a unified framework that integrates GNNs based on spectral graph and approximation theory. The framework incorporates a strong integration between spatial- and spectral-based GNNs while tightly associating approaches that exist within each respective domain.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2111361041",
                        "name": "Zhiqian Chen"
                    },
                    {
                        "authorId": "2149500622",
                        "name": "Fanglan Chen"
                    },
                    {
                        "authorId": "2152829032",
                        "name": "Lei Zhang"
                    },
                    {
                        "authorId": "9553002",
                        "name": "Taoran Ji"
                    },
                    {
                        "authorId": "3097079",
                        "name": "Kaiqun Fu"
                    },
                    {
                        "authorId": "2116734918",
                        "name": "Liang Zhao"
                    },
                    {
                        "authorId": "1399870469",
                        "name": "Feng Chen"
                    },
                    {
                        "authorId": "3008832",
                        "name": "Lingfei Wu"
                    },
                    {
                        "authorId": "1682418",
                        "name": "C. Aggarwal"
                    },
                    {
                        "authorId": "2110142089",
                        "name": "Chang-Tien Lu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Alternatively, works in (Ying et al. 2019; Vu and Thai 2020; Luo et al. 2020) focus on more complex approaches unique to GNN explainability, such as those based on mutual information maximisation, or Markov blanket conditional probabilities of feature explanations."
            ],
            "citingPaper": {
                "paperId": "d6f06d2a07fcb9429c0b3b7b97e71e77e2f8bd6b",
                "externalIds": {
                    "DBLP": "conf/aaai/GeorgievBKVL22",
                    "ArXiv": "2107.07493",
                    "DOI": "10.1609/aaai.v36i6.20623",
                    "CorpusId": 235899244
                },
                "corpusId": 235899244,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d6f06d2a07fcb9429c0b3b7b97e71e77e2f8bd6b",
                "title": "Algorithmic Concept-based Explainable Reasoning",
                "abstract": "Recent research on graph neural network (GNN) models successfully applied GNNs to classical graph algorithms and combinatorial optimisation problems. This has numerous benefits, such as allowing applications of algorithms when preconditions are not satisfied, or reusing learned models when sufficient training data is not available or can't be generated. Unfortunately, a key hindrance of these approaches is their lack of explainability, since GNNs are black-box models that cannot be interpreted directly. In this work, we address this limitation by applying existing work on concept-based explanations to GNN models. We introduce concept-bottleneck GNNs, which rely on a modification to the GNN readout mechanism. Using three case studies we demonstrate that: (i) our proposed model is capable of accurately learning concepts and extracting propositional formulas based on the learned concepts for each target class; (ii) our concept-based GNN models achieve comparative performance with state-of-the-art models; (iii) we can derive global graph concepts, without explicitly providing any supervision on graph-level concepts.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1713791843",
                        "name": "Dobrik Georgiev"
                    },
                    {
                        "authorId": "36648567",
                        "name": "Pietro Barbiero"
                    },
                    {
                        "authorId": "1641643092",
                        "name": "Dmitry Kazhdan"
                    },
                    {
                        "authorId": "1742197495",
                        "name": "Petar Velivckovi'c"
                    },
                    {
                        "authorId": "144269589",
                        "name": "P. Lio\u2019"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Additionally, to understand how any graph neural networks (GNNs) make a certain decision on graphstructured data, GNNExplainer learns soft masks for edges and node features to explain the predictions via maximizing the mutual information between the predictions of the original graph and those of the newly obtained graph [240, 372].",
                "Explanations of graph neural networks have been conducted on a set of molecules graph-labeled for their mutagenic efect on the Gram-negative bacterium Salmonella typhimurium, with the goal of identifying several known mutagenic functional groups \ufffd\ufffd2 and \ufffd\ufffd2 [240, 372, 378]."
            ],
            "citingPaper": {
                "paperId": "8b365890c0224f17fffb90bf33da46fccacd9331",
                "externalIds": {
                    "DBLP": "journals/tist/LiuWFLLJLJT23",
                    "ArXiv": "2107.06641",
                    "DOI": "10.1145/3546872",
                    "CorpusId": 235829506
                },
                "corpusId": 235829506,
                "publicationVenue": {
                    "id": "0d993d4a-09ba-4df8-90a4-7dfe25f0cb9e",
                    "name": "ACM Transactions on Intelligent Systems and Technology",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Intell Syst Technol"
                    ],
                    "issn": "2157-6904",
                    "url": "http://portal.acm.org/tist",
                    "alternate_urls": [
                        "https://tist.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8b365890c0224f17fffb90bf33da46fccacd9331",
                "title": "Trustworthy AI: A Computational Perspective",
                "abstract": "In the past few decades, artificial intelligence (AI) technology has experienced swift developments, changing everyone\u2019s daily life and profoundly altering the course of human society. The intention behind developing AI was and is to benefit humans by reducing labor, increasing everyday conveniences, and promoting social good. However, recent research and AI applications indicate that AI can cause unintentional harm to humans by, for example, making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against a group or groups. Consequently, trustworthy AI has recently garnered increased attention regarding the need to avoid the adverse effects that AI could bring to people, so people can fully trust and live in harmony with AI technologies. A tremendous amount of research on trustworthy AI has been conducted and witnessed in recent years. In this survey, we present a comprehensive appraisal of trustworthy AI from a computational perspective to help readers understand the latest technologies for achieving trustworthy AI. Trustworthy AI is a large and complex subject, involving various dimensions. In this work, we focus on six of the most crucial dimensions in achieving trustworthy AI: (i) Safety & Robustness, (ii) Nondiscrimination & Fairness, (iii) Explainability, (iv) Privacy, (v) Accountability & Auditability, and (vi) Environmental Well-being. For each dimension, we review the recent related technologies according to a taxonomy and summarize their applications in real-world systems. We also discuss the accordant and conflicting interactions among different dimensions and discuss potential aspects for trustworthy AI to investigate in the future.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2143856455",
                        "name": "Haochen Liu"
                    },
                    {
                        "authorId": "2108941389",
                        "name": "Yiqi Wang"
                    },
                    {
                        "authorId": "41031455",
                        "name": "Wenqi Fan"
                    },
                    {
                        "authorId": "1390612725",
                        "name": "Xiaorui Liu"
                    },
                    {
                        "authorId": "1527096073",
                        "name": "Yaxin Li"
                    },
                    {
                        "authorId": "39720946",
                        "name": "Shaili Jain"
                    },
                    {
                        "authorId": "1739705",
                        "name": "Anil K. Jain"
                    },
                    {
                        "authorId": "1736632",
                        "name": "Jiliang Tang"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "However, GNNs as a family of deep learning models are prone to overfitting and lack transparency in their predictions, which prevent their usage in decision-critical applications such as disease diagnosis.",
                "Previous methods usually produce a unique explanation subgraph for each graph subject (e.g. GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), and PGM-Explainer (Vu & Thai, 2020)), or through the model-level explanation (e.g. GAT (Velic\u030ckovic\u0301 et al., 2018)), that cannot drive diseasespecific explanation.",
                "A general approach to generate explanations for GNNs is to find a explanation graph G\u2032 that has the maximum mutual information with the label distribution Y , where G\u2032 can be a subgraph of G (Ying et al., 2019) or other alternations of G (Luo et al., 2020; Schlichtkrull et al., 2020; Yuan et al., 2020).",
                "To unleash the power of GNNs in brain network analysis and enable their interpretability, we propose BrainNNExplainer.",
                "Recently, Graph Neural Networks (GNNs) attract broad interests due to their established power in different downstream tasks (Kipf & Welling, 2017b; Xu et al., 2019; Velickovic et al., 2018; Yang et al., 2020a).",
                "Previous methods usually produce a unique explanation subgraph for each graph subject (e.g. GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), and PGM-Explainer (Vu & Thai, 2020)), or through the model-level explanation (e.g. GAT (Velic\u030ckovic\u0301 et al., 2018)), that cannot drive\u2026",
                "Compared with traditional shallow models such as MK-SVM, our backbone BrainNN outperforms them by large margins, with up to 11% absolute improvements on BP, which demonstrates the potential of using deep GNNs on brain networks.",
                "Although several approaches have been proposed to explain the predictions of GNNs (Ying et al., 2019; Luo et al., 2020; Vu & Thai, 2020; Yuan et al., 2020), none of them is equipped with a backbone GNN specifically designed for brain networks.",
                ", 2019), PGExplainer (Luo et al., 2020), and PGM-Explainer (Vu & Thai, 2020)), or through the model-level explanation (e.",
                "The lack of predictive original ROI features limits the power of GNNs (Cui et al., 2021).",
                "Since the brain region connectivity and correlations are encoded in realvalued edge weights, which can not be handled by existing GNNs, we design an edge-weight-aware message passing mechanism.",
                "\u2026approach to generate explanations for GNNs is to find a explanation graph G\u2032 that has the maximum mutual information with the label distribution Y , where G\u2032 can be a subgraph of G (Ying et al., 2019) or other alternations of G (Luo et al., 2020; Schlichtkrull et al., 2020; Yuan et al., 2020).",
                "Compared with shallow models, GNNs are promising for brain network analysis with more powerful representation abilities to capture the sophisticated brain network structures (Maron et al., 2018; Yang et al., 2019; 2020b).",
                ", 2019) or other alternations of G (Luo et al., 2020; Schlichtkrull et al., 2020; Yuan et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "0cae6ea780fdf7c24933e57e79150e0dfe0bfa0c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-05097",
                    "ArXiv": "2107.05097",
                    "CorpusId": 235795657
                },
                "corpusId": 235795657,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0cae6ea780fdf7c24933e57e79150e0dfe0bfa0c",
                "title": "BrainNNExplainer: An Interpretable Graph Neural Network Framework for Brain Network based Disease Analysis",
                "abstract": "Interpretable brain network models for disease prediction are of great value for the advancement of neuroscience. GNNs are promising to model complicated network data, but they are prone to overfitting and suffer from poor interpretability, which prevents their usage in decision-critical scenarios like healthcare. To bridge this gap, we propose BrainNNExplainer, an interpretable GNN framework for brain network analysis. It is mainly composed of two jointly learned modules: a backbone prediction model that is specifically designed for brain networks and an explanation generator that highlights disease-specific prominent brain network connections. Extensive experimental results with visualizations on two challenging disease prediction datasets demonstrate the unique interpretability and outstanding performance of BrainNNExplainer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2112821580",
                        "name": "Hejie Cui"
                    },
                    {
                        "authorId": "2054963003",
                        "name": "Wei Dai"
                    },
                    {
                        "authorId": "46758883",
                        "name": "Yanqiao Zhu"
                    },
                    {
                        "authorId": "2144456293",
                        "name": "Xiaoxiao Li"
                    },
                    {
                        "authorId": "40901818",
                        "name": "Lifang He"
                    },
                    {
                        "authorId": "1390553618",
                        "name": "Carl Yang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "f7231aee1e18428d6c0b314b5e1e65d6707e8747",
                "externalIds": {
                    "ArXiv": "2107.04086",
                    "DBLP": "journals/corr/abs-2107-04086",
                    "CorpusId": 235790538
                },
                "corpusId": 235790538,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f7231aee1e18428d6c0b314b5e1e65d6707e8747",
                "title": "Robust Counterfactual Explanations on Graph Neural Networks",
                "abstract": "Massive deployment of Graph Neural Networks (GNNs) in high-stake applications generates a strong demand for explanations that are robust to noise and align well with human intuition. Most existing methods generate explanations by identifying a subgraph of an input graph that has a strong correlation with the prediction. These explanations are not robust to noise because independently optimizing the correlation for a single input can easily overfit noise. Moreover, they do not align well with human intuition because removing an identified subgraph from an input graph does not necessarily change the prediction result. In this paper, we propose a novel method to generate robust counterfactual explanations on GNNs by explicitly modelling the common decision logic of GNNs on similar input graphs. Our explanations are naturally robust to noise because they are produced from the common decision boundaries of a GNN that govern the predictions of many similar input graphs. The explanations also align well with human intuition because removing the set of edges identified by an explanation from the input graph changes the prediction significantly. Exhaustive experiments on many public datasets demonstrate the superior performance of our method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2065738416",
                        "name": "Mohit Bajaj"
                    },
                    {
                        "authorId": "2074100",
                        "name": "Lingyang Chu"
                    },
                    {
                        "authorId": "2060445410",
                        "name": "Zihui Xue"
                    },
                    {
                        "authorId": "145525190",
                        "name": "J. Pei"
                    },
                    {
                        "authorId": "49680751",
                        "name": "Lanjun Wang"
                    },
                    {
                        "authorId": "23033976",
                        "name": "P. C. Lam"
                    },
                    {
                        "authorId": "2144288655",
                        "name": "Yong Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "PGExplainer [122] can also provided an explanation for each instance with a global view of the GNN model by incor-"
            ],
            "citingPaper": {
                "paperId": "70727c82f1ab97b3f64ee3e81e6e209c40fa0a02",
                "externalIds": {
                    "ArXiv": "2107.00272",
                    "DBLP": "journals/corr/abs-2107-00272",
                    "DOI": "10.1016/j.compmedimag.2021.102027",
                    "CorpusId": 235694722,
                    "PubMed": "34959100"
                },
                "corpusId": 235694722,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/70727c82f1ab97b3f64ee3e81e6e209c40fa0a02",
                "title": "A Survey on Graph-Based Deep Learning for Computational Histopathology",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1404441879",
                        "name": "David Ahmedt-Aristizabal"
                    },
                    {
                        "authorId": "2179032",
                        "name": "M. Armin"
                    },
                    {
                        "authorId": "1980700",
                        "name": "S. Denman"
                    },
                    {
                        "authorId": "3140440",
                        "name": "C. Fookes"
                    },
                    {
                        "authorId": "47773335",
                        "name": "L. Petersson"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "7675fc907efe86a45f3259784d644897dc95af1b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-04119",
                    "PubMedCentral": "9782255",
                    "ArXiv": "2107.04119",
                    "DOI": "10.1016/j.patter.2022.100628",
                    "CorpusId": 235790705,
                    "PubMed": "36569553"
                },
                "corpusId": 235790705,
                "publicationVenue": {
                    "id": "17bac89e-3dba-467a-b9d4-71e3baefb08b",
                    "name": "Patterns",
                    "type": "journal",
                    "issn": "2666-3899",
                    "url": "https://www.cell.com/patterns",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/patterns"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7675fc907efe86a45f3259784d644897dc95af1b",
                "title": "Quantitative evaluation of explainable graph neural networks for molecular property prediction",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144238145",
                        "name": "Jiahua Rao"
                    },
                    {
                        "authorId": "66119300",
                        "name": "Shuangjia Zheng"
                    },
                    {
                        "authorId": "7607711",
                        "name": "Yuedong Yang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "Recently, Multi-Layer Perceptrons (MLPs) have extensively been used as an explanation network in masking-based explanation methods (Luo et al., 2020; Schlichtkrull et al., 2020) to identify the edges which are affecting final predictions the most."
            ],
            "citingPaper": {
                "paperId": "89f054ea7277f2bc4605f4907b9b8d40b2d8f0e3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-11864",
                    "ArXiv": "2106.11864",
                    "CorpusId": 235593071
                },
                "corpusId": 235593071,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/89f054ea7277f2bc4605f4907b9b8d40b2d8f0e3",
                "title": "Towards Automated Evaluation of Explanations in Graph Neural Networks",
                "abstract": "Explaining Graph Neural Networks predictions to end users of AI applications in easily understandable terms remains an unsolved problem. In particular, we do not have well developed methods for automatically evaluating explanations, in ways that are closer to how users consume those explanations. Based on recent application trends and our own experiences in real world problems, we propose automatic evaluation approaches for GNN Explanations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2114429785",
                        "name": "Bannihati Kumar Vanya"
                    },
                    {
                        "authorId": "27526892",
                        "name": "Balaji Ganesan"
                    },
                    {
                        "authorId": "2114800291",
                        "name": "Aniket Saxena"
                    },
                    {
                        "authorId": "2118804097",
                        "name": "Devbrat Sharma"
                    },
                    {
                        "authorId": "2078528713",
                        "name": "Arvind Agarwal"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "\u2026Zitnik et al., 2018), and recent research has focused on developing methods to explain GNN predictions (Baldassarre and Azizpour, 2019; Pope et al., 2019; Ying et al., 2019; Huang et al., 2020; Luo et al., 2020; Vu and Thai, 2020; Schlichtkrull et al., 2021; Chen et al., 2021; Han et al., 2021).",
                "\u2026end, several approaches have been proposed in recent literature to explain the predictions of GNNs (Baldassarre and Azizpour, 2019; Faber et al., 2020; Huang et al., 2020; Lucic et al., 2021; Luo et al., 2020; Pope et al., 2019; Schlichtkrull et al., 2021; Vu and Thai, 2020; Ying et al., 2019).",
                "Based on the techniques they employ, these approaches can be broadly characterized into perturbation-based (Luo et al., 2020; Schlichtkrull et al., 2021; Ying et al., 2019), gradient-based (Simonyan et al.",
                "Recently, perturbation-based methods (Ying et al., 2019; Luo et al., 2020; Schlichtkrull et al., 2021) explain GNN predictions by observing the change in model predictions w.r.t. different input perturbations to study node and edge importance.",
                "To this end, several approaches have been proposed in recent literature to explain the predictions of GNNs (Baldassarre and Azizpour, 2019; Faber et al., 2020; Huang et al., 2020; Lucic et al., 2021; Luo et al., 2020; Pope et al., 2019; Schlichtkrull et al., 2021; Vu and Thai, 2020; Ying et al., 2019).",
                "In contrast to GNNExplainer, PGExplainer (Luo et al., 2020) generates explanation only on the graph structure.",
                "Recently, perturbation-based methods (Ying et al., 2019; Luo et al., 2020; Schlichtkrull et al., 2021) explain GNN predictions by observing the change in model predictions w.",
                "\u2026(Simonyan et al., 2014), Integrated Gradients (Sundararajan et al., 2017); perturbation-based: GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), GraphMASK (Schlichtkrull et al., 2021); and surrogate-based methods: GraphLIME (Huang et al., 2020), PGMExplainer (Vu and Thai,\u2026",
                "15 is intractable (Ying et al., 2019; Luo et al., 2020).",
                ", 2019), PGExplainer (Luo et al., 2020), GraphMASK (Schlichtkrull et al.",
                "Based on the techniques they employ, these approaches can be broadly characterized into perturbation-based (Luo et al., 2020; Schlichtkrull et al., 2021; Ying et al., 2019), gradient-based (Simonyan et al., 2014; Sundararajan et al., 2017), and surrogate-based (Huang et al., 2020; Vu and Thai,\u2026",
                ", 2018), and recent research has focused on developing methods to explain GNN predictions (Baldassarre and Azizpour, 2019; Pope et al., 2019; Ying et al., 2019; Huang et al., 2020; Luo et al., 2020; Vu and Thai, 2020; Schlichtkrull et al., 2021; Chen et al., 2021; Han et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "6a3c7bcda025a2d7723a429314bb9f76be9702c0",
                "externalIds": {
                    "DBLP": "conf/aistats/AgarwalZL22",
                    "ArXiv": "2106.09078",
                    "CorpusId": 247025974
                },
                "corpusId": 247025974,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6a3c7bcda025a2d7723a429314bb9f76be9702c0",
                "title": "Probing GNN Explainers: A Rigorous Theoretical and Empirical Analysis of GNN Explanation Methods",
                "abstract": "As Graph Neural Networks (GNNs) are increasingly being employed in critical real-world applications, several methods have been proposed in recent literature to explain the predictions of these models. However, there has been little to no work on systematically analyzing the reliability of these methods. Here, we introduce the first-ever theoretical analysis of the reliability of state-of-the-art GNN explanation methods. More specifically, we theoretically analyze the behavior of various state-of-the-art GNN explanation methods with respect to several desirable properties (e.g., faithfulness, stability, and fairness preservation) and establish upper bounds on the violation of these properties. We also empirically validate our theoretical results using extensive experimentation with nine real-world graph datasets. Our empirical results further shed light on several interesting insights about the behavior of state-of-the-art GNN explanation methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40228633",
                        "name": "Chirag Agarwal"
                    },
                    {
                        "authorId": "2095762",
                        "name": "M. Zitnik"
                    },
                    {
                        "authorId": "1892673",
                        "name": "Himabindu Lakkaraju"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Although those methods are designed to highlight important nodes or edges of the input graph for the target decision, their explanations often require additional models to be trained for generating graph masks [14, 15].",
                "Several perturbation-based methods are proposed including GNNExplainer [14], PGExplainer [15], ZORRO [30], GraphMask [16], and Causal Screening [31].",
                "[15], which divides the dataset into 80%, 10%, and 10% portions for training, validation, and testing, respectively.",
                "Dataset split is taken from the PGExplainer code [15], which splits train/validation/test sets by 80/10/10%."
            ],
            "citingPaper": {
                "paperId": "cf5ddf47514783f7e46a16dbdefd158a37183ced",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-08532",
                    "ArXiv": "2106.08532",
                    "CorpusId": 235446854
                },
                "corpusId": 235446854,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cf5ddf47514783f7e46a16dbdefd158a37183ced",
                "title": "SEEN: Sharpening Explanations for Graph Neural Networks using Explanations from Neighborhoods",
                "abstract": "Explaining the foundations for predictions obtained from graph neural networks (GNNs) is critical for credible use of GNN models for real-world problems. Owing to the rapid growth of GNN applications, recent progress in explaining predictions from GNNs, such as sensitivity analysis, perturbation methods, and attribution methods, showed great opportunities and possibilities for explaining GNN predictions. In this study, we propose a method to improve the explanation quality of node classification tasks that can be applied in a post hoc manner through aggregation of auxiliary explanations from important neighboring nodes, named SEEN. Applying SEEN does not require modification of a graph and can be used with diverse explainability techniques due to its independent mechanism. Experiments on matching motif-participating nodes from a given graph show great improvement in explanation accuracy of up to 12.71% and demonstrate the correlation between the auxiliary explanations and the enhanced explanation accuracy through leveraging their contributions. SEEN provides a simple but effective method to enhance the explanation quality of GNN model outputs, and this method is applicable in combination with most explainability techniques.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7752438",
                        "name": "Hyeoncheol Cho"
                    },
                    {
                        "authorId": "2072731671",
                        "name": "Youngrock Oh"
                    },
                    {
                        "authorId": "8426207",
                        "name": "Eunjoo Jeon"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "This style of edge dropping based on a random graph model has also been used for parameterized explanations of GNNs [58]."
            ],
            "citingPaper": {
                "paperId": "aa9ae8096216163ed40dd787917215b5ae4d3d90",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-05819",
                    "ArXiv": "2106.05819",
                    "CorpusId": 235391029
                },
                "corpusId": 235391029,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/aa9ae8096216163ed40dd787917215b5ae4d3d90",
                "title": "Adversarial Graph Augmentation to Improve Graph Contrastive Learning",
                "abstract": "Self-supervised learning of graph neural networks (GNN) is in great need because of the widespread label scarcity issue in real-world graph/network data. Graph contrastive learning (GCL), by training GNNs to maximize the correspondence between the representations of the same graph in its different augmented forms, may yield robust and transferable GNNs even without using labels. However, GNNs trained by traditional GCL often risk capturing redundant graph features and thus may be brittle and provide sub-par performance in downstream tasks. Here, we propose a novel principle, termed adversarial-GCL (AD-GCL), which enables GNNs to avoid capturing redundant information during the training by optimizing adversarial graph augmentation strategies used in GCL. We pair AD-GCL with theoretical explanations and design a practical instantiation based on trainable edge-dropping graph augmentation. We experimentally validate AD-GCL by comparing with the state-of-the-art GCL methods and achieve performance gains of up-to $14\\%$ in unsupervised, $6\\%$ in transfer, and $3\\%$ in semi-supervised learning settings overall with 18 different benchmark datasets for the tasks of molecule property regression and classification, and social network classification.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "31591568",
                        "name": "Susheel Suresh"
                    },
                    {
                        "authorId": "1561672016",
                        "name": "Pan Li"
                    },
                    {
                        "authorId": "145462792",
                        "name": "Cong Hao"
                    },
                    {
                        "authorId": "144050371",
                        "name": "Jennifer Neville"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "PGM and PGE are not included in the feature sparsity because they don\u2019t retrieve feature masks.",
                "On the other side, PGE performs worst out of all masks-based explainers.",
                "In terms of node sparsity, PGE outperforms all other soft-maskbased approaches.",
                "PGExplainer [24] employs a parameterized model to generate soft edge masks with node representations (extracted from target GNN) as input.",
                "Explainability approaches for explaining node level decisions include soft-masking approaches [11], [22], [24], [31], [32], [44], Shapely based approaches [8], [48], surrogate",
                "PGE performs worst\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE.",
                "First, we compare soft-masking approaches, i.e., gradient-based approaches, PGE, andGNNExplainer.",
                "Unlike our approach, PGExplainer is not model agnostic.",
                "Most of the existing approaches for explaining GNNs are based on soft-masking methods [11], [22], [24], [26], [32], [44].",
                "Since PGE and GNNExplainer return soft edge\nTABLE 1 Analysis of the Average Sparsity (Definition 2), RDT-Fidelity (Definition 4), and Validity (Definition 1) of the Explanations\nMetric Method Cora CiteSeer PubMed\nGCN GAT GIN APPNP GCN GAT GIN APPNP GCN GAT GIN APPNP\nFeatures-Sparsity GNNExplainer 7.27 7.27 7.27 7.27 8.21 8.21 8.21 8.21 6.21 6.21 6.21 6.21 Grad 4.08 4.22 4.45 4.08 4.19 4.28 4.41 4.18 4.41 4.51 4.89 4.46 GradInput 4.07 4.25 4.37 4.08 4.17 4.29 4.33 4.17 4.41 4.51 4.92 4.47 ZORRO \u00f0t \u00bc :85\u00de 1.91 2.29 3.51 2.26 1.81 1.84 3.67 1.97 1.60 1.52 2.38 1.75 ZORRO \u00f0t \u00bc :98\u00de 2.69 3.07 4.34 3.18 2.58 2.60 4.68 2.78 2.55 2.58 3.21 2.86\nNode-Sparsity GNNExplainer 2.48 2.49 2.56 2.51 1.67 1.67 1.70 1.68 2.7 2.71 2.71 2.71 PGM 2.06 1.82 1.66 1.99 1.47 1.59 1.10 1.54 1.64 1.16 1.62 2.93 PGE 1.86 1.86 1.78 1.94 1.48 1.40 1.36 1.41 1.91 1.81 1.85 1.92 Grad 2.48 2.34 2.25 2.35 1.70 1.61 1.55 1.60 2.91 2.76 3.11 2.73 GradInput 2.53 2.43 2.23 2.41 1.61 1.58 1.54 1.52 3.02 2.94 3.41 2.81 ZORRO \u00f0t \u00bc :85\u00de 1.28 1.30 1.90 1.16 1.05 0.92 1.36 0.83 1.07 0.87 1.77 0.79 ZORRO \u00f0t \u00bc :98\u00de 1.58 1.59 2.17 1.48 1.26 1.09 1.58 1.07 1.51 1.31 2.18 1.25\nRDT-Fidelity GNNExplainer 0.71 0.66 0.52 0.65 0.68 0.69 0.51 0.62 0.67 0.73 0.67 0.72 PGM 0.84 0.77 0.60 0.89 0.92 0.93 0.73 0.95 0.78 0.69 0.74 0.96 PGE 0.50 0.53 0.35 0.49 0.64 0.60 0.51 0.61 0.49 0.61 0.56 0.50 Grad 0.15 0.18 0.19 0.17 0.17 0.19 0.28 0.18 0.37 0.43 0.42 0.37 GradInput 0.15 0.18 0.18 0.16 0.16 0.18 0.26 0.17 0.36 0.42 0.42 0.36 Empty Explanation 0.15 0.18 0.18 0.16 0.16 0.18 0.26 0.17 0.36 0.42 0.42 0.36 ZORRO \u00f0t \u00bc :85\u00de 0.87 0.88 0.86 0.88 0.87 0.86 0.87 0.86 0.86 0.88 0.88 0.87 ZORRO \u00f0t \u00bc :98\u00de 0.97 0.97 0.96 0.97 0.97 0.97 0.97 0.96 0.96 0.97 0.97 0.96\nValidity GNNExplainer 0.89 0.95 0.83 0.84 0.87 0.92 0.58 0.93 0.60 0.81 0.71 0.87 PGM 0.89 0.90 0.64 0.94 0.95 0.95 0.76 0.97 0.86 0.80 0.62 0.97 PGE 0.51 0.54 0.34 0.45 0.62 0.59 0.54 0.62 0.51 0.61 0.57 0.48 Grad 0.26 0.25 0.15 0.18 0.28 0.25 0.12 0.26 0.36 0.49 0.50 0.38 GradInput 0.22 0.22 0.12 0.17 0.18 0.16 0.08 0.19 0.36 0.49 0.50 0.37 Empty Explanation 0.22 0.22 0.11 0.17 0.18 0.16 0.08 0.19 0.36 0.49 0.50 0.37 ZORRO \u00f0t \u00bc :85\u00de 1.00 1.00 0.83 1.00 1.00 1.00 0.77 1.00 0.90 1.00 0.84 1.00 ZORRO \u00f0t \u00bc :98\u00de 1.00 1.00 0.90 1.00 1.00 1.00 0.91 1.00 0.98 1.00 0.87 1.00\nThe smaller the explanation size larger is the sparsity.",
                "As PGE does not produce a feature mask, in other words, it selects all features, feature sparsity is not provided.",
                "For a comprehensive quantitative evaluation we chose our baselines from the three different categories of post-hoc explanations models consisting of (i) soft-masking approaches like GNNExplainer, which returns a continuous feature and edge mask and PGE [24] learns soft masks over edges in the graph (ii) surrogate model based hard-masking approach, PGM [40], which returns a binary node mask, iii) Shapely based hard masking approach SubgraphX [48], which returns a subgraph as an explanation, and (iv) gradient-basedmethodsGrad&GradInput [34] which utilize gradients to compute feature attribu-",
                "We can conclude\nTABLE 3 Experiments on Faithfulness According to [30] Measured With Kendall\u2019s tau tKendall of the Retrieved Explanation Precision and Test Accuracy\nMethod 1 200 400 600 1400 2000 tKendall\nGNNExplainer 0.50 0.54 0.41 0.40 0.37 0.40 0:73 PGM 0.83 0.47 0.68 0.71 0.76 0.75 0.20 PGE 0.20 0.19 0.23 0.21 0.23 0.20 0.36 Grad 0.94 0.80 0.62 0.73 0.84 0.87 0.07 GradInput 0.88 0.89 0.78 0.79 0.87 0.89 0.07 ZORRO \u00f0t \u00bc :85\u00de 0.00 0.92 0.88 0.93 0.94 0.94 0.73 ZORRO \u00f0t \u00bc :98\u00de 0.00 0.90 0.85 0.84 0.87 0.90 0.47 To simulate different model performances, we saved the GCN model during different epochs on the synthetic dataset.",
                "For a comprehensive quantitative evaluation we chose our baselines from the three different categories of post-hoc explanations models consisting of (i) soft-masking approaches like GNNExplainer, which returns a continuous feature and edge mask and PGE [24] learns soft masks over edges in the graph (ii) surrogate model based hard-masking approach, PGM [40], which returns a binary node mask, iii) Shapely based hard masking approach SubgraphX [48], which returns a subgraph as an explanation, and (iv) gradient-basedmethodsGrad&GradInput [34] which utilize gradients to compute feature attributions."
            ],
            "citingPaper": {
                "paperId": "7c85ad5f11ef9afb4f568c33304d86105da956ce",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-08621",
                    "ArXiv": "2105.08621",
                    "DOI": "10.1109/TKDE.2022.3201170",
                    "CorpusId": 234762791
                },
                "corpusId": 234762791,
                "publicationVenue": {
                    "id": "c6840156-ee10-4d78-8832-7f8909811576",
                    "name": "IEEE Transactions on Knowledge and Data Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Knowl Data Eng"
                    ],
                    "issn": "1041-4347",
                    "url": "https://www.computer.org/web/tkde",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=69"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7c85ad5f11ef9afb4f568c33304d86105da956ce",
                "title": "Zorro: Valid, Sparse, and Stable Explanations in Graph Neural Networks",
                "abstract": "With the ever-increasing popularity and applications of graph neural networks, several proposals have been made to explain and understand the decisions of a graph neural network. Explanations for graph neural networks differ in principle from other input settings. It is important to attribute the decision to input features and other related instances connected by the graph structure. We find that the previous explanation generation approaches that maximize the mutual information between the label distribution produced by the model and the explanation to be restrictive. Specifically, existing approaches do not enforce explanations to be valid, sparse, or robust to input perturbations. In this paper, we lay down some of the fundamental principles that an explanation method for graph neural networks should follow and introduce a metric RDT-Fidelity as a measure of the explanation's effectiveness. We propose a novel approach Zorro based on the principles from rate-distortion theory that uses a simple combinatorial procedure to optimize for RDT-Fidelity. Extensive experiments on real and synthetic datasets reveal that Zorro produces sparser, stable, and more faithful explanations than existing graph neural network explanation approaches.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143923185",
                        "name": "Thorben Funke"
                    },
                    {
                        "authorId": "35070805",
                        "name": "Megha Khosla"
                    },
                    {
                        "authorId": "39775488",
                        "name": "Avishek Anand"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "ac9f32ad8c188c35f3d174d8f18160582804f00d",
                "externalIds": {
                    "MAG": "3130274577",
                    "CorpusId": 236923472
                },
                "corpusId": 236923472,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ac9f32ad8c188c35f3d174d8f18160582804f00d",
                "title": "A Framework For Differentiable Discovery Of Graph Algorithms",
                "abstract": "Recently there is a surge of interests in using graph neural networks (GNNs) to learn algorithms. However, these works focus more on imitating existing algorithms, and are limited in two important aspects: the search space for algorithms is too small and the learned GNN models are not interpretable. To address these issues, we propose a novel framework which enlarge the search space using cheap global information from tree decomposition of the graphs, and can explain the structures of the graph leading to the decision of learned algorithms. We apply our framework to three NP-complete problems on graphs and show that the framework is able to discover effective and explainable algorithms.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2791430",
                        "name": "H. Dai"
                    },
                    {
                        "authorId": "66273798",
                        "name": "Xinshi Chen"
                    },
                    {
                        "authorId": "2116610243",
                        "name": "Yu Li"
                    },
                    {
                        "authorId": "2118502233",
                        "name": "Xin Gao"
                    },
                    {
                        "authorId": "1779453",
                        "name": "Le Song"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "GraphLIME and PGExplainer use sampling without any approximation guarantees.",
                "In this paper, we approach the explanation problem from a new angle, proposing a unified view that regroups existing explainers under a single framework: GNNExplainer, PGExplainer, GraphLIME, PGM-Explainer, XGNN, and the proposed GraphSVX.",
                "More recently, PGExplainer [21] and GraphMask [31] generalize GNNExplainer to an inductive setting; they use re-parametrisation tricks to alleviate the \u201cintroduced evidence\u201d problem [6]\u2014 i.e. continuous masks deform the adjacency matrix and introduce new semantics to the generated graph.",
                "More recently, PGExplainer [16] and GraphMask [26] generalize GNNExplainer to an inductive setting; they use re-parametrisation tricks to alleviate the \u201cintroduced evidence\u201d problem [5] \u2014 i.",
                "We compare the performance of GraphSVX to the main explanation baselines that incorporate graph structure in explanations, namely GNNExplainer, PGExplainer and PGM-Explainer.",
                "Overall, this often yields explanations with a poor signification, like a probability score stating how essential a variable is [16, 26, 33].",
                "Two other explainers provide even more general explanations: XGNN, a true model-level explanation method and PGExplainer, which provides collective and inductive explanations.",
                "In terms of efficiency, our explainer is slower than the scalable PGExplainer despite our efficient approximation, but is often comparable to GNNExplainer.",
                "Hence, we expect PGExplainer to perform better.",
                "Perturbation methods [16,26,33] monitor variations in model prediction with respect to different input perturbations.",
                "Comparing running times with baselines [21,42], GraphSVX is still slower than PGExplainer, which is very scalable, but matches GNNExplainer thanks to our efficient approximation, especially when the data is relatively sparse (e.g., BA-Shapes or Tree-Cycles).",
                "PGExplainer is very similar to GNNExplainer.",
                "We follow the same setting as [16] and [33], where four kinds of datasets are constructed.",
                "GNNExplainer, XGNN, GraphLIME and PGExplainer are simply selective.",
                "Regarding other baselines, PGExplainer and GNNExplainer output probability scores, PGM-Explainer a bayesian network, and XGNN a subgraph without further information.",
                "It supports only graph classification, requires passing a candidate node set as input and is challenged by local methods also providing global explanations [16].",
                "PGExplainer initialises randomly the parameters of the mask generator and uses intermediate model representations, which could respectively lead to local optimum and differences across similar models."
            ],
            "citingPaper": {
                "paperId": "6c26f638f0b16244f634e163562594993b06ec9b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-10482",
                    "ArXiv": "2104.10482",
                    "DOI": "10.1007/978-3-030-86520-7_19",
                    "CorpusId": 233324152
                },
                "corpusId": 233324152,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6c26f638f0b16244f634e163562594993b06ec9b",
                "title": "GraphSVX: Shapley Value Explanations for Graph Neural Networks",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "103022486",
                        "name": "Alexandre Duval"
                    },
                    {
                        "authorId": "2817467",
                        "name": "Fragkiskos D. Malliaros"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "For this end, PGExplainer (Luo et al., 2020) learns a multilayer perceptron (MLP) to explain multiple instances collectively.",
                "While explaining graph neural networks on graphs is still a nascent research topic, a few recent works have emerged (Luo et al., 2020; Vu & Thai, 2020; Ying et al., 2019; Yuan et al., 2020), each with its own perspective on this topic.",
                "We consider the state-of-the-art baselines that belong to the unified framework of additive feature attribution methods (The proof is provided in Appendix A) (Lundberg & Lee, 2017): GNNExplainer (Ying et al., 2019) and PGExplainer (Luo et al., 2020)2.",
                "We do not include gradient-based method (Ying et al., 2019), graph attention method (Velic\u030ckovic\u0301 et al., 2018), and Gradient (Pope et al., 2019), since previous explainers (Luo et al., 2020; Ying et al., 2019) have shown their superiority over these methods.",
                "For fair comparisons, we report the results of PGExplainer following its setting reported in (Luo et al., 2020) and compare them with the results of GNNExplainer and Gem when explaining on mutagen graphs, indicated as PGExplainer0, GNNExplainer-0, and Gem-0 in Table 2.",
                ", 2019), since previous explainers (Luo et al., 2020; Ying et al., 2019) have shown their superiority over these methods.",
                "For fair comparisons, we report the results of PGExplainer following its setting reported in (Luo et al., 2020) and com-"
            ],
            "citingPaper": {
                "paperId": "2a5a8db41940990dc8fe8e7717ed85ba043204e1",
                "externalIds": {
                    "DBLP": "conf/icml/LinLL21",
                    "ArXiv": "2104.06643",
                    "CorpusId": 233231366
                },
                "corpusId": 233231366,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2a5a8db41940990dc8fe8e7717ed85ba043204e1",
                "title": "Generative Causal Explanations for Graph Neural Networks",
                "abstract": "This paper presents Gem, a model-agnostic approach for providing interpretable explanations for any GNNs on various graph learning tasks. Specifically, we formulate the problem of providing explanations for the decisions of GNNs as a causal learning task. Then we train a causal explanation model equipped with a loss function based on Granger causality. Different from existing explainers for GNNs, Gem explains GNNs on graph-structured data from a causal perspective. It has better generalization ability as it has no requirements on the internal structure of the GNNs or prior knowledge on the graph learning tasks. In addition, Gem, once trained, can be used to explain the target GNN very quickly. Our theoretical analysis shows that several recent explainers fall into a unified framework of additive feature attribution methods. Experimental results on synthetic and real-world datasets show that Gem achieves a relative increase of the explanation accuracy by up to $30\\%$ and speeds up the explanation process by up to $110\\times$ as compared to its state-of-the-art alternatives.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2526241",
                        "name": "Wanyu Lin"
                    },
                    {
                        "authorId": "2105547066",
                        "name": "Hao Lan"
                    },
                    {
                        "authorId": "91269142",
                        "name": "Baochun Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                ", 2019), PGExplainer (Luo et al., 2020), DeepLIFT (Shrikumar et al.",
                "For data interfaces, we consider the widely used synthetic datasets (i.e., BA-shapes, BA-Community, etc.) (Ying et al., 2019; Luo et al., 2020) and molecule datasets (i.e., BBBP, Tox21, etc.) (Wu et al., 2018).",
                "We include the following algorithms: GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), DeepLIFT (Shrikumar et al., 2017), GNN-LRP (Schnake et al., 2020), Grad-CAM (Pope et al., 2019), SubgraphX (Yuan et al., 2021), and XGNN (Yuan et al., 2020a)."
            ],
            "citingPaper": {
                "paperId": "323ca85c5c1ebd0ec9bf74897d8c8e8fbf203ae6",
                "externalIds": {
                    "ArXiv": "2103.12608",
                    "DBLP": "journals/jmlr/LiuLWXYGYXZLYLF21",
                    "CorpusId": 232320529
                },
                "corpusId": 232320529,
                "publicationVenue": {
                    "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                    "name": "Journal of machine learning research",
                    "type": "journal",
                    "alternate_names": [
                        "Journal of Machine Learning Research",
                        "J mach learn res",
                        "J Mach Learn Res"
                    ],
                    "issn": "1532-4435",
                    "alternate_issns": [
                        "1533-7928"
                    ],
                    "url": "http://www.ai.mit.edu/projects/jmlr/",
                    "alternate_urls": [
                        "http://jmlr.csail.mit.edu/",
                        "http://www.jmlr.org/",
                        "http://portal.acm.org/affiliated/jmlr"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/323ca85c5c1ebd0ec9bf74897d8c8e8fbf203ae6",
                "title": "DIG: A Turnkey Library for Diving into Graph Deep Learning Research",
                "abstract": "Although there exist several libraries for deep learning on graphs, they are aiming at implementing basic operations for graph deep learning. In the research community, implementing and benchmarking various advanced tasks are still painful and time-consuming with existing libraries. To facilitate graph deep learning research, we introduce DIG: Dive into Graphs, a turnkey library that provides a unified testbed for higher level, research-oriented graph deep learning tasks. Currently, we consider graph generation, self-supervised learning on graphs, explainability of graph neural networks, and deep learning on 3D graphs. For each direction, we provide unified implementations of data interfaces, common algorithms, and evaluation metrics. Altogether, DIG is an extensible, open-source, and turnkey library for researchers to develop new methods and effortlessly compare with common baselines using widely used datasets and evaluation metrics. Source code is available at https://github.com/divelab/DIG.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "38813990",
                        "name": "Meng Liu"
                    },
                    {
                        "authorId": "2004524780",
                        "name": "Youzhi Luo"
                    },
                    {
                        "authorId": "2109120459",
                        "name": "Limei Wang"
                    },
                    {
                        "authorId": "14629242",
                        "name": "Yaochen Xie"
                    },
                    {
                        "authorId": "1491238705",
                        "name": "Haonan Yuan"
                    },
                    {
                        "authorId": "1914700964",
                        "name": "Shurui Gui"
                    },
                    {
                        "authorId": "2115510017",
                        "name": "Zhao Xu"
                    },
                    {
                        "authorId": "2119316118",
                        "name": "Haiyang Yu"
                    },
                    {
                        "authorId": "2108134764",
                        "name": "Jingtun Zhang"
                    },
                    {
                        "authorId": "2153630672",
                        "name": "Yi Liu"
                    },
                    {
                        "authorId": "1879114760",
                        "name": "Keqiang Yan"
                    },
                    {
                        "authorId": "1734808354",
                        "name": "Bora Oztekin"
                    },
                    {
                        "authorId": "2143857491",
                        "name": "Haoran Liu"
                    },
                    {
                        "authorId": "2108232316",
                        "name": "Xuan Zhang"
                    },
                    {
                        "authorId": "2084647086",
                        "name": "Cong Fu"
                    },
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Recently, more researches focus on the interpretations of GNN models, such as GraphLIME [76], CoGE [51], Counterfactual explanations on GNNs [18] and others [20, 111, 132]."
            ],
            "citingPaper": {
                "paperId": "3d9f067d97cf21f3b0c4d406ccff98b06abafb5c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-10689",
                    "ArXiv": "2103.10689",
                    "DOI": "10.1007/s10115-022-01756-8",
                    "CorpusId": 232290756
                },
                "corpusId": 232290756,
                "publicationVenue": {
                    "id": "1f55639d-134e-44ae-b050-ccf2a6676bc5",
                    "name": "Knowledge and Information Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Inf Syst"
                    ],
                    "issn": "0219-3116",
                    "url": "https://link.springer.com/journal/10115"
                },
                "url": "https://www.semanticscholar.org/paper/3d9f067d97cf21f3b0c4d406ccff98b06abafb5c",
                "title": "Interpretable deep learning: interpretation, interpretability, trustworthiness, and beyond",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48568841",
                        "name": "Xuhong Li"
                    },
                    {
                        "authorId": "40518823",
                        "name": "Haoyi Xiong"
                    },
                    {
                        "authorId": "2155445773",
                        "name": "Xingjian Li"
                    },
                    {
                        "authorId": "2117921638",
                        "name": "Xuanyu Wu"
                    },
                    {
                        "authorId": "2115476207",
                        "name": "Xiao Zhang"
                    },
                    {
                        "authorId": "2118971193",
                        "name": "Ji Liu"
                    },
                    {
                        "authorId": "2143957850",
                        "name": "Jiang Bian"
                    },
                    {
                        "authorId": "1721158",
                        "name": "D. Dou"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                ", 2018] popularized by PGExplainer [Luo et al., 2020] and PTDNet [Luo et al.",
                "Apart from the Gumbel-Softmax trick, other implementations of end-to-end discrete sampling include the Gumbel-Max trick used by AD-GCL [Suresh et al., 2021] and hard concrete sampling [Louizos et al., 2018] popularized by PGExplainer [Luo et al., 2020] and PTDNet [Luo et al., 2021a]."
            ],
            "citingPaper": {
                "paperId": "47baf71f85c59948010641c82a50898cf5a77be2",
                "externalIds": {
                    "ArXiv": "2103.03036",
                    "CorpusId": 246863522
                },
                "corpusId": 246863522,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/47baf71f85c59948010641c82a50898cf5a77be2",
                "title": "A Survey on Graph Structure Learning: Progress and Opportunities",
                "abstract": "Graphs are widely used to describe real-world objects and their interactions. Graph Neural Networks (GNNs) as a de facto model for analyzing graphstructured data, are highly sensitive to the quality of the given graph structures. Therefore, noisy or incomplete graphs often lead to unsatisfactory representations and prevent us from fully understanding the mechanism underlying the system. In pursuit of an optimal graph structure for downstream tasks, recent studies have sparked an effort around the central theme of Graph Structure Learning (GSL), which aims to jointly learn an optimized graph structure and corresponding graph representations. In the presented survey, we broadly review recent progress in GSL methods. Specifically, we first formulate a general pipeline of GSL and review state-of-the-art methods classified by the way of modeling graph structures, followed by applications of GSL across domains. Finally, we point out some issues in current studies and discuss future directions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2653121",
                        "name": "Yanqiao Zhu"
                    },
                    {
                        "authorId": "2054230",
                        "name": "Weizhi Xu"
                    },
                    {
                        "authorId": "2108045911",
                        "name": "Jinghao Zhang"
                    },
                    {
                        "authorId": "93584228",
                        "name": "Yuanqi Du"
                    },
                    {
                        "authorId": "47540245",
                        "name": "Jieyu Zhang"
                    },
                    {
                        "authorId": "2146553789",
                        "name": "Qiang Liu"
                    },
                    {
                        "authorId": "1390553618",
                        "name": "Carl Yang"
                    },
                    {
                        "authorId": "50425438",
                        "name": "Shu Wu"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Meanwhile, PGExplainer (Luo et al., 2020) learns a parameterized model to predict whether an edge is important, which is trained using all edges in the dataset.",
                "Then we compare our SubgraphX with several baselines, including MCTS GNN, GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020).",
                "While several recent studies have developed GNN explanation methods, such as GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), and PGM-Explainer (Vu & Thai, 2020), they invariably focus on explainability at node, edge, or node feature levels.",
                ", 2019), PGExplainer (Luo et al., 2020), and PGM-Explainer (Vu & Thai, 2020), they invariably focus on explainability at node, edge, or node feature levels."
            ],
            "citingPaper": {
                "paperId": "123139463809b5acf98b95d4c8e958be334a32b5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-05152",
                    "ArXiv": "2102.05152",
                    "CorpusId": 231861768
                },
                "corpusId": 231861768,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/123139463809b5acf98b95d4c8e958be334a32b5",
                "title": "On Explainability of Graph Neural Networks via Subgraph Explorations",
                "abstract": "We consider the problem of explaining the predictions of graph neural networks (GNNs), which otherwise are considered as black boxes. Existing methods invariably focus on explaining the importance of graph nodes or edges but ignore the substructures of graphs, which are more intuitive and human-intelligible. In this work, we propose a novel method, known as SubgraphX, to explain GNNs by identifying important subgraphs. Given a trained GNN model and an input graph, our SubgraphX explains its predictions by efficiently exploring different subgraphs with Monte Carlo tree search. To make the tree search more effective, we propose to use Shapley values as a measure of subgraph importance, which can also capture the interactions among different subgraphs. To expedite computations, we propose efficient approximation schemes to compute Shapley values for graph data. Our work represents the first attempt to explain GNNs via identifying subgraphs explicitly and directly. Experimental results show that our SubgraphX achieves significantly improved explanations, while keeping computations at a reasonable level.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1498527026",
                        "name": "Hao Yuan"
                    },
                    {
                        "authorId": "2119316118",
                        "name": "Haiyang Yu"
                    },
                    {
                        "authorId": "2146041754",
                        "name": "Jie Wang"
                    },
                    {
                        "authorId": "2154305557",
                        "name": "Kang Li"
                    },
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Although GNNs have shown state-of-the-art results on tasks involving graph data [50; 6], existing methods for explaining the predictions of GNNs have primarily focused on generating subgraphs that are relevant for a particular prediction [48; 1; 9; 24; 28; 31; 33; 42; 46; 49]."
            ],
            "citingPaper": {
                "paperId": "11b9f4729c8e355dec7122993076f6e2788c03c4",
                "externalIds": {
                    "DBLP": "conf/aistats/LucicHTRS22",
                    "ArXiv": "2102.03322",
                    "CorpusId": 231839528
                },
                "corpusId": 231839528,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/11b9f4729c8e355dec7122993076f6e2788c03c4",
                "title": "CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks",
                "abstract": "Given the increasing promise of graph neural networks (GNNs) in real-world applications, several methods have been developed for explaining their predictions. Existing methods for interpreting predictions from GNNs have primarily focused on generating subgraphs that are especially relevant for a particular prediction. However, such methods are not counterfactual (CF) in nature: given a prediction, we want to understand how the prediction can be changed in order to achieve an alternative outcome. In this work, we propose a method for generating CF explanations for GNNs: the minimal perturbation to the input (graph) data such that the prediction changes. Using only edge deletions, we find that our method, CF-GNNExplainer, can generate CF explanations for the majority of instances across three widely used datasets for GNN explanations, while removing less than 3 edges on average, with at least 94\\% accuracy. This indicates that CF-GNNExplainer primarily removes edges that are crucial for the original predictions, resulting in minimal CF explanations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "38702106",
                        "name": "Ana Lucic"
                    },
                    {
                        "authorId": "41096186",
                        "name": "Maartje ter Hoeve"
                    },
                    {
                        "authorId": "2651748",
                        "name": "Gabriele Tolomei"
                    },
                    {
                        "authorId": "1696030",
                        "name": "M. de Rijke"
                    },
                    {
                        "authorId": "144925193",
                        "name": "F. Silvestri"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "14f0ee2594c550de7fb5e590b322bcb1bcec8061",
                "externalIds": {
                    "ArXiv": "2012.15445",
                    "DBLP": "journals/corr/abs-2012-15445",
                    "DOI": "10.1109/TPAMI.2022.3204236",
                    "CorpusId": 229923402,
                    "PubMed": "36063508"
                },
                "corpusId": 229923402,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/14f0ee2594c550de7fb5e590b322bcb1bcec8061",
                "title": "Explainability in Graph Neural Networks: A Taxonomic Survey",
                "abstract": "Deep learning methods are achieving ever-increasing performance on many artificial intelligence tasks. A major limitation of deep models is that they are not amenable to interpretability. This limitation can be circumvented by developing post hoc techniques to explain predictions, giving rise to the area of explainability. Recently, explainability of deep models on images and texts has achieved significant progress. In the area of graph data, graph neural networks (GNNs) and their explainability are experiencing rapid developments. However, there is neither a unified treatment of GNN explainability methods, nor a standard benchmark and testbed for evaluations. In this survey, we provide a unified and taxonomic view of current GNN explainability methods. Our unified and taxonomic treatments of this subject shed lights on the commonalities and differences of existing methods and set the stage for further methodological developments. To facilitate evaluations, we provide a testbed for GNN explainability, including datasets, common algorithms and evaluation metrics. Furthermore, we conduct comprehensive experiments to compare and analyze the performance of many techniques. Altogether, this work provides a unified methodological treatment of GNN explainability and a standardized testbed for evaluations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1498527026",
                        "name": "Hao Yuan"
                    },
                    {
                        "authorId": "2119316118",
                        "name": "Haiyang Yu"
                    },
                    {
                        "authorId": "1914700964",
                        "name": "Shurui Gui"
                    },
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                ", PGExplainer [56], SubgraphX [59], and Gem [60]), we",
                "Contrary to most of the above interpretability methods that provide a post-hoc explanatory subgraph at the structure-level (e.g., PGExplainer [56], SubgraphX [59], and Gem [60]), we design an inherently interpretable message passing scheme for homogeneous graphs at the feature-level by identifying important input node capsules from the extracted subgraph.",
                "PGExplainer [56] uses parameterized neural network to identify the important subgraphs at the model-level."
            ],
            "citingPaper": {
                "paperId": "9d6039c022cdab1ea78a562aedd3b5e6a7b67eb6",
                "externalIds": {
                    "ArXiv": "2012.03476",
                    "DOI": "10.1109/TNNLS.2022.3179306",
                    "CorpusId": 249544281,
                    "PubMed": "35679381"
                },
                "corpusId": 249544281,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9d6039c022cdab1ea78a562aedd3b5e6a7b67eb6",
                "title": "NCGNN: Node-Level Capsule Graph Neural Network for Semisupervised Classification.",
                "abstract": "Message passing has evolved as an effective tool for designing graph neural networks (GNNs). However, most existing methods for message passing simply sum or average all the neighboring features to update node representations. They are restricted by two problems: 1) lack of interpretability to identify node features significant to the prediction of GNNs and 2) feature overmixing that leads to the oversmoothing issue in capturing long-range dependencies and inability to handle graphs under heterophily or low homophily. In this article, we propose a node-level capsule graph neural network (NCGNN) to address these problems with an improved message passing scheme. Specifically, NCGNN represents nodes as groups of node-level capsules, in which each capsule extracts distinctive features of its corresponding node. For each node-level capsule, a novel dynamic routing procedure is developed to adaptively select appropriate capsules for aggregation from a subgraph identified by the designed graph filter. NCGNN aggregates only the advantageous capsules and restrains irrelevant messages to avoid overmixing features of interacting nodes. Therefore, it can relieve the oversmoothing issue and learn effective node representations over graphs with homophily or heterophily. Furthermore, our proposed message passing scheme is inherently interpretable and exempt from complex post hoc explanations, as the graph filter and the dynamic routing procedure identify a subset of node features that are most significant to the model prediction from the extracted subgraph. Extensive experiments on synthetic as well as real-world graphs demonstrate that NCGNN can well address the oversmoothing issue and produce better node representations for semisupervised node classification. It outperforms the state of the arts under both homophily and heterophily.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2115430809",
                        "name": "Rui Yang"
                    },
                    {
                        "authorId": "3207464",
                        "name": "Wenrui Dai"
                    },
                    {
                        "authorId": "144535686",
                        "name": "Chenglin Li"
                    },
                    {
                        "authorId": "38871632",
                        "name": "Junni Zou"
                    },
                    {
                        "authorId": "144045763",
                        "name": "H. Xiong"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Concurrently to this paper, Luo et al. (2020) have also developed an interpretability technique for GNNs relying on differentiable edge masking."
            ],
            "citingPaper": {
                "paperId": "c30c0092bf4eb8a44faec3fc60cdd5006276bcdc",
                "externalIds": {
                    "ArXiv": "2010.00577",
                    "DBLP": "journals/corr/abs-2010-00577",
                    "MAG": "3091534907",
                    "CorpusId": 222090060
                },
                "corpusId": 222090060,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c30c0092bf4eb8a44faec3fc60cdd5006276bcdc",
                "title": "Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking",
                "abstract": "Graph neural networks (GNNs) have become a popular approach to integrating structural inductive biases into NLP models. However, there has been little work on interpreting them, and specifically on understanding which parts of the graphs (e.g. syntactic trees or co-reference structures) contribute to a prediction. In this work, we introduce a post-hoc method for interpreting the predictions of GNNs which identifies unnecessary edges. Given a trained GNN model, we learn a simple classifier that, for every edge in every layer, predicts if that edge can be dropped. We demonstrate that such a classifier can be trained in a fully differentiable fashion, employing stochastic gates and encouraging sparsity through the expected $L_0$ norm. We use our technique as an attribution method to analyze GNN models for two tasks -- question answering and semantic role labeling -- providing insights into the information flow in these models. We show that we can drop a large proportion of edges without deteriorating the performance of the model, while we can analyse the remaining edges for interpreting model predictions.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "8804828",
                        "name": "M. Schlichtkrull"
                    },
                    {
                        "authorId": "41019080",
                        "name": "Nicola De Cao"
                    },
                    {
                        "authorId": "144889265",
                        "name": "Ivan Titov"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "05 to the previous baseline performance in [27].",
                "We see that for the node case, explanationmethods Pope et al. (LRP), GNN-LRP, PGExplainer and GNNExplainer, all have an AUROC score above 0.9, which shows that all these explanation methods have been able to extract from the model the class-specific motif in the input graphs.",
                "Some explanation techniques such as GNNExplainer [26], PGExplainer [27] and SubgraphX [32] are based solely on evaluating the function or its gradient multiple times.",
                "The values for PGExplainer are extracted from [27].",
                "As a second quantitative evaluation, we consider the BA2motifs [27] benchmark that comeswith \u2018ground-truth\u2019 explan-",
                "GNNExplainer [26] and PGExplainer [27] explain the model by extracting the subgraph that maximizes the mutual information to the prediction for the original graph.",
                "[26], PGExplainer [27] and SubgraphX [32] are based solely",
                "GNNExplainer [26] and PGExplainer [27] explain the model by extracting the subgraph that maximizes",
                "Regarding the quality of the relevance features, most of the proposed methods attribute the GNN prediction to nodes or edges of the input graph [24], [26], [27], whereas GNN-LRP gives scores for higher-order features, such as sequences of edges.",
                "As a second quantitative evaluation, we consider the BA2motifs [27] benchmark that comeswith \u2018ground-truth\u2019 explanations."
            ],
            "citingPaper": {
                "paperId": "9e707dd89bba25a3dd22c96f43bd72b9b3ab94bb",
                "externalIds": {
                    "ArXiv": "2006.03589",
                    "DBLP": "journals/pami/SchnakeELNSMM22",
                    "MAG": "3108823960",
                    "DOI": "10.1109/TPAMI.2021.3115452",
                    "CorpusId": 227225626,
                    "PubMed": "34559639"
                },
                "corpusId": 227225626,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9e707dd89bba25a3dd22c96f43bd72b9b3ab94bb",
                "title": "Higher-Order Explanations of Graph Neural Networks via Relevant Walks",
                "abstract": "Graph Neural Networks (GNNs) are a popular approach for predicting graph structured data. As GNNs tightly entangle the input graph into the neural network structure, common explainable AI approaches are not applicable. To a large extent, GNNs have remained black-boxes for the user so far. In this paper, we show that GNNs can in fact be naturally explained using higher-order expansions, i.e., by identifying groups of edges that jointly contribute to the prediction. Practically, we find that such explanations can be extracted using a nested attribution scheme, where existing techniques such as layer-wise relevance propagation (LRP) can be applied at each step. The output is a collection of walks into the input graph that are relevant for the prediction. Our novel explanation method, which we denote by GNN-LRP, is applicable to a broad range of graph neural networks and lets us extract practically relevant insights on sentiment analysis of text data, structure-property relationships in quantum chemistry, and image classification.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "90387439",
                        "name": "Thomas Schnake"
                    },
                    {
                        "authorId": "1557932201",
                        "name": "Oliver Eberle"
                    },
                    {
                        "authorId": "95930534",
                        "name": "Jonas Lederer"
                    },
                    {
                        "authorId": "3187484",
                        "name": "Shinichi Nakajima"
                    },
                    {
                        "authorId": "51257580",
                        "name": "K. T. Schutt"
                    },
                    {
                        "authorId": "116099820",
                        "name": "Klaus-Robert Muller"
                    },
                    {
                        "authorId": "144535526",
                        "name": "G. Montavon"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "For example, [9] was proposed to utilize mutual information to find a subgraph with associated features for interpreting GNN models; PGExplainer [16] learns a parameterized model to predict whether an edge is important; SubgraphX [17] explains GNNs by exploring and identifying important subgraphs; GraphSVX [18] utilizes decomposition technique to explain GNNs based on the Shapley Values from game theory."
            ],
            "citingPaper": {
                "paperId": "760ab37ab4d5a68b53035208d2e179494d879322",
                "externalIds": {
                    "DBLP": "journals/tkde/HuangYTSC23",
                    "MAG": "3000120900",
                    "ArXiv": "2001.06216",
                    "DOI": "10.1109/TKDE.2022.3187455",
                    "CorpusId": 210714016
                },
                "corpusId": 210714016,
                "publicationVenue": {
                    "id": "c6840156-ee10-4d78-8832-7f8909811576",
                    "name": "IEEE Transactions on Knowledge and Data Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Knowl Data Eng"
                    ],
                    "issn": "1041-4347",
                    "url": "https://www.computer.org/web/tkde",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=69"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/760ab37ab4d5a68b53035208d2e179494d879322",
                "title": "GraphLIME: Local Interpretable Model Explanations for Graph Neural Networks",
                "abstract": "Recently, graph neural networks (GNN) were shown to be successful in effectively representing graph structured data because of their good performance and generalization ability. However, explaining the effectiveness of GNN models is a challenging task because of the complex nonlinear transformations made over the iterations. In this paper, we propose GraphLIME, a local interpretable model explanation for graphs using the Hilbert-Schmidt Independence Criterion (HSIC) Lasso, which is a nonlinear feature selection method. GraphLIME is a generic GNN-model explanation framework that learns a nonlinear interpretable model locally in the subgraph of the node being explained. Through experiments on two real-world datasets, the explanations of GraphLIME are found to be of extraordinary degree and more descriptive in comparison to the existing explanation methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144938775",
                        "name": "Q. Huang"
                    },
                    {
                        "authorId": "50142992",
                        "name": "M. Yamada"
                    },
                    {
                        "authorId": "2152948229",
                        "name": "Yuan Tian"
                    },
                    {
                        "authorId": "2112755810",
                        "name": "Dinesh Singh"
                    },
                    {
                        "authorId": "50559722",
                        "name": "Dawei Yin"
                    },
                    {
                        "authorId": "2118858577",
                        "name": "Yi Chang"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "Despite players\u2019 understanding of the meaning of edge weights in GNNExplainer and PGExplainer tasks, the weights sometimes confused their decisions.",
                "\u2022 PGExplainer: We presented players with PGExplainer\u2019s selection probabilities of edges in subgraphs and node colors.",
                "Specifically, it achieves outstanding precision and recall scores in node classification datasets and outperforms state-of-the-art methods GNNExplainer and PGExplainer.",
                "Second, we performed qualitative assessments of our framework by comparing it with two state-of-the-art post-hoc explanation methods [7], [8], highlighting SCALE\u2019s superior quality of explanations.",
                "Moreover, post-hoc explanation methods [7], [8] often transform node classification problems into graph classification problems via subgraph (K-hop) sampling, which can be suboptimal when graphs contain numerous small cycles.",
                "research area with several subsequent papers [8], [23], [24] exploring the topic.",
                "Additionally, their explanation solutions are too straightforward, making it challenging to achieve significant results on other datasets [7], [8].",
                "For fair comparisons, we contacted the authors of GNNExplainer, PGExplainer, and SEGNN to request evaluation scripts for all datasets.",
                "Even though GNNExplainer and PGExplainer can highlight impactful edges in\nVOLUME 11, 2023 40799\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE.",
                "Similar to previous works [7], [8], our framework aims to identify the key factors from graph structures, node features, and edge features that contribute the most to predictions.",
                "Specifically, BA-2motifs (BA-2m) [8] consists of 1000 graphs with two classes constructed by adding specific motifs to BA graphs, where half contain 5-node house motifs and the other half include 5-node cycle motifs.",
                "Among them, perturbationmethods like GNNExplainer [7] and PGExplainer [8] have gained widespread acceptance, primarily because they introduce benchmark datasets for GNN explanation tasks and exhibit exceptional performance.",
                "However, adjusting the visibility threshold in GNNExplainer and PGExplainer to present explanations on multiple levels may result in outputs with multiple disconnected components, given the independence of edge selections.",
                "Explanation models, such as those proposed in [7] and [8], are students trained with hard labels provided by the original GNNs.",
                "Inspired by [8], the mask matrix is initialized via an MLP network in which inputs are edge embedding vectors constructed by concatenating embedding vectors of source and target nodes taken from the black-box GNN model.",
                "SCALE outperforms baselines on Mutag, with precision score gains of 15.54% compared to PGExplainer and 51.52% compared to GNNExplainer.",
                "Our observations indicated that PGExplainer\u2019s reparameterization trick [8] led the selection probabilities to approach 1 in most cases, making it difficult for participants to differentiate edge influences.",
                "Although the average accuracy scores were almost identical for both GNNExplainer and PGExplainer tasks, PGExplainer caused more confusion in explanation weights.",
                "Furthermore, its performance is comparable to that of PGExplainer on the BA-2motifs dataset.",
                "As shown in Table 4, SCALE\noutperforms post-hoc explanation methods by a significant margin in all experiments, with performance gains of up to 94x compared to GNNExplainer and 120x compared to PGExplainer.",
                "\u2022 PGExplainer [8] shared the same approach as GNNExplainer [7] but initialized masks using embedding vectors from the pre-trained model.",
                "B. QUALITATIVE COMPARISON WITH BASELINES For each dataset, we chose one instance and visualized explanations provided by SCALE, GNNExplainer, and PGExplainer in Figure 3.",
                "plainer [7] and PGExplainer [8] have gained widespread acceptance, primarily because they introduce benchmark datasets for GNN explanation tasks and exhibit exceptional performance.",
                "Other baselines except PGExplainer were also executed using the same PyTorch version.",
                "[8], which formulate structural explanations as binary classification tasks by including influential nodes and edges in explanations.",
                "9.1 for experiments with PGExplainer.",
                "As a result, user prediction accuracies varied more in the PGExplainer task than in the GNNExplainer task."
            ],
            "citingPaper": {
                "paperId": "bc0802aa3f2b80c5273774f57a1490f2ef37fdb7",
                "externalIds": {
                    "DBLP": "journals/access/BuiLL23",
                    "DOI": "10.1109/ACCESS.2023.3270385",
                    "CorpusId": 258350116
                },
                "corpusId": 258350116,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bc0802aa3f2b80c5273774f57a1490f2ef37fdb7",
                "title": "Generating Real-Time Explanations for GNNs via Multiple Specialty Learners and Online Knowledge Distillation",
                "abstract": "Graph Neural Networks have become increasingly ubiquitous in numerous applications, necessitating explanations of their predictions. However, explaining GNNs is challenging due to the complexity of graph data and model execution. Post-hoc explanation approaches have gained popularity due to their versatility, despite their additional computational costs. Although intrinsically interpretable models can provide instant explanations, they are usually model-specific and can only explain particular GNNs. To address these challenges, we propose a novel, general, and fast GNN explanation framework named SCALE. SCALE trains multiple specialty learners to explain GNNs, as creating a single powerful explainer for examining the attributions of interactions in input graphs is complicated. In training, a black-box GNN model guides learners based on an online knowledge distillation paradigm. During the explanation phase, explanations of predictions are generated by multiple explainers corresponding to trained learners. Edge masking and random walk with restart procedures are implemented to provide structural explanations for graph-level and node-level predictions. A feature attribution module provides overall summaries and instance-level feature contributions. We compare SCALE with state-of-the-art baselines through extensive experiments to demonstrate its explanation correctness and execution performance. Furthermore, we conduct a user study and a series of ablation studies to understand its strengths and weaknesses.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "41022579",
                        "name": "Tien-Cuong Bui"
                    },
                    {
                        "authorId": "2055470540",
                        "name": "Van-Duc Le"
                    },
                    {
                        "authorId": "2108718185",
                        "name": "Wen-Syan Li"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We see that the objective tends to be positive or zero for \u03b3 \u2265 0.2, although it can be negative with significant frequency, compared to the positive frequency, in BA-2motif.",
                "PGExplainer learns approximate discrete masks by training a parametric predictor, and masks out unimportant edges according to the learned masks.",
                "For one graph each from the positive (Table 4) and the negative (Table 5) classes in BA-2motif, we list the top 100 most absolute relevant walks (before omitting the negative-relevant walks) found by EMP-neu.",
                "BA-2motif\nBA-2motif (Luo et al., 2020) is a synthetic dataset of graphs that are built by attaching one of two different motifs (either a house or a circle shape) to a random graph, which is generated by the Baraba\u0301si-Albert (BA) model.",
                "Note that these heuristics are compatible with all edge-level explanability methods, including GNNExplainer and PGExplainer, which however are incomparably slow.",
                "We also used the BA-2motif dataset, which provides the ground truth subgraphs as motifs, and evaluated how accurately explanation methods can detect the motifs.",
                "Table 1 shows computation time (on an M1Pro CPU) of explanation methods on the BA-2motif and Infection datasets.",
                "(a) Top-1 walk search with GIN-L for L = 2, . . . , 7 on BA-2motif.",
                "These include general explanation methods, e.g., sensitive analysis (SA), guided backpropagation (GBP), class activation mapping (CAM) and excitation backpropagation (EB) (Baldassarre & Azizpour, 2019; Pope et al., 2019), adapted to the GNN structure, and novel methods specialized for GNNs, e.g., GNNExplainer (Ying et al., 2019) and PGExplainer (Luo et al., 2020).",
                "This paper focuses on the instance-level explanation, for which most of the existing methods, e.g., GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020) and methods in Pope et al. (2019), only consider lower-order features, i.e., nodes and edges, ignoring higher-order interactions.",
                "(14)\nTable 3 shows how often the objective of the maximization objective in the AMP-ave message passing (14) is positive, negative, or zero on randomly chosen 10 correctly classified samples from BA-2motif, MUTAG, and Graph-SST2 with different \u03b3.",
                "\u2026(SA), guided backpropagation (GBP), class activation mapping (CAM) and excitation backpropagation (EB) (Baldassarre & Azizpour, 2019; Pope et al., 2019), adapted to the GNN structure, and novel methods specialized for GNNs, e.g., GNNExplainer (Ying et al., 2019) and PGExplainer (Luo et al., 2020).",
                "Then, we performed approximate top-K walk search by AMP-ave for different K, and evaluated its performance in terms of precision TP/K and recall TP/K\u2217, where TP = |{Approx. top-K walks} \u2229 {True top-K\u2217 walks}|, on randomly chosen samples among the correctly classified test samples from each dataset.3 Figure 3 shows the precision-recall curves on BA-2motif and Mutagenicity for different K\u2217 and different \u03b3 of LRP-\u03b3 rules.",
                "We use common benchmark datasets including BA-2motif, MUTAG, Mutagenicity, and Graph-SST2 (see Appendix F for details on data and employed GNNs)."
            ],
            "citingPaper": {
                "paperId": "7a3c3084a2c109d7497f4981661df9f58bda3d65",
                "externalIds": {
                    "DBLP": "conf/icml/XiongSGMMN23",
                    "CorpusId": 260879711
                },
                "corpusId": 260879711,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7a3c3084a2c109d7497f4981661df9f58bda3d65",
                "title": "Relevant Walk Search for Explaining Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) have become important machine learning tools for graph analysis, and its explainability is crucial for safety, fairness, and robustness. Layer-wise relevance propagation for GNNs (GNN-LRP) evaluates the relevance of walks to reveal important information flows in the network, and provides higher-order explanations, which have been shown to be superior to the lower-order, i.e., node-/edge-level, explanations. However, identifying relevant walks by GNN-LRP requires exponential computational complexity with respect to the network depth, which we will remedy in this paper. Specifically, we propose polynomial-time algorithms for finding top-K relevant walks, which drastically reduces the computation and thus increases the applicability of GNN-LRP to large-scale problems. Our proposed algorithms are based on the max-product algorithm\u2014a common tool for finding the maximum likelihood configurations in probabilistic graphical models\u2014and can find the most relevant walks exactly at the neuron level and approximately at the node level. Our experiments demonstrate the performance of our algorithms at scale and their utility across application domains, i.e., on epidemiology, molecular, and natural language benchmarks. We provide our codes under",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2062775158",
                        "name": "Pin Xiong"
                    },
                    {
                        "authorId": "90387439",
                        "name": "Thomas Schnake"
                    },
                    {
                        "authorId": "5742764",
                        "name": "M. Gastegger"
                    },
                    {
                        "authorId": "144535526",
                        "name": "G. Montavon"
                    },
                    {
                        "authorId": "2113612432",
                        "name": "Klaus-Robert M\u00fcller"
                    },
                    {
                        "authorId": "2055678647",
                        "name": "S. Nakajima"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "(1) We implement PGExplainer (PG) in (Luo et al., 2020) and adapt it for the temporal graph scenario.",
                "More details about reparameterization can be found in (Luo et al., 2020).",
                "Inspired by previous parameterized explainers (Luo et al., 2020), we pretrain a navigator to provide a global understanding of the relationship among events.",
                "Learning-based methods (Luo et al., 2020; Shan et al., 2021; Vu & Thai, 2020) leverage node representations generated by the trained GNN and adopt a neural network to learn crucial nodes/edges.",
                "While currently there are no methods for explaining temporal graph models, some recent explanation methods (e.g., GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020) and SubgraphX (Yuan et al., 2021)) for static GNNs are the most related.",
                ", 2019), PGExplainer (Luo et al., 2020) and SubgraphX (Yuan et al."
            ],
            "citingPaper": {
                "paperId": "7d6aa3d0a9113501e658ff939dda4b01e0f6a785",
                "externalIds": {
                    "DBLP": "conf/iclr/XiaLS0D0023",
                    "CorpusId": 259298210
                },
                "corpusId": 259298210,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7d6aa3d0a9113501e658ff939dda4b01e0f6a785",
                "title": "Explaining Temporal Graph Models through an Explorer-Navigator Framework",
                "abstract": "While Graph Neural Network (GNN) explanation has recently received significant attention, existing works are generally designed for static graphs. Due to the prevalence of temporal graphs, many temporal graph models have been proposed, but explaining their predictions still remains to be explored. To bridge the gap, in this paper, we propose a Temporal GNN Explainer (T-GNNExplainer) method. Specifically, we regard a temporal graph as a sequence of temporal events between nodes. Given a temporal prediction of a model, our task is to find a subset of historical events that lead to the prediction. To handle this combinatorial optimization problem, T-GNNExplainer includes an explorer to find the event subsets with Monte Carlo Tree Search (MCTS), and a navigator that learns the correlations between events and helps reduce the search space. In particular, the navigator is trained in advance and then integrated with the explorer to speed up searching and achieve better results. To the best of our knowledge, T-GNNExplainer is the first explainer tailored for temporal graph models. We conduct extensive experiments to evaluate the performance of T-GNNExplainer. Experimental results demonstrate that T-GNNExplainer can achieve superior performance with up to \u21e050% improvement in Area under Fidelity-Sparsity Curve.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "102410799",
                        "name": "Wenwen Xia"
                    },
                    {
                        "authorId": "3075031",
                        "name": "Mincai Lai"
                    },
                    {
                        "authorId": "145663545",
                        "name": "Caihua Shan"
                    },
                    {
                        "authorId": "2195028945",
                        "name": "Yaofang Zhang"
                    },
                    {
                        "authorId": "104993629",
                        "name": "Xinnan Dai"
                    },
                    {
                        "authorId": "47875796",
                        "name": "Xiang Li"
                    },
                    {
                        "authorId": "2119081394",
                        "name": "Dongsheng Li"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "contexts": [
                ", PGExplainer [141]), where the aim is to train a model to interpret a pre-trained GL-GNN.",
                "Most algorithms for explaining the predictions of GL-GNNs are post-hoc (e.g., PGExplainer [141]), where the aim is to train a model to interpret a pre-trained GL-GNN.",
                "PGExplainer [141] trains an MLP to determine which edges are valuable to a GNN\u2019s prediction and then removes any irrelevant edges to form a new graph."
            ],
            "citingPaper": {
                "paperId": "5ff8c0c4c8de76e0e925fe716d0411d3e74653fc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-05860",
                    "DOI": "10.48550/arXiv.2301.05860",
                    "CorpusId": 255942689
                },
                "corpusId": 255942689,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5ff8c0c4c8de76e0e925fe716d0411d3e74653fc",
                "title": "A Comprehensive Survey of Graph-level Learning",
                "abstract": "\u2014Graphs have a superior ability to represent re- lational data, like chemical compounds, proteins, and social networks. Hence, graph-level learning, which takes a set of graphs as input, has been applied to many tasks including comparison, regression, classi\ufb01cation, and more. Traditional approaches to learning a set of graphs tend to rely on hand-crafted features, such as substructures. But while these methods bene\ufb01t from good interpretability, they often suffer from computational bottlenecks as they cannot skirt the graph isomorphism problem. Conversely, deep learning has helped graph-level learning adapt to the growing scale of graphs by extracting features automatically and decoding graphs into low-dimensional representations. As a result, these deep graph learning methods have been responsible for many successes. Yet, there is no comprehensive survey that reviews graph-level learning starting with traditional learning and moving through to the deep learning approaches. This article \ufb01lls this gap and frames the representative algorithms into a systematic taxonomy covering traditional learning, graph-level deep neural networks, graph-level graph neural networks, and graph pooling. To ensure a thoroughly comprehensive survey, the evolutions, interactions, and communications between methods from four different branches of development are also examined. This is followed by a brief review of the benchmark data sets, evaluation metrics, and common downstream applications. The survey concludes with 13 future directions of necessary research that will help to overcome the challenges facing this booming \ufb01eld.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "152747612",
                        "name": "Zhenyu Yang"
                    },
                    {
                        "authorId": "2151251543",
                        "name": "Ge Zhang"
                    },
                    {
                        "authorId": "2142734769",
                        "name": "Jia Wu"
                    },
                    {
                        "authorId": "2118801701",
                        "name": "Jian Yang"
                    },
                    {
                        "authorId": "120607997",
                        "name": "Quan.Z Sheng"
                    },
                    {
                        "authorId": "2057237074",
                        "name": "Shan Xue"
                    },
                    {
                        "authorId": "2110713858",
                        "name": "Chuan Zhou"
                    },
                    {
                        "authorId": "1682418",
                        "name": "C. Aggarwal"
                    },
                    {
                        "authorId": "2138443697",
                        "name": "Hao Peng"
                    },
                    {
                        "authorId": "2146226874",
                        "name": "Wenbin Hu"
                    },
                    {
                        "authorId": "2064408469",
                        "name": "Edwin R. Hancock"
                    },
                    {
                        "authorId": "2075355155",
                        "name": "Pietro Lio'"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "The chemical fragments -NO2 and -NH2 in mutagen graphs are labeled as ground-truth explanations [27].",
                "Mask Generation (MG) [27, 49, 29, 43] The mask generation method is to optimize a mask generator g\u03b8 to generate the edge mask M for the input graph G.",
                "BA-2Motifs [27] is a synthetic dataset with binary graph labels.",
                "Explainability methods We compare non-generative methods: Saliency [6], Integrated Gradient [36], Occlusion [53], Grad-CAM [32], GNNExplainer [48], PGMExplainer [39], and SubgraphX [52], with generative ones: PGExplainer [27], GSAT [29], GraphCFE (CLEAR) [28], D4Explainer and RCExplainer [42].",
                "Method Generator Information Constraint Level Scenario Output PGExplainer [27] Mask Generation size instance factual E GIB [49] Mask Generation mutual information instance factual N GSAT [29] Mask Generation variational instance factual E GNNInterpreter [43] Mask Generation size model factual N / E / NF GEM [25] VGAE size instance factual E CLEAR [28] VGAE size instance counterfactual E / NF OrphicX [26] VGAE variational & size instance factual E D4Explainer Diffusion size instance & model counterfactual E GANExplainer [24] GAN instance factual E RCExplainer [42] RL-MDP size instance factual SUBGRAPH XGNN [50] RL-MDP size model factual SUBGRAPH GFlowExplainer [22] RL-DAG size instance factual SUBGRAPH",
                "We follow the original setting to train PGExplainer, GSAT, and RCExplainer."
            ],
            "citingPaper": {
                "paperId": "9c6f0e3e89e73303ec3c76d31e5371d7e59635dc",
                "externalIds": {
                    "CorpusId": 260744309
                },
                "corpusId": 260744309,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9c6f0e3e89e73303ec3c76d31e5371d7e59635dc",
                "title": "Generative Explanation for Graph Neural Network: Methods and Evaluation",
                "abstract": "Graph Neural Networks (GNNs) achieve state-of-the-art performance in various graph-related tasks. However the black-box nature often limits their interpretability and trustworthiness. Numerous explanation methods have been proposed to uncover the decision-making logic of GNNs, by generating underlying explanatory substructures. In this paper, we conduct a comprehensive review of the existing explanation methods for GNNs from the perspective of graph generation. Specifically, we propose a unified optimization objective for current generative explanation methods, comprising two sub-objectives: Attribution and Information constraints. We further demonstrate their specific manifestations in different generative model architectures and explanation scenarios. With the unified objective of the explanation problem, we reveal the shared characteristics and distinctions among current methods, laying the foundation for future methodological advancements. Empirical results demonstrate the advantages and limitations of different approaches in terms of explanation performance, efficiency, and generalizability.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "83539859",
                        "name": "Rex Ying"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The graph generator is trained via a policy gradient method based on information from the trained GNNs.\nPGExplainer adopts a deep neural network to parameterize the generation process of explanations, which enables PGExplainer a natural approach to explaining multiple instances collectively.",
                "We drew the relationship curves of fidelity and sparsity in BBBP and tree-grids data sets by adjusting parameters, and comparing them with GNNExplainer, PGExplainer, XGNN, and SubgraphX.",
                "In recent years, the main methods that have been used include XGNN, PGExplainer, and GraphSVX.",
                "In recent years, several methods have been proposed to interpret GNN prediction [27, 41], such as XGNN [42], PGExplainer [23], and GraphSVX [4, 7]."
            ],
            "citingPaper": {
                "paperId": "754f1363451be2bced84a2c63b4afd3bf2a00b53",
                "externalIds": {
                    "CorpusId": 261613967
                },
                "corpusId": 261613967,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/754f1363451be2bced84a2c63b4afd3bf2a00b53",
                "title": "Explanations for Graph Neural Networks via Layer Analysis",
                "abstract": ". Like many deep learning models, graph neural networks (GNNs) are regardedasblackboxesandlackinterpretability.Therefore,itisdif\ufb01cultforGNNs tobefullytrustedbyhumanstobeappliedtovariouslifescenarios.Basedonthis problem,weproposeanewinterpretabilitymethodcalledLAExplainer,whichis usedtoexplainGNNshierarchicallyatthemodellevel.Inparticular,LAExplainer notonlyfocusesontheoverallinterpretationofthemodel,butalsoanalyzesthe interpretationproblemsbetweenlayers.Ourapproachinterpretsthemiddle-level processofthemodelthroughlayer-by-layeranalysis,andusesitasabasistoguide theconstructionofsub-graphstoreducethesizeofthesub-graphset,whicheffec-tivelyexplaintheoverallmodel.Inaddition,theapproachwillanalyzetheimpor-tanceofmodelfeaturesandproduceanadjustableprincipalcomponentselection mechanism.Intermsofevaluationindicators,weproposetosethyperparameters sothatthetworesultsofFidelityandSparsitycanbechangedsimultaneouslyby adjustingthehyperparametersduringtheinterpretationofGNNs.Experimental resultsshowthatourproposedmethodiseffectiveinsyntheticdatasetsandreal datasets,andtheresultsofthevisualizedsub-graphsaremoreinlinewithhuman understanding.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108049317",
                        "name": "Qinfeng Li"
                    },
                    {
                        "authorId": "2172124591",
                        "name": "Xinrui Kang"
                    },
                    {
                        "authorId": "2238545978",
                        "name": "Wenyuan Li"
                    },
                    {
                        "authorId": "2176030959",
                        "name": "Dong Liang"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Independently from this categorization, a further fundamental distinction is among explainers providing explanations in terms of edge [87, 52, 66, 92] or node masks [75, 76, 55, 6, 55, 67].",
                "However, their study is limited to node classification and the three explainers under analysis [87, 52, 66] are not well representative of the diversity of explanation strategies that",
                "[91] proposed a categorization of explainers into four categories: gradient-based which exploit gradients of the input neural network [75, 76, 55]; perturbation-based where perturbations of the input graphs are aimed at obtaining explainable subgraphs [87, 52, 23, 66]; decomposition-based which try to decompose the input identifying the explanations [6, 55, 67]; and surrogate-based where a simple interpretable surrogate model is used to explain the original neural network [36, 97, 80].",
                "PgExpl: (Perturbation) [52]: The Parametrized Explainer for GNNs (PgExpl) [52] adopts a very similar formulation of the explanation problem as GnnExpl where the two major differences are: i) PgExpl provides solely explanations in terms of subgraph structures, neglecting explanations in terms of node features; ii) instead of directly optimizing continuous edge and features masks as done by GnnExpl, it uses Gradient Descend to train a MLP which, given the two concatenated node embeddings",
                "Roughly speaking, gradient-based explainers exploit gradients of the input neural network [75, 76, 55], perturbation-based models perturb the input aiming to obtain explainable subgraphs[87, 52, 23, 66], decomposition-based models try to decompose the input identifying the explanations [6, 55, 67], while surrogatebased models use a simple interpretable surrogate to explain the original neural network [36, 97, 80]."
            ],
            "citingPaper": {
                "paperId": "b8e00ad63de8c6963bb8c18994ac65874ec89abb",
                "externalIds": {
                    "CorpusId": 262698822
                },
                "corpusId": 262698822,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b8e00ad63de8c6963bb8c18994ac65874ec89abb",
                "title": "Understanding how explainers work in graph neural networks",
                "abstract": ",",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2130615549",
                        "name": "Antonio Longa"
                    },
                    {
                        "authorId": "2165224662",
                        "name": "Steve Azzolin"
                    },
                    {
                        "authorId": "2042269425",
                        "name": "G. Santin"
                    },
                    {
                        "authorId": "50139333",
                        "name": "G. Cencetti"
                    },
                    {
                        "authorId": "2075355155",
                        "name": "Pietro Lio'"
                    },
                    {
                        "authorId": "1776476",
                        "name": "B. Lepri"
                    },
                    {
                        "authorId": "1702610",
                        "name": "Andrea Passerini"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Recently, although the explainability of graph neural networks (GNNs) [9], [10], [11], [12], [13], [14] has been explored as in [15], [16], [17], [18], [19], [20], and [21], they are limited to understanding",
                "models, recent explanation models for GNNs [9], [10], [11], [12], [13], [34] using gradients [35], decomposition [36], surrogates [17], [37]), and perturbation [15], [16], [38] have been proposed."
            ],
            "citingPaper": {
                "paperId": "17328001f78d936c45ba7cb883107664fb27d2b4",
                "externalIds": {
                    "DBLP": "journals/access/Park23",
                    "DOI": "10.1109/ACCESS.2022.3233036",
                    "CorpusId": 255282566
                },
                "corpusId": 255282566,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/17328001f78d936c45ba7cb883107664fb27d2b4",
                "title": "Providing Post-Hoc Explanation for Node Representation Learning Models Through Inductive Conformal Predictions",
                "abstract": "Learning with graph-structured data, such as social, biological, and financial networks, requires effective low-dimensional representations to handle their large and complex interactions. Recently, with the advances of neural networks and embedding algorithms, many unsupervised approaches have been proposed for many downstream tasks with promising results; however, there has been limited research on interpreting the unsupervised representations and, specifically, on understanding which parts of the neighboring nodes contribute to the representation of a node. To mitigate this problem, we propose a statistical framework to interpret the learned representations. Many of the existing works, which are designed for supervised node presentation models, compute the difference in prediction scores after perturbing the edges of a candidate explanation node; however, our proposed framework leverages a conformal prediction (CP)-based statistical test to verify the importance of the candidate node in each node representation. In our evaluation, our proposed framework was verified in many experimental settings and presented promising results compared to those of the recent baseline methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1845151",
                        "name": "Hogun Park"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Techniques have been used and implemented to create post hoc explanations of GNN models [4, 5], however, none of them can truly follow the decision process."
            ],
            "citingPaper": {
                "paperId": "8f4d5d0df32f818b1a56182cd749c113774be8f3",
                "externalIds": {
                    "CorpusId": 259255365
                },
                "corpusId": 259255365,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8f4d5d0df32f818b1a56182cd749c113774be8f3",
                "title": "Developing interpretable graph neural networks for high dimensional feature spaces",
                "abstract": "Graph Neural Networks (GNNs) are a powerful variations of classical neural network models that are suited for learning on graph structured data. However, as is the case for other neural networks, GNNs are not transparent and their internal logic cannot be readily interpreted by humans. In this thesis we built on top of an existing interpretable Decision Tree GNN architecture (DT-GNN) that acts as a surrogate for the underlying GNN. This architecture however struggles with high dimensional datasets. The aim of the thesis is twofold: First we facilitated the transfer from GNN to DT-GNN by using dimensionality reduction as well as L1-orthogonal regularization. Second, we provided an extension to the DT-GNN approach, which allows us to keep the generically trained GNN architecture in place, without requiring training with Gumbel Softmax. This extension could lead to downthe-line benefits when working with high dimensional datasets and facilitates the implementation of an interpretable alternative in existing models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220572076",
                        "name": "Arvid Ban"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Perturbation-based methods [16], [17], [18], [19], [20], [21] measure importance scores by masking the"
            ],
            "citingPaper": {
                "paperId": "8ce5298c1dce9d6f422ab0b7b0f4b3eb8a0ff09b",
                "externalIds": {
                    "CorpusId": 259281125
                },
                "corpusId": 259281125,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8ce5298c1dce9d6f422ab0b7b0f4b3eb8a0ff09b",
                "title": "Towards Semantic Interpretation and Validation of Graph Attention-based Explanations",
                "abstract": "\u2014In this work, we investigate the use of semantic attention to explain the performance of a Graph Neural Network (GNN)-based pose estimation model. To validate our approach, we apply semantically-informed perturbations to the input data and correlate the predicted feature importance weights with the model\u2019s accuracy. Graph Deep Learning (GDL) is an emerging field of machine learning for tasks like scene interpretation, as it exploits flexible graph structures to describe complex features and relationships in a very concise format. However, due to the unconventional structure of the graphs, traditional explainability methods used in eXplainable AI (XAI) require further adaptation and thus, graph-specific methods are introduced. Attention is a powerful tool, introduced to estimate the importance of input features in deep learning models. It has been previously used to provide feature-based explanations on the predictions of GNN models. In our proposed work, we exploit graph attention to identify key semantic classes for lidar pointcloud pose estimation. We extend the current attention-based graph explainability methods by investigating the use of attention weights as importance indicators of semantically sorted feature sets by analysing the correlation between attention weights distribution and model accuracy. Our method has shown promising results for post-hoc semantic explanation of graph-based pose estimation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220785800",
                        "name": "Efimia Panagiotaki"
                    },
                    {
                        "authorId": "7764753",
                        "name": "D. Martini"
                    },
                    {
                        "authorId": "51177608",
                        "name": "Lars Kunze"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "\u2022 BA 2Motifs (Luo et al., 2020) is a synthetic graph classification dataset.",
                "Luo et al. (2020) follow a similar idea but emphasize finding structures that explain multiple instances at the same time.",
                "We use the Infection and Negative Evidence benchmarks from Faber et al. (2021), The BA-Shapes, Tree-Cycle, and Tree-Grid benchmarks from Ying et al. (2019), and the BA2Motifs dataset from Luo et al. (2020)."
            ],
            "citingPaper": {
                "paperId": "57a9eb4f3d9f614d5507f1f79b20a91a08edfffc",
                "externalIds": {
                    "CorpusId": 259926240
                },
                "corpusId": 259926240,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/57a9eb4f3d9f614d5507f1f79b20a91a08edfffc",
                "title": "GraphChef: Learning the Recipe of Your Dataset",
                "abstract": "We propose a new graph model, GraphChef, that enables us to understand graph datasets as a whole. Given a dataset, GraphChef returns a set of rules (a recipe) that describes each class in the dataset. Existing GNNs and explanation methods reason on individual graphs not on the entire dataset. GraphChef uses decision trees to build recipes that are understandable by humans. We show how to compute decision trees in the message passing framework in order to create GraphChef. We also present a new pruning method to produce small and easy to digest trees. In the experiments, we present and analyze GraphChef\u2019s recipes for Reddit-Binary, MUTAG, BA-2Motifs, BA-Shapes, Tree-Cycle, and Tree-Grid. We verify the correctness of the discovered recipes against the datasets\u2019 ground truth.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2223560094",
                        "name": "Peter M\u00fcller"
                    },
                    {
                        "authorId": "36352356",
                        "name": "Lukas Faber"
                    },
                    {
                        "authorId": "1995092493",
                        "name": "Karolis Martinkus"
                    },
                    {
                        "authorId": "2075356250",
                        "name": "Roger Wattenhofer"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "PGExplainer [3] trains a mask predictor to generate a discrete masks for learning the importance."
            ],
            "citingPaper": {
                "paperId": "d8b9bfe6485870e3076cd72a905a2715746d4422",
                "externalIds": {
                    "CorpusId": 260604556
                },
                "corpusId": 260604556,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d8b9bfe6485870e3076cd72a905a2715746d4422",
                "title": "Graph Model Explainer Tool",
                "abstract": "Graph Neural Networks (GNNs) have gained popularity in various fields, such as recommendation systems, social network analysis and fraud detection. However, despite their effectiveness, the topological nature of GNNs makes it challenging for users to understand the model predictions. To address this challenge, we built a user-friendly UI to visualize the most important relationships for both homogeneous and heterogeneous static graphs models, which a post-hoc explanation technique called GNNExplainer is implemented. This UI can be applied to a wide range of applications that use graph models. It offers an intuitive and interpretable way for users to understand the complex relationships within a graph and how they influence the model\u2019s predictions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2226180643",
                        "name": "Yudi Zhang"
                    },
                    {
                        "authorId": "2228929658",
                        "name": "Naveed Janvekar"
                    },
                    {
                        "authorId": "2228930523",
                        "name": "Phanindra Reddy Madduru"
                    },
                    {
                        "authorId": "2227971277",
                        "name": "Nitika Bhaskar"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "\u2022 PGExplainer PGExplainer was proposed after GNNExplainer in order to provide a more comprehensive understanding on the predictions made by GNNs.[11] It uses an optimization framework that is similar to GNNExplainer, however it uses continuous variables in the range (0, 1) for the edge weights.",
                "9\n\u2022 PGExplainer\nPGExplainer was proposed after GNNExplainer in order to provide a more comprehensive understanding on the predictions made by GNNs.[11] It uses an optimization framework that is similar to GNNExplainer, however it uses continuous variables in the range (0, 1) for the edge weights."
            ],
            "citingPaper": {
                "paperId": "85f4aba430387a337ec3a4b2aa39bfc7361dea1f",
                "externalIds": {
                    "CorpusId": 260960001
                },
                "corpusId": 260960001,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/85f4aba430387a337ec3a4b2aa39bfc7361dea1f",
                "title": "Evaluation of Explainability Methods on Single-Cell Classification Tasks Using Graph Neural Networks",
                "abstract": "The emergence of single cell sequencing data has led to new developments in using Graph Neural Networks (GNNs) to classify cells. In particular, these methods have been crucial for finding explanations that reveal underlying cellular mechanisms for GNNs, which are known to be \u201copaque\u201d boxes. However, there is still a lack of research on how reliable the explanations are, mostly due to di\ufb00iculties in obtaining datasets with known ground truth graphs. To this end, this thesis evaluates two graph neural network explanation methods (GNNExplainer, PGExplainer) on synthetic datasets with a known ground truth network and varying levels of sparsity produced using SERGIO. A binary graph classifier is then trained using the GraphSage convolution network. After being generated using the aforementioned explanation methods, the explanations are then evaluated using fidelity and sparsity metrics. The results revealed that GNNExplainer consistently outperformed the other meth-ods. However, the fluctuations in the evaluation metrics due to the use of gene inference methods and sparsity in the datasets indicate that further evaluations are needed to test the reliability of explanation models for high sparsity, complex single cell data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2232118403",
                        "name": "Professor Ritambhara Singh"
                    },
                    {
                        "authorId": "2232013567",
                        "name": "Momoka Kobayashi"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [],
            "contexts": [
                "In fact, GNNExplainer, PGExplainer, and SubgraphX can never generate explanations including only disconnected oxygen atoms but not nitrogen atoms like GStarX, because the former two solve the explanation problem by optimizing edges (as opposed to Equation 3), and the latter requires connectedness.",
                ", 2019), PGExplainer (Luo et al., 2020), SubgraphX (Yuan et al.",
                "GStarX is not as fast as GNNExplainer, PGExplainer, and GraphSVX, but it is about more than two times faster than SubgraphX.",
                "BA2Motifs (Luo et al., 2020) is a synthetic dataset for graph classification.",
                "GNNExplainer and PGExplainer choose some but not all important words, with extra neutral words appearing in the explanations as well.",
                "PGExplainer (Luo et al., 2020) uses the same scoring function, but generates a discrete mask on edges by training an edge mask predictor.",
                "We compare with 4 strong baselines representing the state-of-the-art methods for GNN explanation: GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), SubgraphX (Yuan et al., 2021), and GraphSVX (Duval & Malliaros, 2021)."
            ],
            "citingPaper": {
                "paperId": "699a946c7e519e4f288baae422bae8920070bff3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-12380",
                    "CorpusId": 246430826
                },
                "corpusId": 246430826,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/699a946c7e519e4f288baae422bae8920070bff3",
                "title": "Explaining Graph-level Predictions with Communication Structure-Aware Cooperative Games",
                "abstract": "Explaining predictions made by machine learning models is important and have attracted an increased interest. The Shapley value from cooperative game theory has been proposed as a prime approach to compute feature importances towards predictions, especially for images, text, tabular data, and recently graph neural networks (GNNs) on graphs. In this work, we revisit the appropriate-ness of the Shapley value for graph explanation, where the task is to identify the most important subgraph and constituent nodes for graph-level predictions. We purport that the Shapley value is a no-ideal choice for graph data because it is by def-inition not structure-aware. We propose a Graph Structure-aware eXplanation (GStarX) method to leverage the critical graph structure information to improve the explanation. Speci\ufb01cally, we propose a scoring function based on a new structure-aware value from the cooperative game theory called the HN value. When used to score node importance, the HN value utilizes graph structures to attribute cooperation surplus between neighbor nodes, re-sembling message passing in GNNs, so that node importance scores re\ufb02ect not only the node feature importance, but also the structural roles. We demonstrate that GstarX produces qualitatively more intuitive explanations, and quantitatively improves over strong baselines on chemical graph property prediction and text graph sentiment classi\ufb01cation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145408511",
                        "name": "Shichang Zhang"
                    },
                    {
                        "authorId": "145474474",
                        "name": "Neil Shah"
                    },
                    {
                        "authorId": "152891495",
                        "name": "Yozen Liu"
                    },
                    {
                        "authorId": "2109461904",
                        "name": "Yizhou Sun"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We construct 3-class synthetic datasets based on BAMotif (Ying et al., 2019; Luo et al., 2020) following (Wu et al.",
                "To begin with, we construct 3-class synthetic datasets based on BAMotif (Luo et al., 2020) and follow Wu et al.",
                "Built upon the graph generation process, can existing methods produce a desired invariant GNN model? Using the BAMotif task (Luo et al., 2020) as Fig.",
                "2.2\nTo begin with, we construct 3-class synthetic datasets based on BAMotif (Luo et al., 2020) and follow Wu et al. (2022c) to inject spurious correlations between motif graph and base graph during the generation.",
                "We construct 3-class synthetic datasets based on BAMotif (Ying et al., 2019; Luo et al., 2020) following (Wu et al., 2022c), where the model needs to tell which one of three motifs (House, Cycle, Crane) that the graph contains.",
                "Using the BAMotif task (Luo et al., 2020) as Fig."
            ],
            "citingPaper": {
                "paperId": "7c15ee1018a87708815c359d920820d8742dd3c8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-05441",
                    "CorpusId": 246823193
                },
                "corpusId": 246823193,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7c15ee1018a87708815c359d920820d8742dd3c8",
                "title": "Invariance Principle Meets Out-of-Distribution Generalization on Graphs",
                "abstract": "Despite recent success in using the invariance principle for out-of-distribution (OOD) generalization on Euclidean data (e.g., images), studies on graph data are still limited. Different from images, the complex nature of graphs poses unique challenges to adopting the invariance principle. In particular, distribution shifts on graphs can appear in a variety of forms such as attributes and structures, making it dif\ufb01cult to identify the invariance. Moreover, domain or environment partitions, which are often required by OOD methods on Euclidean data, could be highly expensive to obtain for graphs. To bridge this gap, we propose a new framework to capture the invariance of graphs for guaranteed OOD generalization under various distribution shifts. Speci\ufb01cally, we characterize potential distribution shifts on graphs with causal models, concluding that OOD generalization on graphs is achievable when models focus only on subgraphs containing the most information about the causes of labels. Accordingly, we propose an information-theoretic objective to extract the desired subgraphs that maximally preserve the invariant intra-class information. Learning with these subgraphs is immune to distribution shifts. Extensive experiments on both synthetic and real-world datasets, including a challenging setting in AI-aided drug discovery, validate the su-perior OOD generalization ability of our method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108962670",
                        "name": "Yongqiang Chen"
                    },
                    {
                        "authorId": "2109116068",
                        "name": "Yonggang Zhang"
                    },
                    {
                        "authorId": "50841357",
                        "name": "Han Yang"
                    },
                    {
                        "authorId": "47737190",
                        "name": "Kaili Ma"
                    },
                    {
                        "authorId": "2051756680",
                        "name": "Binghui Xie"
                    },
                    {
                        "authorId": "121698214",
                        "name": "Tongliang Liu"
                    },
                    {
                        "authorId": "2153287285",
                        "name": "Bo Han"
                    },
                    {
                        "authorId": "1717691",
                        "name": "James Cheng"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "We compare the proposed method with popular post-hoc explanation techniques including the GNN-Explainer (Ying et al. 2019), PGEExplainer (Luo et al. 2020), GradCAM (Pope et al. 2019) and SubgraphX (Yuan et al. 2021)2.",
                "2019), PGEExplainer (Luo et al. 2020), GradCAM (Pope et al.",
                "To this end, we use the BA2Motifs dataset (Luo et al. 2020).",
                "BA2Motifs is a synthetic dataset that was first introduced in Luo et al. (2020).",
                "(Ying et al. 2019; Luo et al. 2020; Funke, Khosla, and Anand 2021; Loveland et al. 2021;\nSchlichtkrull, Cao, and Titov 2021; Yuan et al. 2021; Perotti et al. 2022)."
            ],
            "citingPaper": {
                "paperId": "1067830966ce4f3c3e3b039adff6c8bc9be988cd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-14402",
                    "DOI": "10.48550/arXiv.2209.14402",
                    "CorpusId": 252596276
                },
                "corpusId": 252596276,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1067830966ce4f3c3e3b039adff6c8bc9be988cd",
                "title": "Learning to Explain Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) are a popular class of ma- chine learning models. Inspired by the learning to explain (L2X) paradigm, we propose L2 X G NN , a framework for ex- plainable GNNs which provides faithful explanations by design. L2 X G NN learns a mechanism for selecting explanatory subgraphs (motifs) which are exclusively used in the GNNs message-passing operations. L2 X G NN is able to select, for each input graph, a subgraph with speci\ufb01c properties such as being sparse and connected. Imposing such constraints on the motifs often leads to more interpretable and effective explanations. Experiments on several datasets suggest that L2 X G NN achieves the same classi\ufb01cation accuracy as baseline methods using the entire input graph while ensuring that only the provided explanations are used to make predictions. Moreover, we show that L2 X G NN is able to identify motifs responsible for the graph\u2019s properties it is intended to predict.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2275344",
                        "name": "G. Serra"
                    },
                    {
                        "authorId": "2780262",
                        "name": "Mathias Niepert"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Methods like GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020) and PGMexplainer (Vu & Thai, 2020) allow importance analysis of nodes and edges within the input data sample.",
                "GNNExplainer (Ying et al., 2019) and PGExplainer (Luo et al., 2020) explain GNNs by finding masks that maximize the mutual information between\nthe predictions of the original graph and a masked graph.",
                "BA2-Motif\nBA-2motif (Luo et al., 2020) is a synthetic dataset of graphs that can be classified into two classes according to the different motifs.",
                "Understanding the relevance contribution of subgraphs in the input graph to the model prediction is a key challenge when explaining models on graphs (Yuan et al., 2021; Luo et al., 2020; Schnake et al., 2021).",
                "GNNExplainer learns soft masks for edges or node features, while PGExplainer trains a parametric predictor to determine if an edge should be masked out.",
                "We used the following five popular datasets: BA-2motif (Luo et al., 2020), MUTAG (Debnath et al., 1991), Mutagenicity (Kazius et al., 2005b), REDDIT-BINARY (Yanardag & Vishwanathan, 2015), and Graph-SST2 (Yuan et al., 2020b).",
                "Here, we use the BA-2motif dataset to evaluate the node ordering performance of subgraph attribution.",
                "We first evaluate the node ordering performance on the BA-2motif dataset, for which the ground truth is available.",
                "Most explainability techniques (Pope et al., 2019; Ying et al., 2019; Luo et al., 2020) for GNNs explains the model at the level of nodes, edges and node features, while a few of them, including SubgraphX (Yuan et al., 2021) and GNN-LRP (Schnake et al., 2021), analyze the relevance of subgraphs as\u2026",
                "Figure 4 shows the computation time for subgraph attribution on BA-2motif, as functions of (a) the network depth L and (b) the subgraph size |S|, respectively."
            ],
            "citingPaper": {
                "paperId": "bd0769c931e27ba638cc58b20f2dcfa9d16473d9",
                "externalIds": {
                    "CorpusId": 253456746
                },
                "corpusId": 253456746,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bd0769c931e27ba638cc58b20f2dcfa9d16473d9",
                "title": "Efficient Higher-order Subgraph Attribution via Message Passing",
                "abstract": "Explaining graph neural networks (GNNs) has become more and more important recently. Higher-order interpretation schemes, such as GNN-LRP (layer-wise relevance propagation for GNN), emerged as powerful tools for unraveling how different features interact thereby contributing to explaining GNNs. GNN-LRP gives a relevance attribution of walks between nodes at each layer, and the subgraph attribution is expressed as a sum over exponentially many such walks. In this work, we demonstrate that such exponential complexity can be avoided. In particular, we propose novel algorithms that enable to attribute subgraphs with GNN-LRP in linear-time (w.r.t. the network depth). Our algorithms are derived via message passing techniques that make use of the distributive property, thereby directly computing quantities for higher-order explanations. We further adapt our efficient algorithms to compute a generalization of subgraph attributions that also takes into account the neighboring graph features. Experimental results show the significant acceler-ation of the proposed algorithms and demonstrate the high usefulness and scalability of our novel generalized subgraph attribution method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2062775158",
                        "name": "Pin Xiong"
                    },
                    {
                        "authorId": "90387439",
                        "name": "Thomas Schnake"
                    },
                    {
                        "authorId": "144535526",
                        "name": "G. Montavon"
                    },
                    {
                        "authorId": "2113612432",
                        "name": "Klaus-Robert M\u00fcller"
                    },
                    {
                        "authorId": "2055678647",
                        "name": "S. Nakajima"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "In this context, the most important features are those that lead to similar predictions once retained [24,52,33,54]."
            ],
            "citingPaper": {
                "paperId": "33037ce7a4024973a0203e7ac718c14068b18241",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-08815",
                    "CorpusId": 246904782
                },
                "corpusId": 246904782,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/33037ce7a4024973a0203e7ac718c14068b18241",
                "title": "GRAPHSHAP: Motif-based Explanations for Black-box Graph Classifiers",
                "abstract": "Most methods for explaining black-box classifiers (e.g., on tabular data, images, or time series) rely on measuring the impact that the removal/perturbation of features has on the model output. This forces the explanation language to match the classifier features space. However, when dealing with graph data, in which the basic features correspond essentially to the adjacency information describing the graph structure (i.e., the edges), this matching between features space and explanation language might not be appropriate. In this regard, we argue that (i) a good explanation method for graph classification should be fully agnostic with respect to the internal representation used by the black-box; and (ii) a good explanation language for graph classification tasks should be represented by higher-order structures, such as motifs. The need to decouple the feature space (edges) from the explanation space (motifs) is thus a major challenge towards developing actionable explanations for graph classification tasks. In this paper we introduce GRAPHSHAP, a Shapley-based approach able to provide motif-based explanations for black-box graph classifiers, assuming no knowledge whatsoever about the model or its training data: the only requirement is that the black-box can be queried at will. For the sake of computational efficiency we explore a progressive approximation strategy and show how a simple kernel can efficiently approximate explanation scores, thus allowing GRAPHSHAP to scale on scenarios with a large explanation space (i.e., large number of motifs). We devise a synthetic dataset generator with artificially injected motifs in order to empirically compare different masking approaches and to demonstrate that the proposed kernel is able to approximate the exact Shapley values with a computational complexity that is linear with respect to the number of explained features. Furthermore, we introduce additional auxiliary components such as a custom graph convolutional layer and algorithms for motif mining and ranking. Finally, we test GRAPHSHAP on a real-world brain-network dataset consisting of patients affected by Autism Spectrum Disorder and a control group. Our experiments highlight how the classification provided by a black-box model can be effectively explained by few connectomics patterns.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "26582424",
                        "name": "A. Perotti"
                    },
                    {
                        "authorId": "46726748",
                        "name": "P. Bajardi"
                    },
                    {
                        "authorId": "1705764",
                        "name": "F. Bonchi"
                    },
                    {
                        "authorId": "2735649",
                        "name": "A. Panisson"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "0385e920e79fe5e3c698ca125306adb4bd2ab07d",
                "externalIds": {
                    "DBLP": "conf/f-egc/Veyrin-ForrerKD22",
                    "CorpusId": 247296811
                },
                "corpusId": 247296811,
                "publicationVenue": {
                    "id": "c8547021-ed28-4f5e-a67e-9198df5d4354",
                    "name": "European Grid Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Eur Grid Conf",
                        "EGC",
                        "Extraction et Gestion des Connaissances",
                        "Extr Gest Connaissances",
                        "Spanish Meeting on Computational Geometry",
                        "Workshop Embrac Glob Comput Emerg Econ",
                        "Workshop on Embracing Global Computing in Emerging Economies",
                        "Span Meet Comput Geom"
                    ],
                    "url": "http://www.polytech.univ-nantes.fr/associationEGC/index.html"
                },
                "url": "https://www.semanticscholar.org/paper/0385e920e79fe5e3c698ca125306adb4bd2ab07d",
                "title": "Qu'est-ce que mon GNN capture vraiment ? Exploration des repr\u00e9sentations internes d'un GNN",
                "abstract": "R\u00e9sum\u00e9. Nous consid\u00e9rons l\u2019explication de GNN. Alors que les travaux existants expliquent la d\u00e9cision du mod\u00e8le en s\u2019appuyant sur la couche de sortie, nous cherchons \u00e0 analyser les couches cach\u00e9es pour identifier les attributs construits par le GNN. Nous extrayons d\u2019abord des r\u00e8gles d\u2019activation qui identifient des ensembles de neurones co-activ\u00e9s pour une classe. Ces r\u00e8gles d\u00e9finissent des repr\u00e9sentations internes ayant un impact fort sur la classification. Ensuite, nous associons \u00e0 celles-ci un graphe dont le plongement produit par le GNN est tr\u00e8s proche de celui identifi\u00e9 par la r\u00e8gle. Des exp\u00e9riences sur 6 jeux de donn\u00e9es et 3 baselines d\u00e9montrent que notre m\u00e9thode g\u00e9n\u00e8re des graphes r\u00e9alistes de haute qualit\u00e9.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2157976788",
                        "name": "Luca Veyrin-Forrer"
                    },
                    {
                        "authorId": "2157966180",
                        "name": "Ataollah Kamal"
                    },
                    {
                        "authorId": "2066952666",
                        "name": "Stefan Duffner"
                    },
                    {
                        "authorId": "2065863961",
                        "name": "Marc Plantevit"
                    },
                    {
                        "authorId": "2064598065",
                        "name": "C\u00e9line Robardet"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "The interpretation at the subgraph-level enables more intuitive and effective description of the GNN [134-136]."
            ],
            "citingPaper": {
                "paperId": "b45ea295f8bab57f48680df4610909131695fe64",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-06127",
                    "DOI": "10.48550/arXiv.2204.06127",
                    "CorpusId": 248157202
                },
                "corpusId": 248157202,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b45ea295f8bab57f48680df4610909131695fe64",
                "title": "Reinforcement learning on graph: A survey",
                "abstract": "Graph mining tasks arise from many different application domains, ranging from social networks, transportation, E-commerce, etc., which have been receiving great attention from the theoretical and algorithm design communities in recent years, and there has been some pioneering work using the hotly researched Reinforcement Learning (RL) techniques to address graph data mining tasks. However, these graph mining algorithms and RL models are dispersed in different research areas, which makes it hard to compare different algorithms with each other. In this survey, we provide a comprehensive overview of RL models and graph mining and generalize these algorithms to Graph Reinforcement Learning (GRL) as a unified formulation. We further discuss the applications of GRL methods across various domains and summarize the method description, open-source codes, and benchmark datasets of GRL methods. Finally, we propose possible important directions and challenges to be solved in the future. This is the latest work on a comprehensive survey of GRL literature, and this work provides a global view for scholars as well as a learning resource for scholars outside the domain. In addition, we create an online open-source for both interested scholars who want to enter this rapidly developing domain and experts who would like to compare GRL methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2153114137",
                        "name": "Mingshuo Nie"
                    },
                    {
                        "authorId": "3252139",
                        "name": "Dongming Chen"
                    },
                    {
                        "authorId": "2111215516",
                        "name": "Dongqi Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "533181183d2d76e752913f273a6d5ab3344829e0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-13733",
                    "DOI": "10.48550/arXiv.2205.13733",
                    "CorpusId": 249151859
                },
                "corpusId": 249151859,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/533181183d2d76e752913f273a6d5ab3344829e0",
                "title": "On Consistency in Graph Neural Network Interpretation",
                "abstract": "Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over recent years. Instance-level GNN explanation aims to discover critical input elements, like nodes or edges, that the target GNN relies upon for making predictions. These identi\ufb01ed sub-structures can provide interpretations of GNN\u2019s behavior. Though various algorithms are proposed, most of them formalize this task by searching the minimal subgraph which can preserve original predictions. An inductive bias is deep-rooted in this framework: the same output cannot guarantee that two inputs are processed under the same rationale. Consequently, they have the danger of providing spurious explanations and fail to provide consistent explanations. Applying them to explain weakly-performed GNNs would further amplify these issues. To address the issues, we propose to obtain more faithful and consistent explanations of GNNs. After a close examination on predictions of GNNs from the causality perspective, we attribute spurious explanations to two typical reasons: confounding effect of latent variables like distribution shift, and causal factors distinct from the original input. Motivated by the observation that both confounding effects and diverse causal rationales are encoded in internal representations, we propose a simple yet effective countermeasure by aligning embeddings. This new objective can be incorporated into existing GNN explanation algorithms with no effort. We implement both a simpli\ufb01ed version based on absolute distance and a distribution-aware version based on anchors. Experiments on 5 datasets validate its effectiveness, and theoretical analysis shows that it is in effect optimizing a more faithful explanation objective in design, which further justi\ufb01es the proposed approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1999191869",
                        "name": "Tianxiang Zhao"
                    },
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    },
                    {
                        "authorId": "48505793",
                        "name": "Xiang Zhang"
                    },
                    {
                        "authorId": "2116430057",
                        "name": "Suhang Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "\u2026be developed in Section 7 belongs to the explainablility category, where the objective is to discover qualitative justifications on the learning outcome of a GCN; see (Luo et al., 2020; Lin et al., 2021; Schlichtkrull et al., 2021; Ying et al., 2019; Yuan et al., 2021) and the references therein.",
                "The statistic model to be developed in Section 7 belongs to the explainablility category, where the objective is to discover qualitative justifications on the learning outcome of a GCN; see (Luo et al., 2020; Lin et al., 2021; Schlichtkrull et al., 2021; Ying et al., 2019; Yuan et al., 2021) and the references therein."
            ],
            "citingPaper": {
                "paperId": "14b610eeacb97d2ad5b40b8def1fcaadc52ee217",
                "externalIds": {
                    "DBLP": "conf/icml/TaoWD22",
                    "CorpusId": 250242203
                },
                "corpusId": 250242203,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/14b610eeacb97d2ad5b40b8def1fcaadc52ee217",
                "title": "Cross-Space Active Learning on Graph Convolutional Networks",
                "abstract": "This paper formalizes cross-space active learning on a graph convolutional network (GCN). The objective is to attain the most accurate hypothesis available in any of the instance spaces generated by the GCN. Subject to the objective, the chal-lenge is to minimize the label cost , measured in the number of vertices whose labels are requested. Our study covers both budget algorithms which terminate after a designated number of label requests, and verifiable algorithms which terminate only after having found an accurate hypothesis. A new separation in label complexity between the two algorithm types is established. The separation is unique to GCNs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144779196",
                        "name": "Yufei Tao"
                    },
                    {
                        "authorId": "1664776313",
                        "name": "Hao Wu"
                    },
                    {
                        "authorId": "148032393",
                        "name": "Shiyuan Deng"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": null,
                "externalIds": null,
                "corpusId": "251732960",
                "publicationVenue": null,
                "url": null,
                "title": "DIRECTIONAL FEATURE INTERACTIONS",
                "abstract": null,
                "year": 2022,
                "authors": []
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Numerous methods focus on identifying relevant subgraphs in the embedding space (Ying et al., 2019; Luo et al., 2020; Lin et al., 2021), but such approaches do not explain predictions symbolically\u2014that is, by showing how a prediction can be derived from the input KG via logical inferences of a\u2026",
                "Existing methods for explaining the predictions of a GNN focus on identifying parts of the graph in the embedding space that are most relevant to a given prediction (Ying et al., 2019; Luo et al., 2020; Lin et al., 2021).",
                "Numerous methods focus on identifying relevant subgraphs in the embedding space (Ying et al., 2019; Luo et al., 2020; Lin et al., 2021), but such approaches do not explain predictions symbolically\u2014that is, by showing how a prediction can be derived from the input KG via logical inferences of a knowledge representation formalism."
            ],
            "citingPaper": {
                "paperId": "cfaee3f8e967aa5f2c93aaf629888c40767f1bd6",
                "externalIds": {
                    "CorpusId": 251734865
                },
                "corpusId": 251734865,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cfaee3f8e967aa5f2c93aaf629888c40767f1bd6",
                "title": "E XPLAINABLE GNN-B ASED M ODELS OVER K NOWLEDGE G RAPHS",
                "abstract": "Graph Neural Networks (GNNs) are often used to learn transformations of graph data. While effective in practice, such approaches make predictions via numeric manipulations so their output cannot be easily explained symbolically. We pro-pose a new family of GNN-based transformations of graph data that can be trained effectively, but where all predictions can be explained symbolically as logical inferences in Datalog\u2014a well-known rule-based formalism. In particular, we show how to encode an input knowledge graph into a graph with numeric feature vectors, process this graph using a GNN, and decode the result into an output knowledge graph. We use a new class of monotonic GNNs (MGNNs) to ensure that this process is equivalent to a round of application of a set of Datalog rules. We also show that, given an arbitrary MGNN, we can automatically extract rules that completely characterise the transformation. We evaluate our approach by applying it to classi\ufb01cation tasks in knowledge graph completion.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2114630929",
                        "name": "David J. Tena Cucala"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "The explanation is 1365 attributable to subgraph decomposition theory [198], where 1366 it is feasible to determine whether the learned model is inter- 1367 pretable by identifying the subgraph with the most significant 1368 influence on prediction and judging whether the subgraph is 1369 faithful to general knowledge."
            ],
            "citingPaper": {
                "paperId": "9196214e8f81b4a6efda6c8891a6365bccf865df",
                "externalIds": {
                    "DBLP": "journals/access/CapuanoFLS22",
                    "DOI": "10.1109/ACCESS.2022.3204171",
                    "CorpusId": 252107164
                },
                "corpusId": 252107164,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9196214e8f81b4a6efda6c8891a6365bccf865df",
                "title": "Explainable Artificial Intelligence in CyberSecurity: A Survey",
                "abstract": "Nowadays, Artificial Intelligence (AI) is widely applied in every area of human being\u2019s daily life. Despite the AI benefits, its application suffers from the opacity of complex internal mechanisms and doesn\u2019t satisfy by design the principles of Explainable Artificial Intelligence (XAI). The lack of transparency further exacerbates the problem in the field of CyberSecurity because entrusting crucial decisions to a system that cannot explain itself presents obvious dangers. There are several methods in the literature capable of providing explainability of AI results. Anyway, the application of XAI in CyberSecurity can be a double-edged sword. It substantially improves the CyberSecurity practices but simultaneously leaves the system vulnerable to adversary attacks. Therefore, there is a need to analyze the state-of-the-art of XAI methods in CyberSecurity to provide a clear vision for future research. This study presents an in-depth examination of the application of XAI in CyberSecurity. It considers more than 300 papers to comprehensively analyze the main CyberSecurity application fields, like Intrusion Detection Systems, Malware detection, Phishing and Spam detection, BotNets detection, Fraud detection, Zero-Day vulnerabilities, Digital Forensics and Crypto-Jacking. Specifically, this study focuses on the explainability methods adopted or proposed in these fields, pointing out promising works and new challenges.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2067064455",
                        "name": "N. Capuano"
                    },
                    {
                        "authorId": "2721973",
                        "name": "G. Fenza"
                    },
                    {
                        "authorId": "2061104792",
                        "name": "V. Loia"
                    },
                    {
                        "authorId": "2121344176",
                        "name": "Claudio Stanzione"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "8daa07ebc32c0ff54383e5c244b85bea44e542fe",
                "externalIds": {
                    "CorpusId": 253481386
                },
                "corpusId": 253481386,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8daa07ebc32c0ff54383e5c244b85bea44e542fe",
                "title": "On GNN explanability with activation rules",
                "abstract": "GNNs are powerful models based on node representation learning that perform particularly well in many machine learning problems related to graphs. The major obstacle to the deployment of GNNs is mostly a problem of societal acceptability and trustworthiness, properties which require making explicit the internal functioning of such models. Here, we propose to mine activation rules in the hidden layers to understand how the GNNs perceive the world. The problem is not to discover activation rules that are individually highly discriminating for an output of the model. Instead, the challenge is to provide a small set of rules that cover all input graphs. To this end, we introduce the subjective activation pattern domain. We de\ufb01ne an e\ufb00ective and principled algorithm to enumerate activations rules in each hidden layer. The proposed approach for quantifying the interest of these rules is rooted in information theory and is able to account for background knowledge on the input graph data. The activation rules can then be redescribed thanks to pattern languages involving interpretable features. We show that the activation rules provide insights on the characteristics used by the GNN to classify the graphs. Especially, this allows to identify the hidden features built by the GNN through its di\ufb00erent layers. Also, these rules can subsequently be used for explaining GNN decisions. Experiments on both synthetic and real-life datasets show highly competitive performance, with up to 200% improvement in \ufb01delity on explaining graph classi\ufb01cation over the SOTA methods. The activation pattern set is then used to provide instance-level explanations. To this end, several mask strategies involving nodes that support activation rules are devised. (7) For each activation rule, we use exploratory analysis techniques (e.g., subgroup discovery on graph propositionalization, subgraph mining) to characterize the nodes supporting the rules and provide interpretable insights on what the GNN really captures.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1452681405",
                        "name": "Luca Veyrin-Forrer"
                    },
                    {
                        "authorId": "2158653859",
                        "name": "Ataollah Kamal"
                    },
                    {
                        "authorId": "2244635954",
                        "name": "Stefan Du\ufb00ner"
                    },
                    {
                        "authorId": "2331463",
                        "name": "M. Plantevit"
                    },
                    {
                        "authorId": "1763302",
                        "name": "C. Robardet"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "c1fba66d589248ffc4d044c72d19036dc1c2f160",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-08966",
                    "DOI": "10.48550/arXiv.2212.08966",
                    "CorpusId": 254853889
                },
                "corpusId": 254853889,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c1fba66d589248ffc4d044c72d19036dc1c2f160",
                "title": "Graph Learning: A Comprehensive Survey and Future Directions",
                "abstract": "\u2014Graph learning aims to learn complex relationships among nodes and the topological structure of graphs, such as social networks, academic networks and e-commerce networks, which are common in the real world. Those relationships make graphs special compared with traditional tabular data in which nodes are dependent on non-Euclidean space and contain rich information to explore. Graph learning developed from graph theory to graph data mining and now is empowered with representation learning, making it achieve great performances in various scenarios, even including text, image, chemistry, and biology. Due to the broad application prospects in the real world, graph learning has become a popular and promising area in machine learning. Thousands of works have been proposed to solve various kinds of problems in graph learning and is appealing more and more attention in academic community, which makes it pivotal to survey previous valuable works. Although some of the researchers have noticed this phenomenon and \ufb01nished impressive surveys on graph learning. However, they failed to link related objectives, methods and applications in a more logical way and cover current ample scenarios as well as challenging problems due to the rapid expansion of the graph learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1409866591",
                        "name": "Shaopeng Wei"
                    },
                    {
                        "authorId": "97522134",
                        "name": "Yu Zhao"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Numerous methods focus on identifying relevant subgraphs in the embedding space (Ying et al., 2019; Luo et al., 2020; Lin et al., 2021), but such approaches do not explain predictions symbolically\u2014that is, by showing how a prediction can be derived from the input KG via logical inferences of a\u2026",
                "Existing methods for explaining the predictions of a GNN focus on identifying parts of the graph in the embedding space that are most relevant to a given prediction (Ying et al., 2019; Luo et al., 2020; Lin et al., 2021).",
                "Numerous methods focus on identifying relevant subgraphs in the embedding space (Ying et al., 2019; Luo et al., 2020; Lin et al., 2021), but such approaches do not explain predictions symbolically\u2014that is, by showing how a prediction can be derived from the input KG via logical inferences of a knowledge representation formalism."
            ],
            "citingPaper": {
                "paperId": "18637c36a2737e51051113e56b4438b06b568a3d",
                "externalIds": {
                    "DBLP": "conf/iclr/CucalaGKM22",
                    "CorpusId": 251647315
                },
                "corpusId": 251647315,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/18637c36a2737e51051113e56b4438b06b568a3d",
                "title": "Explainable GNN-Based Models over Knowledge Graphs",
                "abstract": "Graph Neural Networks (GNNs) are often used to learn transformations of graph data. While effective in practice, such approaches make predictions via numeric manipulations so their output cannot be easily explained symbolically. We propose a new family of GNN-based transformations of graph data that can be trained effectively, but where all predictions can be explained symbolically as logical inferences in Datalog\u2014a well-known rule-based formalism. In particular, we show how to encode an input knowledge graph into a graph with numeric feature vectors, process this graph using a GNN, and decode the result into an output knowledge graph. We use a new class of monotonic GNNs (MGNNs) to ensure that this process is equivalent to a round of application of a set of Datalog rules. We also show that, given an arbitrary MGNN, we can automatically extract rules that completely characterise the transformation. We evaluate our approach by applying it to classification tasks in knowledge graph completion.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2114630929",
                        "name": "David J. Tena Cucala"
                    },
                    {
                        "authorId": "1784440",
                        "name": "B. C. Grau"
                    },
                    {
                        "authorId": "1783695",
                        "name": "Egor V. Kostylev"
                    },
                    {
                        "authorId": "1703160",
                        "name": "B. Motik"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Consequently, an increasing number of works are focussing on explaining [11, 12, 13, 14] the decisions of",
                "In terms of methodologies, several techniques based on input perturbations [11, 12, 13], input gradients[36, 37], causal techniques [34, 38, 33] as well as utilizing simpler surrogate models [14] have been explored.",
                "Explanations usually include the importance scores for nodes/edges in a subgraph (or node\u2019s neighborhood in case of node-level task) and the node features [11, 12, 13]."
            ],
            "citingPaper": {
                "paperId": "43cc3e235f743c86edca32eed8e4931afcdf3639",
                "externalIds": {
                    "DBLP": "conf/cikm/Kosla22",
                    "CorpusId": 255547926
                },
                "corpusId": 255547926,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/43cc3e235f743c86edca32eed8e4931afcdf3639",
                "title": "Privacy and transparency in graph machine learning: A unified perspective",
                "abstract": "Graph Machine Learning (GraphML), whereby classical machine learning is generalized to irregular graph domains, has enjoyed a recent renaissance, leading to a dizzying array of models and their applications in several domains. With its growing applicability to sensitive domains and regulations by governmental agencies for trustworthy AI systems, researchers have started looking into the issues of transparency and privacy of graph learning. However, these topics have been mainly investigated independently. In this position paper, we provide a unified perspective on the interplay of privacy and transparency in GraphML. In particular, we describe the challenges and possible research directions for a formal investigation of privacy-transparency tradeoffs in GraphML.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2199752197",
                        "name": "Megha Kosla"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Besides, learning M for each graph G separately hinders the method from handling unseen test graphs [14]."
            ],
            "citingPaper": {
                "paperId": "25a71df8e58a4eec89f1f46649f8781a1dbbfb32",
                "externalIds": {
                    "DBLP": "conf/nips/LiZ0022",
                    "CorpusId": 258509573
                },
                "corpusId": 258509573,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/25a71df8e58a4eec89f1f46649f8781a1dbbfb32",
                "title": "Learning Invariant Graph Representations for Out-of-Distribution Generalization",
                "abstract": "Graph representation learning has shown effectiveness when testing and training graph data come from the same distribution, but most existing approaches fail to generalize under distribution shifts. Invariant learning, backed by the invariance principle from causality, can achieve guaranteed generalization under distribution shifts in theory and has shown great successes in practice. However, invariant learning for graphs under distribution shifts remains unexplored and challenging. To solve this problem, we propose Graph Invariant Learning ( GIL ) model capable of learning generalized graph representations under distribution shifts. Our proposed method can capture the invariant relationships between predictive graph structural information and labels in a mixture of latent environments through jointly optimizing three tailored modules. Specifically, we first design a GNN-based subgraph generator to identify invariant subgraphs. Then we use the variant subgraphs, i.e., complements of invariant subgraphs, to infer the latent environment labels. We further propose an invariant learning module to learn graph representations that can generalize to unknown test graphs. Theoretical justifications for our proposed method are also provided. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of our method against state-of-the-art baselines under distribution shifts for the graph classification task.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145538097",
                        "name": "Haoyang Li"
                    },
                    {
                        "authorId": "2116460208",
                        "name": "Ziwei Zhang"
                    },
                    {
                        "authorId": "153316152",
                        "name": "Xin Wang"
                    },
                    {
                        "authorId": "2156154955",
                        "name": "Wenwu Zhu"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "\u2022 PGExplainer [7] hires a neural network to learn to generate the masks for the input edges.",
                "We follow previous works [6, 7, 10, 19] and focus on the contributions of the structural features (i.",
                "We combine the contrastive learning [17, 18] into class-wise generative probabilistic models [7], thereby approach coarser-grained explanations (i.",
                "Inspired by the success of generative models [7, 30, 31] in capturing the succinct structures from the graphs, we hire multiple generative probabilistic models [7] as our attribution models (short for attributor), i.",
                "Thus, they lack the global understanding of the model\u2019s workings [7, 13], which is vital to generalize to other instances being explained.",
                "We ascribe this to the limitations of PGExplainer\u2019s global view, which is founded upon all the explained instances, but fails to differentiate the class-wise patterns.",
                "However, most of current explainers focus on either on local [9, 10, 6, 11, 12] or global explainability [13, 7], thereby suffer from inherent limitations correspondingly: \u2217Xiangnan He is the corresponding author.",
                "To approximate the importance score to the discrete distribution and optimize the generator via gradient propagation, we adopt the reparameterization trick [7], where an independent random variable \u223c Uniform(0, 1) is introduced.",
                "\u2022 Although PGExplainer is also equipped with the global view of the target model, its performance is worse than that of ReFine-FT.",
                "It is worth emphasizing that our attributors is different from PGExplainer [7], where only one generative probabilistic model is involved.",
                "The inference time [7] to explain a new instance by the pre-trained ReFine is the same as PGExplainer under the same attributor construction.",
                "To be more clear, we present the difference of PGExplainer [7], ReFine and its ablation models in Table 4.2.",
                "For the parametric explanation methods (GNNExplainer, PGExplainer, PGMExplainer), we apply a grid search to tune their own hyperparameters.",
                "To provide a global understanding of the model prediction, PGExplainer [7] formulates the generation of multiple explanations based on its collective and inductive property, and designs the attributor as a deep neural network whose parameters are shared across the explained instances.",
                "It is also worth mentioning that, although the general understanding of GNN predictions has been considered in a recent work PGExplainer [7], it is only exploited to train a generative probabilistic model shared across all the explained instances, rather than dissecting and modeling the class-wise knowledge explicitly.",
                "Through this way, ReFine can faithfully generate multigrained explanations, and we empirically show its effectiveness as compared to some state-of-the-art explainers [9, 6, 7, 19].",
                "Such multi-grained explainability flexibly and reliably inspects the decision-making process of the GNN [4, 5], which is critical to the applications on safety, fairness, and privacy [6, 7]."
            ],
            "citingPaper": {
                "paperId": "8a8f853c9604bbe068c686e7d38b2c7560c36a1b",
                "externalIds": {
                    "DBLP": "conf/nips/WangWZHC21",
                    "CorpusId": 247405839
                },
                "corpusId": 247405839,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8a8f853c9604bbe068c686e7d38b2c7560c36a1b",
                "title": "Towards Multi-Grained Explainability for Graph Neural Networks",
                "abstract": "When a graph neural network (GNN) made a prediction, one raises question about explainability: \u201cWhich fraction of the input graph is most influential to the model\u2019s decision?\u201d Producing an answer requires understanding the model\u2019s inner workings in general and emphasizing the insights on the decision for the instance at hand. Nonetheless, most of current approaches focus only on one aspect: (1) local explainability, which explains each instance independently, thus hardly exhibits the class-wise patterns; and (2) global explainability, which systematizes the globally important patterns, but might be trivial in the local context. This dichotomy limits the flexibility and effectiveness of explainers greatly. A performant paradigm towards multi-grained explainability is until-now lacking and thus a focus of our work. In this work, we exploit the pre-training and fine-tuning idea to develop our explainer and generate multi-grained explanations. Specifically, the pre-training phase accounts for the contrastivity among different classes, so as to highlight the class-wise characteristics from a global view; afterwards, the fine-tuning phase adapts the explanations in the local context. Experiments on both synthetic and real-world datasets show the superiority of our explainer, in terms of AUC on explaining graph classification over the leading baselines. Our codes and datasets are available at https://github.com/Wuyxin/ReFine.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "98285513",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "10593442",
                        "name": "Yingmin Wu"
                    },
                    {
                        "authorId": "2153659066",
                        "name": "An Zhang"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    },
                    {
                        "authorId": "143779329",
                        "name": "Tat-Seng Chua"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Graph Neural Networks (GNNs) have emerged as powerful tools for effectively representing graph structured data, such as social, information, chemical, and biological networks.",
                "Explainability in Graph Neural Networks (GNNs).",
                "These are gradient-based methods: Gradients [41], Integrated Gradients [45]; perturbation-based methods: GNNExplainer [50], PGExplainer [30], GraphMASK [39]; and surrogate models: GraphLIME [17], PGMExplainer [47].",
                "GraphLIME [17] is a local interpretable model explanation for GNNs that identifies a nonlinear interpretable model over the neighbors of a node that is locally faithful to the node\u2019s prediction.",
                "With the reparameterization, the objective function of PGExplainer becomes:\nmin \u2126\nE \u223cUniform(0,1) H(Y |Gu = G\u0302S), (19)\nwhere H is the conditional entropy when the computational graph for Gu is restricted to GS .",
                "A variety of GNN architectures have been proposed in the literature [31, 52, 54], and recent research has focused on on developing GNN explanation methods [3, 8, 17, 28, 30, 33, 39, 47, 50].",
                "Notation: Graphs and GNNs.",
                "In contrast to GNNExplainer, PGExplainer [30] generates explanation only on the graph structure.",
                "GNNs specify non-linear deep transformation functions that map graph structures (i.e., nodes, edges or entire graphs) into compact vector embeddings [26].",
                "Based on the techniques they employ, these approaches can be broadly characterized into perturbation-based [30, 39, 50], gradient-based [41, 45], and surrogate model based [17, 47] methods [51].",
                "To this end, several approaches have been proposed in recent literature to explain the predictions of GNNs [3, 8, 17, 28, 30, 33, 39, 47, 50].",
                "In the context of GNNs, an explanation Eu corresponding to a node u is considered stable if the explanations corresponding to u (i.e., Eu) and its perturbation u\u2032 (denoted by E\u2032u) are similar.",
                "Due to the discrete nature of GS , PGExplainer employs the reparameterization trick where they relax the edge weights from binary to continuous variables in the range (0, 1) and then optimize the objective function using gradient-based methods.",
                "These bounds need only information about the general message passing form of GNNs [10] and do not make any assumptions about the GNN architecture.",
                "More recently, perturbation-based methods [30, 39, 50] explain GNN predictions by finding small subgraphs that are most influential for the prediction w.",
                "||(1 \u2212 ru) \u25e6 xu||2, where \u03b311 is a constant comprising\nof the product of the Lipschitz constant for GNN\u2019s activation function, weights of the final node classification layer, and self-attention weight of node u across all layers of the GNN.",
                "While several approaches have been proposed to explain the predictions of GNNs, evaluating the quality of the resulting explanations is non-trivial.",
                "PGExplainer.",
                "As GNNs are increasingly being employed in critical applications in domains such as financial lending and criminal justice, it becomes important to ensure that the explanations generated by state-of-the-art GNN explanation methods preserve the fairness properties of the underlying model.",
                "Thus, PGExplainer consider a relaxation by assuming that the explanatory graph GS is a Gilbert random graph, where selections of edges from the input graph Gu are conditionally independent to each other.",
                "Formally, GNNExplainer determines the importance of individual node attributes and incident edges for node u by leveraging Mutual Information (MI) using the following optimization framework:\nmax GS\nMI(Y, (GS ,XS)), (17)\nwhere GS \u2286 Gu is a subgraph and XS is the associated node attributes that are important for the GNN\u2019s prediction y\u0302u.",
                "Further, \u03b3 denotes the product of the Lipschitz constants for GNN\u2019s activation function and GNN\u2019s weight matrices across all layers in the GNN, and \u2206 is an explanation method-specific term.",
                "[30] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang."
            ],
            "citingPaper": {
                "paperId": "ecde6d2301213a8589849826111897fbf4d6d76c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-09078",
                    "CorpusId": 235458610
                },
                "corpusId": 235458610,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ecde6d2301213a8589849826111897fbf4d6d76c",
                "title": "Towards a Rigorous Theoretical Analysis and Evaluation of GNN Explanations",
                "abstract": "As Graph Neural Networks (GNNs) are increasingly employed in real-world applications, it becomes critical to ensure that the stakeholders understand the rationale behind their predictions. While several GNN explanation methods have been proposed recently, there has been little to no work on theoretically analyzing the behavior of these methods or systematically evaluating their effectiveness. Here, we introduce the first axiomatic framework for theoretically analyzing, evaluating, and comparing state-of-the-art GNN explanation methods. We outline and formalize the key desirable properties that all GNN explanation methods should satisfy in order to generate reliable explanations, namely, faithfulness, stability, and fairness. We leverage these properties to present the first ever theoretical analysis of the effectiveness of state-of-the-art GNN explanation methods. Our analysis establishes upper bounds on all the aforementioned properties for popular GNN explanation methods. We also leverage our framework to empirically evaluate these methods on multiple real-world datasets from diverse domains. Our empirical results demonstrate that some popular GNN explanation methods (e.g., gradient-based methods) perform no better than a random baseline and that methods which leverage the graph structure are more effective than those that solely rely on the node features.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40228633",
                        "name": "Chirag Agarwal"
                    },
                    {
                        "authorId": "2095762",
                        "name": "M. Zitnik"
                    },
                    {
                        "authorId": "1892673",
                        "name": "Himabindu Lakkaraju"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Taking the classifying mutagenic property of a molecular graph (Luo et al., 2020) as an example, G is a molecular graph where the nodes are atoms and the edges are the chemical bonds between atoms, and Y is the class label, e.g., whether the molecule is mutagenic or not.",
                "\u2026causes Z such as the factors to generate nitrogen dioxide (NO2) group, which has an determinative effect on the mutagenicity of molecule, and the effect of irrelevant variable M , such as the carbon ring which exist more frequently in mutagenic molecule but not determinative (Luo et al., 2020).",
                "Taking the classifying mutagenic property of a molecular graph (Luo et al., 2020) as an example, G is a molecular graph where the nodes are atoms and the edges are the chemical bonds between atoms, and Y is the class label, e.",
                ", 1991), as we could explain the results based on the knowledge used in (Luo et al., 2020).",
                "StableGNN correctly identifies chemicalNO2 andNH2, which are known to be mutagenic (Luo et al., 2020) while baselines fail in.",
                "The whole molecular graph G is an effect of relevant latent causes Z such as the factors to generate nitrogen dioxide (NO2) group, which has an determinative effect on the mutagenicity of molecule, and the effect of irrelevant variable M , such as the carbon ring which exist more frequently in mutagenic molecule but not determinative (Luo et al., 2020).",
                "Moreover, we also conduct the experiments on a commonly used graph classification dataset, MUTAG (Debnath et al., 1991), as we could explain the results based on the knowledge used in (Luo et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "fc50940a580dfd37e0d027175657ac44e41fa3aa",
                "externalIds": {
                    "CorpusId": 244478673
                },
                "corpusId": 244478673,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fc50940a580dfd37e0d027175657ac44e41fa3aa",
                "title": "AGNOSTIC LABEL SELECTION BIAS",
                "abstract": "Graph Neural Networks (GNNs) are proposed without considering the agnostic distribution shifts between training graphs and testing graphs, inducing the degeneration of the generalization ability of GNNs on Out-Of-Distribution (OOD) settings. The fundamental reason for such degeneration is that most GNNs are developed based on the I.I.D hypothesis. In such a setting, GNNs tend to exploit subtle statistical correlations existing in the training set for predictions, even though it is a spurious correlation. This learning mechanism inherits from the common characteristics of machine learning approaches. However, such spurious correlations may change in the wild testing environments, leading to the failure of GNNs. Therefore, eliminating the impact of spurious correlations is crucial for stable GNN models. To this end, in this paper, we argue that the spurious correlation exists among subgraph-level units and analyze the degeneration of GNN in causal view. Based on the causal view analysis, we propose a general causal representation framework for stable GNN, called StableGNN. The main idea of this framework is to extract high-level representations from raw graph data first and resort to the distinguishing ability of causal inference to help the model get rid of spurious correlations. Particularly, to extract meaningful high-level representations, we exploit a differentiable graph pooling layer to extract subgraph-based representations by an end-to-end manner. Furthermore, inspired by the confounder balancing techniques from causal inference, based on the learned high-level representations, we propose a causal variable distinguishing regularizer to correct the biased training distribution by learning a set of sample weights. Hence, GNNs would concentrate more on the true connection between discriminative substructures and labels. Extensive experiments are conducted on both synthetic datasets with various distribution shift degrees and eight real-world OOD graph datasets. The results well verify that the proposed model StableGNN not only outperforms the state-of-the-arts but also provides a flexible framework to enhance existing GNNs. In addition, the interpretability experiments validate that StableGNN could leverage causal structures for predictions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48635390",
                        "name": "Shaohua Fan"
                    },
                    {
                        "authorId": "2118449003",
                        "name": "Xiao Wang"
                    },
                    {
                        "authorId": "2151458697",
                        "name": "Chuan Shi"
                    },
                    {
                        "authorId": "143738684",
                        "name": "Peng Cui"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "contexts": [
                "Similarly, PGExplainer (Luo et al., 2020) uses a generative probabilistic model to learn succinct underlying structures from the input graph data as explanations.",
                "Similarly, PGExplainer (Luo et al., 2020) uses a generative probabilistic",
                ", 2019), PGExplainer (Luo et al., 2020) and PGMExplainer (Vu and Thai, 2020).",
                "As GNNExplainer and PGExplainer provide continuous masks, we report, for fair comparisons, the performance with both continuous and discrete masks built with the k best edges.",
                "According to the literature, the best competitors are GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020) and PGM-Explainer (Vu and Thai, 2020).",
                "GNNExplainer, PGExplainer, and PGM-Explainer are the methods that report the best performance on many datasets.",
                "INSIDE-GNN obtain similar scores or outperforms the other competitors (i.e., PGExplainer, PGM-Explainer, Grad) at equal sparsity on most of the datasets.",
                "It remains slightly slower than PGExplainer (6ms to 20ms).",
                "Methods based on perturbation (Luo et al., 2020; Ying et al., 2019) aim to learn a mask seen as an explanation of the model decision for a graph instance."
            ],
            "citingPaper": {
                "paperId": "e3a8de889a3d486e515266d6cd249561f007f4f2",
                "externalIds": {
                    "CorpusId": 247410390
                },
                "corpusId": 247410390,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e3a8de889a3d486e515266d6cd249561f007f4f2",
                "title": "On GNN explanability with activation patterns",
                "abstract": "GNNs are powerful models based on node representation learning that perform particularly well in many machine learning problems related to graphs. The major obstacle to the deployment of GNNs is mostly a problem of societal acceptability and trustworthiness, properties which require making explicit the internal functioning of such models. Here, we propose to mine activation patterns in the hidden layers to understand how the GNNs perceive the world. The problem is not to discover activation patterns that are individually highly discriminating for an output of the model. Instead, the challenge is to provide a small set of patterns that cover all input graphs. To this end, we introduce the subjective activation pattern domain. We de\ufb01ne an e\ufb00ective and principled algorithm to enumerate patterns of activations in each hidden layer. The proposed approach for quantifying the interest of these patterns is rooted in information theory and is able to account for background knowledge on the input graph data. The activation patterns can then be redescribed thanks to pattern languages involving interpretable features. We show that the activation patterns provide insights on the characteristics used by the GNN to classify the graphs. Especially, this allows to identify the hidden features built by the GNN through its di\ufb00erent layers. Also, these patterns can subsequently be used for explaining GNN decisions. Experiments on both synthetic and real-life datasets show highly compet-itive performance, with up to 200% improvement in \ufb01delity on explaining graph classi\ufb01cation over the SOTA methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1452681405",
                        "name": "Luca Veyrin-Forrer"
                    },
                    {
                        "authorId": "2158653859",
                        "name": "Ataollah Kamal"
                    },
                    {
                        "authorId": "2244635954",
                        "name": "Stefan Du\ufb00ner"
                    },
                    {
                        "authorId": "2331463",
                        "name": "M. Plantevit"
                    },
                    {
                        "authorId": "1763302",
                        "name": "C. Robardet"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                ", 2019), PGExplainer (Luo et al., 2020), and GraphMask (Schlichtkrull",
                "\u2026our FlowX with eight baselines, including GradCAM (Pope et al., 2019), DeepLIFT (Shrikumar et al., 2017), GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), PGMExplainer (Vu & Thai, 2020), SubgraphX (Yuan et al., 2021), GNN-GI (Schnake et al., 2020), GNN-LRP (Schnake et al., 2020).",
                ", 2019), PGExplainer (Luo et al., 2020), and SubgraphX (Yuan et al.",
                ", 2019), PGExplainer (Luo et al., 2020), PGMExplainer (Vu & Thai, 2020), SubgraphX (Yuan et al.",
                "\u2026methods have been proposed to explain the predictions of GNNs, such as GraphLime (Huang et al., 2020), GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), PGMExplainer (Vu & Thai, 2020), SubgraphX (Yuan et al., 2021), XGNN (Yuan et al., 2020b), and GraphSVX (Duval & Malliaros, 2021).",
                "Recently, several techniques have been proposed to explain GNNs, such as XGNN (Yuan et al., 2020b), GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), and SubgraphX (Yuan et al., 2021), etc.",
                "Second, several existing methods, such as GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), and GraphMask (Schlichtkrull\net al., 2021), explain GNNs by studying the importance of different graph edges."
            ],
            "citingPaper": {
                "paperId": "421b495c2379182b8874ce91af5ab1121d356834",
                "externalIds": {
                    "CorpusId": 248384864
                },
                "corpusId": 248384864,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/421b495c2379182b8874ce91af5ab1121d356834",
                "title": "F LOW X: T OWARDS E XPLAINABLE G RAPH N EURAL N ETWORKS VIA M ESSAGE F LOWS",
                "abstract": "We investigate the explainability of graph neural networks (GNNs) as a step towards elucidating their working mechanisms. While most current methods focus on explaining graph nodes, edges, or features, we argue that, as the inherent functional mechanism of GNNs, message \ufb02ows are more natural for performing explainability. To this end, we propose a novel method here, known as FlowX, to explain GNNs by identifying important message \ufb02ows. To quantify the importance of \ufb02ows, we propose to follow the philosophy of Shapley values from cooperative game theory. To tackle the complexity of computing all coalitions\u2019 marginal contributions, we propose an approximation scheme to compute Shapley-like values as initial assessments of further redistribution training. We then propose a learning algorithm to train \ufb02ow scores and improve explainability. Experimental studies on both synthetic and real-world datasets demonstrate that our proposed FlowX leads to improved explainability of GNNs.",
                "year": 2021,
                "authors": []
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Following [17], we use the cross-entropy function to replace the conditional entropy function minS H(Y |S) with N given instances.",
                ", GNNExplainer [34] and PGExplainer [17], attempt to solve the optimization problem with continuous relaxation.",
                "For fairness, we follow the experimental setup in [17, 12], i.",
                "Then we compare RG-Explainer with two state-of-the-art baselines GNNExplainer [34] and PGExplainer [17] in both qualitative and quantitative evaluations.",
                "Following the problem setting, PGExplainer [17] leverages the representations generated by the trained GNN and adopts a deep neural network to learn the crucial nodes/edges.",
                "We use the trained GNN model in [12], whose architecture is given in [17, 34].",
                "To enhance the interpretability of GNNs, a line of works [34, 17, 30, 37, 31] focused on developing GNN explainers."
            ],
            "citingPaper": {
                "paperId": "9d557d85c206ddbf1b9fb1ad0c848a64e0973360",
                "externalIds": {
                    "DBLP": "conf/nips/ShanSZLL21",
                    "CorpusId": 245117010
                },
                "corpusId": 245117010,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9d557d85c206ddbf1b9fb1ad0c848a64e0973360",
                "title": "Reinforcement Learning Enhanced Explainer for Graph Neural Networks",
                "abstract": "Graph neural networks (GNNs) have recently emerged as revolutionary technolo-gies for machine learning tasks on graphs. In GNNs, the graph structure is generally incorporated with node representation via the message passing scheme, making the explanation much more challenging. Given a trained GNN model, a GNN explainer aims to identify a most in\ufb02uential subgraph to interpret the prediction of an instance (e.g., a node or a graph), which is essentially a combinatorial optimization problem over graph. The existing works solve this problem by continuous relaxation or search-based heuristics. But they suffer from key issues such as violation of message passing and hand-crafted heuristics, leading to inferior interpretability. To address these issues, we propose a RL-enhanced GNN explainer, RG-Explainer , which consists of three main components: starting point selection, iterative graph generation and stopping criteria learning. RG-Explainer could construct a connected explanatory subgraph by sequentially adding nodes from the boundary of the current generated graph, which is consistent with the message passing scheme. Further, we design an effective seed locator to select the starting point, and learn stopping criteria to generate superior explanations. Extensive experiments on both synthetic and real datasets show that RG-Explainer outperforms state-of-the-art GNN explainers. Moreover, RG-Explainer can be applied in the inductive setting, demonstrating its better generalization ability.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145663545",
                        "name": "Caihua Shan"
                    },
                    {
                        "authorId": "2115383310",
                        "name": "Yifei Shen"
                    },
                    {
                        "authorId": "2118390430",
                        "name": "Yao Zhang"
                    },
                    {
                        "authorId": "47875796",
                        "name": "Xiang Li"
                    },
                    {
                        "authorId": "2119081394",
                        "name": "Dongsheng Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "For example, GNNExplainer [Ying et al., 2019] and PGExplainer [Luo et al., 2020] identify compact subgraph structures that are crucial in prediction as the explanation for the GNN models.",
                ", 2019] and PGExplainer [Luo et al., 2020] identify compact subgraph structures that are crucial in prediction as the explanation for the GNN models."
            ],
            "citingPaper": {
                "paperId": "5679d86ad22221693891d7560c2d5b053329ecd4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-03036",
                    "CorpusId": 232110397
                },
                "corpusId": 232110397,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5679d86ad22221693891d7560c2d5b053329ecd4",
                "title": "Deep Graph Structure Learning for Robust Representations: A Survey",
                "abstract": "Graph Neural Networks (GNNs) are widely used for analyzing graph-structured data. Most GNN methods are highly sensitive to the quality of graph structures and usually require a perfect graph structure for learning informative embeddings. However, the pervasiveness of noise in graphs necessitates learning robust representations for real-world problems. To improve the robustness of GNN models, many studies have been proposed around the central concept of Graph Structure Learning (GSL), which aims to jointly learn an optimized graph structure and corresponding representations. Towards this end, in the presented survey, we broadly review recent progress of GSL methods for learning robust representations. Speci\ufb01cally, we \ufb01rst formulate a general paradigm of GSL, and then review state-of-the-art methods classi\ufb01ed by how they model graph structures, followed by applications that incorporate the idea of GSL in other graph tasks. Finally, we point out some issues in current studies and discuss future directions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2653121",
                        "name": "Yanqiao Zhu"
                    },
                    {
                        "authorId": "2054230",
                        "name": "Weizhi Xu"
                    },
                    {
                        "authorId": "2108045911",
                        "name": "Jinghao Zhang"
                    },
                    {
                        "authorId": "2146553789",
                        "name": "Qiang Liu"
                    },
                    {
                        "authorId": "50425438",
                        "name": "Shu Wu"
                    },
                    {
                        "authorId": "123865558",
                        "name": "Liang Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "fbf41d54f909f47aa497c9790c0aa63f4a630174",
                "externalIds": {
                    "DBLP": "journals/access/AsifSCRASBDAMIT21",
                    "DOI": "10.1109/ACCESS.2021.3071274",
                    "CorpusId": 233376412
                },
                "corpusId": 233376412,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fbf41d54f909f47aa497c9790c0aa63f4a630174",
                "title": "Graph Neural Network: A Comprehensive Review on Non-Euclidean Space",
                "abstract": "This review provides a comprehensive overview of the state-of-the-art methods of graph-based networks from a deep learning perspective. Graph networks provide a generalized form to exploit non-euclidean space data. A graph can be visualized as an aggregation of nodes and edges without having any order. Data-driven architecture tends to follow a fixed neural network trying to find the pattern in feature space. These strategies have successfully been applied to many applications for euclidean space data. Since graph data in a non-euclidean space does not follow any kind of order, these solutions can be applied to exploit the node relationships. Graph Neural Networks (GNNs) solve this problem by exploiting the relationships among graph data. Recent developments in computational hardware and optimization allow graph networks possible to learn the complex graph relationships. Graph networks are therefore being actively used to solve many problems including protein interface, classification, and learning representations of fingerprints. To encapsulate the importance of graph models, in this paper, we formulate a systematic categorization of GNN models according to their applications from theory to real-life problems and provide a direction of the future scope for the applications of graph models as well as highlight the limitations of existing graph networks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2082350148",
                        "name": "Nurul A. Asif"
                    },
                    {
                        "authorId": "1491231077",
                        "name": "Yeahia Sarker"
                    },
                    {
                        "authorId": "3185091",
                        "name": "R. Chakrabortty"
                    },
                    {
                        "authorId": "2072629252",
                        "name": "Michael J. Ryan"
                    },
                    {
                        "authorId": "117088145",
                        "name": "M. H. Ahamed"
                    },
                    {
                        "authorId": "151355666",
                        "name": "D. K. Saha"
                    },
                    {
                        "authorId": "9785172",
                        "name": "F. Badal"
                    },
                    {
                        "authorId": "152220602",
                        "name": "S. Das"
                    },
                    {
                        "authorId": "2110903762",
                        "name": "M. F. Ali"
                    },
                    {
                        "authorId": "2004373428",
                        "name": "S. I. Moyeen"
                    },
                    {
                        "authorId": "98155858",
                        "name": "M. R. Islam"
                    },
                    {
                        "authorId": "9414043",
                        "name": "Z. Tasneem"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "7b799eeb9ee5f8b22c7e0e90ff75ac28f2801748",
                "externalIds": {
                    "CorpusId": 237371589
                },
                "corpusId": 237371589,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7b799eeb9ee5f8b22c7e0e90ff75ac28f2801748",
                "title": "[Re] Parameterized Explainer for Graph Neural Network",
                "abstract": "In thisworkweperforma replication study of the paperParameterizedExplainer forGraph Neural Network. The replication experiment focuses on three main claims: (1) Is it possible to reimplement the proposed method in a different framework? (2) Do the main claims with respect to the GNNExplainer hold? (3) Is the used evaluationmethod a valid method for explaining the classification decision by Graph Neural Networks?",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "52221725",
                        "name": "Lars Holdijk"
                    },
                    {
                        "authorId": "29920571",
                        "name": "M. Boon"
                    },
                    {
                        "authorId": "2127600706",
                        "name": "Stijn Henckens"
                    },
                    {
                        "authorId": "2174570330",
                        "name": "Lysander de Jong"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "c6b82d60bc99d7e4a2d38cb78f7ed3bc679bcbeb",
                "externalIds": {
                    "CorpusId": 253082672
                },
                "corpusId": 253082672,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c6b82d60bc99d7e4a2d38cb78f7ed3bc679bcbeb",
                "title": "Intelligent \ufb01 nancial fraud detection practices in post-pandemic era",
                "abstract": "Future directions of \ufb01 nancial fraud detection from task, data, and model-oriented perspectives The great losses caused by \ufb01 nancial fraud have attracted continuous attention from academia, industry, and regulatory agencies. More con-cerning, the ongoing coronavirus pandemic (COVID-19) unexpectedly shocks the global \ufb01 nancial system and accelerates the use of digital \ufb01 nancial services, which brings new challenges in effective \ufb01 nancial fraud detection. This paper provides a comprehensive overview of intelligent \ufb01 fraud risk caused by the pandemic and review the development of data types used in fraud detection practices from quantitative tabular data to various unstructured data. The evolution of methods in \ufb01 nancial fraud detection is summarized, and the emerging Graph Neural Network methods in the post-pandemic era are discussed in particular. Finally, some of the key challenges and potential directions are proposed to provide inspiring information on intelligent \ufb01 nancial fraud detection in the future.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1702020",
                        "name": "Xiaoqian Zhu"
                    },
                    {
                        "authorId": "2125319109",
                        "name": "Xiang Ao"
                    },
                    {
                        "authorId": "2106704739",
                        "name": "Zidi Qin"
                    },
                    {
                        "authorId": "1399879324",
                        "name": "Ya-Hsin Chang"
                    },
                    {
                        "authorId": "40457423",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "2158548",
                        "name": "Qing He"
                    },
                    {
                        "authorId": "2144535241",
                        "name": "Jianping Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "It would be beneficial - and is considered future work - if GraphLIME was also trying to find important graph substructures instead of just features, if it was compared with other methods like PGExplainer [52], PGMExplainer [83], GNN-LRP [72], and if it were extended to multiple instance explanations."
            ],
            "citingPaper": {
                "paperId": "f4f6bf7c3d708102359e86730df15e514e201854",
                "externalIds": {
                    "DBLP": "conf/icml/HolzingerSMBS20",
                    "DOI": "10.1007/978-3-031-04083-2_2",
                    "CorpusId": 248301956
                },
                "corpusId": 248301956,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f4f6bf7c3d708102359e86730df15e514e201854",
                "title": "Explainable AI Methods - A Brief Overview",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "47596587",
                        "name": "Andreas Holzinger"
                    },
                    {
                        "authorId": "1947785",
                        "name": "Anna Saranti"
                    },
                    {
                        "authorId": "50621691",
                        "name": "Christoph Molnar"
                    },
                    {
                        "authorId": "144356944",
                        "name": "P. Biecek"
                    },
                    {
                        "authorId": "1699054",
                        "name": "W. Samek"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "PGExplainer can explain multiple instances collectively and also works in an inductive setting.",
                "We shall discuss and compare 19 recent GNN interpretability methods from the aforementioned categories: GNNExplainer [36], PGExplainer [37], GraphMask [38], SubgraphX [39], PGMExplainer [40], RelEx [41], GraphLime [42], RCExplainer [43], DnX [44], GCFExplainer [45], CF2 [46], SA [26], GuidedBP [26], CAM [21], Grad-CAM [47], LRP [48], GNNLRP [49], ExcitationBP [21], and XGNN [50].",
                "We shall discuss and compare 19 recent GNN interpretability methods from the aforementioned categories: GNNExplainer [36], PGExplainer [37], GraphMask [38], SubgraphX [39], PGMExplainer [40], RelEx [41], GraphLime [42], RCExplainer [43], DnX [44], GCFExplainer [45], CF(2) [46], SA [26], GuidedBP [26], CAM [21], Grad-CAM [47], LRP [48], GNNLRP [49], ExcitationBP [21], and XGNN [50].",
                "PGExplainer performs a similar process as GNNExplainer that maximizes the mutual information between the predictions of the input graph and that of the evidence subgraph; however, it only focuses on the graph structure by using a deep neural network to parameterize the generation of the evidence subgraph."
            ],
            "citingPaper": {
                "paperId": "a4fadb1331a3faee62fdd90c226417a593e35b51",
                "externalIds": {
                    "CorpusId": 260890101
                },
                "corpusId": 260890101,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a4fadb1331a3faee62fdd90c226417a593e35b51",
                "title": "Interpretability Methods for Graph Neural Networks",
                "abstract": "\u2014The emerging graph neural network models (GNNs) have demonstrated great potential and success for downstream graph machine learning tasks, such as graph and node classification, link prediction, entity resolution, and question answering. However, neural networks are \u201cblack-box\u201d \u2013 it is difficult to understand which aspects of the input data and the model guide the decisions of the network. Recently, several interpretability methods for GNNs have been developed, aiming at improving the model\u2019s transparency and fairness, thus making them trustworthy in decision-critical applications, leading to democratization of deep learning approaches and easing their adoptions. The tutorial is designed to offer an overview of the state-of-the-art interpretability techniques for graph neural networks, including their taxonomy, evaluation metrics, benchmarking study, and ground truth. In addition, the tutorial discusses open problems and important research directions.",
                "year": null,
                "authors": [
                    {
                        "authorId": "2108514592",
                        "name": "Arijit Khan"
                    },
                    {
                        "authorId": "2231550366",
                        "name": "Ehsan B. Mobaraki"
                    }
                ]
            }
        }
    ]
}