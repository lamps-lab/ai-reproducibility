{
    "offset": 0,
    "data": [
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1fbe27efc8249ee6640a5825b3d7da148cc8cf1d",
                "externalIds": {
                    "PubMedCentral": "10552306",
                    "DOI": "10.1186/s13059-023-03064-y",
                    "CorpusId": 263625397,
                    "PubMed": "37798735"
                },
                "corpusId": 263625397,
                "publicationVenue": {
                    "id": "41dbb273-8d18-4a45-8154-852882a37b34",
                    "name": "Genome Biology",
                    "type": "journal",
                    "issn": "1474-7596",
                    "alternate_issns": [
                        "1474-760X",
                        "1465-6906"
                    ],
                    "url": "http://genomebiology.com/"
                },
                "url": "https://www.semanticscholar.org/paper/1fbe27efc8249ee6640a5825b3d7da148cc8cf1d",
                "title": "Large sample size and nonlinear sparse models outline epistatic effects in inflammatory bowel disease",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1583752660",
                        "name": "N. Verplaetse"
                    },
                    {
                        "authorId": "2046866685",
                        "name": "Antoine Passemiers"
                    },
                    {
                        "authorId": "40528222",
                        "name": "Adam Arany"
                    },
                    {
                        "authorId": "2253744638",
                        "name": "Yves Moreau"
                    },
                    {
                        "authorId": "2711044",
                        "name": "D. Raimondi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Despite numerous existing algorithms for pruning (Singh & Alistarh, 2020; Zhu & Gupta, 2017; Gale et al., 2019; Jaiswal et al., 2022; Lin et al., 2020; Liu et al., 2021a; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020) and quantization (Dong et al."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4e13ecf80443a4135d516b7ba77eca82b5c6d347",
                "externalIds": {
                    "ArXiv": "2310.01382",
                    "CorpusId": 263605754
                },
                "corpusId": 263605754,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4e13ecf80443a4135d516b7ba77eca82b5c6d347",
                "title": "Compressing LLMs: The Truth is Rarely Pure and Never Simple",
                "abstract": "Despite their remarkable achievements, modern Large Language Models (LLMs) encounter exorbitant computational and memory footprints. Recently, several works have shown significant success in training-free and data-free compression (pruning and quantization) of LLMs achieving 50-60% sparsity and reducing the bit-width down to 3 or 4 bits per weight, with negligible perplexity degradation over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back, and re-evaluates the effectiveness of existing SoTA compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a collection of carefully-curated tasks to re-define the evaluation protocol for compressed LLMs, which have significant alignment with their dense counterparts, and perplexity fail to capture subtle change in their true capabilities. LLM-KICK unveils many favorable merits and unfortunate plights of current SoTA compression methods: all pruning methods suffer significant performance degradation, sometimes at trivial sparsity ratios (e.g., 25-30%), and fail for N:M sparsity on knowledge-intensive tasks; current quantization methods are more successful than pruning; yet, pruned LLMs even at $\\geq 50$% sparsity are robust in-context retrieval and summarization systems; among others. LLM-KICK is designed to holistically access compressed LLMs' ability for language understanding, reasoning, generation, in-context retrieval, in-context summarization, etc. We hope our study can foster the development of better LLM compression methods. All our related codes are planed to be open-sourced.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2253397454",
                        "name": "Ajay Jaiswal"
                    },
                    {
                        "authorId": "2253397669",
                        "name": "Zhe Gan"
                    },
                    {
                        "authorId": "2239065938",
                        "name": "Xianzhi Du"
                    },
                    {
                        "authorId": "2256276486",
                        "name": "Bowen Zhang"
                    },
                    {
                        "authorId": "2254949434",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "2249897805",
                        "name": "Yinfei Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Sparse training (Mocanu et al., 2018; Jaiswal et al., 2022; Mostafa & Wang, 2019; Evci et al., 2020; Liu et al., 2021; Yuan et al., 2021; Yin et al., 2023; Kundu et al., 2021), on the other hand, starts with a (random) sparse network and updates network connectivity during training to search for good sparse neural network without any pre-training and dense training steps."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6bfd1c8cc501a78fdb88c00a6e25da7a78de925a",
                "externalIds": {
                    "ArXiv": "2310.02277",
                    "CorpusId": 263620664
                },
                "corpusId": 263620664,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6bfd1c8cc501a78fdb88c00a6e25da7a78de925a",
                "title": "Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity",
                "abstract": "The traditional notion of\"Junk DNA\"has long been linked to non-coding segments within the human genome, constituting roughly 98% of its composition. However, recent research has unveiled the critical roles some of these seemingly non-functional DNA sequences play in cellular processes. Intriguingly, the weights within deep neural networks exhibit a remarkable similarity to the redundancy observed in human genes. It was believed that weights in gigantic models contained excessive redundancy, and could be removed without compromising performance. This paper challenges this conventional wisdom by presenting a compelling counter-argument. We employ sparsity as a tool to isolate and quantify the nuanced significance of low-magnitude weights in pre-trained large language models (LLMs). Our study demonstrates a strong correlation between these weight magnitudes and the knowledge they encapsulate, from a downstream task-centric angle. we raise the\"Junk DNA Hypothesis\"backed by our in-depth investigation: while small-magnitude weights may appear\"useless\"for simple tasks and suitable for pruning, they actually encode crucial knowledge necessary for solving more difficult downstream tasks. Removing these seemingly insignificant weights can lead to irreversible knowledge forgetting and performance damage in difficult tasks. These findings offer fresh insights into how LLMs encode knowledge in a task-sensitive manner, pave future research direction in model pruning, and open avenues for task-aware conditional computation during inference.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2254142682",
                        "name": "Lu Yin"
                    },
                    {
                        "authorId": "2255081092",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2253397454",
                        "name": "Ajay Jaiswal"
                    },
                    {
                        "authorId": "2253458053",
                        "name": "Souvik Kundu"
                    },
                    {
                        "authorId": "2254949434",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, unstructured sparsity [16]\u2013[18] involves pruning elements in any position without constraints, leading to a high sparse ratio (over 80% sparse ratio in [9]) and a significant reduction in the number of FLOPs for training.",
                "A majority of prior arts mainly focus on ReLU-based [42], [43], structured [6], [7] or unstructured sparsity [8], [9] for sparse training.",
                "Structured sparsity has limitations in reducing computational complexity (less than 40% sparse ratio in [6]), while unstructured sparsity, despite saving a great number of operations (over 80% sparse ratio in [9]), is difficult to accelerate on hardware [20]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8b615e2df8e615f1cc072422979ce2c44d011c4b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-13015",
                    "ArXiv": "2309.13015",
                    "DOI": "10.1109/TCAD.2023.3317789",
                    "CorpusId": 262164369
                },
                "corpusId": 262164369,
                "publicationVenue": {
                    "id": "e86c30b0-c1dd-4f0e-be5e-22af711f7d5f",
                    "name": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Comput Des Integr Circuit Syst"
                    ],
                    "issn": "0278-0070",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=43",
                    "alternate_urls": [
                        "http://ieee-ceda.org/publications/tcad",
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=43"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8b615e2df8e615f1cc072422979ce2c44d011c4b",
                "title": "Efficient N: M Sparse DNN Training Using Algorithm, Architecture, and Dataflow Co-Design",
                "abstract": "Sparse training is one of the promising techniques to reduce the computational cost of DNNs while retaining high accuracy. In particular, N:M fine-grained structured sparsity, where only N out of consecutive M elements can be nonzero, has attracted attention due to its hardware-friendly pattern and capability of achieving a high sparse ratio. However, the potential to accelerate N:M sparse DNN training has not been fully exploited, and there is a lack of efficient hardware supporting N:M sparse training. To tackle these challenges, this paper presents a computation-efficient training scheme for N:M sparse DNNs using algorithm, architecture, and dataflow co-design. At the algorithm level, a bidirectional weight pruning method, dubbed BDWP, is proposed to leverage the N:M sparsity of weights during both forward and backward passes of DNN training, which can significantly reduce the computational cost while maintaining model accuracy. At the architecture level, a sparse accelerator for DNN training, namely SAT, is developed to neatly support both the regular dense operations and the computation-efficient N:M sparse operations. At the dataflow level, multiple optimization methods ranging from interleave mapping, pre-generation of N:M sparse weights, and offline scheduling, are proposed to boost the computational efficiency of SAT. Finally, the effectiveness of our training scheme is evaluated on a Xilinx VCU1525 FPGA card using various DNN models and datasets. Experimental results show the SAT accelerator with the BDWP sparse training method under 2:8 sparse ratio achieves an average speedup of 1.75x over that with the dense training, accompanied by a negligible accuracy loss of 0.56% on average. Furthermore, our proposed training scheme significantly improves the training throughput by 2.97~25.22x and the energy efficiency by 1.36~3.58x over prior FPGA-based accelerators.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2048996183",
                        "name": "Chao Fang"
                    },
                    {
                        "authorId": "2218674903",
                        "name": "Wei Sun"
                    },
                    {
                        "authorId": "2244354043",
                        "name": "Aojun Zhou"
                    },
                    {
                        "authorId": "2239628722",
                        "name": "Zhongfeng Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9b98ca94b7733feee1cff5c57596611ad35fa7aa",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-08520",
                    "ArXiv": "2309.08520",
                    "DOI": "10.48550/arXiv.2309.08520",
                    "CorpusId": 262013578
                },
                "corpusId": 262013578,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9b98ca94b7733feee1cff5c57596611ad35fa7aa",
                "title": "Scaling Laws for Sparsely-Connected Foundation Models",
                "abstract": "We explore the impact of parameter sparsity on the scaling behavior of Transformers trained on massive datasets (i.e.,\"foundation models\"), in both vision and language domains. In this setting, we identify the first scaling law describing the relationship between weight sparsity, number of non-zero parameters, and amount of training data, which we validate empirically across model and data scales; on ViT/JFT-4B and T5/C4. These results allow us to characterize the\"optimal sparsity\", the sparsity level which yields the best performance for a given effective model size and training budget. For a fixed number of non-zero parameters, we identify that the optimal sparsity increases with the amount of data used for training. We also extend our study to different sparsity structures (such as the hardware-friendly n:m pattern) and strategies (such as starting from a pretrained dense model). Our findings shed light on the power and limitations of weight sparsity across various parameter and computational settings, offering both theoretical understanding and practical implications for leveraging sparsity towards computational efficiency improvements.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1502248377",
                        "name": "Elias Frantar"
                    },
                    {
                        "authorId": "145814174",
                        "name": "C. Riquelme"
                    },
                    {
                        "authorId": "2815290",
                        "name": "N. Houlsby"
                    },
                    {
                        "authorId": "3311387",
                        "name": "Dan Alistarh"
                    },
                    {
                        "authorId": "3399348",
                        "name": "Utku Evci"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent pruning policies improve efficiency by starting with a sparse network (Evci et al. 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4655a442c50184370a7efa5b813cc6ea2e2a42df",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-08171",
                    "ArXiv": "2309.08171",
                    "DOI": "10.48550/arXiv.2309.08171",
                    "CorpusId": 262013136
                },
                "corpusId": 262013136,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4655a442c50184370a7efa5b813cc6ea2e2a42df",
                "title": "Unveiling Invariances via Neural Network Pruning",
                "abstract": "Invariance describes transformations that do not alter data's underlying semantics. Neural networks that preserve natural invariance capture good inductive biases and achieve superior performance. Hence, modern networks are handcrafted to handle well-known invariances (ex. translations). We propose a framework to learn novel network architectures that capture data-dependent invariances via pruning. Our learned architectures consistently outperform dense neural networks on both vision and tabular datasets in both efficiency and effectiveness. We demonstrate our framework on multiple deep learning models across 3 vision and 40 tabular datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46697020",
                        "name": "Derek Xu"
                    },
                    {
                        "authorId": "2244171114",
                        "name": "Yizhou Sun"
                    },
                    {
                        "authorId": "145200778",
                        "name": "Wei Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "If considering s0 > 0, the layerwise sparsity of the initial mask follows the ERK distribution introduced in [31].",
                "PruneFL starts with a pre-selected node to train a global shared mask function, while FedDIP generates the mask function with weights following the Erd\u0151s-Ren\u00e9yi-Kernel (ERK) distribution [31], as we will discuss in the later sections."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "3c1195dda05f4f3ad9dc36817ed414c97e6d5381",
                "externalIds": {
                    "ArXiv": "2309.06805",
                    "DBLP": "journals/corr/abs-2309-06805",
                    "DOI": "10.48550/arXiv.2309.06805",
                    "CorpusId": 261706135
                },
                "corpusId": 261706135,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3c1195dda05f4f3ad9dc36817ed414c97e6d5381",
                "title": "FedDIP: Federated Learning with Extreme Dynamic Pruning and Incremental Regularization",
                "abstract": "Federated Learning (FL) has been successfully adopted for distributed training and inference of large-scale Deep Neural Networks (DNNs). However, DNNs are characterized by an extremely large number of parameters, thus, yielding significant challenges in exchanging these parameters among distributed nodes and managing the memory. Although recent DNN compression methods (e.g., sparsification, pruning) tackle such challenges, they do not holistically consider an adaptively controlled reduction of parameter exchange while maintaining high accuracy levels. We, therefore, contribute with a novel FL framework (coined FedDIP), which combines (i) dynamic model pruning with error feedback to eliminate redundant information exchange, which contributes to significant performance improvement, with (ii) incremental regularization that can achieve \\textit{extreme} sparsity of models. We provide convergence analysis of FedDIP and report on a comprehensive performance and comparative assessment against state-of-the-art methods using benchmark data sets and DNN models. Our results showcase that FedDIP not only controls the model sparsity but efficiently achieves similar or better performance compared to other model pruning methods adopting incremental regularization during distributed model training. The code is available at: https://github.com/EricLoong/feddip.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2178913406",
                        "name": "Qianyu Long"
                    },
                    {
                        "authorId": "2239201726",
                        "name": "Christos Anagnostopoulos"
                    },
                    {
                        "authorId": "2131091",
                        "name": "S. P. Parambath"
                    },
                    {
                        "authorId": "2239202036",
                        "name": "Daning Bi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7109eeefbfa7d81541a05419184a46c7ba3facb8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-02022",
                    "ArXiv": "2309.02022",
                    "DOI": "10.48550/arXiv.2309.02022",
                    "CorpusId": 261530798
                },
                "corpusId": 261530798,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7109eeefbfa7d81541a05419184a46c7ba3facb8",
                "title": "Dynamic Early Exiting Predictive Coding Neural Networks",
                "abstract": "Internet of Things (IoT) sensors are nowadays heavily utilized in various real-world applications ranging from wearables to smart buildings passing by agrotechnology and health monitoring. With the huge amounts of data generated by these tiny devices, Deep Learning (DL) models have been extensively used to enhance them with intelligent processing. However, with the urge for smaller and more accurate devices, DL models became too heavy to deploy. It is thus necessary to incorporate the hardware's limited resources in the design process. Therefore, inspired by the human brain known for its efficiency and low power consumption, we propose a shallow bidirectional network based on predictive coding theory and dynamic early exiting for halting further computations when a performance threshold is surpassed. We achieve comparable accuracy to VGG-16 in image classification on CIFAR-10 with fewer parameters and less computational complexity.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2236671445",
                        "name": "Alaa Zniber"
                    },
                    {
                        "authorId": "51898188",
                        "name": "Ouassim Karrakchou"
                    },
                    {
                        "authorId": "2125342650",
                        "name": "Mounir Ghogho"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "16cfe5b8b7a89fdbf4338dd7cdee46292df5a125",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-00864",
                    "ArXiv": "2309.00864",
                    "DOI": "10.48550/arXiv.2309.00864",
                    "CorpusId": 261530755
                },
                "corpusId": 261530755,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/16cfe5b8b7a89fdbf4338dd7cdee46292df5a125",
                "title": "Equitable-FL: Federated Learning with Sparsity for Resource-Constrained Environment",
                "abstract": "In Federated Learning, model training is performed across multiple computing devices, where only parameters are shared with a common central server without exchanging their data instances. This strategy assumes abundance of resources on individual clients and utilizes these resources to build a richer model as user's models. However, when the assumption of the abundance of resources is violated, learning may not be possible as some nodes may not be able to participate in the process. In this paper, we propose a sparse form of federated learning that performs well in a Resource Constrained Environment. Our goal is to make learning possible, regardless of a node's space, computing, or bandwidth scarcity. The method is based on the observation that model size viz a viz available resources defines resource scarcity, which entails that reduction of the number of parameters without affecting accuracy is key to model training in a resource-constrained environment. In this work, the Lottery Ticket Hypothesis approach is utilized to progressively sparsify models to encourage nodes with resource scarcity to participate in collaborative training. We validate Equitable-FL on the $MNIST$, $F-MNIST$, and $CIFAR-10$ benchmark datasets, as well as the $Brain-MRI$ data and the $PlantVillage$ datasets. Further, we examine the effect of sparsity on performance, model size compaction, and speed-up for training. Results obtained from experiments performed for training convolutional neural networks validate the efficacy of Equitable-FL in heterogeneous resource-constrained learning environment.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2000844798",
                        "name": "Indrajeet Kumar Sinha"
                    },
                    {
                        "authorId": "2237787986",
                        "name": "Shekhar Verma"
                    },
                    {
                        "authorId": "2237955551",
                        "name": "Krishna Pratap Singh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Initial studies by [26] showed how neural networks produce accurate yet overconfident predictions, and it has been seen that this problem is exacerbated even further [13] by certain sparse training regimes like RigL [27]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "99b0a1fb96a3dd9740d00b309cbc39d3619e3336",
                "externalIds": {
                    "ArXiv": "2308.14969",
                    "DBLP": "journals/corr/abs-2308-14969",
                    "DOI": "10.48550/arXiv.2308.14969",
                    "CorpusId": 261276429
                },
                "corpusId": 261276429,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/99b0a1fb96a3dd9740d00b309cbc39d3619e3336",
                "title": "Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets",
                "abstract": "In the era of foundation models with huge pre-training budgets, the downstream tasks have been shifted to the narrative of efficient and fast adaptation. For classification-based tasks in the domain of computer vision, the two most efficient approaches have been linear probing (LP) and visual prompting/reprogramming (VP); the former aims to learn a classifier in the form of a linear head on the features extracted by the pre-trained model, while the latter maps the input data to the domain of the source data on which the model was originally pre-trained on. Although extensive studies have demonstrated the differences between LP and VP in terms of downstream performance, we explore the capabilities of the two aforementioned methods via the sparsity axis: (a) Data sparsity: the impact of few-shot adaptation and (b) Model sparsity: the impact of lottery tickets (LT). We demonstrate that LT are not universal reprogrammers, i.e., for certain target datasets, reprogramming an LT yields significantly lower performance than the reprogrammed dense model although their corresponding upstream performance is similar. Further, we demonstrate that the calibration of dense models is always superior to that of their lottery ticket counterparts under both LP and VP regimes. Our empirical study opens a new avenue of research into VP for sparse models and encourages further understanding of the performance beyond the accuracy achieved by VP under constraints of sparsity. Code and logs can be accessed at \\url{https://github.com/landskape-ai/Reprogram_LT}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145397116",
                        "name": "Diganta Misra"
                    },
                    {
                        "authorId": "2685624",
                        "name": "Agam Goyal"
                    },
                    {
                        "authorId": "2180166734",
                        "name": "Bharat Runwal"
                    },
                    {
                        "authorId": "2158177808",
                        "name": "Pin-Yu Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Gradient growth [31]: It involves adding new connections based on the gradients values of the connections during the training process.",
                "Similar to the previous studies [25, 31, 32, 34], we use Cosine Annealing fdecay(t, \u03b1, Tend) = \u03b12 (1 + cos( t\u03c0 Tend )) for topology update.",
                "Erd\u0151s-R\u00e9nyi Kernel (ERK) [31]: Inspired by Erd\u0151s-R\u00e9nyi initialization [28], ERK is formulated as \u03b5(nl\u22121+nl+wl+hl) nl\u22121nlwlhl where \u03b5 indicates the sparsity level, n denotes the number of neurons at layer l, w and h are the width and the height of the l convolutional kernel.",
                "Inspired by SET, RigL [31] and SNFS [32] introduced gradient growth and momentum growth: the idea of using gradient and momentum information obtained by backward pass is to grow the promising connections that accelerate the exploration of optimal sparse topology.",
                "We know that uniform initialization assigns equal connections to each layer without considering the layer size and ERK allocates more connections to the layers with more parameters and less connections to the layers with fewer parameters [28, 31]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4a6d6bc1155ca80cdf9f58e08c9518a0389136f3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-14831",
                    "ArXiv": "2308.14831",
                    "DOI": "10.48550/arXiv.2308.14831",
                    "CorpusId": 261276874
                },
                "corpusId": 261276874,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4a6d6bc1155ca80cdf9f58e08c9518a0389136f3",
                "title": "Continual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model Updates",
                "abstract": "Continual learning (CL) refers to the ability of an intelligent system to sequentially acquire and retain knowledge from a stream of data with as little computational overhead as possible. To this end; regularization, replay, architecture, and parameter isolation approaches were introduced to the literature. Parameter isolation using a sparse network which enables to allocate distinct parts of the neural network to different tasks and also allows to share of parameters between tasks if they are similar. Dynamic Sparse Training (DST) is a prominent way to find these sparse networks and isolate them for each task. This paper is the first empirical study investigating the effect of different DST components under the CL paradigm to fill a critical research gap and shed light on the optimal configuration of DST for CL if it exists. Therefore, we perform a comprehensive study in which we investigate various DST components to find the best topology per task on well-known CIFAR100 and miniImageNet benchmarks in a task-incremental CL setup since our primary focus is to evaluate the performance of various DST criteria, rather than the process of mask selection. We found that, at a low sparsity level, Erdos-Renyi Kernel (ERK) initialization utilizes the backbone more efficiently and allows to effectively learn increments of tasks. At a high sparsity level, however, uniform initialization demonstrates more reliable and robust performance. In terms of growth strategy; performance is dependent on the defined initialization strategy, and the extent of sparsity. Finally, adaptivity within DST components is a promising way for better continual learners.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1948500935",
                        "name": "Murat Onur Yildirim"
                    },
                    {
                        "authorId": "2188331133",
                        "name": "Elif Ceren Gok Yildirim"
                    },
                    {
                        "authorId": "67102129",
                        "name": "Ghada Sokar"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "1717534",
                        "name": "J. Vanschoren"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent discoveries [10, 13, 25] demonstrate that layer-adaptive sparsity is the superior pruning scheme.",
                "[10], which is an extended Erd\u0151s-R\u00e9nyi method for CNNs pruning, where layer-wise sparsity is selected by a closed-form criterion dependent on merely the layer architecture (e.",
                "[45, 10] adopted a global pruning threshold throughout all layers in the network to meet the model sparsity constraint."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4dd5752eff94246005db1028f8281c2f17545f44",
                "externalIds": {
                    "ArXiv": "2308.10438",
                    "DBLP": "journals/corr/abs-2308-10438",
                    "DOI": "10.48550/arXiv.2308.10438",
                    "CorpusId": 261049780
                },
                "corpusId": 261049780,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4dd5752eff94246005db1028f8281c2f17545f44",
                "title": "Efficient Joint Optimization of Layer-Adaptive Weight Pruning in Deep Neural Networks",
                "abstract": "In this paper, we propose a novel layer-adaptive weight-pruning approach for Deep Neural Networks (DNNs) that addresses the challenge of optimizing the output distortion minimization while adhering to a target pruning ratio constraint. Our approach takes into account the collective influence of all layers to design a layer-adaptive pruning scheme. We discover and utilize a very important additivity property of output distortion caused by pruning weights on multiple layers. This property enables us to formulate the pruning as a combinatorial optimization problem and efficiently solve it through dynamic programming. By decomposing the problem into sub-problems, we achieve linear time complexity, making our optimization algorithm fast and feasible to run on CPUs. Our extensive experiments demonstrate the superiority of our approach over existing methods on the ImageNet and CIFAR-10 datasets. On CIFAR-10, our method achieves remarkable improvements, outperforming others by up to 1.0% for ResNet-32, 0.5% for VGG-16, and 0.7% for DenseNet-121 in terms of top-1 accuracy. On ImageNet, we achieve up to 4.7% and 4.6% higher top-1 accuracy compared to other methods for VGG-16 and ResNet-50, respectively. These results highlight the effectiveness and practicality of our approach for enhancing DNN performance through layer-adaptive weight pruning. Code will be available on https://github.com/Akimoto-Cris/RD_VIT_PRUNE.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109539057",
                        "name": "Kaixin Xu"
                    },
                    {
                        "authorId": "2241483692",
                        "name": "Zhe Wang"
                    },
                    {
                        "authorId": "117174641",
                        "name": "Xue Geng"
                    },
                    {
                        "authorId": "1390606776",
                        "name": "Min Wu"
                    },
                    {
                        "authorId": "2108674591",
                        "name": "Xiaoli Li"
                    },
                    {
                        "authorId": "144968898",
                        "name": "Weisi Lin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "04fb21b37406cd85f5e367cab451b71bb16d112e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-10051",
                    "ArXiv": "2308.10051",
                    "DOI": "10.48550/arXiv.2308.10051",
                    "CorpusId": 261049092
                },
                "corpusId": 261049092,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/04fb21b37406cd85f5e367cab451b71bb16d112e",
                "title": "The Snowflake Hypothesis: Training Deep GNN with One Node One Receptive field",
                "abstract": "Despite Graph Neural Networks demonstrating considerable promise in graph representation learning tasks, GNNs predominantly face significant issues with over-fitting and over-smoothing as they go deeper as models of computer vision realm. In this work, we conduct a systematic study of deeper GNN research trajectories. Our findings indicate that the current success of deep GNNs primarily stems from (I) the adoption of innovations from CNNs, such as residual/skip connections, or (II) the tailor-made aggregation algorithms like DropEdge. However, these algorithms often lack intrinsic interpretability and indiscriminately treat all nodes within a given layer in a similar manner, thereby failing to capture the nuanced differences among various nodes. To this end, we introduce the Snowflake Hypothesis -- a novel paradigm underpinning the concept of ``one node, one receptive field''. The hypothesis draws inspiration from the unique and individualistic patterns of each snowflake, proposing a corresponding uniqueness in the receptive fields of nodes in the GNNs. We employ the simplest gradient and node-level cosine distance as guiding principles to regulate the aggregation depth for each node, and conduct comprehensive experiments including: (1) different training schemes; (2) various shallow and deep GNN backbones, and (3) various numbers of layers (8, 16, 32, 64) on multiple benchmarks (six graphs including dense graphs with millions of nodes); (4) compare with different aggregation strategies. The observational results demonstrate that our hypothesis can serve as a universal operator for a range of tasks, and it displays tremendous potential on deep GNNs. It can be applied to various GNN frameworks, enhancing its effectiveness when operating in-depth, and guiding the selection of the optimal network depth in an explainable and generalizable way.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2119043696",
                        "name": "Kun Wang"
                    },
                    {
                        "authorId": "1669997484",
                        "name": "Guohao Li"
                    },
                    {
                        "authorId": "2154767163",
                        "name": "Shilong Wang"
                    },
                    {
                        "authorId": "2232926268",
                        "name": "Guibin Zhang"
                    },
                    {
                        "authorId": "40636684",
                        "name": "K. Wang"
                    },
                    {
                        "authorId": "2147330258",
                        "name": "Yang You"
                    },
                    {
                        "authorId": "1766837",
                        "name": "Xiaojiang Peng"
                    },
                    {
                        "authorId": "3431194",
                        "name": "Yuxuan Liang"
                    },
                    {
                        "authorId": null,
                        "name": "Yang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Table 1: Classification accuracy of sparse NNs for varying pruning rates \u03ba based on our proposed method ART with L1, L2, and HyperSparse regularization LHS compared to dense models, and masks obtained by SNIP [20], GraSP [42], SRatio [34], LTH [9], IMP [16] and RigL [8].",
                "In addition we evaluate IMP [16] and RigL [8] as dynamic pruning methods.",
                "The more resource efficient sparse\u2192sparse methods sustain sparse NNs during training [4,8,20,32,34,37,42], whereas dense\u2192sparse methods utilize all parameters before finding the final mask [9, 15, 16, 24, 31, 36].",
                "formes the methods SNIP [20], Grasp [42], SRatio [34], LTH [9] and RigL [8] on all sparsity levels.",
                "For example, RigL [8] iteratively prunes weights with low magnitude and therefore reactivates weights with highest gradient.",
                "The weights that are reactivated can be selected randomly [25] or determined by the gradients [3, 4, 8]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "34d62a481f178e926a82efe433885f39fb2a4d43",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-07163",
                    "ArXiv": "2308.07163",
                    "DOI": "10.48550/arXiv.2308.07163",
                    "CorpusId": 260886877
                },
                "corpusId": 260886877,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/34d62a481f178e926a82efe433885f39fb2a4d43",
                "title": "HyperSparse Neural Networks: Shifting Exploration to Exploitation through Adaptive Regularization",
                "abstract": "Sparse neural networks are a key factor in developing resource-efficient machine learning applications. We propose the novel and powerful sparse learning method Adaptive Regularized Training (ART) to compress dense into sparse networks. Instead of the commonly used binary mask during training to reduce the number of model weights, we inherently shrink weights close to zero in an iterative manner with increasing weight regularization. Our method compresses the pre-trained model knowledge into the weights of highest magnitude. Therefore, we introduce a novel regularization loss named HyperSparse that exploits the highest weights while conserving the ability of weight exploration. Extensive experiments on CIFAR and TinyImageNet show that our method leads to notable performance gains compared to other sparsification methods, especially in extremely high sparsity regimes up to 99.8 percent model sparsity. Additional investigations provide new insights into the patterns that are encoded in weights with high magnitudes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2127946406",
                        "name": "Patrick Glandorf"
                    },
                    {
                        "authorId": "2191607674",
                        "name": "Timo Kaiser"
                    },
                    {
                        "authorId": "1779035",
                        "name": "B. Rosenhahn"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[101] systematically investigate some dynamic sparse training methods (such as RigL [84], SET [93]) in RL.",
                "[84] propose Rigged Lottery (RigL) that actives new connections during training by using gradient magnitude.",
                "After pruning, many PDT methods ([64, 84, 85, 86, 87]) directly obtain the subnetworks and do not require trainingfrom-scratch or fine-tuning process anymore.",
                "Graesser et al. [101] systematically investigate some dynamic sparse training methods (such as RigL [84], SET [93]) in RL.\nEvci et al. [102] analyze the reasonability of dynamic sparse training.",
                "Evci et al. [84] propose Rigged Lottery (RigL) that actives new connections during training by using gradient magnitude.",
                "A class of the PDT methods ([84, 93, 94, 95, 96, 97, 98]) take randomly initialized sparse network rather than dense network as the input model.",
                "By repeating the pruneand-grow cycle during training, this kind of method keeps searching for better sparse architecture, which is classified as dynamic sparse training in [84]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "81b268e98042864c025b401eb6a54dcb566486d5",
                "externalIds": {
                    "ArXiv": "2308.06767",
                    "DBLP": "journals/corr/abs-2308-06767",
                    "DOI": "10.48550/arXiv.2308.06767",
                    "CorpusId": 260887757
                },
                "corpusId": 260887757,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/81b268e98042864c025b401eb6a54dcb566486d5",
                "title": "A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations",
                "abstract": "Modern deep neural networks, particularly recent large language models, come with massive model sizes that require significant computational and storage resources. To enable the deployment of modern models on resource-constrained environments and accelerate inference time, researchers have increasingly explored pruning techniques as a popular research direction in neural network compression. However, there is a dearth of up-to-date comprehensive review papers on pruning. To address this issue, in this survey, we provide a comprehensive review of existing research works on deep neural network pruning in a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to prune, and 4) fusion of pruning and other compression techniques. We then provide a thorough comparative analysis of seven pairs of contrast settings for pruning (e.g., unstructured/structured) and explore emerging topics, including post-training pruning, different levels of supervision for pruning, and broader applications (e.g., adversarial robustness) to shed light on the commonalities and differences of existing methods and lay the foundation for further method development. To facilitate future research, we build a curated collection of datasets, networks, and evaluations on different applications. Finally, we provide some valuable recommendations on selecting pruning methods and prospect promising research directions. We build a repository at https://github.com/hrcheng1066/awesome-pruning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2230189096",
                        "name": "Hongrong Cheng"
                    },
                    {
                        "authorId": "2211872272",
                        "name": "Miao Zhang"
                    },
                    {
                        "authorId": "3177281",
                        "name": "Javen Qinfeng Shi"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", inference), researchers have proposed various techniques (pruning [6, 13, 14, 40], sparse training [9, 21, 43], etc."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c8f859874a35f48a51f55fb5bd3710210dc8f1bb",
                "externalIds": {
                    "DBLP": "conf/icpp/JiangH0H23",
                    "DOI": "10.1145/3605573.3605625",
                    "CorpusId": 261706595
                },
                "corpusId": 261706595,
                "publicationVenue": {
                    "id": "29df4b17-9a16-4a4c-94a6-002f52e628b4",
                    "name": "International Conference on Parallel Processing",
                    "type": "conference",
                    "alternate_names": [
                        "ICPP",
                        "Int Conf Parallel Process",
                        "IEEE Int Conf Pulsed Power",
                        "IEEE International Conference on Pulsed Power"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1447"
                },
                "url": "https://www.semanticscholar.org/paper/c8f859874a35f48a51f55fb5bd3710210dc8f1bb",
                "title": "SNICIT: Accelerating Sparse Neural Network Inference via Compression at Inference Time on GPU",
                "abstract": "Sparse deep neural network (DNN) has become an important technique for reducing the inference cost of large DNNs. However, computing large sparse DNNs is very challenging because inference iterations can incur highly irregular patterns and unbalanced loads. To address this challenge, the recent HPEC Graph Challenge seeks novel high-performance inference methods for large sparse DNNs. Despite the rapid progress over the past four years, solutions have largely focused on static model compression or sparse multiplication kernels, while ignoring dynamic data compression at inference time which can achieve significant yet untapped performance benefits. Consequently, we propose SNICIT, a new GPU algorithm to accelerate large sparse DNN inference via compression at inference time. SNICIT leverages data clustering to transform intermediate results into a sparser representation that largely reduces computation over inference iterations. Evaluated on both HPEC Graph Challenge benchmarks and conventional DNNs (MNIST, CIFAR-10), SNICIT achieves 6 \u223c 444 \u00d7 and 1.36 \u223c 1.95 \u00d7 speed-ups over the previous champions, respectively.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2191048029",
                        "name": "Shui Jiang"
                    },
                    {
                        "authorId": "2239226085",
                        "name": "Tsung-Wei Huang"
                    },
                    {
                        "authorId": "48317300",
                        "name": "Bei Yu"
                    },
                    {
                        "authorId": "2069430186",
                        "name": "Tsung-Yi Ho"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our method slightly differs from the original pruning and regrowing strategy [5, 10] in that we allow the pruned mask to regrow back in the same round, which allows the amounts of readjusted masks to differ across clients.",
                "\u2022 Enforcing model sparsity proves effective to reduce the training workload by learning a sparse sub-network [10, 34].",
                "Given a sparsity rate S, both the communication and the computation cost in federated learning will roughly reduce by S percent since they are proportional to the number of parameters [5, 10].",
                "The reduction in computation cost is also proportional to the degree of sparsity, but slightly lower, since our sparsity setting follows the ERK distribution [10] which assigns lower sparsity to layers with fewer redundant parameters.",
                "Afterward, each client can readjust its maskmc via pruning and regrowing [5, 10], with a readjustment ratio of \u03b1s (Line 10-12).",
                "FedDST [5] utilizes dynamic sparse training [10] in generic FL setting."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "3296479d37c141e58eb2856241a97afef24d9bff",
                "externalIds": {
                    "DBLP": "conf/kdd/ZhangZWT23",
                    "DOI": "10.1145/3580305.3599311",
                    "CorpusId": 260499888
                },
                "corpusId": 260499888,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/3296479d37c141e58eb2856241a97afef24d9bff",
                "title": "DM-PFL: Hitchhiking Generic Federated Learning for Efficient Shift-Robust Personalization",
                "abstract": "Personalized federated learning collaboratively trains client-specific models, which holds potential for various mobile and IoT applications with heterogeneous data. However, existing solutions are vulnerable to distribution shifts between training and test data, and involve high training workloads on local devices. These two shortcomings hinder the practical usage of personalized federated learning on real-world mobile applications. To overcome these drawbacks, we explore efficient shift-robust personalization for federated learning. The principle is to hitchhike the global model to improve the shift-robustness of personalized models with minimal extra training overhead. To this end, we present DM-PFL, a novel framework that utilizes a dual masking mechanism to train both global and personalized models with weight-level parameter sharing and end-to-end sparse training. Evaluations on various datasets show that our methods not only improve the test accuracy in presence of test-time distribution shifts but also save the communication and computation costs compared to state-of-the-art personalized federated learning schemes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2227518658",
                        "name": "Wenhao Zhang"
                    },
                    {
                        "authorId": "2735715",
                        "name": "Zimu Zhou"
                    },
                    {
                        "authorId": "2107987065",
                        "name": "Yansheng Wang"
                    },
                    {
                        "authorId": "8230559",
                        "name": "Yongxin Tong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Roughly, for both benchmarks, it is known that sparsities lower than 90% can be achieved with approximately 1% accuracy loss relative to the original dense model, but accuracy rapidly decreases in the 90-95% range [27, 16], and that decreases are drastic at higher (\u2265 95%) sparsities [67, 43].",
                "In this context, we implemented three leading sparse training methods: Gradual Magnitude Pruning (GMP) [81], RigL [16] and AC/DC [60], which we execute for an increasing number of epochs between 100 (standard) and 1000 (10x).",
                "In prior work, RigL executed >5x extended training for a 99%-sparse model only [16].",
                "By contrast, in sparse training methods, parameters are pruned from the model during training from scratch, either close to initialization [16, 37, 46, 70, 66], or progressively as the model is trained [24, 21, 65].",
                "[21] found that extended training did improved results for GMP in some cases, while RigL [16] and Powerpropagation [66] found diminishing improvements.",
                "Therefore, to create a more fair comparison, we consider estimated Floating-Point Operations (FLOPs) necessary for inference; these are computed as in [16].",
                "A subset of sparse training methods are dynamic, in the sense that weights may be reintroduced during training [16, 60].",
                "[27, 14, 21, 16, 67, 65, 60], and language modelling using the BERT-base model [12] on the GLUE benchmark datasets [73], e.",
                "While several novel ways of choosing or updating the sparsity mask choice (step 1), have been investigated, by and large, for the second step, that of optimizing the remaining weights, sparse training methods largely emulate the hyperparameters of the baseline dense model, including the total number of training epochs [21, 36, 16, 60].",
                "The RigL pruning method [16] is a common, high-performing benchmark for dynamic sparse training."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "d2a42864605a502325a874bc470481ca1904ea0a",
                "externalIds": {
                    "ArXiv": "2308.02060",
                    "DBLP": "journals/corr/abs-2308-02060",
                    "DOI": "10.48550/arXiv.2308.02060",
                    "CorpusId": 260611140
                },
                "corpusId": 260611140,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d2a42864605a502325a874bc470481ca1904ea0a",
                "title": "Accurate Neural Network Pruning Requires Rethinking Sparse Optimization",
                "abstract": "Obtaining versions of deep neural networks that are both highly-accurate and highly-sparse is one of the main challenges in the area of model compression, and several high-performance pruning techniques have been investigated by the community. Yet, much less is known about the interaction between sparsity and the standard stochastic optimization techniques used for training sparse networks, and most existing work uses standard dense schedules and hyperparameters for training sparse networks. In this work, we examine the impact of high sparsity on model training using the standard computer vision and natural language processing sparsity benchmarks. We begin by showing that using standard dense training recipes for sparse training is suboptimal, and results in under-training. We provide new approaches for mitigating this issue for both sparse pre-training of vision models (e.g. ResNet50/ImageNet) and sparse fine-tuning of language models (e.g. BERT/GLUE), achieving state-of-the-art results in both settings in the high-sparsity regime, and providing detailed analyses for the difficulty of sparse training in both scenarios. Our work sets a new threshold in terms of the accuracies that can be achieved under high sparsity, and should inspire further research into improving sparse model training, to reach higher accuracies under high sparsity, but also to do so efficiently.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2006108901",
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "authorId": "40992614",
                        "name": "Eldar Kurtic"
                    },
                    {
                        "authorId": "2229578027",
                        "name": "Eugenia Iofinova"
                    },
                    {
                        "authorId": "1502248377",
                        "name": "Elias Frantar"
                    },
                    {
                        "authorId": "3341722",
                        "name": "Alexandra Peste"
                    },
                    {
                        "authorId": "3311387",
                        "name": "Dan Alistarh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While not the focus of this work, a similar motivation can also be found in dynamic sparse training methods, which aim to identify relevant sub-networks during training and promote the prunability of the network [70, 23, 26]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0b0d22adc201913c7ff186504db129cc51d9971c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-06440",
                    "ArXiv": "2307.06440",
                    "DOI": "10.48550/arXiv.2307.06440",
                    "CorpusId": 259847436
                },
                "corpusId": 259847436,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0b0d22adc201913c7ff186504db129cc51d9971c",
                "title": "No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models",
                "abstract": "The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "66914903",
                        "name": "Jean Kaddour"
                    },
                    {
                        "authorId": "12389060",
                        "name": "Oscar Key"
                    },
                    {
                        "authorId": "40284207",
                        "name": "Piotr Nawrot"
                    },
                    {
                        "authorId": "3051815",
                        "name": "Pasquale Minervini"
                    },
                    {
                        "authorId": "1940272",
                        "name": "Matt J. Kusner"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "90a0991d7943d33f8bb83694014fafe9e6140324",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-03930",
                    "ArXiv": "2307.03930",
                    "DOI": "10.48550/arXiv.2307.03930",
                    "CorpusId": 259501639
                },
                "corpusId": 259501639,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/90a0991d7943d33f8bb83694014fafe9e6140324",
                "title": "Rosko: Row Skipping Outer Products for Sparse Matrix Multiplication Kernels",
                "abstract": "We propose Rosko -- row skipping outer products -- for deriving sparse matrix multiplication (SpMM) kernels in reducing computation and memory access requirements of deep neural networks (DNNs). Rosko allows skipping of entire row computations during program execution with low sparsity-management overheads. We analytically derive sparse CPU kernels that adapt to given hardware characteristics to effectively utilize processor cores and minimize data movement without the need for auto-tuning or search space exploration. Rosko can be integrated with other outer product scheduling methods, allowing them to leverage row skipping by using Rosko's packing format to skip unnecessary computation. Rosko kernels outperform existing auto-tuning and search-based solutions as well as state-of-the-art vendor-optimized libraries on real hardware across a variety of neural network workloads. For matrices with sparsities ranging from 65% to 99.8% typically found in machine learning, Rosko kernels achieve up to a 6.5x runtime reduction on Intel and ARM CPUs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2071390207",
                        "name": "Vikas Natesh"
                    },
                    {
                        "authorId": "2133406117",
                        "name": "Andrew Sabot"
                    },
                    {
                        "authorId": "1475690539",
                        "name": "H. Kung"
                    },
                    {
                        "authorId": "2183454860",
                        "name": "Mark Ting"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This sparse-to-sparse strategy with improved training efficiency facilitates \u201dsingle-shot\u201d pruning [20] and Dynamic Sparse Training (DST) [21]\u2013[23].",
                "RigL-PFL attains better results than SNIP, which veri\ufb01es the effectiveness of dynamically updating sparse networks.",
                "4) RigL-PFL [23].",
                "In particular, RigL [23] activates more important parameters according to the instantaneous gradient information in the previous round of training.",
                "These graphs depict the results of the other two most competitive approaches (RigL-PFL, Hermes) compared to ASPFL.",
                "In contrast, RigL-PFL is unable to reach the desired 99% accuracy level, indicating the superior performance of ASPFL in terms of convergence speed.",
                "For Static, SNIP, RigL-PFL and ASPFL with sparse-to-sparse training, the communication overhead is similar because they have the same amount of remaining parameters.",
                "6 (c), ASPFL achieved a signi\ufb01cantly higher accuracy of 44% compared to the accuracies of only 40% and 28% achieved by Hermes and RigL-PFL at the 350th communication round, respectively."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6753c74610663834163d2a91b9aa0a8a10c08eb0",
                "externalIds": {
                    "DBLP": "conf/icws/JiangLZM23",
                    "DOI": "10.1109/ICWS60048.2023.00046",
                    "CorpusId": 262075862
                },
                "corpusId": 262075862,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6753c74610663834163d2a91b9aa0a8a10c08eb0",
                "title": "ASPFL: Efficient Personalized Federated Learning for Edge Based on Adaptive Sparse Training",
                "abstract": "One of the primary challenges in cloud-edge environments is efficiently utilizing significant amounts of data on edge devices for machine learning tasks, enabling adaptation to increasingly complex computing and service scenarios. Federated Learning (FL) is a machine learning paradigm that enables collaborative training of models involving multiple data warehouses in a privacy-preserving manner. However, classical federated learning has poor convergence on highly heterogeneous data, which limits its performance of global model on each edge device. The emergence of Personalized Federated Learning (PFL) effectively alleviates data heterogeneity, but learning a personalized model may incur greater overheads. In this paper, we propose an efficient FL framework named as ASPFL, which uses dynamic sparse training for personalized federated learning to maintain model performance while reducing computational and communication overheads in cloud-edge environments. By adaptively allocating the dynamic sparsity from a global perspective to explore sparse network structure during training, ASPFL improves the independent parameter exploration process of local sparse training to adapt to various heterogeneous situations and solves the Non-IID challenge of FL. The abundant experimental results show that ASPFL outperforms state-of-the-art methods in performance, overheads, and convergence speed in PFL.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2243911687",
                        "name": "Yuhui Jiang"
                    },
                    {
                        "authorId": "2243144576",
                        "name": "Xingjian Lu"
                    },
                    {
                        "authorId": "2243372774",
                        "name": "Haikun Zheng"
                    },
                    {
                        "authorId": "2242904898",
                        "name": "Wei Mao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Sparse training is an efficient and effective way to add this type of regularization to a neural network [27,30,31]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c9ea2c6c108ee3e94e4c248639a868242fa624a3",
                "externalIds": {
                    "DBLP": "journals/sensors/BentoRCRB23",
                    "PubMedCentral": "10386236",
                    "DOI": "10.3390/s23146511",
                    "CorpusId": 260051177,
                    "PubMed": "37514805"
                },
                "corpusId": 260051177,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c9ea2c6c108ee3e94e4c248639a868242fa624a3",
                "title": "Exploring Regularization Methods for Domain Generalization in Accelerometer-Based Human Activity Recognition",
                "abstract": "The study of Domain Generalization (DG) has gained considerable momentum in the Machine Learning (ML) field. Human Activity Recognition (HAR) inherently encompasses diverse domains (e.g., users, devices, or datasets), rendering it an ideal testbed for exploring Domain Generalization. Building upon recent work, this paper investigates the application of regularization methods to bridge the generalization gap between traditional models based on handcrafted features and deep neural networks. We apply various regularizers, including sparse training, Mixup, Distributionally Robust Optimization (DRO), and Sharpness-Aware Minimization (SAM), to deep learning models and assess their performance in Out-of-Distribution (OOD) settings across multiple domains using homogenized public datasets. Our results show that Mixup and SAM are the best-performing regularizers. However, they are unable to match the performance of models based on handcrafted features. This suggests that while regularization techniques can improve OOD robustness to some extent, handcrafted features remain superior for domain generalization in HAR tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2224337887",
                        "name": "Nuno Bento"
                    },
                    {
                        "authorId": "2186473580",
                        "name": "Joana Rebelo"
                    },
                    {
                        "authorId": "34934072",
                        "name": "A. Carreiro"
                    },
                    {
                        "authorId": "98009671",
                        "name": "Fran\u00e7ois Ravache"
                    },
                    {
                        "authorId": "2929788",
                        "name": "M. Barandas"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following [37, 39], we apply a cosine decay scheduler to alleviate this problem:",
                "The criterion of pruning could be weight magnitude [44], gradient [37] and Hessian [35, 45], etc.",
                "Limited by the requirement for the pretrained model, some recent research [37, 38, 39, 40, 41, 42, 43] attempts to discover a sparse network directly from the training process."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "a0a5ed97c30b38dd32bbd755b6f9a9e1f6ec5846",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-17504",
                    "ArXiv": "2306.17504",
                    "DOI": "10.48550/arXiv.2306.17504",
                    "CorpusId": 259308883
                },
                "corpusId": 259308883,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a0a5ed97c30b38dd32bbd755b6f9a9e1f6ec5846",
                "title": "Systematic Investigation of Sparse Perturbed Sharpness-Aware Minimization Optimizer",
                "abstract": "Deep neural networks often suffer from poor generalization due to complex and non-convex loss landscapes. Sharpness-Aware Minimization (SAM) is a popular solution that smooths the loss landscape by minimizing the maximized change of training loss when adding a perturbation to the weight. However, indiscriminate perturbation of SAM on all parameters is suboptimal and results in excessive computation, double the overhead of common optimizers like Stochastic Gradient Descent (SGD). In this paper, we propose Sparse SAM (SSAM), an efficient and effective training scheme that achieves sparse perturbation by a binary mask. To obtain the sparse mask, we provide two solutions based on Fisher information and dynamic sparse training, respectively. We investigate the impact of different masks, including unstructured, structured, and $N$:$M$ structured patterns, as well as explicit and implicit forms of implementing sparse perturbation. We theoretically prove that SSAM can converge at the same rate as SAM, i.e., $O(\\log T/\\sqrt{T})$. Sparse SAM has the potential to accelerate training and smooth the loss landscape effectively. Extensive experimental results on CIFAR and ImageNet-1K confirm that our method is superior to SAM in terms of efficiency, and the performance is preserved or even improved with a perturbation of merely 50\\% sparsity. Code is available at https://github.com/Mi-Peng/Systematic-Investigation-of-Sparse-Perturbed-Sharpness-Aware-Minimization-Optimizer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2148392043",
                        "name": "Peng Mi"
                    },
                    {
                        "authorId": "2144035454",
                        "name": "Li Shen"
                    },
                    {
                        "authorId": "2143150727",
                        "name": "Tianhe Ren"
                    },
                    {
                        "authorId": "2110191063",
                        "name": "Yiyi Zhou"
                    },
                    {
                        "authorId": "2152009106",
                        "name": "Tianshuo Xu"
                    },
                    {
                        "authorId": "1759841",
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "authorId": "121698214",
                        "name": "Tongliang Liu"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    },
                    {
                        "authorId": "2135519749",
                        "name": "Dacheng Tao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[51] use dynamic sparse training [55, 13, 16] methods to efficiently generate diverse networks for ensembling.",
                "Ensuring a fair comparison we only consider the dense-to-sparse training paradigm, as opposed to pruning at initialization [46, 66] and dynamic sparse training methods [13, 16]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "8ab20dd7f0ffb988cbf498215435201afb069909",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-16788",
                    "ArXiv": "2306.16788",
                    "DOI": "10.48550/arXiv.2306.16788",
                    "CorpusId": 259287063
                },
                "corpusId": 259287063,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8ab20dd7f0ffb988cbf498215435201afb069909",
                "title": "Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging",
                "abstract": "Neural networks can be significantly compressed by pruning, leading to sparse models requiring considerably less storage and floating-point operations while maintaining predictive performance. Model soups (Wortsman et al., 2022) improve generalization and out-of-distribution performance by averaging the parameters of multiple models into a single one without increased inference time. However, identifying models in the same loss basin to leverage both sparsity and parameter averaging is challenging, as averaging arbitrary sparse models reduces the overall sparsity due to differing sparse connectivities. In this work, we address these challenges by demonstrating that exploring a single retraining phase of Iterative Magnitude Pruning (IMP) with varying hyperparameter configurations, such as batch ordering or weight decay, produces models that are suitable for averaging and share the same sparse connectivity by design. Averaging these models significantly enhances generalization performance compared to their individual components. Building on this idea, we introduce Sparse Model Soups (SMS), a novel method for merging sparse models by initiating each prune-retrain cycle with the averaged model of the previous phase. SMS maintains sparsity, exploits sparse network benefits being modular and fully parallelizable, and substantially improves IMP's performance. Additionally, we demonstrate that SMS can be adapted to enhance the performance of state-of-the-art pruning during training approaches.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2056708985",
                        "name": "Max Zimmer"
                    },
                    {
                        "authorId": "2064617407",
                        "name": "Christoph Spiegel"
                    },
                    {
                        "authorId": "145729210",
                        "name": "S. Pokutta"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As the growing criterion, we choose the uniform random [40] and gradient [12] approaches, as they are widely used in the literature.",
                "2 and gradient growth (we select the gradient-based growth method as it is known to provide good performance in this setup [12]).",
                "One of the most common choices for initialization schemes is the Erd\u0151s-R\u00e9nyi (ER) method [40], and its convolutional variant, the ER-Kernel (ERK) [12].",
                "In DST, the neural network structure is constantly evolving by pruning and growing back weights during training [40, 3, 11, 12, 59, 56, 1, 9, 27].",
                "Most research in this domain is devoted to investigating the best growth criterion as the most influential factor in the performance of DST [3, 12, 11, 1], disregarding any potential contribution coming from the choice of the weight removal algorithm.",
                "The second category selects weights based on the largest gradient magnitude [12].",
                "We compare the results with the original dense model and static sparse training with ERK initialization.",
                "Due to its simplicity and effectiveness, the magnitude criterion has been a common choice in standard post-training pruning, as well as in sparse training [15, 12].",
                "One of the most common choices for initialization schemes is the Erdo\u030bs-R\u00e9nyi (ER) method [40], and its convolutional variant, the ER-Kernel (ERK) [12].",
                "Most notably, in computer vision, DST demonstrated that it is sufficient to use only 20% of the original parameters of ResNet50 to train ImageNet without any drop in performance [12, 37].",
                "DST Settings specific for dynamic sparse training: We use the ER initialization for the distribution of sparsity levels in MLP, and the ERK initialization for the convolutional models.",
                "CMagnitude CSET CMEST CSensitivity CSNIP |\u03b8| |\u03b8+|, |\u03b8\u2212| |\u03b8|+ \u03bb|\u2207\u03b8L(D)| |\u2207\u03b8L(D)| |\u03b8| |\u03b8||\u2207\u03b8L(D)| random growth \u00d7 SET[40] MEST[59] Sensitivity[43] \u00d7 gradient growth RigL[12] \u00d7 \u00d7 \u00d7 \u00d7"
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "70be7596d63efc18ec7ec9e0613a6136efe8c299",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-12230",
                    "ArXiv": "2306.12230",
                    "DOI": "10.48550/arXiv.2306.12230",
                    "CorpusId": 259212242
                },
                "corpusId": 259212242,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/70be7596d63efc18ec7ec9e0613a6136efe8c299",
                "title": "Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training",
                "abstract": "Dynamic Sparse Training (DST) is a rapidly evolving area of research that seeks to optimize the sparse initialization of a neural network by adapting its topology during training. It has been shown that under specific conditions, DST is able to outperform dense models. The key components of this framework are the pruning and growing criteria, which are repeatedly applied during the training process to adjust the network's sparse connectivity. While the growing criterion's impact on DST performance is relatively well studied, the influence of the pruning criterion remains overlooked. To address this issue, we design and perform an extensive empirical analysis of various pruning criteria to better understand their effect on the dynamics of DST solutions. Surprisingly, we find that most of the studied methods yield similar results. The differences become more significant in the low-density regime, where the best performance is predominantly given by the simplest technique: magnitude-based pruning. The code is provided at https://github.com/alooow/fantastic_weights_paper",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46662285",
                        "name": "A. Nowak"
                    },
                    {
                        "authorId": "2122740553",
                        "name": "Bram Grooten"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "145541197",
                        "name": "J. Tabor"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The distribution of s \u03bb adopts the ERK distribution model [7], and the mask matrix m is generated according to the ERK distribution model."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "60726b120960472a68705482b0bb23980fe9c019",
                "externalIds": {
                    "DBLP": "conf/bmsb/LiuCSDZ23",
                    "DOI": "10.1109/BMSB58369.2023.10211422",
                    "CorpusId": 260935048
                },
                "corpusId": 260935048,
                "publicationVenue": {
                    "id": "6813b010-0f27-487d-afd0-c3d67cd5654f",
                    "name": "IEEE international Symposium on Broadband Multimedia Systems and Broadcasting",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE int Symp Broadband Multimedia Syst Broadcast",
                        "International Symposium on Broadband Multimedia Systems and Broadcasting",
                        "BMSB",
                        "Int Symp Broadband Multimedia Syst Broadcast"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/60726b120960472a68705482b0bb23980fe9c019",
                "title": "Federated Learning Model Training Mechanism with Edge Cloud Collaboration for Services in Smart Cities",
                "abstract": "With the development of big data and artificial intelligence, problems related to data privacy have emerged in smart cities. In the context of large-scale data, federated learning can effectively utilize data resources and ensure user data privacy. This paper designs a training mechanism of edge cloud collaborative federated learning model for smart city applications, so that the model training is carried out on the edge side, without the need to gather the original data set to the cloud computing center, to ensure the privacy and security of data. Finally, it is verified and tested in the vehicle recognition scene in the traffic field. The results show that this mechanism has certain advantages in detecting delay and protecting privacy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118822075",
                        "name": "Dan Liu"
                    },
                    {
                        "authorId": "1491646821",
                        "name": "Enfang Cui"
                    },
                    {
                        "authorId": "2117689125",
                        "name": "Yun Shen"
                    },
                    {
                        "authorId": "2065859492",
                        "name": "Peng Ding"
                    },
                    {
                        "authorId": "2110148389",
                        "name": "Zhichao Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "69300ad5b60d02d8938f75a48a8f6442e670645b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-07030",
                    "ArXiv": "2306.07030",
                    "DOI": "10.48550/arXiv.2306.07030",
                    "CorpusId": 259138263
                },
                "corpusId": 259138263,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/69300ad5b60d02d8938f75a48a8f6442e670645b",
                "title": "Resource Efficient Neural Networks Using Hessian Based Pruning",
                "abstract": "Neural network pruning is a practical way for reducing the size of trained models and the number of floating-point operations. One way of pruning is to use the relative Hessian trace to calculate sensitivity of each channel, as compared to the more common magnitude pruning approach. However, the stochastic approach used to estimate the Hessian trace needs to iterate over many times before it can converge. This can be time-consuming when used for larger models with many millions of parameters. To address this problem, we modify the existing approach by estimating the Hessian trace using FP16 precision instead of FP32. We test the modified approach (EHAP) on ResNet-32/ResNet-56/WideResNet-28-8 trained on CIFAR10/CIFAR100 image classification tasks and achieve faster computation of the Hessian trace. Specifically, our modified approach can achieve speed ups ranging from 17% to as much as 44% during our experiments on different combinations of model architectures and GPU devices. Our modified approach also takes up around 40% less GPU memory when pruning ResNet-32 and ResNet-56 models, which allows for a larger Hessian batch size to be used for estimating the Hessian trace. Meanwhile, we also present the results of pruning using both FP16 and FP32 Hessian trace calculation and show that there are no noticeable accuracy differences between the two. Overall, it is a simple and effective way to compute the relative Hessian trace faster without sacrificing on pruned model performance. We also present a full pipeline using EHAP and quantization aware training (QAT), using INT8 QAT to compress the network further after pruning. In particular, we use symmetric quantization for the weights and asymmetric quantization for the activations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2171822385",
                        "name": "J. Chong"
                    },
                    {
                        "authorId": "2109835363",
                        "name": "Manas Gupta"
                    },
                    {
                        "authorId": "2108562840",
                        "name": "Lihui Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "RigL [8] alternatively removes and revives weights based on magnitudes and gradients.",
                "This variability in spatial sparsity remains consistent across different layers and network types, irrespective of the specific unstructured sparsity technique employed [11, 8, 27].",
                "Figure 2: Spatial sparsity of common unstructured sparsity methods including Magnitude-based sparsity [11], RigL [8], GraNet [23].",
                "We do not delve deeply into this matter at present, but it is unequivocal that unstructured sparsity methods [11, 8, 27] allocate more weights to certain fixed visual points simultaneously."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "93dc77f12afd0ce3e042828880d308342b861f17",
                "externalIds": {
                    "ArXiv": "2306.05612",
                    "DBLP": "journals/corr/abs-2306-05612",
                    "DOI": "10.48550/arXiv.2306.05612",
                    "CorpusId": 259129570
                },
                "corpusId": 259129570,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/93dc77f12afd0ce3e042828880d308342b861f17",
                "title": "Spatial Re-parameterization for N: M Sparsity",
                "abstract": "This paper presents a Spatial Re-parameterization (SpRe) method for the N:M sparsity in CNNs. SpRe is stemmed from an observation regarding the restricted variety in spatial sparsity present in N:M sparsity compared with unstructured sparsity. Particularly, N:M sparsity exhibits a fixed sparsity rate within the spatial domains due to its distinctive pattern that mandates N non-zero components among M successive weights in the input channel dimension of convolution filters. On the contrary, we observe that unstructured sparsity displays a substantial divergence in sparsity across the spatial domains, which we experimentally verified to be very crucial for its robust performance retention compared with N:M sparsity. Therefore, SpRe employs the spatial-sparsity distribution of unstructured sparsity to assign an extra branch in conjunction with the original N:M branch at training time, which allows the N:M sparse network to sustain a similar distribution of spatial sparsity with unstructured sparsity. During inference, the extra branch can be further re-parameterized into the main N:M branch, without exerting any distortion on the sparse pattern or additional computation costs. SpRe has achieved a commendable feat by matching the performance of N:M sparsity methods with state-of-the-art unstructured sparsity methods across various benchmarks. Code and models are anonymously available at \\url{https://github.com/zyxxmu/SpRe}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108078624",
                        "name": "Yu-xin Zhang"
                    },
                    {
                        "authorId": "49352079",
                        "name": "Mingbao Lin"
                    },
                    {
                        "authorId": "2112888098",
                        "name": "Yunshan Zhong"
                    },
                    {
                        "authorId": "2133595441",
                        "name": "Mengzhao Chen"
                    },
                    {
                        "authorId": "2070266415",
                        "name": "Fei Chao"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The gradient-based growth rule forms new synaptic connections by selecting the pruned connections with the largest gradient obtained from the instantaneous weight gradient information in (Evci et al. 2020; Dettmers and Zettlemoyer 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f4e685992c8635225acabbe87d3900d104c9e78e",
                "externalIds": {
                    "DBLP": "conf/aaai/ShenXLW0T23",
                    "ArXiv": "2306.03693",
                    "DOI": "10.48550/arXiv.2306.03693",
                    "CorpusId": 259088553
                },
                "corpusId": 259088553,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/f4e685992c8635225acabbe87d3900d104c9e78e",
                "title": "ESL-SNNs: An Evolutionary Structure Learning Strategy for Spiking Neural Networks",
                "abstract": "Spiking neural networks (SNNs) have manifested remarkable advantages in power consumption and event-driven property during the inference process. To take full advantage of low power consumption and improve the efficiency of these models further, the pruning methods have been explored to find sparse SNNs without redundancy connections after training. However, parameter redundancy still hinders the efficiency of SNNs during training. In the human brain, the rewiring process of neural networks is highly dynamic, while synaptic connections maintain relatively sparse during brain development. Inspired by this, here we propose an efficient evolutionary structure learning (ESL) framework for SNNs, named ESL-SNNs, to implement the sparse SNN training from scratch. The pruning and regeneration of synaptic connections in SNNs evolve dynamically during learning, yet keep the structural sparsity at a certain level. As a result, the ESL-SNNs can search for optimal sparse connectivity by exploring all possible parameters across time. Our experiments show that the proposed ESL-SNNs framework is able to learn SNNs with sparse structures effectively while reducing the limited accuracy. The ESL-SNNs achieve merely 0.28% accuracy loss with 10% connection density on the DVS-Cifar10 dataset. Our work presents a brand-new approach for sparse training of SNNs from scratch with biologically plausible evolutionary mechanisms, closing the gap in the expressibility between sparse training and dense training. Hence, it has great potential for SNN lightweight training and inference with low power consumption and small memory usage.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "28995069",
                        "name": "Jiangrong Shen"
                    },
                    {
                        "authorId": "2110405890",
                        "name": "Qi Xu"
                    },
                    {
                        "authorId": "2150168180",
                        "name": "Jian K. Liu"
                    },
                    {
                        "authorId": "2108895611",
                        "name": "Yueming Wang"
                    },
                    {
                        "authorId": "2055725832",
                        "name": "Gang Pan"
                    },
                    {
                        "authorId": "3134548",
                        "name": "Huajin Tang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This also excludes any strategy that requires, even if only intermittently, a full, dense gradient to be computed, such as [10]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2e831c222724a5e3b21bad3131cc77c3dce4d314",
                "externalIds": {
                    "ArXiv": "2306.03725",
                    "DBLP": "journals/corr/abs-2306-03725",
                    "DOI": "10.48550/arXiv.2306.03725",
                    "CorpusId": 259088764
                },
                "corpusId": 259088764,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2e831c222724a5e3b21bad3131cc77c3dce4d314",
                "title": "Towards Memory-Efficient Training for Extremely Large Output Spaces - Learning with 500k Labels on a Single Commodity GPU",
                "abstract": "In classification problems with large output spaces (up to millions of labels), the last layer can require an enormous amount of memory. Using sparse connectivity would drastically reduce the memory requirements, but as we show below, it can result in much diminished predictive performance of the model. Fortunately, we found that this can be mitigated by introducing a penultimate layer of intermediate size. We further demonstrate that one can constrain the connectivity of the sparse layer to be uniform, in the sense that each output neuron will have the exact same number of incoming connections. This allows for efficient implementations of sparse matrix multiplication and connection redistribution on GPU hardware. Via a custom CUDA implementation, we show that the proposed approach can scale to datasets with 670,000 labels on a single commodity GPU with only 4GB memory.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2064791909",
                        "name": "Erik Schultheis"
                    },
                    {
                        "authorId": "1986256",
                        "name": "Rohit Babbar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another fashion is dynamic sparsity exploration [49, 50, 51, 52, 53, 54, 55, 56, 57, 58] which allows pruned connections can be re-activated in the latter training stage."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9d460930d9b5d12a65ff2b3efa23047ec75fbca1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-03805",
                    "ArXiv": "2306.03805",
                    "DOI": "10.48550/arXiv.2306.03805",
                    "CorpusId": 259088941
                },
                "corpusId": 259088941,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9d460930d9b5d12a65ff2b3efa23047ec75fbca1",
                "title": "The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter",
                "abstract": "Large pre-trained transformers are show-stealer in modern-day deep learning, and it becomes crucial to comprehend the parsimonious patterns that exist within them as they grow in scale. With exploding parameter counts, Lottery Ticket Hypothesis (LTH) and its variants, have lost their pragmatism in sparsifying them due to high computation and memory bottleneck of repetitive train-prune-retrain routine of iterative magnitude pruning (IMP) which worsens with increasing model size. This paper comprehensively studies induced sparse patterns across multiple large pre-trained vision and language transformers. We propose the existence of -- essential sparsity defined with a sharp dropping point beyond which the performance declines much faster w.r.t the rise of sparsity level, when we directly remove weights with the smallest magnitudes in one-shot without re-training. We also find essential sparsity to hold valid for N:M sparsity patterns as well as on modern-scale large language models (Vicuna-7B). We also present an intriguing emerging phenomenon of abrupt sparsification during the pre-training of BERT, i.e., BERT suddenly becomes heavily sparse in pre-training after certain iterations. Moreover, our observations also indicate a counter-intuitive finding that BERT trained with a larger amount of pre-training data tends to have a better ability to condense knowledge in comparatively relatively fewer parameters. Lastly, we investigate the effect of the pre-training loss on essential sparsity and discover that self-supervised learning (SSL) objectives trigger stronger emergent sparsification properties than supervised learning (SL). Our codes are available at \\url{https://github.com/VITA-Group/essential_sparsity}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145018564",
                        "name": "Ajay Jaiswal"
                    },
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026parameter redundancy (Aghajanyan et al., 2021), allowing arbitrary selection of trainable parameters for tuning without greatly degrading performance (Desai et al., 2019; Chen et al., 2020; Prasanna et al., 2020; Evci et al., 2020); thus, controllers (modules) might have higher degrees of freedom.",
                ", 2021), allowing arbitrary selection of trainable parameters for tuning without greatly degrading performance (Desai et al., 2019; Chen et al., 2020; Prasanna et al., 2020; Evci et al., 2020); thus, controllers (modules) might have higher degrees of freedom."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b5b25c34f6d55c89686b070937b0cb8029c2732e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-02320",
                    "ArXiv": "2306.02320",
                    "DOI": "10.48550/arXiv.2306.02320",
                    "CorpusId": 259076283
                },
                "corpusId": 259076283,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b5b25c34f6d55c89686b070937b0cb8029c2732e",
                "title": "Arbitrary Few Parameters are Good Enough for Adapting Large-scale Pre-trained Language Models",
                "abstract": "Parameter-efficient tuning (PET) methods can effectively drive extremely large pre-trained language models (PLMs) by only training minimal parameters. Different PET methods utilize different manually designed modules. In a small PLM, there are usually noticeable performance differences among PET methods. Nevertheless, when a PLM's scale grows up to tens of billions of parameters, all PET methods achieve almost the same performance and even perform on par with the full-parameter fine-tuning method. Hence, we hypothesize that model scaling can mitigate the design differences (the module structures and the number of trainable parameters) among PET methods. To study this hypothesis, we introduce a more flexible PET method - arbitrary PET (APET) method - to be compatible with arbitrary module structures and any number of trainable parameters. Then, we experiment on $11$ NLP tasks of $5$ types and $2$ representative PLMs. From our investigations, we find that the model scaling (1) mitigates the effects of the arbitrary module structure on the performance of tuning methods, and (2) enables the tuning methods to optimize fewer parameters to achieve the full-parameter fine-tuning performance. Intriguingly, we also observe that all tuning methods require almost the same number of trainable parameters to drive PLMs. We discuss this phenomenon and the above two findings collectively from optimization perspectives to fathom the mechanisms behind them. These conclusions not only demonstrate the positive impact of model scaling on tuning methods but disclose its mechanisms, which help us design more effective and efficient tuning methods on larger-scale PLMs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48576745",
                        "name": "Yusheng Su"
                    },
                    {
                        "authorId": "2151547817",
                        "name": "Chi-Min Chan"
                    },
                    {
                        "authorId": "2112798028",
                        "name": "Jialing Cheng"
                    },
                    {
                        "authorId": "50625437",
                        "name": "Yujia Qin"
                    },
                    {
                        "authorId": "2149202150",
                        "name": "Yankai Lin"
                    },
                    {
                        "authorId": "1576223501",
                        "name": "Shengding Hu"
                    },
                    {
                        "authorId": "19343873",
                        "name": "Zonghan Yang"
                    },
                    {
                        "authorId": "46649145",
                        "name": "Ning Ding"
                    },
                    {
                        "authorId": "2141313179",
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "authorId": "1753344",
                        "name": "Maosong Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "One approach is to determine nonzero weights dynamically, such as dense-to-sparse training [2], pruning [11], and sparse-to-sparse training [12, 13]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "115e062438efda602eddee4cb4a4087157a87ffa",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-01470",
                    "ArXiv": "2306.01470",
                    "DOI": "10.48550/arXiv.2306.01470",
                    "CorpusId": 259064181
                },
                "corpusId": 259064181,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/115e062438efda602eddee4cb4a4087157a87ffa",
                "title": "MLP-Mixer as a Wide and Sparse MLP",
                "abstract": "Multi-layer perceptron (MLP) is a fundamental component of deep learning that has been extensively employed for various problems. However, recent empirical successes in MLP-based architectures, particularly the progress of the MLP-Mixer, have revealed that there is still hidden potential in improving MLPs to achieve better performance. In this study, we reveal that the MLP-Mixer works effectively as a wide MLP with certain sparse weights. Initially, we clarify that the mixing layer of the Mixer has an effective expression as a wider MLP whose weights are sparse and represented by the Kronecker product. This expression naturally defines a permuted-Kronecker (PK) family, which can be regarded as a general class of mixing layers and is also regarded as an approximation of Monarch matrices. Subsequently, because the PK family effectively constitutes a wide MLP with sparse weights, one can apply the hypothesis proposed by Golubeva, Neyshabur and Gur-Ari (2021) that the prediction performance improves as the width (sparsity) increases when the number of weights is fixed. We empirically verify this hypothesis by maximizing the effective width of the MLP-Mixer, which enables us to determine the appropriate size of the mixing layers quantitatively.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "41017338",
                        "name": "Tomohiro Hayase"
                    },
                    {
                        "authorId": "3430922",
                        "name": "Ryo Karakida"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c34746415e104428d6f791fbc7c61cf0081dcfa1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-19844",
                    "ArXiv": "2305.19844",
                    "DOI": "10.48550/arXiv.2305.19844",
                    "CorpusId": 258987857
                },
                "corpusId": 258987857,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c34746415e104428d6f791fbc7c61cf0081dcfa1",
                "title": "Learning Task-preferred Inference Routes for Gradient De-conflict in Multi-output DNNs",
                "abstract": "Multi-output deep neural networks(MONs) contain multiple task branches, and these tasks usually share partial network filters that lead to the entanglement of different task inference routes. Due to the inconsistent optimization objectives, the task gradients used for training MONs will interfere with each other on the shared routes, which will decrease the overall model performance. To address this issue, we propose a novel gradient de-conflict algorithm named DR-MGF(Dynamic Routes and Meta-weighted Gradient Fusion) in this work. Different from existing de-conflict methods, DR-MGF achieves gradient de-conflict in MONs by learning task-preferred inference routes. The proposed method is motivated by our experimental findings: the shared filters are not equally important to different tasks. By designing the learnable task-specific importance variables, DR-MGF evaluates the importance of filters for different tasks. Through making the dominances of tasks over filters be proportional to the task-specific importance of filters, DR-MGF can effectively reduce the inter-task interference. The task-specific importance variables ultimately determine task-preferred inference routes at the end of training iterations. Extensive experimental results on CIFAR, ImageNet, and NYUv2 illustrate that DR-MGF outperforms the existing de-conflict methods both in prediction accuracy and convergence speed of MONs. Furthermore, DR-MGF can be extended to general MONs without modifying the overall network structures.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2116959668",
                        "name": "Yi Sun"
                    },
                    {
                        "authorId": "2152774720",
                        "name": "Xin Xu"
                    },
                    {
                        "authorId": "48514961",
                        "name": "J. Li"
                    },
                    {
                        "authorId": "2109697827",
                        "name": "Xiaochang Hu"
                    },
                    {
                        "authorId": "49593503",
                        "name": "Yifei Shi"
                    },
                    {
                        "authorId": "40092507",
                        "name": "L. Zeng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is common to initialize sparse subnetworks \u03b8s randomly based on the uniform [40; 5] or nonuniform layer-wise sparsity ratios with Erd\u0151s-R\u00e9nyi (ER) graph [39; 10; 35; 31].",
                "Static sparse training determines the structure of the sparse network at the initial stage of training by using certain pre-defined layer-wise sparsity ratios [38; 39; 10; 33].",
                "We follow the widely-used sparse training framework used in [39; 10].",
                "RigL [10] and SNFS [5] propose to uses gradient information to grow weights.",
                "However, with the popularity of RigL [10], it is common to use a fixed set of layer-wise sparsities.",
                ", magnitude) of the nonzero weights into consideration due to the crucial role of magnitude to dynamic sparse training [39; 10; 35].",
                "Dynamic sparse training (DST) [39; 10; 35], on the contrary, jointly optimizes the weights and sparse patterns during training, usually delivering better performance than the static ones.",
                "Method Channel Sparsity 10% 20% 30% Standard RigL [10] 76.",
                ", pruning r proportion of the least important parameters based on their magnitude, and immediately grow the same number of parameters randomly [39] or using the potential gradient [10].",
                "Taking the most representative DST approaches SET [39] and RigL [10] as examples, we measure the number of the Sparse Amenable Channels across layers in Figure 2, with v equals 0%, 20%, 30%, and 40%."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "28902dc5b6dc0289962c8b88a04ffe03503388ea",
                "externalIds": {
                    "ArXiv": "2305.19454",
                    "DBLP": "journals/corr/abs-2305-19454",
                    "DOI": "10.48550/arXiv.2305.19454",
                    "CorpusId": 258987797
                },
                "corpusId": 258987797,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/28902dc5b6dc0289962c8b88a04ffe03503388ea",
                "title": "Dynamic Sparsity Is Channel-Level Sparsity Learner",
                "abstract": "Sparse training has received an upsurging interest in machine learning due to its tantalizing saving potential for the entire training process as well as inference. Dynamic sparse training (DST), as a leading sparse training approach, can train deep neural networks at high sparsity from scratch to match the performance of their dense counterparts. However, most if not all DST prior arts demonstrate their effectiveness on unstructured sparsity with highly irregular sparse patterns, which receives limited support in common hardware. This limitation hinders the usage of DST in practice. In this paper, we propose Channel-aware dynamic sparse (Chase), which for the first time seamlessly translates the promise of unstructured dynamic sparsity to GPU-friendly channel-level sparsity (not fine-grained N:M or group sparsity) during one end-to-end training process, without any ad-hoc operations. The resulting small sparse networks can be directly accelerated by commodity hardware, without using any particularly sparsity-aware hardware accelerators. This appealing outcome is partially motivated by a hidden phenomenon of dynamic sparsity: off-the-shelf unstructured DST implicitly involves biased parameter reallocation across channels, with a large fraction of channels (up to 60\\%) being sparser than others. By progressively identifying and removing these channels during training, our approach translates unstructured sparsity to channel-wise sparsity. Our experimental results demonstrate that Chase achieves 1.7 X inference throughput speedup on common GPU devices without compromising accuracy with ResNet-50 on ImageNet. We release our codes in https://github.com/luuyin/chase.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1410465360",
                        "name": "Lu Yin"
                    },
                    {
                        "authorId": "2218586784",
                        "name": "Gen Li"
                    },
                    {
                        "authorId": "2218527981",
                        "name": "Meng Fang"
                    },
                    {
                        "authorId": "2202890812",
                        "name": "Lijuan Shen"
                    },
                    {
                        "authorId": "8242939",
                        "name": "Tianjin Huang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "49917515",
                        "name": "V. Menkovski"
                    },
                    {
                        "authorId": "151480882",
                        "name": "Xiaolong Ma"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[8] investigates DST for BERT language modeling tasks and shows Pareto improvement\nover the dense model in terms of FLOPs.",
                "We report the theoretical FLOPs to be independent of the used hardware, as it is done in the unstructured pruning literature [31, 10].",
                "1 Performance Comparison with Pruning and Sparse Training Algorithms We compare PALS with a standard during-training pruning approach (GMP [68]), GraNet [31], and a well-known sparse training method (RigL [10]).",
                "While DST and GMP use fixed sparsification policies (fixed sparsity level (Stable in Figure 1) and constantly prune the network until the desired sparsity level is reached (Shrink in Figure 1), respectively) and require the final sparsity level before training, PALS exploits heuristic information from the network at each iteration to automatically determine whether to increase,\ndecrease, or keep the sparsity level at each connectivity update step.",
                "Method Shrink Stable Expand Adaptive Sparsity Schedule Automatically tuning sparsity level RigL [10] \u2717 \u2713 \u2717 \u2717 \u2717 GMP [68] \u2713 \u2717 \u2717 \u2717 \u2717 GraNet [31] \u2713 \u2713 \u2717 \u2717 \u2717 PALS (ours) \u2713 \u2713 \u2713 \u2713 \u2713 In this work, we take advantage of the successful mechanism of \u201cShrink\u201d from during-training pruning (such as GraNet [31]) and \u201cStable\u201d from DST (such as RigL [10]) and propose for the first time the \u201cExpand\u201d mechanism, to design a method to automatically optimize the sparsity level during training without requiring to determine it beforehand.",
                "[10] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen.",
                "It prunes the weights (as performed in GMP) while allowing for connection regeneration (as seen in Dynamic Sparse Training (DST) which will be explained in the following).",
                "GraNet gradually shrinks a network (here, we start from a dense network) during the training to reach a pre-determined sparsity level, while allowing for connection regeneration inspired by DST. GraNet algorithm is described in Appendix B.",
                "PALS GraNet*[31] RigL*[10] GMP*[68] Dataset loss sparsity epochs loss sparsity epochs loss sparsity epochs loss sparsity epochs Electricity 0.",
                "The sparse topology can remain fixed (static) [38] or dynamically optimized during training (a.k.a Dynamic Sparse Training (DST)) [39, 10, 33, 20, 1, 61, 47].",
                "In this work, we take advantage of the successful mechanism of \u201cShrink\u201d from during-training pruning (such as GraNet [31]) and \u201cStable\u201d from DST (such as RigL [10]) and propose for the first time the \u201cExpand\u201d mechanism, to design a method to automatically optimize the sparsity level during training without\nrequiring to determine it beforehand.",
                "PALS is in essence inspired by the DST framework [39] and gradual magnitude pruning (GMP) [68, 31].",
                "PALS leverages the effective strategies of \"Shrink\" from during-training pruning (e.g., GraNet) and \"Stable\" from DST (e.g., SET, RigL).",
                "Figure 1: Schematic overview of the proposed method, PALS (Algorithm 1), Dynamic Sparse Training (DST) [39, 10], During-training pruning (Gradual Magnitude Pruning (GMP) [68], and GraNet [31]).",
                "The growth criteria can be random, as in the Sparse Evolutionary Training (SET) algorithm [39], or based on the gradient, as in the Rigged Lottery (RigL) algorithm [10]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "60e9ee92833ac232a99462a8202562759894ec43",
                "externalIds": {
                    "ArXiv": "2305.18382",
                    "DBLP": "journals/corr/abs-2305-18382",
                    "DOI": "10.48550/arXiv.2305.18382",
                    "CorpusId": 258967419
                },
                "corpusId": 258967419,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/60e9ee92833ac232a99462a8202562759894ec43",
                "title": "Adaptive Sparsity Level during Training for Efficient Time Series Forecasting with Transformers",
                "abstract": "Efficient time series forecasting has become critical for real-world applications, particularly with deep neural networks (DNNs). Efficiency in DNNs can be achieved through sparse connectivity and reducing the model size. However, finding the sparsity level automatically during training remains a challenging task due to the heterogeneity in the loss-sparsity tradeoffs across the datasets. In this paper, we propose \\enquote{\\textbf{P}runing with \\textbf{A}daptive \\textbf{S}parsity \\textbf{L}evel} (\\textbf{PALS}), to automatically seek an optimal balance between loss and sparsity, all without the need for a predefined sparsity level. PALS draws inspiration from both sparse training and during-training methods. It introduces the novel\"expand\"mechanism in training sparse neural networks, allowing the model to dynamically shrink, expand, or remain stable to find a proper sparsity level. In this paper, we focus on achieving efficiency in transformers known for their excellent time series forecasting performance but high computational cost. Nevertheless, PALS can be applied directly to any DNN. In the scope of these arguments, we demonstrate its effectiveness also on the DLinear model. Experimental results on six benchmark datasets and five state-of-the-art transformer variants show that PALS substantially reduces model size while maintaining comparable performance to the dense model. More interestingly, PALS even outperforms the dense model, in 12 and 14 cases out of 30 cases in terms of MSE and MAE loss, respectively, while reducing 65% parameter count and 63% FLOPs on average. Our code will be publicly available upon acceptance of the paper.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1768130522",
                        "name": "Zahra Atashgahi"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "2154952601",
                        "name": "Raymond N. J. Veldhuis"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, unstructured pruned models also have this advantages as it can lead to reduced computations for some hardware while maintaining a low number of parameters [19].",
                "Such methods can improve efficiency in parameters for both training and inference [19].",
                "Specifically, it has been shown that the winning ticket can be transferred between datasets and tasks [19, 47, 13, 57].",
                "Other works demonstrated the generality of the winning ticket and showed that a single pruned network can be transferred across datasets and achieve good performance after fine-tuning [19, 47, 13] and even that LT can be used for non-natural datasets [57]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "f71da88502c3b1ad8ac285ab0be15b423413e44c",
                "externalIds": {
                    "ArXiv": "2305.17559",
                    "DBLP": "journals/corr/abs-2305-17559",
                    "DOI": "10.48550/arXiv.2305.17559",
                    "CorpusId": 258959362
                },
                "corpusId": 258959362,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f71da88502c3b1ad8ac285ab0be15b423413e44c",
                "title": "Pruning at Initialization - A Sketching Perspective",
                "abstract": "The lottery ticket hypothesis (LTH) has increased attention to pruning neural networks at initialization. We study this problem in the linear setting. We show that finding a sparse mask at initialization is equivalent to the sketching problem introduced for efficient matrix multiplication. This gives us tools to analyze the LTH problem and gain insights into it. Specifically, using the mask found at initialization, we bound the approximation error of the pruned linear model at the end of training. We theoretically justify previous empirical evidence that the search for sparse networks may be data independent. By using the sketching perspective, we suggest a generic improvement to existing algorithms for pruning at initialization, which we show to be beneficial in the data-independent case.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2051500472",
                        "name": "Noga Bar"
                    },
                    {
                        "authorId": "2711839",
                        "name": "R. Giryes"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[14] Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich.",
                "A third family of algorithms, called Dynamic Sparse Training (DST) techniques, aims at modifying the structure of the sparse network during the training process [18, 12, 34, 45, 14, 30].",
                "Their ability to improve performance over random pruning algorithms has been discussed in [39], which shows how even small perturbations of random pruning (Layer-wise Random Pruning such as ER [12] and ERK [14]) are able to outperform well-engineered PaI algorithms.",
                "The experimental setup is based on the ones used in [45, 14, 32].",
                "98 on MobileNet-V2, that is known to be difficult to prune due to its separable convolutions layers [54, 14].",
                "Algorithm Layer-wise Sparsity Uniform [54] s \u2200 l \u2208 [0, N) ER [12] 1 \u2212 n l\u22121+nl nl\u22121\u00d7nl ERK [14] 1 \u2212 n l\u22121+nl+wl+hl nl\u22121\u00d7nl\u00d7wl\u00d7hl Table 4: Dynamic Sparse Training Pruning Algorithms.",
                "based on random growth [12, 34], momentum [45], or absolute gradients [14].",
                "Algorithm Layer-wise Sparsity\nUniform [54] sl \u2200 l \u2208 [0, N) ER [12] 1 \u2212 n l\u22121+nl\nnl\u22121\u00d7nl\nERK [14] 1 \u2212 n l\u22121+nl+wl+hl\nnl\u22121\u00d7nl\u00d7wl\u00d7hl\nSince the graph size of the proposed GE is based on the size of the input data, we selected three datasets with the same data sizes, namely CIFAR-10, CIFAR-100 [28], and the downscaled Tiny-ImageNet (of size 32 \u00d7 32 pixels) [11].",
                "DSR [34] |\u03b8| random random (zero layers) Uniform SNFS [45] |\u03b8| momentum momentum Uniform RigL [14] |\u03b8| gradient \u2717 ERK SET-ITOP [32] |\u03b8| random \u2717 ERK",
                "This model has been benchmarked in [14, 27, 7].",
                "Sources are the following:\n\u2022 SNIP https://github.com/mil-ad/snip \u2022 GraSP https://github.com/alecwangcq/GraSP \u2022 SynFlow https://github.com/ganguli-lab/Synaptic-Flow \u2022 ProsPR https://github.com/mil-ad/prospr \u2022 DSR and SNFS https://github.com/TimDettmers/sparse_learning \u2022 ITOP https://github.com/Shiweiliuiiiiiii/In-Time-Over-Parameterization \u2022 RigL https://github.com/nollied/rigl-torch9\n\u2022 Uniform, ER and ERK https://github.com/VITA-Group/Random_Pruning"
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "e546849b6e9b2e10a8dab0d82766b8453b4b55ee",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-16886",
                    "ArXiv": "2305.16886",
                    "DOI": "10.48550/arXiv.2305.16886",
                    "CorpusId": 258947759
                },
                "corpusId": 258947759,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e546849b6e9b2e10a8dab0d82766b8453b4b55ee",
                "title": "Peeking inside Sparse Neural Networks using Multi-Partite Graph Representations",
                "abstract": "Modern Deep Neural Networks (DNNs) have achieved very high performance at the expense of computational resources. To decrease the computational burden, several techniques have proposed to extract, from a given DNN, efficient subnetworks which are able to preserve performance while reducing the number of network parameters. The literature provides a broad set of techniques to discover such subnetworks, but few works have studied the peculiar topologies of such pruned architectures. In this paper, we propose a novel \\emph{unrolled input-aware} bipartite Graph Encoding (GE) that is able to generate, for each layer in an either sparse or dense neural network, its corresponding graph representation based on its relation with the input data. We also extend it into a multipartite GE, to capture the relation between layers. Then, we leverage on topological properties to study the difference between the existing pruning algorithms and algorithm categories, as well as the relation between topologies and performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2162450849",
                        "name": "Elia Cunegatti"
                    },
                    {
                        "authorId": "14392067",
                        "name": "Doina Bucur"
                    },
                    {
                        "authorId": "1986638",
                        "name": "Giovanni Iacca"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This includes one-shot pruning methods (SNIP [Lee et al., 2018]; GraSP [Wang et al., 2020]; SynFlow [Tanaka et al., 2020]), dense-to-sparse training with dynamic masks (DST [Liu et al., 2020]; RigL [Evci et al., 2020]), and SAM-optimized IMP [Na et al., 2022].",
                "Evci et al. [2020] proposes a rigged lottery (RigL) that regrows the dead weights yet with a large gradient flow.",
                "As IMP provides so-called sparse-to-sparse training, i.e., the mask is pre-defined, some work investigates dense-to-sparse training with dynamic masks [Evci et al., 2020, Liu et al., 2020].",
                ", 2020]; RigL [Evci et al., 2020]), and SAM-optimized IMP [Na et al.",
                "It has been demonstrated empirically that RigL can escape local minima and discover improved basins during optimization process."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "feeb1fc53c3d52083041697ccaeb800bbe42162b",
                "externalIds": {
                    "ArXiv": "2305.14852",
                    "DBLP": "journals/corr/abs-2305-14852",
                    "DOI": "10.48550/arXiv.2305.14852",
                    "CorpusId": 258865885
                },
                "corpusId": 258865885,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/feeb1fc53c3d52083041697ccaeb800bbe42162b",
                "title": "SWAMP: Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning",
                "abstract": "Given the ever-increasing size of modern neural networks, the significance of sparse architectures has surged due to their accelerated inference speeds and minimal memory demands. When it comes to global pruning techniques, Iterative Magnitude Pruning (IMP) still stands as a state-of-the-art algorithm despite its simple nature, particularly in extremely sparse regimes. In light of the recent finding that the two successive matching IMP solutions are linearly connected without a loss barrier, we propose Sparse Weight Averaging with Multiple Particles (SWAMP), a straightforward modification of IMP that achieves performance comparable to an ensemble of two IMP solutions. For every iteration, we concurrently train multiple sparse models, referred to as particles, using different batch orders yet the same matching ticket, and then weight average such models to produce a single mask. We demonstrate that our method consistently outperforms existing baselines across different sparsities through extensive experiments on various data and neural network structures.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2187600505",
                        "name": "Moonseok Choi"
                    },
                    {
                        "authorId": "2110211216",
                        "name": "Hyungi Lee"
                    },
                    {
                        "authorId": "2065197138",
                        "name": "G. Nam"
                    },
                    {
                        "authorId": "2124954802",
                        "name": "Juho Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To generate more efficient models, researchers have explored various techniques for compressing DNNs without sacrificing their accuracy, such as pruning [15, 16, 17, 18, 19, 20], quantization [21, 22, 23, 24], knowledge distillation [25, 26, 27], and matrix low-rank approximation [28, 29, 30, 31]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1a98126ae57a6b9b0b5c3d761f930b5198ce846a",
                "externalIds": {
                    "ArXiv": "2305.14403",
                    "DBLP": "journals/corr/abs-2305-14403",
                    "DOI": "10.48550/arXiv.2305.14403",
                    "CorpusId": 258865654
                },
                "corpusId": 258865654,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1a98126ae57a6b9b0b5c3d761f930b5198ce846a",
                "title": "Layer-adaptive Structured Pruning Guided by Latency",
                "abstract": "Structured pruning can simplify network architecture and improve inference speed. Combined with the underlying hardware and inference engine in which the final model is deployed, better results can be obtained by using latency collaborative loss function to guide network pruning together. Existing pruning methods that optimize latency have demonstrated leading performance, however, they often overlook the hardware features and connection in the network. To address this problem, we propose a global importance score SP-LAMP(Structured Pruning Layer-Adaptive Magnitude-based Pruning) by deriving a global importance score LAMP from unstructured pruning to structured pruning. In SP-LAMP, each layer includes a filter with an SP-LAMP score of 1, and the remaining filters are grouped. We utilize a group knapsack solver to maximize the SP-LAMP score under latency constraints. In addition, we improve the strategy of collect the latency to make it more accurate. In particular, for ResNet50/ResNet18 on ImageNet and CIFAR10, SP-LAMP is 1.28x/8.45x faster with +1.7%/-1.57% top-1 accuracy changed, respectively. Experimental results in ResNet56 on CIFAR10 demonstrate that our algorithm achieves lower latency compared to alternative approaches while ensuring accuracy and FLOPs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1993548897",
                        "name": "Siyuan Pan"
                    },
                    {
                        "authorId": "2201632539",
                        "name": "Linna Zhang"
                    },
                    {
                        "authorId": "2155863122",
                        "name": "Jie Zhang"
                    },
                    {
                        "authorId": "2108698638",
                        "name": "Xiaoshuang Li"
                    },
                    {
                        "authorId": "2055765400",
                        "name": "Liang Hou"
                    },
                    {
                        "authorId": "2158817631",
                        "name": "Xiaobing Tu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0861910d3541e69cda794b98208c689dff4f0bc7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-13538",
                    "ArXiv": "2305.13538",
                    "DOI": "10.1109/tste.2023.3274735",
                    "CorpusId": 258653375
                },
                "corpusId": 258653375,
                "publicationVenue": {
                    "id": "72c05908-68f2-4360-8c60-6e518d8edc3d",
                    "name": "IEEE Transactions on Sustainable Energy",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Sustain Energy"
                    ],
                    "issn": "1949-3029",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5165391",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5165391"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0861910d3541e69cda794b98208c689dff4f0bc7",
                "title": "Encoding Carbon Emission Flow in Energy Management: A Compact Constraint Learning Approach",
                "abstract": "Decarbonizing the energy supply is essential and urgent to mitigate the increasingly visible climate change. Its basis is identifying emission responsibility during power allocation by the carbon emission flow (CEF) model. However, the main challenge of CEF application is the intractable nonlinear relationship between carbon emission and power allocation. So this paper leverages the high approximation capability and the mixed-integer linear programming (MILP) representability of the deep neural networks to tackle the complex CEF model in carbon-electricity coordinated optimization. The compact constraint learning approach is proposed to learn the mapping from power injection to bus emission with sparse neural networks (SNNs). Then the trained SNNs are transformed equivalently as MILP constraints in the downstream optimization. In light of the ``high emission with high price'' principle, the blocked carbon price mechanism is designed to price emissions from the demand side. Based on the constraint learning and mechanism design, this paper proposes the carbon-aware energy management model in the tractable MILP form to unlock the carbon reduction potential from the demand side. The case study verifies the approximation accuracy and sparsity of SNN with fewer parameters for accelerating optimization solution and reduction effectiveness of demand-side capability for mitigating emission.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "121302581",
                        "name": "Linwei Sang"
                    },
                    {
                        "authorId": "33528828",
                        "name": "Yinliang Xu"
                    },
                    {
                        "authorId": "2174730064",
                        "name": "Hongbin Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, Mostafa and Wang (2019); Mocanu et al. (2018); Evci et al. (2020) proposed to leverage information gathered during the training process to dynamically update the sparsity pattern of kernels."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1319103d2fd61f17605e2c0e515585f59fdb4eba",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-10964",
                    "ArXiv": "2305.10964",
                    "DOI": "10.48550/arXiv.2305.10964",
                    "CorpusId": 258762701
                },
                "corpusId": 258762701,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1319103d2fd61f17605e2c0e515585f59fdb4eba",
                "title": "Learning Activation Functions for Sparse Neural Networks",
                "abstract": "Sparse Neural Networks (SNNs) can potentially demonstrate similar performance to their dense counterparts while saving significant energy and memory at inference. However, the accuracy drop incurred by SNNs, especially at high pruning ratios, can be an issue in critical deployment conditions. While recent works mitigate this issue through sophisticated pruning techniques, we shift our focus to an overlooked factor: hyperparameters and activation functions. Our analyses have shown that the accuracy drop can additionally be attributed to (i) Using ReLU as the default choice for activation functions unanimously, and (ii) Fine-tuning SNNs with the same hyperparameters as dense counterparts. Thus, we focus on learning a novel way to tune activation functions for sparse networks and combining these with a separate hyperparameter optimization (HPO) regime for sparse networks. By conducting experiments on popular DNN models (LeNet-5, VGG-16, ResNet-18, and EfficientNet-B0) trained on MNIST, CIFAR-10, and ImageNet-16 datasets, we show that the novel combination of these two approaches, dubbed Sparse Activation Function Search, short: SAFS, results in up to 15.53%, 8.88%, and 6.33% absolute improvement in the accuracy for LeNet-5, VGG-16, and ResNet-18 over the default training protocols, especially at high pruning ratios. Our code can be found at https://github.com/automl/SAFS",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51228426",
                        "name": "Mohammad Loni"
                    },
                    {
                        "authorId": "2064332188",
                        "name": "Aditya Mohan"
                    },
                    {
                        "authorId": "2202473458",
                        "name": "Mehdi Asadi"
                    },
                    {
                        "authorId": "145963266",
                        "name": "M. Lindauer"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c9052f3ccd02c3a841c1ca9ebda6b24dfd847762",
                "externalIds": {
                    "ArXiv": "2305.11203",
                    "DBLP": "journals/corr/abs-2305-11203",
                    "DOI": "10.48550/arXiv.2305.11203",
                    "CorpusId": 258823372
                },
                "corpusId": 258823372,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c9052f3ccd02c3a841c1ca9ebda6b24dfd847762",
                "title": "PDP: Parameter-free Differentiable Pruning is All You Need",
                "abstract": "DNN pruning is a popular way to reduce the size of a model, improve the inference latency, and minimize the power consumption on DNN accelerators. However, existing approaches might be too complex, expensive or ineffective to apply to a variety of vision/language tasks, DNN architectures and to honor structured pruning constraints. In this paper, we propose an efficient yet effective train-time pruning scheme, Parameter-free Differentiable Pruning (PDP), which offers state-of-the-art qualities in model size, accuracy, and training cost. PDP uses a dynamic function of weights during training to generate soft pruning masks for the weights in a parameter-free manner for a given pruning target. While differentiable, the simplicity and efficiency of PDP make it universal enough to deliver state-of-the-art random/structured/channel pruning results on various vision and natural language tasks. For example, for MobileNet-v1, PDP can achieve 68.2% top-1 ImageNet1k accuracy at 86.6% sparsity, which is 1.7% higher accuracy than those from the state-of-the-art algorithms. Also, PDP yields over 83.1% accuracy on Multi-Genre Natural Language Inference with 90% sparsity for BERT, while the next best from the existing techniques shows 81.5% accuracy. In addition, PDP can be applied to structured pruning, such as N:M pruning and channel pruning. For 1:4 structured pruning of ResNet18, PDP improved the top-1 ImageNet1k accuracy by over 3.6% over the state-of-the-art. For channel pruning of ResNet50, PDP reduced the top-1 ImageNet1k accuracy by 0.6% from the state-of-the-art.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1801812",
                        "name": "Minsik Cho"
                    },
                    {
                        "authorId": "145073156",
                        "name": "Saurabh N. Adya"
                    },
                    {
                        "authorId": "100925254",
                        "name": "D. Naik"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "bc9f777afde9e903c7cf918b6d4b1b0ac11673e7",
                "externalIds": {
                    "ArXiv": "2305.03395",
                    "DBLP": "journals/corr/abs-2305-03395",
                    "DOI": "10.48550/arXiv.2305.03395",
                    "CorpusId": 258546818
                },
                "corpusId": 258546818,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bc9f777afde9e903c7cf918b6d4b1b0ac11673e7",
                "title": "Sparsifying Bayesian neural networks with latent binary variables and normalizing flows",
                "abstract": "Artificial neural networks (ANNs) are powerful machine learning methods used in many modern applications such as facial recognition, machine translation, and cancer diagnostics. A common issue with ANNs is that they usually have millions or billions of trainable parameters, and therefore tend to overfit to the training data. This is especially problematic in applications where it is important to have reliable uncertainty estimates. Bayesian neural networks (BNN) can improve on this, since they incorporate parameter uncertainty. In addition, latent binary Bayesian neural networks (LBBNN) also take into account structural uncertainty by allowing the weights to be turned on or off, enabling inference in the joint space of weights and structures. In this paper, we will consider two extensions to the LBBNN method: Firstly, by using the local reparametrization trick (LRT) to sample the hidden units directly, we get a more computationally efficient algorithm. More importantly, by using normalizing flows on the variational posterior distribution of the LBBNN parameters, the network learns a more flexible variational posterior distribution than the mean field Gaussian. Experimental results show that this improves predictive power compared to the LBBNN method, while also obtaining more sparse networks. We perform two simulation studies. In the first study, we consider variable selection in a logistic regression setting, where the more flexible variational distribution leads to improved results. In the second study, we compare predictive uncertainty based on data generated from two-dimensional Gaussian distributions. Here, we argue that our Bayesian methods lead to more realistic estimates of predictive uncertainty.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2216488625",
                        "name": "Lars Skaaret-Lund"
                    },
                    {
                        "authorId": "1926674",
                        "name": "G. Storvik"
                    },
                    {
                        "authorId": "51116121",
                        "name": "A. Hubin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, researchers have explored sparsifying NNs during training (Evci et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a06a4a38668c4737ab2ce80badc177ea3f520456",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-02538",
                    "ArXiv": "2305.02538",
                    "DOI": "10.48550/arXiv.2305.02538",
                    "CorpusId": 258480187
                },
                "corpusId": 258480187,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a06a4a38668c4737ab2ce80badc177ea3f520456",
                "title": "Cuttlefish: Low-Rank Model Training without All the Tuning",
                "abstract": "Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacrificing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing Cuttlefish, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. Cuttlefish leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. Cuttlefish switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that Cuttlefish generates models up to 5.6 times smaller than full-rank models, and attains up to a 1.2 times faster end-to-end training process while preserving comparable accuracy. Moreover, Cuttlefish outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109798334",
                        "name": "Hongyi Wang"
                    },
                    {
                        "authorId": "2216074086",
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "authorId": "150266516",
                        "name": "Pongsakorn U-chupala"
                    },
                    {
                        "authorId": "2112517704",
                        "name": "Yoshiki Tanaka"
                    },
                    {
                        "authorId": "2064963077",
                        "name": "Eric P. Xing"
                    },
                    {
                        "authorId": "1740595",
                        "name": "Dimitris Papailiopoulos"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The ERK distribution has been demonstrated to outperform uniform sparsity distributions by reallocating parameters to layers with fewer parameters (Mocanu et al., 2018; Evci et al., 2021).",
                "We propose a novel DST method, Structured RigL (SRigL), based on RigL (Evci et al., 2021).",
                "We achieve the desired overall sparsity by distributing the per-layer sparsity according to the Erd\u0151s-R\u00e9nyi-Kernel (ERK) (Evci et al., 2021; Mocanu et al., 2018) distribution, which scales the per-layer sparsity based on the number of neurons and the dimensions of the convolutional kernel, if present.",
                "Dynamic Sparse Training (DST) methods such as RigL (Evci et al., 2021) are the state-of-the-art in sparse training methods, learning unstructured Sparse Neural Networks (SNNs) with 85\u201395% fewer weights than dense models, while maintaining similar generalization.",
                ", 2018) removes weights with the smallest magnitude and adds weights randomly; similarly, RigL (Evci et al., 2021) prunes weights with the smallest magnitude and regrows weights that have large-magnitude gradients.",
                "\u2022 With the constant fan-in constraint, per-layer sparsity distributions such as Erd\u02ddos-R\u00e9nyi-Kernel (ERK) can be applied to the model.",
                "We achieve the desired overall sparsity by distributing the per-layer sparsity according to the ERK (Evci et al., 2021; Mocanu et al., 2018) distribution, which scales the per-layer sparsity based on the number of neurons and the dimensions of the convolutional kernel, if present."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "0f322ac4f9f12b11fe5d042508497148a84fe8a5",
                "externalIds": {
                    "ArXiv": "2305.02299",
                    "DBLP": "journals/corr/abs-2305-02299",
                    "DOI": "10.48550/arXiv.2305.02299",
                    "CorpusId": 258461498
                },
                "corpusId": 258461498,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0f322ac4f9f12b11fe5d042508497148a84fe8a5",
                "title": "Dynamic Sparse Training with Structured Sparsity",
                "abstract": "Dynamic Sparse Training (DST) methods achieve state-of-the-art results in sparse neural network training, matching the generalization of dense models while enabling sparse training and inference. Although the resulting models are highly sparse and theoretically less computationally expensive, achieving speedups with unstructured sparsity on real-world hardware is challenging. In this work, we propose a sparse-to-sparse DST method, Structured RigL (SRigL), to learn a variant of fine-grained structured N:M sparsity by imposing a constant fan-in constraint. Using our empirical analysis of existing DST methods at high sparsity, we additionally employ a neuron ablation method which enables SRigL to achieve state-of-the-art sparse-to-sparse structured DST performance on a variety of Neural Network (NN) architectures. We demonstrate reduced real-world timings on CPU for online inference -- 3.6x/2x faster at 90% sparsity than equivalent dense/unstructured sparse layers, respectively. Our source code is available at https://github.com/calgaryml/condensed-sparsity",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2146696514",
                        "name": "Mike Lasby"
                    },
                    {
                        "authorId": "143999820",
                        "name": "A. Golubeva"
                    },
                    {
                        "authorId": "3399348",
                        "name": "Utku Evci"
                    },
                    {
                        "authorId": "31589393",
                        "name": "M. Nica"
                    },
                    {
                        "authorId": "3067465",
                        "name": "Yani Andrew Ioannou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The new trend of dynamic sparse training shows that any random initialized sparse neural networks can achieves comparable accuracy to the dense neural networks (Ye et al., 2020; Evci et al., 2020; Hou et al., 2022; Ma et al., 2021a; Yuan et al., 2021)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8cb97eb1acb9db3e683ab531bdf7b81e68a0cd46",
                "externalIds": {
                    "ArXiv": "2305.02190",
                    "DBLP": "conf/iclr/Hui0MK23",
                    "DOI": "10.48550/arXiv.2305.02190",
                    "CorpusId": 258461067
                },
                "corpusId": 258461067,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8cb97eb1acb9db3e683ab531bdf7b81e68a0cd46",
                "title": "Rethinking Graph Lottery Tickets: Graph Sparsity Matters",
                "abstract": "Lottery Ticket Hypothesis (LTH) claims the existence of a winning ticket (i.e., a properly pruned sub-network together with original weight initialization) that can achieve competitive performance to the original dense network. A recent work, called UGS, extended LTH to prune graph neural networks (GNNs) for effectively accelerating GNN inference. UGS simultaneously prunes the graph adjacency matrix and the model weights using the same masking mechanism, but since the roles of the graph adjacency matrix and the weight matrices are very different, we find that their sparsifications lead to different performance characteristics. Specifically, we find that the performance of a sparsified GNN degrades significantly when the graph sparsity goes beyond a certain extent. Therefore, we propose two techniques to improve GNN performance when the graph sparsity is high. First, UGS prunes the adjacency matrix using a loss formulation which, however, does not properly involve all elements of the adjacency matrix; in contrast, we add a new auxiliary loss head to better guide the edge pruning by involving the entire adjacency matrix. Second, by regarding unfavorable graph sparsification as adversarial data perturbations, we formulate the pruning process as a min-max optimization problem to gain the robustness of lottery tickets when the graph sparsity is high. We further investigate the question: Can the\"retrainable\"winning ticket of a GNN be also effective for graph transferring learning? We call it the transferable graph lottery ticket (GLT) hypothesis. Extensive experiments were conducted which demonstrate the superiority of our proposed sparsification method over UGS, and which empirically verified our transferable GLT hypothesis.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2064059011",
                        "name": "Bo Hui"
                    },
                    {
                        "authorId": "2149274549",
                        "name": "Da Yan"
                    },
                    {
                        "authorId": "2184257253",
                        "name": "Xiaolong Ma"
                    },
                    {
                        "authorId": "1758909",
                        "name": "Wei-Shinn Ku"
                    }
                ]
            }
        },
        {
            "contexts": [
                "1) Experiment Setup: To answer the RQ3, we select 4 state-of-the-art model compression techniques proposed in the last two years (i.e., LAMP [30], Global [31], Uniform+ [32] and ERK [33]) to see whether our DeepArc framework can speed up the model compression efficiency.",
                ", LAMP [30], Global [31], Uniform+ [32] and ERK [33]) to see whether our DeepArc framework can speed up the model compression efficiency."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "47f9e5501758763ecda7bbb7b98b0f66054ef5f0",
                "externalIds": {
                    "DBLP": "conf/icse/RenLXLSFD23",
                    "DOI": "10.1109/ICSE48619.2023.00092",
                    "CorpusId": 259860518
                },
                "corpusId": 259860518,
                "publicationVenue": {
                    "id": "a36dc29e-4ea1-4567-b0fe-1c06daf8bee8",
                    "name": "International Conference on Software Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Softw Eng",
                        "ICSE"
                    ],
                    "url": "http://www.icse-conferences.org/"
                },
                "url": "https://www.semanticscholar.org/paper/47f9e5501758763ecda7bbb7b98b0f66054ef5f0",
                "title": "DeepArc: Modularizing Neural Networks for the Model Maintenance",
                "abstract": "Neural networks are an emerging data-driven programming paradigm widely used in many areas. Unlike traditional software systems consisting of decomposable modules, a neural network is usually delivered as a monolithic package, raising challenges for some maintenance tasks such as model restructure and re-adaption. In this work, we propose DeepArc, a novel modularization method for neural networks, to reduce the cost of model maintenance tasks. Specifically, DeepArc decomposes a neural network into several consecutive modules, each of which encapsulates consecutive layers with similar semantics. The network modularization facilitates practical tasks such as refactoring the model to preserve existing features (e.g., model compression) and enhancing the model with new features (e.g., fitting new samples). The modularization and encapsulation allow us to restructure or retrain the model by only pruning and tuning a few localized neurons and layers. Our experiments show that (1) DeepArc can boost the runtime efficiency of the state-of-the-art model compression techniques by 14.8%; (2) compared to the traditional model retraining, DeepArc only needs to train less than 20% of the neurons on average to fit adversarial samples and repair under-performing models, leading to 32.85% faster training performance while achieving similar model prediction performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2217983250",
                        "name": "Xiaoning Ren"
                    },
                    {
                        "authorId": "47904366",
                        "name": "Yun Lin"
                    },
                    {
                        "authorId": "2367687",
                        "name": "Yinxing Xue"
                    },
                    {
                        "authorId": "2143183668",
                        "name": "Ruofan Liu"
                    },
                    {
                        "authorId": "2155020757",
                        "name": "Jun Sun"
                    },
                    {
                        "authorId": "2113908670",
                        "name": "Zhiyong Feng"
                    },
                    {
                        "authorId": "2152487387",
                        "name": "J. Dong"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018) and gradient based (RIGL) (Evci et al., 2020) growth.",
                "We use ERK sparsity distribution (Mocanu et al., 2018; Evci et al., 2020) in our ViT experiments.",
                "Sparse Training including static sparse training (STATIC) and dynamic sparse training with random (SET) (Mocanu et al., 2018) and gradient based (RIGL) (Evci et al., 2020) growth.",
                "Though most results match previous work, we observe a significant improvement for the accuracy achieved by the SET algorithm compared to the implementation done in (Evci et al., 2020).",
                "We train 80% sparse ResNet-50 models on ImageNet to reproduce previous results reported in the literature (Gale et al., 2019; Evci et al., 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "5aade9012e952fee96935a710d2eccd059f96337",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-14082",
                    "ArXiv": "2304.14082",
                    "DOI": "10.48550/arXiv.2304.14082",
                    "CorpusId": 258352400
                },
                "corpusId": 258352400,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5aade9012e952fee96935a710d2eccd059f96337",
                "title": "JaxPruner: A concise library for sparsity research",
                "abstract": "This paper introduces JaxPruner, an open-source JAX-based pruning and sparse training library for machine learning research. JaxPruner aims to accelerate research on sparse neural networks by providing concise implementations of popular pruning and sparse training algorithms with minimal memory and latency overhead. Algorithms implemented in JaxPruner use a common API and work seamlessly with the popular optimization library Optax, which, in turn, enables easy integration with existing JAX based libraries. We demonstrate this ease of integration by providing examples in four different codebases: Scenic, t5x, Dopamine and FedJAX and provide baseline experiments on popular benchmarks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2146048352",
                        "name": "Jooyoung Lee"
                    },
                    {
                        "authorId": "107950764",
                        "name": "Wonpyo Park"
                    },
                    {
                        "authorId": "1664853300",
                        "name": "Nicole Mitchell"
                    },
                    {
                        "authorId": "104354626",
                        "name": "Jonathan Pilault"
                    },
                    {
                        "authorId": "1403752231",
                        "name": "Johan S. Obando-Ceron"
                    },
                    {
                        "authorId": "2188978862",
                        "name": "Han-Byul Kim"
                    },
                    {
                        "authorId": "2702448",
                        "name": "Namhoon Lee"
                    },
                    {
                        "authorId": "1502248377",
                        "name": "Elias Frantar"
                    },
                    {
                        "authorId": "2215478121",
                        "name": "Yun Long"
                    },
                    {
                        "authorId": "2112229",
                        "name": "A. Yazdanbakhsh"
                    },
                    {
                        "authorId": "3504647",
                        "name": "Shivani Agrawal"
                    },
                    {
                        "authorId": "1929462",
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "authorId": "2153688851",
                        "name": "Xin Wang"
                    },
                    {
                        "authorId": "27057088",
                        "name": "Sheng-Chun Kao"
                    },
                    {
                        "authorId": "2215521125",
                        "name": "Xingyao Zhang"
                    },
                    {
                        "authorId": "2066558041",
                        "name": "Trevor Gale"
                    },
                    {
                        "authorId": "144211012",
                        "name": "Aart J. C. Bik"
                    },
                    {
                        "authorId": "2215449616",
                        "name": "Woohyun Han"
                    },
                    {
                        "authorId": "2215480045",
                        "name": "Milen Ferev"
                    },
                    {
                        "authorId": "2215471465",
                        "name": "Zhonglin Han"
                    },
                    {
                        "authorId": "2110136833",
                        "name": "Hong-Seok Kim"
                    },
                    {
                        "authorId": "2921469",
                        "name": "Yann Dauphin"
                    },
                    {
                        "authorId": "2215482536",
                        "name": "Karolina Dziugaite"
                    },
                    {
                        "authorId": "39163115",
                        "name": "P. S. Castro"
                    },
                    {
                        "authorId": "3399348",
                        "name": "Utku Evci"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Using GMP and an extended training schedule, we are able to obtain sparse models that match or outperform the dense baseline, both in terms of accuracy and ROC-AUC values, even at high (\u2265 99%) sparsities, while providing substantial improvements in theoretical FLOPs (computed as in [14]), and practical inference speed on CPU when using the DeepSparse inference engine [10]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0898134e11fe1a04aea9e65de77b92497cea97e1",
                "externalIds": {
                    "ArXiv": "2304.12622",
                    "DBLP": "journals/corr/abs-2304-12622",
                    "DOI": "10.1109/CVPR52729.2023.02334",
                    "CorpusId": 258309395
                },
                "corpusId": 258309395,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0898134e11fe1a04aea9e65de77b92497cea97e1",
                "title": "Bias in Pruned Vision Models: In-Depth Analysis and Countermeasures",
                "abstract": "Pruning\u2014that is, setting a significant subset of the parameters of a neural network to zero\u2014is one of the most popular methods of model compression. Yet, several recent works have raised the issue that pruning may induce or exacerbate bias in the output of the compressed model. Despite existing evidence for this phenomenon, the relationship between neural network pruning and induced bias is not well-understood. In this work, we systematically investigate and characterize this phenomenon in Convolutional Neural Networks for computer vision. First, we show that it is in fact possible to obtain highly-sparse models, e.g. with less than 10% remaining weights, which do not decrease in accuracy nor substantially increase in bias when compared to dense models. At the same time, we also find that, at higher sparsities, pruned models exhibit higher uncertainty in their outputs, as well as increased correlations, which we directly link to increased bias. We propose easy-to-use criteria which, based only on the uncompressed model, establish whether bias will increase with pruning, and identify the samples most susceptible to biased predictions post-compression. Our code can be found at https://github.com/IST-DASLab/pruned-vision-model-bias.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2082370867",
                        "name": "Eugenia Iofinova"
                    },
                    {
                        "authorId": "3341722",
                        "name": "Alexandra Peste"
                    },
                    {
                        "authorId": "3311387",
                        "name": "Dan Alistarh"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", SET [20], RigL [22]) on SNN models (i.",
                "For the latter, we implement the sparse training methods (i.e., SET [20], RigL [22]) on SNN models (i.e., SET-SNN, RigL-SNN).",
                "Compared to RigL-SNN, NDSNN has up to 5.45% and 17.83% increase in accuracy at a sparsity of 99% for VGG-16 and ResNet-19, respectively.",
                "While for ResNet-19, NDSNN has 15.42%, 14.17%, 23.88% and 18.15% increase in accuracy (that is relatively 28.2%, 14.17%, 23.38%, 18.15% higher accuracy) compared to LTH-SNN, obtains 1.96%, 4.30%, 7.99%, 10.5% higher accuracy than SET-SNN and achieves 2.75%, 3.72%, 8.52%, 11.65% higher accuracy than RigL-SNN at a sparsity of 90%, 95%, 98% and 99%, respectively."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "669dcb3371441154b48c811cbec7d8e4b65b9df3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-12214",
                    "ArXiv": "2304.12214",
                    "DOI": "10.1109/DAC56929.2023.10247810",
                    "CorpusId": 258298898
                },
                "corpusId": 258298898,
                "publicationVenue": {
                    "id": "021b37d3-cef1-4c12-a442-257f7900c23d",
                    "name": "Design Automation Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Des Autom Conf",
                        "DAC"
                    ],
                    "url": "http://www.dac.com/"
                },
                "url": "https://www.semanticscholar.org/paper/669dcb3371441154b48c811cbec7d8e4b65b9df3",
                "title": "Neurogenesis Dynamics-inspired Spiking Neural Network Training Acceleration",
                "abstract": "Biologically inspired Spiking Neural Networks (SNNs) have attracted significant attention for their ability to provide extremely energy-efficient machine intelligence through event-driven operation and sparse activities. As artificial intelligence (AI) becomes ever more democratized, there is an increasing need to execute SNN models on edge devices. Existing works adopt weight pruning to reduce SNN model size and accelerate inference. However, these methods mainly focus on how to obtain a sparse model for efficient inference, rather than training efficiency. To overcome these drawbacks, in this paper, we propose a Neurogenesis Dynamics-inspired Spiking Neural Network training acceleration framework, NDSNN. Our framework is computational efficient and trains a model from scratch with dynamic sparsity without sacrificing model fidelity. Specifically, we design a new drop-and-grow strategy with decreasing number of non-zero weights, to maintain extreme high sparsity and high accuracy. We evaluate NDSNN using VGG-16 and ResNet-19 on CIFAR-10, CIFAR-100 and TinyImageNet. Experimental results show that NDSNN achieves up to 20.52% improvement in accuracy on Tiny-ImageNet using ResNet-19 (with a sparsity of 99%) as compared to other SOTA methods (e.g., Lottery Ticket Hypothesis (LTH), SET-SNN, RigL-SNN). In addition, the training cost of NDSNN is only 40.89% of the LTH training cost on ResNet-19 and 31.35% of the LTH training cost on VGG-16 on CIFAR-10.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2122804649",
                        "name": "Shaoyi Huang"
                    },
                    {
                        "authorId": "122851204",
                        "name": "Haowen Fang"
                    },
                    {
                        "authorId": "145460048",
                        "name": "Kaleel Mahmood"
                    },
                    {
                        "authorId": "2144399315",
                        "name": "Bowen Lei"
                    },
                    {
                        "authorId": "2072805812",
                        "name": "Nuo Xu"
                    },
                    {
                        "authorId": "2053235014",
                        "name": "Bin Lei"
                    },
                    {
                        "authorId": "2215228805",
                        "name": "Yue Sun"
                    },
                    {
                        "authorId": "2116459424",
                        "name": "Dongkuan Xu"
                    },
                    {
                        "authorId": "35420329",
                        "name": "Wujie Wen"
                    },
                    {
                        "authorId": "2881873",
                        "name": "Caiwen Ding"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "afd591d017738a487670476f08cb375929942ebc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-10720",
                    "ArXiv": "2304.10720",
                    "DOI": "10.1109/TSTE.2023.3268140",
                    "CorpusId": 258226310
                },
                "corpusId": 258226310,
                "publicationVenue": {
                    "id": "72c05908-68f2-4360-8c60-6e518d8edc3d",
                    "name": "IEEE Transactions on Sustainable Energy",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Sustain Energy"
                    ],
                    "issn": "1949-3029",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5165391",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5165391"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/afd591d017738a487670476f08cb375929942ebc",
                "title": "Conservative Sparse Neural Network Embedded Frequency-Constrained Unit Commitment With Distributed Energy Resources",
                "abstract": "The increasing penetration of distributed energy resources (DERs) will decrease the rotational inertia of the power system and further degrade the system frequency stability. To address the above issues, this article leverages the advanced neural network (NN) to learn the frequency dynamics and incorporates NN to facilitate system reliable operation. This article proposes the conservative sparse neural network (CSNN) embedded frequency-constrained unit commitment (FCUC) with converter-based DERs, including the learning and optimization stages. In the learning stage, it samples the inertia parameters, calculates the corresponding frequency, and characterizes the stability region of the sampled parameters using the convex hulls to ensure stability and avoid extrapolation. For conservativeness, the positive prediction error penalty is added to the loss function to prevent possible frequency requirement violation. For the sparsity, the NN topology pruning is employed to eliminate unnecessary connections for solving acceleration. In the optimization stage, the trained CSNN is transformed into mixed-integer linear constraints using the big-M method and then incorporated to establish the data-enhanced model. The case study verifies 1) the effectiveness of the proposed model in terms of high accuracy, fewer parameters, and significant solving acceleration; 2) the stable system operation against frequency violation under contingency.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "121302581",
                        "name": "Linwei Sang"
                    },
                    {
                        "authorId": "33528828",
                        "name": "Yinliang Xu"
                    },
                    {
                        "authorId": "30678885",
                        "name": "Zhongkai Yi"
                    },
                    {
                        "authorId": "2118367965",
                        "name": "Lun Yang"
                    },
                    {
                        "authorId": "39007850",
                        "name": "Huan Long"
                    },
                    {
                        "authorId": "2174730064",
                        "name": "Hongbin Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such a formulation is favorable because: (i) allowing the growth of connections has been shown to yield better sparse networks (Evci et al., 2020); and (ii) recovering weights to previous values is compatible with the finding that tuning pruned networks with earlier weight values or learning rate\u2026",
                "Such a formulation is favorable because: (i) allowing the growth of connections has been shown to yield better sparse networks (Evci et al., 2020); and (ii) recovering weights to previous values is compatible with the finding that tuning pruned networks with earlier weight values or learning rate schedule is beneficial (Renda et al."
            ],
            "isInfluential": false,
            "intents": [
                "result"
            ],
            "citingPaper": {
                "paperId": "8c82901a48ce2e1a3ed39a2fad85fa98dd81ade2",
                "externalIds": {
                    "ArXiv": "2304.11237",
                    "DBLP": "journals/corr/abs-2304-11237",
                    "DOI": "10.48550/arXiv.2304.11237",
                    "CorpusId": 258298360
                },
                "corpusId": 258298360,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8c82901a48ce2e1a3ed39a2fad85fa98dd81ade2",
                "title": "Effective Neural Network L0 Regularization With BinMask",
                "abstract": "$L_0$ regularization of neural networks is a fundamental problem. In addition to regularizing models for better generalizability, $L_0$ regularization also applies to selecting input features and training sparse neural networks. There is a large body of research on related topics, some with quite complicated methods. In this paper, we show that a straightforward formulation, BinMask, which multiplies weights with deterministic binary masks and uses the identity straight-through estimator for backpropagation, is an effective $L_0$ regularizer. We evaluate BinMask on three tasks: feature selection, network sparsification, and model regularization. Despite its simplicity, BinMask achieves competitive performance on all the benchmarks without task-specific tuning compared to methods designed for each task. Our results suggest that decoupling weights from mask optimization, which has been widely adopted by previous work, is a key component for effective $L_0$ regularization.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "49104216",
                        "name": "Kai Jia"
                    },
                    {
                        "authorId": "1720971",
                        "name": "M. Rinard"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In particular Li et al. (2020a) implemented randomly initialized sparse mask, FedDST Bibikar et al. (2022) built on the idea of RigL Evci et al. (2020) and mostly focussed on magnitude pruning on the server-side resulting in similar constraints."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "af96cf1737a167bfd442d4d23cb94cf6b7281453",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-07488",
                    "ArXiv": "2304.07488",
                    "DOI": "10.48550/arXiv.2304.07488",
                    "CorpusId": 258179154
                },
                "corpusId": 258179154,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/af96cf1737a167bfd442d4d23cb94cf6b7281453",
                "title": "SalientGrads: Sparse Models for Communication Efficient and Data Aware Distributed Federated Training",
                "abstract": "Federated learning (FL) enables the training of a model leveraging decentralized data in client sites while preserving privacy by not collecting data. However, one of the significant challenges of FL is limited computation and low communication bandwidth in resource limited edge client nodes. To address this, several solutions have been proposed in recent times including transmitting sparse models and learning dynamic masks iteratively, among others. However, many of these methods rely on transmitting the model weights throughout the entire training process as they are based on ad-hoc or random pruning criteria. In this work, we propose Salient Grads, which simplifies the process of sparse training by choosing a data aware subnetwork before training, based on the model-parameter's saliency scores, which is calculated from the local client data. Moreover only highly sparse gradients are transmitted between the server and client models during the training process unlike most methods that rely on sharing the entire dense model in each round. We also demonstrate the efficacy of our method in a real world federated learning application and report improvement in wall-clock communication time.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "31048326",
                        "name": "Riyasat Ohib"
                    },
                    {
                        "authorId": "150941167",
                        "name": "Bishal Thapaliya"
                    },
                    {
                        "authorId": "2214582716",
                        "name": "Pratyush Gaggenapalli"
                    },
                    {
                        "authorId": "46701354",
                        "name": "J. Liu"
                    },
                    {
                        "authorId": "2133976600",
                        "name": "Vince D. Calhoun"
                    },
                    {
                        "authorId": "32611384",
                        "name": "S. Plis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(2015); Guo et al. (2016)). These methods are computationally more expensive than dense training and are useful only for FLOPS and memory reduction in inference. Pruning before Training: Such methods are inspired by the \u2018Lottery Ticket Hypothesis\u2019 (Frankle & Carbin (2019)), which tries to find a sparse weight mask (subnetwork) that can be trained from scratch to match the quality of dense trained model. However, they use an iterative pruning scheme that is repeated for several full training passes to find the mask. SNIP (Lee et al. (2019)) preserves the loss after pruning based on connection sensitivity. GraSP (Wang et al. (2020)) prunes connection such that it preserves the network\u2019s gradient flow. SynFlow (Tanaka et al. (2020)) uses iterative synaptic flow pruning to preserve the total flow of synaptic strength (global pruning at initialization with no data and no backpropagation). 3SP (van Amersfoort et al. (2020)) introduces compute-aware pruning of channels (structured pruning).",
                "Pruning after Training: Here sparse models are created using a three-stage pipeline\u2013 train a dense model, prune, and re-train (Han et al. (2015); Guo et al.",
                "(2015); Guo et al. (2016)). These methods are computationally more expensive than dense training and are useful only for FLOPS and memory reduction in inference. Pruning before Training: Such methods are inspired by the \u2018Lottery Ticket Hypothesis\u2019 (Frankle & Carbin (2019)), which tries to find a sparse weight mask (subnetwork) that can be trained from scratch to match the quality of dense trained model. However, they use an iterative pruning scheme that is repeated for several full training passes to find the mask. SNIP (Lee et al. (2019)) preserves the loss after pruning based on connection sensitivity. GraSP (Wang et al. (2020)) prunes connection such that it preserves the network\u2019s gradient flow. SynFlow (Tanaka et al. (2020)) uses iterative synaptic flow pruning to preserve the total flow of synaptic strength (global pruning at initialization with no data and no backpropagation).",
                "(2015); Guo et al. (2016)). These methods are computationally more expensive than dense training and are useful only for FLOPS and memory reduction in inference. Pruning before Training: Such methods are inspired by the \u2018Lottery Ticket Hypothesis\u2019 (Frankle & Carbin (2019)), which tries to find a sparse weight mask (subnetwork) that can be trained from scratch to match the quality of dense trained model. However, they use an iterative pruning scheme that is repeated for several full training passes to find the mask. SNIP (Lee et al. (2019)) preserves the loss after pruning based on connection sensitivity. GraSP (Wang et al. (2020)) prunes connection such that it preserves the network\u2019s gradient flow. SynFlow (Tanaka et al. (2020)) uses iterative synaptic flow pruning to preserve the total flow of synaptic strength (global pruning at initialization with no data and no backpropagation). 3SP (van Amersfoort et al. (2020)) introduces compute-aware pruning of channels (structured pruning). The main advantage of these methods is that they are compute and memory efficient in every iteration. However, they often suffer from significant loss of accuracy. Note that, the weight pruning mask is fixed throughout the training. Pruning during Training: For these methods, the weight pruning mask evolves dynamically with training. Methods of this category can belong to either sparse-to-sparse or dense-to-sparse training. In sparse-to-sparse training, we have a sparse model to start with (based on sparsity budget) and the budget sparsity is maintained throughout the training. SET (Mocanu et al. (2018)) pioneered this approach where they replaced a fraction of least magnitude weights by random weights for better exploration. DSR (Mostafa & Wang (2019)) allowed sparsity budget to be non-uniform across layers heuristically, e.",
                "(2015); Guo et al. (2016)). These methods are computationally more expensive than dense training and are useful only for FLOPS and memory reduction in inference. Pruning before Training: Such methods are inspired by the \u2018Lottery Ticket Hypothesis\u2019 (Frankle & Carbin (2019)), which tries to find a sparse weight mask (subnetwork) that can be trained from scratch to match the quality of dense trained model. However, they use an iterative pruning scheme that is repeated for several full training passes to find the mask. SNIP (Lee et al. (2019)) preserves the loss after pruning based on connection sensitivity. GraSP (Wang et al. (2020)) prunes connection such that it preserves the network\u2019s gradient flow. SynFlow (Tanaka et al. (2020)) uses iterative synaptic flow pruning to preserve the total flow of synaptic strength (global pruning at initialization with no data and no backpropagation). 3SP (van Amersfoort et al. (2020)) introduces compute-aware pruning of channels (structured pruning). The main advantage of these methods is that they are compute and memory efficient in every iteration. However, they often suffer from significant loss of accuracy. Note that, the weight pruning mask is fixed throughout the training. Pruning during Training: For these methods, the weight pruning mask evolves dynamically with training. Methods of this category can belong to either sparse-to-sparse or dense-to-sparse training. In sparse-to-sparse training, we have a sparse model to start with (based on sparsity budget) and the budget sparsity is maintained throughout the training. SET (Mocanu et al. (2018)) pioneered this approach where they replaced a fraction of least magnitude weights by random weights for better exploration.",
                "(2015); Guo et al. (2016)). These methods are computationally more expensive than dense training and are useful only for FLOPS and memory reduction in inference. Pruning before Training: Such methods are inspired by the \u2018Lottery Ticket Hypothesis\u2019 (Frankle & Carbin (2019)), which tries to find a sparse weight mask (subnetwork) that can be trained from scratch to match the quality of dense trained model. However, they use an iterative pruning scheme that is repeated for several full training passes to find the mask. SNIP (Lee et al. (2019)) preserves the loss after pruning based on connection sensitivity.",
                "(2015); Guo et al. (2016)). These methods are computationally more expensive than dense training and are useful only for FLOPS and memory reduction in inference. Pruning before Training: Such methods are inspired by the \u2018Lottery Ticket Hypothesis\u2019 (Frankle & Carbin (2019)), which tries to find a sparse weight mask (subnetwork) that can be trained from scratch to match the quality of dense trained model. However, they use an iterative pruning scheme that is repeated for several full training passes to find the mask. SNIP (Lee et al. (2019)) preserves the loss after pruning based on connection sensitivity. GraSP (Wang et al. (2020)) prunes connection such that it preserves the network\u2019s gradient flow. SynFlow (Tanaka et al. (2020)) uses iterative synaptic flow pruning to preserve the total flow of synaptic strength (global pruning at initialization with no data and no backpropagation). 3SP (van Amersfoort et al. (2020)) introduces compute-aware pruning of channels (structured pruning). The main advantage of these methods is that they are compute and memory efficient in every iteration. However, they often suffer from significant loss of accuracy. Note that, the weight pruning mask is fixed throughout the training. Pruning during Training: For these methods, the weight pruning mask evolves dynamically with training. Methods of this category can belong to either sparse-to-sparse or dense-to-sparse training. In sparse-to-sparse training, we have a sparse model to start with (based on sparsity budget) and the budget sparsity is maintained throughout the training. SET (Mocanu et al. (2018)) pioneered this approach where they replaced a fraction of least magnitude weights by random weights for better exploration. DSR (Mostafa & Wang (2019)) allowed sparsity budget to be non-uniform across layers heuristically, e.g., higher sparsity for later layers. SNFS (Dettmers & Zettlemoyer (2019)) proposed to use momentum of each parameter as a criterion to grow the weights leading to increased accuracy.",
                "(2015); Guo et al. (2016)). These methods are computationally more expensive than dense training and are useful only for FLOPS and memory reduction in inference. Pruning before Training: Such methods are inspired by the \u2018Lottery Ticket Hypothesis\u2019 (Frankle & Carbin (2019)), which tries to find a sparse weight mask (subnetwork) that can be trained from scratch to match the quality of dense trained model. However, they use an iterative pruning scheme that is repeated for several full training passes to find the mask. SNIP (Lee et al. (2019)) preserves the loss after pruning based on connection sensitivity. GraSP (Wang et al. (2020)) prunes connection such that it preserves the network\u2019s gradient flow.",
                "(2015); Guo et al. (2016)). These methods are computationally more expensive than dense training and are useful only for FLOPS and memory reduction in inference. Pruning before Training: Such methods are inspired by the \u2018Lottery Ticket Hypothesis\u2019 (Frankle & Carbin (2019)), which tries to find a sparse weight mask (subnetwork) that can be trained from scratch to match the quality of dense trained model.",
                "RigL (Evci et al. (2021)) activates/revives new weights ranked according to gradient magnitude (using infrequent full gradient calculation), i.e., masked weights receive gradients only after certain iterations."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7579570fff7ac460d63b46e5b6966148a68dc82d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-06941",
                    "ArXiv": "2304.06941",
                    "DOI": "10.48550/arXiv.2304.06941",
                    "CorpusId": 258170268
                },
                "corpusId": 258170268,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7579570fff7ac460d63b46e5b6966148a68dc82d",
                "title": "AUTOSPARSE: Towards Automated Sparse Training of Deep Neural Networks",
                "abstract": "Sparse training is emerging as a promising avenue for reducing the computational cost of training neural networks. Several recent studies have proposed pruning methods using learnable thresholds to efficiently explore the non-uniform distribution of sparsity inherent within the models. In this paper, we propose Gradient Annealing (GA), where gradients of masked weights are scaled down in a non-linear manner. GA provides an elegant trade-off between sparsity and accuracy without the need for additional sparsity-inducing regularization. We integrated GA with the latest learnable pruning methods to create an automated sparse training algorithm called AutoSparse, which achieves better accuracy and/or training/inference FLOPS reduction than existing learnable pruning methods for sparse ResNet50 and MobileNetV1 on ImageNet-1K: AutoSparse achieves (2x, 7x) reduction in (training,inference) FLOPS for ResNet50 on ImageNet at 80% sparsity. Finally, AutoSparse outperforms sparse-to-sparse SotA method MEST (uniform sparsity) for 80% sparse ResNet50 with similar accuracy, where MEST uses 12% more training FLOPS and 50% more inference FLOPS.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40100744",
                        "name": "Abhisek Kundu"
                    },
                    {
                        "authorId": "8792111",
                        "name": "Naveen Mellempudi"
                    },
                    {
                        "authorId": "3376608",
                        "name": "Dharma Teja Vooturi"
                    },
                    {
                        "authorId": "40279588",
                        "name": "Bharat Kaul"
                    },
                    {
                        "authorId": "145126868",
                        "name": "P. Dubey"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Network pruning is studied rigorously in literature for single-task models and there exist many pruning criteria based on weight magnitude [11, 26], connection sensitivity [10, 25], and even learning-based pruning methods [7, 19]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "911c653f6537582d0ab19248eee9945c41668466",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-06840",
                    "ArXiv": "2304.06840",
                    "DOI": "10.48550/arXiv.2304.06840",
                    "CorpusId": 258170358
                },
                "corpusId": 258170358,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/911c653f6537582d0ab19248eee9945c41668466",
                "title": "Structured Pruning for Multi-Task Deep Neural Networks",
                "abstract": "Although multi-task deep neural network (DNN) models have computation and storage benefits over individual single-task DNN models, they can be further optimized via model compression. Numerous structured pruning methods are already developed that can readily achieve speedups in single-task models, but the pruning of multi-task networks has not yet been extensively studied. In this work, we investigate the effectiveness of structured pruning on multi-task models. We use an existing single-task filter pruning criterion and also introduce an MTL-based filter pruning criterion for estimating the filter importance scores. We prune the model using an iterative pruning strategy with both pruning methods. We show that, with careful hyper-parameter tuning, architectures obtained from different pruning methods do not have significant differences in their performances across tasks when the number of parameters is similar. We also show that iterative structure pruning may not be the best way to achieve a well-performing pruned model because, at extreme pruning levels, there is a high drop in performance across all tasks. But when the same models are randomly initialized and re-trained, they show better results.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2295877",
                        "name": "Siddhant Garg"
                    },
                    {
                        "authorId": "2108877321",
                        "name": "Lijun Zhang"
                    },
                    {
                        "authorId": "2055337890",
                        "name": "Hui Guan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DST methods (Mocanu et al., 2018; Bellec et al., 2017; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Liu et al., 2021b; Evci et al., 2020; Liu et al., 2021c;a) train the neural network with a fixed parameter count budget while gradually adjust the parameters throughout training."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b6da4e11e24da4e863bbc1c5c7bd6080d0906b98",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-02840",
                    "ArXiv": "2304.02840",
                    "DOI": "10.48550/arXiv.2304.02840",
                    "CorpusId": 257985378
                },
                "corpusId": 257985378,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b6da4e11e24da4e863bbc1c5c7bd6080d0906b98",
                "title": "NTK-SAP: Improving neural network pruning by aligning training dynamics",
                "abstract": "Pruning neural networks before training has received increasing interest due to its potential to reduce training time and memory. One popular method is to prune the connections based on a certain metric, but it is not entirely clear what metric is the best choice. Recent advances in neural tangent kernel (NTK) theory suggest that the training dynamics of large enough neural networks is closely related to the spectrum of the NTK. Motivated by this finding, we propose to prune the connections that have the least influence on the spectrum of the NTK. This method can help maintain the NTK spectrum, which may help align the training dynamics to that of its dense counterpart. However, one possible issue is that the fixed-weight-NTK corresponding to a given initial point can be very different from the NTK corresponding to later iterates during the training phase. We further propose to sample multiple realizations of random weights to estimate the NTK spectrum. Note that our approach is weight-agnostic, which is different from most existing methods that are weight-dependent. In addition, we use random inputs to compute the fixed-weight-NTK, making our method data-agnostic as well. We name our foresight pruning algorithm Neural Tangent Kernel Spectrum-Aware Pruning (NTK-SAP). Empirically, our method achieves better performance than all baselines on multiple datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2159213654",
                        "name": "Yite Wang"
                    },
                    {
                        "authorId": "49620929",
                        "name": "Dawei Li"
                    },
                    {
                        "authorId": "2068169846",
                        "name": "Ruoyu Sun"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c9072c9e0150dc676ca5d580f332f54d20f28a55",
                "externalIds": {
                    "DBLP": "conf/cvpr/Singh23",
                    "ArXiv": "2304.02186",
                    "DOI": "10.1109/CVPRW59228.2023.00016",
                    "CorpusId": 257952202
                },
                "corpusId": 257952202,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c9072c9e0150dc676ca5d580f332f54d20f28a55",
                "title": "Training Strategies for Vision Transformers for Object Detection",
                "abstract": "Vision-based Transformer have shown huge application in the perception module of autonomous driving in terms of predicting accurate 3D bounding boxes, owing to their strong capability in modeling long-range dependencies between the visual features. However Transformers, initially designed for language models, have mostly focused on the performance accuracy, and not so much on the inference-time budget. For a safety critical system like autonomous driving, real-time inference at the on-board compute is an absolute necessity. This keeps our object detection algorithm under a very tight run-time budget. In this paper, we evaluated a variety of strategies to optimize on the inference-time of vision transformers based object detection methods keeping a close-watch on any performance variations. Our chosen metric for these strategies is accuracy-runtime joint optimization. Moreover, for actual inference-time analysis we profile our strategies with float32 and float16 precision with TensorRT module. This is the most common format used by the industry for deployment of their Machine Learning networks on the edge devices. We showed that our strategies are able to improve inference-time by 63% at the cost of performance drop of mere 3% for our problem-statement defined in Sec. 3. These strategies brings down Vision Transformers detectors [3], [15], [18], [19], [36] inference-time even less than traditional single-image based CNN detectors like FCOS [17], [25], [33]. We recommend practitioners use these techniques to deploy Transformers based hefty multi-view networks on a budge-constrained robotic platform.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2155174824",
                        "name": "Apoorv Singh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Gale et al. (2020) designed high-performance SpMM kernels targeted specifically at deep learning applications. Using SpMM as the fundamental component, pruning has been used to optimally sparsify the dense weight matrix in deep neural networks. Zhu & Gupta (2017) pruned the large models in an unstructured way and compared the accuracy of the models with their small-dense counterparts across a broad range of neural network architectures (CNN, LSTM etc.) and found that large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters. Yao et al. (2019) showed a novel fine-grained unstructured sparsity approach and corresponding pruning method to fixing the number of non-zeros in small regions of the sparse matrix in matrix row. Wang (2020) presented SparseRT, a system to support efficient inference with unstructured weight pruning on GPU.",
                "This is typically done through a single pruning step at the end of training (Zhu & Gupta, 2017) or potentially through some sparse training regime (Evci et al., 2019). These approaches have typically achieved between 90-99% reduction in model weights while maintaining and acceptable level of accuracy depending on the model and technique used (Hoefler et al., 2021). While the benefits to model size can be easily realised, increased computational efficiency can often be more difficult to achieve as sparse computation can be challenging to execute effectively on modern, highly parallel deep learning accelerators (Qin et al., 2022). As such, sparse training methods often fall into a gap between theoretical and practical efficiency. For example Mostafa & Wang (2019) demonstrated FLOP efficient deep residual CNN training with dynamic sparse reparameterisation techniques but struggled to achieve speed ups in practice.",
                "Gale et al. (2020) designed high-performance SpMM kernels targeted specifically at deep learning applications. Using SpMM as the fundamental component, pruning has been used to optimally sparsify the dense weight matrix in deep neural networks. Zhu & Gupta (2017) pruned the large models in an unstructured way and compared the accuracy of the models with their small-dense counterparts across a broad range of neural network architectures (CNN, LSTM etc.) and found that large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters. Yao et al. (2019) showed a novel fine-grained unstructured sparsity approach and corresponding pruning method to fixing the number of non-zeros in small regions of the sparse matrix in matrix row.",
                "Gale et al. (2020) designed high-performance SpMM kernels targeted specifically at deep learning applications.",
                "Gale et al. (2020) designed high-performance SpMM kernels targeted specifically at deep learning applications. Using SpMM as the fundamental component, pruning has been used to optimally sparsify the dense weight matrix in deep neural networks. Zhu & Gupta (2017) pruned the large models in an unstructured way and compared the accuracy of the models with their small-dense counterparts across a broad range of neural network architectures (CNN, LSTM etc."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "512d4d0a856d6e553df9e124b3b939ee0e2a8797",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-16999",
                    "ArXiv": "2303.16999",
                    "DOI": "10.48550/arXiv.2303.16999",
                    "CorpusId": 257833740
                },
                "corpusId": 257833740,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/512d4d0a856d6e553df9e124b3b939ee0e2a8797",
                "title": "PopSparse: Accelerated block sparse matrix multiplication on IPU",
                "abstract": "Reducing the computational cost of running large scale neural networks using sparsity has attracted great attention in the deep learning community. While much success has been achieved in reducing FLOP and parameter counts while maintaining acceptable task performance, achieving actual speed improvements has typically been much more difficult, particularly on general purpose accelerators (GPAs) such as NVIDIA GPUs using low precision number formats. In this work we introduce PopSparse, a library that enables fast sparse operations on Graphcore IPUs by leveraging both the unique hardware characteristics of IPUs as well as any block structure defined in the data. We target two different types of sparsity: static, where the sparsity pattern is fixed at compile-time; and dynamic, where it can change each time the model is run. We present benchmark results for matrix multiplication for both of these modes on IPU with a range of block sizes, matrix sizes and densities. Results indicate that the PopSparse implementations are faster than dense matrix multiplications on IPU at a range of sparsity levels with large matrix size and block size. Furthermore, static sparsity in general outperforms dynamic sparsity. While previous work on GPAs has shown speedups only for very high sparsity (typically 99\\% and above), the present work demonstrates that our static sparse implementation outperforms equivalent dense calculations in FP16 at lower sparsity (around 90%). IPU code is available to view and run at ipu.dev/sparsity-benchmarks, GPU code will be made available shortly.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2193649370",
                        "name": "Zhiyi Li"
                    },
                    {
                        "authorId": "145474032",
                        "name": "Douglas Orr"
                    },
                    {
                        "authorId": "29445502",
                        "name": "V. Ohan"
                    },
                    {
                        "authorId": "115750048",
                        "name": "Godfrey Da Costa"
                    },
                    {
                        "authorId": "2057256524",
                        "name": "Tom Murray"
                    },
                    {
                        "authorId": "2182085134",
                        "name": "Adam Sanders"
                    },
                    {
                        "authorId": "1753625471",
                        "name": "D. Beker"
                    },
                    {
                        "authorId": "36709240",
                        "name": "Dominic Masters"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "464770587aece80cc9e3451050058e30c2aa6666",
                "externalIds": {
                    "ArXiv": "2303.14177",
                    "DBLP": "journals/corr/abs-2303-14177",
                    "DOI": "10.48550/arXiv.2303.14177",
                    "CorpusId": 257756896
                },
                "corpusId": 257756896,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/464770587aece80cc9e3451050058e30c2aa6666",
                "title": "Scaling Expert Language Models with Unsupervised Domain Discovery",
                "abstract": "Large language models are typically trained densely: all parameters are updated with respect to all inputs. This requires synchronization of billions of parameters across thousands of GPUs. We introduce a simple but effective method to asynchronously train large, sparse language models on arbitrary text corpora. Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference. This approach generalizes embarrassingly parallel training by automatically discovering the domains for each expert, and eliminates nearly all the communication overhead of existing sparse language models. Our technique outperforms dense baselines on multiple corpora and few-shot tasks, and our analysis shows that specializing experts to meaningful clusters is key to these gains. Performance also improves with the number of experts and size of training data, suggesting this is a highly efficient and accessible approach to training large language models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40895369",
                        "name": "Suchin Gururangan"
                    },
                    {
                        "authorId": "2118481100",
                        "name": "Margaret Li"
                    },
                    {
                        "authorId": "35084211",
                        "name": "M. Lewis"
                    },
                    {
                        "authorId": "3040379",
                        "name": "Weijia Shi"
                    },
                    {
                        "authorId": "1745524",
                        "name": "Tim Althoff"
                    },
                    {
                        "authorId": "144365875",
                        "name": "Noah A. Smith"
                    },
                    {
                        "authorId": "1982950",
                        "name": "Luke Zettlemoyer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Hence, layer-wise sparsity distributions such as the Erdo\u0308s-Re\u0301nyi-Kernel (Evci et al., 2020), Ideal Gas Quota (Chen et al., 2022b), and parameter leveling (Golubeva et al., 2021) are often used with sparse training to boost accuracies.",
                "For example, SOTA sparse training methods (Jayakumar et al., 2020; Evci et al., 2020) invest these FLOP savings into longer training schedules to close the accuracy gap and compensate for the inability to discover an optimal mask earlier in training.",
                "We adopt the default hyperparameters from RigL (Evci et al., 2020) for dynamic sparsity.",
                "In this work, we focus on improving the training efficiency (test-accuracy w.r.t training FLOPs) of DNNs.\nRecent works (Evci et al., 2020; Jayakumar et al., 2020) have explored using weight sparsity to reduce the FLOPs spent in training.",
                "Inspired by this result, various dynamic sparse training (DST) methods (Ma et al., 2022; Evci et al., 2020; Liu et al., 2021a; Jayakumar et al., 2020) attempt to find optimal sparse subnetworks in a single training run.",
                "As a result, these techniques can sometimes even require more FLOPs than training the dense model (Ma et al., 2022; Evci et al., 2020; Jayakumar et al., 2020).",
                "Recent works (Evci et al., 2020; Jayakumar et al., 2020) have explored using weight sparsity to reduce the FLOPs spent in training.",
                "These methods either try to find the subnetworks at initialization (Tanaka et al., 2020; Wang et al., 2020a; de Jorge et al., 2020; Lee et al., 2018) or dynamically during training (Mocanu et al., 2018; Evci et al., 2020; Jayakumar et al., 2020; Raihan & Aamodt, 2020).",
                ", 2018) or dynamically during training (Mocanu et al., 2018; Evci et al., 2020; Jayakumar et al., 2020; Raihan & Aamodt, 2020).",
                "%} using three sparse training methods: static sparsity, SET (Mocanu et al., 2018) and RigL (Evci et al., 2020).",
                "Sparsity Setup For enabling the SIFT transformations, we use the RigL (Evci et al., 2020) algorithm in its default hyperparameter settings (\u03b1 = 0.3,\u2206T = 100), with the drop-fraction (\u03b1) annealed using a cosine decay schedule for 75% of the training run.",
                "Hence, layer-wise sparsity distributions such as the Erd\u00f6s-R\u00e9nyi-Kernel (Evci et al., 2020), Ideal Gas Quota (Chen et al."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "9c5b457ca44c56bd9c57033ff28fbb614275cf04",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-11525",
                    "ArXiv": "2303.11525",
                    "DOI": "10.48550/arXiv.2303.11525",
                    "CorpusId": 257636503
                },
                "corpusId": 257636503,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9c5b457ca44c56bd9c57033ff28fbb614275cf04",
                "title": "Sparse Iso-FLOP Transformations for Maximizing Training Efficiency",
                "abstract": "Recent works have explored the use of weight sparsity to improve the training efficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs). These works aim to reduce training FLOPs but training with sparse weights often leads to accuracy loss or requires longer training schedules, making the resulting training efficiency less clear. In contrast, we focus on using sparsity to increase accuracy while using the same FLOPs as the dense model and show training efficiency gains through higher accuracy. In this work, we introduce Sparse-IFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in replacements for dense layers to improve their representational capacity and FLOP efficiency. Each transformation is parameterized by a single hyperparameter (sparsity level) and provides a larger search space to find optimal sparse masks. Without changing any training hyperparameters, replacing dense layers with Sparse-IFT leads to significant improvements across computer vision (CV) and natural language processing (NLP) tasks, including ResNet-18 on ImageNet (+3.5%) and GPT-3 Small on WikiText-103 (-0.4 PPL), both matching larger dense model variants that use 2x or more FLOPs. To our knowledge, this is the first work to demonstrate the use of sparsity for improving the accuracy of dense models via a simple-to-use set of sparse transformations. Code is available at: https://github.com/CerebrasResearch/Sparse-IFT.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46708564",
                        "name": "S. Saxena"
                    },
                    {
                        "authorId": "51153332",
                        "name": "Vithursan Thangarasa"
                    },
                    {
                        "authorId": "1922581610",
                        "name": "Abhay Gupta"
                    },
                    {
                        "authorId": "2212029838",
                        "name": "Sean Lie"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7d46073cce9f5ed8d931b804cfaf327201dd4e26",
                "externalIds": {
                    "ArXiv": "2303.10464",
                    "DBLP": "journals/corr/abs-2303-10464",
                    "DOI": "10.48550/arXiv.2303.10464",
                    "CorpusId": 257631823
                },
                "corpusId": 257631823,
                "publicationVenue": {
                    "id": "f9af8000-42f8-410d-a622-e8811e41660a",
                    "name": "Conference on Uncertainty in Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Uncertainty in Artificial Intelligence",
                        "UAI",
                        "Conf Uncertain Artif Intell",
                        "Uncertain Artif Intell"
                    ],
                    "url": "http://www.auai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7d46073cce9f5ed8d931b804cfaf327201dd4e26",
                "title": "SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models",
                "abstract": "The pre-training and fine-tuning paradigm has contributed to a number of breakthroughs in Natural Language Processing (NLP). Instead of directly training on a downstream task, language models are first pre-trained on large datasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.) and then fine-tuned on task-specific data (e.g., natural language generation, text summarization, etc.). Scaling the model and dataset size has helped improve the performance of LLMs, but unfortunately, this also lead to highly prohibitive computational costs. Pre-training LLMs often require orders of magnitude more FLOPs than fine-tuning and the model capacity often remains the same between the two phases. To achieve training efficiency w.r.t training FLOPs, we propose to decouple the model capacity between the two phases and introduce Sparse Pre-training and Dense Fine-tuning (SPDF). In this work, we show the benefits of using unstructured weight sparsity to train only a subset of weights during pre-training (Sparse Pre-training) and then recover the representational capacity by allowing the zeroed weights to learn (Dense Fine-tuning). We demonstrate that we can induce up to 75% sparsity into a 1.3B parameter GPT-3 XL model resulting in a 2.5x reduction in pre-training FLOPs, without a significant loss in accuracy on the downstream tasks relative to the dense baseline. By rigorously evaluating multiple downstream tasks, we also establish a relationship between sparsity, task complexity and dataset size. Our work presents a promising direction to train large GPT models at a fraction of the training FLOPs using weight sparsity, while retaining the benefits of pre-trained textual representations for downstream tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51153332",
                        "name": "Vithursan Thangarasa"
                    },
                    {
                        "authorId": "1922581610",
                        "name": "Abhay Gupta"
                    },
                    {
                        "authorId": "2058346405",
                        "name": "William Marshall"
                    },
                    {
                        "authorId": "6574899",
                        "name": "Tianda Li"
                    },
                    {
                        "authorId": "2212028628",
                        "name": "Kevin Leong"
                    },
                    {
                        "authorId": "1703049",
                        "name": "D. DeCoste"
                    },
                    {
                        "authorId": "2212029838",
                        "name": "Sean Lie"
                    },
                    {
                        "authorId": "46708564",
                        "name": "S. Saxena"
                    }
                ]
            }
        },
        {
            "contexts": [
                "During training, DST methods periodically update the sparse connectivity of the network; e.g., in Mocanu et al. (2018); Evci et al. (2020) authors remove a fraction \u03b6 of the parameters \u03b8s and add the same number of parameters to the network to keep the sparsity level fixed.",
                "Estimating the FLOPs (floating-point operations) and parameter count is a commonly used approach to analyze the efficiency gained by a sparse neural network compared to its dense equivalent network Evci et al. (2020); Sokar et al.",
                "We use gradients for weight regrowth Evci et al. (2020).",
                "The starting point of our implementation is based on the sparse evolutionary training introduced as SET in Mocanu et al. (2018)3 to which we added the gradient-based connections growth proposed in RigL Evci et al. (2020).",
                "However, there exists various approaches for weight regrowth including, random Mocanu et al. (2018); Mostafa & Wang (2019), gradientbased Evci et al. (2020); Dai et al. (2019); Dettmers & Zettlemoyer (2019); Jayakumar et al. (2020), localitybased Hoefler et al. (2021), and similarity-based\u2026",
                "Estimating the FLOPs (floating-point operations) and parameter count is a commonly used approach to analyze the efficiency gained by a sparse neural network compared to its dense equivalent network Evci et al. (2020); Sokar et al. (2021). Number of parameters indicates the size of the model, which directly affects the memory consumption and also computational complexity. FLOPs estimates the time complexity of an algorithm independently of its implementation. In addition, since existing deep learning hardware is not optimized for sparse matrix computations, most methods for obtaining sparse neural networks only simulate sparsity using a binary mask over the weights. Consequently, the running time of these methods does not reflect their efficiency. Besides, developing proper pure sparse implementations for sparse neural networks is currently a highly researched topic pursued by the community Hooker (2021). Thus, as our paper is, in its essence, theoretical, we decided to let this engineering research aspect for future work.",
                "It has been shown in Evci et al. (2020) that adding the zero-connections with the largest gradients magnitude in the DST process accelerates the learning and improves the accuracy.",
                "Estimating the FLOPs (floating-point operations) and parameter count is a commonly used approach to analyze the efficiency gained by a sparse neural network compared to its dense equivalent network Evci et al. (2020); Sokar et al. (2021). Number of parameters indicates the size of the model, which directly affects the memory consumption and also computational complexity.",
                "Estimating the FLOPs (floating-point operations) and parameter count is a commonly used approach to analyze the efficiency gained by a sparse neural network compared to its dense equivalent network Evci et al. (2020); Sokar et al. (2021).",
                "In this section, we compare NeuroFS with RigL Evci et al. (2020), which is a DST method mainly designed for classification; it uses gradient for weight regrowth when updating the sparse connectivity in the DST framework.",
                "We use gradients for weight regrowth Evci et al. (2020). For each hidden layer h(l), NeuroFS performs the following two steps: 1."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "42fc11d7a5d317bb467afa3b6f36dc5f3136b33f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-07200",
                    "ArXiv": "2303.07200",
                    "DOI": "10.48550/arXiv.2303.07200",
                    "CorpusId": 257496298
                },
                "corpusId": 257496298,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/42fc11d7a5d317bb467afa3b6f36dc5f3136b33f",
                "title": "Supervised Feature Selection with Neuron Evolution in Sparse Neural Networks",
                "abstract": "Feature selection that selects an informative subset of variables from data not only enhances the model interpretability and performance but also alleviates the resource demands. Recently, there has been growing attention on feature selection using neural networks. However, existing methods usually suffer from high computational costs when applied to high-dimensional datasets. In this paper, inspired by evolution processes, we propose a novel resource-efficient supervised feature selection method using sparse neural networks, named \\enquote{NeuroFS}. By gradually pruning the uninformative features from the input layer of a sparse neural network trained from scratch, NeuroFS derives an informative subset of features efficiently. By performing several experiments on $11$ low and high-dimensional real-world benchmarks of different types, we demonstrate that NeuroFS achieves the highest ranking-based score among the considered state-of-the-art supervised feature selection models. The code is available on GitHub.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1768130522",
                        "name": "Zahra Atashgahi"
                    },
                    {
                        "authorId": "47956877",
                        "name": "Xuhao Zhang"
                    },
                    {
                        "authorId": "2211430581",
                        "name": "Neil Kichler"
                    },
                    {
                        "authorId": "2131166985",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "1410465360",
                        "name": "Lu Yin"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "2154952601",
                        "name": "Raymond N. J. Veldhuis"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "At the same time, sparse training (Mocanu et al., 2018; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Liu et al., 2021c; Schwarz et al., 2021) was proposed that can train a randomlyinitialized sparse neural network from scratch while dynamically optimizing the sparse\u2026",
                "Following a mainstream convention in many sparse training papers (Frankle & Carbin, 2019; Gale et al., 2019; Evci et al., 2020; Lee et al., 2019; Liu et al., 2021c), we sparsify most layers in the model including embedding layers and classifier heads, and we do not apply advanced techniques such as\u2026",
                "Following a mainstream convention in many sparse training papers (Frankle & Carbin, 2019; Gale et al., 2019; Evci et al., 2020; Lee et al., 2019; Liu et al., 2021c), we sparsify most layers in the model including embedding layers and classifier heads, and we do not apply advanced techniques such as Iterative Learning Rate Rewinding (Renda et al.",
                "More importantly, oBERT produces a completely different layerwise sparsity pattern from magnitudebased pruning approaches, which is consistent with the patterns that are commonly observed in sparse computer vision models: deeper layers tend to have higher sparsities than lower layers (Evci et al., 2020; Kusupati et al., 2020; Tanaka et al., 2020; Liu et al., 2021b).",
                "\u2026pattern from magnitudebased pruning approaches, which is consistent with the patterns that are commonly observed in sparse computer vision models: deeper layers tend to have higher sparsities than lower layers (Evci et al., 2020; Kusupati et al., 2020; Tanaka et al., 2020; Liu et al., 2021b).",
                "At the same time, sparse training (Mocanu et al., 2018; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Liu et al., 2021c; Schwarz et al., 2021) was proposed that can train a randomlyinitialized sparse neural network from scratch while dynamically optimizing the sparse connectivity with promising performance.",
                "\u2022 Rigging the Lottery (RigL) (Evci et al., 2020) is a leading sparse training method that updates the topology of sparse neural networks during training via a prune-and-grow scheme."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "fdacdbc6a00eeb42efe7f81848b0bc09be5ca997",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-02141",
                    "ArXiv": "2303.02141",
                    "DOI": "10.48550/arXiv.2303.02141",
                    "CorpusId": 257353428
                },
                "corpusId": 257353428,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/fdacdbc6a00eeb42efe7f81848b0bc09be5ca997",
                "title": "Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!",
                "abstract": "Sparse Neural Networks (SNNs) have received voluminous attention predominantly due to growing computational and memory footprints of consistently exploding parameter count in large-scale models. Similar to their dense counterparts, recent SNNs generalize just as well and are equipped with numerous favorable benefits (e.g., low complexity, high scalability, and robustness), sometimes even better than the original dense networks. As research effort is focused on developing increasingly sophisticated sparse algorithms, it is startling that a comprehensive benchmark to evaluate the effectiveness of these algorithms has been highly overlooked. In absence of a carefully crafted evaluation benchmark, most if not all, sparse algorithms are evaluated against fairly simple and naive tasks (eg. CIFAR, ImageNet, GLUE, etc.), which can potentially camouflage many advantages as well unexpected predicaments of SNNs. In pursuit of a more general evaluation and unveiling the true potential of sparse algorithms, we introduce\"Sparsity May Cry\"Benchmark (SMC-Bench), a collection of carefully-curated 4 diverse tasks with 10 datasets, that accounts for capturing a wide range of domain-specific and sophisticated knowledge. Our systemic evaluation of the most representative sparse algorithms reveals an important obscured observation: the state-of-the-art magnitude- and/or gradient-based sparse algorithms seemingly fail to perform on SMC-Bench when applied out-of-the-box, sometimes at significantly trivial sparsity as low as 5%. By incorporating these well-thought and diverse tasks, SMC-Bench is designed to favor and encourage the development of more scalable and generalizable sparse algorithms.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2109338656",
                        "name": "Zhenyu (Allen) Zhang"
                    },
                    {
                        "authorId": "1753619492",
                        "name": "Xuxi Chen"
                    },
                    {
                        "authorId": "8242939",
                        "name": "Tianjin Huang"
                    },
                    {
                        "authorId": "145018564",
                        "name": "Ajay Jaiswal"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We have also verified that various during-training pruning methods like GMP [55], RigL [7], and GraNet [32] could improve the performance of OOD detection, which are often more efficient.",
                "We show the performance of three methods: GMP [55], RigL [7], and GraNet [32].",
                "Methods are proposed to drop the unimportant weights [13, 55, 7, 32]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "955dd252793ea3f07de81b2f61165b6a822e07d5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-01201",
                    "ArXiv": "2303.01201",
                    "DOI": "10.48550/arXiv.2303.01201",
                    "CorpusId": 257279924
                },
                "corpusId": 257279924,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/955dd252793ea3f07de81b2f61165b6a822e07d5",
                "title": "Average of Pruning: Improving Performance and Stability of Out-of-Distribution Detection",
                "abstract": "Detecting Out-of-distribution (OOD) inputs have been a critical issue for neural networks in the open world. However, the unstable behavior of OOD detection along the optimization trajectory during training has not been explored clearly. In this paper, we first find the performance of OOD detection suffers from overfitting and instability during training: 1) the performance could decrease when the training error is near zero, and 2) the performance would vary sharply in the final stage of training. Based on our findings, we propose Average of Pruning (AoP), consisting of model averaging and pruning, to mitigate the unstable behaviors. Specifically, model averaging can help achieve a stable performance by smoothing the landscape, and pruning is certified to eliminate the overfitting by eliminating redundant features. Comprehensive experiments on various datasets and architectures are conducted to verify the effectiveness of our method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2162219075",
                        "name": "Zhen Cheng"
                    },
                    {
                        "authorId": "2075372121",
                        "name": "Fei Zhu"
                    },
                    {
                        "authorId": "2870877",
                        "name": "Xu-Yao Zhang"
                    },
                    {
                        "authorId": "1689269",
                        "name": "Cheng-Lin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As a remedy, DST methods (Bellec et al., 2017; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Liu et al., 2021a;b;c; Mocanu et al., 2018; Mostafa & Wang, 2019; Graesser et al., 2022) were introduced to train the neural networks under a given parameter budget while mask change is allowed during\u2026",
                "For the growing criterion, we test both random growth SDST(SET) (Mocanu et al., 2018; Liu et al., 2021c) and gradient-based growth SDST(RigL) (Evci et al., 2020).",
                "For SDST methods, we test both grow methods, i.e., SDST(SET) (Mocanu et al., 2018) which grows connections randomly and SDST(RigL) (Evci et al., 2020) which grows connections via gradient.",
                "Following Evci et al. (2020), we specify the hyper-parameters of DST through sparsity distribution, update schedule, drop criterion, and grow criterion.",
                ", 2018) which grows connections randomly and SDST(RigL) (Evci et al., 2020) which grows connections via gradient.",
                "However, recent advances in dynamic sparse training (DST) (Evci et al., 2020; Liu et al., 2021a;b;c; Mocanu et al., 2018) for the first time show that pruning-during-training methods \u2217Corresponding author.",
                "Following Evci et al. (2020); Liu et al. (2021c), only parameters of fully connected layers and convolutional layers will be pruned.",
                "At initialization, we use the commonly adopted Erdo\u030bs-Re\u0301nyi-Kernel (ERK) strategy (Evci et al., 2020; Dettmers & Zettlemoyer, 2019; Liu et al., 2021c) to allocates higher sparsity to larger layers.",
                "Layer-wise sparsity ratio and masks mG,mD are determined using Erd\u0151s-R\u00e9nyi-Kernel (ERK) graph topology (Evci et al., 2020) and are fixed throughout the training.",
                "Layer-wise sparsity ratio and masks mG,mD are determined using Erdo\u030bs-Re\u0301nyi-Kernel (ERK) graph topology (Evci et al., 2020) and are fixed throughout the training.",
                "As a remedy, DST methods (Bellec et al., 2017; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Liu et al., 2021a;b;c; Mocanu et al., 2018; Mostafa & Wang, 2019; Graesser et al., 2022) were introduced to train the neural networks under a given parameter budget while mask change is allowed during training.",
                "Out of simplicity, we increase the density by growing the connections with the largest gradient magnitude (Evci et al., 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "df1ad8f44fc73282581a389f079df7adc446ac3e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-14670",
                    "ArXiv": "2302.14670",
                    "DOI": "10.48550/arXiv.2302.14670",
                    "CorpusId": 257232904
                },
                "corpusId": 257232904,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/df1ad8f44fc73282581a389f079df7adc446ac3e",
                "title": "Double Dynamic Sparse Training for GANs",
                "abstract": "The past decade has witnessed a drastic increase in modern deep neural networks (DNNs) size, especially for generative adversarial networks (GANs). Since GANs usually suffer from high computational complexity, researchers have shown an increased interest in applying pruning methods to reduce the training and inference costs of GANs. Among different pruning methods invented for supervised learning, dynamic sparse training (DST) has gained increasing attention recently as it enjoys excellent training efficiency with comparable performance to post-hoc pruning. Hence, applying DST on GANs, where we train a sparse GAN with a fixed parameter count throughout training, seems to be a good candidate for reducing GAN training costs. However, a few challenges, including the degrading training instability, emerge due to the adversarial nature of GANs. Hence, we introduce a quantity called balance ratio (BR) to quantify the balance of the generator and the discriminator. We conduct a series of experiments to show the importance of BR in understanding sparse GAN training. Building upon single dynamic sparse training (SDST), where only the generator is adjusted during training, we propose double dynamic sparse training (DDST) to control the BR during GAN training. Empirically, DDST automatically determines the density of the discriminator and greatly boosts the performance of sparse GANs on multiple datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2159213654",
                        "name": "Yite Wang"
                    },
                    {
                        "authorId": "2149267217",
                        "name": "Jing Wu"
                    },
                    {
                        "authorId": "2130392",
                        "name": "N. Hovakimyan"
                    },
                    {
                        "authorId": "153899948",
                        "name": "Ruoyu Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2017), RIGL (Evci et al., 2020), SNFS (Dettmers & Zettlemoyer, 2020) and DNW (Wortsman et al.",
                "We compare our approach against Incremental (Zhu & Gupta, 2018), STR (Kusu-pati et al., 2020), Global Magnitude (Singh & Alistarh, 2020), WoodFisher (Singh & Alistarh, 2020), GMP (Gale et al., 2019), Variational Dropout (Molchanov et al., 2017), RIGL (Evci et al., 2020), SNFS (Dettmers & Zettlemoyer, 2020) and DNW (Wortsman et al., 2019 MobileNetV1."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0c40850c24bf543b14ceb44124db1f4cf88211f3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-14623",
                    "ArXiv": "2302.14623",
                    "DOI": "10.48550/arXiv.2302.14623",
                    "CorpusId": 257232716
                },
                "corpusId": 257232716,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0c40850c24bf543b14ceb44124db1f4cf88211f3",
                "title": "Fast as CHITA: Neural Network Pruning with Combinatorial Optimization",
                "abstract": "The sheer size of modern neural networks makes model serving a serious computational challenge. A popular class of compression techniques overcomes this challenge by pruning or sparsifying the weights of pretrained networks. While useful, these techniques often face serious tradeoffs between computational requirements and compression quality. In this work, we propose a novel optimization-based pruning framework that considers the combined effect of pruning (and updating) multiple weights subject to a sparsity constraint. Our approach, CHITA, extends the classical Optimal Brain Surgeon framework and results in significant improvements in speed, memory, and performance over existing optimization-based approaches for network pruning. CHITA's main workhorse performs combinatorial optimization updates on a memory-friendly representation of local quadratic approximation(s) of the loss function. On a standard benchmark of pretrained models and datasets, CHITA leads to significantly better sparsity-accuracy tradeoffs than competing methods. For example, for MLPNet with only 2% of the weights retained, our approach improves the accuracy by 63% relative to the state of the art. Furthermore, when used in conjunction with fine-tuning SGD steps, our method achieves significant accuracy gains over the state-of-the-art approaches.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2114206387",
                        "name": "Riade Benbaki"
                    },
                    {
                        "authorId": "2144302707",
                        "name": "Wenyu Chen"
                    },
                    {
                        "authorId": "4444470",
                        "name": "X. Meng"
                    },
                    {
                        "authorId": "2858060",
                        "name": "Hussein Hazimeh"
                    },
                    {
                        "authorId": "145367710",
                        "name": "N. Ponomareva"
                    },
                    {
                        "authorId": "48634137",
                        "name": "Zhe Zhao"
                    },
                    {
                        "authorId": "48912652",
                        "name": "R. Mazumder"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026in this area includes STR (Kusupati et al., 2020), Top-KAST (Jayakumar et al., 2020), CS (Continuous Sparsification) (Savarese et al., 2020), RigL (Evci et al., 2020), WoodFisher (Singh & Alistarh, 2020), PSGD (Kim et al., 2020), GraNet (Liu et al., 2021a), Powerprop (Schwarz et al., 2021),\u2026",
                "Recent works in this area includes STR (Kusupati et al., 2020), Top-KAST (Jayakumar et al., 2020), CS (Continuous Sparsification) (Savarese et al., 2020), RigL (Evci et al., 2020), WoodFisher (Singh & Alistarh, 2020), PSGD (Kim et al., 2020), GraNet (Liu et al., 2021a), Powerprop (Schwarz et al., 2021), ProbMask (Zhou et al., 2021), GPO (Wang et al., 2022), OptG (Zhang et al., 2022) and STDS (Chen et al., 2022).",
                ", 2020), RigL (Evci et al., 2020), WoodFisher (Singh & Alistarh, 2020), PSGD (Kim et al.",
                "\u2026wheresoever equally, and yet becomes totally ignorant of nowadays pruning researches like sparsity budget allocation, e.g., Erdo\u030bs-Re\u0301nyi (Mocanu et al., 2018) and Erdo\u030bs-Re\u0301nyi-Kernel (ERK) (Evci et al., 2020), or commonsense in this area like \u201cLeave at least one path from input through output\u201d.",
                "This design endows our method with versatility while treating weight wheresoever equally, and yet becomes totally ignorant of nowadays pruning researches like sparsity budget allocation, e.g., Erdo\u030bs-Re\u0301nyi (Mocanu et al., 2018) and Erdo\u030bs-Re\u0301nyi-Kernel (ERK) (Evci et al., 2020), or commonsense in this area like \u201cLeave at least one path from input through output\u201d.",
                ", 2018) and Erd\u0151s-R\u00e9nyi-Kernel (ERK) (Evci et al., 2020), or commonsense in this area like \u201cLeave at least one path from input through output\u201d."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3e617414c5ce17b69710897cbd2acd371b91c74b",
                "externalIds": {
                    "ArXiv": "2302.13019",
                    "DBLP": "journals/corr/abs-2302-13019",
                    "DOI": "10.48550/arXiv.2302.13019",
                    "CorpusId": 257219926
                },
                "corpusId": 257219926,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3e617414c5ce17b69710897cbd2acd371b91c74b",
                "title": "A Unified Framework for Soft Threshold Pruning",
                "abstract": "Soft threshold pruning is among the cutting-edge pruning methods with state-of-the-art performance. However, previous methods either perform aimless searching on the threshold scheduler or simply set the threshold trainable, lacking theoretical explanation from a unified perspective. In this work, we reformulate soft threshold pruning as an implicit optimization problem solved using the Iterative Shrinkage-Thresholding Algorithm (ISTA), a classic method from the fields of sparse recovery and compressed sensing. Under this theoretical framework, all threshold tuning strategies proposed in previous studies of soft threshold pruning are concluded as different styles of tuning $L_1$-regularization term. We further derive an optimal threshold scheduler through an in-depth study of threshold scheduling based on our framework. This scheduler keeps $L_1$-regularization coefficient stable, implying a time-invariant objective function from the perspective of optimization. In principle, the derived pruning algorithm could sparsify any mathematical model trained via SGD. We conduct extensive experiments and verify its state-of-the-art performance on both Artificial Neural Networks (ResNet-50 and MobileNet-V1) and Spiking Neural Networks (SEW ResNet-18) on ImageNet datasets. On the basis of this framework, we derive a family of pruning methods, including sparsify-during-training, early pruning, and pruning at initialization. The code is available at https://github.com/Yanqi-Chen/LATS.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2115935255",
                        "name": "Yanqing Chen"
                    },
                    {
                        "authorId": "2125040566",
                        "name": "Zhengyu Ma"
                    },
                    {
                        "authorId": "2087000501",
                        "name": "Wei Fang"
                    },
                    {
                        "authorId": "51056401",
                        "name": "Xiawu Zheng"
                    },
                    {
                        "authorId": "1746114",
                        "name": "Zhaofei Yu"
                    },
                    {
                        "authorId": "2115729952",
                        "name": "Yonghong Tian"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Evci et al. (2020) builds on this approach to give a method that does not require frequent gradient re-computation."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b6ffd0b74620ceff699df661a7bccc56d0e932d9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-10494",
                    "ArXiv": "2302.10494",
                    "DOI": "10.48550/arXiv.2302.10494",
                    "CorpusId": 257050449
                },
                "corpusId": 257050449,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b6ffd0b74620ceff699df661a7bccc56d0e932d9",
                "title": "MaskedKD: Efficient Distillation of Vision Transformers with Masked Images",
                "abstract": "Knowledge distillation is an effective method for training lightweight models, but it introduces a significant amount of computational overhead to the training cost, as the method requires acquiring teacher supervisions on training samples. This additional cost -- called distillation cost -- is most pronounced when we employ large-scale teacher models such as vision transformers (ViTs). We present MaskedKD, a simple yet effective strategy that can significantly reduce the cost of distilling ViTs without sacrificing the prediction accuracy of the student model. Specifically, MaskedKD diminishes the cost of running teacher at inference by masking a fraction of image patch tokens fed to the teacher, and therefore skipping the computations required to process those patches. The mask locations are selected to prevent masking away the core features of an image that the student model uses for prediction. This mask selection mechanism operates based on some attention score of the student model, which is already computed during the student forward pass, and thus incurs almost no additional computation. Without sacrificing the final student accuracy, MaskedKD dramatically reduces the amount of computations required for distilling ViTs. We demonstrate that MaskedKD can save up the distillation cost by $50\\%$ without any student performance drop, leading to approximately $28\\%$ drop in the overall training FLOPs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2162166029",
                        "name": "Seungwook Son"
                    },
                    {
                        "authorId": "2702448",
                        "name": "Namhoon Lee"
                    },
                    {
                        "authorId": "2127384453",
                        "name": "Jaehoon Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Implementations: We follow the settings in (Evci et al., 2020; Sundar & Dwaraknath, 2021).",
                "Consistent with widely used sparse training methods (Evci et al., 2020; Liu et al., 2021), the deterministic mask stops updating near the end of the training process.",
                "We perform a comprehensive empirical evaluation of CigL, comparing it with the popular baseline method RigL (Evci et al., 2020).",
                "Datasets & Model Architectures: We follow the settings in Evci et al. (2020) for a comprehensive comparison.",
                "Inspired by the widely-used sparse training method RigL (Evci et al., 2020), we believe a larger weight/gradient magnitude implies that the weight is more helpful for loss reduction and needs to be activated.",
                "The sparse topology is usually controlled by a mask, and various sparse training methods have been proposed to find a suitable mask to achieve comparable or even higher accuracy compared to dense training (Evci et al., 2020; Liu et al., 2021; Schwarz et al., 2021).",
                "\u2026magnitude,\nare designed (Mocanu et al., 2018; Bellec et al., 2018; Frankle & Carbin, 2019; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Jayakumar et al., 2020; Liu et al., 2021; O\u0308zdenizci & Legenstein, 2021; Zhou et al., 2021; Schwarz et al., 2021; Yin et al.,\u2026",
                "Sparse training is gaining increasing attention and has been used in various deep neural network (DNN) learning tasks (Evci et al., 2020; Dietrich et al., 2021; Bibikar et al., 2022)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "b1c88b29bb53f83abe89fa74b2873e1a9d4aaccd",
                "externalIds": {
                    "ArXiv": "2302.09369",
                    "DBLP": "conf/iclr/LeiZXM23",
                    "DOI": "10.48550/arXiv.2302.09369",
                    "CorpusId": 257038366
                },
                "corpusId": 257038366,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b1c88b29bb53f83abe89fa74b2873e1a9d4aaccd",
                "title": "Calibrating the Rigged Lottery: Making All Tickets Reliable",
                "abstract": "Although sparse training has been successfully used in various resource-limited deep learning tasks to save memory, accelerate training, and reduce inference time, the reliability of the produced sparse models remains unexplored. Previous research has shown that deep neural networks tend to be over-confident, and we find that sparse training exacerbates this problem. Therefore, calibrating the sparse models is crucial for reliable prediction and decision-making. In this paper, we propose a new sparse training method to produce sparse models with improved confidence calibration. In contrast to previous research that uses only one mask to control the sparse topology, our method utilizes two masks, including a deterministic mask and a random mask. The former efficiently searches and activates important weights by exploiting the magnitude of weights and gradients. While the latter brings better exploration and finds more appropriate weight values by random updates. Theoretically, we prove our method can be viewed as a hierarchical variational approximation of a probabilistic deep Gaussian process. Extensive experiments on multiple datasets, model architectures, and sparsities show that our method reduces ECE values by up to 47.8\\% and simultaneously maintains or even improves accuracy with only a slight increase in computation and storage burden.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144399315",
                        "name": "Bowen Lei"
                    },
                    {
                        "authorId": "1718601",
                        "name": "Ruqi Zhang"
                    },
                    {
                        "authorId": "2116459424",
                        "name": "Dongkuan Xu"
                    },
                    {
                        "authorId": "1789565",
                        "name": "B. Mallick"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(Cited in Section 5) [17] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen.",
                "In supervised learning, especially computer vision, many promising results have been achieved with sparsity over the last few years [11, 17, 31].",
                "Graesser et al. [20] compared DST methods such as SET [33] and RigL [17] in many deep RL environments.",
                "[20] compared DST methods such as SET [33] and RigL [17] in many deep RL environments."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4f7dfea2fa810a3cc3fc65bcca52ae2ec1cbe672",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-06548",
                    "ArXiv": "2302.06548",
                    "DOI": "10.48550/arXiv.2302.06548",
                    "CorpusId": 256827148
                },
                "corpusId": 256827148,
                "publicationVenue": {
                    "id": "6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                    "name": "Adaptive Agents and Multi-Agent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Adapt Agent Multi-agent Syst",
                        "International Joint Conference on Autonomous Agents & Multiagent Systems",
                        "Adapt Agent Multi-agents Syst",
                        "AAMAS",
                        "Adaptive Agents and Multi-Agents Systems",
                        "Int Jt Conf Auton Agent  Multiagent Syst"
                    ],
                    "url": "http://www.ifaamas.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4f7dfea2fa810a3cc3fc65bcca52ae2ec1cbe672",
                "title": "Automatic Noise Filtering with Dynamic Sparse Training in Deep Reinforcement Learning",
                "abstract": "Tomorrow's robots will need to distinguish useful information from noise when performing different tasks. A household robot for instance may continuously receive a plethora of information about the home, but needs to focus on just a small subset to successfully execute its current chore. Filtering distracting inputs that contain irrelevant data has received little attention in the reinforcement learning literature. To start resolving this, we formulate a problem setting in reinforcement learning called the $\\textit{extremely noisy environment}$ (ENE), where up to $99\\%$ of the input features are pure noise. Agents need to detect which features provide task-relevant information about the state of the environment. Consequently, we propose a new method termed $\\textit{Automatic Noise Filtering}$ (ANF), which uses the principles of dynamic sparse training in synergy with various deep reinforcement learning algorithms. The sparse input layer learns to focus its connectivity on task-relevant features, such that ANF-SAC and ANF-TD3 outperform standard SAC and TD3 by a large margin, while using up to $95\\%$ fewer weights. Furthermore, we devise a transfer learning setting for ENEs, by permuting all features of the environment after 1M timesteps to simulate the fact that other information sources can become relevant as the world evolves. Again, ANF surpasses the baselines in final performance and sample complexity. Our code is available at https://github.com/bramgrooten/automatic-noise-filtering",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2122740553",
                        "name": "Bram Grooten"
                    },
                    {
                        "authorId": "67102129",
                        "name": "Ghada Sokar"
                    },
                    {
                        "authorId": "13295612",
                        "name": "Shibhansh Dohare"
                    },
                    {
                        "authorId": "146634864",
                        "name": "Elena Mocanu"
                    },
                    {
                        "authorId": "1389208435",
                        "name": "Matthew E. Taylor"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "C V\n] 1\n3 Fe\nb 20\n23\ntee (Han et al., 2015; Evci et al., 2020).",
                "Sparse training serves as an effective tool to boost the performance of network sparsity (Hoefler et al., 2021; Evci et al., 2020; Sanh et al., 2020).",
                "For example, RigL (Evci et al., 2020) removes smaller-magnitude weights and revives weights with larger-magnitude gradients."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4477998ffdb78df97eeaf8463179e6c240147fa7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-06058",
                    "ArXiv": "2302.06058",
                    "DOI": "10.48550/arXiv.2302.06058",
                    "CorpusId": 256827025
                },
                "corpusId": 256827025,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4477998ffdb78df97eeaf8463179e6c240147fa7",
                "title": "Bi-directional Masks for Efficient N: M Sparse Training",
                "abstract": "We focus on addressing the dense backward propagation issue for training efficiency of N:M fine-grained sparsity that preserves at most N out of M consecutive weights and achieves practical speedups supported by the N:M sparse tensor core. Therefore, we present a novel method of Bi-directional Masks (Bi-Mask) with its two central innovations in: 1) Separate sparse masks in the two directions of forward and backward propagation to obtain training acceleration. It disentangles the forward and backward weight sparsity and overcomes the very dense gradient computation. 2) An efficient weight row permutation method to maintain performance. It picks up the permutation candidate with the most eligible N:M weight blocks in the backward to minimize the gradient gap between traditional uni-directional masks and our bi-directional masks. Compared with existing uni-directional scenario that applies a transposable mask and enables backward acceleration, our Bi-Mask is experimentally demonstrated to be more superior in performance. Also, our Bi-Mask performs on par with or even better than methods that fail to achieve backward acceleration. Project of this paper is available at \\url{https://github.com/zyxxmu/Bi-Mask}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108078624",
                        "name": "Yu-xin Zhang"
                    },
                    {
                        "authorId": "1753623782",
                        "name": "Yiting Luo"
                    },
                    {
                        "authorId": "49352079",
                        "name": "Mingbao Lin"
                    },
                    {
                        "authorId": "2112888098",
                        "name": "Yunshan Zhong"
                    },
                    {
                        "authorId": "2205729484",
                        "name": "Jingjing Xie"
                    },
                    {
                        "authorId": "2070266415",
                        "name": "Fei Chao"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4117f8b1aee5907cb8c0907f3cffbb11b27f28e0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-05601",
                    "ArXiv": "2302.05601",
                    "DOI": "10.48550/arXiv.2302.05601",
                    "CorpusId": 256827141
                },
                "corpusId": 256827141,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4117f8b1aee5907cb8c0907f3cffbb11b27f28e0",
                "title": "Pruning Deep Neural Networks from a Sparsity Perspective",
                "abstract": "In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may under-prune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a limit that appears to correspond to the beginning of underfitting. Subsequently, PQI decreases again when the model collapse and significant deterioration in the performance of the model start to occur. Additionally, our experiments demonstrate that the proposed adaptive pruning algorithm with proper choice of hyper-parameters is superior to the iterative pruning algorithms such as the lottery ticket-based pruning methods, in terms of both compression efficiency and robustness.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51298945",
                        "name": "Enmao Diao"
                    },
                    {
                        "authorId": "2096527",
                        "name": "G. Wang"
                    },
                    {
                        "authorId": "14895949",
                        "name": "Jiawei Zhan"
                    },
                    {
                        "authorId": "2168770330",
                        "name": "Yuhong Yang"
                    },
                    {
                        "authorId": "143798670",
                        "name": "Jie Ding"
                    },
                    {
                        "authorId": "1780864",
                        "name": "V. Tarokh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Notably, the speedups attained through RigL surpass those obtained through GMP.",
                "This is because RigL initializes the network layers with the target sparsity at the outset of training, while GMP progressively achieves the target sparsity during training.",
                "We set the hyper-parameters \u03b1 = 0.3, Tend = 80, and \u2206T = 10 (introduced in the original paper (Evci et al., 2020)).",
                "On the algorithmic side, there are several interesting proposals for sparse training algorithms (Dettmers & Zettlemoyer, 2019; Kusupati et al., 2020; Evci et al., 2020; Jayakumar et al., 2020; Schwarz et al., 2021), i.e. variants of stochastic gradient descent (SGD) which aim to keep as many\u2026",
                "As such, SparseProp can provide direct support for pruning-during training methods like Gradual Pruning (Zhu & Gupta, 2017), RigL (Evci et al., 2020) or AC/DC (Peste et al., 2021), which assume a fixed sparsity mask for any forward / backward pass, and can be modified to support more complex methods (Jayakumar et al., 2020), which specify different sparsities for weights and gradients.",
                "\u2026while trying to maximize sparsity in the models\u2019 internal representations (Zhu & Gupta, 2017; Lis et al., 2019; Dettmers & Zettlemoyer, 2019; Zhang et al., 2020; Wiedemann et al., 2020; Kusupati et al., 2020; Evci et al., 2020; Jayakumar et al., 2020; Peste et al., 2021; Schwarz et al., 2021).",
                "Finally, we examine the performance of SparseProp on the RigL method (Evci et al., 2020), a dynamic sparse training technique, to train a sparse ResNet18 model.",
                "As noted, there has been a significant amount of work on SGD-like algorithms for sparse training of DNNs, balancing accuracy while trying to maximize sparsity in the models\u2019 internal representations (Zhu & Gupta, 2017; Lis et al., 2019; Dettmers & Zettlemoyer, 2019; Zhang et al., 2020; Wiedemann et al., 2020; Kusupati et al., 2020; Evci et al., 2020; Jayakumar et al., 2020; Peste et al., 2021; Schwarz et al., 2021).",
                "On the algorithmic side, there are several interesting proposals for sparse training algorithms (Dettmers & Zettlemoyer, 2019; Kusupati et al., 2020; Evci et al., 2020; Jayakumar et al., 2020; Schwarz et al., 2021), i.",
                "3, Tend = 80, and \u2206T = 10 (introduced in the original paper (Evci et al., 2020)).",
                "As such, SparseProp can provide direct support for pruning-during training methods like Gradual Pruning (Zhu & Gupta, 2017), RigL (Evci et al., 2020) or AC/DC (Peste et al.",
                "As such, SparseProp can provide direct support for pruning-during training methods like Gradual Pruning (Zhu & Gupta, 2017), RigL (Evci et al., 2020) or AC/DC (Peste et al., 2021), which assume a fixed sparsity mask for any forward / backward pass, and can be modified to support more complex\u2026"
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "503ea8dc1614a61f64a0648495227a0fbcc7a729",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-04852",
                    "ArXiv": "2302.04852",
                    "DOI": "10.48550/arXiv.2302.04852",
                    "CorpusId": 256697569
                },
                "corpusId": 256697569,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/503ea8dc1614a61f64a0648495227a0fbcc7a729",
                "title": "SparseProp: Efficient Sparse Backpropagation for Faster Training of Neural Networks",
                "abstract": "We provide a new efficient version of the backpropagation algorithm, specialized to the case where the weights of the neural network being trained are sparse. Our algorithm is general, as it applies to arbitrary (unstructured) sparsity and common layer types (e.g., convolutional or linear). We provide a fast vectorized implementation on commodity CPUs, and show that it can yield speedups in end-to-end runtime experiments, both in transfer learning using already-sparsified networks, and in training sparse networks from scratch. Thus, our results provide the first support for sparse training on commodity hardware.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2204967430",
                        "name": "Mahdi Nikdan"
                    },
                    {
                        "authorId": "2192519479",
                        "name": "Tommaso Pegolotti"
                    },
                    {
                        "authorId": "2082370867",
                        "name": "Eugenia Iofinova"
                    },
                    {
                        "authorId": "40992614",
                        "name": "Eldar Kurtic"
                    },
                    {
                        "authorId": "3311387",
                        "name": "Dan Alistarh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "712573cc74633ec2283724e868328fd2d319c091",
                "externalIds": {
                    "ArXiv": "2302.02596",
                    "DBLP": "journals/corr/abs-2302-02596",
                    "DOI": "10.48550/arXiv.2302.02596",
                    "CorpusId": 256615905
                },
                "corpusId": 256615905,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/712573cc74633ec2283724e868328fd2d319c091",
                "title": "Ten Lessons We Have Learned in the New \"Sparseland\": A Short Handbook for Sparse Neural Network Researchers",
                "abstract": "This article does not propose any novel algorithm or new hardware for sparsity. Instead, it aims to serve the\"common good\"for the increasingly prosperous Sparse Neural Network (SNN) research community. We attempt to summarize some most common confusions in SNNs, that one may come across in various scenarios such as paper review/rebuttal and talks - many drawn from the authors' own bittersweet experiences! We feel that doing so is meaningful and timely, since the focus of SNN research is notably shifting from traditional pruning to more diverse and profound forms of sparsity before, during, and after training. The intricate relationships between their scopes, assumptions, and approaches lead to misunderstandings, for non-experts or even experts in SNNs. In response, we summarize ten Q\\&As of SNNs from many key aspects, including dense vs. sparse, unstructured sparse vs. structured sparse, pruning vs. sparse training, dense-to-sparse training vs. sparse-to-sparse training, static sparsity vs. dynamic sparsity, before-training/during-training vs. post-training sparsity, and many more. We strive to provide proper and generically applicable answers to clarify those confusions to the best extent possible. We hope our summary provides useful general knowledge for people who want to enter and engage with this exciting community; and also provides some\"mind of ease\"convenience for SNN researchers to explain their work in the right contexts. At the very least (and perhaps as this article's most insignificant target functionality), if you are writing/planning to write a paper or rebuttal in the field of SNNs, we hope some of our answers could help you!",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Compared to traditional unstructured sparsity (Frankle and Carbin, 2018; Lee et al., 2018; Evci et al., 2020) or channel/block structured sparsity algorithms (Wen et al.",
                "Compared to traditional unstructured sparsity (Frankle and Carbin, 2018; Lee et al., 2018; Evci et al., 2020) or channel/block structured sparsity algorithms (Wen et al., 2016; Li et al., 2016; He et al., 2017), adopting N:M masks has negligible evaluation degradation and progressively co-design\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "05deb6c1862b2f129d6652a09eaedbc1f655cc8f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-01172",
                    "ArXiv": "2302.01172",
                    "DOI": "10.48550/arXiv.2302.01172",
                    "CorpusId": 256503843
                },
                "corpusId": 256503843,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/05deb6c1862b2f129d6652a09eaedbc1f655cc8f",
                "title": "STEP: Learning N: M Structured Sparsity Masks from Scratch with Precondition",
                "abstract": "Recent innovations on hardware (e.g. Nvidia A100) have motivated learning N:M structured sparsity masks from scratch for fast model inference. However, state-of-the-art learning recipes in this regime (e.g. SR-STE) are proposed for non-adaptive optimizers like momentum SGD, while incurring non-trivial accuracy drop for Adam-trained models like attention-based LLMs. In this paper, we first demonstrate such gap origins from poorly estimated second moment (i.e. variance) in Adam states given by the masked weights. We conjecture that learning N:M masks with Adam should take the critical regime of variance estimation into account. In light of this, we propose STEP, an Adam-aware recipe that learns N:M masks with two phases: first, STEP calculates a reliable variance estimate (precondition phase) and subsequently, the variance remains fixed and is used as a precondition to learn N:M masks (mask-learning phase). STEP automatically identifies the switching point of two phases by dynamically sampling variance changes over the training trajectory and testing the sample concentration. Empirically, we evaluate STEP and other baselines such as ASP and SR-STE on multiple tasks including CIFAR classification, machine translation and LLM fine-tuning (BERT-Base, GPT-2). We show STEP mitigates the accuracy drop of baseline recipes and is robust to aggressive structured sparsity ratios.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1454006122",
                        "name": "Yucheng Lu"
                    },
                    {
                        "authorId": "3504647",
                        "name": "Shivani Agrawal"
                    },
                    {
                        "authorId": "1929462",
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "authorId": "145573927",
                        "name": "Oleg Rybakov"
                    },
                    {
                        "authorId": "2081393182",
                        "name": "Chris De Sa"
                    },
                    {
                        "authorId": "2112229",
                        "name": "A. Yazdanbakhsh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In light of this, follow-up works with higher training efficiency can be roughly categorized into three groups: (i) find sparse networks once at initialization by measuring the importance of connections on the loss, eliminating the need for the complex iterative optimization schedule [47, 77] ; (ii) identify the winning tickets in Transformers at a very early training stage via low-cost schemes and then merely train these early tickets until convergence [79, 15]; (iii) use an alternating pruning and growing schedule to dynamically update model sparsity patterns throughout training, suitable for general architectures [23, 13]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "fe5a72e0a4aeb5ea5058d9e4531858be5548dfe0",
                "externalIds": {
                    "ArXiv": "2302.01107",
                    "DBLP": "journals/corr/abs-2302-01107",
                    "DOI": "10.48550/arXiv.2302.01107",
                    "CorpusId": 256503897
                },
                "corpusId": 256503897,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/fe5a72e0a4aeb5ea5058d9e4531858be5548dfe0",
                "title": "A Survey on Efficient Training of Transformers",
                "abstract": "Recent advances in Transformers have come with a huge requirement on computing resources, highlighting the importance of developing efficient training techniques to make Transformer training faster, at lower cost, and to higher accuracy by the efficient use of computation and memory resources. This survey provides the first systematic overview of the efficient training of Transformers, covering the recent progress in acceleration arithmetic and hardware, with a focus on the former. We analyze and compare methods that save computation and memory costs for intermediate tensors during training, together with techniques on hardware/algorithm co-design. We finally discuss challenges and promising areas for future research.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3194022",
                        "name": "Bohan Zhuang"
                    },
                    {
                        "authorId": "49270464",
                        "name": "Jing Liu"
                    },
                    {
                        "authorId": "1840579673",
                        "name": "Zizheng Pan"
                    },
                    {
                        "authorId": "11270586",
                        "name": "Haoyu He"
                    },
                    {
                        "authorId": "2178388899",
                        "name": "Yuetian Weng"
                    },
                    {
                        "authorId": "12459603",
                        "name": "Chunhua Shen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several studies have been carried out to identify subnetworks across the model that can provide the best transferability (Gale et al., 2019; Evci et al., 2020; Lee et al., 2021; Guo et al., 2021; Hu et al., 2021)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "684b0f42aa5d5b2bea09f6e0400336cf288cc787",
                "externalIds": {
                    "ArXiv": "2302.00378",
                    "DBLP": "conf/emnlp/AkbarTajariRP22",
                    "ACL": "2022.emnlp-main.726",
                    "DOI": "10.48550/arXiv.2302.00378",
                    "CorpusId": 256459901
                },
                "corpusId": 256459901,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/684b0f42aa5d5b2bea09f6e0400336cf288cc787",
                "title": "An Empirical Study on the Transferability of Transformer Modules in Parameter-efficient Fine-tuning",
                "abstract": "Parameter-efficient fine-tuning has garnered lots of attention in recent studies.On this subject, we investigate the capability of different transformer modules in transferring knowledge from a pre-trained model to a downstream task. Our empirical results suggest that every transformer module is a winning ticket such that fine-tuning the specific module while the rest of the network is frozen achieves a comparable performance to the full fine-tuning case. Among different modules in LMs, LayerNorms exhibit a significant capacity for transfer learning to the extent that with only 0.003% updateable parameters in the layer-wise analysis, they can show acceptable performance on various target tasks.We argue that the performance of LayerNorms could be attributed to their high-magnitude weights compared to other components in a pre-trained model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2203801874",
                        "name": "Mohammad AkbarTajari"
                    },
                    {
                        "authorId": "31280249",
                        "name": "S. Rajaee"
                    },
                    {
                        "authorId": "1717641",
                        "name": "Mohammad Taher Pilehvar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such observation has been made by some sparse training papers, e.g., RigL [16] notes that \u201csparse training methods benefit significantly from increased training steps\u201d.",
                ", RigL [16] notes that \u201csparse training methods benefit significantly from increased training steps\u201d."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1825493cc6a12c1a509b03593991653ff3c76c49",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-05219",
                    "ArXiv": "2301.05219",
                    "DOI": "10.48550/arXiv.2301.05219",
                    "CorpusId": 255749315
                },
                "corpusId": 255749315,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1825493cc6a12c1a509b03593991653ff3c76c49",
                "title": "Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning",
                "abstract": "The state of neural network pruning has been noticed to be unclear and even confusing for a while, largely due to\"a lack of standardized benchmarks and metrics\"[3]. To standardize benchmarks, first, we need to answer: what kind of comparison setup is considered fair? This basic yet crucial question has barely been clarified in the community, unfortunately. Meanwhile, we observe several papers have used (severely) sub-optimal hyper-parameters in pruning experiments, while the reason behind them is also elusive. These sub-optimal hyper-parameters further exacerbate the distorted benchmarks, rendering the state of neural network pruning even more obscure. Two mysteries in pruning represent such a confusing status: the performance-boosting effect of a larger finetuning learning rate, and the no-value argument of inheriting pretrained weights in filter pruning. In this work, we attempt to explain the confusing state of network pruning by demystifying the two mysteries. Specifically, (1) we first clarify the fairness principle in pruning experiments and summarize the widely-used comparison setups; (2) then we unveil the two pruning mysteries and point out the central role of network trainability, which has not been well recognized so far; (3) finally, we conclude the paper and give some concrete suggestions regarding how to calibrate the pruning benchmarks in the future. Code: https://github.com/mingsun-tse/why-the-state-of-pruning-so-confusing.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2113269100",
                        "name": "Huan Wang"
                    },
                    {
                        "authorId": "12282768",
                        "name": "Can Qin"
                    },
                    {
                        "authorId": "153802755",
                        "name": "Yue Bai"
                    },
                    {
                        "authorId": "2156255943",
                        "name": "Yun Fu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "During sparse training, a certain percentage of connections are removed to save memory (Bellec et al., 2017; Evci et al., 2020).",
                "We calculate the gradient variance and correlation of the ResNet-50 on CIFAR-100 from RigL (Evci et al., 2020) and SET (Mocanu et al., 2018) at different sparsities including 0%, 50%, 80%, 90%, and 95%.",
                "Aligned with popular sparse training methods (Evci et al., 2020; \u00d6zdenizci & Legenstein, 2021; Liu et al., 2021), we choose piecewise constant decay schedulers for learning rate and weight decay.",
                "Despite of the good performance, such large models are not applicable when memory or computational resources are limited (Bellec et al., 2017; Evci et al., 2020; Liu et al., 2022).",
                ", 2018), RigL (Evci et al., 2020), BSR-Net (\u00d6zdenizci & Legenstein, 2021) and ITOP (Liu et al.",
                "We add our AGENT to three recent sparse training pipelines, namely SET (Mocanu et al., 2018), RigL (Evci et al., 2020), BSR-Net (\u00d6zdenizci & Legenstein, 2021) and ITOP (Liu et al., 2021).",
                "pruning and growth criteria are proposed, such as weight/gradient magnitude, random selection, and weight sign (Mocanu et al., 2018; Bellec et al., 2018; Frankle & Carbin, 2019; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Jayakumar et al., 2020; Liu et al., 2021; \u00d6zdenizci & Legenstein, 2021; Zhou et al., 2021b; Schwarz et al., 2021; Huang et al., 2022; Liu et al., 2022).",
                "We summarize additional experimental results for the BSR-Net-based \u00d6zdenizci & Legenstein (2021), RigLbased Evci et al. (2020), and ITOP-based Liu et al. (2021) models.",
                "Sparse training (Mocanu et al., 2018; Evci et al., 2020; Liu et al., 2022) is one of the most popular classes of methods to improve efficiency in terms of space (e.g. memory storage) and is receiving increasing attention.",
                "iteratively updated with various criteria (Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Liu et al., 2021; \u00d6zdenizci & Legenstein, 2021).",
                "Implementations: Aligned with the choice of Evci et al. (2020); Sundar & Dwaraknath (2021); \u00d6zdenizci & Legenstein (2021), the parameters of the model are optimized by SGD with momentum.",
                "In RigL-based results, we follow the settings in Evci et al. (2020); Sundar & Dwaraknath (2021).",
                "As for the blue curve for our A-RigL, it is always on the top of the green curve for RigL, indicating that the speedup is successful.\nmethods in RigL-based models Evci et al. (2020).",
                "\u2026selection, and weight sign (Mocanu et al., 2018; Bellec et al., 2018; Frankle & Carbin, 2019; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Jayakumar et al., 2020; Liu et al., 2021; \u00d6zdenizci & Legenstein, 2021; Zhou et al., 2021b; Schwarz et al., 2021; Huang et al.,\u2026",
                "L G\niteratively updated with various criteria (Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Liu et al., 2021; \u00d6zdenizci & Legenstein, 2021).",
                "Sparse training (Mocanu et al., 2018; Evci et al., 2020; Liu et al., 2022) is one of the most popular classes of methods to improve efficiency in terms of space (e."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "f1c04965baab8a9b15fc1dc65413c174b7fbabd3",
                "externalIds": {
                    "ArXiv": "2301.03573",
                    "DBLP": "journals/corr/abs-2301-03573",
                    "DOI": "10.48550/arXiv.2301.03573",
                    "CorpusId": 255546005
                },
                "corpusId": 255546005,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f1c04965baab8a9b15fc1dc65413c174b7fbabd3",
                "title": "Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction",
                "abstract": "Despite impressive performance on a wide variety of tasks, deep neural networks require significant memory and computation costs, prohibiting their application in resource-constrained scenarios. Sparse training is one of the most common techniques to reduce these costs, however, the sparsity constraints add difficulty to the optimization, resulting in an increase in training time and instability. In this work, we aim to overcome this problem and achieve space-time co-efficiency. To accelerate and stabilize the convergence of sparse training, we analyze the gradient changes and develop an adaptive gradient correction method. Specifically, we approximate the correlation between the current and previous gradients, which is used to balance the two gradients to obtain a corrected gradient. Our method can be used with most popular sparse training pipelines under both standard and adversarial setups. Theoretically, we prove that our method can accelerate the convergence rate of sparse training. Extensive experiments on multiple datasets, model architectures, and sparsities demonstrate that our method outperforms leading sparse training methods by up to \\textbf{5.0\\%} in accuracy given the same number of training epochs, and reduces the number of training epochs by up to \\textbf{52.1\\%} to achieve the same accuracy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144399315",
                        "name": "Bowen Lei"
                    },
                    {
                        "authorId": "2116459424",
                        "name": "Dongkuan Xu"
                    },
                    {
                        "authorId": "1718601",
                        "name": "Ruqi Zhang"
                    },
                    {
                        "authorId": "2199813144",
                        "name": "Shuren He"
                    },
                    {
                        "authorId": "1789565",
                        "name": "B. Mallick"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Crucially, following [11], the updated inverse (HUj+1) \u22121 can be calculated efficiently by removing the first row and column, corresponding to j in the original H, from the inverse of (HUj ) \u22121 in (3)For example, structured (column-wise) pruning ResNet50 to 50% structured sparsity without accuracy loss is challenging, even with extensive retraining [30], while unstructured pruning to 90% sparsity is easily achievable with state-of-the-art methods [6, 39]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-00774",
                    "ArXiv": "2301.00774",
                    "DOI": "10.48550/arXiv.2301.00774",
                    "CorpusId": 255372747
                },
                "corpusId": 255372747,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/909ad57ce8caa6b390a65ae09db352d27d8f3996",
                "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
                "abstract": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1502248377",
                        "name": "Elias Frantar"
                    },
                    {
                        "authorId": "3311387",
                        "name": "Dan Alistarh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026finding a fixed sparse mask at the initialization as we mentioned in introduction, on the other hand, dynamic sparse training allows the sparse mask to be updated during training, e.g., (Mocanu et al., 2018; Mostafa and Wang, 2019; Evci et al., 2020; Jayakumar et al., 2020; Liu et al., 2021a,c,d)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "303d96bab3766557529ceb1d9d5856c1090a7ba2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-00327",
                    "ArXiv": "2301.00327",
                    "DOI": "10.48550/arXiv.2301.00327",
                    "CorpusId": 256389889
                },
                "corpusId": 256389889,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/303d96bab3766557529ceb1d9d5856c1090a7ba2",
                "title": "Convergence and Generalization of Wide Neural Networks with Large Bias",
                "abstract": "This work studies training one-hidden-layer overparameterized ReLU networks via gradient descent in the neural tangent kernel (NTK) regime, where the networks' biases are initialized to some constant rather than zero. The tantalizing benefit of such initialization is that the neural network will provably have sparse activation through the entire training process, which enables fast training procedures. The first set of results characterizes the convergence of gradient descent training. Surprisingly, it is shown that the network after sparsification can achieve as fast convergence as the dense network, in comparison to the previous work indicating that the sparse networks converge slower. Further, the required width is improved to ensure gradient descent can drive the training error towards zero at a linear rate. Secondly, the networks' generalization is studied: a width-sparsity dependence is provided which yields a sparsity-dependent Rademacher complexity and generalization bound. To our knowledge, this is the first sparsity-dependent generalization result via Rademacher complexity. Lastly, this work further studies the least eigenvalue of the limiting NTK. Surprisingly, while it is not shown that trainable biases are necessary, trainable bias, which is enabled by our improved analysis scheme, helps to identify a nice data-dependent region where a much finer analysis of the NTK's smallest eigenvalue can be conducted. This leads to a much sharper lower bound on the NTK's smallest eigenvalue than the one previously known and, consequently, an improved generalization bound.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118571035",
                        "name": "Hongru Yang"
                    },
                    {
                        "authorId": "152420547",
                        "name": "Ziyu Jiang"
                    },
                    {
                        "authorId": "2140713366",
                        "name": "Ruizhe Zhang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "50014661",
                        "name": "Yingbin Liang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026pruning (Mariet and Sra, 2015; He et al., 2017; Gale et al., 2019; Suau et al., 2018), non-uniform pruning (Mocanu et al., 2016), expander-graph-related techniques (Prabhu et al., 2018; Kepner and Robinett, 2019) Erdo\u0308s-Re\u0301nyi (Mocanu et al., 2018) and Erdo\u0308s-Re\u0301nyi-Kernel (Evci et al., 2020).",
                "Random pruning has also been considered in static sparse training such as uniform pruning (Mariet and Sra, 2015; He et al., 2017; Gale et al., 2019; Suau et al., 2018), non-uniform pruning (Mocanu et al., 2016), expander-graph-related techniques (Prabhu et al., 2018; Kepner and Robinett, 2019) Erdo\u0308s-Re\u0301nyi (Mocanu et al., 2018) and Erdo\u0308s-Re\u0301nyi-Kernel (Evci et al., 2020).",
                "On the other hand, dynamic sparse training allows the sparse mask to be updated (Mocanu et al., 2018; Mostafa and Wang, 2019; Evci et al., 2020; Jayakumar et al., 2020; Liu et al., 2021c,d,a; Peste et al., 2021)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "179c8c7364caaf3b4d0d03018657aaec6c22a372",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-00335",
                    "ArXiv": "2301.00335",
                    "DOI": "10.48550/arXiv.2301.00335",
                    "CorpusId": 256389914
                },
                "corpusId": 256389914,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/179c8c7364caaf3b4d0d03018657aaec6c22a372",
                "title": "Pruning Before Training May Improve Generalization, Provably",
                "abstract": "It has been observed in practice that applying pruning-at-initialization methods to neural networks and training the sparsified networks can not only retain the testing performance of the original dense models, but also sometimes even slightly boost the generalization performance. Theoretical understanding for such experimental observations are yet to be developed. This work makes the first attempt to study how different pruning fractions affect the model's gradient descent dynamics and generalization. Specifically, this work considers a classification task for overparameterized two-layer neural networks, where the network is randomly pruned according to different rates at the initialization. It is shown that as long as the pruning fraction is below a certain threshold, gradient descent can drive the training loss toward zero and the network exhibits good generalization performance. More surprisingly, the generalization bound gets better as the pruning fraction gets larger. To complement this positive result, this work further shows a negative result: there exists a large pruning fraction such that while gradient descent is still able to drive the training loss toward zero (by memorizing noise), the generalization performance is no better than random guessing. This further suggests that pruning can change the feature learning process, which leads to the performance drop of the pruned neural network.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118571035",
                        "name": "Hongru Yang"
                    },
                    {
                        "authorId": "50014661",
                        "name": "Yingbin Liang"
                    },
                    {
                        "authorId": "46909769",
                        "name": "Xiaojie Guo"
                    },
                    {
                        "authorId": "3008832",
                        "name": "Lingfei Wu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Rather than growing weights randomly (as in the line 9 of Algorithm 1), RigL grows them with the highest magnitude gradients.",
                "Different from the traditional dynamic sparse training methods, which explore the connections in a layer-wise manner [40, 34, 13] and can hardly cover smaller RF, we propose a more fine-grained sparse training strategy for TSC.",
                "Introduced as a new training paradigm before the lottery ticket hypothesis, DST starts from a sparse network and allows the sparse connectivity to be evolved dynamically with a fixed number of parameters throughout training [40, 13, 35, 60, 10].",
                "4.6 Case Study on other DST Method\nLikewise, the effectiveness of the proposed DSN method is also analyzed in the presence of other DST methods, such as RigL [13].",
                "Likewise, the effectiveness of the proposed DSN method is also analyzed in the presence of other DST methods, such as RigL [13]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "6de57c70a2b661f4c8171e26b5f53b5782d937d1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-09840",
                    "ArXiv": "2212.09840",
                    "DOI": "10.48550/arXiv.2212.09840",
                    "CorpusId": 254103020
                },
                "corpusId": 254103020,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6de57c70a2b661f4c8171e26b5f53b5782d937d1",
                "title": "Dynamic Sparse Network for Time Series Classification: Learning What to \"see\"",
                "abstract": "The receptive field (RF), which determines the region of time series to be ``seen'' and used, is critical to improve the performance for time series classification (TSC). However, the variation of signal scales across and within time series data, makes it challenging to decide on proper RF sizes for TSC. In this paper, we propose a dynamic sparse network (DSN) with sparse connections for TSC, which can learn to cover various RF without cumbersome hyper-parameters tuning. The kernels in each sparse layer are sparse and can be explored under the constraint regions by dynamic sparse training, which makes it possible to reduce the resource cost. The experimental results show that the proposed DSN model can achieve state-of-art performance on both univariate and multivariate TSC datasets with less than 50\\% computational cost compared with recent baseline methods, opening the path towards more accurate resource-aware methods for time series analyses. Our code is publicly available at: https://github.com/QiaoXiao7282/DSN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2056427208",
                        "name": "Qiao Xiao"
                    },
                    {
                        "authorId": "46791907",
                        "name": "Boqian Wu"
                    },
                    {
                        "authorId": null,
                        "name": "Yu Zhang"
                    },
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "146634864",
                        "name": "Elena Mocanu"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, \u201cdynamic pruning\u201d (Evci et al., 2020) has also been used during training to find sparser architectures from dense models."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-05055",
                    "ArXiv": "2212.05055",
                    "DOI": "10.48550/arXiv.2212.05055",
                    "CorpusId": 254535822
                },
                "corpusId": 254535822,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
                "title": "Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints",
                "abstract": "Training large, deep neural networks to convergence can be prohibitively expensive. As a result, often only a small selection of popular, dense models are reused across different contexts and tasks. Increasingly, sparsely activated models, which seek to decouple model size from computation costs, are becoming an attractive alternative to dense models. Although more efficient in terms of quality and computation cost, sparse models remain data-hungry and costly to train from scratch in the large scale regime. In this work, we propose sparse upcycling -- a simple way to reuse sunk training costs by initializing a sparsely activated Mixture-of-Experts model from a dense checkpoint. We show that sparsely upcycled T5 Base, Large, and XL language models and Vision Transformer Base and Large models, respectively, significantly outperform their dense counterparts on SuperGLUE and ImageNet, using only ~50% of the initial dense pretraining sunk cost. The upcycled models also outperform sparse models trained from scratch on 100% of the initial dense pretraining computation budget.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51891020",
                        "name": "Aran Komatsuzaki"
                    },
                    {
                        "authorId": "1794202",
                        "name": "J. Puigcerver"
                    },
                    {
                        "authorId": "1405626394",
                        "name": "J. Lee-Thorp"
                    },
                    {
                        "authorId": "2135570886",
                        "name": "Carlos Riquelme Ruiz"
                    },
                    {
                        "authorId": "40608942",
                        "name": "Basil Mustafa"
                    },
                    {
                        "authorId": "1643737606",
                        "name": "J. Ainslie"
                    },
                    {
                        "authorId": "97947517",
                        "name": "Yi Tay"
                    },
                    {
                        "authorId": "3226635",
                        "name": "Mostafa Dehghani"
                    },
                    {
                        "authorId": "2815290",
                        "name": "N. Houlsby"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Inspired by RigL [14], devices only rate partial model parameters (e.",
                "Although RigL [14] tries to reduce memory consumption, it needs to compute gradients for all parameters, which is computationally expensive and may lead to straggling issues in federated learning.",
                "Such negative impact becomes more challenging when pruning towards an extremely tiny subnetwork, as the biased initial subnetwork can deviate significantly from the optimal structure, resulting in poor accuracy [14].",
                "Inspired by RigL [14], devices only rate partial model parameters (e.g., a single layer) at a time, where the top-K importance scores are stored locally and uploaded to the server, significantly reducing memory, computation, and communication cost.",
                "The other category is dynamic sparse training [14], [25], [26]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "c0e7a85fbedee72ad171b80d74160d887acca0de",
                "externalIds": {
                    "ArXiv": "2212.01977",
                    "CorpusId": 259695965
                },
                "corpusId": 259695965,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c0e7a85fbedee72ad171b80d74160d887acca0de",
                "title": "Distributed Pruning Towards Tiny Neural Networks in Federated Learning",
                "abstract": "Neural network pruning is an essential technique for reducing the size and complexity of deep neural networks, enabling large-scale models on devices with limited resources. However, existing pruning approaches heavily rely on training data for guiding the pruning strategies, making them ineffective for federated learning over distributed and confidential datasets. Additionally, the memory- and computation-intensive pruning process becomes infeasible for recourse-constrained devices in federated learning. To address these challenges, we propose FedTiny, a distributed pruning framework for federated learning that generates specialized tiny models for memory- and computing-constrained devices. We introduce two key modules in FedTiny to adaptively search coarse- and finer-pruned specialized models to fit deployment scenarios with sparse and cheap local computation. First, an adaptive batch normalization selection module is designed to mitigate biases in pruning caused by the heterogeneity of local data. Second, a lightweight progressive pruning module aims to finer prune the models under strict memory and computational budgets, allowing the pruning policy for each layer to be gradually determined rather than evaluating the overall model structure. The experimental results demonstrate the effectiveness of FedTiny, which outperforms state-of-the-art approaches, particularly when compressing deep models to extremely sparse tiny models. FedTiny achieves an accuracy improvement of 2.61% while significantly reducing the computational cost by 95.91% and the memory footprint by 94.01% compared to state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2194197373",
                        "name": "Hong Huang"
                    },
                    {
                        "authorId": "2131620609",
                        "name": "Lan Zhang"
                    },
                    {
                        "authorId": "2193707949",
                        "name": "Chaoyue Sun"
                    },
                    {
                        "authorId": "2666471",
                        "name": "R. Fang"
                    },
                    {
                        "authorId": "152162529",
                        "name": "Xiaoyong Yuan"
                    },
                    {
                        "authorId": "144953170",
                        "name": "Dapeng Oliver Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To limit these computations, sparse networks have been thoroughly investigated in the past few years [43, 4, 27, 41, 21, 38, 3, 6, 32, 22, 42, 25], and significant efforts have been devoted to their efficient hardware implementation [8, 29].",
                "First, Erdos-R\u00e9nyi-Kernel (ERK) [30, 3] proposes to scale the global sparsity ratio (= ratio of zeros to the total parameter count) with a layer-wise factor.",
                "Most of the persistent pruning literature makes use of hard thresholding to increase the pruning ratio [9, 4, 3].",
                "First, Erdos-Re\u0301nyi-Kernel (ERK) [30, 3] proposes to scale the global sparsity ratio (= ratio of zeros to the total parameter count) with a layer-wise factor. so as to induce a higher (smaller) sparsity for layers with more (less) parameters, e.g. the first convolution layers of a ResNet-50 remain dense, thereby preserving accuracy.",
                "It has the advantage of being straightforward, and our experiments reveal that it results in better accuracy/sparsity tradeoffs as ERK or LAMP.",
                "RIGL [3] builds on gradient momentum to resurrect some of the zeroed weights.",
                "Most methods use gradient-magnitude [3], weight-magnitude [9, 43, 17], or a mix of both [36] to select the weights to set to zero after some preliminary training.",
                "Both ERK and LAMP target higher accuracy at the cost of an increased complexity (as measured in FLOPS).",
                "As a consequence, as shown experimentally [3], after global thresholding, those kernels with larger weights end-up in being relatively less sparse than the kernels associated to the smaller resolutions channels of wider layers.",
                "GraNet [25], and other similar approaches like [3, 26], update the pruning mask every few thousands iterations, by regenerating connections when pruning others.",
                "For completeness, comparison is also provided with DNW [38], RIGL [3] and GraNet [25] (see Section 2 for the presentation of those methods), implemented as recommended by their respective authors.",
                "ResNet-50 [10] and MobileNetv1 [13] have been trained on ImageNet [2] using the same standard hyper-parameters, number of epochs (=100), and data-augmentation as previous related works [3, 17, 42, 25], leading to a dense baseline accuracy of 77.",
                "It has been widely documented [30, 3, 20] that the network prediction accuracy is more impacted when pruning occurs close to the input."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "e2a06471cd317ff426c1cc79c0139cd12f699f00",
                "externalIds": {
                    "DBLP": "conf/wacv/VanderschuerenV23",
                    "ArXiv": "2212.01076",
                    "DOI": "10.1109/WACV56688.2023.00380",
                    "CorpusId": 254221223
                },
                "corpusId": 254221223,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/e2a06471cd317ff426c1cc79c0139cd12f699f00",
                "title": "Are Straight-Through gradients and Soft-Thresholding all you need for Sparse Training?",
                "abstract": "Turning the weights to zero when training a neural network helps in reducing the computational complexity at inference. To progressively increase the sparsity ratio in the network without causing sharp weight discontinuities during training, our work combines soft-thresholding and straight-through gradient estimation to update the raw, i.e. non-thresholded, version of zeroed weights. Our method, named ST-3 for straight-through/soft-thresholding/sparse-training2, obtains SoA results, both in terms of accuracy/sparsity and accuracy/FLOPS trade-offs, when progressively increasing the sparsity ratio in a single training cycle. In particular, despite its simplicity, ST-3 favorably compares to the most recent methods, adopting differentiable formulations [42] or bio-inspired neuroregeneration principles [25]. This suggests that the key ingredients for effective sparsification primarily lie in the ability to give the weights the freedom to evolve smoothly across the zero state while progressively increasing the sparsity ratio.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "98683669",
                        "name": "A. Vanderschueren"
                    },
                    {
                        "authorId": "1719942",
                        "name": "C. Vleeschouwer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "RigL [14] updated the sparsity topology of the sparse network during training using the same magnitudebased weights dropping method while growing back the weights using top-k absolute largest gradients, achieving better accuracy than static mask training under same sparsity.",
                "For other baselines, we select SNIP [10] and GraSP [11] as the static mask training baselines while adopting DeepR [32], SNFS [22], DSR [13], SET [12], RigL [14], MEST [23], RigL-ITOP [1] as the dynamic mask training baselines as shown in Table II.",
                "We use the same sparsity initialization method ERK in the state-of-the-art sparse training method such as RigL [14] and ITOP [1].",
                "To improve the flexibility, dynamic mask training has been proposed [12, 13, 14], where the sparse mask is periodically updated by drop-and-grow to search for better Drop-andgrow Non-active state (weight=0)"
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "94d8e29580b6fc29c9ffbee88d49401e0e2da1b5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-16667",
                    "ArXiv": "2211.16667",
                    "DOI": "10.1109/DAC56929.2023.10247716",
                    "CorpusId": 254096500
                },
                "corpusId": 254096500,
                "publicationVenue": {
                    "id": "021b37d3-cef1-4c12-a442-257f7900c23d",
                    "name": "Design Automation Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Des Autom Conf",
                        "DAC"
                    ],
                    "url": "http://www.dac.com/"
                },
                "url": "https://www.semanticscholar.org/paper/94d8e29580b6fc29c9ffbee88d49401e0e2da1b5",
                "title": "Dynamic Sparse Training via Balancing the Exploration-Exploitation Trade-off",
                "abstract": "Over-parameterization of deep neural networks (DNNs) has shown high prediction accuracy for many applications. Although effective, the large number of parameters hinders its popularity on resource-limited devices and has an outsize environmental impact. Sparse training (using a fixed number of nonzero weights in each iteration) could significantly mitigate the training costs by reducing the model size. However, existing sparse training methods mainly use either random-based or greedy-based drop-and-grow strategies, resulting in local minimal and low accuracy. In this work, to assist explainable sparse training, we propose important weights Exploitation and coverage Exploration to characterize Dynamic Sparse Training (DST-EE), and provide quantitative analysis of these two metrics. We further design an acquisition function and provide the theoretical guarantees for the proposed method and clarify its convergence property. Experimental results show that sparse models (up to 98% sparsity) obtained by our proposed method outperform the SOTA sparse training methods on a wide variety of deep learning tasks. On VGG-19 / CIFAR-100, ResNet-50 / CIFAR-10, ResNet-50 / CIFAR-100, our method has even higher accuracy than dense models. On ResNet-50 / ImageNet, the proposed method has up to 8.2% accuracy improvement compared to SOTA sparse training methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2122804649",
                        "name": "Shaoyi Huang"
                    },
                    {
                        "authorId": "2144399315",
                        "name": "Bowen Lei"
                    },
                    {
                        "authorId": "2116459424",
                        "name": "Dongkuan Xu"
                    },
                    {
                        "authorId": "144490597",
                        "name": "Hongwu Peng"
                    },
                    {
                        "authorId": "2116969722",
                        "name": "Yue Sun"
                    },
                    {
                        "authorId": "3197711",
                        "name": "Mimi Xie"
                    },
                    {
                        "authorId": "2881873",
                        "name": "Caiwen Ding"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The second limitation is that the existing methods sparsify networks layer-wise with a uniform sparsity ratio, which typically leads to inferior performance compared with the non-uniform layer-wise sparsity [20, 39, 40], especially for deep architectures [41].",
                "We do this because [20] showed that the layer-wise sparsity obtained by this scheme outperforms the other well-studied sparsity ratios [19, 37, 39]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "11972f9f5a0226546aa3f21d54efab84e7a8ce01",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-15335",
                    "ArXiv": "2211.15335",
                    "DOI": "10.48550/arXiv.2211.15335",
                    "CorpusId": 254044688
                },
                "corpusId": 254044688,
                "publicationVenue": {
                    "id": "50534c12-f4ba-4c64-806b-01647d1baacf",
                    "name": "LOG IN",
                    "type": "journal",
                    "alternate_names": [
                        "Log in",
                        "Log",
                        "LOG"
                    ],
                    "issn": "0720-8642",
                    "alternate_issns": [
                        "1547-4690",
                        "0024-5798"
                    ],
                    "url": "https://www.log-in-verlag.de/informatische_bildung/",
                    "alternate_urls": [
                        "https://www.anycorp.com/log/about",
                        "https://www.jstor.org/journal/log",
                        "https://www.anycorp.com/",
                        "http://www.jstor.org/action/showPublication?journalCode=log",
                        "http://www.anycorp.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/11972f9f5a0226546aa3f21d54efab84e7a8ce01",
                "title": "You Can Have Better Graph Neural Networks by Not Training Weights at All: Finding Untrained GNNs Tickets",
                "abstract": "Recent works have impressively demonstrated that there exists a subnetwork in randomly initialized convolutional neural networks (CNNs) that can match the performance of the fully trained dense networks at initialization, without any optimization of the weights of the network (i.e., untrained networks). However, the presence of such untrained subnetworks in graph neural networks (GNNs) still remains mysterious. In this paper we carry out the first-of-its-kind exploration of discovering matching untrained GNNs. With sparsity as the core tool, we can find \\textit{untrained sparse subnetworks} at the initialization, that can match the performance of \\textit{fully trained dense} GNNs. Besides this already encouraging finding of comparable performance, we show that the found untrained subnetworks can substantially mitigate the GNN over-smoothing problem, hence becoming a powerful tool to enable deeper GNNs without bells and whistles. We also observe that such sparse untrained subnetworks have appealing performance in out-of-distribution detection and robustness of input perturbations. We evaluate our method across widely-used GNN architectures on various popular datasets including the Open Graph Benchmark (OGB).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8242939",
                        "name": "Tianjin Huang"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2055723543",
                        "name": "Meng Fang"
                    },
                    {
                        "authorId": "49917515",
                        "name": "V. Menkovski"
                    },
                    {
                        "authorId": "1481819499",
                        "name": "Jiaxu Zhao"
                    },
                    {
                        "authorId": "1410465360",
                        "name": "Lu Yin"
                    },
                    {
                        "authorId": "1382535564",
                        "name": "Yulong Pei"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "2131166985",
                        "name": "Shiwei Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many works have been proposed, focusing on improving the performance of sparse training for supervised image classification tasks by introducing different criteria for connection regrowth [20, 34, 59, 49, 73, 50].",
                "Aligned with many prior works [20, 54, 27], we find that the weight magnitude is an effective metric for estimating the connection importance (#4)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ad4e0c0444b724c9004a53a7a504dc7b8aa32189",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-14627",
                    "ArXiv": "2211.14627",
                    "DOI": "10.48550/arXiv.2211.14627",
                    "CorpusId": 254044452
                },
                "corpusId": 254044452,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ad4e0c0444b724c9004a53a7a504dc7b8aa32189",
                "title": "Where to Pay Attention in Sparse Training for Feature Selection?",
                "abstract": "A new line of research for feature selection based on neural networks has recently emerged. Despite its superiority to classical methods, it requires many training iterations to converge and detect informative features. The computational time becomes prohibitively long for datasets with a large number of samples or a very high dimensional feature space. In this paper, we present a new efficient unsupervised method for feature selection based on sparse autoencoders. In particular, we propose a new sparse training algorithm that optimizes a model's sparse topology during training to pay attention to informative features quickly. The attention-based adaptation of the sparse topology enables fast detection of informative features after a few training iterations. We performed extensive experiments on 10 datasets of different types, including image, speech, text, artificial, and biological. They cover a wide range of characteristics, such as low and high-dimensional feature spaces, and few and large training samples. Our proposed approach outperforms the state-of-the-art methods in terms of selecting informative features while reducing training iterations and computational costs substantially. Moreover, the experiments show the robustness of our method in extremely noisy environments.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "67102129",
                        "name": "Ghada Sokar"
                    },
                    {
                        "authorId": "1768130522",
                        "name": "Zahra Atashgahi"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020), model compression based on weights and gradient magnitude (Evci et al., 2020), and improved generalization performance by estimating the loss landscape (Foret et al.",
                "\u2026weight- or/and function-space regularizers (Kirkpatrick et al., 2017; Pan et al., 2020), model compression based on weights and gradient magnitude (Evci et al., 2020), and improved generalization performance by estimating the loss landscape (Foret et al., 2021) and avoiding sharp minima\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4e9625d1f3d591a2d4ea7dc7ff0f683fde6d682b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-14133",
                    "ArXiv": "2211.14133",
                    "DOI": "10.48550/arXiv.2211.14133",
                    "CorpusId": 254017667
                },
                "corpusId": 254017667,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4e9625d1f3d591a2d4ea7dc7ff0f683fde6d682b",
                "title": "PipeFisher: Efficient Training of Large Language Models Using Pipelining and Fisher Information Matrices",
                "abstract": "Pipeline parallelism enables efficient training of Large Language Models (LLMs) on large-scale distributed accelerator clusters. Yet, pipeline bubbles during startup and tear-down reduce the utilization of accelerators. Although efficient pipeline schemes with micro-batching and bidirectional pipelines have been proposed to maximize utilization, a significant number of bubbles cannot be filled using synchronous forward and backward passes. To address this problem, we suggest that extra work be assigned to the bubbles to gain auxiliary benefits in LLM training. As an example in this direction, we propose PipeFisher, which assigns the work of K-FAC, a second-order optimization method based on the Fisher information matrix, to the bubbles to accelerate convergence. In Phase 1 pretraining of BERT-Base and -Large models, PipeFisher reduces the (simulated) training time to 50-75% compared to training with a first-order optimizer by greatly improving the accelerator utilization and benefiting from the improved convergence by K-FAC.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "25003800",
                        "name": "Kazuki Osawa"
                    },
                    {
                        "authorId": "1738041",
                        "name": "Shigang Li"
                    },
                    {
                        "authorId": "1713648",
                        "name": "T. Hoefler"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The value of pl is decided based on Erd\u0151sR\u00e9nyi-Kernel scaling (used in (Evci et al. 2021; Raihan and Aamodt 2020))."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0687d4d5da9222d7d2ee84fdbe092585a1043e60",
                "externalIds": {
                    "ArXiv": "2211.09945",
                    "CorpusId": 257772035
                },
                "corpusId": 257772035,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0687d4d5da9222d7d2ee84fdbe092585a1043e60",
                "title": "VeriCompress: A Tool to Streamline the Synthesis of Verified Robust Compressed Neural Networks from Scratch",
                "abstract": "AI's widespread integration has led to neural networks (NNs) deployment on edge and similar limited-resource platforms for safety-critical scenarios. Yet, NN's fragility raises concerns about reliable inference. Moreover, constrained platforms demand compact networks. This study introduces VeriCompress, a tool that automates the search and training of compressed models with robustness guarantees. These models are well-suited for safety-critical applications and adhere to predefined architecture and size limitations, making them deployable on resource-restricted platforms. The method trains models 2-3 times faster than the state-of-the-art approaches, surpassing relevant baseline approaches by average accuracy and robustness gains of 15.1 and 9.8 percentage points, respectively. When deployed on a resource-restricted generic platform, these models require 5-8 times less memory and 2-4 times less inference time than models used in verified robustness literature. Our comprehensive evaluation across various model architectures and datasets, including MNIST, CIFAR, SVHN, and a relevant pedestrian detection dataset, showcases VeriCompress's capacity to identify compressed verified robust models with reduced computation overhead compared to current standards. This underscores its potential as a valuable tool for end users, such as developers of safety-critical applications on edge or Internet of Things platforms, empowering them to create suitable models for safety-critical, resource-constrained platforms in their respective domains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2054619254",
                        "name": "Sawinder Kaur"
                    },
                    {
                        "authorId": "2159075376",
                        "name": "Yi Xiao"
                    },
                    {
                        "authorId": "2080152511",
                        "name": "A. Salekin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Sparse training imposes sparsity constraints during network training [4, 13, 14, 42, 43, 45, 49]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "12246cbfc5328dac166f3ae87548760352cb7c8e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-08544",
                    "ArXiv": "2211.08544",
                    "DOI": "10.48550/arXiv.2211.08544",
                    "CorpusId": 253553264
                },
                "corpusId": 253553264,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/12246cbfc5328dac166f3ae87548760352cb7c8e",
                "title": "Exploiting the Partly Scratch-off Lottery Ticket for Quantization-Aware Training",
                "abstract": "Quantization-aware training (QAT) receives extensive popularity as it well retains the performance of quantized networks. In QAT, the contemporary experience is that all quantized weights are updated for an entire training process. In this paper, this experience is challenged based on an interesting phenomenon we observed. Specifically, a large portion of quantized weights reaches the optimal quantization level after a few training epochs, which we refer to as the partly scratch-off lottery ticket. This straightforward-yet-valuable observation naturally inspires us to zero out gradient calculations of these weights in the remaining training period to avoid meaningless updating. To effectively find the ticket, we develop a heuristic method, dubbed lottery ticket scratcher (LTS), which freezes a weight once the distance between the full-precision one and its quantization level is smaller than a controllable threshold. Surprisingly, the proposed LTS typically eliminates 50%-70% weight updating and 25%-35% FLOPs of the backward pass, while still resulting on par with or even better performance than the compared baseline. For example, compared with the baseline, LTS improves 2-bit MobileNetV2 by 5.05%, eliminating 46% weight updating and 23% FLOPs of the backward pass. Code is at url{https://github.com/zysxmu/LTS}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112888098",
                        "name": "Yunshan Zhong"
                    },
                    {
                        "authorId": "49352079",
                        "name": "Mingbao Lin"
                    },
                    {
                        "authorId": "2108078624",
                        "name": "Yu-xin Zhang"
                    },
                    {
                        "authorId": "2141029301",
                        "name": "Gongrui Nan"
                    },
                    {
                        "authorId": "2070266415",
                        "name": "Fei Chao"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "87b6b88a9ae174872c212faa420f224f94563478",
                "externalIds": {
                    "DBLP": "conf/acl/HeDDLYT23",
                    "ArXiv": "2211.05528",
                    "ACL": "2023.acl-long.803",
                    "DOI": "10.18653/v1/2023.acl-long.803",
                    "CorpusId": 258887710
                },
                "corpusId": 258887710,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/87b6b88a9ae174872c212faa420f224f94563478",
                "title": "PAD-Net: An Efficient Framework for Dynamic Networks",
                "abstract": "Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model\u2019s representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert the given static layers into fully dynamic ones where all parameters are dynamic (at least within a single layer) and vary with the input. However, such a fully dynamic setting may cause redundant parameters and high deployment costs, limiting the applicability of dynamic networks to a broader range of tasks and models. The main contributions of our work are challenging the basic commonsense in dynamic networks and proposing a partially dynamic network, namely PAD-Net, to transform the redundant dynamic parameters into static ones. Also, we further design Iterative Mode Partition to partition dynamic and static parameters efficiently. Our method is comprehensively supported by large-scale experiments with two typical advanced dynamic architectures, i.e., DY-Conv and MoE, on both image classification and GLUE benchmarks. Encouragingly, we surpass the fully dynamic networks by +0.7\\% top-1 acc with only 30% dynamic parameters for ResNet-50 and +1.9\\% average score in language understanding with only 50% dynamic parameters for BERT. Code will be released at: https://github.com/Shwai-He/PAD-Net.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152235390",
                        "name": "Shwai He"
                    },
                    {
                        "authorId": "46573238",
                        "name": "Liang Ding"
                    },
                    {
                        "authorId": "2187286687",
                        "name": "Daize Dong"
                    },
                    {
                        "authorId": "2185826034",
                        "name": "Boan Liu"
                    },
                    {
                        "authorId": "46597961",
                        "name": "Fuqiang Yu"
                    },
                    {
                        "authorId": "2140448089",
                        "name": "Dacheng Tao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b2322a59bd5b49d920d41128934192f77f5def0d",
                "externalIds": {
                    "ArXiv": "2211.03013",
                    "DBLP": "journals/corr/abs-2211-03013",
                    "ACL": "2022.acl-long.157",
                    "DOI": "10.18653/v1/2022.acl-long.157",
                    "CorpusId": 248780046
                },
                "corpusId": 248780046,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/b2322a59bd5b49d920d41128934192f77f5def0d",
                "title": "Robust Lottery Tickets for Pre-trained Language Models",
                "abstract": "Recent works on Lottery Ticket Hypothesis have shown that pre-trained language models (PLMs) contain smaller matching subnetworks(winning tickets) which are capable of reaching accuracy comparable to the original models. However, these tickets are proved to be notrobust to adversarial examples, and even worse than their PLM counterparts. To address this problem, we propose a novel method based on learning binary weight masks to identify robust tickets hidden in the original PLMs. Since the loss is not differentiable for the binary mask, we assign the hard concrete distribution to the masks and encourage their sparsity using a smoothing approximation of L0 regularization.Furthermore, we design an adversarial loss objective to guide the search for robust tickets and ensure that the tickets perform well bothin accuracy and robustness. Experimental results show the significant improvement of the proposed method over previous work on adversarial robustness evaluation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2058585152",
                        "name": "Rui Zheng"
                    },
                    {
                        "authorId": "2165225344",
                        "name": "Rong Bao"
                    },
                    {
                        "authorId": "2110347739",
                        "name": "Yuhao Zhou"
                    },
                    {
                        "authorId": "2113983954",
                        "name": "Di Liang"
                    },
                    {
                        "authorId": "2592528",
                        "name": "Sirui Wang"
                    },
                    {
                        "authorId": "39533001",
                        "name": "Wei Wu"
                    },
                    {
                        "authorId": "2067331064",
                        "name": "Tao Gui"
                    },
                    {
                        "authorId": "47835189",
                        "name": "Qi Zhang"
                    },
                    {
                        "authorId": "1790227",
                        "name": "Xuanjing Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "98844f755d41303fa53227e054265c6dcd00ea92",
                "externalIds": {
                    "ArXiv": "2211.03033",
                    "DBLP": "journals/corr/abs-2211-03033",
                    "DOI": "10.48550/arXiv.2211.03033",
                    "CorpusId": 253384070
                },
                "corpusId": 253384070,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/98844f755d41303fa53227e054265c6dcd00ea92",
                "title": "Efficient Traffic State Forecasting using Spatio-Temporal Network Dependencies: A Sparse Graph Neural Network Approach",
                "abstract": "\u2014Traf\ufb01c state prediction in a transportation network is paramount for effective traf\ufb01c operations and management, as well as informed user and system-level decision-making. However, long-term traf\ufb01c prediction (beyond 30 minutes into the future) remains challenging in current research. In this work, we integrate the spatio-temporal dependencies in the transportation network from network modeling, together with the graph convolutional network (GCN) and graph attention network (GAT). To further tackle the dramatic computation and memory cost caused by the giant model size (i.e., number of weights) caused by multiple cascaded layers, we propose sparse training to mitigate the training cost, while preserving the prediction accuracy. It is a process of training using a \ufb01xed number of nonzero weights in each layer in each iteration. We consider the problem of long-term traf\ufb01c speed forecasting for a real large-scale transportation network data from the California Department of Transportation (Caltrans) Performance Measurement System (PeMS). Experimental results show that the proposed GCN-STGT and GAT-STGT models achieve low prediction errors on short-, mid- and long-term prediction horizons, of 15, 30 and 45 minutes in duration, respectively. Using our sparse training, we could train from scratch with high sparsity (e.g., up to 90%), equivalent to 10 \u00d7 \ufb02oating point operations per second (FLOPs) reduction on computational cost using the and same epochs as dense training, and arrive at a model with very small accuracy loss compared with the original dense training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2053235014",
                        "name": "Bin Lei"
                    },
                    {
                        "authorId": "2122804649",
                        "name": "Shaoyi Huang"
                    },
                    {
                        "authorId": "2881873",
                        "name": "Caiwen Ding"
                    },
                    {
                        "authorId": "151113008",
                        "name": "Monika Filipovska"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[8], Mostafa and Wang [31], and Wortsman et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "002d3f265e2fc6ec81cecdbde27f16181618a8b5",
                "externalIds": {
                    "DBLP": "conf/eccv/HumbleSLDA22",
                    "ArXiv": "2211.02206",
                    "DOI": "10.48550/arXiv.2211.02206",
                    "CorpusId": 253370572
                },
                "corpusId": 253370572,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/002d3f265e2fc6ec81cecdbde27f16181618a8b5",
                "title": "Soft Masking for Cost-Constrained Channel Pruning",
                "abstract": "Structured channel pruning has been shown to significantly accelerate inference time for convolution neural networks (CNNs) on modern hardware, with a relatively minor loss of network accuracy. Recent works permanently zero these channels during training, which we observe to significantly hamper final accuracy, particularly as the fraction of the network being pruned increases. We propose Soft Masking for cost-constrained Channel Pruning (SMCP) to allow pruned channels to adaptively return to the network while simultaneously pruning towards a target cost constraint. By adding a soft mask re-parameterization of the weights and channel pruning from the perspective of removing input channels, we allow gradient updates to previously pruned channels and the opportunity for the channels to later return to the network. We then formulate input channel pruning as a global resource allocation problem. Our method outperforms prior works on both the ImageNet classification and PASCAL VOC detection datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1785409034",
                        "name": "Ryan Humble"
                    },
                    {
                        "authorId": "9461499",
                        "name": "Maying Shen"
                    },
                    {
                        "authorId": "2060797517",
                        "name": "J. Latorre"
                    },
                    {
                        "authorId": "2190039284",
                        "name": "Eric Darve1"
                    },
                    {
                        "authorId": "2974008",
                        "name": "J. \u00c1lvarez"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, MobileNetv2 can be trained on the CIFAR-100 dataset with a 99% weight sparsity using ERK.",
                "To confirm this, we trained\nGGR only using either gradient-based insertion or random insertion using a sparsity of 90% and ERK.",
                "We implement two different approaches for selecting the layer density at initialization: Erd\u0151s-R\u00e9nyi-Kernel (ERK) (Mocanu et al., 2018b; Evci et al., 2020) and uniform density.",
                "Researchers have shown many times that large, sparse models outperform dense, small models with equal parameter count significantly (Evci et al., 2020; Mostafa & Wang, 2019).",
                "While GGR still achieves the best accuracy, the network cannot be shrunk as well as with ERK initialization.",
                "We implement two different approaches for selecting the layer density at initialization: Erdo\u030bs-Re\u0301nyi-Kernel (ERK) (Mocanu et al., 2018b; Evci et al., 2020) and uniform density.",
                "Looking at the individual results with a sparsity of 90% and ERK reveals that GGR is performing worse than previous work when applied to large models.",
                "RigL (Evci et al., 2020) improved the criteria introduced in SET.",
                "The percentage of weights to be redistributed is decreased every epoch using the cosinefunction as suggested by the authors of RigL (Evci et al., 2020).",
                "Previous work shows that zero-initialization of newly inserted weights leads to equal or even slightly better performance than gradient-based insertion (Evci et al., 2020).",
                "The best example for that is VGG16 which fails to learn using RigL and DSR using a uniform layer sparsity of 99%, while GGR performs nearly as good as with ERK initialization."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "c9504dfeabeffc836bcaba20a04afce0bb731394",
                "externalIds": {
                    "ArXiv": "2210.14012",
                    "DBLP": "journals/corr/abs-2210-14012",
                    "DOI": "10.48550/arXiv.2210.14012",
                    "CorpusId": 253107217
                },
                "corpusId": 253107217,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c9504dfeabeffc836bcaba20a04afce0bb731394",
                "title": "Gradient-based Weight Density Balancing for Robust Dynamic Sparse Training",
                "abstract": "Training a sparse neural network from scratch requires optimizing connections at the same time as the weights themselves. Typically, the weights are redistributed after a predefined number of weight updates, removing a fraction of the parameters of each layer and inserting them at different locations in the same layers. The density of each layer is determined using heuristics, often purely based on the size of the parameter tensor. While the connections per layer are optimized multiple times during training, the density of each layer remains constant. This leaves great unrealized potential, especially in scenarios with a high sparsity of 90% and more. We propose Global Gradient-based Redistribution, a technique which distributes weights across all layers - adding more weights to the layers that need them most. Our evaluation shows that our approach is less prone to unbalanced weight distribution at initialization than previous work and that it is able to find better performing sparse subnetworks at very high sparsity levels.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "52116137",
                        "name": "Mathias Parger"
                    },
                    {
                        "authorId": "2188778393",
                        "name": "Alexander Ertl"
                    },
                    {
                        "authorId": "2188778607",
                        "name": "Paul Eibensteiner"
                    },
                    {
                        "authorId": "2110565449",
                        "name": "J. H. Mueller"
                    },
                    {
                        "authorId": "2061234702",
                        "name": "Martin Winter"
                    },
                    {
                        "authorId": "1681353",
                        "name": "M. Steinberger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[139] took inspiration from the recent sparse training work [140] and dynamically extracted and trained sparse sub-networks."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "88a6a6e0194b99b3df98a7b3a884747b10f76be4",
                "externalIds": {
                    "DBLP": "journals/mms/FengT23",
                    "DOI": "10.1007/s00530-022-01003-8",
                    "CorpusId": 253027533
                },
                "corpusId": 253027533,
                "publicationVenue": {
                    "id": "d1997ea9-9d41-4458-9280-94feb013bd15",
                    "name": "Multimedia Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Multimedia Syst"
                    ],
                    "issn": "0942-4962",
                    "url": "http://www.springer.com/computer/information+systems+and+applications/journal/530?changeHeader",
                    "alternate_urls": [
                        "https://link.springer.com/journal/530",
                        "http://www.springer.com/computer/information+systems+and+applications/journal/530?changeHeader="
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/88a6a6e0194b99b3df98a7b3a884747b10f76be4",
                "title": "A survey of visual neural networks: current trends, challenges and opportunities",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2056811337",
                        "name": "Ping Feng"
                    },
                    {
                        "authorId": "2173392",
                        "name": "Zhenjun Tang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We compare with global magnitude (GM) following the same schedule as CAP, as well as the state-of-the-art SViTE [Chen et al., 2021] paper, which trains the sparse model from scratch using a variant of RigL [Evci et al., 2020], but for a total of 600 epochs.",
                "Many other criteria exist, such as gradient magnitude [Evci et al., 2020] or \u201crates of change\u201d in the weights [Sanh et al.",
                "In addition to the sparse training from scratch with periodic updates of the sparsity weights with some saliency criterion for weight elimination and regrowth [Evci et al., 2020] one can consider alternating compressed/decompressed training (AC/DC), proposed in [Peste et al., 2021].",
                "Many other criteria exist, such as gradient magnitude [Evci et al., 2020] or \u201crates of change\u201d in the weights [Sanh et al., 2020].",
                "The only existing prior work on unstructured ViT pruning is SViTE [Chen et al., 2021], which performed careful customization of the RigL pruning method [Evci et al., 2020] to the special case of ViT models.",
                ", 2021], which performed careful customization of the RigL pruning method [Evci et al., 2020] to the special case of ViT models.",
                ", 2021] paper, which trains the sparse model from scratch using a variant of RigL [Evci et al., 2020], but for a total of 600 epochs."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "3137c3ae4d21cf19d1bec8648f5f2935b5a3378e",
                "externalIds": {
                    "ArXiv": "2210.09223",
                    "CorpusId": 258987321
                },
                "corpusId": 258987321,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3137c3ae4d21cf19d1bec8648f5f2935b5a3378e",
                "title": "CAP: Correlation-Aware Pruning for Highly-Accurate Sparse Vision Models",
                "abstract": "Driven by significant improvements in architectural design and training pipelines, computer vision has recently experienced dramatic progress in terms of accuracy on classic benchmarks such as ImageNet. These highly-accurate models are challenging to deploy, as they appear harder to compress using standard techniques such as pruning. We address this issue by introducing the Correlation Aware Pruner (CAP), a new unstructured pruning framework which significantly pushes the compressibility limits for state-of-the-art architectures. Our method is based on two technical advancements: a new theoretically-justified pruner, which can handle complex weight correlations accurately and efficiently during the pruning process itself, and an efficient finetuning procedure for post-compression recovery. We validate our approach via extensive experiments on several modern vision models such as Vision Transformers (ViT), modern CNNs, and ViT-CNN hybrids, showing for the first time that these can be pruned to high sparsity levels (e.g. $\\geq 75$%) with low impact on accuracy ($\\leq 1$% relative drop). Our approach is also compatible with structured pruning and quantization, and can lead to practical speedups of 1.5 to 2.4x without accuracy loss. To further showcase CAP's accuracy and scalability, we use it to show for the first time that extremely-accurate large vision models, trained via self-supervised techniques, can also be pruned to moderate sparsities, with negligible accuracy loss.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2006108901",
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "authorId": "40992614",
                        "name": "Eldar Kurtic"
                    },
                    {
                        "authorId": "1502248377",
                        "name": "Elias Frantar"
                    },
                    {
                        "authorId": "3311387",
                        "name": "Dan Alistarh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To name a few, Rigging the Lottery [10] edits the network connections and jointly updates the learnable weights.",
                "We take a simple 5-layer MLP to clarify this operation: its dimensions are [512, 100, 100, 100, 10] with 4 weight matrices w1 \u2208 R512\u00d7100, w2 \u2208 R100\u00d7100, w3 \u2208 R100\u00d7100, and w4 \u2208 R100\u00d710.",
                "Another promising direction is to obtain sparse network by dynamic sparse training [10, 29].",
                "The potential of randomly initialized network is pioneeringly explored by the Lottery Ticket Hypothesis [11], and further investigated by [28, 2, 39]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "eb5ef20c02cd6951b1659db75c6a260fd49371c3",
                "externalIds": {
                    "ArXiv": "2210.06699",
                    "DBLP": "conf/nips/Bai00ZT022",
                    "DOI": "10.48550/arXiv.2210.06699",
                    "CorpusId": 252872848
                },
                "corpusId": 252872848,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/eb5ef20c02cd6951b1659db75c6a260fd49371c3",
                "title": "Parameter-Efficient Masking Networks",
                "abstract": "A deeper network structure generally handles more complicated non-linearity and performs more competitively. Nowadays, advanced network designs often contain a large number of repetitive structures (e.g., Transformer). They empower the network capacity to a new level but also increase the model size inevitably, which is unfriendly to either model restoring or transferring. In this study, we are the first to investigate the representative potential of fixed random weights with limited unique values by learning diverse masks and introduce the Parameter-Efficient Masking Networks (PEMN). It also naturally leads to a new paradigm for model compression to diminish the model size. Concretely, motivated by the repetitive structures in modern neural networks, we utilize one random initialized layer, accompanied with different masks, to convey different feature mappings and represent repetitive network modules. Therefore, the model can be expressed as \\textit{one-layer} with a bunch of masks, which significantly reduce the model storage cost. Furthermore, we enhance our strategy by learning masks for a model filled by padding a given random weights vector. In this way, our method can further lower the space complexity, especially for models without many repetitive architectures. We validate the potential of PEMN learning masks on random weights with limited unique values and test its effectiveness for a new compression paradigm based on different network architectures. Code is available at https://github.com/yueb17/PEMN",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "153802755",
                        "name": "Yue Bai"
                    },
                    {
                        "authorId": "2113269100",
                        "name": "Huan Wang"
                    },
                    {
                        "authorId": "1663912566",
                        "name": "Xu Ma"
                    },
                    {
                        "authorId": "2108284606",
                        "name": "Yitian Zhang"
                    },
                    {
                        "authorId": "6018169",
                        "name": "Zhiqiang Tao"
                    },
                    {
                        "authorId": "2156255943",
                        "name": "Yun Fu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In spirit, our effort can be likened to methods like RigL (Evci et al., 2020), which discovers the wiring of a sparse neural network during training rather than pruning a dense network post training."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "caa3d5833fb2cae452f0d41e2ece3cefacb916e0",
                "externalIds": {
                    "ArXiv": "2210.05974",
                    "CorpusId": 256416386
                },
                "corpusId": 256416386,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/caa3d5833fb2cae452f0d41e2ece3cefacb916e0",
                "title": "Clustering the Sketch: A Novel Approach to Embedding Table Compression",
                "abstract": "Embedding tables are used by machine learning systems to work with categorical features. These tables can become exceedingly large in modern recommendation systems, necessitating the development of new methods for fitting them in memory, even during training. The best previous methods for table compression are so called\"post training\"quantization schemes such as\"product\"and\"residual\"quantization (Gray&Neuhoff, 1998). These methods replace table rows with references to k-means clustered\"codewords\". Unfortunately, clustering requires prior knowledge of the table to be compressed, which limits the memory savings to inference time and not training time. Hence, recent work, like the QR method (Shi et al., 2020), has used random references (linear sketching), which can be computed with hash functions before training. Unfortunately, the compression achieved is inferior to that achieved by post-training quantization. The new algorithm, CQR, shows how to get the best of two worlds by combining clustering and sketching: First IDs are randomly assigned to a codebook and codewords are trained (end to end) for an epoch. Next, we expand the codebook and apply clustering to reduce the size again. Finally, we add new random references and continue training. We show experimentally close to those of post-training quantization with the training time memory reductions of sketch-based methods, and we prove that our method always converges to the optimal embedding table for least-squares training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2187580005",
                        "name": "Henry Ling-Hei Tsang"
                    },
                    {
                        "authorId": "3416152",
                        "name": "Thomas Dybdahl Ahle"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The criterion of pruning could be weight magnitude [18], gradient [15] and Hessian [35, 49], etc.",
                "Following [15, 9], we apply a cosine decay scheduler to alleviate this problem:",
                "Limited by the requirement for the pre-trained model, some recent research [15, 2, 9, 30, 43, 37, 38] attempts to discover a sparse network directly from the training process."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7a86ebcb1b6238cdae16ec17ce59501e33d722f7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-05177",
                    "ArXiv": "2210.05177",
                    "DOI": "10.48550/arXiv.2210.05177",
                    "CorpusId": 252815924
                },
                "corpusId": 252815924,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7a86ebcb1b6238cdae16ec17ce59501e33d722f7",
                "title": "Make Sharpness-Aware Minimization Stronger: A Sparsified Perturbation Approach",
                "abstract": "Deep neural networks often suffer from poor generalization caused by complex and non-convex loss landscapes. One of the popular solutions is Sharpness-Aware Minimization (SAM), which smooths the loss landscape via minimizing the maximized change of training loss when adding a perturbation to the weight. However, we find the indiscriminate perturbation of SAM on all parameters is suboptimal, which also results in excessive computation, i.e., double the overhead of common optimizers like Stochastic Gradient Descent (SGD). In this paper, we propose an efficient and effective training scheme coined as Sparse SAM (SSAM), which achieves sparse perturbation by a binary mask. To obtain the sparse mask, we provide two solutions which are based onFisher information and dynamic sparse training, respectively. In addition, we theoretically prove that SSAM can converge at the same rate as SAM, i.e., $O(\\log T/\\sqrt{T})$. Sparse SAM not only has the potential for training acceleration but also smooths the loss landscape effectively. Extensive experimental results on CIFAR10, CIFAR100, and ImageNet-1K confirm the superior efficiency of our method to SAM, and the performance is preserved or even better with a perturbation of merely 50% sparsity. Code is availiable at https://github.com/Mi-Peng/Sparse-Sharpness-Aware-Minimization.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2068673155",
                        "name": "Peng Mi"
                    },
                    {
                        "authorId": "2144035454",
                        "name": "Li Shen"
                    },
                    {
                        "authorId": "2143150727",
                        "name": "Tianhe Ren"
                    },
                    {
                        "authorId": "2110191063",
                        "name": "Yiyi Zhou"
                    },
                    {
                        "authorId": "1759841",
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    },
                    {
                        "authorId": "2135519749",
                        "name": "Dacheng Tao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mocanu et al. (2018); Evci et al. (2020) specify each layer with a random topology in which larger layers are allocated with higher sparsity than smaller layers."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7bd551537b67a31a733ac12a8de69968bc190f66",
                "externalIds": {
                    "ArXiv": "2210.04284",
                    "DBLP": "journals/corr/abs-2210-04284",
                    "DOI": "10.48550/arXiv.2210.04284",
                    "CorpusId": 252780333
                },
                "corpusId": 252780333,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/7bd551537b67a31a733ac12a8de69968bc190f66",
                "title": "SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters",
                "abstract": "Adapter Tuning, which freezes the pretrained language models (PLMs) and only fine-tunes a few extra modules, becomes an appealing efficient alternative to the full model fine-tuning. Although computationally efficient, the recent Adapters often increase parameters (e.g. bottleneck dimension) for matching the performance of full model fine-tuning, which we argue goes against their original intention. In this work, we re-examine the parameter-efficiency of Adapters through the lens of network pruning (we name such plug-in concept as \\texttt{SparseAdapter}) and find that SparseAdapter can achieve comparable or better performance than standard Adapters when the sparse ratio reaches up to 80\\%. Based on our findings, we introduce an easy but effective setting ``\\textit{Large-Sparse}'' to improve the model capacity of Adapters under the same parameter budget. Experiments on five competitive Adapters upon three advanced PLMs show that with proper sparse method (e.g. SNIP) and ratio (e.g. 40\\%) SparseAdapter can consistently outperform their corresponding counterpart. Encouragingly, with the \\textit{Large-Sparse} setting, we can obtain further appealing gains, even outperforming the full fine-tuning by a large margin. Our code will be released at: https://github.com/Shwai-He/SparseAdapter.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152235390",
                        "name": "Shwai He"
                    },
                    {
                        "authorId": "46573238",
                        "name": "Liang Ding"
                    },
                    {
                        "authorId": "2187286687",
                        "name": "Daize Dong"
                    },
                    {
                        "authorId": "2187384924",
                        "name": "Miao Zhang"
                    },
                    {
                        "authorId": "2140448089",
                        "name": "Dacheng Tao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The former identifies redundant model weights by leveraging heuristics-based metrics such as weight magnitudes [6, 17, 19, 11, 22, 37, 31, 36, 38], gradient magnitudes [21, 23, 24, 39, 40], and Hessian statistics [41\u201346]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "13cf75c5fb62db04e2485997b03be33e2125ace5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-04092",
                    "ArXiv": "2210.04092",
                    "DOI": "10.48550/arXiv.2210.04092",
                    "CorpusId": 252780187
                },
                "corpusId": 252780187,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/13cf75c5fb62db04e2485997b03be33e2125ace5",
                "title": "Advancing Model Pruning via Bi-level Optimization",
                "abstract": "The deployment constraints in practical applications necessitate the pruning of large-scale deep learning models, i.e., promoting their weight sparsity. As illustrated by the Lottery Ticket Hypothesis (LTH), pruning also has the potential of improving their generalization ability. At the core of LTH, iterative magnitude pruning (IMP) is the predominant pruning method to successfully find 'winning tickets'. Yet, the computation cost of IMP grows prohibitively as the targeted pruning ratio increases. To reduce the computation overhead, various efficient 'one-shot' pruning methods have been developed, but these schemes are usually unable to find winning tickets as good as IMP. This raises the question of how to close the gap between pruning accuracy and pruning efficiency? To tackle it, we pursue the algorithmic advancement of model pruning. Specifically, we formulate the pruning problem from a fresh and novel viewpoint, bi-level optimization (BLO). We show that the BLO interpretation provides a technically-grounded optimization base for an efficient implementation of the pruning-retraining learning paradigm used in IMP. We also show that the proposed bi-level optimization-oriented pruning method (termed BiP) is a special class of BLO problems with a bi-linear problem structure. By leveraging such bi-linearity, we theoretically show that BiP can be solved as easily as first-order optimization, thus inheriting the computation efficiency. Through extensive experiments on both structured and unstructured pruning with 5 model architectures and 4 data sets, we demonstrate that BiP can find better winning tickets than IMP in most cases, and is computationally as efficient as the one-shot pruning schemes, demonstrating 2-7 times speedup over IMP for the same level of model accuracy and sparsity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155369380",
                        "name": "Yihua Zhang"
                    },
                    {
                        "authorId": "100630765",
                        "name": "Yuguang Yao"
                    },
                    {
                        "authorId": "2944292",
                        "name": "P. Ram"
                    },
                    {
                        "authorId": "31643513",
                        "name": "Pu Zhao"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "66603575",
                        "name": "Min-Fong Hong"
                    },
                    {
                        "authorId": "2136922252",
                        "name": "Yanzhi Wang"
                    },
                    {
                        "authorId": "2118464654",
                        "name": "Sijia Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ER networks rewired with DST: Test Accuracies for an ER(p) VGG16 with a fixed mask and after rewiring edges with RiGL (Evci et al., 2020; Liu et al., 2021) on CIFAR10.",
                "For sparse to sparse training with DST, we us weight magnitude as importance score for pruning (with prune rate 0.5) and gradient for growth.",
                "99 an ER network of some initial sparsity and further pruned to a final sparsity (initial \u2192 final) while modifying the mask using the RiGL (Evci et al., 2020) algorithm.",
                "In the DST experiments, we use the same setup as random pruning, and modify the mask every 100 iterations.",
                "These can be partially remedied by using the rewiring strategy of Dynamic Sparse Training (DST) (Evci et al., 2020).",
                "61 In addition to the rewiring experiments shown in Table 2, we use Dynamical Sparse Training to prune an already sparse ER network to a higher sparsity and see if this can achieve the same performance as performing DST starting from a denser network.",
                "However, it further relies on edge rewiring steps that sometimes require the computation of gradients of the corresponding dense network (Evci et al., 2020).",
                "Our main results could also be interpreted as theoretical justification for Dynamic Sparse Training (DST) (Evci et al., 2020; Liu et al., 2021.; Bellec et al., 2018), which prunes random networks of moderate sparsity.",
                "These can be partially remedied by using the rewiring strategy of Dynamic Sparse Training (DST) (Evci et al., 2020",
                "Sparse to sparse training with DST Final test accuracy for VGG16 on CIFAR10 is reported where the model is initialized with an ER network of some initial sparsity and further pruned to a final sparsity (initial \u2192 final) while modifying the mask using the RiGL (Evci et al., 2020) algorithm.",
                "Our main results could also be interpreted as theoretical justification for Dynamic Sparse Training (DST) (Evci et al., 2020; Liu et al., 2021.",
                "This shortcoming could be potentially addressed by targeted rewiring of random edges with Dynamical Sparse Training (DST) that starts pruning from an ER network (Liu et al., 2021.",
                "Dynamical Sparse Training To improve randomly pruned networks at extremely high sparsities, we employ the RiGL algorithm (Evci et al., 2020) to obtain Table 2."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "11422fff4d42b70c31af69381ff32d35031c939d",
                "externalIds": {
                    "DBLP": "conf/icml/GadhikarMB23",
                    "ArXiv": "2210.02412",
                    "CorpusId": 258987596
                },
                "corpusId": 258987596,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/11422fff4d42b70c31af69381ff32d35031c939d",
                "title": "Why Random Pruning Is All We Need to Start Sparse",
                "abstract": "Random masks define surprisingly effective sparse neural network models, as has been shown empirically. The resulting sparse networks can often compete with dense architectures and state-of-the-art lottery ticket pruning algorithms, even though they do not rely on computationally expensive prune-train iterations and can be drawn initially without significant computational overhead. We offer a theoretical explanation of how random masks can approximate arbitrary target networks if they are wider by a logarithmic factor in the inverse sparsity $1 / \\log(1/\\text{sparsity})$. This overparameterization factor is necessary at least for 3-layer random networks, which elucidates the observed degrading performance of random networks at higher sparsity. At moderate to high sparsity levels, however, our results imply that sparser networks are contained within random source networks so that any dense-to-sparse training scheme can be turned into a computationally more efficient sparse-to-sparse one by constraining the search to a fixed random mask. We demonstrate the feasibility of this approach in experiments for different pruning methods and propose particularly effective choices of initial layer-wise sparsity ratios of the random source network. As a special case, we show theoretically and experimentally that random source networks also contain strong lottery tickets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2048027747",
                        "name": "Advait Gadhikar"
                    },
                    {
                        "authorId": "79760097",
                        "name": "Sohom Mukherjee"
                    },
                    {
                        "authorId": "8813467",
                        "name": "R. Burkholz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similarly, RigL 161 updates the topology of a sparse DNN during training by 162 considering a measure based on the magnitude of parame- 163 ters and infrequent gradient calculations.",
                "As mentioned, we select GMP [36], AMC [21], 448 GSM [27], DSR [33], SM [34], DNW [37], RigL [35], 449 DPF [29], and STR [38] as our competitors.",
                "Over 154 this dataset, the most prominent results are delivered by 155 DSR [33], SM [34], RigL [35], GMP [36], DNW [37], and 156 STR [38] algorithms."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4dcf4226560b2369d1100d2180ea8e4d7537eed5",
                "externalIds": {
                    "DBLP": "journals/tcsv/CamciGWL22",
                    "DOI": "10.1109/TCSVT.2022.3167951",
                    "CorpusId": 248255490
                },
                "corpusId": 248255490,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4dcf4226560b2369d1100d2180ea8e4d7537eed5",
                "title": "QLP: Deep Q-Learning for Pruning Deep Neural Networks",
                "abstract": "We present a novel, deep Q-learning based method, QLP, for pruning deep neural networks (DNNs). Given a DNN, our method intelligently determines favorable layer-wise sparsity ratios, which are then implemented via unstructured, magnitude-based, weight pruning. In contrast to previous reinforcement learning (RL) based pruning methods, our method is not forced to prune a DNN within a single, sequential pass from the first layer to the last. It visits each layer multiple times and prunes them little by little at each visit, achieving superior granular pruning. Moreover, our method is not restricted to a subset of actions within the feasible action space. It has the flexibility to execute a whole range of sparsity ratios (0% - 100%) for each layer. This enables aggressive pruning without compromising accuracy. Furthermore, our method does not require a complex state definition; it features a simple, generic definition that is composed of only the index and the density of the layers, which leads to less computational demand while observing the state at each interaction. Lastly, our method utilizes a carefully designed curriculum that enables learning targeted policies for each sparsity regime, which helps to deliver better accuracy, especially at high sparsity levels. We conduct batched performance tests at compelling sparsity levels (up to 98%), present extensive ablation studies to justify our RL-related design choices, and compare our method with the state-of-the-art, including RL-based and other pruning methods. Our method sets the new state-of-the-art results in most of the experiments with ResNet-32 and ResNet-56 over CIFAR-10 dataset as well as ResNet-50 and MobileNet-v1 over ILSVRC2012 (ImageNet) dataset.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7197691",
                        "name": "Efe Camci"
                    },
                    {
                        "authorId": "2109835363",
                        "name": "Manas Gupta"
                    },
                    {
                        "authorId": "2144152506",
                        "name": "Min Wu"
                    },
                    {
                        "authorId": "2143453415",
                        "name": "Jie Lin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We compare Global MP with various popular SOTA algorithms that are well known for pruning, such as SNIP [42], SM [7], DSR [6], DPF [9], GMP [18], DNW [19], RigL [8], and STR [10].",
                "Another popular SOTA technique, RigL [8], also iteratively prunes and re-grows weights every few iterations.",
                "Similarly, many SOTA papers do not use Global MP for benchmarking and miss out on capturing its remarkable performance [8], [10], [18], [2], [19].",
                "Using this dataset, we compare Global MP with SOTA algorithms like GMP [18], DSR [6], DNW [19], SM [7], RigL [8], WoodFisher [63], MFAC [62], DPF [9], and STR [10].",
                "Furthermore, many SOTA algorithms miss out on benchmarking their algorithms against Global MP and hence are unable to capture its efficacy [8], [10], [18], [2], [19].",
                "Many pruning techniques have been developed over the years, which use first or second order derivatives [40], [41], [1], gradient based methods [42], [43], [44], sensitivity to or feedback from some objective function [9], [45], [46], [47], [48], distance or similarity measures [49], Bayesian optimisation [50], regularization-based techniques [51], [10], [52], [53], [54], and magnitude-based criterion [8], [17], [18],",
                "Rigging the Lottery (RigL) [8] allocates sparsity based on the number of parameters in a layer."
            ],
            "isInfluential": true,
            "intents": [],
            "citingPaper": {
                "paperId": "b6408c7dc8ce8c386197990d90ccb528419db25b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-14624",
                    "ArXiv": "2209.14624",
                    "DOI": "10.48550/arXiv.2209.14624",
                    "CorpusId": 252595918
                },
                "corpusId": 252595918,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b6408c7dc8ce8c386197990d90ccb528419db25b",
                "title": "Is Complexity Required for Neural Network Pruning? A Case Study on Global Magnitude Pruning",
                "abstract": "Pruning neural networks has become popular in the last decade when it was shown that a large number of weights can be safely removed from modern neural networks without compromising accuracy. Numerous pruning methods have been proposed since then, each claiming to be better than the previous. Many state-of-the-art (SOTA) techniques today rely on complex pruning methodologies utilizing importance scores, getting feedback through back-propagation or having heuristics-based pruning rules amongst others. In this work, we question whether this pattern of introducing complexity is really necessary to achieve better pruning results. We benchmark these SOTA techniques against a naive pruning baseline, namely, Global Magnitude Pruning (Global MP). Global MP ranks weights in order of their magnitudes and prunes the smallest ones. Hence, in its vanilla form, it is one of the simplest pruning techniques. Surprisingly, we find that vanilla Global MP outperforms all the other SOTA techniques and achieves a new SOTA result. It also achieves promising performance on FLOPs sparsification, which we find is enhanced, when pruning is conducted in a gradual fashion. We also find that Global MP is generalizable across tasks, datasets, and models with superior performance. Moreover, a common issue that many pruning algorithms run into at high sparsity rates, namely, layer-collapse, can be easily fixed in Global MP by setting a minimum threshold of weights to be retained in each layer. Lastly, unlike many other SOTA techniques, Global MP does not require any additional algorithm specific hyper-parameters and is very straightforward to tune and implement. We showcase our findings on various models (WRN-28-8, ResNet-32, ResNet-50, MobileNet-V1 and FastGRNN) and multiple datasets (CIFAR-10, ImageNet and HAR-2). Code is available at https://github.com/manasgupta-1/GlobalMP.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109835363",
                        "name": "Manas Gupta"
                    },
                    {
                        "authorId": "7197691",
                        "name": "Efe Camci"
                    },
                    {
                        "authorId": "2186404613",
                        "name": "Vishandi Rudy Keneta"
                    },
                    {
                        "authorId": "2186403073",
                        "name": "Abhishek Vaidyanathan"
                    },
                    {
                        "authorId": "2186404868",
                        "name": "Ritwik Kanodia"
                    },
                    {
                        "authorId": "2121484",
                        "name": "Chuan-Sheng Foo"
                    },
                    {
                        "authorId": "2186403431",
                        "name": "Wu Min"
                    },
                    {
                        "authorId": "2056595772",
                        "name": "Lin Jie"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", SNIP [2], GraSP [3], RigL [5], ITOP [6], SET [19], DSR [4], and MEST [1], is provided in Tab.",
                "Research on the sparse training strategies [47, 48, 49] can be categorized into fixed-mask and sparsemask sparse training, where the former aims to make it feasible that the training of the pruned models can be implemented on edge devices [2, 3, 50, 51, 52] and the latter studies how to reduce memory costs along with the computation during training [53, 19, 4, 54, 5].",
                "We compare the accuracy, training FLOPs, and memory costs of our framework with the most representative sparse training works [2, 3, 53, 54, 4, 19, 5, 1] at different sparsity ratios.",
                "The baseline results of \u201cDST methods Min.\u201d only consider the minimum memory costs requirement for DST methods [2, 3, 53, 54, 4, 19, 5, 1, 6], which ignores the memory overhead such as the periodic dense back-propagation in RigL [5], dense sparse structure searching at initialization in [2, 3], and the soft memory bound in MEST [1].",
                "\u201d only consider the minimum memory costs requirement for DST methods [2, 3, 53, 54, 4, 19, 5, 1, 6], which ignores the memory overhead such as the periodic dense back-propagation in RigL [5], dense sparse structure searching at initialization in [2, 3], and the soft memory bound in MEST [1].",
                "Another category is Dynamic Sparse Training (DST), which usually starts the training from a randomly selected sparse structure [4, 5].",
                "The Dynamic Sparse Training (DST) method shows its superior performance by continuously changing its sparse model structure during training to search for a better sparse architecture, making it a desirable sparse training paradigm [5].",
                "The comparison of key features between SpFDE and other representative sparse training works, i.e., SNIP [2], GraSP [3], RigL [5], ITOP [6], SET [19], DSR [4], and MEST [1], is provided in Tab.",
                "The reason is that sparse training algorithms (e.g., MEST and RigL) may force less important weights/locations to be changed during sparse training, regardless of whether the sparse structure has already been stabilized."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "5bdaa55068f5e19ee1d3e63f21bf8a27eff2b8ae",
                "externalIds": {
                    "ArXiv": "2209.11204",
                    "DBLP": "journals/corr/abs-2209-11204",
                    "DOI": "10.48550/arXiv.2209.11204",
                    "CorpusId": 252438973
                },
                "corpusId": 252438973,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5bdaa55068f5e19ee1d3e63f21bf8a27eff2b8ae",
                "title": "Layer Freezing & Data Sieving: Missing Pieces of a Generic Framework for Sparse Training",
                "abstract": "Recently, sparse training has emerged as a promising paradigm for efficient deep learning on edge devices. The current research mainly devotes efforts to reducing training costs by further increasing model sparsity. However, increasing sparsity is not always ideal since it will inevitably introduce severe accuracy degradation at an extremely high sparsity level. This paper intends to explore other possible directions to effectively and efficiently reduce sparse training costs while preserving accuracy. To this end, we investigate two techniques, namely, layer freezing and data sieving. First, the layer freezing approach has shown its success in dense model training and fine-tuning, yet it has never been adopted in the sparse training domain. Nevertheless, the unique characteristics of sparse training may hinder the incorporation of layer freezing techniques. Therefore, we analyze the feasibility and potentiality of using the layer freezing technique in sparse training and find it has the potential to save considerable training costs. Second, we propose a data sieving method for dataset-efficient training, which further reduces training costs by ensuring only a partial dataset is used throughout the entire training process. We show that both techniques can be well incorporated into the sparse training algorithm to form a generic framework, which we dub SpFDE. Our extensive experiments demonstrate that SpFDE can significantly reduce training costs while preserving accuracy from three dimensions: weight sparsity, layer freezing, and dataset sieving.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9347641",
                        "name": "Geng Yuan"
                    },
                    {
                        "authorId": "1527091497",
                        "name": "Yanyu Li"
                    },
                    {
                        "authorId": "39541577",
                        "name": "Sheng Li"
                    },
                    {
                        "authorId": "32409528",
                        "name": "Zhenglun Kong"
                    },
                    {
                        "authorId": "145582202",
                        "name": "S. Tulyakov"
                    },
                    {
                        "authorId": "8573809",
                        "name": "Xulong Tang"
                    },
                    {
                        "authorId": "2136922252",
                        "name": "Yanzhi Wang"
                    },
                    {
                        "authorId": "2111473627",
                        "name": "Jian Ren"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also adapt representative sparse training methods (SNIP (34), RigL (19)) to the CL setting by combining them with DER++ (SNIP-DER++, RigL-DER++).",
                "We also use the best hyperparameter setting reported in (8; 56) for CL methods, and in (19; 34) for CL-adapted sparse training methods.",
                "A straightforward idea is to directly combine existing sparse training methods, such as SNIP (34), RigL (19), with a rehearsal buffer under the CL setting.",
                "data from a single training task (19; 66), TDM considers also the importance of weights w.",
                "Recently, another stream of work, sparse training (4; 19; 34) has emerged as a new training trend to achieve training acceleration, which embraces the promising training-on-the-edge paradigm.",
                "Moreover, we evaluate the training FLOPs (19), and memory footprint (66) (including feature map pixels and model parameters during training) to demonstrate the efficiency of each method.",
                "To overcome these drawbacks, dynamic mask methods (4; 16; 19; 44; 45) adjust the sparsity topology during training while maintaining low memory footprint."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "894d1ac91443eb7c28dc52116c93d5436a0acecf",
                "externalIds": {
                    "DBLP": "conf/nips/000200YNJRIWD22",
                    "ArXiv": "2209.09476",
                    "DOI": "10.48550/arXiv.2209.09476",
                    "CorpusId": 252383174
                },
                "corpusId": 252383174,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/894d1ac91443eb7c28dc52116c93d5436a0acecf",
                "title": "SparCL: Sparse Continual Learning on the Edge",
                "abstract": "Existing work in continual learning (CL) focuses on mitigating catastrophic forgetting, i.e., model performance deterioration on past tasks when learning a new task. However, the training efficiency of a CL system is under-investigated, which limits the real-world application of CL systems under resource-limited scenarios. In this work, we propose a novel framework called Sparse Continual Learning(SparCL), which is the first study that leverages sparsity to enable cost-effective continual learning on edge devices. SparCL achieves both training acceleration and accuracy preservation through the synergy of three aspects: weight sparsity, data efficiency, and gradient sparsity. Specifically, we propose task-aware dynamic masking (TDM) to learn a sparse network throughout the entire CL process, dynamic data removal (DDR) to remove less informative training data, and dynamic gradient masking (DGM) to sparsify the gradient updates. Each of them not only improves efficiency, but also further mitigates catastrophic forgetting. SparCL consistently improves the training efficiency of existing state-of-the-art (SOTA) CL methods by at most 23X less training FLOPs, and, surprisingly, further improves the SOTA accuracy by at most 1.7%. SparCL also outperforms competitive baselines obtained from adapting SOTA sparse training methods to the CL setting in both efficiency and accuracy. We also evaluate the effectiveness of SparCL on a real mobile phone, further indicating the practical potential of our method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2135785111",
                        "name": "Zifeng Wang"
                    },
                    {
                        "authorId": "2949135",
                        "name": "Zheng Zhan"
                    },
                    {
                        "authorId": "2114981469",
                        "name": "Yifan Gong"
                    },
                    {
                        "authorId": "9347641",
                        "name": "Geng Yuan"
                    },
                    {
                        "authorId": "48643324",
                        "name": "Wei Niu"
                    },
                    {
                        "authorId": "2070398451",
                        "name": "T. Jian"
                    },
                    {
                        "authorId": "2042633100",
                        "name": "Bin Ren"
                    },
                    {
                        "authorId": "1776006",
                        "name": "Stratis Ioannidis"
                    },
                    {
                        "authorId": "2136922252",
                        "name": "Yanzhi Wang"
                    },
                    {
                        "authorId": "153140737",
                        "name": "Jennifer G. Dy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The second includes methods which prune before training like Single-shot Network Pruning (SNIP) [8] and Gradient Signal Preservation (GraSP) [9], or prune during training like Sparse Momentum (SM) [10] and Rigged Lottery (RigL) [11].",
                "\u2022 Rigged Lottery (RigL) [11] saves on the computational cost of SM by defining the sparsities of each layer beforehand, then operating layer by layer instead of across all layers."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "688d5bd53be1c02fe1d313ebc2d49eb382b175bb",
                "externalIds": {
                    "ArXiv": "2209.10890",
                    "DBLP": "conf/interspeech/LamZCS22",
                    "DOI": "10.21437/Interspeech.2022-10626",
                    "CorpusId": 252343017
                },
                "corpusId": 252343017,
                "publicationVenue": {
                    "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
                    "name": "Interspeech",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Int Speech Commun Assoc",
                        "INTERSPEECH",
                        "Conference of the International Speech Communication Association"
                    ],
                    "issn": "2308-457X",
                    "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
                    "alternate_urls": [
                        "http://www.isca-speech.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/688d5bd53be1c02fe1d313ebc2d49eb382b175bb",
                "title": "EPIC TTS Models: Empirical Pruning Investigations Characterizing Text-To-Speech Models",
                "abstract": "Neural models are known to be over-parameterized, and recent work has shown that sparse text-to-speech (TTS) models can outperform dense models. Although a plethora of sparse methods has been proposed for other domains, such methods have rarely been applied in TTS. In this work, we seek to answer the question: what are the characteristics of selected sparse techniques on the performance and model complexity? We compare a Tacotron2 baseline and the results of applying five techniques. We then evaluate the performance via the factors of naturalness, intelligibility and prosody, while reporting model size and training time. Complementary to prior research, we find that pruning before or during training can achieve similar performance to pruning after training and can be trained much faster, while removing entire neurons degrades performance much more than removing parameters. To our best knowledge, this is the first work that compares sparsity paradigms in text-to-speech synthesis.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2065835327",
                        "name": "Perry Lam"
                    },
                    {
                        "authorId": "3069527",
                        "name": "Huayun Zhang"
                    },
                    {
                        "authorId": "2185019",
                        "name": "Nancy F. Chen"
                    },
                    {
                        "authorId": "3450504",
                        "name": "Berrak Sisman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This recipe has been proven to be effective as shown in the previous experiments and many prior works [33, 40, 12, 28, 11, 18, 17, 21, 36, 52, 60, 15, 37, 38, 8, 39, 23, 9].",
                "Some other metrics such as gradientbased [54, 9], Hessian based [27], connection sensitivity [28], and salient-based [35, 28] are also used.",
                "Unstructured Sparsity prunes the model without any sparsity pattern constraint [43, 17, 28, 12, 14, 60, 18, 31, 51, 6, 14, 26, 9, 4, 34].",
                "4) From-scratch while learning sparsity pattern (Figure 1i) [51, 6, 14, 26, 9, 4, 34, 58, 10], which trains a sparse model from scratch while learning sparsity patterns simultaneously.",
                "Another line of effort in the DNN community is to propose different methods to compress the models, such as quantization [45, 25, 55, 57, 53], sparsification [11, 18, 17, 21, 36, 52, 60, 15, 37, 38, 8, 39, 23, 9], and distillation [44, 22, 46, 49].",
                "2) Fine-tuning with iterative pruning (Figure 1g) [11, 18, 17, 21, 36, 52, 60, 15, 37, 38, 8, 39, 23, 9], which trains a dense model and then iterates between pruning and re-training."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "citingPaper": {
                "paperId": "a10dd5e0f74a14ca0e7c37a121bf77970f932e32",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-07617",
                    "ArXiv": "2209.07617",
                    "DOI": "10.48550/arXiv.2209.07617",
                    "CorpusId": 252355271
                },
                "corpusId": 252355271,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a10dd5e0f74a14ca0e7c37a121bf77970f932e32",
                "title": "Training Recipe for N: M Structured Sparsity with Decaying Pruning Mask",
                "abstract": "Sparsity has become one of the promising methods to compress and accelerate Deep Neural Networks (DNNs). Among different categories of sparsity, structured sparsity has gained more attention due to its efficient execution on modern accelerators. Particularly, N:M sparsity is attractive because there are already hardware accelerator architectures that can leverage certain forms of N:M structured sparsity to yield higher compute-efficiency. In this work, we focus on N:M sparsity and extensively study and evaluate various training recipes for N:M sparsity in terms of the trade-off between model accuracy and compute cost (FLOPs). Building upon this study, we propose two new decay-based pruning methods, namely\"pruning mask decay\"and\"sparse structure decay\". Our evaluations indicate that these proposed methods consistently deliver state-of-the-art (SOTA) model accuracy, comparable to unstructured sparsity, on a Transformer-based model for a translation task. The increase in the accuracy of the sparse model using the new training recipes comes at the cost of marginal increase in the total training compute (FLOPs).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "27057088",
                        "name": "Sheng-Chun Kao"
                    },
                    {
                        "authorId": "2112229",
                        "name": "A. Yazdanbakhsh"
                    },
                    {
                        "authorId": "1929462",
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "authorId": "3504647",
                        "name": "Shivani Agrawal"
                    },
                    {
                        "authorId": "3399348",
                        "name": "Utku Evci"
                    },
                    {
                        "authorId": "145984583",
                        "name": "T. Krishna"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For sparse training, we follow a similar drop and grow method that was proposed in RigL [47] to explore the sparse model architecture during the sparse training process.",
                "The initial selection of A element could be a random process [47] or restricted to the top-K proportion of weights by magnitude [64].",
                "Then, for each training epoch, the dense model consumes 3fD FLOPs for forward and backward path [47], sparse model consumes 3fS FLOPs for forward and backward path.",
                "RigL [47] grew back the weights with top-k largest gradients.",
                "The second one, sparse training [47], gives up the hypothesis that the dense model could guide the sparsification process [19] and directly trains a model with fixed sparsity from scratch."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "0f6e84d48014921f876fc96047e1b0426f53da0e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-04766",
                    "ArXiv": "2209.04766",
                    "DOI": "10.1109/ICCD56317.2022.00048",
                    "CorpusId": 252199460
                },
                "corpusId": 252199460,
                "publicationVenue": {
                    "id": "77bd93b1-91df-4192-9b10-d7698cca4768",
                    "name": "ICCD",
                    "type": "journal",
                    "alternate_names": [
                        "International Conference on Computer Design",
                        "Int Conf Comput Des",
                        "CDES"
                    ],
                    "issn": "2622-5611",
                    "url": "http://iccd.asia/ojs/index.php/jiccd/index",
                    "alternate_urls": [
                        "http://www.wikicfp.com/cfp/program?id=1304"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0f6e84d48014921f876fc96047e1b0426f53da0e",
                "title": "Towards Sparsification of Graph Neural Networks",
                "abstract": "As real-world graphs expand in size, larger GNN models with billions of parameters are deployed. High parameter count in such models makes training and inference on graphs expensive and challenging. To reduce the computational and memory costs of GNNs, optimization methods such as pruning the redundant nodes and edges in input graphs have been commonly adopted. However, model compression, which directly targets the sparsification of model layers, has been mostly limited to traditional Deep Neural Networks (DNNs) used for tasks such as image classification and object detection. In this paper, we utilize two state-of-the-art model compression methods (1) train and prune and (2) sparse training for the sparsification of weight layers in GNNs. We evaluate and compare the efficiency of both methods in terms of accuracy, training sparsity, and training FLOPs on real-world graphs. Our experimental results show that on the ia-email, wiki-talk, and stackoverflow datasets for link prediction, sparse training with much lower training FLOPs achieves a comparable accuracy with the train and prune method. On the brain dataset for node classification, sparse training uses a lower number FLOPs Oess than 1/7 FLOPs of train and prune method) and preserves a much better accuracy performance under extreme model sparsity. Our model sparsification code is publicly available on GitHubl1.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144490597",
                        "name": "Hongwu Peng"
                    },
                    {
                        "authorId": "1753362334",
                        "name": "Deniz Gurevin"
                    },
                    {
                        "authorId": "2122804649",
                        "name": "Shaoyi Huang"
                    },
                    {
                        "authorId": "4444498",
                        "name": "Tong Geng"
                    },
                    {
                        "authorId": "1937259",
                        "name": "Weiwen Jiang"
                    },
                    {
                        "authorId": "145704836",
                        "name": "O. Khan"
                    },
                    {
                        "authorId": "2881873",
                        "name": "Caiwen Ding"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These are a special case of sparse neural networks (Gale et al., 2019; Dettmers and Zettlemoyer, 2019; Evci et al., 2020) which are similar in that they only use a subset of parameters, but differ because they have potentially irregular sparsity patterns."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ca086f4c09cf8de705830ac2b70951737fab93ca",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-01667",
                    "ArXiv": "2209.01667",
                    "DOI": "10.48550/arXiv.2209.01667",
                    "CorpusId": 252089870
                },
                "corpusId": 252089870,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ca086f4c09cf8de705830ac2b70951737fab93ca",
                "title": "A Review of Sparse Expert Models in Deep Learning",
                "abstract": "Sparse expert models are a thirty-year old concept re-emerging as a popular architecture in deep learning. This class of architecture encompasses Mixture-of-Experts, Switch Transformers, Routing Networks, BASE layers, and others, all with the unifying idea that each example is acted on by a subset of the parameters. By doing so, the degree of sparsity decouples the parameter count from the compute per example allowing for extremely large, but efficient models. The resulting models have demonstrated significant improvements across diverse domains such as natural language processing, computer vision, and speech recognition. We review the concept of sparse expert models, provide a basic description of the common algorithms, contextualize the advances in the deep learning era, and conclude by highlighting areas for future work.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "26958176",
                        "name": "W. Fedus"
                    },
                    {
                        "authorId": "48448318",
                        "name": "J. Dean"
                    },
                    {
                        "authorId": "2368067",
                        "name": "Barret Zoph"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In computer vision tasks, it has been demonstrated that different CNN layers have different redundancy, and rule- or heuristic-based unstructured pruning [9, 18, 31] may be used to find the appropriate layer-wise sparsities to reduce their redundancy."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "2fd1194f6a217403388d640a823503b33cefac05",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-00625",
                    "ArXiv": "2209.00625",
                    "DOI": "10.1145/3511808.3557139",
                    "CorpusId": 251979775
                },
                "corpusId": 251979775,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/2fd1194f6a217403388d640a823503b33cefac05",
                "title": "SwiftPruner: Reinforced Evolutionary Pruning for Efficient Ad Relevance",
                "abstract": "Ad relevance modeling plays a critical role in online advertising systems including Microsoft Bing. To leverage powerful transformers like BERT in this low-latency setting, many existing approaches perform ad-side computations offline. While efficient, these approaches are unable to serve cold start ads, resulting in poor relevance predictions for such ads. This work aims to design a new, low-latency BERT via structured pruning to empower real-time online inference for cold start ads relevance on a CPU platform. Our challenge is that previous methods typically prune all layers of the transformer to a high, uniform sparsity, thereby producing models which cannot achieve satisfactory inference speed with an acceptable accuracy. In this paper, we propose SwiftPruner - an efficient framework that leverages evolution-based search to automatically find the best-performing layer-wise sparse BERT model under the desired latency constraint. Different from existing evolution algorithms that conduct random mutations, we propose a reinforced mutator with a latency-aware multi-objective reward to conduct better mutations for efficiently searching the large space of layer-wise sparse models. Extensive experiments demonstrate that our method consistently achieves higher ROC AUC and lower latency than the uniform sparse baseline and state-of-the-art search methods. Remarkably, under our latency requirement of 1900us on CPU, SwiftPruner achieves a 0.86% higher AUC than the state-of-the-art uniform sparse baseline for BERT-Mini on a large scale real-world dataset. Online A/B testing shows that our model also achieves a significant 11.7% cut in the ratio of defective cold start ads with satisfactory real-time serving latency.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48571328",
                        "name": "L. Zhang"
                    },
                    {
                        "authorId": "4133298",
                        "name": "Youkow Homma"
                    },
                    {
                        "authorId": "46394401",
                        "name": "Yujing Wang"
                    },
                    {
                        "authorId": "1390606776",
                        "name": "Min Wu"
                    },
                    {
                        "authorId": "2168609907",
                        "name": "Mao Yang"
                    },
                    {
                        "authorId": "2124601065",
                        "name": "Ruofei Zhang"
                    },
                    {
                        "authorId": "2137096570",
                        "name": "Ting Cao"
                    },
                    {
                        "authorId": null,
                        "name": "Wei Shen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In areas such as machine learning and neural networks, sparsity is universally present, a phenomenon primarily resulting from two aspects: (1) Activation sparsity from the activation function ReLU, roughly 50% sparsity [27]; (2) weight sparsity that derived from the pruning algorithms, along with sparsity training algorithms, up to 90% sparsity [7].",
                "1 INTRODUCTION Sparse matrix computation is widely employed in various applications, including scientific computation [14, 18, 26], neural network training [6, 7, 19, 24, 25], language processing models [4, 34] and so on.",
                "The sparse training method RigL [7] is applied in our experiment to ensure that the model weights are sparse."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "b0a3a297180674ca01108d0bacd93d2d62938450",
                "externalIds": {
                    "DBLP": "conf/icpp/TangWCSYFG022",
                    "DOI": "10.1145/3545008.3545053",
                    "CorpusId": 255775690
                },
                "corpusId": 255775690,
                "publicationVenue": {
                    "id": "29df4b17-9a16-4a4c-94a6-002f52e628b4",
                    "name": "International Conference on Parallel Processing",
                    "type": "conference",
                    "alternate_names": [
                        "ICPP",
                        "Int Conf Parallel Process",
                        "IEEE Int Conf Pulsed Power",
                        "IEEE International Conference on Pulsed Power"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1447"
                },
                "url": "https://www.semanticscholar.org/paper/b0a3a297180674ca01108d0bacd93d2d62938450",
                "title": "Mentha: Enabling Sparse-Packing Computation on Systolic Arrays",
                "abstract": "Generalized Sparse Matrix-Matrix Multiplication (SpGEMM) is a critical kernel in domains like graph analytic and scientific computation. As a kind of classical special-purpose architecture, systolic arrays were first used for complex computing problems, e.g., matrix multiplication. However, classical systolic arrays are not efficient enough when handling sparse matrices due to the fact that the PEs containing zero-valued entries perform unnecessary operations that do not contribute to the result. Accordingly, in this paper, we propose Mentha, a framework that enables systolic arrays to accelerate sparse matrix computation by employing a sparse-packing algorithm suitable for various dataflow of systolic array. Firstly, Mentha supports both online and offline methods. By packing the rows or columns of the sparse matrix, the zero-valued items in the matrix are significantly reduced and the density of the matrix is improved. In addition, acceleration benefits can be obtained by the adaptation scheme even with limited resources. Moreover, we reconfigure PEs in systolic arrays at a low cost (1.28x in area, 1.21x in power) and find that our method outperforms TPU-like systolic arrays by 1.2~3.3x in terms of SpMM and 1.3~4.4x in terms of SpGEMM when dealing with moderately sparse matrices (sparsity < 0.9), while its performance is at least 9.7x better than cuSPARSE. Furthermore, experimental results show a FLOPs reduction of roughly 3.4x in the neural network.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2113729271",
                        "name": "Minjin Tang"
                    },
                    {
                        "authorId": "2165490242",
                        "name": "Mei Wen"
                    },
                    {
                        "authorId": "2165888858",
                        "name": "Yasong Cao"
                    },
                    {
                        "authorId": "2197895",
                        "name": "Junzhong Shen"
                    },
                    {
                        "authorId": "2167561891",
                        "name": "Jianchao Yang"
                    },
                    {
                        "authorId": "20801572",
                        "name": "Jiawei Fei"
                    },
                    {
                        "authorId": "144270189",
                        "name": "Yang Guo"
                    },
                    {
                        "authorId": "2200556038",
                        "name": "Sheng Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "More recently, sparse learning Evci et al. (2020); Kundu et al. (2021b); Dettmers & Zettlemoyer (2019); Raihan & Aamodt (2020); Kundu et al.",
                "In particular, recently proposed sparse learning (SL) strategies Evci et al. (2020); Kundu et al. (2021b); Mocanu et al. (2018); Dettmers & Zettlemoyer (2019); Raihan & Aamodt (2020) effectively train weights and associated binary sparse masks to allow only a fraction of model parameters to be updated during training, potentially enabling the lucrative reduction in both the training time and compute cost Qiu et al. (2021); Raihan & Aamodt (2020), while creating a model to meet a target parameter density denoted as d, and is able to yield accuracy close to that of the unpruned baseline.",
                "In particular, recently proposed sparse learning (SL) strategies Evci et al. (2020); Kundu et al. (2021b); Mocanu et al. (2018); Dettmers & Zettlemoyer (2019); Raihan & Aamodt (2020) effectively train weights and associated binary sparse masks to allow only a fraction of model parameters to be updated during training, potentially enabling the lucrative reduction in both the training time and compute cost Qiu et al.",
                "In particular, recently proposed sparse learning (SL) strategies Evci et al. (2020); Kundu et al.",
                "We now compare our SPDST mask initialization, with that of parameter density distribution evaluated via ERK+ Huang et al. (2022); Evci et al. (2020).",
                "More recently, sparse learning Evci et al. (2020); Kundu et al.",
                "In particular, recently proposed sparse learning (SL) strategies Evci et al. (2020); Kundu et al. (2021b); Mocanu et al. (2018); Dettmers & Zettlemoyer (2019); Raihan & Aamodt (2020) effectively train weights and associated binary sparse masks to allow only a fraction of model parameters to be\u2026",
                "In particular, recently proposed sparse learning (SL) strategies Evci et al. (2020); Kundu et al. (2021b); Mocanu et al.",
                "FedDST Bibikar et al. (2021), on the other hand, leveraged the idea of RigL Evci et al. (2020) to perform sparse learning of the clients, relied on a large number of local epochs to avoid gradient noise, and focused primarily on only highly non-IID data without targeting ultra-low density d.",
                "More recently, sparse learning Evci et al. (2020); Kundu et al. (2021b); Dettmers & Zettlemoyer (2019); Raihan & Aamodt (2020); Kundu et al. (2020), a popular form of model pruning, has gained significant traction as it can yield FLOPs advantage even during training."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "79d20e40d60968bf9c64a58d44e1a3dd40bfb7ba",
                "externalIds": {
                    "ArXiv": "2208.13092",
                    "CorpusId": 253107734
                },
                "corpusId": 253107734,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/79d20e40d60968bf9c64a58d44e1a3dd40bfb7ba",
                "title": "Lottery Aware Sparsity Hunting: Enabling Federated Learning on Resource-Limited Edge",
                "abstract": "Limited computation and communication capabilities of clients pose significant challenges in federated learning (FL) over resource-limited edge nodes. A potential solution to this problem is to deploy off-the-shelf sparse learning algorithms that train a binary sparse mask on each client with the expectation of training a consistent sparse server mask yielding sparse weight tensors. However, as we investigate in this paper, such naive deployments result in a significant drop in accuracy compared to FL with dense models, especially for clients with limited resource budgets. In particular, our investigations reveal a serious lack of consensus among the trained sparsity masks on clients, which prevents convergence for the server mask and potentially leads to a substantial drop in model performance. Based on such key observations, we propose \\textit{federated lottery aware sparsity hunting} (FLASH), a unified sparse learning framework to make the server win a lottery in terms of yielding a sparse sub-model, able to maintain classification performance under highly resource-limited client settings. Moreover, to support FL on different devices requiring different parameter density, we leverage our findings to present \\textit{hetero-FLASH}, where clients can have different target sparsity budgets based on their device resource limits. Experimental evaluations with multiple models on various datasets (both IID and non-IID) show superiority of our models in closing the gap with unpruned baseline while yielding up to $\\mathord{\\sim}10.1\\%$ improved accuracy with $\\mathord{\\sim}10.26\\times$ fewer communication costs, compared to existing alternatives, at similar hyperparameter settings.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2098816244",
                        "name": "Sara Babakniya"
                    },
                    {
                        "authorId": "2965493",
                        "name": "Souvik Kundu"
                    },
                    {
                        "authorId": "47592370",
                        "name": "Saurav Prakash"
                    },
                    {
                        "authorId": "144829346",
                        "name": "Yue Niu"
                    },
                    {
                        "authorId": "121011351",
                        "name": "S. Avestimehr"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The two growing strategies are gradient-based growth [12, 19]",
                "Follow-up works further introduce weight redistribution [12, 48], gradient-based weight growth [12, 19], and extra weights update in the backward pass [27, 54] to improve the sparse training performance."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ded0f3ac4c904f0f91bb937bb2bfd5fcb591c777",
                "externalIds": {
                    "DBLP": "conf/kbse/ZhuW022",
                    "ArXiv": "2208.05969",
                    "DOI": "10.1145/3551349.3556906",
                    "CorpusId": 251554822
                },
                "corpusId": 251554822,
                "publicationVenue": {
                    "id": "1c2ab05c-7d69-465e-929d-0920857aedce",
                    "name": "International Conference on Automated Software Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "Autom Softw Eng",
                        "ASE",
                        "Automated Software Engineering",
                        "Int Conf Autom Softw Eng"
                    ],
                    "url": "http://ase.informatik.uni-essen.de/"
                },
                "url": "https://www.semanticscholar.org/paper/ded0f3ac4c904f0f91bb937bb2bfd5fcb591c777",
                "title": "Safety and Performance, Why not Both? Bi-Objective Optimized Model Compression toward AI Software Deployment",
                "abstract": "The size of deep learning models in artificial intelligence (AI) software is increasing rapidly, which hinders the large-scale deployment on resource-restricted devices (e.g., smartphones). To mitigate this issue, AI software compression plays a crucial role, which aims to compress model size while keeping high performance. However, the intrinsic defects in the big model may be inherited by the compressed one. Such defects may be easily leveraged by attackers, since the compressed models are usually deployed in a large number of devices without adequate protection. In this paper, we try to address the safe model compression problem from a safety-performance co-optimization perspective. Specifically, inspired by the test-driven development (TDD) paradigm in software engineering, we propose a test-driven sparse training framework called SafeCompress. By simulating the attack mechanism as the safety test, SafeCompress can automatically compress a big model to a small one following the dynamic sparse training paradigm. Further, considering a representative attack, i.e., membership inference attack (MIA), we develop a concrete safe model compression mechanism, called MIA-SafeCompress. Extensive experiments are conducted to evaluate MIA-SafeCompress on five datasets for both computer vision and natural language processing tasks. The results verify the effectiveness and generalization of our method. We also discuss how to adapt SafeCompress to other attacks besides MIA, demonstrating the flexibility of SafeCompress.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2117121934",
                        "name": "Jie Zhu"
                    },
                    {
                        "authorId": "2143500842",
                        "name": "Leye Wang"
                    },
                    {
                        "authorId": "2118233331",
                        "name": "Xiao Han"
                    }
                ]
            }
        },
        {
            "contexts": [
                "More recently, the Lottery Ticket Hypothesis [12] has sparked interest in techniques that provide the storage and computation benefits of sparse models directly during training [35, 8]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "598b65b8f514304abd34b7e14296559388b20f25",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-04425",
                    "ArXiv": "2208.04425",
                    "DOI": "10.48550/arXiv.2208.04425",
                    "CorpusId": 251442494
                },
                "corpusId": 251442494,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/598b65b8f514304abd34b7e14296559388b20f25",
                "title": "Controlled Sparsity via Constrained Optimization or: How I Learned to Stop Tuning Penalties and Love Constraints",
                "abstract": "The performance of trained neural networks is robust to harsh levels of pruning. Coupled with the ever-growing size of deep learning models, this observation has motivated extensive research on learning sparse models. In this work, we focus on the task of controlling the level of sparsity when performing sparse learning. Existing methods based on sparsity-inducing penalties involve expensive trial-and-error tuning of the penalty factor, thus lacking direct control of the resulting model sparsity. In response, we adopt a constrained formulation: using the gate mechanism proposed by Louizos et al. (2018), we formulate a constrained optimization problem where sparsification is guided by the training objective and the desired sparsity target in an end-to-end fashion. Experiments on CIFAR-{10, 100}, TinyImageNet, and ImageNet using WideResNet and ResNet{18, 50} models validate the effectiveness of our proposal and demonstrate that we can reliably achieve pre-determined sparsity targets without compromising on predictive performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1410596066",
                        "name": "Jose Gallego-Posada"
                    },
                    {
                        "authorId": "2111835126",
                        "name": "Juan Ramirez"
                    },
                    {
                        "authorId": "3429013",
                        "name": "Akram Erraqabi"
                    },
                    {
                        "authorId": "1865800402",
                        "name": "Y. Bengio"
                    },
                    {
                        "authorId": "1388317459",
                        "name": "S. Lacoste-Julien"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Sparse Language Models Sparsely activated language models have been considered in a few forms (Evci et al., 2020; Mostafa and Wang, 2019; Dettmers and Zettlemoyer, 2019), but the Mixtureof-Experts (MoE) model is of particular note."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8b3a67c7e5289eed160d2acfd04d71cfb552c67d",
                "externalIds": {
                    "ArXiv": "2208.03306",
                    "DBLP": "journals/corr/abs-2208-03306",
                    "DOI": "10.48550/arXiv.2208.03306",
                    "CorpusId": 251371375
                },
                "corpusId": 251371375,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8b3a67c7e5289eed160d2acfd04d71cfb552c67d",
                "title": "Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models",
                "abstract": "We present Branch-Train-Merge (BTM), a communication-efficient algorithm for embarrassingly parallel training of large language models (LLMs). We show it is possible to independently train subparts of a new class of LLMs on different subsets of the data, eliminating the massive multi-node synchronization currently required to train LLMs. BTM learns a set of independent expert LMs (ELMs), each specialized to a different textual domain, such as scientific or legal text. These ELMs can be added and removed to update data coverage, ensembled to generalize to new domains, or averaged to collapse back to a single LM for efficient inference. New ELMs are learned by branching from (mixtures of) ELMs in the current set, further training the parameters on data for the new domain, and then merging the resulting model back into the set for future use. Experiments show that BTM improves in- and out-of-domain perplexities as compared to GPT-style Transformer LMs, when controlling for training cost. Through extensive analysis, we show that these results are robust to different ELM initialization schemes, but require expert domain specialization; LM ensembles with random data splits do not perform well. We also present a study of scaling BTM into a new corpus of 64 domains (192B whitespace-separated tokens in total); the resulting LM (22.4B total parameters) performs as well as a Transformer LM trained with 2.5 times more compute. These gains grow with the number of domains, suggesting more aggressive parallelism could be used to efficiently train larger models in future work.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118481100",
                        "name": "Margaret Li"
                    },
                    {
                        "authorId": "40895369",
                        "name": "Suchin Gururangan"
                    },
                    {
                        "authorId": "3239480",
                        "name": "Tim Dettmers"
                    },
                    {
                        "authorId": "35084211",
                        "name": "M. Lewis"
                    },
                    {
                        "authorId": "1745524",
                        "name": "Tim Althoff"
                    },
                    {
                        "authorId": "1685669",
                        "name": "Noah A. Smith"
                    },
                    {
                        "authorId": "1982950",
                        "name": "Luke Zettlemoyer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Additionally, fine-tuning a sparse network will actually cost the same or even more time than a dense one [3]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7e467eeb3e93147d69acd5a786b4c53c0472de11",
                "externalIds": {
                    "DBLP": "conf/mipr/HeBSD22",
                    "DOI": "10.1109/MIPR54900.2022.00011",
                    "CorpusId": 252165303
                },
                "corpusId": 252165303,
                "publicationVenue": {
                    "id": "eb35da90-3350-4969-9944-7744cb0bf6fb",
                    "name": "Conference on Multimedia Information Processing and Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "MIPR",
                        "Conf Multimedia Inf Process Retr"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7e467eeb3e93147d69acd5a786b4c53c0472de11",
                "title": "AgileGCN: Accelerating Deep GCN with Residual Connections using Structured Pruning",
                "abstract": "Deep Graph Convolutional Networks (GCNs) with multiple layers have been used for applications such as point cloud classification and semantic segmentation and achieved state-of-the-art results. However, they are computationally expensive and have a high run-time latency. In this paper, we propose AgileGCN, a novel framework to compress and accelerate deep GCN models with residual connections using structured pruning. Specifically, in each residual structure of a deep GCN, channel sampling and padding are applied to the input and output channels of a convolutional layer, respectively, to significantly reduce its floating point operations (FLOPs) and number of parameters. Experimental results on two benchmark point cloud datasets demonstrate that AgileGCN achieves significant FLOPs and parameters reduction while maintaining the performance of the unpruned models for both point cloud classification and segmentation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152880886",
                        "name": "Qisheng He"
                    },
                    {
                        "authorId": "2541245",
                        "name": "Soumyanil Banerjee"
                    },
                    {
                        "authorId": "145809145",
                        "name": "L. Schwiebert"
                    },
                    {
                        "authorId": null,
                        "name": "Ming Dong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While post-training quantization can be an efficient and successful technique for quantizing models without any retraining (Frantar and Alistarh, 2022), in the case of pruning the gold standard is still training a separate model for every target sparsity level (Zhu and Gupta, 2017; Singh and Alistarh, 2020; Evci et al., 2020; Peste et al., 2021); the latter can be an expensive procedure, which would still rely on powerful computational resources to obtain the sparse models in the first place."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1e5c62492b2fd7d7dc428d5d6a44c694d281b395",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-14200",
                    "ArXiv": "2207.14200",
                    "DOI": "10.48550/arXiv.2207.14200",
                    "CorpusId": 251135247
                },
                "corpusId": 251135247,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1e5c62492b2fd7d7dc428d5d6a44c694d281b395",
                "title": "CrAM: A Compression-Aware Minimizer",
                "abstract": "Deep neural networks (DNNs) often have to be compressed, via pruning and/or quantization, before they can be deployed in practical settings. In this work we propose a new compression-aware minimizer dubbed CrAM that modifies the optimization step in a principled way, in order to produce models whose local loss behavior is stable under compression operations such as pruning. Thus, dense models trained via CrAM should be compressible post-training, in a single step, without significant accuracy loss. Experimental results on standard benchmarks, such as residual networks for ImageNet classification and BERT models for language modelling, show that CrAM produces dense models that can be more accurate than the standard SGD/Adam-based baselines, but which are stable under weight pruning: specifically, we can prune models in one-shot to 70-80% sparsity with almost no accuracy loss, and to 90% with reasonable ($\\sim 1\\%$) accuracy loss, which is competitive with gradual compression methods. Additionally, CrAM can produce sparse models which perform well for transfer learning, and it also works for semi-structured 2:4 pruning patterns supported by GPU hardware. The code for reproducing the results is available at https://github.com/IST-DASLab/CrAM .",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3341722",
                        "name": "Alexandra Peste"
                    },
                    {
                        "authorId": "2869958",
                        "name": "Adrian Vladu"
                    },
                    {
                        "authorId": "3311387",
                        "name": "Dan Alistarh"
                    },
                    {
                        "authorId": "48523189",
                        "name": "Christoph H. Lampert"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Hence, we can approximate the number of MAC operations as 2Nc as done in [21]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b87b3287f2b429fd9eb21a89611a04026bd9d260",
                "externalIds": {
                    "ArXiv": "2207.09387",
                    "DBLP": "journals/corr/abs-2207-09387",
                    "DOI": "10.48550/arXiv.2207.09387",
                    "CorpusId": 250644501
                },
                "corpusId": 250644501,
                "publicationVenue": {
                    "id": "bb40a041-3875-45d5-afd4-e1c75f896fa6",
                    "name": "IEEE Transactions on Wireless Communications",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Wirel Commun"
                    ],
                    "issn": "1536-1276",
                    "url": "http://www.comsoc.org/twc/",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=7693",
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7693&year=2005"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b87b3287f2b429fd9eb21a89611a04026bd9d260",
                "title": "Green, Quantized Federated Learning over Wireless Networks: An Energy-Efficient Design",
                "abstract": "In this paper, a green-quantized FL framework, which represents data with a finite precision level in both local training and uplink transmission, is proposed. Here, the finite precision level is captured through the use of quantized neural networks (QNNs) that quantize weights and activations in fixed-precision format. In the considered FL model, each device trains its QNN and transmits a quantized training result to the base station. Energy models for the local training and the transmission with quantization are rigorously derived. To minimize the energy consumption and the number of communication rounds simultaneously, a multi-objective optimization problem is formulated with respect to the number of local iterations, the number of selected devices, and the precision levels for both local training and transmission while ensuring convergence under a target accuracy constraint. To solve this problem, the convergence rate of the proposed FL system is analytically derived with respect to the system control variables. Then, the Pareto boundary of the problem is characterized to provide efficient solutions using the normal boundary inspection method. Design insights on balancing the tradeoff between the two objectives while achieving a target accuracy are drawn from using the Nash bargaining solution and analyzing the derived convergence rate. Simulation results show that the proposed FL framework can reduce energy consumption until convergence by up to 70\\% compared to a baseline FL algorithm that represents data with full precision without damaging the convergence rate.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2116507338",
                        "name": "Minsu Kim"
                    },
                    {
                        "authorId": "145412074",
                        "name": "W. Saad"
                    },
                    {
                        "authorId": "2250918",
                        "name": "Mohammad Mozaffari"
                    },
                    {
                        "authorId": "145118318",
                        "name": "M. Debbah"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0cde51d846952f5b5e6811313e2257fff9642dad",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-08629",
                    "ArXiv": "2207.08629",
                    "DOI": "10.48550/arXiv.2207.08629",
                    "CorpusId": 250627206,
                    "PubMed": "37368807"
                },
                "corpusId": 250627206,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0cde51d846952f5b5e6811313e2257fff9642dad",
                "title": "Comprehensive Graph Gradual Pruning for Sparse Training in Graph Neural Networks",
                "abstract": "Graph neural networks (GNNs) tend to suffer from high computation costs due to the exponentially increasing scale of graph data and a large number of model parameters, which restricts their utility in practical applications. To this end, some recent works focus on sparsifying GNNs (including graph structures and model parameters) with the lottery ticket hypothesis (LTH) to reduce inference costs while maintaining performance levels. However, the LTH-based methods suffer from two major drawbacks: 1) they require exhaustive and iterative training of dense models, resulting in an extremely large training computation cost, and 2) they only trim graph structures and model parameters but ignore the node feature dimension, where vast redundancy exists. To overcome the above limitations, we propose a comprehensive graph gradual pruning framework termed CGP. This is achieved by designing a during-training graph pruning paradigm to dynamically prune GNNs within one training process. Unlike LTH-based methods, the proposed CGP approach requires no retraining, which significantly reduces the computation costs. Furthermore, we design a cosparsifying strategy to comprehensively trim all the three core elements of GNNs: graph structures, node features, and model parameters. Next, to refine the pruning operation, we introduce a regrowth process into our CGP framework, to reestablish the pruned but important connections. The proposed CGP is evaluated over a node classification task across six GNN architectures, including shallow models graph convolutional network (GCN) and graph attention network (GAT), shallow-but-deep-propagation models simple graph convolution (SGC) and approximate personalized propagation of neural predictions (APPNP), and deep models GCN via initial residual and identity mapping (GCNII) and residual GCN (ResGCN), on a total of 14 real-world graph datasets, including large-scale graph datasets from the challenging Open Graph Benchmark (OGB). Experiments reveal that the proposed strategy greatly improves both training and inference efficiency while matching or even exceeding the accuracy of the existing methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145760439",
                        "name": "Chuang Liu"
                    },
                    {
                        "authorId": "8848225",
                        "name": "Xueqi Ma"
                    },
                    {
                        "authorId": "2176785211",
                        "name": "Yinbing Zhan"
                    },
                    {
                        "authorId": "46573238",
                        "name": "Liang Ding"
                    },
                    {
                        "authorId": "1701119",
                        "name": "Dapeng Tao"
                    },
                    {
                        "authorId": "2064618916",
                        "name": "Bo Du"
                    },
                    {
                        "authorId": "2566294",
                        "name": "Wenbin Hu"
                    },
                    {
                        "authorId": "2155041542",
                        "name": "Danilo P. Mandic"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b5e7274eb2eac64b202d610e6e76b60c36faeaea",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-04089",
                    "ArXiv": "2207.04089",
                    "DOI": "10.48550/arXiv.2207.04089",
                    "CorpusId": 250426324
                },
                "corpusId": 250426324,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b5e7274eb2eac64b202d610e6e76b60c36faeaea",
                "title": "SInGE: Sparsity via Integrated Gradients Estimation of Neuron Relevance",
                "abstract": "The leap in performance in state-of-the-art computer vision methods is attributed to the development of deep neural networks. However it often comes at a computational price which may hinder their deployment. To alleviate this limitation, structured pruning is a well known technique which consists in removing channels, neurons or filters, and is commonly applied in order to produce more compact models. In most cases, the computations to remove are selected based on a relative importance criterion. At the same time, the need for explainable predictive models has risen tremendously and motivated the development of robust attribution methods that highlight the relative importance of pixels of an input image or feature map. In this work, we discuss the limitations of existing pruning heuristics, among which magnitude and gradient-based methods. We draw inspiration from attribution methods to design a novel integrated gradient pruning criterion, in which the relevance of each neuron is defined as the integral of the gradient variation on a path towards this neuron removal. Furthermore, we propose an entwined DNN pruning and fine-tuning flowchart to better preserve DNN accuracy while removing parameters. We show through extensive validation on several datasets, architectures as well as pruning scenarios that the proposed method, dubbed SInGE, significantly outperforms existing state-of-the-art DNN pruning methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1632928879",
                        "name": "Edouard Yvinec"
                    },
                    {
                        "authorId": "3190846",
                        "name": "Arnaud Dapogny"
                    },
                    {
                        "authorId": "51021910",
                        "name": "M. Cord"
                    },
                    {
                        "authorId": "2521061",
                        "name": "K\u00e9vin Bailly"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Evci et al. (2020) further extended ER to CNN and brings significant gains to sparse CNN training with the Erdo\u030bs-Re\u0301nyi-Kernel (ERK) ratio.",
                "Weight Grow: The most common ways to grow new weights are random-based growth (Mocanu et al., 2018) and gradient-based growth (Dettmers & Zettlemoyer, 2019; Evci et al., 2020).",
                "A long-standing research topic, recent attempts on sparsity (Mocanu et al., 2018; Liu et al., 2021b;c; Evci et al., 2020; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Chen et al., 2021) train intrinsically sparse neural networks from scratch using only a small proportion of parameters and FLOPs (as illustrated in Figure 2).",
                "This ratio determines the computational FLOPs (floating-point operations) of the sparse model and has a significant impact on its final performance (Evci et al., 2020; Liu et al., 2022a; Hoang et al., 2023).",
                "A long-standing research topic, recent attempts on sparsity (Mocanu et al., 2018; Liu et al., 2021b;c; Evci et al., 2020; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Chen et al., 2021) train intrinsically sparse neural networks from scratch using only a small proportion of parameters and\u2026",
                "While there is an upsurge in increasingly efficient ways for sparse training (Bellec et al., 2018; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Liu et al., 2021c; Jayakumar et al., 2020; Chen et al., 2021; Schwarz et al., 2021; Jiang et al.)"
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "2038aade6e46a4b0624c261fea9e0e2543bf0240",
                "externalIds": {
                    "DBLP": "conf/iclr/LiuCCCXWKPMW23",
                    "ArXiv": "2207.03620",
                    "DOI": "10.48550/arXiv.2207.03620",
                    "CorpusId": 250408169
                },
                "corpusId": 250408169,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2038aade6e46a4b0624c261fea9e0e2543bf0240",
                "title": "More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity",
                "abstract": "Transformers have quickly shined in the computer vision world since the emergence of Vision Transformers (ViTs). The dominant role of convolutional neural networks (CNNs) seems to be challenged by increasingly effective transformer-based models. Very recently, a couple of advanced convolutional models strike back with large kernels motivated by the local-window attention mechanism, showing appealing performance and efficiency. While one of them, i.e. RepLKNet, impressively manages to scale the kernel size to 31x31 with improved performance, the performance starts to saturate as the kernel size continues growing, compared to the scaling trend of advanced ViTs such as Swin Transformer. In this paper, we explore the possibility of training extreme convolutions larger than 31x31 and test whether the performance gap can be eliminated by strategically enlarging convolutions. This study ends up with a recipe for applying extremely large kernels from the perspective of sparsity, which can smoothly scale up kernels to 61x61 with better performance. Built on this recipe, we propose Sparse Large Kernel Network (SLaK), a pure CNN architecture equipped with sparse factorized 51x51 kernels that can perform on par with or better than state-of-the-art hierarchical Transformers and modern ConvNet architectures like ConvNeXt and RepLKNet, on ImageNet classification as well as a wide range of downstream tasks including semantic segmentation on ADE20K, object detection on PASCAL VOC 2007, and object detection/segmentation on MS COCO.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2116298570",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": null,
                        "name": "Xuxi Chen"
                    },
                    {
                        "authorId": "2056427852",
                        "name": "Q. Xiao"
                    },
                    {
                        "authorId": "46791907",
                        "name": "Boqian Wu"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[7] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "fd898ba3980f88067c7369b9732e875d6936056d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-00670",
                    "ArXiv": "2207.00670",
                    "DOI": "10.48550/arXiv.2207.00670",
                    "CorpusId": 250264366
                },
                "corpusId": 250264366,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fd898ba3980f88067c7369b9732e875d6936056d",
                "title": "DRESS: Dynamic REal-time Sparse Subnets",
                "abstract": "The limited and dynamically varied resources on edge devices motivate us to deploy an optimized deep neural network that can adapt its sub-networks to fit in different resource constraints. However, existing works often build sub-networks through searching different network architectures in a hand-crafted sampling space, which not only can result in a subpar performance but also may cause on-device re-configuration overhead. In this paper, we propose a novel training algorithm, Dynamic REal-time Sparse Subnets (DRESS). DRESS samples multiple sub-networks from the same backbone network through row-based unstructured sparsity, and jointly trains these sub-networks in parallel with weighted loss. DRESS also exploits strategies including parameter reusing and row-based fine-grained sampling for efficient storage consumption and efficient on-device adaptation. Extensive experiments on public vision datasets show that DRESS yields significantly higher accuracy than state-of-the-art sub-networks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39518318",
                        "name": "Zhongnan Qu"
                    },
                    {
                        "authorId": "32830876",
                        "name": "Syed Shakib Sarwar"
                    },
                    {
                        "authorId": "2118103534",
                        "name": "Xin Dong"
                    },
                    {
                        "authorId": "2144463202",
                        "name": "Yuecheng Li"
                    },
                    {
                        "authorId": "2226899867",
                        "name": "Ekin Sumbul"
                    },
                    {
                        "authorId": "27596334",
                        "name": "B. D. Salvo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Backward propagation has the same computation characteristics as forward propagation but has two times the computation expense of the forward propagation [9]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0b20229534bb59c13df81bb40a2abc76c12f3024",
                "externalIds": {
                    "DBLP": "conf/IEEEcloud/JiaYLM22",
                    "DOI": "10.1109/CLOUD55607.2022.00068",
                    "CorpusId": 251773501
                },
                "corpusId": 251773501,
                "publicationVenue": {
                    "id": "406d9f60-417a-4dc5-a6b7-1fe4689a4ff7",
                    "name": "IEEE International Conference on Cloud Computing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Cloud Comput [services Soc",
                        "CLOUD",
                        "International Conference on Cloud Computing [Services Society]",
                        "IEEE Int Conf Cloud Comput"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0b20229534bb59c13df81bb40a2abc76c12f3024",
                "title": "A Data-Loader Tunable Knob to Shorten GPU Idleness for Distributed Deep Learning",
                "abstract": "Deep Neural Network (DNN) has been applied as an effective machine learning algorithm to tackle problems in different domains. However, training a sophisticated DNN model takes days to weeks and becomes a challenge in constructing research on large-scale DNN models. Distributed Deep Learning (DDL) contributes to accelerating DNN training by distributing training workloads across multiple computation accelerators (e.g., GPUs). Although a surge of research works has been devoted to optimizing DDL training, the impact of data-loading on GPU usage and training performance has been relatively under-explored. It is non-trivial to optimize data-loading in DDL applications that need intensive CPU and I/O resources to process enormous training data. When multiple DDL applications are deployed on a system (e.g., Cloud and HPC), the lack of a practical and efficient technique for data-loader allocation incurs GPU idleness and degrades the training throughput. Therefore, our work first focuses on investigating the impact of data-loading on the global training throughput. We then propose a throughput prediction model to predict the maximum throughput for an individual DDL training application. By leveraging the predicted results, A-Dloader is designed to dynamically allocate CPU and I/O resources to concurrently running DDL applications and use the data-loader allocation as a knob to reduce GPU idle intervals and thus improve the overall training throughput. We implement and evaluate A-Dloader in a DDL framework for a series of DDL applications arriving and completing across the runtime. Our experimental results show that A-Dloader can achieve a 23.5% throughput improvement and a 10% makespan improvement, compared to allocating resources evenly across applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1486100460",
                        "name": "Danlin Jia"
                    },
                    {
                        "authorId": "9347641",
                        "name": "Geng Yuan"
                    },
                    {
                        "authorId": "2143778722",
                        "name": "Xue Lin"
                    },
                    {
                        "authorId": "145154819",
                        "name": "N. Mi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although a higher compression rate can be achieved, the unstructured sparsity pattern (Sun et al. 2017; Goli and Aamodt 2020; Lin et al. 2020; Evci et al. 2020) cannot be directly employed to commercial",
                "Although a higher compression rate can be achieved, the unstructured sparsity pattern (Sun et al. 2017; Goli and Aamodt 2020; Lin et al. 2020; Evci et al. 2020) cannot be directly employed to commercial Dataset Model Sparsity mAP(%) Speedup",
                "The prune-redistribute-regrowth cycle is adopted in (Dettmers and Zettlemoyer 2019; Mostafa and Wang 2019; Evci et al. 2020) to change weights according to different criteria dynamically.",
                "5 3 7 \u2013 RigL (Evci et al. 2020)\u2217 Unstructured 50%\u00d71 W \u2013 \u2013 76."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "722e0640c064c4068c46a02fdf0ddf3037740ac3",
                "externalIds": {
                    "DBLP": "conf/aaai/XuHCW022",
                    "DOI": "10.1609/aaai.v36i3.20198",
                    "CorpusId": 250300983
                },
                "corpusId": 250300983,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/722e0640c064c4068c46a02fdf0ddf3037740ac3",
                "title": "Towards Fully Sparse Training: Information Restoration with Spatial Similarity",
                "abstract": "The 2:4 structured sparsity pattern released by NVIDIA Ampere architecture, requiring four consecutive values containing at least two zeros, enables doubling math throughput for matrix multiplications. Recent works mainly focus on inference speedup via 2:4 sparsity while training acceleration has been largely overwhelmed where backpropagation consumes around 70% of the training time. However, unlike inference, training speedup with structured pruning is nontrivial due to the need to maintain the fidelity of gradients and reduce the additional overhead of performing 2:4 sparsity online. For the first time, this article proposes fully sparse training (FST) where `fully' indicates that ALL matrix multiplications in forward/backward propagation are structurally pruned while maintaining accuracy. To this end, we begin with saliency analysis, investigating the sensitivity of different sparse objects to structured pruning. Based on the observation of spatial similarity among activations, we propose pruning activations with fixed 2:4 masks. Moreover, an Information Restoration block is proposed to retrieve the lost information, which can be implemented by efficient gradient-shift operation. Evaluation of accuracy and efficiency shows that we can achieve 2\u00d7 training acceleration with negligible accuracy degradation on challenging large-scale classification and detection tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7978284",
                        "name": "Weixiang Xu"
                    },
                    {
                        "authorId": "48535072",
                        "name": "Xiangyu He"
                    },
                    {
                        "authorId": "1998966851",
                        "name": "Ke Cheng"
                    },
                    {
                        "authorId": "1656803942",
                        "name": "Peisong Wang"
                    },
                    {
                        "authorId": "2149023580",
                        "name": "Jian Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In sparse masks, the number of incoming/outgoing connections is not identical for all the neurons in the layer (Evci et al., 2020b) and this raises direct concerns against the blind usage of dense network initialization for sparse subnetworks.",
                "Yet, (Evci et al., 2020b) also showed that completely random re-initialization of sparse subnetworks can lead the sparse masks to converge to poorer solutions.",
                "They usually prunes weights based on the magnitude and grows weights back (Mocanu et al., 2018) at random or based on the gradient (Evci et al., 2020a; Liu et al., 2021; Chen et al., 2022; 2021a).",
                "Aware of the sensitivity and negative impact of changing initialization identified by (Evci et al., 2020c), we point that that linear scaling will not hurt the original sparse mask\u2019s initialization, thanks to the BatchNorm layer which will effectively absorb any linear scaling of the weights."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "185f533ad0d74e8ef86354167dd55b1026a39616",
                "externalIds": {
                    "DBLP": "conf/icml/JaiswalMC0W22",
                    "ArXiv": "2206.12755",
                    "DOI": "10.48550/arXiv.2206.12755",
                    "CorpusId": 250072221
                },
                "corpusId": 250072221,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/185f533ad0d74e8ef86354167dd55b1026a39616",
                "title": "Training Your Sparse Neural Network Better with Any Mask",
                "abstract": "Pruning large neural networks to create high-quality, independently trainable sparse masks, which can maintain similar performance to their dense counterparts, is very desirable due to the reduced space and time complexity. As research effort is focused on increasingly sophisticated pruning methods that leads to sparse subnetworks trainable from the scratch, we argue for an orthogonal, under-explored theme: improving training techniques for pruned sub-networks, i.e. sparse training. Apart from the popular belief that only the quality of sparse masks matters for sparse training, in this paper we demonstrate an alternative opportunity: one can carefully customize the sparse training techniques to deviate from the default dense network training protocols, consisting of introducing ``ghost\"neurons and skip connections at the early stage of training, and strategically modifying the initialization as well as labels. Our new sparse training recipe is generally applicable to improving training from scratch with various sparse masks. By adopting our newly curated techniques, we demonstrate significant performance gains across various popular datasets (CIFAR-10, CIFAR-100, TinyImageNet), architectures (ResNet-18/32/104, Vgg16, MobileNet), and sparse mask options (lottery ticket, SNIP/GRASP, SynFlow, or even randomly pruning), compared to the default training protocols, especially at high sparsity levels. Code is at https://github.com/VITA-Group/ToST",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145018564",
                        "name": "Ajay Jaiswal"
                    },
                    {
                        "authorId": "2126795",
                        "name": "Haoyu Ma"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "119663804",
                        "name": "Ying Ding"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Most pruning research since then has followed this approach (Zhou et al., 2019; Evci et al., 2020; Mostafa & Wang, 2019; Bellec et al., 2018; Dettmers & Zettlemoyer, 2019; Mocanu et al., 2018; You et al., 2020; Chen et al., 2020).",
                "However, a large body of literature has shown that a high number of carefully chosen parameters can be removed (i.e. pruned) while maintaining the network\u2019s predictive performance (LeCun et al., 1990; Molchanov et al., 2017; Evci et al., 2020; Su et al., 2020; Lee et al., 2019; Wang et al., 2020).",
                "pruned) while maintaining the network\u2019s predictive performance (LeCun et al., 1990; Molchanov et al., 2017; Evci et al., 2020; Su et al., 2020; Lee et al., 2019; Wang et al., 2020).",
                "\u2026networks (Han et al., 2015; LeCun et al., 1990; Hassibi et al., 1993; Wang et al., 2019; Li et al., 2016), or throughout training (Srinivas & Babu, 2016; Louizos et al., 2018; Evci et al., 2020; Mostafa & Wang, 2019; Bellec et al.,\n2018; Dettmers & Zettlemoyer, 2019; Mocanu et al., 2018)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "cf8f5baabeba69ddca9f8b65e05d467b50253e0a",
                "externalIds": {
                    "DBLP": "conf/icml/RachwanZCGAG22",
                    "ArXiv": "2206.10451",
                    "DOI": "10.48550/arXiv.2206.10451",
                    "CorpusId": 249890244
                },
                "corpusId": 249890244,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/cf8f5baabeba69ddca9f8b65e05d467b50253e0a",
                "title": "Winning the Lottery Ahead of Time: Efficient Early Network Pruning",
                "abstract": "Pruning, the task of sparsifying deep neural networks, received increasing attention recently. Although state-of-the-art pruning methods extract highly sparse models, they neglect two main challenges: (1) the process of finding these sparse models is often very expensive; (2) unstructured pruning does not provide benefits in terms of GPU memory, training time, or carbon emissions. We propose Early Compression via Gradient Flow Preservation (EarlyCroP), which efficiently extracts state-of-the-art sparse models before or early in training addressing challenge (1), and can be applied in a structured manner addressing challenge (2). This enables us to train sparse networks on commodity GPUs whose dense versions would be too large, thereby saving costs and reducing hardware requirements. We empirically show that EarlyCroP outperforms a rich set of baselines for many tasks (incl. classification, regression) and domains (incl. computer vision, natural language processing, and reinforcment learning). EarlyCroP leads to accuracy comparable to dense training while outperforming pruning baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2122080487",
                        "name": "John Rachwan"
                    },
                    {
                        "authorId": "73775589",
                        "name": "D. Zugner"
                    },
                    {
                        "authorId": "50997190",
                        "name": "Bertrand Charpentier"
                    },
                    {
                        "authorId": "79462643",
                        "name": "Simon Geisler"
                    },
                    {
                        "authorId": "1580152221",
                        "name": "Morgane Ayle"
                    },
                    {
                        "authorId": "51249380",
                        "name": "Stephan Gunnemann"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Training sparse neural networks has been extensively explored (Bellec et al., 2018; Evci et al., 2020; Mocanu et al., 2018; Liu et al., 2021b; 2020; 2021a)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "152fca044ef6262065b3f2c718f06eae4c171be5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-09117",
                    "ArXiv": "2206.09117",
                    "DOI": "10.48550/arXiv.2206.09117",
                    "CorpusId": 249889352
                },
                "corpusId": 249889352,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/152fca044ef6262065b3f2c718f06eae4c171be5",
                "title": "NISPA: Neuro-Inspired Stability-Plasticity Adaptation for Continual Learning in Sparse Networks",
                "abstract": "The goal of continual learning (CL) is to learn different tasks over time. The main desiderata associated with CL are to maintain performance on older tasks, leverage the latter to improve learning of future tasks, and to introduce minimal overhead in the training process (for instance, to not require a growing model or retraining). We propose the Neuro-Inspired Stability-Plasticity Adaptation (NISPA) architecture that addresses these desiderata through a sparse neural network with fixed density. NISPA forms stable paths to preserve learned knowledge from older tasks. Also, NISPA uses connection rewiring to create new plastic paths that reuse existing knowledge on novel tasks. Our extensive evaluation on EMNIST, FashionMNIST, CIFAR10, and CIFAR100 datasets shows that NISPA significantly outperforms representative state-of-the-art continual learning baselines, and it uses up to ten times fewer learnable parameters compared to baselines. We also make the case that sparsity is an essential ingredient for continual learning. The NISPA code is available at https://github.com/BurakGurbuz97/NISPA.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1988961221",
                        "name": "Mustafa Burak Gurbuz"
                    },
                    {
                        "authorId": "144734756",
                        "name": "C. Dovrolis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "FLOPs behaviour of Sparse ERK networks in DRL\nAs reported in Evci et al. (2020), using ERK sparsity distribution often doubles the FLOPs needed for sparse models compared using the uniform sparsity distribution.",
                "\u2026(Frankle et al., 2019; Zhou et al., 2019), dynamic sparse training (Dettmers & Zettlemoyer, 2019; Mostafa & Wang, 2019; Mocanu et al., 2018; Bellec et al., 2018; Evci et al., 2020; Liu et al., 2021) and one-shot pruning (Lee et al., 2019; Wang et al., 2020; Tanaka et al., 2020; Liu & Zenke, 2020).",
                "Code Our code is built upon the TF-Agents (Guadarrama et al., 2018), Dopamine (Castro et al., 2018), and RigL (Evci et al., 2020) codebases.",
                "Within network sparsity In Figure 4 (left) we turn our attention to the question of distributing parameters within networks and compare two strategies; uniform and ERK (Evci et al., 2020).",
                "Inline with previous observations made in speech (Kalchbrenner et al., 2018), natu-\nral language modelling (Li et al., 2020) and computer vision (Evci et al., 2020), in almost all environments, sparse networks found by pruning achieve significantly higher rewards than the dense baseline.",
                "Due to weight sharing in convolutional layers, ERK sparsity distribution doubles the FLOPs required at a given sparsity (Evci et al., 2020), which we also found to be the case with the convolutional networks used by DQN in the Atari environments (see subsection A.",
                "However gradient based growth (Evci et al., 2020) seems to have a limited effect on performance.",
                "Rigged Lottery (RigL) (Evci et al., 2020): is the same as SET, except the new connections are activated using the gradient signal (highest magnitude) instead of at random.",
                ", 2020) and computer vision (Evci et al., 2020), in almost all environments, sparse networks found by pruning achieve significantly higher rewards than the dense baseline.",
                "Due to weight sharing in convolutional layers, ERK sparsity distribution doubles the FLOPs required at a given sparsity (Evci et al., 2020), which we also found to be the case with the convolutional networks used by DQN in the Atari environments (see subsection A.2 for further discussion).",
                ", 2019), dynamic sparse training (Dettmers & Zettlemoyer, 2019; Mostafa & Wang, 2019; Mocanu et al., 2018; Bellec et al., 2018; Evci et al., 2020; Liu et al., 2021) and one-shot pruning (Lee et al.",
                "Known as Dynamic sparse training (DST), such approaches have been shown to match pruning results, making it possible to train sparse networks efficiently without sacrificing performance (Dettmers & Zettlemoyer, 2019; Evci et al., 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7c337018654121e85ac3159a5217d663a8222064",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-10369",
                    "ArXiv": "2206.10369",
                    "DOI": "10.48550/arXiv.2206.10369",
                    "CorpusId": 249890186
                },
                "corpusId": 249890186,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7c337018654121e85ac3159a5217d663a8222064",
                "title": "The State of Sparse Training in Deep Reinforcement Learning",
                "abstract": "The use of sparse neural networks has seen rapid growth in recent years, particularly in computer vision. Their appeal stems largely from the reduced number of parameters required to train and store, as well as in an increase in learning efficiency. Somewhat surprisingly, there have been very few efforts exploring their use in Deep Reinforcement Learning (DRL). In this work we perform a systematic investigation into applying a number of existing sparse training techniques on a variety of DRL agents and environments. Our results corroborate the findings from sparse training in the computer vision domain - sparse networks perform better than dense networks for the same parameter count - in the DRL domain. We provide detailed analyses on how the various components in DRL are affected by the use of sparse networks and conclude by suggesting promising avenues for improving the effectiveness of sparse training methods, as well as for advancing their use in DRL.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "30131402",
                        "name": "L. Graesser"
                    },
                    {
                        "authorId": "3399348",
                        "name": "Utku Evci"
                    },
                    {
                        "authorId": "152585800",
                        "name": "Erich Elsen"
                    },
                    {
                        "authorId": "39163115",
                        "name": "P. S. Castro"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Comparison of our model with SOTA pruning methods, DPF [29], STR[23], LAMP[27], RiGL[11], and SuRP[19].",
                "DPF [29], STR[23], LAMP[27], RiGL[11], and SuRP[19] are the SOTA methods that use a large sparsity (+50\u00d7) and maintain a reasonable accuracy."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "c41ad6df0663860e9fbad3c5f32c9bad98119498",
                "externalIds": {
                    "ArXiv": "2206.08464",
                    "CorpusId": 261276255
                },
                "corpusId": 261276255,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c41ad6df0663860e9fbad3c5f32c9bad98119498",
                "title": "PRANC: Pseudo RAndom Networks for Compacting deep models",
                "abstract": "We demonstrate that a deep model can be reparametrized as a linear combination of several randomly initialized and frozen deep models in the weight space. During training, we seek local minima that reside within the subspace spanned by these random models (i.e., `basis' networks). Our framework, PRANC, enables significant compaction of a deep model. The model can be reconstructed using a single scalar `seed,' employed to generate the pseudo-random `basis' networks, together with the learned linear mixture coefficients. In practical applications, PRANC addresses the challenge of efficiently storing and communicating deep models, a common bottleneck in several scenarios, including multi-agent learning, continual learners, federated systems, and edge devices, among others. In this study, we employ PRANC to condense image classification models and compress images by compacting their associated implicit neural networks. PRANC outperforms baselines with a large margin on image classification when compressing a deep model almost $100$ times. Moreover, we show that PRANC enables memory-efficient inference by generating layer-wise weights on the fly. The source code of PRANC is here: \\url{https://github.com/UCDvision/PRANC}",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1994086651",
                        "name": "Parsa Nooralinejad"
                    },
                    {
                        "authorId": "2142114752",
                        "name": "Ali Abbasi"
                    },
                    {
                        "authorId": "2004045536",
                        "name": "Soroush Abbasi Koohpayegani"
                    },
                    {
                        "authorId": "2235058809",
                        "name": "Kossar Pourahmadi Meibodi"
                    },
                    {
                        "authorId": "2204648295",
                        "name": "Rana Muhammad Shahroz Khan"
                    },
                    {
                        "authorId": "2062432",
                        "name": "Soheil Kolouri"
                    },
                    {
                        "authorId": "2367683",
                        "name": "H. Pirsiavash"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Dozens of earlier studies [22, 17, 7] have demonstrated that unstructured sparsity is able to reach negligible performance degradation under",
                "The use of a binary mask is originated from many traditional unstructured sparsity methods [7, 16].",
                "4.3, we further show the performance of LBC and traditional unstructured sparsity methods including RigL [7],\nGMP [41], STR [17] under similar total compression rates.",
                "3, we further show the performance of LBC and traditional unstructured sparsity methods including RigL [7],",
                "RigL [7] alternately removes and revives weights according to their magnitudes and dense gradients."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "996b75c88ebc8c8d2e69bd46b28fa08332681a49",
                "externalIds": {
                    "DBLP": "conf/nips/0002LLL00WJ22",
                    "ArXiv": "2206.06662",
                    "DOI": "10.48550/arXiv.2206.06662",
                    "CorpusId": 249642429
                },
                "corpusId": 249642429,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/996b75c88ebc8c8d2e69bd46b28fa08332681a49",
                "title": "Learning Best Combination for Efficient N: M Sparsity",
                "abstract": "By forcing at most N out of M consecutive weights to be non-zero, the recent N:M network sparsity has received increasing attention for its two attractive advantages: 1) Promising performance at a high sparsity. 2) Significant speedups on NVIDIA A100 GPUs. Recent studies require an expensive pre-training phase or a heavy dense-gradient computation. In this paper, we show that the N:M learning can be naturally characterized as a combinatorial problem which searches for the best combination candidate within a finite collection. Motivated by this characteristic, we solve N:M sparsity in an efficient divide-and-conquer manner. First, we divide the weight vector into $C_{\\text{M}}^{\\text{N}}$ combination subsets of a fixed size N. Then, we conquer the combinatorial problem by assigning each combination a learnable score that is jointly optimized with its associate weights. We prove that the introduced scoring mechanism can well model the relative importance between combination subsets. And by gradually removing low-scored subsets, N:M fine-grained sparsity can be efficiently optimized during the normal training phase. Comprehensive experiments demonstrate that our learning best combination (LBC) performs consistently better than off-the-shelf N:M sparsity methods across various networks. Our project is released at \\url{https://github.com/zyxxmu/LBC}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108078624",
                        "name": "Yu-xin Zhang"
                    },
                    {
                        "authorId": "49352079",
                        "name": "Mingbao Lin"
                    },
                    {
                        "authorId": "2170182934",
                        "name": "Zhihang Lin"
                    },
                    {
                        "authorId": "1753623782",
                        "name": "Yiting Luo"
                    },
                    {
                        "authorId": "2149140038",
                        "name": "Ke Li"
                    },
                    {
                        "authorId": "2070266415",
                        "name": "Fei Chao"
                    },
                    {
                        "authorId": "47096329",
                        "name": "Yongjian Wu"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Sparse models for these accelerators are obtained through the many pruning and sparsification techniques, [23, 25, 36, 48, 49, 58, 62] including channel pruning [37, 55] and advanced compression [34]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "562acf986de6e4c18c920965178c4ec6c7dab5ab",
                "externalIds": {
                    "DBLP": "conf/isca/LewLGGEA22",
                    "DOI": "10.1145/3470496.3527404",
                    "CorpusId": 249205229
                },
                "corpusId": 249205229,
                "publicationVenue": {
                    "id": "deedf64a-dd5c-4b33-b345-ff83bfb93d71",
                    "name": "International Symposium on Computer Architecture",
                    "type": "conference",
                    "alternate_names": [
                        "Int Symp Comput Archit",
                        "ISCA"
                    ],
                    "url": "http://www.cs.wisc.edu/~arch/www/"
                },
                "url": "https://www.semanticscholar.org/paper/562acf986de6e4c18c920965178c4ec6c7dab5ab",
                "title": "Anticipating and eliminating redundant computations in accelerated sparse training",
                "abstract": "Deep Neural Networks (DNNs) are the state of art in image, speech, and text processing. To address long training times and high energy consumption, custom accelerators can exploit sparsity, that is zero-valued weights, activations, and gradients. Proposed sparse Convolution Neural Network (CNN) accelerators support training with no more than one dynamic sparse convolution input. Among existing accelerator classes, the only ones supporting two-sided dynamic sparsity are outer-product-based accelerators. However, when mapping a convolution onto an outer product, multiplications occur that do not correspond to any valid output. These Redundant Cartesian Products (RCPs) decrease energy efficiency and performance. We observe that in sparse training, up to 90% of computations are RCPs resulting from the convolution of large matrices for weight updates during the backward pass of CNN training. In this work, we design a mechanism, ANT, to anticipate and eliminate RCPs, enabling more efficient sparse training when integrated with an outer-product accelerator. By anticipating over 90% of RCPs, ANT achieves a geometric mean of 3.71\u00d7 speed up over an SCNN-like accelerator [67] on 90% sparse training using DenseNet-121 [38], ResNet18 [35], VGG16 [73], Wide ResNet (WRN) [85], and ResNet-50 [35], with 4.40\u00d7 decrease in energy consumption and 0.0017mm2 of additional area. We extend ANT to sparse matrix multiplication, so that the same accelerator can anticipate RCPs in sparse fully-connected layers, transformers, and RNNs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2064787698",
                        "name": "Jonathan Lew"
                    },
                    {
                        "authorId": "2117421138",
                        "name": "Y. Liu"
                    },
                    {
                        "authorId": "2167130772",
                        "name": "Wenyi Gong"
                    },
                    {
                        "authorId": "51963313",
                        "name": "Negar Goli"
                    },
                    {
                        "authorId": "2115603904",
                        "name": "R. D. Evans"
                    },
                    {
                        "authorId": "1742561",
                        "name": "Tor M. Aamodt"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7ca1ee1f506c10dc156f5fa43d715e7a0985819a",
                "externalIds": {
                    "DBLP": "conf/nips/PaulLGFD22",
                    "ArXiv": "2206.01278",
                    "DOI": "10.48550/arXiv.2206.01278",
                    "CorpusId": 249375248
                },
                "corpusId": 249375248,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7ca1ee1f506c10dc156f5fa43d715e7a0985819a",
                "title": "Lottery Tickets on a Data Diet: Finding Initializations with Sparse Trainable Networks",
                "abstract": "A striking observation about iterative magnitude pruning (IMP; Frankle et al. 2020) is that $\\unicode{x2014}$ after just a few hundred steps of dense training $\\unicode{x2014}$ the method can find a sparse sub-network that can be trained to the same accuracy as the dense network. However, the same does not hold at step 0, i.e. random initialization. In this work, we seek to understand how this early phase of pre-training leads to a good initialization for IMP both through the lens of the data distribution and the loss landscape geometry. Empirically we observe that, holding the number of pre-training iterations constant, training on a small fraction of (randomly chosen) data suffices to obtain an equally good initialization for IMP. We additionally observe that by pre-training only on\"easy\"training data, we can decrease the number of steps necessary to find a good initialization for IMP compared to training on the full dataset or a randomly chosen subset. Finally, we identify novel properties of the loss landscape of dense networks that are predictive of IMP performance, showing in particular that more examples being linearly mode connected in the dense network correlates well with good initializations for IMP. Combined, these results provide new insight into the role played by the early phase training in IMP.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1690452",
                        "name": "Mansheej Paul"
                    },
                    {
                        "authorId": "152574768",
                        "name": "Brett W. Larsen"
                    },
                    {
                        "authorId": "25769960",
                        "name": "S. Ganguli"
                    },
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "2533850",
                        "name": "G. Dziugaite"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Inspired from (Evci et al., 2020), we take similar steps to update the local mask on each client.",
                "We highlight the contributions we make to extend the traditional sparse training techniques like Rigl (Evci et al., 2020), and Set (Mocanu\net al., 2018) to the proposed decentralized sparse training technique as follows: (i) To deal with the data heterogeneity and to learn different sparse models\u2026",
                "We highlight the contributions we make to extend the traditional sparse training techniques like Rigl (Evci et al., 2020), and Set (Mocanu",
                "The mask is initialized based on Erdos-Renyi Kernel (ERK) (Evci et al., 2020), which assigns higher sparsities to layers with more parameters and lower sparsities to layers with fewer parameters.",
                "We highlight the contributions we make to extend the traditional sparse training techniques like Rigl (Evci et al., 2020), and Set (Mocanu\net al., 2018) to the proposed decentralized sparse training technique as follows: (i) To deal with the data heterogeneity and to learn different sparse models for each client, decentralized sparse training operates on the local client instead of operating on the centralized device."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "88c2326aaacffccfd9ffc78b8b87cab90b7a6110",
                "externalIds": {
                    "ArXiv": "2206.00187",
                    "DBLP": "conf/icml/Dai0H0T22",
                    "DOI": "10.48550/arXiv.2206.00187",
                    "CorpusId": 249240036
                },
                "corpusId": 249240036,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/88c2326aaacffccfd9ffc78b8b87cab90b7a6110",
                "title": "DisPFL: Towards Communication-Efficient Personalized Federated Learning via Decentralized Sparse Training",
                "abstract": "Personalized federated learning is proposed to handle the data heterogeneity problem amongst clients by learning dedicated tailored local models for each user. However, existing works are often built in a centralized way, leading to high communication pressure and high vulnerability when a failure or an attack on the central server occurs. In this work, we propose a novel personalized federated learning framework in a decentralized (peer-to-peer) communication protocol named Dis-PFL, which employs personalized sparse masks to customize sparse local models on the edge. To further save the communication and computation cost, we propose a decentralized sparse training technique, which means that each local model in Dis-PFL only maintains a fixed number of active parameters throughout the whole local training and peer-to-peer communication process. Comprehensive experiments demonstrate that Dis-PFL significantly saves the communication bottleneck for the busiest node among all clients and, at the same time, achieves higher model accuracy with less computation cost and communication rounds. Furthermore, we demonstrate that our method can easily adapt to heterogeneous local clients with varying computation complexities and achieves better personalized performances.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2179580866",
                        "name": "Rong Dai"
                    },
                    {
                        "authorId": "2144035454",
                        "name": "Li Shen"
                    },
                    {
                        "authorId": "51209425",
                        "name": "Fengxiang He"
                    },
                    {
                        "authorId": "40434674",
                        "name": "Xinmei Tian"
                    },
                    {
                        "authorId": "2135519749",
                        "name": "Dacheng Tao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other criteria include gradient [7], low-rank decomposition [47], Hessian [22], etc.",
                "[7] proposed RigL for training sparse models without the need of a \u201clucky\u201d initializations.",
                "Some works [7] keep the first layer dense because the sparsity of the first layer will have a significant effect on the performance but has little effect on reducing the model size.",
                "Neural network pruning can be roughly categorized into structured pruning and unstructured pruning [7].",
                "The decay rule can be polynomial [25], sinusoidal [7], or other forms.",
                "We follow RigL [7], which regenerates part of the pruned connections based on the gradient magnitude.",
                "Evci et al. [7] proposed RigL for training sparse models without the need of a \u201clucky\u201d initializations."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7f02598dc27f348c86a09c059d7238d6298b8d5a",
                "externalIds": {
                    "DBLP": "conf/cvpr/ZhaoCXLF22",
                    "DOI": "10.1109/CVPRW56347.2022.00223",
                    "CorpusId": 251020191
                },
                "corpusId": 251020191,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7f02598dc27f348c86a09c059d7238d6298b8d5a",
                "title": "Pruning rPPG Networks: Toward Small Dense Network with Limited Number of Training Samples",
                "abstract": "Neural network pruning reduces network complexity and storage by removing unimportant connections in the network, enabling network miniaturization, fast training and inference, easy deployment to portable devices, etc. The emerging lottery ticket hypotheses and sparse initialization technique have shed new lights on the pruning research. However, few research focuses on the pruning of the networks for remote photoplethysmography (rPPG) pulse signal extraction. Opposite to the existing pruning researches that prune large network, rPPG networks are relatively small. It is interesting to see how it behaves when the pruning is applied. In this paper, we investigate the behavior of common pruning techniques when applied to an existing rPPG network. Experiments on PURE dataset show that the pruning rate decay is beneficial to the performance improvement, whereas the connection regeneration has a detrimental effect. Given the same final sparsity, dense initialization generally performs better than sparse initialization. The network seems insensitive to initial sparsity. The combination si=1.0, sf=0.1, with decay, and without regeneration is the best trade-off between SNR and FLOPs, achieving average SNR 9.78 dB, increased by 0.48 dB in comparison with the original PhysNet.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1985214",
                        "name": "Changchen Zhao"
                    },
                    {
                        "authorId": "2179019705",
                        "name": "Pengcheng Cao"
                    },
                    {
                        "authorId": "1466505567",
                        "name": "Shoushuai Xu"
                    },
                    {
                        "authorId": "2145367105",
                        "name": "Zhengguo Li"
                    },
                    {
                        "authorId": "3156839",
                        "name": "Yuanjing Feng"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d6e8c9425e9a376a47e6ca30c60a66d1227b9c1b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-04662",
                    "ArXiv": "2206.04662",
                    "DOI": "10.1109/CVPR52688.2022.01206",
                    "CorpusId": 249538140
                },
                "corpusId": 249538140,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d6e8c9425e9a376a47e6ca30c60a66d1227b9c1b",
                "title": "DiSparse: Disentangled Sparsification for Multitask Model Compression",
                "abstract": "Despite the popularity of Model Compression and Mul-titask Learning, how to effectively compress a multitask model has been less thoroughly analyzed due to the chal-lenging entanglement of tasks in the parameter space. In this paper, we propose DiSparse, a simple, effective, and first-of-its-kind multitask pruning and sparse training scheme. We consider each task independently by disentangling the importance measurement and take the unani-mous decisions among all tasks when performing parame-ter pruning and selection. Our experimental results demon-strate superior performance on various configurations and settings compared to popular sparse training and pruning methods. Besides the effectiveness in compression, DiS-parse also provides a powerful tool to the multitask learning community. Surprisingly, we even observed better per-formance than some dedicated multitask learning methods in several cases despite the high model sparsity enforced by DiSparse. We analyzed the pruning masks generated with DiSparse and observed strikingly similar sparse net-work architecture identified by each task even before the training starts. We also observe the existence of a \u201cwater-shed\u201d layer where the task relatedness sharply drops, implying no benefits in continued parameters sharing. Our code and models will be available at: https://github.com/SHI-Labs/DiSparse-Multitask-Model-Compression.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143900241",
                        "name": "Xing Sun"
                    },
                    {
                        "authorId": "2855934",
                        "name": "Ali Hassani"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "143983679",
                        "name": "Gao Huang"
                    },
                    {
                        "authorId": "48667025",
                        "name": "Humphrey Shi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We compressed the pre-trained state-of-the-art deraining models by the same FLOPs, for a fair comparison to the most classical (l1 [13]) and modern (erk [8], lamp [15]) pruning methods.",
                "Considering that the original data is not available, we mainly compare with the alternative magnitude-based pruning methods, including the most classical l1 regularization [13], and the most modern methods of erk [8] and lamp [15].",
                "In practice, various attempts have been made to compress the heavy CNN models, including quantization [11, 12, 23], pruning [8, 13, 15, 21], distillation [3, 7, 14], and so on."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3df9709065c9167d91f3fdfa3e621737c7e2ef32",
                "externalIds": {
                    "DBLP": "conf/cvpr/ZouWFC22",
                    "DOI": "10.1109/CVPR52688.2022.00593",
                    "CorpusId": 250056341
                },
                "corpusId": 250056341,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3df9709065c9167d91f3fdfa3e621737c7e2ef32",
                "title": "Dreaming to Prune Image Deraining Networks",
                "abstract": "Convolutional image deraining networks have achieved great success while suffering from tremendous computational and memory costs. Most model compression methods require original data for iterative fine-tuning, which is limited in real-world applications due to storage, privacy, and transmission constraints. We note that it is overstretched to fine-tune the compressed model using self-collected data, as it exhibits poor generalization over images with different degradation characteristics. To address this problem, we propose a novel data-free compression framework for de-raining networks. It is based on our observation that deep degradation representations can be clustered by degradation characteristics (types of rain) while independent of image content. Therefore, in our framework, we \u201cdream\u201d diverse in-distribution degraded images using a deep inversion paradigm, thus leveraging them to distill the pruned model. Specifically, we preserve the performance of the pruned model in a dual-branch way. In one branch, we invert the pre-trained model (teacher) to reconstruct the degraded inputs that resemble the original distribution and employ the orthogonal regularization for deep features to yield degradation diversity. In the other branch, the pruned model (student) is distilled to fit the teacher's original statistical modeling on these dreamed inputs. Further, an adaptive pruning scheme is proposed to determine the hierarchical sparsity, which alleviates the regression drift of the initial pruned model. Experiments on various deraining datasets demonstrate that our method can reduce about 40% FLOPs of the state-of-the-art models while maintaining comparable performance without original data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2169376207",
                        "name": "Wei Zou"
                    },
                    {
                        "authorId": "2155654785",
                        "name": "Yang Wang"
                    },
                    {
                        "authorId": "3061449",
                        "name": "Xueyang Fu"
                    },
                    {
                        "authorId": "145871531",
                        "name": "Yang Cao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These works adjust structures of sparse networks during training, including Deep Rewiring (DeepR) [2], Sparse Evolutionary Training (SET) [26], Dynamic Sparse Reparameterization (DSR) [30], Sparse Networks from Scratch (SNFS) [8], and Rigged Lottery (RigL) [10].",
                "Sparse Models in DRL [10, 36] show that finding a sparse model in DRL is difficult due to training instability.",
                "Many works [2, 26, 30, 8, 10] also try to train a sparse neural network from scratch without having to pre-trained dense models.",
                ", SET [26] and RigL [10], can train a 90%-sparse network (i.",
                "RigL [10], which uses dynamic sparse training by dropping and growing connections with magnitude and gradient criteria, respectively, the same as RLx2\u2019s topology evolution procedure.",
                "The topology evolution in RLx2 is made by adopting the RigL [10] method.",
                "Our RLx2 algorithm, which contains both topology evolution [10] (using the same scheme in TE) and accurate value estimation (using multi-step TD targets [19]), is able to achieve a performance close to TE+Q\u2217 without the need for pre-trained expert Q-values.",
                "Algorithm 1 Topology Evolution [10] 1: Nl: Number of parameters in layer l 2: \u03b8l: Parameters in layer l 3: M\u03b8l : Sparse mask of layer l 4: sl: Sparsity of layer l 5: L: Loss function 6: \u03b6t: Update fraction in training step t 7: for each layer l do 8: k = \u03b6t(1\u2212 sl)Nl 9: Idrop = ArgTopK(\u2212|\u03b8l M\u03b8l |, k) 10: Igrow = ArgTopKi/ \u2208\u03b8l M\u03b8l\\Idrop(|\u2207\u03b8lL, k|) 11: Update M\u03b8l according to Idrop and Igrow 12: \u03b8l \u2190 \u03b8l M\u03b8l 13: end for",
                "In particular, we apply a gradient-guided topology search scheme [10] to enable dynamic network evolution.",
                "Topology evolution RLx2 drops and grows connections with magnitude and gradient criteria, respectively, which has been adopted in RigL [10] for deep supervised learning."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "b831edf994dcfba95706bb93790c7e2cf697ab80",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-15043",
                    "ArXiv": "2205.15043",
                    "DOI": "10.48550/arXiv.2205.15043",
                    "CorpusId": 249192027
                },
                "corpusId": 249192027,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b831edf994dcfba95706bb93790c7e2cf697ab80",
                "title": "RLx2: Training a Sparse Deep Reinforcement Learning Model from Scratch",
                "abstract": "Training deep reinforcement learning (DRL) models usually requires high computation costs. Therefore, compressing DRL models possesses immense potential for training acceleration and model deployment. However, existing methods that generate small models mainly adopt the knowledge distillation-based approach by iteratively training a dense network. As a result, the training process still demands massive computing resources. Indeed, sparse training from scratch in DRL has not been well explored and is particularly challenging due to non-stationarity in bootstrap training. In this work, we propose a novel sparse DRL training framework,\"the Rigged Reinforcement Learning Lottery\"(RLx2), which builds upon gradient-based topology evolution and is capable of training a sparse DRL model based entirely on a sparse network. Specifically, RLx2 introduces a novel multi-step TD target mechanism with a dynamic-capacity replay buffer to achieve robust value learning and efficient topology exploration in sparse models. It also reaches state-of-the-art sparse training performance in several tasks, showing 7.5\\times-20\\times model compression with less than 3% performance degradation and up to 20\\times and 50\\times FLOPs reduction for training and inference, respectively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35472650",
                        "name": "Y. Tan"
                    },
                    {
                        "authorId": "35694520",
                        "name": "Pihe Hu"
                    },
                    {
                        "authorId": "144738857",
                        "name": "L. Pan"
                    },
                    {
                        "authorId": null,
                        "name": "Longbo Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Gradient-based regrowth e.g., momentum [Dettmers and Zettlemoyer, 2019] and gradient [Evci et al., 2020], shows strong results in image classification,\nwhereas random regrowth outperforms the former in language modeling [Dietrich et al., 2021].",
                ", 2021b]; one SST method: ERK [Evci et al., 2020]; and one pruning at initialization approach: SNIP [Lee et al.",
                "To ver-\nify the effectiveness of Sup-tickets, we apply it to various sparse training methods, including 3 DST methods: SET, RigL [Evci et al., 2020], and GraNet [Liu et al., 2021b]; one SST method: ERK [Evci et al., 2020]; and one pruning at initialization approach: SNIP [Lee et al., 2018].",
                "Among them, sparse neural network training [Mocanu et al., 2018, Evci et al., 2020, Bellec et al., 2018] stands out and receives growing attention recently due to its high efficiency in both the training and inference phases.",
                ", 2018] and its CNNs variant Erd\u0151s-R\u00e9nyi-Kernel (ERK) [Evci et al., 2020] allocates lower sparsity to smaller layers, avoiding the layer collapse problem [Tanaka et al.",
                "Inspired by the graph theory, Erdo\u030bs-R\u00e9nyi (ER) [Mocanu et al., 2018] and its CNNs variant Erdo\u030bs-R\u00e9nyi-Kernel (ERK) [Evci et al., 2020] allocates lower sparsity to smaller layers, avoiding the layer collapse problem [Tanaka et al., 2020] and achieving stronger results than the uniform sparsity in general.",
                ", momentum [Dettmers and Zettlemoyer, 2019] and gradient [Evci et al., 2020], shows strong results in image classification,",
                "Inspired by the graph theory, Erdo\u030bs-R\u00e9nyi (ER) [Mocanu et al., 2018] and its CNNs variant Erdo\u030bs-R\u00e9nyi-Kernel (ERK) [Evci et al., 2020] allocates lower sparsity to smaller layers, avoiding the layer collapse problem [Tanaka et al., 2020] and achieving stronger results than the uniform sparsity in\u2026",
                "Besides the sparse structures, in the most sparse training literature [Dettmers and Zettlemoyer, 2019, Evci et al., 2020, Mostafa and Wang, 2019, Liu et al., 2021b], it is usually a safe choice to keep the other training configurations, such as optimizers, hyperparameters, and learning rate\u2026",
                "ify the effectiveness of Sup-tickets, we apply it to various sparse training methods, including 3 DST methods: SET, RigL [Evci et al., 2020], and GraNet [Liu et al.",
                "%) usually remains below the full dense training under a regular training epoch number [Evci et al., 2020, Liu et al., 2021b]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "723cb6687b2138b494d40ea1b376731851846731",
                "externalIds": {
                    "ArXiv": "2205.15322",
                    "DBLP": "conf/uai/0006MFHPP22",
                    "DOI": "10.48550/arXiv.2205.15322",
                    "CorpusId": 249210187
                },
                "corpusId": 249210187,
                "publicationVenue": {
                    "id": "f9af8000-42f8-410d-a622-e8811e41660a",
                    "name": "Conference on Uncertainty in Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Uncertainty in Artificial Intelligence",
                        "UAI",
                        "Conf Uncertain Artif Intell",
                        "Uncertain Artif Intell"
                    ],
                    "url": "http://www.auai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/723cb6687b2138b494d40ea1b376731851846731",
                "title": "Superposing Many Tickets into One: A Performance Booster for Sparse Neural Network Training",
                "abstract": "Recent works on sparse neural network training (sparse training) have shown that a compelling trade-off between performance and efficiency can be achieved by training intrinsically sparse neural networks from scratch. Existing sparse training methods usually strive to find the best sparse subnetwork possible in one single run, without involving any expensive dense or pre-training steps. For instance, dynamic sparse training (DST), is capable of reaching a competitive performance of dense training by iteratively evolving the sparse topology during the course of training. In this paper, we argue that it is better to allocate the limited resources to create multiple low-loss sparse subnetworks and superpose them into a stronger one, instead of allocating all resources entirely to find an individual subnetwork. To achieve this, two desiderata are required: (1) efficiently producing many low-loss subnetworks, the so-called cheap tickets, within one training process limited to the standard training time used in dense training; (2) effectively superposing these cheap tickets into one stronger subnetwork. To corroborate our conjecture, we present a novel sparse training approach, termed Sup-tickets, which can satisfy the above two desiderata concurrently in a single sparse-to-sparse training process. Across various modern architectures on CIFAR-10/100 and ImageNet, we show that Sup-tickets integrates seamlessly with the existing sparse training methods and demonstrates consistent performance improvement.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1410465360",
                        "name": "Lu Yin"
                    },
                    {
                        "authorId": "2266428",
                        "name": "Vlado Menkovski"
                    },
                    {
                        "authorId": "2055723958",
                        "name": "Meng Fang"
                    },
                    {
                        "authorId": "8242939",
                        "name": "Tianjin Huang"
                    },
                    {
                        "authorId": "1382535564",
                        "name": "Yulong Pei"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Pruning aware training is currently an actively researched topic and a few works have presented impressive results that increase the sparsity of the kernel/weight parameters up to 80% [21] during the training.",
                "This evaluation shows that a nominal sparsity range of 50% to 80% is achievable for most networks with minor accuracy loss, which was also the similar sparsity presented in [21]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "33daacd818df74bafd50774de35fe0674bd61982",
                "externalIds": {
                    "DBLP": "conf/iscas/SudarshanSWW22",
                    "DOI": "10.1109/ISCAS48785.2022.9937832",
                    "CorpusId": 253461038
                },
                "corpusId": 253461038,
                "publicationVenue": {
                    "id": "9bc219ae-a4dc-4241-8e1a-0552f9ee9ef7",
                    "name": "International Symposium on Circuits and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "ISCAS",
                        "Int Symp Circuit Syst"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/33daacd818df74bafd50774de35fe0674bd61982",
                "title": "Optimization of DRAM based PIM Architecture for Energy-Efficient Deep Neural Network Training",
                "abstract": "Deep Neural Network (DNN) training consumes high-energy. On the other hand, DNNs deployed on edge devices demand very high-energy efficiency. In this context, Processing-in-Memory (PIM) is an emerging compute paradigm that bridges the memory-computation gap to improve the energy-efficiency. DRAMs are one such memory type employed for designing energy-efficient PIM architectures for DNN training. One of the major issues of DRAM-PIM architectures designed for DNN training is the high number of internal data accesses within a bank between the memory arrays and the PIM computation units (e.g. 51% more than inference). These internal data accesses in the state-of-the-art DRAM PIM architectures consume very high energy compared to computation units. Hence, it is important to reduce the internal data access energy within the DRAM bank for further improving the energy efficiency of DRAMPIM architectures. We present three novel optimizations that together reduce the internal data access energy up to 81.54%. Our first optimization modifies the bank data access circuit to enable partial accesses of data instead of the conventional fixed granularity accesses, thereby exploiting the available sparsity during training. The second optimization is to have a dedicated low-energy region within the DRAM bank that has low capacitive load of global wires and shorter data movement. Finally, we propose a 12-bit high dynamic range floating-point format called TinyFloat that reduces the total number of data access energy by 20% compared to IEEE 754 half and single precision.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40975335",
                        "name": "C. Sudarshan"
                    },
                    {
                        "authorId": "2114518611",
                        "name": "Mohammad Hassani Sadi"
                    },
                    {
                        "authorId": "22917969",
                        "name": "C. Weis"
                    },
                    {
                        "authorId": "1690688",
                        "name": "N. Wehn"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[16] observed that the FLOP count of a ResNet-50 model at a fixed global sparsity can vary by a factor of over 2\u00d7 due to differences in the individual sparsities of convolutional layers with differing output dimensions.",
                "Our baselines are iterative magnitude pruning [48], RigL with the Erdos-Renyi-Kernel (ERK) sparsity distribution [16], Soft Threshold Weight Reparameterization (STR) [25], probabilistic masking (ProbMask) [47], OptG [46], and Top-KAST with Powerpropagation and ERK [24; 37].",
                "Spartan belongs to the family of \u201cparameter dense\u201d training algorithms that maintains a dense parameter vector \u03b8 \u2208 R throughout training [48; 24], in contrast to \u201cparameter sparse\u201d training algorithms that adhere to a \u00d5(k) memory budget for representing the parameters of a ksparse model [3; 31; 32; 13; 16].",
                "For Top-KAST, we exclude the first convolutional layer from pruning (following [37; 16]) and we use fully dense backward passes (i.",
                "Since Spartan retains a dense parameter vector and computes dense backward passes during training, it incurs higher memory and computational costs in each iteration than methods like RigL [16] that use both a sparse parameters and sparse backward passes."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "210c47fc0c16bf1cfc9beeb01faf70fcdbd3b978",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-14107",
                    "ArXiv": "2205.14107",
                    "DOI": "10.48550/arXiv.2205.14107",
                    "CorpusId": 249151950
                },
                "corpusId": 249151950,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/210c47fc0c16bf1cfc9beeb01faf70fcdbd3b978",
                "title": "Spartan: Differentiable Sparsity via Regularized Transportation",
                "abstract": "We present Spartan, a method for training sparse neural network models with a predetermined level of sparsity. Spartan is based on a combination of two techniques: (1) soft top-k masking of low-magnitude parameters via a regularized optimal transportation problem and (2) dual averaging-based parameter updates with hard sparsification in the forward pass. This scheme realizes an exploration-exploitation tradeoff: early in training, the learner is able to explore various sparsity patterns, and as the soft top-k approximation is gradually sharpened over the course of training, the balance shifts towards parameter optimization with respect to a fixed sparsity mask. Spartan is sufficiently flexible to accommodate a variety of sparsity allocation policies, including both unstructured and block structured sparsity, as well as general cost-sensitive sparsity allocation mediated by linear models of per-parameter costs. On ImageNet-1K classification, Spartan yields 95% sparse ResNet-50 models and 90% block sparse ViT-B/16 models while incurring absolute top-1 accuracy losses of less than 1% compared to fully dense training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8421815",
                        "name": "Kai Sheng Tai"
                    },
                    {
                        "authorId": "2166787704",
                        "name": "Taipeng Tian"
                    },
                    {
                        "authorId": "153317808",
                        "name": "S. Lim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We do not prune biases and batch-normalization parameters, as they only account for a small fraction of the total number of parameters yet are crucial for obtaining well-performing models [19]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a19a194f84a5bea38b91896f874d872809fa3ab9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-11921",
                    "ArXiv": "2205.11921",
                    "DOI": "10.48550/arXiv.2205.11921",
                    "CorpusId": 249017992
                },
                "corpusId": 249017992,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a19a194f84a5bea38b91896f874d872809fa3ab9",
                "title": "Compression-aware Training of Neural Networks using Frank-Wolfe",
                "abstract": "Many existing Neural Network pruning approaches either rely on retraining to compensate for pruning-caused performance degradation or they induce strong biases to converge to a specific sparse solution throughout training. A third paradigm obtains a wide range of compression ratios from a single dense training run while also avoiding retraining. Recent work of Pokutta et al. (2020) and Miao et al. (2022) suggests that the Stochastic Frank-Wolfe (SFW) algorithm is particularly suited for training state-of-the-art models that are robust to compression. We propose leveraging $k$-support norm ball constraints and demonstrate significant improvements over the results of Miao et al. (2022) in the case of unstructured pruning. We also extend these ideas to the structured pruning domain and propose novel approaches to both ensure robustness to the pruning of convolutional filters as well as to low-rank tensor decompositions of convolutional layers. In the latter case, our approach performs on-par with nuclear-norm regularization baselines while requiring only half of the computational resources. Our findings also indicate that the robustness of SFW-trained models largely depends on the gradient rescaling of the learning rate and we establish a theoretical foundation for that practice.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2056708985",
                        "name": "Max Zimmer"
                    },
                    {
                        "authorId": "2064617407",
                        "name": "Christoph Spiegel"
                    },
                    {
                        "authorId": "145729210",
                        "name": "S. Pokutta"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Current techniques can be mainly categorised as the iterative sparsification of a densely initialised network (Gale et al., 2019) or techniques that maintain constant sparsity\nthroughout learning (e.g. Evci et al., 2020; Jayakumar et al., 2020).",
                "It is also worth pointing out that existing hand-designed sparsity distributions (e.g. Mocanu et al., 2018; Evci et al., 2020) would result in a different pattern, allocating equal sparsity to layers 2-4, whereas our empirical results suggest this might not be optimal in all cases."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "2270ebe7d3ee925abbc062b937aa43805c702cf9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-08957",
                    "ArXiv": "2205.08957",
                    "DOI": "10.48550/arXiv.2205.08957",
                    "CorpusId": 248863054
                },
                "corpusId": 248863054,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2270ebe7d3ee925abbc062b937aa43805c702cf9",
                "title": "Meta-Learning Sparse Compression Networks",
                "abstract": "Recent work in Deep Learning has re-imagined the representation of data as functions mapping from a coordinate space to an underlying continuous signal. When such functions are approximated by neural networks this introduces a compelling alternative to the more common multi-dimensional array representation. Recent work on such Implicit Neural Representations (INRs) has shown that - following careful architecture search - INRs can outperform established compression methods such as JPEG (e.g. Dupont et al., 2021). In this paper, we propose crucial steps towards making such ideas scalable: Firstly, we employ state-of-the-art network sparsification techniques to drastically improve compression. Secondly, introduce the first method allowing for sparsification to be employed in the inner-loop of commonly used Meta-Learning algorithms, drastically improving both compression and the computational cost of learning INRs. The generality of this formalism allows us to present results on diverse data modalities such as images, manifolds, signed distance functions, 3D shapes and scenes, several of which establish new state-of-the-art results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144735987",
                        "name": "Jonathan Schwarz"
                    },
                    {
                        "authorId": "1725303",
                        "name": "Y. Teh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "de3ccbae89aa1462b207888f232ba82b8398f6e7",
                "externalIds": {
                    "ArXiv": "2205.08099",
                    "DBLP": "journals/corr/abs-2205-08099",
                    "DOI": "10.1007/s10462-023-10489-1",
                    "CorpusId": 248834207
                },
                "corpusId": 248834207,
                "publicationVenue": {
                    "id": "ea8553fe-2467-4367-afee-c4deb3754820",
                    "name": "Artificial Intelligence Review",
                    "type": "journal",
                    "alternate_names": [
                        "Artif Intell Rev"
                    ],
                    "issn": "0269-2821",
                    "url": "https://link.springer.com/journal/10462"
                },
                "url": "https://www.semanticscholar.org/paper/de3ccbae89aa1462b207888f232ba82b8398f6e7",
                "title": "Dimensionality reduced training by pruning and freezing parts of a deep neural network: a survey",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2029488873",
                        "name": "Paul Wimmer"
                    },
                    {
                        "authorId": "144442281",
                        "name": "Jens Mehnert"
                    },
                    {
                        "authorId": "2063161",
                        "name": "A. Condurache"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2021b) or by starting the pruning not from a dense but a sparse random architecture (Evci et al., 2020; Liu et al., 2021b).",
                "This can also be achieved with the help of core sets (Zhang et al., 2021b) or by starting the pruning not from a dense but a sparse random architecture (Evci et al., 2020; Liu et al., 2021b)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "993100a085dbda650fa7f2742bbebcc1c23e66b7",
                "externalIds": {
                    "DBLP": "conf/icml/Burkholz22",
                    "ArXiv": "2205.02343",
                    "DOI": "10.48550/arXiv.2205.02343",
                    "CorpusId": 248524888
                },
                "corpusId": 248524888,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/993100a085dbda650fa7f2742bbebcc1c23e66b7",
                "title": "Convolutional and Residual Networks Provably Contain Lottery Tickets",
                "abstract": "The Lottery Ticket Hypothesis continues to have a profound practical impact on the quest for small scale deep neural networks that solve modern deep learning tasks at competitive performance. These lottery tickets are identified by pruning large randomly initialized neural networks with architectures that are as diverse as their applica-tions. Yet, theoretical insights that attest their existence have been mostly focused on deep fully-connected feed forward networks with ReLU activation functions. We prove that also modern architectures consisting of convolutional and residual layers that can be equipped with almost arbitrary activation functions can contain lottery tickets with high probability.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8813467",
                        "name": "R. Burkholz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It can therefore be beneficial to start pruning from a sparse random architecture rather than a dense network (Evci et al., 2020; Liu et al., 2021b), which saves computational resources.",
                ", 2021a), but also to reduce the computational burden associated with deep learning (You et al., 2020; Evci et al., 2020; Liu et al., 2021b)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "613f0a3f6fad7fca07f50aaf49ad7d53ff4dce78",
                "externalIds": {
                    "ArXiv": "2205.02321",
                    "DBLP": "journals/corr/abs-2205-02321",
                    "DOI": "10.48550/arXiv.2205.02321",
                    "CorpusId": 248524809
                },
                "corpusId": 248524809,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/613f0a3f6fad7fca07f50aaf49ad7d53ff4dce78",
                "title": "Most Activation Functions Can Win the Lottery Without Excessive Depth",
                "abstract": "The strong lottery ticket hypothesis has highlighted the potential for training deep neural networks by pruning, which has inspired interesting practical and theoretical insights into how neural networks can represent functions. For networks with ReLU activation functions, it has been proven that a target network with depth $L$ can be approximated by the subnetwork of a randomly initialized neural network that has double the target's depth $2L$ and is wider by a logarithmic factor. We show that a depth $L+1$ network is sufficient. This result indicates that we can expect to find lottery tickets at realistic, commonly used depths while only requiring logarithmic overparametrization. Our novel construction approach applies to a large class of activation functions and is not limited to ReLUs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8813467",
                        "name": "R. Burkholz"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5674081d096bfb0686fb3e68aa4c998bdaf4cf70",
                "externalIds": {
                    "DBLP": "journals/nn/ChenLYLL22",
                    "DOI": "10.1016/j.neunet.2022.05.002",
                    "CorpusId": 248711030,
                    "PubMed": "35609502"
                },
                "corpusId": 248711030,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5674081d096bfb0686fb3e68aa4c998bdaf4cf70",
                "title": "LAP: Latency-aware automated pruning with dynamic-based filter selection",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2128692360",
                        "name": "Zailong Chen"
                    },
                    {
                        "authorId": "2782087",
                        "name": "Chubo Liu"
                    },
                    {
                        "authorId": "1729226",
                        "name": "Wangdong Yang"
                    },
                    {
                        "authorId": "145730774",
                        "name": "Kenli Li"
                    },
                    {
                        "authorId": "69486668",
                        "name": "Kuan-Ching Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent strategies [14, 30] dynamically extract and train sparse subnetworks instead of training the full models."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "40d50fad844ace7c9bb20486504ee6b3bed7480d",
                "externalIds": {
                    "ArXiv": "2205.00334",
                    "CorpusId": 261533336
                },
                "corpusId": 261533336,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/40d50fad844ace7c9bb20486504ee6b3bed7480d",
                "title": "Engineering flexible machine learning systems by traversing functionally-invariant paths",
                "abstract": "Transformers have emerged as the state of the art neural network architecture for natural language processing and computer vision. In the foundation model paradigm, large transformer models (BERT, GPT3/4, Bloom, ViT) are pre-trained on self-supervised tasks such as word or image masking, and then, adapted through fine-tuning for downstream user applications including instruction following and Question Answering. While many approaches have been developed for model fine-tuning including low-rank weight update strategies (eg. LoRA), underlying mathematical principles that enable network adaptation without knowledge loss remain poorly understood. Here, we introduce a differential geometry framework, functionally invariant paths (FIP), that provides flexible and continuous adaptation of neural networks for a range of machine learning goals and network sparsification objectives. We conceptualize the weight space of a neural network as a curved Riemannian manifold equipped with a metric tensor whose spectrum defines low rank subspaces in weight space that accommodate network adaptation without loss of prior knowledge. We formalize adaptation as movement along a geodesic path in weight space while searching for networks that accommodate secondary objectives. With modest computational resources, the FIP algorithm achieves comparable to state of the art performance on continual learning and sparsification tasks for language models (BERT), vision transformers (ViT, DeIT), and the CNNs. Broadly, we conceptualize a neural network as a mathematical object that can be iteratively transformed into distinct configurations by the path-sampling algorithm to define a sub-manifold of weight space that can be harnessed to achieve user goals.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2059927449",
                        "name": "G. Raghavan"
                    },
                    {
                        "authorId": "2237833287",
                        "name": "Bahey Tharwat"
                    },
                    {
                        "authorId": "2055315595",
                        "name": "S. Hari"
                    },
                    {
                        "authorId": "2237834054",
                        "name": "Dhruvil Satani"
                    },
                    {
                        "authorId": "2232953358",
                        "name": "Matt Thomson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, the empirical success of the popular sparse algorithms implies that the tolerance to such error can be quite large in practice [7], [21], [22], [25], [30], which enables us to implement sparse training in FL for communication efficiency, and meanwhile utilizes the properties of sparse models to address the unreliable communications.",
                "Moreover, the sparse topology\u2019s updates based on parameter magnitudes and infrequent gradient calculations in [25] loosened the limitation on the size relationship between sparse model and the corresponding dense model, which further reduced the computation cost for sparse learning.",
                "Sparsity enabled Communication Efficiency and Similarity assisted Bias Reduction: To save computing resources and training/inference time, sparse learning on large neural networks has been widely deployed in the deep learning field [7], [21], [22], [25], [30]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c359de92b568594aba27dfccb3a90f50d2481a09",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-02321",
                    "ArXiv": "2204.02321",
                    "DOI": "10.48550/arXiv.2204.02321",
                    "CorpusId": 247957783
                },
                "corpusId": 247957783,
                "publicationVenue": {
                    "id": "4e46790b-e240-4236-9b8d-a70ed74f900a",
                    "name": "IEEE Transactions on Mobile Computing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Mob Comput"
                    ],
                    "issn": "1536-1233",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=7755",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tmc"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c359de92b568594aba27dfccb3a90f50d2481a09",
                "title": "SAFARI: Sparsity enabled Federated Learning with Limited and Unreliable Communications",
                "abstract": "Federated learning (FL) enables edge devices to collaboratively learn a model in a distributed fashion. Many existing researches have focused on improving communication efficiency of high-dimensional models and addressing bias caused by local updates. However, most of FL algorithms are either based on reliable communications or assume fixed and known unreliability characteristics. In practice, networks could suffer from dynamic channel conditions and non-deterministic disruptions, with time-varying and unknown characteristics. To this end, in this paper we propose a sparsity enabled FL framework with both communication efficiency and bias reduction, termed as SAFARI. It makes novel use of a similarity among client models to rectify and compensate for bias that is resulted from unreliable communications. More precisely, sparse learning is implemented on local clients to mitigate communication overhead, while to cope with unreliable communications, a similarity-based compensation method is proposed to provide surrogates for missing model updates. We analyze SAFARI under bounded dissimilarity and with respect to sparse models. It is demonstrated that SAFARI under unreliable communications is guaranteed to converge at the same rate as the standard FedAvg with perfect communications. Implementations and evaluations on CIFAR-10 dataset validate the effectiveness of SAFARI by showing that it can achieve the same convergence speed and accuracy as FedAvg with perfect communications, with up to 80% of the model weights being pruned and a high percentage of client updates missing in each round.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2087355867",
                        "name": "Yuzhu Mao"
                    },
                    {
                        "authorId": "2156163661",
                        "name": "Zihao Zhao"
                    },
                    {
                        "authorId": "3114309",
                        "name": "Meilin Yang"
                    },
                    {
                        "authorId": "144362942",
                        "name": "Le Liang"
                    },
                    {
                        "authorId": "40457423",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "35724716",
                        "name": "Wenbo Ding"
                    },
                    {
                        "authorId": "48445858",
                        "name": "Tian Lan"
                    },
                    {
                        "authorId": "2144523777",
                        "name": "Xiaoping Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We train the sparse dynamic convolution following an iterative pruning process [11, 13]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "69ec695050a210c056602449bfc5e87591faff8c",
                "externalIds": {
                    "ArXiv": "2204.02227",
                    "DBLP": "conf/wacv/HeJDD23",
                    "DOI": "10.1109/WACV56688.2023.00639",
                    "CorpusId": 252815542
                },
                "corpusId": 252815542,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/69ec695050a210c056602449bfc5e87591faff8c",
                "title": "SD-Conv: Towards the Parameter-Efficiency of Dynamic Convolution",
                "abstract": "Dynamic convolution achieves better performance for efficient CNNs at the cost of negligible FLOPs increase. However, the performance increase can not match the significantly expanded number of parameters, which is the main bottleneck in real-world applications. Contrastively, mask-based unstructured pruning obtains a lightweight network by removing redundancy in the heavy network. In this paper, we propose a new framework, Sparse Dynamic Convolution (SD-CONV), to naturally integrate these two paths such that it can inherit the advantage of dynamic mechanism and sparsity. We first design a binary mask derived from a learnable threshold to prune static kernels, significantly reducing the parameters and computational cost but achieving higher performance in Imagenet-1K. We further transfer pretrained models into a variety of downstream tasks, showing consistently better results than baselines. We hope our SD-Conv could be an efficient alternative to conventional dynamic convolutions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152235390",
                        "name": "Shwai He"
                    },
                    {
                        "authorId": "2161510458",
                        "name": "Chenbo Jiang"
                    },
                    {
                        "authorId": "2187286687",
                        "name": "Daize Dong"
                    },
                    {
                        "authorId": null,
                        "name": "Liang Ding"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We conjecture that the sparse property [16,18] has reduced the redundancy in high-resolution feature maps in our HRCA and leads to higher performance and efficiency."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "346adcf3ab9cbd06d816586ad30bd3112a5abd0f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-01244",
                    "ArXiv": "2204.01244",
                    "DOI": "10.1109/CVPR52729.2023.01087",
                    "CorpusId": 247939311
                },
                "corpusId": 247939311,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/346adcf3ab9cbd06d816586ad30bd3112a5abd0f",
                "title": "Dynamic Focus-aware Positional Queries for Semantic Segmentation",
                "abstract": "The DETR-like segmentors have underpinned the most recent breakthroughs in semantic segmentation, which end-to-end train a set of queries representing the class prototypes or target segments. Recently, masked attention [8] is proposed to restrict each query to only attend to the foreground regions predicted by the preceding decoder block for easier optimization. Although promising, it relies on the learnable parameterized positional queries which tend to encode the dataset statistics, leading to inaccurate localization for distinct individual queries. In this paper, we propose a simple yet effective query design for semantic segmentation termed Dynamic Focus-aware Positional Queries (DFPQ), which dynamically generates positional queries conditioned on the cross-attention scores from the preceding decoder block and the positional encodings for the corresponding image features, simultaneously. Therefore, our DFPQ preserves rich localization information for the target segments and provides accurate and fine-grained positional priors. In addition, we propose to efficiently deal with high-resolution cross-attention by only aggregating the con-textual tokens based on the low-resolution cross-attention scores to perform local relation aggregation. Extensive experiments on ADE20K and Cityscapes show that with the two modifications on Mask2former, our framework achieves SOTA performance and outperforms Mask2former by clear margins of 1.1%, 1.9%, and 1.1% single-scale mIoU with ResNet-50, Swin-T, and Swin-B backbones on the ADE20K validation set, respectively. Source code is available at https://github.com/ziplab/FASeg.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "11270586",
                        "name": "Haoyu He"
                    },
                    {
                        "authorId": "2152629962",
                        "name": "Jianfei Cai"
                    },
                    {
                        "authorId": "1840579673",
                        "name": "Zizheng Pan"
                    },
                    {
                        "authorId": "49270464",
                        "name": "Jing Liu"
                    },
                    {
                        "authorId": "1519070643",
                        "name": "Jing Zhang"
                    },
                    {
                        "authorId": "2075330732",
                        "name": "Dacheng Tao"
                    },
                    {
                        "authorId": "3194022",
                        "name": "Bohan Zhuang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In sparse training and efficient Auto-ML algorithms, it is a common practice to estimate future ranking of models with current parameters and their gradients [22, 67], or with parameters after a single step of gradient descent update [9, 51, 53]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "159be298e25b7210ae577d7962cceb5e73aee687",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-14509",
                    "ArXiv": "2203.14509",
                    "DOI": "10.1109/CVPR52688.2022.01216",
                    "CorpusId": 247763137
                },
                "corpusId": 247763137,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/159be298e25b7210ae577d7962cceb5e73aee687",
                "title": "Automated Progressive Learning for Efficient Training of Vision Transformers",
                "abstract": "Recent advances in vision Transformers (ViTs) have come with a voracious appetite for computing power, highlighting the urgent need to develop efficient training methods for ViTs. Progressive learning, a training scheme where the model capacity grows progressively during training, has started showing its ability in efficient training. In this paper, we take a practical step towards efficient training of ViTs by customizing and automating progressive learning. First, we develop a strong manual baseline for progressive learning of ViTs, by introducing momentum growth (MoGrow) to bridge the gap brought by model growth. Then, we propose automated progressive learning (AutoProg), an efficient training scheme that aims to achieve lossless acceleration by automatically increasing the training overload on-the-fly; this is achieved by adaptively deciding whether, where and how much should the model grow during progressive learning. Specifically, we first relax the optimization of the growth schedule to sub-network architecture optimization problem, then propose one-shot estimation of the sub-network performance via an elastic supernet. The searching overhead is reduced to minimal by recycling the parameters of the supernet. Extensive experiments of efficient training on ImageNet with two representative ViT models, DeiT and VOLO, demonstrate that AutoProg can accelerate ViTs training by up to 85.1% with no performance drop.11Code:https://github.com/changlin31/AutoProg.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46651877",
                        "name": "Changlin Li"
                    },
                    {
                        "authorId": "3194022",
                        "name": "Bohan Zhuang"
                    },
                    {
                        "authorId": "2749191",
                        "name": "Guangrun Wang"
                    },
                    {
                        "authorId": "13246332",
                        "name": "Xiaodan Liang"
                    },
                    {
                        "authorId": "144950946",
                        "name": "Xiaojun Chang"
                    },
                    {
                        "authorId": "1698559",
                        "name": "Yi Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As for random pruning, every layer can be uniformly pruned with the same pre-defined pruning ratio (Mariet and Sra, 2015; He et al., 2017; Gale et al., 2019) or the pruning ratio can be varied for different layers such as Erdo\u0308-Re\u0301nyi (Mocanu et al., 2018) and Erdo\u0308-Re\u0301nyi Kernel (Evci et al., 2020).",
                ", 2021a) explores the sparsity pattern in a prune-and-grow scheme according to some criteria (Mocanu et al., 2018; Mostafa and Wang, 2019; Dettmers and Zettlemoyer, 2019; Evci et al., 2020; Ye et al., 2020; Jayakumar et al., 2020; Liu et al., 2021b).",
                "\u2026for random pruning, every layer can be uniformly pruned with the same pre-defined pruning ratio (Mariet and Sra, 2015; He et al., 2017; Gale et al., 2019) or the pruning ratio can be varied for different layers such as Erdo\u0308-Re\u0301nyi (Mocanu et al., 2018) and Erdo\u0308-Re\u0301nyi Kernel (Evci et al., 2020).",
                "\u2026sparse training (Mocanu et al., 2018; Liu et al., 2021a) explores the sparsity pattern in a prune-and-grow scheme according to some criteria (Mocanu et al., 2018; Mostafa and Wang, 2019; Dettmers and Zettlemoyer, 2019; Evci et al., 2020; Ye et al., 2020; Jayakumar et al., 2020; Liu et al., 2021b)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "0dd693206e94be5d10305d4b9e73ab68024a499a",
                "externalIds": {
                    "DBLP": "conf/aistats/YangW23",
                    "ArXiv": "2203.14328",
                    "CorpusId": 257631949
                },
                "corpusId": 257631949,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0dd693206e94be5d10305d4b9e73ab68024a499a",
                "title": "On the Neural Tangent Kernel Analysis of Randomly Pruned Neural Networks",
                "abstract": "Motivated by both theory and practice, we study how random pruning of the weights affects a neural network's neural tangent kernel (NTK). In particular, this work establishes an equivalence of the NTKs between a fully-connected neural network and its randomly pruned version. The equivalence is established under two cases. The first main result studies the infinite-width asymptotic. It is shown that given a pruning probability, for fully-connected neural networks with the weights randomly pruned at the initialization, as the width of each layer grows to infinity sequentially, the NTK of the pruned neural network converges to the limiting NTK of the original network with some extra scaling. If the network weights are rescaled appropriately after pruning, this extra scaling can be removed. The second main result considers the finite-width case. It is shown that to ensure the NTK's closeness to the limit, the dependence of width on the sparsity parameter is asymptotically linear, as the NTK's gap to its limit goes down to zero. Moreover, if the pruning probability is set to zero (i.e., no pruning), the bound on the required width matches the bound for fully-connected neural networks in previous works up to logarithmic factors. The proof of this result requires developing a novel analysis of a network structure which we called \\textit{mask-induced pseudo-networks}. Experiments are provided to evaluate our results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118571035",
                        "name": "Hongru Yang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "All these types of unstructured pruning only reduce the memory footprint [9, 8].",
                "Most of the pruning methods focus on pruning the weights [8, 9, 12, 18]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "dba2c4505dda38a57413064080513a65e1df7a34",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-10991",
                    "ArXiv": "2203.10991",
                    "DOI": "10.48550/arXiv.2203.10991",
                    "CorpusId": 247594087
                },
                "corpusId": 247594087,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dba2c4505dda38a57413064080513a65e1df7a34",
                "title": "Optimal Fine-Grained N: M sparsity for Activations and Neural Gradients",
                "abstract": "In deep learning, fine-grained N:M sparsity reduces the data footprint and bandwidth of a General Matrix multiply (GEMM) by x2, and doubles throughput by skipping computation of zero values. So far, it was only used to prune weights. We examine how this method can be used also for activations and their gradients (i.e.,\"neural gradients\"). To this end, we first establish a tensor-level optimality criteria. Previous works aimed to minimize the mean-square-error (MSE) of each pruned block. We show that while minimization of the MSE works fine for pruning the activations, it catastrophically fails for the neural gradients. Instead, we show that optimal pruning of the neural gradients requires an unbiased minimum-variance pruning mask. We design such specialized masks, and find that in most cases, 1:2 sparsity is sufficient for training, and 2:4 sparsity is usually enough when this is not the case. Further, we suggest combining several such methods together in order to potentially speed up training even more. A reference implementation is supplied in https://github.com/brianchmiel/Act-and-Grad-structured-sparsity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "104387774",
                        "name": "Brian Chmiel"
                    },
                    {
                        "authorId": "2477463",
                        "name": "Itay Hubara"
                    },
                    {
                        "authorId": "2607278",
                        "name": "Ron Banner"
                    },
                    {
                        "authorId": "1912398",
                        "name": "Daniel Soudry"
                    }
                ]
            }
        },
        {
            "contexts": [
                "5.3, we discuss IP and SP for more sophisticated sparse training methods, namely LTs [15] and the DST methods SET [49] and RigL [13].",
                "Table 2 shows training and test accuracy for the IP- and SP versions of SET and SNIP for a VGG16 on CIFAR-10 as well as RigL for a ResNet50 on ImageNet.",
                "For training sparse networks, we distinguish between (i) pruning at initialization (PaI) [9,35,66,71,75] which prunes the network at initialization and fixes zeroed parameters during training, (ii) finding the sparse architecture to be finally trained by iterative train-prune-reset cycles, a so called lottery ticket (LT) [14,15], and (iii) dynamic sparse training (DST) [13,39,49] which prunes the network at initialization, but allows the pruning mask to be changed during training.",
                "We demonstrate this by achieving SOTA results with the application of IP to SOTA standard PaI, LT, DST as well as classical pruning methods.",
                "Furthermore, we want to check if IP boosts the SOTA methods LT and RigL as well.",
                "3, we discuss IP and SP for more sophisticated sparse training methods, namely LTs [15] and the DST methods SET [49] and RigL [13].",
                "SET regrows coefficients randomly whereas RigL regrows those with high gradient magnitude.",
                "Related work covers general pruning and pruning before training and DST. Training a sparse model allows to learn non-zero FB coefficients and FBs jointly from scratch.",
                "5 compare SP and IP on various sparse training and other pruning methods, namely:\nDST randomly prunes the model at initialization.",
                "As shown in this work, using expensive pre-training to find a better pruning mask via LTs or adapting suppR during training via DST further improves IP\u2019s performance.",
                "For SP, more expensive or sophisticated methods like LT and DST improve sparse training results compared to PaI.",
                "became of interest, providing the benefits of reduced memory requirements and runtime not only for inference but also for training [13, 14, 35, 47, 49, 55, 66, 71, 75].",
                "RigL [13] improves this by recovering those weights with the biggest gradient magnitude.",
                "Dynamic sparse training [10,13,39,49] adjusts pruning masks during training to ensure sparse networks while adapting the architecture to different conditions.",
                "DST and LT on CIFAR-10.",
                "IP improves DST and LTs significantly, see Figs.",
                "The pruning mask is updated each 1, 500 iterations for SET and 4, 000 for RigL.",
                "Table 1 compares IP and SP on the SOTA pruning methods RigL [13], GMP [17] and FT [59]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "cd01df0c950e733021d536f9e1a0f28a14941c2e",
                "externalIds": {
                    "DBLP": "conf/cvpr/WimmerMC22",
                    "ArXiv": "2203.07808",
                    "DOI": "10.1109/CVPR52688.2022.01220",
                    "CorpusId": 247451051
                },
                "corpusId": 247451051,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cd01df0c950e733021d536f9e1a0f28a14941c2e",
                "title": "Interspace Pruning: Using Adaptive Filter Representations to Improve Training of Sparse CNNs",
                "abstract": "Unstructured pruning is well suited to reduce the memory footprint of convolutional neural networks (CNNs), both at training and inference time. CNNs contain parameters arranged in K x K filters. Standard unstructured pruning (SP) reduces the memory footprint of CNNs by setting filter elements to zero, thereby specifying a fixed subspace that constrains the filter. Especially if pruning is applied before or during training, this induces a strong bias. To overcome this, we introduce interspace pruning (IP), a general tool to improve existing pruning methods. It uses filters represented in a dynamic interspace by linear combinations of an underlying adaptive filter basis (FB). For IP, FB coefficients are set to zero while un-pruned coefficients and FBs are trained jointly. In this work, we provide mathematical evidence for IP's superior performance and demonstrate that IP outperforms SP on all tested state-of-the-art unstructured pruning methods. Especially in challenging situations, like pruning for ImageNet or pruning to high sparsity, IP greatly exceeds SP with equal runtime and parameter costs. Finally, we show that advances of IP are due to improved trainability and superior generalization ability.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2029488873",
                        "name": "Paul Wimmer"
                    },
                    {
                        "authorId": "144442281",
                        "name": "Jens Mehnert"
                    },
                    {
                        "authorId": "2063161",
                        "name": "A. Condurache"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "bbe8932677c927b0183e57c6343822ae7e590de0",
                "externalIds": {
                    "DBLP": "conf/date/ZhangYMSCF22",
                    "DOI": "10.23919/DATE54114.2022.9774660",
                    "CorpusId": 248922402
                },
                "corpusId": 248922402,
                "publicationVenue": {
                    "id": "e02accb9-dacc-43fd-995e-92fe9a825cc4",
                    "name": "Design, Automation and Test in Europe",
                    "type": "conference",
                    "alternate_names": [
                        "Design, Automation, and Test in Europe",
                        "DATE",
                        "Des Autom Test Eur"
                    ],
                    "issn": "1530-1591",
                    "alternate_issns": [
                        "1558-1101"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000198/all-proceedings",
                    "alternate_urls": [
                        "http://www.date-conference.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bbe8932677c927b0183e57c6343822ae7e590de0",
                "title": "XST: A Crossbar Column-wise Sparse Training for Efficient Continual Learning",
                "abstract": "Leveraging the ReRAM crossbar-based In-Memory-Computing (IMC) to accelerate single task DNN inference has been widely studied. However, using the ReRAM crossbar for continual learning has not been explored yet. In this work, we propose XST, a novel crossbar column-wise sparse training framework for continual learning. XST significantly reduces the training cost and saves inference energy. More importantly, it is friendly to existing crossbar-based convolution engine with almost no hardware overhead. Compared with the state-of-the-art CPG method, the experiments show that XST's accuracy achieves 4.95 % higher accuracy. Furthermore, XST demonstrates ~5.59 \u00d7 training speedup and 1.5 \u00d7 inference energy-saving.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2153305372",
                        "name": "Fan Zhang"
                    },
                    {
                        "authorId": "2153203945",
                        "name": "Li Yang"
                    },
                    {
                        "authorId": "2065106589",
                        "name": "Jian Meng"
                    },
                    {
                        "authorId": "1706798",
                        "name": "Jae-sun Seo"
                    },
                    {
                        "authorId": "1965873861",
                        "name": "Yu Cao"
                    },
                    {
                        "authorId": "2054268034",
                        "name": "Deliang Fan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Rigging the Lottery (RigL) Evci et al. (2020) enhances the sparse network training by editing the network connectivity along with the optimizing the parameter by taking advantages of both weight magnitude and gradient information."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "94d353a313021f2237afb28d93965c4767263352",
                "externalIds": {
                    "ArXiv": "2203.04248",
                    "DBLP": "journals/corr/abs-2203-04248",
                    "DOI": "10.48550/arXiv.2203.04248",
                    "CorpusId": 247315014
                },
                "corpusId": 247315014,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/94d353a313021f2237afb28d93965c4767263352",
                "title": "Dual Lottery Ticket Hypothesis",
                "abstract": "Fully exploiting the learning capacity of neural networks requires overparameterized dense networks. On the other side, directly training sparse neural networks typically results in unsatisfactory performance. Lottery Ticket Hypothesis (LTH) provides a novel view to investigate sparse network training and maintain its capacity. Concretely, it claims there exist winning tickets from a randomly initialized network found by iterative magnitude pruning and preserving promising trainability (or we say being in trainable condition). In this work, we regard the winning ticket from LTH as the subnetwork which is in trainable condition and its performance as our benchmark, then go from a complementary direction to articulate the Dual Lottery Ticket Hypothesis (DLTH): Randomly selected subnetworks from a randomly initialized dense network can be transformed into a trainable condition and achieve admirable performance compared with LTH -- random tickets in a given lottery pool can be transformed into winning tickets. Specifically, by using uniform-randomly selected subnetworks to represent the general cases, we propose a simple sparse network training strategy, Random Sparse Network Transformation (RST), to substantiate our DLTH. Concretely, we introduce a regularization term to borrow learning capacity and realize information extrusion from the weights which will be masked. After finishing the transformation for the randomly selected subnetworks, we conduct the regular finetuning to evaluate the model using fair comparisons with LTH and other strong baselines. Extensive experiments on several public datasets and comparisons with competitive approaches validate our DLTH as well as the effectiveness of the proposed model RST. Our work is expected to pave a way for inspiring new research directions of sparse network training in the future. Our code is available at https://github.com/yueb17/DLTH.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "153802755",
                        "name": "Yue Bai"
                    },
                    {
                        "authorId": "46507194",
                        "name": "Haiquan Wang"
                    },
                    {
                        "authorId": "6018169",
                        "name": "Zhiqiang Tao"
                    },
                    {
                        "authorId": "49243413",
                        "name": "Kunpeng Li"
                    },
                    {
                        "authorId": "2156255943",
                        "name": "Yun Fu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ER and ERK typically achieve better performance on CNNs than the naive uniform distribution, i.e., allocating the same sparsity to all layers [31].",
                "Given that the most the state-of-the-art GANs are constructed based on convolutional neural networks (CNNs), we initialize both G(z) and D(x) with the Erdo\u030bs-Re\u0301nyi-Kernel (ERK) graph topology [36], which automatically allocates higher sparsity to larger layers and lower sparsity to smaller ones.",
                ", momentum [38] and gradient [36] shows strong results in convolutional neural networks, whereas random regrowth outperforms the former in language modeling [40].",
                "Sparse initialization of D(x,\u03b8D) 2: G(z,\u03b8sG)\u2190 ERK(G(z,\u03b8), sG) .",
                "Output: Sparse Generator G(z,\u03b8sG), Sparse discriminator D(x,\u03b8sD )\n1: D(x,\u03b8sD )\u2190 ERK(D(x,\u03b8), sD) .",
                "Given that the most the state-of-the-art GANs are constructed based on convolutional neural networks (CNNs), we initialize both G(z) and D(x) with the Erd\u0151s-R\u00e9nyi-Kernel (ERK) graph topology [36], which automatically allocates higher sparsity to larger layers and lower sparsity to smaller ones.",
                "While not initially designed for SST, Erd\u0151s-R\u00e9nyi (ER) [35] and Erd\u0151sR\u00e9nyi-Kernel (ERK) [36] are two advanced layer-wise sparsities introduced from the field of graph theory with strong results.",
                "We consider unstructured sparsity (individual weights are removed from a network) in this paper, not only due to its promising ability to preserve performance even at extreme sparsities [15, 36] but also the increasing support for sparse operations on the practical hardware [42\u201345].",
                "While not initially designed for SST, Erdo\u030bs-Re\u0301nyi (ER) [35] and Erdo\u030bsRe\u0301nyi-Kernel (ERK) [36] are two advanced layer-wise sparsities introduced from the field of graph theory with strong results."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "61a6a8929dd880b3f5f17c32471161977748d665",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-02770",
                    "ArXiv": "2203.02770",
                    "DOI": "10.1007/s11263-023-01824-8",
                    "CorpusId": 247292115
                },
                "corpusId": 247292115,
                "publicationVenue": {
                    "id": "939ee07c-6009-43f8-b884-69238b40659e",
                    "name": "International Journal of Computer Vision",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Comput Vis"
                    ],
                    "issn": "0920-5691",
                    "url": "https://www.springer.com/computer/image+processing/journal/11263",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11263",
                        "http://link.springer.com/journal/11263"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/61a6a8929dd880b3f5f17c32471161977748d665",
                "title": "Don\u2019t Be So Dense: Sparse-to-Sparse GAN Training Without Sacrificing Performance",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "151265717",
                        "name": "Yuesong Tian"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": null,
                        "name": "Li Shen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As demonstrated in Evci et al. (2020a), allowing new connections to grow yields improved flexibility in navigating the loss surfaces, which creates the opportunity to\n5\nescape bad local minima and search for the optimal sparse connectivity Liu et al. (2021b).",
                "Note that newly added connections are not activated in the last sparse topology, and are initialized to zero since it establishes better performance as indicated in (Evci et al., 2020a; Liu et al., 2021b).",
                "However, our flying bird first removes the parameters with the lowest magnitude, which ensures a small term of the first-order Taylor approximation of the loss and thus limits the impact on the output of networks (Evci et al., 2020a).",
                "As two major components in the dynamic sparsity exploration (Evci et al., 2020a), we conduct thorough ablation studies in Table 4 and 5.",
                "\u2026the huge family of sparse training (Mocanu et al., 2016; Evci et al., 2019; Mostafa & Wang, 2019; Liu et al., 2021a; Dettmers & Zettlemoyer, 2019; Jayakumar et al., 2021; Raihan & Aamodt, 2020), the recent methods Evci et al. (2020a); Liu et al. (2021b) lead to the state-of-the-art performance.",
                "This training pipeline, called as Flying Bird (FB), is motivated by the latest sparse training approaches (Evci et al., 2020b) to further reduce robust generalization gap in AT, while ensuring low training costs.",
                "And then, it allows new connectivity with the largest gradient to grow to reduce the loss quickly (Evci et al., 2020a).",
                "Comprehensive results of these subnetworks at 80% and 90% sparsity are reported in Table 1, where the chosen sparsity follows routine options (Evci et al., 2020a; Liu et al., 2021b)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "01594f00b0deed32cba4fc4ea8c74b60be31db4a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-09844",
                    "ArXiv": "2202.09844",
                    "CorpusId": 247011143
                },
                "corpusId": 247011143,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/01594f00b0deed32cba4fc4ea8c74b60be31db4a",
                "title": "Sparsity Winning Twice: Better Robust Generalization from More Efficient Training",
                "abstract": "Recent studies demonstrate that deep networks, even robustified by the state-of-the-art adversarial training (AT), still suffer from large robust generalization gaps, in addition to the much more expensive training costs than standard training. In this paper, we investigate this intriguing problem from a new perspective, i.e., injecting appropriate forms of sparsity during adversarial training. We introduce two alternatives for sparse adversarial training: (i) static sparsity, by leveraging recent results from the lottery ticket hypothesis to identify critical sparse subnetworks arising from the early training; (ii) dynamic sparsity, by allowing the sparse subnetwork to adaptively adjust its connectivity pattern (while sticking to the same sparsity ratio) throughout training. We find both static and dynamic sparse methods to yield win-win: substantially shrinking the robust generalization gap and alleviating the robust overfitting, meanwhile significantly saving training and inference FLOPs. Extensive experiments validate our proposals with multiple network architectures on diverse datasets, including CIFAR-10/100 and Tiny-ImageNet. For example, our methods reduce robust generalization gap and overfitting by 34.44% and 4.02%, with comparable robust/standard accuracy boosts and 87.83%/87.82% training/inference FLOPs savings on CIFAR-100 with ResNet-18. Besides, our approaches can be organically combined with existing regularizers, establishing new state-of-the-art results in AT. Codes are available in https://github.com/VITA-Group/Sparsity-Win-Robust-Generalization.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2109338656",
                        "name": "Zhenyu (Allen) Zhang"
                    },
                    {
                        "authorId": "2108815093",
                        "name": "Pengju Wang"
                    },
                    {
                        "authorId": "2155513986",
                        "name": "Santosh Balachandra"
                    },
                    {
                        "authorId": "2126795",
                        "name": "Haoyu Ma"
                    },
                    {
                        "authorId": "2155897901",
                        "name": "Zehao Wang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The current state-ofthe-art [21] and [5] solve this task via one single round of training.",
                "We compare ASNI-I against its counterparts [45,21,5] where the two last ones are the state-of-the-art methods.",
                "87 Rigging the Lottery, [5] 5,120,000 74."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "3a6be2d54b48fd4a3d3db04d8cd45d08d30d5d53",
                "externalIds": {
                    "ArXiv": "2202.09284",
                    "DBLP": "journals/corr/abs-2202-09284",
                    "CorpusId": 246996738
                },
                "corpusId": 246996738,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3a6be2d54b48fd4a3d3db04d8cd45d08d30d5d53",
                "title": "Amenable Sparse Network Investigator",
                "abstract": "We present\"Amenable Sparse Network Investigator\"(ASNI) algorithm that utilizes a novel pruning strategy based on a sigmoid function that induces sparsity level globally over the course of one single round of training. The ASNI algorithm fulfills both tasks that current state-of-the-art strategies can only do one of them. The ASNI algorithm has two subalgorithms: 1) ASNI-I, 2) ASNI-II. ASNI-I learns an accurate sparse off-the-shelf network only in one single round of training. ASNI-II learns a sparse network and an initialization that is quantized, compressed, and from which the sparse network is trainable. The learned initialization is quantized since only two numbers are learned for initialization of nonzero parameters in each layer L. Thus, quantization levels for the initialization of the entire network is 2L. Also, the learned initialization is compressed because it is a set consisting of 2L numbers. The special sparse network that can be trained from such a quantized and compressed initialization is called amenable. To the best of our knowledge, there is no other algorithm that can learn a quantized and compressed initialization from which the network is still trainable and is able to solve both pruning tasks. Our numerical experiments show that there is a quantized and compressed initialization from which the learned sparse network can be trained and reach to an accuracy on a par with the dense version. We experimentally show that these 2L levels of quantization are concentration points of parameters in each layer of the learned sparse network by ASNI-I. To corroborate the above, we have performed a series of experiments utilizing networks such as ResNets, VGG-style, small convolutional, and fully connected ones on ImageNet, CIFAR10, and MNIST datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46421959",
                        "name": "S. Damadi"
                    },
                    {
                        "authorId": "2155273519",
                        "name": "Erfan Nouri"
                    },
                    {
                        "authorId": "2367683",
                        "name": "H. Pirsiavash"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Neural networks have been shown to have great expressive power even when the weights are randomly initialized (Frankle & Carbin, 2018; Evci et al., 2020; Ramanujan et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4d30136a901d413b0bc08cb6757c5ece384bb9cc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-06438",
                    "ArXiv": "2202.06438",
                    "CorpusId": 246823828
                },
                "corpusId": 246823828,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4d30136a901d413b0bc08cb6757c5ece384bb9cc",
                "title": "Learning from Randomly Initialized Neural Network Features",
                "abstract": "We present the surprising result that randomly initialized neural networks are good feature extractors in expectation. These random features correspond to finite-sample realizations of what we call Neural Network Prior Kernel (NNPK), which is inherently infinite-dimensional. We conduct ablations across multiple architectures of varying sizes as well as initializations and activation functions. Our analysis suggests that certain structures that manifest in a trained model are already present at initialization. Therefore, NNPK may provide further insight into why neural networks are so effective in learning such structures.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2142862",
                        "name": "E. Amid"
                    },
                    {
                        "authorId": "1508890387",
                        "name": "Rohan Anil"
                    },
                    {
                        "authorId": "2305834",
                        "name": "W. Kot\u0142owski"
                    },
                    {
                        "authorId": "1794034",
                        "name": "Manfred K. Warmuth"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "821b08d595b6482e3d1f5bab6835b72d67ebd894",
                "externalIds": {
                    "ArXiv": "2202.02643",
                    "DBLP": "journals/corr/abs-2202-02643",
                    "CorpusId": 246634950
                },
                "corpusId": 246634950,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/821b08d595b6482e3d1f5bab6835b72d67ebd894",
                "title": "The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training",
                "abstract": "Random pruning is arguably the most naive way to attain sparsity in neural networks, but has been deemed uncompetitive by either post-training pruning or sparse training. In this paper, we focus on sparse training and highlight a perhaps counter-intuitive finding, that random pruning at initialization can be quite powerful for the sparse training of modern neural networks. Without any delicate pruning criteria or carefully pursued sparsity structures, we empirically demonstrate that sparsely training a randomly pruned network from scratch can match the performance of its dense equivalent. There are two key factors that contribute to this revival: (i) the network sizes matter: as the original dense networks grow wider and deeper, the performance of training a randomly pruned sparse network will quickly grow to matching that of its dense equivalent, even at high sparsity ratios; (ii) appropriate layer-wise sparsity ratios can be pre-chosen for sparse training, which shows to be another important performance booster. Simple as it looks, a randomly pruned subnetwork of Wide ResNet-50 can be sparsely trained to outperforming a dense Wide ResNet-50, on ImageNet. We also observed such randomly pruned networks outperform dense counterparts in other favorable aspects, such as out-of-distribution detection, uncertainty estimation, and adversarial robustness. Overall, our results strongly suggest there is larger-than-expected room for sparse training at scale, and the benefits of sparsity might be more universal beyond carefully designed pruning. Our source code can be found at https://github.com/VITA-Group/Random_Pruning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2116298570",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": null,
                        "name": "Li Shen"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "de4db22224b732a62571420fafe256df2b26156c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-01290",
                    "ArXiv": "2202.01290",
                    "DOI": "10.1109/CVPRW56347.2022.00312",
                    "CorpusId": 246485603
                },
                "corpusId": 246485603,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/de4db22224b732a62571420fafe256df2b26156c",
                "title": "Cyclical Pruning for Sparse Neural Networks",
                "abstract": "Current methods for pruning neural network weights iteratively apply magnitude-based pruning on the model weights and re-train the resulting model to recover lost accuracy. In this work, we show that such strategies do not allow for the recovery of erroneously pruned weights. To enable weight recovery, we propose a simple strategy called cyclical pruning which requires the pruning schedule to be periodic and allows for weights pruned erroneously in one cycle to recover in subsequent ones. Experimental results on both linear models and large-scale deep neural networks show that cyclical pruning outperforms existing pruning algorithms, especially at high sparsity ratios. Our approach is easy to tune and can be readily incorporated into existing pruning pipelines to boost performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2822290",
                        "name": "Suraj Srinivas"
                    },
                    {
                        "authorId": "2060464024",
                        "name": "Andrey Kuzmin"
                    },
                    {
                        "authorId": "41229153",
                        "name": "Markus Nagel"
                    },
                    {
                        "authorId": "147409784",
                        "name": "Mart van Baalen"
                    },
                    {
                        "authorId": "66777364",
                        "name": "Andrii Skliar"
                    },
                    {
                        "authorId": "83133279",
                        "name": "Tijmen Blankevoort"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Current techniques range from simple approaches like gradual magnitude pruning [17, 56], which periodically drops the fraction of the weights with lowest magnitude, followed by model finetuning, to dynamic techniques like Soft Threshold Reparametrization [32], Movement Pruning [49], or Rigging the Lottery [11], which adapt mask selection during training itself.",
                "Surprisingly, properly-tuned gradual magnitude pruning is often competitive with more complex methods [51, 11, 14, 46].",
                "[11, 51, 50, 46], do not directly take the behavior of acceleration methods into account, while existing speedup-aware structured pruning methods are not straightforward to adapt to the unstructured case."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "60dc56d57c1ba0e5d922cc50b5c3da7ad9110005",
                "externalIds": {
                    "DBLP": "conf/icml/FrantarA22",
                    "ArXiv": "2201.13096",
                    "CorpusId": 246430271
                },
                "corpusId": 246430271,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/60dc56d57c1ba0e5d922cc50b5c3da7ad9110005",
                "title": "SPDY: Accurate Pruning with Speedup Guarantees",
                "abstract": "The recent focus on the efficiency of deep neural networks (DNNs) has led to significant work on model compression approaches, of which weight pruning is one of the most popular. At the same time, there is rapidly-growing computational support for efficiently executing the unstructured-sparse models obtained via pruning. Yet, most existing pruning methods minimize just the number of remaining weights, i.e. the size of the model, rather than optimizing for inference time. We address this gap by introducing SPDY, a new compression method which automatically determines layer-wise sparsity targets achieving a desired inference speedup on a given system, while minimizing accuracy loss. SPDY is composed of two new techniques: the first is an efficient dynamic programming algorithm for solving the speedup-constrained layer-wise compression problem assuming a set of given layer-wise sensitivity scores; the second is a local search procedure for determining accurate layer-wise sensitivity scores. Experiments across popular vision and language models show that SPDY guarantees speedups while recovering higher accuracy relative to existing strategies, both for one-shot and gradual pruning scenarios, and is compatible with most existing pruning approaches. We also extend our approach to the recently-proposed task of pruning with very little data, where we achieve the best known accuracy recovery when pruning to the GPU-supported 2:4 sparsity pattern.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1502248377",
                        "name": "Elias Frantar"
                    },
                    {
                        "authorId": "3311387",
                        "name": "Dan Alistarh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "53b4feab1858f39744220b61717b7f848ff36822",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-13361",
                    "ArXiv": "2201.13361",
                    "CorpusId": 246430723
                },
                "corpusId": 246430723,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/53b4feab1858f39744220b61717b7f848ff36822",
                "title": "Signing the Supermask: Keep, Hide, Invert",
                "abstract": "The exponential growth in numbers of parameters of neural networks over the past years has been accompanied by an increase in performance across several fields. However, due to their sheer size, the networks not only became difficult to interpret but also problematic to train and use in real-world applications, since hardware requirements increased accordingly. Tackling both issues, we present a novel approach that either drops a neural network's initial weights or inverts their respective sign. Put simply, a network is trained by weight selection and inversion without changing their absolute values. Our contribution extends previous work on masking by additionally sign-inverting the initial weights and follows the findings of the Lottery Ticket Hypothesis. Through this extension and adaptations of initialization methods, we achieve a pruning rate of up to 99%, while still matching or exceeding the performance of various baseline and previous models. Our approach has two main advantages. First, and most notable, signed Supermask models drastically simplify a model's structure, while still performing well on given tasks. Second, by reducing the neural network to its very foundation, we gain insights into which weights matter for performance. The code is available on GitHub.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151790829",
                        "name": "Nils Koster"
                    },
                    {
                        "authorId": "1979259",
                        "name": "O. Grothe"
                    },
                    {
                        "authorId": "1748257",
                        "name": "Achim Rettinger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note that, some recent advances [8, 51] advocate incremental pruning that removes a small portion of weights each time.",
                "For instance, RigL [8] re-allocates the removed",
                "Note that our OptG sorts the weights in a global manner to automatically decide a layer-wise sparsity budget, thus avoiding the rule-of-thumb design [8] or complex hyper-parameter tuning for learning sparsity distributions [16].",
                "For instance, RigL [8] removes a small fraction of weights and activates new ones iteratively, while Zhu et al.",
                "Besides, we compare our OptG with several the state-of-the-arts including SNIP [25], GraSP [45], SET [34], GMP [11], SynFlow [44], DNW [46], RigL [8], GSM [6], STR [22] and GraNet [29].",
                "Majorities of existing methods implement layer-wise sparsity using a static or dynamic design [8,24].",
                "Recent advances advocate during-training sparsity which consults the sparsity process throughout network training [8, 22].",
                "On CIFAR-100, our OptG provides significantly better accuracy against other gradient-driven approaches including GrasP [45] and RigL [8], which demonstrates the superiority of optimizing the gradient-driven criteria in network sparsity."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "840b6c72911a64540ba51155652486ee2ab5f14d",
                "externalIds": {
                    "ArXiv": "2201.12826",
                    "CorpusId": 253735450
                },
                "corpusId": 253735450,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/840b6c72911a64540ba51155652486ee2ab5f14d",
                "title": "OptG: Optimizing Gradient-driven Criteria in Network Sparsity",
                "abstract": "Network sparsity receives popularity mostly due to its capability to reduce the complexity of the network. Extensive studies excavate gradient-driven sparsity. Typically, these methods are constructed upon the premise of weight independence, which however, is contrary to the fact that weights are mutually in\ufb02uenced. Thus, their performance remains to be improved. In this paper, we propose to optimize gradient-driven sparsity (OptG) by solving this independence paradox. Our motive comes from the recent advances in supermask training which shows that high-performing sparse subnetworks can be located by simply updating mask values without modifying any weight. We prove that supermask training is to accumulate the criteria of gradient-driven sparsity for both removed and preserved weights, and it can partly solve the independence paradox. Consequently, OptG integrates supermask training into gradient-driven sparsity, and a novel supermask optimizer is further proposed to comprehensively mitigate the independence paradox. Experiments show that OptG can well surpass many existing state-of-the-art competitors, especially at ultra-high sparsity levels. Our project is available at https://github.com/zyxxmu/OptG .",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108078624",
                        "name": "Yu-xin Zhang"
                    },
                    {
                        "authorId": "49352079",
                        "name": "Mingbao Lin"
                    },
                    {
                        "authorId": "2133595441",
                        "name": "Mengzhao Chen"
                    },
                    {
                        "authorId": "2070266415",
                        "name": "Fei Chao"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6a8861073800aaa3040bce1f81af7aa21788b13f",
                "externalIds": {
                    "ArXiv": "2201.11380",
                    "DBLP": "journals/corr/abs-2201-11380",
                    "CorpusId": 246294447
                },
                "corpusId": 246294447,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6a8861073800aaa3040bce1f81af7aa21788b13f",
                "title": "Achieving Personalized Federated Learning with Sparse Local Models",
                "abstract": "Federated learning (FL) is vulnerable to heterogeneously distributed data, since a common global model in FL may not adapt to the heterogeneous data distribution of each user. To counter this issue, personalized FL (PFL) was proposed to produce dedicated local models for each individual user. However, PFL is far from its maturity, because existing PFL solutions either demonstrate unsatisfactory generalization towards different model architectures or cost enormous extra computation and memory. In this work, we propose federated learning with personalized sparse mask (FedSpa), a novel PFL scheme that employs personalized sparse masks to customize sparse local models on the edge. Instead of training an intact (or dense) PFL model, FedSpa only maintains a fixed number of active parameters throughout training (aka sparse-to-sparse training), which enables users' models to achieve personalization with cheap communication, computation, and memory cost. We theoretically show that the iterates obtained by FedSpa converge to the local minimizer of the formulated SPFL problem at rate of $\\mathcal{O}(\\frac{1}{\\sqrt{T}})$. Comprehensive experiments demonstrate that FedSpa significantly saves communication and computation costs, while simultaneously achieves higher model accuracy and faster convergence speed against several state-of-the-art PFL methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9410636",
                        "name": "Tiansheng Huang"
                    },
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2144035454",
                        "name": "Li Shen"
                    },
                    {
                        "authorId": "51209425",
                        "name": "Fengxiang He"
                    },
                    {
                        "authorId": "2154072278",
                        "name": "Weiwei Lin"
                    },
                    {
                        "authorId": "2135519749",
                        "name": "Dacheng Tao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The other direction is through sparse mask exploration [2, 31, 7], where a sparsity in neural networks are maintained during the training process, while the fraction of the weights is explored based on random or heuristics methods.",
                "[7] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1b12c780263864d44f86e708c4f511890b5f031b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-11803",
                    "ArXiv": "2201.11803",
                    "CorpusId": 246411685
                },
                "corpusId": 246411685,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1b12c780263864d44f86e708c4f511890b5f031b",
                "title": "On the Convergence of Heterogeneous Federated Learning with Arbitrary Adaptive Online Model Pruning",
                "abstract": "One of the biggest challenges in Federated Learning (FL) is that client devices often have drastically different computation and communication resources for local updates. To this end, recent research efforts have focused on training heterogeneous local models obtained by pruning a shared global model. Despite empirical success, theoretical guarantees on convergence remain an open question. In this paper, we present a unifying framework for heterogeneous FL algorithms with {\\em arbitrary} adaptive online model pruning and provide a general convergence analysis. In particular, we prove that under certain sufficient conditions and on both IID and non-IID data, these algorithms converges to a stationary point of standard FL for general smooth cost functions, with a convergence rate of $O(\\frac{1}{\\sqrt{Q}})$. Moreover, we illuminate two key factors impacting convergence: pruning-induced noise and minimum coverage index, advocating a joint design of local pruning masks for efficient training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2007579020",
                        "name": "Hanhan Zhou"
                    },
                    {
                        "authorId": "143928529",
                        "name": "Tian Lan"
                    },
                    {
                        "authorId": "2836326",
                        "name": "Guru Venkataramani"
                    },
                    {
                        "authorId": "2152130976",
                        "name": "Wenbo Ding"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The fast optimizer [4] and sparse training [5] have reduced the training algorithm complexity."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "723511b2ec784bf892018c713ad568de17ebd912",
                "externalIds": {
                    "DBLP": "conf/aspdac/YangLSYYL22",
                    "DOI": "10.1109/ASP-DAC52403.2022.9712505",
                    "CorpusId": 247050215
                },
                "corpusId": 247050215,
                "publicationVenue": {
                    "id": "9b12daea-e3b6-4d55-92a5-6803ca0d3e3d",
                    "name": "Asia and South Pacific Design Automation Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Asia s pac Des Autom Conf",
                        "ASP-DAC"
                    ],
                    "url": "http://www.aspdac.com/"
                },
                "url": "https://www.semanticscholar.org/paper/723511b2ec784bf892018c713ad568de17ebd912",
                "title": "Toward Low-Bit Neural Network Training Accelerator by Dynamic Group Accumulation",
                "abstract": "Low-bit quantization is a big challenge for neural network training. Conventional training hardware adopts FP32 to accumulate the partial-sum result, which seriously degrades energy efficiency. In this paper, a technology called dynamic group accumulation (DGA) is proposed to reduce the accumulation error. First, we model the proposed group accumulation method and give the optimal DGA algorithm. Second, we design a training architecture and implement a hardware-efficient DGA unit. Third, we make a comprehensive analysis of the DGA algorithm and training architecture. The proposed method is evaluated on CIFAR and ImageNet datasets, and results show that DGA can reduce accumulation bit-width by 6 bits while achieving the same precision as the static group method. With the FP12 DGA, the CNN algorithm only loses 0.11% accuracy in ImageNet training, and our architecture saves 32% of power consumption compared to the FP32 baseline.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108585951",
                        "name": "Yixiong Yang"
                    },
                    {
                        "authorId": "48757644",
                        "name": "Ruoyang Liu"
                    },
                    {
                        "authorId": "1819427054",
                        "name": "Wenyu Sun"
                    },
                    {
                        "authorId": "3405772",
                        "name": "Jinshan Yue"
                    },
                    {
                        "authorId": "39150998",
                        "name": "Huazhong Yang"
                    },
                    {
                        "authorId": "2442306",
                        "name": "Yongpan Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Choosing s is also a hard search problem [22, 23]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e13652b8b1f57ad04ee343806fbebcdaa301f3e5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-05842",
                    "ArXiv": "2201.05842",
                    "CorpusId": 246015880
                },
                "corpusId": 246015880,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e13652b8b1f57ad04ee343806fbebcdaa301f3e5",
                "title": "UDC: Unified DNAS for Compressible TinyML Models",
                "abstract": "Deploying TinyML models on low-cost IoT hardware is very challenging, due to limited device memory capacity. Neural processing unit (NPU) hardware address the memory challenge by using model compression to exploit weight quantization and sparsity to fit more parameters in the same footprint. However, designing compressible neural networks (NNs) is challenging, as it expands the design space across which we must make balanced trade-offs. This paper demonstrates Unified DNAS for Compressible (UDC) NNs, which explores a large search space to generate state-of-the-art compressible NNs for NPU. ImageNet results show UDC networks are up to $3.35\\times$ smaller (iso-accuracy) or 6.25% more accurate (iso-model size) than previous work.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144097639",
                        "name": "Igor Fedorov"
                    },
                    {
                        "authorId": "2150348516",
                        "name": "Ramon Matas"
                    },
                    {
                        "authorId": "3437148",
                        "name": "Hokchhay Tann"
                    },
                    {
                        "authorId": "48622443",
                        "name": "Chu Zhou"
                    },
                    {
                        "authorId": "39045061",
                        "name": "Matthew Mattina"
                    },
                    {
                        "authorId": "3313708",
                        "name": "P. Whatmough"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Gradient based growth criteria is used in the context of growing sparse connections (Liu et al., 2017; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; ab Tessera et al., 2021; Evci et al., 2022) and when initializing neural networks (Dauphin & Schoenholz, 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b03fe9e4f02a43078386df4c0b44116e400bf56a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-05125",
                    "ArXiv": "2201.05125",
                    "CorpusId": 245906452
                },
                "corpusId": 245906452,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b03fe9e4f02a43078386df4c0b44116e400bf56a",
                "title": "GradMax: Growing Neural Networks using Gradient Information",
                "abstract": "The architecture and the parameters of neural networks are often optimized independently, which requires costly retraining of the parameters whenever the architecture is modified. In this work we instead focus on growing the architecture without requiring costly retraining. We present a method that adds new neurons during training without impacting what is already learned, while improving the training dynamics. We achieve the latter by maximizing the gradients of the new weights and find the optimal initialization efficiently by means of the singular value decomposition (SVD). We call this technique Gradient Maximizing Growth (GradMax) and demonstrate its effectiveness in variety of vision tasks and architectures.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3399348",
                        "name": "Utku Evci"
                    },
                    {
                        "authorId": "3316311",
                        "name": "Max Vladymyrov"
                    },
                    {
                        "authorId": "2465270",
                        "name": "Thomas Unterthiner"
                    },
                    {
                        "authorId": "3158246",
                        "name": "Bart van Merrienboer"
                    },
                    {
                        "authorId": "2570016",
                        "name": "Fabian Pedregosa"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition to undertaking iterative pruning, algorithms can iteratively grow connections, working to ensure that the optimal set of interconnections is retained [17].",
                "Using such adaptive techniques it is now possible to create accurate networks with 90% sparsity on ImageNet [17] and Transformers [13].",
                "This stem performs a 7 \u00d7 7\u00d7 3 (RGB color values) convolution on the input image [17].",
                "Pruning techniques primarily focused on reducing computational overheads are also in use [17, 53]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "d27efe00105cc81a655649114d32af7c736c9356",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-13896",
                    "ArXiv": "2112.13896",
                    "DOI": "10.1088/2634-4386/ac7c8a",
                    "CorpusId": 245537213
                },
                "corpusId": 245537213,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d27efe00105cc81a655649114d32af7c736c9356",
                "title": "Two sparsities are better than one: unlocking the performance benefits of sparse\u2013sparse networks",
                "abstract": "In principle, sparse neural networks should be significantly more efficient than traditional dense networks. Neurons in the brain exhibit two types of sparsity; they are sparsely interconnected and sparsely active. These two types of sparsity, called weight sparsity and activation sparsity, when combined, offer the potential to reduce the computational cost of neural networks by two orders of magnitude. Despite this potential, today\u2019s neural networks deliver only modest performance benefits using just weight sparsity, because traditional computing hardware cannot efficiently process sparse networks. In this article we introduce Complementary Sparsity, a novel technique that significantly improves the performance of dual sparse networks on existing hardware. We demonstrate that we can achieve high performance running weight-sparse networks, and we can multiply those speedups by incorporating activation sparsity. Using Complementary Sparsity, we show up to 100\u00d7 improvement in throughput and energy efficiency performing inference on FPGAs. We analyze scalability and resource tradeoffs for a variety of kernels typical of commercial convolutional networks such as ResNet-50 and MobileNetV2. Our results with Complementary Sparsity suggest that weight plus activation sparsity can be a potent combination for efficiently scaling future AI models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2148281289",
                        "name": "Kevin Lee Hunter"
                    },
                    {
                        "authorId": "2547738",
                        "name": "Lawrence Spracklen"
                    },
                    {
                        "authorId": "2109973472",
                        "name": "Subutai Ahmad"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We begin on the server by initializing a server network \u03b8(1) and a sparse maskm(1), following the layer-wise sparsity distribution described in (Evci et al. 2020).",
                "Note that following the convention of (Mocanu et al. 2018; Evci et al. 2020; Liu et al. 2021c), FedDST so far only considers element-wise unstructured sparsity.",
                "We begin on the server by initializing a server network \u03b81 and a sparse maskm1, following the layer-wise sparsity distribution described in (Evci et al. 2020).",
                "As in (Evci et al. 2020; Dettmers and Zettlemoyer 2019), we use a cosine decay update schedule for \u03b1r,",
                "As in (Evci et al. 2020; Dettmers and Zettlemoyer 2019), we use a cosine decay update schedule for \u03b1r,\n\u03b1r = \u03b1\n2\n( 1 + cos ( (r \u2212 1)\u03c0 Rend )) .",
                "However, suitable values for \u03b1 are still smaller than in RigL; for \u03b1 \u2208 [0.001, 0.05], FedDST significantly outperforms other methods.",
                "The client-side mask readjustment procedure is familiar and takes inspiration from RigL (Evci et al. 2020).",
                "At the core of FedDST is a judiciously designed federated approach to dynamic sparse training (Evci et al. 2020).",
                "In RigL (Evci et al. 2020), the authors initialize the sparsity mask randomly and perform layer-wise magnitude pruning and gradient-magnitude weight growth.",
                "For RandomMask, we randomly sample weights at the server, then perform layer-wise magnitude pruning, following the ERK sparsity distribution (Evci et al. 2020), before the first round, and perform FedAvgM on this sparse network.",
                "More works show sparsity can emerge at initialization (Lee, Ajanthan, and Torr 2019; Wang, Zhang, and Grosse 2020) or can be exploited in dynamic forms during training (Evci et al. 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "8657e637a20ff1775307adcde2ed10494661a908",
                "externalIds": {
                    "ArXiv": "2112.09824",
                    "DBLP": "conf/aaai/BibikarVWC22",
                    "DOI": "10.1609/aaai.v36i6.20555",
                    "CorpusId": 245335012
                },
                "corpusId": 245335012,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/8657e637a20ff1775307adcde2ed10494661a908",
                "title": "Federated Dynamic Sparse Training: Computing Less, Communicating Less, Yet Learning Better",
                "abstract": "Federated learning (FL) enables distribution of machine learning workloads from the cloud to resource-limited edge devices. Unfortunately, current deep networks remain not only too compute-heavy for inference and training on edge devices, but also too large for communicating updates over bandwidth-constrained networks. In this paper, we develop, implement, and experimentally validate a novel FL framework termed Federated Dynamic Sparse Training (FedDST) by which complex neural networks can be deployed and trained with substantially improved efficiency in both on-device computation and in-network communication. At the core of FedDST is a dynamic process that extracts and trains sparse sub-networks from the target full network. With this scheme, \"two birds are killed with one stone:'' instead of full models, each client performs efficient training of its own sparse networks, and only sparse networks are transmitted between devices and the cloud. Furthermore, our results reveal that the dynamic sparsity during FL training more flexibly accommodates local heterogeneity in FL agents than the fixed, shared sparse masks. Moreover, dynamic sparsity naturally introduces an \"in-time self-ensembling effect'' into the training dynamics, and improves the FL performance even over dense training. In a realistic and challenging non i.i.d. FL setting, FedDST consistently outperforms competing algorithms in our experiments: for instance, at any fixed upload data cap on non-iid CIFAR-10, it gains an impressive accuracy advantage of 10% over FedAvgM when given the same upload data cap; the accuracy gap remains 3% even when FedAvgM is given 2 times the upload data cap, further demonstrating efficacy of FedDST. Code is available at: https://github.com/bibikar/feddst.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2146377233",
                        "name": "Sameer Bibikar"
                    },
                    {
                        "authorId": "1806145",
                        "name": "H. Vikalo"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "66273769",
                        "name": "Xiaohan Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020b), or in dynamic forms throughout training (Evci et al., 2020) by updating model parameters and architecture typologies simultaneously.",
                "\u2026works reveal that sparsity patterns might emerge at the initialization (Lee et al., 2018), the early stage of training (You et al., 2019) and (Chen et al., 2020b), or in dynamic forms throughout training (Evci et al., 2020) by updating model parameters and architecture typologies simultaneously."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9262d3c2ec7cf629ecf7d81d400373f029bf0541",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-11480",
                    "ArXiv": "2112.11480",
                    "CorpusId": 245385588
                },
                "corpusId": 245385588,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9262d3c2ec7cf629ecf7d81d400373f029bf0541",
                "title": "On the Compression of Natural Language Models",
                "abstract": "Deep neural networks are effective feature extractors but they are prohibitively large for deployment scenarios. Due to the huge number of parameters, interpretability of parameters in different layers is not straight-forward. This is why neural networks are sometimes considered black boxes. Although simpler models are easier to explain, finding them is not easy. If found, a sparse network that can fit to a data from scratch would help to interpret parameters of a neural network. To this end, lottery ticket hypothesis states that typical dense neural networks contain a small sparse sub-network that can be trained to a reach similar test accuracy in an equal number of steps. The goal of this work is to assess whether such a trainable subnetwork exists for natural language models (NLM)s. To achieve this goal we will review state-of-the-art compression techniques such as quantization, knowledge distillation, and pruning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46421959",
                        "name": "S. Damadi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5c7280126811c12505460c0c9579c707ac0de070",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-04905",
                    "ArXiv": "2112.04905",
                    "CorpusId": 245005856
                },
                "corpusId": 245005856,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5c7280126811c12505460c0c9579c707ac0de070",
                "title": "i-SpaSP: Structured Neural Pruning via Sparse Signal Recovery",
                "abstract": "We propose a novel, structured pruning algorithm for neural networks\u2014the i terative, Spa rse S tructured P runing algorithm, dubbed as i-SpaSP. Inspired by ideas from sparse signal recovery, i-SpaSP operates by iteratively identifying a larger set of important parameter groups (e.g., \ufb01lters or neurons) within a network that contribute most to the residual between pruned and dense network output, then thresholding these groups based on a smaller, pre-de\ufb01ned pruning ratio. For both two-layer and multi-layer network architectures with ReLU activations, we show the error induced by pruning with i-SpaSP decays polynomially, where the degree of this polynomial becomes arbitrarily large based on the sparsity of the dense network\u2019s hidden representations. In our experiments, i-SpaSP is evaluated across a variety of datasets (i.e., MNIST, ImageNet, and XNLI) and architectures (i.e., feed forward networks, ResNet34, MobileNetV2, and BERT), where it is shown to discover high-performing sub-networks and improve upon the pruning ef\ufb01ciency of provable baseline methodologies by several orders of magnitude. Put simply, i-SpaSP is easy to implement with automatic differentiation, achieves strong empirical results, comes with theoretical convergence guarantees, and is ef\ufb01cient, thus distinguishing itself as one of the few computationally ef\ufb01cient, practical, and provable pruning algorithms.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34210029",
                        "name": "Cameron R. Wolfe"
                    },
                    {
                        "authorId": "3393746",
                        "name": "Anastasios Kyrillidis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For RigL, we use the PyTorch implementation of Sundar & Dwaraknath (2021).",
                "Results of 98% sparsity in Table 2 show that RMDA consistently outdoes RigL, indicating regularized training could be a promising alternative to pruning.",
                "We run RigL with 1000 epochs, as its performance at the default 500 epochs was unstable, and let\nRMDA use the same number of epochs.",
                "When the desired structure is (unstructured) sparsity, a popular approach is pruning that trims a given dense model to a specified level, and works like (Gale et al., 2019; Blalock et al., 2020; Evci et al., 2020; Verma & Pesquet, 2021) have shown promising results.",
                "We compare RMDA with a state-of-the-art pruning method RigL (Evci et al., 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "458a5aaf98a159e8c0d1280b0e034e3864000375",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-02612",
                    "ArXiv": "2112.02612",
                    "CorpusId": 244908535
                },
                "corpusId": 244908535,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/458a5aaf98a159e8c0d1280b0e034e3864000375",
                "title": "Training Structured Neural Networks Through Manifold Identification and Variance Reduction",
                "abstract": "This paper proposes an algorithm (RMDA) for training neural networks (NNs) with a regularization term for promoting desired structures. RMDA does not incur computation additional to proximal SGD with momentum, and achieves variance reduction without requiring the objective function to be of the finite-sum form. Through the tool of manifold identification from nonlinear optimization, we prove that after a finite number of iterations, all iterates of RMDA possess a desired structure identical to that induced by the regularizer at the stationary point of asymptotic convergence, even in the presence of engineering tricks like data augmentation and dropout that complicate the training process. Experiments on training NNs with structured sparsity confirm that variance reduction is necessary for such an identification, and show that RMDA thus significantly outperforms existing methods for this task. For unstructured sparsity, RMDA also outperforms a state-of-the-art pruning method, validating the benefits of training structured NNs through regularization.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2156501232",
                        "name": "Zih-Syuan Huang"
                    },
                    {
                        "authorId": "37376821",
                        "name": "Ching-pei Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is commonly believed (Evci et al. 2020; Frankle and Carbin 2020; Malach et al. 2020; Zhou et al. 2019) that finding the \u201cimportant\u201d weight values is crucial for retraining a small pruned model."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "686193a33d56d6cc0f9df299f92f3f0bbf555810",
                "externalIds": {
                    "DBLP": "conf/aaai/LiPG22",
                    "ArXiv": "2112.03406",
                    "DOI": "10.1609/aaai.v36i2.20039",
                    "CorpusId": 244920648
                },
                "corpusId": 244920648,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/686193a33d56d6cc0f9df299f92f3f0bbf555810",
                "title": "Equal Bits: Enforcing Equally Distributed Binary Network Weights",
                "abstract": "Binary networks are extremely efficient as they use only two symbols to define the network: {+1, \u22121}. One can make the prior distribution of these symbols a design choice. The recent IR-Net of Qin et al. argues that imposing a Bernoulli distribution with equal priors (equal bit ratios) over the binary weights leads to maximum entropy and thus minimizes information loss. However, prior work cannot precisely control the binary weight distribution during training, and therefore cannot guarantee maximum entropy. Here, we show that quantizing using optimal transport can guarantee any bit ratio, including equal ratios. We investigate experimentally that equal bit ratios are indeed preferable and show that our method leads to optimization benefits. We show that our quantization method is effective when compared to state-of-the-art binarization methods, even when using binary weight pruning. Our code is available at https://github.com/liyunqianggyn/Equal-Bits-BNN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "152998393",
                        "name": "Yun-qiang Li"
                    },
                    {
                        "authorId": "37041694",
                        "name": "S. Pintea"
                    },
                    {
                        "authorId": "1738975",
                        "name": "J. V. Gemert"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As expected, RigL does not speed up training (the pioneering work has unstructured sparsity and does not achieve speed up on GPU) but surprisingly Pixelfly outperforms both dense and RigL in terms of accuracy while achieving 2.1\u00d7 speedup.",
                "RigL aims to sparsify model weights/parameters, so we use it as a baseline in MLP-based models (Mixer).",
                "On the WikiText-103 1State-of-the-art sparse training methods require up to 5\u00d7 more training epochs compared to dense models [Evci et al., 2020] 2An unstructured sparse model with 1% nonzero weights can be as slow as a dense model [Hooker, 2020]",
                "Specifically, we achieve training speed up on both MLP-Mixer and ViT models by up to 2.3\u00d7 wall-clock time with no drop in accuracy compared to the dense model and up to 4\u00d7 compared to RigL, BigBird and other sparse baselines.",
                "On the WikiText-103\n1State-of-the-art sparse training methods require up to 5\u00d7 more training epochs compared to dense models [Evci et al., 2020]\n2An unstructured sparse model with 1% nonzero weights can be as slow as a dense model [Hooker, 2020]\nlanguage modeling task, we speed up GPT-2 Medium\u2026",
                "Figure 6: Comparison with a representative sparse training baseline RigL [Evci et al., 2020].",
                "For a fair comparison, we conduct the experiment on Mixer-S/32 model for 100 epochs because RigL aims for sparsity on weights, while we aim for both weights & attention."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "90b21dbad8969b74d704eed15a3d98722a88e464",
                "externalIds": {
                    "DBLP": "conf/iclr/ChenDLY0RR22",
                    "ArXiv": "2112.00029",
                    "CorpusId": 244773609
                },
                "corpusId": 244773609,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/90b21dbad8969b74d704eed15a3d98722a88e464",
                "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models",
                "abstract": "Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.5x faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "4319427",
                        "name": "Beidi Chen"
                    },
                    {
                        "authorId": "24593911",
                        "name": "Tri Dao"
                    },
                    {
                        "authorId": "102461072",
                        "name": "Kaizhao Liang"
                    },
                    {
                        "authorId": "2142772202",
                        "name": "Jiaming Yang"
                    },
                    {
                        "authorId": "2119235975",
                        "name": "Zhao Song"
                    },
                    {
                        "authorId": "1755572",
                        "name": "A. Rudra"
                    },
                    {
                        "authorId": "2114485554",
                        "name": "C. R\u00e9"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Additionally, we include the \u201cThe Rigged Lottery\u201d (RigL) method [12] with Erd\u0151s-R\u00e9nyi-Kernel (ERK) weight density.",
                "The top-performing methods we consider here are Soft Threshold Reparametrization (STR) [41], Alternating Compressed/DeCompressed Training (AC/DC) [57] and \u201cThe Rigged Lottery\u201d (RigL) [12].",
                "We also consider the \u201cThe Rigged Lottery\u201d (RigL) method [12], which achieves close to state-of-the-art ImageNet results by allowing for dynamic weight pruning and re-introduction with long finetuning periods, to be a regularization method."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6988237a7d8ffc226d21d897724543c915a159ee",
                "externalIds": {
                    "DBLP": "conf/cvpr/IofinovaPKA22",
                    "ArXiv": "2111.13445",
                    "DOI": "10.1109/CVPR52688.2022.01195",
                    "CorpusId": 244709731
                },
                "corpusId": 244709731,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6988237a7d8ffc226d21d897724543c915a159ee",
                "title": "How Well Do Sparse ImageNet Models Transfer?",
                "abstract": "Transfer learning is a classic paradigm by which models pretrained on large \u201cupstream\u201d datasets are adapted to yield good results on \u201cdownstream\u201d specialized datasets. Generally, more accurate models on the \u201cupstream\u201d dataset tend to provide better transfer accuracy \u201cdownstream\u201d. In this work, we perform an in-depth investigation of this phenomenon in the context of convolutional neural networks (CNNs) trained on the ImageNet dataset, which have been pruned-that is, compressed by sparsifiying their connections. We consider transfer using unstructured pruned models obtained by applying several state-of-the-art pruning methods, including magnitude-based, second-order, regrowth, lottery-ticket, and regularization approaches, in the context of twelve standard transfer tasks. In a nutshell, our study shows that sparse models can match or even outperform the transfer performance of dense models, even at high sparsities, and, while doing so, can lead to significant inference and even training speedups. At the same time, we observe and analyze significant differences in the behaviour of different pruning methods. The code is available at: https://github.com/IST-DASLab/sparse-imagenet-transfer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2082370867",
                        "name": "Eugenia Iofinova"
                    },
                    {
                        "authorId": "3341722",
                        "name": "Alexandra Peste"
                    },
                    {
                        "authorId": "2070446213",
                        "name": "Mark Kurtz"
                    },
                    {
                        "authorId": "3311387",
                        "name": "Dan Alistarh"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", S(2)ViTE [11] employs a prune-and-grow strategy to explore a larger pruning space under the lottery ticket hypothesis [21]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5b5f5ece7e73317ff1d66a6a79372c66ea960a2e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-11802",
                    "ArXiv": "2111.11802",
                    "CorpusId": 244488257
                },
                "corpusId": 244488257,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5b5f5ece7e73317ff1d66a6a79372c66ea960a2e",
                "title": "Pruning Self-attentions into Convolutional Layers in Single Path",
                "abstract": "Vision Transformers (ViTs) have achieved impressive performance over various computer vision tasks. However, modelling global correlations with multi-head self-attention (MSA) layers leads to two widely recognized issues: the massive computational resource consumption and the lack of intrinsic inductive bias for modelling local visual patterns. To solve both issues, we devise a simple yet effective method named Single-Path Vision Transformer pruning (SPViT), to efficiently and automatically compress the pre-trained ViTs into compact models with proper locality added. Specifically, we first propose a novel weight-sharing scheme between MSA and convolutional operations, delivering a single-path space to encode all candidate operations. In this way, we cast the operation search problem as finding which subset of parameters to use in each MSA layer, which significantly reduces the computational cost and optimization difficulty, and the convolution kernels can be well initialized using pre-trained MSA parameters. Relying on the single-path space, we further introduce learnable binary gates to encode the operation choices, which are jointly optimized with network parameters to automatically determine the configuration of each layer. We conduct extensive experiments on two representative ViTs showing that our SPViT achieves a new SOTA for pruning on ImageNet-1k. For example, our SPViT can trim 52.0% FLOPs for DeiT-B and get an impressive 0.6% top-1 accuracy gain simultaneously. The source code is available at https://github.com/ziplab/SPViT.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "11270586",
                        "name": "Haoyu He"
                    },
                    {
                        "authorId": "49270464",
                        "name": "Jing Liu"
                    },
                    {
                        "authorId": "1840579673",
                        "name": "Zizheng Pan"
                    },
                    {
                        "authorId": "1688642",
                        "name": "Jianfei Cai"
                    },
                    {
                        "authorId": "1519066969",
                        "name": "Jing Zhang"
                    },
                    {
                        "authorId": "143719920",
                        "name": "D. Tao"
                    },
                    {
                        "authorId": "3194022",
                        "name": "Bohan Zhuang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In comparison with the multishot results, we observe that SYNFLOW, SNIP, and MAGNITUDE pruning outperform RIGL on this task for the extreme sparsity levels (compare Fig.",
                "1Note that Evci et al. (2020) use percentage of pruned parameters for their plots, i.e. 1\u2212sparsity in our paper.",
                "Note that the version of SNIP used in the original paper is essentially the singleshot approach, which indeed performs worse than RIGL (compare Fig.",
                "For our benchmark data, we construct similar networks as for the multishot experiments \u2013 i.e. depth 6 and width 100 fully connected networks \u2013 and run the available implementation of RIGL with default parameters as suggested in the paper, and Adam optimization with the same parameter settings as for all other considered methods.",
                "Our results on Circlematch those results, with RIGL being able to match ground truth ticket performance for sparsity .5 and .1.",
                "We show in Appendix B that neither dynamic sparse training (Evci et al., 2020) nor LT fine-tuning techniques (Liu et al.",
                "The original results reported in Evci et al. (2020) indicate that for their considered (classification) tasks, RIGL outperforms other dynamic sparse training methods, and that for target sparsity levels of .1 and lower, performance quickly deteriorates for all methods.1 In the original paper, there\u2026",
                "Yet, RIGL allows for efficient computations, saving FLOPS by only infrequently updating gradients, which render it the method of choice for target sparsities of \u2265 .1 in specific applications.",
                "Beyond LT pruning, many more methods have been developed to reduce computational resources and perform structure learning, including dynamic sparse training (Evci et al., 2020; Liu et al., 2021b), adaptations (Frankle et al.",
                "For regression tasks, we see a similar trend, with RIGL performing comparably good as SNIP for sparsity levels \u2265 .1, but the performance decreases rapidly for more extreme target sparsitiy levels.",
                "The original results reported in Evci et al. (2020) indicate that for their considered (classification) tasks, RIGL outperforms other dynamic sparse training methods, and that for target sparsity levels of .1 and lower, performance quickly deteriorates for all methods.1 In the original paper, there was no exploration of the more extreme sparsities considered here, nor a comparison to ticket pruning other than SNIP.",
                "\u2026pruning, many more methods have been developed to reduce computational resources and perform structure learning, including dynamic sparse training (Evci et al., 2020; Liu et al., 2021b), adaptations (Frankle et al., 2020; Renda et al., 2020; Liu et al., 2021a; Liu et al.) of Iterative Magnitude\u2026",
                "We show in Appendix B that neither dynamic sparse training (Evci et al., 2020) nor LT fine-tuning techniques (Liu et al., 2021a; Renda et al., 2020) find architectures that are competitive with our constructed ground truth tickets.",
                "RIGL While the main focus of our paper are lottery tickets, we here briefly discuss results for RIGL (Evci et al., 2020), a state-of-the-art dynamic sparse training approach, which results in sparsified and trained network architectures which are comparable to trained \u2019weak\u2019 tickets.",
                "Generally, for the regression tasks we observe that RIGL is outperformed by the state-of-the-art ticket pruner SYNFLOW and iterative MAGNITUDE pruning."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "67618071e2e63921dde7471bc3c835f0cebe5a41",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-11153",
                    "ArXiv": "2111.11153",
                    "CorpusId": 244478279
                },
                "corpusId": 244478279,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/67618071e2e63921dde7471bc3c835f0cebe5a41",
                "title": "Plant 'n' Seek: Can You Find the Winning Ticket?",
                "abstract": "The lottery ticket hypothesis has sparked the rapid development of pruning algorithms that aim to reduce the computational costs associated with deep learning during training and model deployment. Currently, such algorithms are primarily evaluated on imaging data, for which we lack ground truth information and thus the understanding of how sparse lottery tickets could be. To fill this gap, we develop a framework that allows us to plant and hide winning tickets with desirable properties in randomly initialized neural networks. To analyze the ability of state-of-the-art pruning to identify tickets of extreme sparsity, we design and hide such tickets solving four challenging tasks. In extensive experiments, we observe similar trends as in imaging studies, indicating that our framework can provide transferable insights into realistic problems. Additionally, we can now see beyond such relative trends and highlight limitations of current pruning methods. Based on our results, we conclude that the current limitations in ticket sparsity are likely of algorithmic rather than fundamental nature. We anticipate that comparisons to planted tickets will facilitate future developments of efficient pruning algorithms.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145285968",
                        "name": "Jonas Fischer"
                    },
                    {
                        "authorId": "8813467",
                        "name": "R. Burkholz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another line of methods [180], [181], [182], [183], [184] focus on jointly learning the sparse structures with model parameters by considering different downstream hardware features, e."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "157bddb85786b980c962e649bed33d8ebbf7b4a1",
                "externalIds": {
                    "ArXiv": "2111.06061",
                    "DBLP": "journals/corr/abs-2111-06061",
                    "DOI": "10.1109/TKDE.2022.3178211",
                    "CorpusId": 243985820
                },
                "corpusId": 243985820,
                "publicationVenue": {
                    "id": "c6840156-ee10-4d78-8832-7f8909811576",
                    "name": "IEEE Transactions on Knowledge and Data Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Knowl Data Eng"
                    ],
                    "issn": "1041-4347",
                    "url": "https://www.computer.org/web/tkde",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=69"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/157bddb85786b980c962e649bed33d8ebbf7b4a1",
                "title": "Edge-Cloud Polarization and Collaboration: A Comprehensive Survey for AI",
                "abstract": "Influenced by the great success of deep learning via cloud computing and the rapid development of edge chips, research in artificial intelligence (AI) has shifted to both of the computing paradigms, i.e., cloud computing and edge computing. In recent years, we have witnessed significant progress in developing more advanced AI models on cloud servers that surpass traditional deep learning models owing to model innovations (e.g., Transformers, Pretrained families), explosion of training data and soaring computing capabilities. However, edge computing, especially edge and cloud collaborative computing, are still in its infancy to announce their success due to the resource-constrained IoT scenarios with very limited algorithms deployed. In this survey, we conduct a systematic review for both cloud and edge AI. Specifically, we are the first to set up the collaborative learning mechanism for cloud and edge modeling with a thorough review of the architectures that enable such mechanism. We also discuss potentials and practical experiences of some on-going advanced edge AI topics including pretraining models, graph neural networks and reinforcement learning. Finally, we discuss the promising directions and challenges in this field.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110069725",
                        "name": "Jiangchao Yao"
                    },
                    {
                        "authorId": "1739188006",
                        "name": "Shengyu Zhang"
                    },
                    {
                        "authorId": "2110317730",
                        "name": "Yang Yao"
                    },
                    {
                        "authorId": "2145756591",
                        "name": "Feng Wang"
                    },
                    {
                        "authorId": "47793076",
                        "name": "Jianxin Ma"
                    },
                    {
                        "authorId": "2108091429",
                        "name": "Jianwei Zhang"
                    },
                    {
                        "authorId": "2070014163",
                        "name": "Yunfei Chu"
                    },
                    {
                        "authorId": "2147295496",
                        "name": "Luo Ji"
                    },
                    {
                        "authorId": "2878254",
                        "name": "Kunyang Jia"
                    },
                    {
                        "authorId": "2057972289",
                        "name": "T. Shen"
                    },
                    {
                        "authorId": "1748975704",
                        "name": "Anpeng Wu"
                    },
                    {
                        "authorId": "1452322269",
                        "name": "Fengda Zhang"
                    },
                    {
                        "authorId": "2093185713",
                        "name": "Ziqi Tan"
                    },
                    {
                        "authorId": "33870528",
                        "name": "Kun Kuang"
                    },
                    {
                        "authorId": "2115422697",
                        "name": "Chao Wu"
                    },
                    {
                        "authorId": "2110921249",
                        "name": "Fei Wu"
                    },
                    {
                        "authorId": "1709595",
                        "name": "Jingren Zhou"
                    },
                    {
                        "authorId": "38385080",
                        "name": "Hongxia Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[32, 6] automatically reallocate parameters across layers during training via controlling the global sparsity.",
                "granularity non-parametric parametric weight-level [6, 8, 51, 24, 20, 31, 42, 32, 5] [45, 40, 28, 50, 20] channel-level [44, 14] [21, 26, 47, 28, 18]"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ed1b1acd7a36b22397f052d10426f1531cfc18ce",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-05685",
                    "ArXiv": "2111.05685",
                    "CorpusId": 243938721
                },
                "corpusId": 243938721,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ed1b1acd7a36b22397f052d10426f1531cfc18ce",
                "title": "Efficient Neural Network Training via Forward and Backward Propagation Sparsification",
                "abstract": "Sparse training is a natural idea to accelerate the training speed of deep neural networks and save the memory usage, especially since large modern neural networks are significantly over-parameterized. However, most of the existing methods cannot achieve this goal in practice because the chain rule based gradient (w.r.t. structure parameters) estimators adopted by previous methods require dense computation at least in the backward propagation step. This paper solves this problem by proposing an efficient sparse training method with completely sparse forward and backward passes. We first formulate the training process as a continuous minimization problem under global sparsity constraint. We then separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, we use the conventional chain rule, which can be sparse via exploiting the sparse structure. For the latter step, instead of using the chain rule based gradient estimators as in existing methods, we propose a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation, thus achieving completely sparse training. We prove that the variance of our gradient estimator is bounded. Extensive experimental results on real-world datasets demonstrate that compared to previous methods, our algorithm is much more effective in accelerating the training process, up to an order of magnitude faster.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109127048",
                        "name": "Xiao Zhou"
                    },
                    {
                        "authorId": "47527753",
                        "name": "Weizhong Zhang"
                    },
                    {
                        "authorId": "2049680620",
                        "name": "Zonghao Chen"
                    },
                    {
                        "authorId": "50826757",
                        "name": "Shizhe Diao"
                    },
                    {
                        "authorId": "50728655",
                        "name": "Tong Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "af8e81159854ede7fb5ae4f267f2822e47219cc8",
                "externalIds": {
                    "DBLP": "journals/tnn/WangLLC23",
                    "DOI": "10.1109/TNNLS.2021.3120409",
                    "CorpusId": 243940617,
                    "PubMed": "34752405"
                },
                "corpusId": 243940617,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/af8e81159854ede7fb5ae4f267f2822e47219cc8",
                "title": "Extremely Sparse Networks via Binary Augmented Pruning for Fast Image Classification",
                "abstract": "Network pruning and binarization have been demonstrated to be effective in neural network accelerator design for high speed and energy efficiency. However, most existing pruning approaches achieve a poor tradeoff between accuracy and efficiency, which on the other hand, has limited the progress of neural network accelerators. At the same time, binary networks are highly efficient, however, a large accuracy gap exists between binary networks and their full-precision counterparts. In this article, we investigate the merits of extremely sparse networks with binary connections for image classification through software-hardware codesign. More specifically, we first propose a binary augmented extremely pruning method that can achieve ~98% sparsity with small accuracy degradation. Then we design the hardware architecture based on the resulting sparse and binary networks, which extensively explores the benefits of extreme sparsity with negligible resource consumption introduced by binary branch. Experiments on large-scale ImageNet classification and field-programmable gate array (FPGA) demonstrate that the proposed software-hardware architecture can achieve a prominent tradeoff between accuracy and efficiency.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1656803942",
                        "name": "Peisong Wang"
                    },
                    {
                        "authorId": "2146327608",
                        "name": "Fanrong Li"
                    },
                    {
                        "authorId": "2155119406",
                        "name": "Gang Li"
                    },
                    {
                        "authorId": "2112798294",
                        "name": "Jian Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We note that we follow the advice of Evci et al. (2020) and Dettmers & Zettlemoyer (2019) and do not prune biases and batch-normalization parameters, since they only amount to a negligible fraction of the total weights, however keeping them has a very positive impact on the performance of the\u2026",
                ", 2019), where specific criteria have been proposed that take the particular network architecture into consideration (Zhu & Gupta, 2017; Gale et al., 2019; Evci et al., 2020; Lee et al., 2020).",
                "Evci et al. (2020) propose a reformulation of the ERDO\u030bS-R\u00c9NYI KERNEL (ERK) (Mocanu et al., 2018) to take the layer and kernel dimensions into account when determining the layerwise sparsity distribution.",
                "\u2026on the magnitude of their current values has established itself as the approach of choice (Lee et al., 2019), where specific criteria have been proposed that take the particular network architecture into consideration (Zhu & Gupta, 2017; Gale et al., 2019; Evci et al., 2020; Lee et al., 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "dde57499f185730d4dfc9c5df00d6aebef5bb8a9",
                "externalIds": {
                    "ArXiv": "2111.00843",
                    "DBLP": "journals/corr/abs-2111-00843",
                    "CorpusId": 247026123
                },
                "corpusId": 247026123,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/dde57499f185730d4dfc9c5df00d6aebef5bb8a9",
                "title": "How I Learned to Stop Worrying and Love Retraining",
                "abstract": "Many Neural Network Pruning approaches consist of several iterative training and pruning steps, seemingly losing a significant amount of their performance after pruning and then recovering it in the subsequent retraining phase. Recent works of Renda et al. (2020) and Le&Hua (2021) demonstrate the significance of the learning rate schedule during the retraining phase and propose specific heuristics for choosing such a schedule for IMP (Han et al., 2015). We place these findings in the context of the results of Li et al. (2020) regarding the training of models within a fixed training budget and demonstrate that, consequently, the retraining phase can be massively shortened using a simple linear learning rate schedule. Improving on existing retraining approaches, we additionally propose a method to adaptively select the initial value of the linear schedule. Going a step further, we propose similarly imposing a budget on the initial dense training phase and show that the resulting simple and efficient method is capable of outperforming significantly more complex or heavily parameterized state-of-the-art approaches that attempt to sparsify the network during training. These findings not only advance our understanding of the retraining phase, but more broadly question the belief that one should aim to avoid the need for retraining and reduce the negative effects of 'hard' pruning by incorporating the sparsification process into the standard training.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2056708985",
                        "name": "Max Zimmer"
                    },
                    {
                        "authorId": "2064617407",
                        "name": "Christoph Spiegel"
                    },
                    {
                        "authorId": "145729210",
                        "name": "S. Pokutta"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A couple of methods have been proposed for training deep neural networks from scratch using sparse connections and sparse training (Dettmers & Zettlemoyer, 2019; Mocanu et\u00a0 al., 2018; Bellec et\u00a0 al., 2017; Mostafa & Wang, 2019; Evci et\u00a0 al., 2019; Zhu & Jin, 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "547b2f4846dc0093842efd654ae74783907d9970",
                "externalIds": {
                    "DOI": "10.1007/s10994-021-06063-x",
                    "CorpusId": 254796819
                },
                "corpusId": 254796819,
                "publicationVenue": {
                    "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
                    "name": "Machine-mediated learning",
                    "type": "journal",
                    "alternate_names": [
                        "Mach learn",
                        "Machine Learning",
                        "Mach Learn"
                    ],
                    "issn": "0732-6718",
                    "alternate_issns": [
                        "0885-6125"
                    ],
                    "url": "http://www.springer.com/computer/artificial/journal/10994",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10994",
                        "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/547b2f4846dc0093842efd654ae74783907d9970",
                "title": "Quick and robust feature selection: the strength of energy-efficient sparse training for autoencoders",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1768130522",
                        "name": "Zahra Atashgahi"
                    },
                    {
                        "authorId": "67102129",
                        "name": "Ghada Sokar"
                    },
                    {
                        "authorId": "2197346752",
                        "name": "Tim van der Lee"
                    },
                    {
                        "authorId": "146634864",
                        "name": "Elena Mocanu"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "2154952601",
                        "name": "Raymond N. J. Veldhuis"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The other reference works (except SET [14] and RigL [11] that use uniform sparsity) use non-uniform sparsity, which leads to a higher computation FLOPs compared to the uniform sparsity under the same sparsity ratio.",
                "1 Sparsity Scheme in Sparse Training on the Edge It is common to see that sparse training works [9, 10, 15, 14, 13, 12, 11, 45] represent the training speed performance using the training FLOP count.",
                "7\u00d7, which is 250 epochs, to compare with the RigL with 5\u00d7 longer training, which is 500 epochs as reported in [11].",
                "RigL [11] proposes to iteratively update sparse model topology during training by calculating dense gradients only at the update step.",
                "RigL [11] is a recent milestone of dynamic sparse training works, which has considerable improvements compared to previous works.",
                ", GraSP [10], SNIP [9], RigL [11], SNFS [12], DSR [13], SET [14], and DeepR [15].",
                "2 Sparse Training with Dynamic Sparsity Mask To reduce the computation as well as memory footprint during the whole training phase, sparse training is exploited in many works [15, 14, 13, 12, 11], which can adjust the sparsity topology during training as well as maintain a low memory footprint.",
                "Furthermore, sparse training with dynamic sparsity mask such as SET [14], DeepR [15], DSR [13], RigL [11], and SNFS [12] have been proposed, showing great potential towards end-toend edge training.",
                ", RigL [11]) that use gradients of the dense model to find the weights to grow back, we only use sparse gradients to identify less important"
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "d18d01be0a0a40327e13c1c89aa547a5c73fe3e8",
                "externalIds": {
                    "ArXiv": "2110.14032",
                    "DBLP": "journals/corr/abs-2110-14032",
                    "CorpusId": 239998338
                },
                "corpusId": 239998338,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d18d01be0a0a40327e13c1c89aa547a5c73fe3e8",
                "title": "MEST: Accurate and Fast Memory-Economic Sparse Training Framework on the Edge",
                "abstract": "Recently, a new trend of exploring sparsity for accelerating neural network training has emerged, embracing the paradigm of training on the edge. This paper proposes a novel Memory-Economic Sparse Training (MEST) framework targeting for accurate and fast execution on edge devices. The proposed MEST framework consists of enhancements by Elastic Mutation (EM) and Soft Memory Bound (&S) that ensure superior accuracy at high sparsity ratios. Different from the existing works for sparse training, this current work reveals the importance of sparsity schemes on the performance of sparse training in terms of accuracy as well as training speed on real edge devices. On top of that, the paper proposes to employ data efficiency for further acceleration of sparse training. Our results suggest that unforgettable examples can be identified in-situ even during the dynamic exploration of sparsity masks in the sparse training process, and therefore can be removed for further training speedup on edge devices. Comparing with state-of-the-art (SOTA) works on accuracy, our MEST increases Top-1 accuracy significantly on ImageNet when using the same unstructured sparsity scheme. Systematical evaluation on accuracy, training speed, and memory footprint are conducted, where the proposed MEST framework consistently outperforms representative SOTA works. A reviewer strongly against our work based on his false assumptions and misunderstandings. On top of the previous submission, we employ data efficiency for further acceleration of sparse training. And we explore the impact of model sparsity, sparsity schemes, and sparse training algorithms on the number of removable training examples. Our codes are publicly available at: https://github.com/boone891214/MEST.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9347641",
                        "name": "Geng Yuan"
                    },
                    {
                        "authorId": "151480882",
                        "name": "Xiaolong Ma"
                    },
                    {
                        "authorId": "48643324",
                        "name": "Wei Niu"
                    },
                    {
                        "authorId": "2145366406",
                        "name": "Zhengang Li"
                    },
                    {
                        "authorId": "32409528",
                        "name": "Zhenglun Kong"
                    },
                    {
                        "authorId": "2152354569",
                        "name": "Ning Liu"
                    },
                    {
                        "authorId": "2114981469",
                        "name": "Yifan Gong"
                    },
                    {
                        "authorId": "2949135",
                        "name": "Zheng Zhan"
                    },
                    {
                        "authorId": "31927890",
                        "name": "Chaoyang He"
                    },
                    {
                        "authorId": "153792333",
                        "name": "Qing Jin"
                    },
                    {
                        "authorId": "1421267787",
                        "name": "Siyue Wang"
                    },
                    {
                        "authorId": "39449475",
                        "name": "Minghai Qin"
                    },
                    {
                        "authorId": "2042633100",
                        "name": "Bin Ren"
                    },
                    {
                        "authorId": "46393431",
                        "name": "Yanzhi Wang"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "1662772707",
                        "name": "Xue Lin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To date, various types of compression strategies, such as network pruning [14, 15, 36, 59, 27, 13, 1, 48, 9, 62, 35, 17, 12, 3, 2, 1, 24, 52, 8, 38, 32], bit-precision reduction [15, 54, 42, 49], low-rank approximation [55, 39, 57, 56], knowledge distillation [21, 40] and structured matrix-based construction [44, 28, 6], have been proposed and explored."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "47dba44d0b655c31817115d399059165a284bfe1",
                "externalIds": {
                    "ArXiv": "2110.13981",
                    "DBLP": "conf/nips/SuiYXPZY21",
                    "CorpusId": 239998772
                },
                "corpusId": 239998772,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/47dba44d0b655c31817115d399059165a284bfe1",
                "title": "CHIP: CHannel Independence-based Pruning for Compact Neural Networks",
                "abstract": "Filter pruning has been widely used for neural network compression because of its enabled practical acceleration. To date, most of the existing filter pruning works explore the importance of filters via using intra-channel information. In this paper, starting from an inter-channel perspective, we propose to perform efficient filter pruning using Channel Independence, a metric that measures the correlations among different feature maps. The less independent feature map is interpreted as containing less useful information$/$knowledge, and hence its corresponding filter can be pruned without affecting model capacity. We systematically investigate the quantification metric, measuring scheme and sensitiveness$/$reliability of channel independence in the context of filter pruning. Our evaluation results for different models on various datasets show the superior performance of our approach. Notably, on CIFAR-10 dataset our solution can bring $0.90\\%$ and $0.94\\%$ accuracy increase over baseline ResNet-56 and ResNet-110 models, respectively, and meanwhile the model size and FLOPs are reduced by $42.8\\%$ and $47.4\\%$ (for ResNet-56) and $48.3\\%$ and $52.1\\%$ (for ResNet-110), respectively. On ImageNet dataset, our approach can achieve $40.8\\%$ and $44.8\\%$ storage and computation reductions, respectively, with $0.15\\%$ accuracy increase over the baseline ResNet-50 model. The code is available at https://github.com/Eclipsess/CHIP_NeurIPS2021.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2117517225",
                        "name": "Yang Sui"
                    },
                    {
                        "authorId": "1471722186",
                        "name": "Miao Yin"
                    },
                    {
                        "authorId": "2149182305",
                        "name": "Yi Xie"
                    },
                    {
                        "authorId": "2064916516",
                        "name": "Huy Phan"
                    },
                    {
                        "authorId": "1800447",
                        "name": "S. Zonouz"
                    },
                    {
                        "authorId": "1471729588",
                        "name": "Bo Yuan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Unstructured sparse models are models where weights of dense operations are made to contain many (almost) zeros (Gale et al., 2019; Evci et al., 2020), which do not contribute to the operation\u2019s result and thus, in principle, can be skipped."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "66d735987a31d666a6459566ae026c40ab9a1c3a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-12894",
                    "ArXiv": "2110.12894",
                    "CorpusId": 239768818
                },
                "corpusId": 239768818,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/66d735987a31d666a6459566ae026c40ab9a1c3a",
                "title": "The Efficiency Misnomer",
                "abstract": "Model efficiency is a critical aspect of developing and deploying machine learning models. Inference time and latency directly affect the user experience, and some applications have hard requirements. In addition to inference costs, model training also have direct financial and environmental impacts. Although there are numerous well-established metrics (cost indicators) for measuring model efficiency, researchers and practitioners often assume that these metrics are correlated with each other and report only few of them. In this paper, we thoroughly discuss common cost indicators, their advantages and disadvantages, and how they can contradict each other. We demonstrate how incomplete reporting of cost indicators can lead to partial conclusions and a blurred or incomplete picture of the practical considerations of different models. We further present suggestions to improve reporting of efficiency metrics.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3226635",
                        "name": "Mostafa Dehghani"
                    },
                    {
                        "authorId": "31638576",
                        "name": "Anurag Arnab"
                    },
                    {
                        "authorId": "39611591",
                        "name": "L. Beyer"
                    },
                    {
                        "authorId": "1630664874",
                        "name": "Ashish Vaswani"
                    },
                    {
                        "authorId": "144447820",
                        "name": "Yi Tay"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Despite the empirical success [19, 63, 55, 11], the theoretical justification of winning tickets remains elusive except for a few recent works."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "24fb91f134b9291f7535e7f6305a925a4f63440e",
                "externalIds": {
                    "ArXiv": "2110.05667",
                    "DBLP": "journals/corr/abs-2110-05667",
                    "CorpusId": 238634794
                },
                "corpusId": 238634794,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/24fb91f134b9291f7535e7f6305a925a4f63440e",
                "title": "Why Lottery Ticket Wins? A Theoretical Perspective of Sample Complexity on Pruned Neural Networks",
                "abstract": "The \\textit{lottery ticket hypothesis} (LTH) states that learning on a properly pruned network (the \\textit{winning ticket}) improves test accuracy over the original unpruned network. Although LTH has been justified empirically in a broad range of deep neural network (DNN) involved applications like computer vision and natural language processing, the theoretical validation of the improved generalization of a winning ticket remains elusive. To the best of our knowledge, our work, for the first time, characterizes the performance of training a pruned neural network by analyzing the geometric structure of the objective function and the sample complexity to achieve zero generalization error. We show that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, when the algorithm for training a pruned neural network is specified as an (accelerated) stochastic gradient descent algorithm, we theoretically show that the number of samples required for achieving zero generalization error is proportional to the number of the non-pruned weights in the hidden layer. With a fixed number of samples, training a pruned neural network enjoys a faster convergence rate to the desired model than training the original unpruned one, providing a formal justification of the improved generalization of the winning ticket. Our theoretical results are acquired from learning a pruned neural network of one hidden layer, while experimental results are further provided to justify the implications in pruning multi-layer neural networks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108432203",
                        "name": "Shuai Zhang"
                    },
                    {
                        "authorId": "2146059787",
                        "name": "Meng Wang"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "153191489",
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "authorId": "145042856",
                        "name": "Jinjun Xiong"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0eb95fe7ab2344d005a704a868fc257a96871534",
                "externalIds": {
                    "DBLP": "conf/pkdd/SokarMP22",
                    "ArXiv": "2110.05329",
                    "DOI": "10.1007/978-3-031-26409-2_6",
                    "CorpusId": 250311938
                },
                "corpusId": 250311938,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0eb95fe7ab2344d005a704a868fc257a96871534",
                "title": "Avoiding Forgetting and Allowing Forward Transfer in Continual Learning via Sparse Networks",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "67102129",
                        "name": "Ghada Sokar"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b9b1f80a67e2fc79e292039716fb2e438679397b",
                "externalIds": {
                    "ArXiv": "2110.03676",
                    "DOI": "10.1103/PhysRevB.105.125124",
                    "CorpusId": 238419374
                },
                "corpusId": 238419374,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b9b1f80a67e2fc79e292039716fb2e438679397b",
                "title": "Pruning a restricted Boltzmann machine for quantum state reconstruction",
                "abstract": "Restricted Boltzmann machines (RBMs) have proven to be a powerful tool for learning quantum wavefunction representations from qubit projective measurement data. Since the number of classical parameters needed to encode a quantum wavefunction scales rapidly with the number of qubits, the ability to learn efficient representations is of critical importance. In this paper we study magnitudebased pruning as a way to compress the wavefunction representation in an RBM, focusing on RBMs trained on data from the transverse-field Ising model in one dimension. We find that pruning can reduce the total number of RBM weights, but the threshold at which the reconstruction accuracy starts to degrade varies significantly depending on the phase of the model. In a gapped region of the phase diagram, the RBM admits pruning over half of the weights while still accurately reproducing relevant physical observables. At the quantum critical point however, even a small amount of pruning can lead to significant loss of accuracy in the physical properties of the reconstructed quantum state. Our results highlight the importance of tracking all relevant observables as their sensitivity varies strongly with pruning. Finally, we find that sparse RBMs are trainable and discuss how a successful sparsity pattern can be created without pruning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143999820",
                        "name": "A. Golubeva"
                    },
                    {
                        "authorId": "3422513",
                        "name": "R. Melko"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026and numerous techniques have been proposed, which differ in how weights are identified for removal and the schedule for introducing sparsity/allowing recovery (Cun et al., 1990; Hassibi et al., 1993a; Str\u00f6m, 1997; Louizos et al., 2017; See et al., 2016; Evci et al., 2019; Narang et al., 2017)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3db9649f2ae986cac13f3e748375f8802f9b07fc",
                "externalIds": {
                    "DBLP": "conf/emnlp/AhiaKH21",
                    "ArXiv": "2110.03036",
                    "DOI": "10.18653/v1/2021.findings-emnlp.282",
                    "CorpusId": 238419368
                },
                "corpusId": 238419368,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/3db9649f2ae986cac13f3e748375f8802f9b07fc",
                "title": "The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation",
                "abstract": "A\"bigger is better\"explosion in the number of parameters in deep neural networks has made it increasingly challenging to make state-of-the-art networks accessible in compute-restricted environments. Compression techniques have taken on renewed importance as a way to bridge the gap. However, evaluation of the trade-offs incurred by popular compression techniques has been centered on high-resource datasets. In this work, we instead consider the impact of compression in a data-limited regime. We introduce the term low-resource double bind to refer to the co-occurrence of data limitations and compute resource constraints. This is a common setting for NLP for low-resource languages, yet the trade-offs in performance are poorly studied. Our work offers surprising insights into the relationship between capacity and generalization in data-limited regimes for the task of machine translation. Our experiments on magnitude pruning for translations from English into Yoruba, Hausa, Igbo and German show that in low-resource regimes, sparsity preserves performance on frequent sentences but has a disparate impact on infrequent ones. However, it improves robustness to out-of-distribution shifts, especially for datasets that are very distinct from the training distribution. Our findings suggest that sparsity can play a beneficial role at curbing memorization of low frequency attributes, and therefore offers a promising solution to the low-resource double bind.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1452686038",
                        "name": "Orevaoghene Ahia"
                    },
                    {
                        "authorId": "3422710",
                        "name": "Julia Kreutzer"
                    },
                    {
                        "authorId": "50237813",
                        "name": "Sara Hooker"
                    }
                ]
            }
        },
        {
            "contexts": [
                "introduces a negligible amount of extra FLOPs (over baseline methods) we only show such values in the extended training setting to provide a fair comparison to the setup in [13].",
                "We orient ourselves primarily on the experimental setup in [13] & [14], both of which present techniques among the strongest in the literature.",
                "We also provide results using the Erdos-Renyi Kernel [13], a redistribution of layerwise sparsity subject to the same fixed overall budget.",
                "Rigging the Lottery (RigL) [13] instead activates new weights by"
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "a8abb45a4b79c7a3ac037aa82fa10b86efc97fa7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-00296",
                    "ArXiv": "2110.00296",
                    "CorpusId": 238253278
                },
                "corpusId": 238253278,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a8abb45a4b79c7a3ac037aa82fa10b86efc97fa7",
                "title": "Powerpropagation: A sparsity inducing weight reparameterisation",
                "abstract": "The training of sparse neural networks is becoming an increasingly important tool for reducing the computational footprint of models at training and evaluation, as well enabling the effective scaling up of models. Whereas much work over the years has been dedicated to specialised pruning techniques, little attention has been paid to the inherent effect of gradient based training on model sparsity. In this work, we introduce Powerpropagation, a new weight-parameterisation for neural networks that leads to inherently sparse models. Exploiting the behaviour of gradient descent, our method gives rise to weight updates exhibiting a\"rich get richer\"dynamic, leaving low-magnitude parameters largely unaffected by learning. Models trained in this manner exhibit similar performance, but have a distribution with markedly higher density at zero, allowing more parameters to be pruned safely. Powerpropagation is general, intuitive, cheap and straight-forward to implement and can readily be combined with various other techniques. To highlight its versatility, we explore it in two very different settings: Firstly, following a recent line of work, we investigate its effect on sparse training for resource-constrained settings. Here, we combine Powerpropagation with a traditional weight-pruning technique as well as recent state-of-the-art sparse-to-sparse algorithms, showing superior performance on the ImageNet benchmark. Secondly, we advocate the use of sparsity in overcoming catastrophic forgetting, where compressed representations allow accommodating a large number of tasks at fixed model capacity. In all cases our reparameterisation considerably increases the efficacy of the off-the-shelf methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144735987",
                        "name": "Jonathan Schwarz"
                    },
                    {
                        "authorId": "35880964",
                        "name": "Siddhant M. Jayakumar"
                    },
                    {
                        "authorId": "1996134",
                        "name": "Razvan Pascanu"
                    },
                    {
                        "authorId": "9967304",
                        "name": "P. Latham"
                    },
                    {
                        "authorId": "1725303",
                        "name": "Y. Teh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026on the weights of neural networks via weight sparsity (Frankle & Carbin, 2019; Gale et al., 2019; Bellec et al., 2017; Mostafa & Wang, 2019; Evci et al., 2019; Dettmers & Zettlemoyer, 2019; Mocanu et al., 2018; Jayakumar et al., 2020), or techniques that dynamically route activations to\u2026",
                "In particular, we find that gradient-based re-allocation (Evci et al., 2019) results in a collapse of the explored network parameters (Figure 11), which we mitigate through the use of random parameter re-allocation.",
                "We found that the cosine decay of the pruning\nratio introduced in Evci et al. (2019) outperforms constant pruning schedules and leads to a reduction of the changes in network topology during training.",
                "Dynamic sparsity In DynSparse (Mocanu et al., 2018; Bellec et al., 2017; Liu et al., 2019; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2019; Liu et al., 2021a), the sparse connectivity pattern is evolved during training.",
                "In particular, during the re-allocation step of DynSparse training, we use random re-allocation of pruned weights instead of gradient-based techniques as in RigL (Evci et al., 2019).",
                "However, so far, the limited performance on language modeling task (Evci et al., 2019) has resulted in DynSparse training not seeing wide adoption for large-scale language modeling tasks despite recent advances (Jayakumar et al.",
                "Current sparsity methods can be distinguished into approaches that impose sparsity on the weights of neural networks via weight sparsity (Frankle & Carbin, 2019; Gale et al., 2019; Bellec et al., 2017; Mostafa & Wang, 2019; Evci et al., 2019; Dettmers & Zettlemoyer, 2019; Mocanu et al., 2018; Jayakumar et al., 2020), or techniques that dynamically route activations to only interact with a subset of the network weights via conditional sparsity (Shazeer et al.",
                "To this end, we adopt and investigate DynSparse training techniques (Dettmers & Zettlemoyer, 2019; Evci et al., 2019) for pre-training of BERT bidirectional language encoder (Devlin et al., 2018) based on the highly scalable Transformer architecture (Vaswani et al., 2017).",
                "To this end, we adopt and investigate DynSparse training techniques (Dettmers & Zettlemoyer, 2019; Evci et al., 2019) for pre-training of BERT bidirectional language encoder (Devlin et al.",
                "\u2026is dynamic sparsity (DynSparse), which reduces FLOPs while only requiring training of sparse subsets of the over-parameterized network (Bellec et al., 2017; Mostafa & Wang, 2019; Evci et al., 2019; Dettmers & Zettlemoyer, 2019; Mocanu et al., 2018; Jayakumar et al., 2020; Liu et al., 2021a).",
                "Given that DynSparse training has been primarily developed for vision architectures (Dettmers & Zettlemoyer, 2019; Evci et al., 2019) and did not show competitive performance on the language tasks, we find it necessary to reassess some of the algorithm choices for BERT.",
                "However, so far, the limited performance on language modeling task (Evci et al., 2019) has resulted in DynSparse training not seeing wide adoption for large-scale language modeling tasks despite recent advances (Jayakumar et al., 2020).",
                "One of the most promising candidates for weight sparse training is dynamic sparsity (DynSparse), which reduces FLOPs while only requiring training of sparse subsets of the over-parameterized network (Bellec et al., 2017; Mostafa & Wang, 2019; Evci et al., 2019; Dettmers & Zettlemoyer, 2019; Mocanu et al., 2018; Jayakumar et al., 2020; Liu et al., 2021a).",
                "However, the algorithm show large differences in the exact re-allocation criteria, which range from random re-allocation (Bellec et al., 2017; Mocanu et al., 2018; Liu et al., 2019; 2021a) to a directed evolution based on momentum (Dettmers & Zettlemoyer, 2019) or gradients (Evci et al., 2019).",
                ", 2019; 2021a) to a directed evolution based on momentum (Dettmers & Zettlemoyer, 2019) or gradients (Evci et al., 2019).",
                "This evolution leads to a joint exploration of both network topology and parameters, which has been shown to outperform static sparsity baselines (Bellec et al., 2017; Mostafa & Wang, 2019; Evci et al., 2019; Dettmers & Zettlemoyer, 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4183d028c7b7e54f55e63698232e4d0a6df535bc",
                "externalIds": {
                    "ArXiv": "2108.06277",
                    "DBLP": "journals/corr/abs-2108-06277",
                    "CorpusId": 237048134
                },
                "corpusId": 237048134,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4183d028c7b7e54f55e63698232e4d0a6df535bc",
                "title": "Towards Structured Dynamic Sparse Pre-Training of BERT",
                "abstract": "Identifying algorithms for computational efficient unsupervised training of large language models is an important and active area of research. In this work, we develop and study a straightforward, dynamic always-sparse pre-training approach for BERT language modeling task, which leverages periodic compression steps based on magnitude pruning followed by random parameter re-allocation. This approach enables us to achieve Pareto improvements in terms of the number of floating-point operations (FLOPs) over statically sparse and dense models across a broad spectrum of network sizes. Furthermore, we demonstrate that training remains FLOP-efficient when using coarse-grained block sparsity, making it particularly promising for efficient execution on modern hardware accelerators.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2057429281",
                        "name": "A. Dietrich"
                    },
                    {
                        "authorId": "2123209929",
                        "name": "Frithjof Gressmann"
                    },
                    {
                        "authorId": "145474032",
                        "name": "Douglas Orr"
                    },
                    {
                        "authorId": "66190473",
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "authorId": "39145648",
                        "name": "Daniel Justus"
                    },
                    {
                        "authorId": "49147045",
                        "name": "C. Luschi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many variants have been proposed for both structured [22, 31, 37, 61] and unstructured [13, 14, 16, 21] pruning."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2e1b34719554d130cfe7f5e6f2352cadec7b60a3",
                "externalIds": {
                    "ArXiv": "2108.00259",
                    "CorpusId": 244921154
                },
                "corpusId": 244921154,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2e1b34719554d130cfe7f5e6f2352cadec7b60a3",
                "title": "How much pre-training is enough to discover a good subnetwork?",
                "abstract": "Neural network pruning is useful for discovering efficient, high-performing subnetworks within pre-trained, dense network architectures. More often than not, it involves a three-step process -- pre-training, pruning, and re-training -- that is computationally expensive, as the dense model must be fully pre-trained. While previous work has revealed through experiments the relationship between the amount of pre-training and the performance of the pruned network, a theoretical characterization of such dependency is still missing. Aiming to mathematically analyze the amount of dense network pre-training needed for a pruned network to perform well, we discover a simple theoretical bound in the number of gradient descent pre-training iterations on a two-layer, fully-connected network, beyond which pruning via greedy forward selection [61] yields a subnetwork that achieves good training error. Interestingly, this threshold is shown to be logarithmically dependent upon the size of the dataset, meaning that experiments with larger datasets require more pre-training for subnetworks obtained via pruning to perform well. Lastly, we empirically validate our theoretical results on a multi-layer perceptron trained on MNIST.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34210029",
                        "name": "Cameron R. Wolfe"
                    },
                    {
                        "authorId": "2154497049",
                        "name": "Qihan Wang"
                    },
                    {
                        "authorId": "2120215686",
                        "name": "J. Kim"
                    },
                    {
                        "authorId": "2126894228",
                        "name": "Anastasios Kyrillidis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "B Pruned CNN+ReLU [16], [20] Pruned Trasformer+ReLU [54] sparse /sparse DNN."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "bc79ffc6cb8417119f3c343914da57e73ed34ad8",
                "externalIds": {
                    "DBLP": "conf/hpca/ShinSPALH22",
                    "ArXiv": "2107.12922",
                    "DOI": "10.1109/HPCA53966.2022.00068",
                    "CorpusId": 240288952
                },
                "corpusId": 240288952,
                "publicationVenue": {
                    "id": "b7aa40ac-729b-49d6-9064-4d1a9480e9a9",
                    "name": "International Symposium on High-Performance Computer Architecture",
                    "type": "conference",
                    "alternate_names": [
                        "HPCA",
                        "High Perform Comput Appl",
                        "Int Symp High-performance Comput Archit",
                        "High Performance Computing and Applications"
                    ],
                    "url": "https://web.archive.org/web/*/http://www.hpcaconf.org/"
                },
                "url": "https://www.semanticscholar.org/paper/bc79ffc6cb8417119f3c343914da57e73ed34ad8",
                "title": "Griffin: Rethinking Sparse Optimization for Deep Learning Architectures",
                "abstract": "This paper examines the design space trade-offs of DNNs accelerators aiming to achieve competitive performance and efficiency metrics for all four combinations of dense or sparse activation/weight tensors. To do so, we systematically examine the overheads of supporting sparsity on top of an optimized dense core. These overheads are modeled based on parameters that indicate how a multiplier can borrow a nonzero operation from the neighboring multipliers or future cycles. As a result of this exploration, we identify a few promising designs that perform better than prior work. Our findings suggest that even the best design targeting dual sparsity yields a 20%-30% drop in power efficiency when performing on single sparse models, i.e., those with only sparse weight or sparse activation tensors. We found that one can reuse resources of the same core to maintain high performance and efficiency when running single sparsity or dense models. We call this hybrid architecture Griffin. Griffin is 1.2, 3.0, 3.1, and 1.4x more power-efficient than state-of-the-art sparse architectures, for dense, weight-only sparse, activation-only sparse, and dual sparse models, respectively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116475836",
                        "name": "J. Shin"
                    },
                    {
                        "authorId": "1433007765",
                        "name": "Ali Shafiee"
                    },
                    {
                        "authorId": "9182159",
                        "name": "A. Pedram"
                    },
                    {
                        "authorId": "1403426852",
                        "name": "Hamzah Abdel-Aziz"
                    },
                    {
                        "authorId": "3353457",
                        "name": "Ling Li"
                    },
                    {
                        "authorId": "1491321888",
                        "name": "Joseph Hassoun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[26], Dynamic Sparse Reparamterization (DSR) [27] or the Rigged Lottery (RigL) [6]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "00ce2c2574c82e07febc3a03130bbd9b107ed393",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-12917",
                    "ArXiv": "2107.12917",
                    "DOI": "10.1007/978-3-030-95470-3_29",
                    "CorpusId": 236447887
                },
                "corpusId": 236447887,
                "publicationVenue": {
                    "id": "c8b537fc-13cb-47f4-96a0-670dc526b0c2",
                    "name": "International Conference on Machine Learning, Optimization, and Data Science",
                    "type": "conference",
                    "alternate_names": [
                        "LOD",
                        "Int Conf Mach Learn Optim Data Sci"
                    ],
                    "url": "https://link.springer.com/conference/mod"
                },
                "url": "https://www.semanticscholar.org/paper/00ce2c2574c82e07febc3a03130bbd9b107ed393",
                "title": "Experiments on Properties of Hidden Structures of Sparse Neural Networks",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "114099961",
                        "name": "Julian Stier"
                    },
                    {
                        "authorId": "9457332",
                        "name": "Harsh Darji"
                    },
                    {
                        "authorId": "2389675",
                        "name": "M. Granitzer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While initial approaches mostly focused on pruning models after training [15, 22], contemporary algorithms optimize the sparsity structure of a network while training its parameters [8, 30] or even remove connections before any training whatsoever [24, 39].",
                "A more sophisticated approach, Erd\u00f6s-Renyi-Kernel (ERK), sets the density of a convolutional layer with kernel size w\u00d7 h, fan-in nin and fan-out nout proportional to (w+ h+ nin + nout)/(w \u00b7 h \u00b7 nin \u00b7 nout) [8, 30].",
                "[8] rely on the instantaneous gradient to revive weights but follow SET to maintain the initial layerwise sparsity distribution during training.",
                "Well-engineered LSQ could avoid this and enforce proper redistribution of compression across layers (see [8, 11, 30] for existing baselines)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4ecb57ba76714ed4f14d11d3b30548225b2f15bc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-02306",
                    "ArXiv": "2107.02306",
                    "CorpusId": 235742970
                },
                "corpusId": 235742970,
                "publicationVenue": {
                    "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                    "name": "Journal of machine learning research",
                    "type": "journal",
                    "alternate_names": [
                        "Journal of Machine Learning Research",
                        "J mach learn res",
                        "J Mach Learn Res"
                    ],
                    "issn": "1532-4435",
                    "alternate_issns": [
                        "1533-7928"
                    ],
                    "url": "http://www.ai.mit.edu/projects/jmlr/",
                    "alternate_urls": [
                        "http://jmlr.csail.mit.edu/",
                        "http://www.jmlr.org/",
                        "http://portal.acm.org/affiliated/jmlr"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4ecb57ba76714ed4f14d11d3b30548225b2f15bc",
                "title": "Connectivity Matters: Neural Network Pruning Through the Lens of Effective Sparsity",
                "abstract": "Neural network pruning is a fruitful area of research with surging interest in high sparsity regimes. Benchmarking in this domain heavily relies on faithful representation of the sparsity of subnetworks, which has been traditionally computed as the fraction of removed connections (direct sparsity). This definition, however, fails to recognize unpruned parameters that detached from input or output layers of underlying subnetworks, potentially underestimating actual effective sparsity: the fraction of inactivated connections. While this effect might be negligible for moderately pruned networks (up to 10-100 compression rates), we find that it plays an increasing role for thinner subnetworks, greatly distorting comparison between different pruning algorithms. For example, we show that effective compression of a randomly pruned LeNet-300-100 can be orders of magnitude larger than its direct counterpart, while no discrepancy is ever observed when using SynFlow for pruning [Tanaka et al., 2020]. In this work, we adopt the lens of effective sparsity to reevaluate several recent pruning algorithms on common benchmark architectures (e.g., LeNet-300-100, VGG-19, ResNet-18) and discover that their absolute and relative performance changes dramatically in this new and more appropriate framework. To aim for effective, rather than direct, sparsity, we develop a low-cost extension to most pruning algorithms. Further, equipped with effective sparsity as a reference frame, we partially reconfirm that random pruning with appropriate sparsity allocation across layers performs as well or better than more sophisticated algorithms for pruning at initialization [Su et al., 2020]. In response to this observation, using a simple analogy of pressure distribution in coupled cylinders from physics, we design novel layerwise sparsity quotas that outperform all existing baselines in the context of random pruning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1387979949",
                        "name": "Artem Vysogorets"
                    },
                    {
                        "authorId": "1705963",
                        "name": "J. Kempe"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "439d158e3ab3910d836535dd1aec693f5c0420cf",
                "externalIds": {
                    "DBLP": "conf/iclr/LiuCACSMPWM22",
                    "ArXiv": "2106.14568",
                    "CorpusId": 238856644
                },
                "corpusId": 238856644,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/439d158e3ab3910d836535dd1aec693f5c0420cf",
                "title": "Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity",
                "abstract": "The success of deep ensembles on improving predictive performance, uncertainty estimation, and out-of-distribution robustness has been extensively studied in the machine learning literature. Albeit the promising results, naively training multiple deep neural networks and combining their predictions at inference leads to prohibitive computational costs and memory requirements. Recently proposed efficient ensemble approaches reach the performance of the traditional deep ensembles with significantly lower costs. However, the training resources required by these approaches are still at least the same as training a single dense model. In this work, we draw a unique connection between sparse neural network training and deep ensembles, yielding a novel efficient ensemble learning framework called FreeTickets. Instead of training multiple dense networks and averaging them, we directly train sparse subnetworks from scratch and extract diverse yet accurate subnetworks during this efficient, sparse-to-sparse training. Our framework, FreeTickets, is defined as the ensemble of these relatively cheap sparse subnetworks. Despite being an ensemble method, FreeTickets has even fewer parameters and training FLOPs than a single dense model. This seemingly counter-intuitive outcome is due to the ultra training/inference efficiency of dynamic sparse training. FreeTickets surpasses the dense baseline in all the following criteria: prediction accuracy, uncertainty estimation, out-of-distribution (OoD) robustness, as well as efficiency for both training and inference. Impressively, FreeTickets outperforms the naive deep ensemble with ResNet50 on ImageNet using around only 1/5 of the training FLOPs required by the latter. We have released our source code at https://github.com/VITA-Group/FreeTickets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "1768130522",
                        "name": "Zahra Atashgahi"
                    },
                    {
                        "authorId": "2116298570",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "67102129",
                        "name": "Ghada Sokar"
                    },
                    {
                        "authorId": "146634864",
                        "name": "Elena Mocanu"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We additionally compare AC/DC with Top-KAST and RigL, in terms of the validation accuracy achieved depending on the number of training FLOPs.",
                "RigL can lead to state-of-the-art accuracy results even compared to post-training methods; however, to achieve high accuracy it requires significant additional data passes (e.g. 5x) relative to the dense baseline.",
                "GFLOPs Inference EFLOPs Train\nDense 0 76.84 8.2 3.14\nAC/DC 95 73.14\u00b1 0.2 0.11\u00d7 0.53\u00d7 RigL1\u00d7 95 67.5\u00b1 0.1 0.08\u00d7 0.08\u00d7 RigL1\u00d7 (ERK) 95 69.7\u00b1 0.17 0.12\u00d7 0.13\u00d7 Top-KAST 95 fwd, 50 bwd 71.96 0.08\u00d7 0.22\u00d7\nSTR 94.8 70.97 0.04\u00d7 - WoodFisher 95 72.12 0.09\u00d7 -\nAC/DC 98 68.44\u00b1 0.09 0.06\u00d7 0.46\u00d7 Top-KAST 98 fwd, 90 bwd 67.06 0.05\u00d7 0.08\u00d7\nSTR 97.78 62.84 0.02\u00d7 - WoodFisher 98 65.55 0.05\u00d7 -\nResNet50 Results.",
                "GFLOPs Inference EFLOPs Train\nDense 0 76.84 8.2 3.14\nAC/DC 80 76.3\u00b1 0.1 0.29\u00d7 0.65\u00d7 RigL1\u00d7 80 74.6\u00b1 0.06 0.23\u00d7 0.23\u00d7 RigL1\u00d7(ERK) 80 75.1\u00b1 0.05 0.42\u00d7 0.42\u00d7 Top-KAST 80 fwd, 50 bwd 75.03 0.23\u00d7 0.32\u00d7\nSTR 79.55 76.19 0.19\u00d7 - WoodFisher 80 76.76 0.25\u00d7 -\nAC/DC 90 75.03\u00b1 0.1 0.18\u00d7 0.58\u00d7 RigL1\u00d7 90 72.0\u00b1 0.05 0.13\u00d7 0.13\u00d7 RigL1\u00d7 (ERK) 90 73.0\u00b1 0.04 0.24\u00d7 0.25\u00d7 Top-KAST 90 fwd, 80 bwd 74.76 0.13\u00d7 0.16\u00d7\nSTR 90.23 74.31 0.08\u00d7 - WoodFisher 90 75.21 0.15\u00d7 -\nTable 2: ResNet50/ImageNet, high sparsity results.",
                "The only method which obtains higher accuracy for the same sparsity is the version of RigL [16] which executes for 5x more training epochs than the dense baseline.",
                "For AC/DC and Top-KAST, the first and last layers are kept dense, whereas for RigL, only the first layer is kept dense; however, this has a negligible impact on the number of FLOPs.",
                "The sparsity level is computed with respect to all the parameters, except the biases and Batch Normalization parameters and this is consistent with previous work [16, 52].",
                "For example, the RigL technique [16] randomly \u2217Correspondence to Alexandra Peste: alexandra.peste@ist.ac.at\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\nremoves a large fraction of connections early in training, and then proceeds to optimize over the sparse support, providing savings due to sparse back-propagation.",
                "Then, our results are quite close to RigL2\u00d7, with half the training epochs, and less training FLOPs.",
                "Moreover, RigL does not prune the first layer and the depth-wise convolutions, whereas for the results reported we do not impose any sparsity restrictions.",
                "GFLOPs Inference EFLOPs Train\nDense 0 71.78 1.1 0.44\nAC/DC 75 70.3\u00b1 0.07 0.34\u00d7 0.64\u00d7 RigL1\u00d7 (ERK) 75 68.39 0.52\u00d7 0.53\u00d7\nSTR 75.28 68.35 0.18\u00d7 - WoodFisher 75.28 70.09 0.28\u00d7 -\nAC/DC 90 66.08\u00b1 0.09 0.18\u00d7 0.56\u00d7 RigL1\u00d7 (ERK) 90 63.58 0.27\u00d7 0.29\u00d7\nSTR 89.01 62.1 0.07\u00d7 - WoodFisher 89 63.87 - -\nSemi-structured Sparsity.",
                "RigL [16] prunes weights at random after a warm-up period, and then periodically performs weight re-introduction using a combination of connectivity- and gradient-based statistics, which require periodically evaluating full gradients.",
                "For example, the RigL technique [16] randomly \u2217Correspondence to Alexandra Peste: alexandra.",
                "Additionally, we experiment with extending the number of training iterations for AC/DC at 90% and 95% sparsity two times, similarly to Top-KAST and RigL which also provide experiments for extended training.",
                "Compared to purely sparse training methods, such as Top-KAST or RigL, AC/DC requires dense training phases.",
                "The comparison between AC/DC, Top-KAST and RigL presented in Figure 3 shows that AC/DC is similar or surpasses Top-KAST 2x at 90% and 95% sparsity, and RigL 5x at 95% sparsity both in terms of training FLOPs and validation accuracy.",
                "At the same time, due to dense training phases, AC/DC has higher FLOP requirements relative to RigL or Top-KAST at the same sparsity."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3451010e8fa6a3032c8dd3be1daadb4a08375c64",
                "externalIds": {
                    "DBLP": "conf/nips/PesteIVA21",
                    "ArXiv": "2106.12379",
                    "CorpusId": 235606264
                },
                "corpusId": 235606264,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3451010e8fa6a3032c8dd3be1daadb4a08375c64",
                "title": "AC/DC: Alternating Compressed/DeCompressed Training of Deep Neural Networks",
                "abstract": "The increasing computational requirements of deep neural networks (DNNs) have led to significant interest in obtaining DNN models that are sparse, yet accurate. Recent work has investigated the even harder case of sparse training, where the DNN weights are, for as much as possible, already sparse to reduce computational costs during training. Existing sparse training methods are often empirical and can have lower accuracy relative to the dense baseline. In this paper, we present a general approach called Alternating Compressed/DeCompressed (AC/DC) training of DNNs, demonstrate convergence for a variant of the algorithm, and show that AC/DC outperforms existing sparse training methods in accuracy at similar computational budgets; at high sparsity levels, AC/DC even outperforms existing methods that rely on accurate pre-trained dense models. An important property of AC/DC is that it allows co-training of dense and sparse models, yielding accurate sparse-dense model pairs at the end of the training process. This is useful in practice, where compressed variants may be desirable for deployment in resource-constrained settings without re-doing the entire training flow, and also provides us with insights into the accuracy gap between dense and compressed models. The code is available at: https://github.com/IST-DASLab/ACDC .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3341722",
                        "name": "Alexandra Peste"
                    },
                    {
                        "authorId": "2082370867",
                        "name": "Eugenia Iofinova"
                    },
                    {
                        "authorId": "2869958",
                        "name": "Adrian Vladu"
                    },
                    {
                        "authorId": "3311387",
                        "name": "Dan Alistarh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This finding helps to explain several observations (1) for gradual magnitude pruning (GMP), it is always optimal to end pruning before the second learning rate drop [77, 13]; (2) dynamic sparse training (DST) benefits from a monotonically decreasing pruning rate with cosine or linear update schedule [8, 9]; (3) rewinding techniques [12, 54] outperform fine-tuning as rewinding retrains subnetworks with the original learning rate schedule whereas fine-tuning often retrains with the smallest learning rate.",
                "For this reason, we focus on gradient-based regeneration proposed in Rigged Lottery ( RigL) [9], i.",
                "Different from the existing works for pruning understanding which mainly focus on dense-to-sparse training [42] (training a dense model and prune it to the target sparsity), we also consider sparse-to-sparse training (training a sparse model yet adaptively re-creating the sparsity pattern) which recently has received an upsurge of interest in machine learning [44, 3, 9, 48, 8, 37, 36].",
                "Dynamic Sparse Training (DST) [44, 3, 48, 8, 9, 36, 35, 25] is another class of methods that prune models during training.",
                "For example, Liu et al. [35] illustrated for the first time the true potential of DST, demonstrating significant training/inference efficiency improvement over the dense training.",
                "Again, we use the gradient as the importance score for regeneration, same as the regrow method as used in RigL [9].",
                "This setting is also appealing to GMP [77, 13] and DST [44, 9, 48, 37] in which most of the pruned models are continually trained with the current learning rate for some time.",
                "The key factor of DST is that it starts from a random initialized sparse network and optimizes the sparse topology as well as the weights simultaneously during training (sparse-to-sparse training).",
                "Compared with the methods [33, 69] that require updating all the weights in the backward pass, our method is much more training efficient, as around 2/3 of the training FLOPs is owing to the backward pass [9, 72].",
                ", SET [44], RigL [9], and ITOP [37], in which the sparsity is fixed throughout training, GraNet starts from a denser yet still sparse model and gradually prunes the sparse model to the desired sparsity.",
                "We prune the weights with the smallest magnitude, as it has evolved as the standard method when pruning happens during training, e.g., GMP [77, 13] and DST [44, 9, 37].",
                "All accuracies are in line with the baselines reported in the references [8, 11, 67, 9, 37]."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "a85ba5bb3e97c999f5f6dbc78f277b107af1dba2",
                "externalIds": {
                    "DBLP": "conf/nips/LiuCCAYKSPWM21",
                    "ArXiv": "2106.10404",
                    "CorpusId": 235490153
                },
                "corpusId": 235490153,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a85ba5bb3e97c999f5f6dbc78f277b107af1dba2",
                "title": "Sparse Training via Boosting Pruning Plasticity with Neuroregeneration",
                "abstract": "Works on lottery ticket hypothesis (LTH) and single-shot network pruning (SNIP) have raised a lot of attention currently on post-training pruning (iterative magnitude pruning), and before-training pruning (pruning at initialization). The former method suffers from an extremely large computation cost and the latter usually struggles with insufficient performance. In comparison, during-training pruning, a class of pruning methods that simultaneously enjoys the training/inference efficiency and the comparable performance, temporarily, has been less explored. To better understand during-training pruning, we quantitatively study the effect of pruning throughout training from the perspective of pruning plasticity (the ability of the pruned networks to recover the original performance). Pruning plasticity can help explain several other empirical observations about neural network pruning in literature. We further find that pruning plasticity can be substantially improved by injecting a brain-inspired mechanism called neuroregeneration, i.e., to regenerate the same number of connections as pruned. We design a novel gradual magnitude pruning (GMP) method, named gradual pruning with zero-cost neuroregeneration (\\textbf{GraNet}), that advances state of the art. Perhaps most impressively, its sparse-to-sparse version for the first time boosts the sparse-to-sparse training performance over various dense-to-sparse methods with ResNet-50 on ImageNet without extending the training time. We release all codes in https://github.com/Shiweiliuiiiiiii/GraNet.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2116298570",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "1768130522",
                        "name": "Zahra Atashgahi"
                    },
                    {
                        "authorId": "1410465360",
                        "name": "Lu Yin"
                    },
                    {
                        "authorId": "2113957014",
                        "name": "Huanyu Kou"
                    },
                    {
                        "authorId": null,
                        "name": "Li Shen"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For the uniform sparsity, the first convolutional layer with 7\u00d7 7 kernels is kept dense, the same as in [16].",
                "This may explain why S-GaP achieves better accuracy than RigL, SET, and DSR (see Table 2).",
                "[16] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen.",
                "RigL [16] proposes to use magnitude-based pruning and gradient-flow-based growth that update sparse model topology during training.",
                "Please note that models with the non-uniform sparsifying distribution in Table 2 already have the last FC layer pruned, thus the experiment setup is the same as the ones in [16].",
                "Note that previous work update weights either greedily (e.g., RigL [16]) or randomly (e.g., SET [14] and DSR [15]).",
                "In Table 10, we perform additional experiments to supplement Table 2 by pruning the last FC layer using the S-GaP method and comparing them with [16].",
                "1 n/a n/a n/a RigL [16] non-uniform (ERK) 75.",
                "The results in Table 10 and Table 2 indicate that the accuracy of the ResNet-50 models with sparse and dense FC layers are similar, and both of them outperform the state-of-the-art results in [16].",
                "For non-uniform sparse ResNet-50, the improvement over [16] is 1.",
                "We observe that the improvement over [16] is 1.",
                "Methods based on sparse mask exploration, such as DeepR [13], SET [14], DSR [15], and RigL [16] maintain the target sparsity in all layers throughout the training process and selectively explore a small fraction of the weights periodically."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "279b5affd1a3aec43e1f9d9c21ae69b0d1aa7928",
                "externalIds": {
                    "DBLP": "conf/iclr/MaQSHYXWC0022",
                    "ArXiv": "2106.09857",
                    "CorpusId": 235485330
                },
                "corpusId": 235485330,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/279b5affd1a3aec43e1f9d9c21ae69b0d1aa7928",
                "title": "Effective Model Sparsification by Scheduled Grow-and-Prune Methods",
                "abstract": "Deep neural networks (DNNs) are effective in solving many real-world problems. Larger DNN models usually exhibit better quality (e.g., accuracy) but their excessive computation results in long inference time. Model sparsification can reduce the computation and memory cost while maintaining model quality. Most existing sparsification algorithms unidirectionally remove weights, while others randomly or greedily explore a small subset of weights in each layer for pruning. The limitations of these algorithms reduce the level of achievable sparsity. In addition, many algorithms still require pre-trained dense models and thus suffer from large memory footprint. In this paper, we propose a novel scheduled grow-and-prune (GaP) methodology without having to pre-train a dense model. It addresses the shortcomings of the previous works by repeatedly growing a subset of layers to dense and then pruning them back to sparse after some training. Experiments show that the models pruned using the proposed methods match or beat the quality of the highly optimized dense models at 80% sparsity on a variety of tasks, such as image classification, objective detection, 3D object part segmentation, and translation. They also outperform other state-of-the-art (SOTA) methods for model sparsification. As an example, a 90% non-uniform sparse ResNet-50 model obtained via GaP achieves 77.9% top-1 accuracy on ImageNet, improving the previous SOTA results by 1.5%. Code available at: https://github.com/boone891214/GaP.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "151480882",
                        "name": "Xiaolong Ma"
                    },
                    {
                        "authorId": "39449475",
                        "name": "Minghai Qin"
                    },
                    {
                        "authorId": "2075373569",
                        "name": "Fei Sun"
                    },
                    {
                        "authorId": "26563401",
                        "name": "Zejiang Hou"
                    },
                    {
                        "authorId": "50492964",
                        "name": "Kun Yuan"
                    },
                    {
                        "authorId": "2110289529",
                        "name": "Yi Xu"
                    },
                    {
                        "authorId": "46393431",
                        "name": "Yanzhi Wang"
                    },
                    {
                        "authorId": "123331823",
                        "name": "Yen-kuang Chen"
                    },
                    {
                        "authorId": "144723884",
                        "name": "Rong Jin"
                    },
                    {
                        "authorId": "1410066063",
                        "name": "Yuan Xie"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Pruning algorithms proposed in other works [2, 20, 4] are designed to recover pruned weights by zero-initialization instead of random values, so that the recovered weights do not affect the outputs of the networks."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "14eb496d5ad95f707e5ae9101bfe7bc602f88508",
                "externalIds": {
                    "ArXiv": "2106.09269",
                    "DBLP": "conf/nips/ChijiwaYIUI21",
                    "CorpusId": 235458499
                },
                "corpusId": 235458499,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/14eb496d5ad95f707e5ae9101bfe7bc602f88508",
                "title": "Pruning Randomly Initialized Neural Networks with Iterative Randomization",
                "abstract": "Pruning the weights of randomly initialized neural networks plays an important role in the context of lottery ticket hypothesis. Ramanujan et al. (2020) empirically showed that only pruning the weights can achieve remarkable performance instead of optimizing the weight values. However, to achieve the same level of performance as the weight optimization, the pruning approach requires more parameters in the networks before pruning and thus more memory space. To overcome this parameter inefficiency, we introduce a novel framework to prune randomly initialized neural networks with iteratively randomizing weight values (IteRand). Theoretically, we prove an approximation theorem in our framework, which indicates that the randomizing operations are provably effective to reduce the required number of the parameters. We also empirically demonstrate the parameter efficiency in multiple experiments on CIFAR-10 and ImageNet. The code is available at: https://github.com/dchiji-ntt/iterand",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2113252066",
                        "name": "Daiki Chijiwa"
                    },
                    {
                        "authorId": "36351779",
                        "name": "Shin'ya Yamaguchi"
                    },
                    {
                        "authorId": "1719865",
                        "name": "Yasutoshi Ida"
                    },
                    {
                        "authorId": "33595646",
                        "name": "Kenji Umakoshi"
                    },
                    {
                        "authorId": "2116994373",
                        "name": "T. Inoue"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "71d99af89977f6a3b27f58adbec16f9ce0410405",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-09776",
                    "ArXiv": "2106.09776",
                    "CorpusId": 235485032
                },
                "corpusId": 235485032,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/71d99af89977f6a3b27f58adbec16f9ce0410405",
                "title": "Adapting the Function Approximation Architecture in Online Reinforcement Learning",
                "abstract": "The performance of a reinforcement learning (RL) system depends on the computational architecture used to approximate a value function. Deep learning methods provide both optimization techniques and architectures for approximating nonlinear functions from noisy, high-dimensional observations. However, prevailing optimization techniques are not designed for strictly-incremental online updates. Nor are standard architectures designed for observations with an a priori unknown structure: for example, light sensors randomly dispersed in space. This paper proposes an online RL prediction algorithm with an adaptive architecture that efficiently finds useful nonlinear features. The algorithm is evaluated in a spatial domain with high-dimensional, stochastic observations. The algorithm outperforms non-adaptive baseline architectures and approaches the performance of an architecture given side-channel information. These results are a step towards scalable RL algorithms for more general problems, where the observation structure is not available.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110232578",
                        "name": "John D. Martin"
                    },
                    {
                        "authorId": "3321484",
                        "name": "Joseph Modayil"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a93632237958800217341d7bad847200afdd60e3",
                "externalIds": {
                    "ArXiv": "2106.08962",
                    "DBLP": "journals/corr/abs-2106-08962",
                    "DOI": "10.1145/3578938",
                    "CorpusId": 235446458
                },
                "corpusId": 235446458,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a93632237958800217341d7bad847200afdd60e3",
                "title": "Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better",
                "abstract": "Deep learning has revolutionized the fields of computer vision, natural language understanding, speech recognition, information retrieval, and more. However, with the progressive improvements in deep learning models, their number of parameters, latency, and resources required to train, among others, have all increased significantly. Consequently, it has become important to pay attention to these footprint metrics of a model as well, not just its quality. We present and motivate the problem of efficiency in deep learning, followed by a thorough survey of the five core areas of model efficiency (spanning modeling techniques, infrastructure, and hardware) and the seminal work there. We also present an experiment-based guide along with code for practitioners to optimize their model training and deployment. We believe this is the first comprehensive survey in the efficient deep learning space that covers the landscape of model efficiency from modeling techniques to hardware support. It is our hope that this survey would provide readers with the mental model and the necessary understanding of the field to apply generic efficiency techniques to immediately get significant improvements, and also equip them with ideas for further research and experimentation to achieve additional gains.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2171591",
                        "name": "Gaurav Menghani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Literature on network pruning has been historically focused on accuracy [14, 5] with recently work on robustness [7, 22]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7a64218608c2e3622875dc726ce1c34b605fdfd6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-07849",
                    "ArXiv": "2106.07849",
                    "CorpusId": 235436182
                },
                "corpusId": 235436182,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7a64218608c2e3622875dc726ce1c34b605fdfd6",
                "title": "Simon Says: Evaluating and Mitigating Bias in Pruned Neural Networks with Knowledge Distillation",
                "abstract": "In recent years the ubiquitous deployment of AI has posed great concerns in regards to algorithmic bias, discrimination, and fairness. Compared to traditional forms of bias or discrimination caused by humans, algorithmic bias generated by AI is more abstract and unintuitive therefore more difficult to explain and mitigate. A clear gap exists in the current literature on evaluating and mitigating bias in pruned neural networks. In this work, we strive to tackle the challenging issues of evaluating, mitigating, and explaining induced bias in pruned neural networks. Our paper makes three contributions. First, we propose two simple yet effective metrics, Combined Error Variance (CEV) and Symmetric Distance Error (SDE), to quantitatively evaluate the induced bias prevention quality of pruned models. Second, we demonstrate that knowledge distillation can mitigate induced bias in pruned neural networks, even with unbalanced datasets. Third, we reveal that model similarity has strong correlations with pruning induced bias, which provides a powerful method to explain why bias occurs in pruned neural networks. Our code is available at https://github.com/codestar12/pruning-distilation-bias",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "73066558",
                        "name": "Cody Blakeney"
                    },
                    {
                        "authorId": "2112211588",
                        "name": "Nathaniel Huish"
                    },
                    {
                        "authorId": "2117858668",
                        "name": "Yan Yan"
                    },
                    {
                        "authorId": "36491005",
                        "name": "Ziliang Zong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, this does not conclusively prove the RTI, as alternative explanations remain possible.",
                "We introduce the regurgitating tickets interpretation (RTI) based on the prior work in Evci et al. (2020b) and test its causal relationship with the success of IMP using repellent and attractive losses.",
                "Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50. Although enticing, the authors perform the evaluation only at a single sparsity level. Moreover, this does not conclusively prove the RTI, as alternative explanations remain possible. Solutions from successive pruning rounds might merely be close as a side effect of the very stable training environment or other properties of the lottery tickets. We design a new experiment to affirm the causal relationship between the success of IMP and the RTI across sparsities and on various vision architectures. We modify the loss function to repel previously found optima. We show that this removes the ability of IMP to find lottery tickets. On the other hand, this has no noticeable effect on the performance of random reinitalizations. Finally, we explore what notion of similarity is entailed by the RTI and how it is affected by the stability of training. We show that when IMP becomes stable, all pruning rounds have a very small angular distance. Evci et al. (2020b) make the stronger claim that IMP retrains into the same basin. We find that this is not necessarily the case at very low or high sparsities. The LTH has spawned the idea that lottery tickets could be generated in a mostly data-independent fashion without relying on the very costly IMP. This would open up the possibility of fully sparse training without any prior dense training. For example, Morcos et al. (2019) hint at \"the possibility of parameterizing the distribution of such tickets, allowing us to sample generic, dataset-independent winning tickets.\" Our evidence for the RTI indicates that such efforts might be misguided, as the construction of a lottery ticket requires prior knowledge of a dense solution. This is in line with Frankle et al. (2020b), who empirically examine current attempts at data-independent pruning at initialization (PAI), and show that these attempts significantly underperform compared to IMP.",
                "Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50. Although enticing, the authors perform the evaluation only at a single sparsity level. Moreover, this does not conclusively prove the RTI, as alternative explanations remain possible. Solutions from successive pruning rounds might merely be close as a side effect of the very stable training environment or other properties of the lottery tickets. We design a new experiment to affirm the causal relationship between the success of IMP and the RTI across sparsities and on various vision architectures. We modify the loss function to repel previously found optima. We show that this removes the ability of IMP to find lottery tickets. On the other hand, this has no noticeable effect on the performance of random reinitalizations. Finally, we explore what notion of similarity is entailed by the RTI and how it is affected by the stability of training. We show that when IMP becomes stable, all pruning rounds have a very small angular distance. Evci et al. (2020b) make the stronger claim that IMP retrains into the same basin. We find that this is not necessarily the case at very low or high sparsities. The LTH has spawned the idea that lottery tickets could be generated in a mostly data-independent fashion without relying on the very costly IMP. This would open up the possibility of fully sparse training without any prior dense training. For example, Morcos et al. (2019) hint at \"the possibility of parameterizing the distribution of such tickets, allowing us to sample generic, dataset-independent winning tickets.",
                "Evci et al. (2020b) claim that lottery tickets lie in the same basin as the solution they are pruned from.",
                "Examples include SNIP (Lee et al., 2018), GraSP (Evci et al., 2020b) and SynFlow (Tanaka et al., 2020).",
                "Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50. Although enticing, the authors perform the evaluation only at a single sparsity level. Moreover, this does not conclusively prove the RTI, as alternative explanations remain possible. Solutions from successive pruning rounds might merely be close as a side effect of the very stable training environment or other properties of the lottery tickets. We design a new experiment to affirm the causal relationship between the success of IMP and the RTI across sparsities and on various vision architectures. We modify the loss function to repel previously found optima. We show that this removes the ability of IMP to find lottery tickets. On the other hand, this has no noticeable effect on the performance of random reinitalizations. Finally, we explore what notion of similarity is entailed by the RTI and how it is affected by the stability of training. We show that when IMP becomes stable, all pruning rounds have a very small angular distance. Evci et al. (2020b) make the stronger claim that IMP retrains into the same basin. We find that this is not necessarily the case at very low or high sparsities. The LTH has spawned the idea that lottery tickets could be generated in a mostly data-independent fashion without relying on the very costly IMP. This would open up the possibility of fully sparse training without any prior dense training. For example, Morcos et al. (2019) hint at \"the possibility of parameterizing the distribution of such tickets, allowing us to sample generic, dataset-independent winning tickets.\" Our evidence for the RTI indicates that such efforts might be misguided, as the construction of a lottery ticket requires prior knowledge of a dense solution. This is in line with Frankle et al. (2020b), who empirically examine current attempts at data-independent pruning at initialization (PAI), and show that these attempts significantly underperform compared to IMP. We hope that an improved understanding of IMP and the LTH, will allow future work to design better pruning and sparse optimization algorithms. In summary, our key contributions are as follows: 1. We demonstrate that the lottery ticket hypothesis can be extended to large networks and show that it is possible to train large neural networks from scratch without rewinding by using a sufficiently large batch size. This reinforces the hypothesis of Frankle et al. (2020a) that finding lottery tickets is possible when training is stable with respect to linear mode connectivity. 2. We introduce the regurgitating tickets interpretation (RTI) based on the prior work in Evci et al. (2020b) and test its causal relationship with the success of IMP using repellent and attractive losses.",
                "Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH.",
                "Evci et al. (2020b) make the stronger claim that IMP retrains into the same basin.",
                "Very low and high sparsities are not connected by a linear low error path, in contrast to Evci et al. (2020b), although the angular distances are still considerably smaller than those of reinitializations. 2 Alternative stabilization of the lottery ticket hypothesis Frankle et al. (2020a) link the applicability of IMP with the stability of training.",
                "Given the RTI, there are no clear advantages to using IMP with resetting, instead of ordinary iterative pruning.",
                "To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50.",
                "In the formulation of the RTI, we informally characterize the successive optima from IMP as similar.",
                "Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50. Although enticing, the authors perform the evaluation only at a single sparsity level. Moreover, this does not conclusively prove the RTI, as alternative explanations remain possible. Solutions from successive pruning rounds might merely be close as a side effect of the very stable training environment or other properties of the lottery tickets. We design a new experiment to affirm the causal relationship between the success of IMP and the RTI across sparsities and on various vision architectures. We modify the loss function to repel previously found optima. We show that this removes the ability of IMP to find lottery tickets. On the other hand, this has no noticeable effect on the performance of random reinitalizations. Finally, we explore what notion of similarity is entailed by the RTI and how it is affected by the stability of training. We show that when IMP becomes stable, all pruning rounds have a very small angular distance. Evci et al. (2020b) make the stronger claim that IMP retrains into the same basin.",
                "Evci et al. (2020b) explain that the success of lottery tickets lies in relearning the same solution as the larger net that they were pruned from.",
                "Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50.",
                "As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations.",
                "Using the RTI, we can explain the relation of IMP with stability.",
                "Very low and high sparsities are not connected by a linear low error path, in contrast to Evci et al. (2020b), although the angular distances are still considerably smaller than those of reinitializations.",
                "Based upon Frankle et al. (2020a) and Evci et al. (2020b), we hypothesized and tested the regurgitating tickets interpretation (RTI) as an explanation for the lottery ticket hypothesis.",
                "Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations. To empirically validate the RTI, Evci et al. (2020b) evaluate the similarity of trained lottery tickets on LeNet and ResNet50. Although enticing, the authors perform the evaluation only at a single sparsity level. Moreover, this does not conclusively prove the RTI, as alternative explanations remain possible. Solutions from successive pruning rounds might merely be close as a side effect of the very stable training environment or other properties of the lottery tickets. We design a new experiment to affirm the causal relationship between the success of IMP and the RTI across sparsities and on various vision architectures. We modify the loss function to repel previously found optima. We show that this removes the ability of IMP to find lottery tickets. On the other hand, this has no noticeable effect on the performance of random reinitalizations. Finally, we explore what notion of similarity is entailed by the RTI and how it is affected by the stability of training. We show that when IMP becomes stable, all pruning rounds have a very small angular distance. Evci et al. (2020b) make the stronger claim that IMP retrains into the same basin. We find that this is not necessarily the case at very low or high sparsities. The LTH has spawned the idea that lottery tickets could be generated in a mostly data-independent fashion without relying on the very costly IMP. This would open up the possibility of fully sparse training without any prior dense training. For example, Morcos et al. (2019) hint at \"the possibility of parameterizing the distribution of such tickets, allowing us to sample generic, dataset-independent winning tickets.\" Our evidence for the RTI indicates that such efforts might be misguided, as the construction of a lottery ticket requires prior knowledge of a dense solution. This is in line with Frankle et al. (2020b), who empirically examine current attempts at data-independent pruning at initialization (PAI), and show that these attempts significantly underperform compared to IMP. We hope that an improved understanding of IMP and the LTH, will allow future work to design better pruning and sparse optimization algorithms. In summary, our key contributions are as follows: 1. We demonstrate that the lottery ticket hypothesis can be extended to large networks and show that it is possible to train large neural networks from scratch without rewinding by using a sufficiently large batch size. This reinforces the hypothesis of Frankle et al. (2020a) that finding lottery tickets is possible when training is stable with respect to linear mode connectivity.",
                "However, it is not impossible that other methods to find lottery tickets exist which do not suffer from the RTI, and our discussion of RTI should hence not necessarily be taken as an absolute statement on the LTH but only of the LTH in relation to IMP.",
                "\u2026training, for example, transferring existing lottery tickets Morcos et al. (2019); Mehta (2019), pruning weights during training (You et al., 2019), or dynamically changing the mask during training (Evci et al., 2020a; Savarese et al., 2020; Dettmers and Zettlemoyer, 2019; Kusupati et al., 2020).",
                "Evci et al. (2020b) recently proposed a possible interpretation for the behavior of IMP and the success of the LTH. The authors posit that lottery tickets cannot be considered random initializations but that a lottery ticket contains a prior for rediscovering the solution of the model from which it was pruned. We call this hypothesis the regurgitating tickets interpretation (RTI): a lottery ticket retrains to a similar optimum compared to the network from which it was pruned. As discussed in Evci et al. (2020b), the RTI allows us to informally understand several key questions regarding IMP, such as why lottery tickets transfer or why they are robust to perturbations."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "1f31db2bf95133655c29fd3bedfc2912bc76eb85",
                "externalIds": {
                    "ArXiv": "2106.06955",
                    "DBLP": "journals/corr/abs-2106-06955",
                    "CorpusId": 235421894
                },
                "corpusId": 235421894,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1f31db2bf95133655c29fd3bedfc2912bc76eb85",
                "title": "Towards Understanding Iterative Magnitude Pruning: Why Lottery Tickets Win",
                "abstract": "The lottery ticket hypothesis states that sparse subnetworks exist in randomly initialized dense networks that can be trained to the same accuracy as the dense network they reside in. However, the subsequent work has failed to replicate this on large-scale models and required rewinding to an early stable state instead of initialization. We show that by using a training method that is stable with respect to linear mode connectivity, large networks can also be entirely rewound to initialization. Our subsequent experiments on common vision tasks give strong credence to the hypothesis in Evci et al. (2020b) that lottery tickets simply retrain to the same regions (although not necessarily to the same basin). These results imply that existing lottery tickets could not have been found without the preceding dense training by iterative magnitude pruning, raising doubts about the use of the lottery ticket hypothesis.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2051865997",
                        "name": "Jaron Maene"
                    },
                    {
                        "authorId": "2112132080",
                        "name": "Mingxiao Li"
                    },
                    {
                        "authorId": "100781843",
                        "name": "Marie-Francine Moens"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To do so, we see potential to rely on other forms of sparsity, including dynamic weight sparsity (Evci et al., 2019) and conditional activation sparsity, improving the capability to handle multiple languages and data domains within the same architecture (Fedus et al.",
                "To do so, we see potential to rely on other forms of sparsity, including dynamic weight sparsity (Evci et al., 2019) and conditional activation sparsity, improving the capability to handle multiple languages and data domains within the same architecture (Fedus et al., 2021)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9f9e6b4731d3cf467bf2bfab4ce42bbc6d4afd73",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-05822",
                    "ArXiv": "2106.05822",
                    "CorpusId": 235390408
                },
                "corpusId": 235390408,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9f9e6b4731d3cf467bf2bfab4ce42bbc6d4afd73",
                "title": "GroupBERT: Enhanced Transformer Architecture with Efficient Grouped Structures",
                "abstract": "Attention based language models have become a critical component in state-of-the-art natural language processing systems. However, these models have significant computational requirements, due to long training times, dense operations and large parameter count. In this work we demonstrate a set of modifications to the structure of a Transformer layer, producing a more efficient architecture. First, we add a convolutional module to complement the self-attention module, decoupling the learning of local and global interactions. Secondly, we rely on grouped transformations to reduce the computational cost of dense feed-forward layers and convolutions, while preserving the expressivity of the model. We apply the resulting architecture to language representation learning and demonstrate its superior performance compared to BERT models of different scales. We further highlight its improved efficiency, both in terms of floating-point operations (FLOPs) and time-to-train.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "66190473",
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "authorId": "39145648",
                        "name": "Daniel Justus"
                    },
                    {
                        "authorId": "145474032",
                        "name": "Douglas Orr"
                    },
                    {
                        "authorId": "2057429281",
                        "name": "A. Dietrich"
                    },
                    {
                        "authorId": "10682156",
                        "name": "Frithjof Gressmann"
                    },
                    {
                        "authorId": "2097479",
                        "name": "A. Koliousis"
                    },
                    {
                        "authorId": "49147045",
                        "name": "C. Luschi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Second, unlike other DST methods that use the values of non-existing (masked) weights in the evolution process, SET uses only the values of existing sparse connections.",
                "Dynamic sparse training (DST) [3, 13, 25, 39, 40, 45] has recently emerged, aiming to achieve training efficiency and inference efficiency.",
                "Recently, DST has emerged in other domains like text classification [38], feature selection [2], lifelong learning [58], and federated learning [72]. ar X iv :2\n10 6.",
                "In most of the DST algorithms that are applied in the supervised setting, the dynamic evolution of the sparse topology is performed each training epoch.",
                "This is the traditional metric used in the literature to compare a DST method against its dense counterpart.",
                "Our experimental results show that DST brings other favorable advantages to the DRL agents besides memory and computation efficiency.",
                "In the rest of this section, we will explain the details of our proposed DST method for the TD3 algorithm (DS-TD3).",
                "DST methods show their success in outperforming dense neural networks with high sparsity levels in supervised classification tasks [9, 45, 49].",
                "DST has emerged and showed its success in many other fields as well [25, 44].",
                "In [9, 13, 14, 29], the gradient information is used to determine which connections would be changed",
                "training (DST) [3, 13, 25, 39, 40, 45] has recently emerged, aiming to achieve training efficiency and inference efficiency.",
                "We follow the method described in [13] to calculate the FLOPs."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "d59324d2534051042ed575e98f656a9e5dfe041c",
                "externalIds": {
                    "ArXiv": "2106.04217",
                    "DBLP": "journals/corr/abs-2106-04217",
                    "DOI": "10.24963/ijcai.2022/477",
                    "CorpusId": 235368271
                },
                "corpusId": 235368271,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d59324d2534051042ed575e98f656a9e5dfe041c",
                "title": "Dynamic Sparse Training for Deep Reinforcement Learning",
                "abstract": "Deep reinforcement learning (DRL) agents are trained through trial-and-error interactions with the environment. This leads to a long training time for dense neural networks to achieve good performance. Hence, prohibitive computation and memory resources are consumed. Recently, learning efficient DRL agents has received increasing attention. Yet, current methods focus on accelerating inference time. In this paper, we introduce for the first time a dynamic sparse training approach for deep reinforcement learning to accelerate the training process. The proposed approach trains a sparse neural network from scratch and dynamically adapts its topology to the changing data distribution during training. Experiments on continuous control tasks show that our dynamic sparse agents achieve higher performance than the equivalent dense methods, reduce the parameter count and floating-point operations (FLOPs) by 50%, and have a faster learning speed that enables reaching the performance of dense agents with 40\u221250% reduction in the training steps.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "67102129",
                        "name": "Ghada Sokar"
                    },
                    {
                        "authorId": "146634864",
                        "name": "Elena Mocanu"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "144848112",
                        "name": "P. Stone"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For the \u00b7 update schedule, it contains: (i) the update interval \u2206T, which is the number of training iterations between two sparse topology updates; (ii) the end iteration Tend, indicating when to stop updating the sparsity connectivity, and we set Tend to 80% of total training iterations in our experiments; (iii) the initial fraction \u03b1 of connections that can be pruned or grow, which is 50% in our case; (iv) a decay schedule of the fraction of changeable connections fdecay(t, \u03b1,Tend) = \u03b1 2 (1+cos( t\u00d7\u03c0 Tend )), where a cosine annealing is used, following [24, 25].",
                "To meet this challenging demand, we draw inspirations from the latest sparse training works [24, 25] that dynamically extract and train sparse subnetworks instead of training the full models.",
                "Infrequent gradient calculation [24] is adopted in our case, which computes the gradients in an online manner and only stores the top gradient values.",
                "Our SViTE method (and its variants S(2)ViTE and SViTE+) is inspired from state-of-the-art sparse training approaches [24, 25] in CNNs.",
                "Newly added connections are not activated in the last sparse topology, and are initialized to zero since it produces better performance as demonstrated in [24, 25].",
                "As illustrated in [24], such fashion amortizes the",
                "Furthermore, gradient information from the backward pass is utilized to guide the update of the dynamic sparse connectivity [28, 24], which produces substantial performance gains.",
                "Grow criterion: Similar to [24, 25], we active the new units with the highest magnitude gradients, such as \u2016 ) \u2202A(l,h) \u2016`1 and \u2016 \u2202L(X) \u2202W (l,1) j,\u00b7 \u2016`1 for the hth attention head and the jth neuron of the MLP (W ), respectively.",
                "For a consistent description, we follow the standard notations in [24, 25]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "efbe9f591090018f78b42c84613c8afda9292fdb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-04533",
                    "ArXiv": "2106.04533",
                    "CorpusId": 235367934
                },
                "corpusId": 235367934,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/efbe9f591090018f78b42c84613c8afda9292fdb",
                "title": "Chasing Sparsity in Vision Transformers: An End-to-End Exploration",
                "abstract": "Vision transformers (ViTs) have recently received explosive popularity, but their enormous model sizes and training costs remain daunting. Conventional post-training pruning often incurs higher training budgets. In contrast, this paper aims to trim down both the training memory overhead and the inference complexity, without sacrificing the achievable accuracy. We carry out the first-of-its-kind comprehensive exploration, on taking a unified approach of integrating sparsity in ViTs\"from end to end\". Specifically, instead of training full ViTs, we dynamically extract and train sparse subnetworks, while sticking to a fixed small parameter budget. Our approach jointly optimizes model parameters and explores connectivity throughout training, ending up with one sparse network as the final output. The approach is seamlessly extended from unstructured to structured sparsity, the latter by considering to guide the prune-and-grow of self-attention heads inside ViTs. We further co-explore data and architecture sparsity for additional efficiency gains by plugging in a novel learnable token selector to adaptively determine the currently most vital patches. Extensive results on ImageNet with diverse ViT backbones validate the effectiveness of our proposals which obtain significantly reduced computational cost and almost unimpaired generalization. Perhaps most surprisingly, we find that the proposed sparse (co-)training can sometimes improve the ViT accuracy rather than compromising it, making sparsity a tantalizing\"free lunch\". For example, our sparsified DeiT-Small at (5%, 50%) sparsity for (data, architecture), improves 0.28% top-1 accuracy, and meanwhile enjoys 49.32% FLOPs and 4.40% running time savings. Our codes are available at https://github.com/VITA-Group/SViTE.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "153655416",
                        "name": "Yu Cheng"
                    },
                    {
                        "authorId": "144702900",
                        "name": "Zhe Gan"
                    },
                    {
                        "authorId": "145347147",
                        "name": "Lu Yuan"
                    },
                    {
                        "authorId": "2152828578",
                        "name": "Lei Zhang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We start by demonstrating the efficacy of our method on the ImageNet dataset for image classification, where we train a sparse ResNet-50 as in previous works [7, 10].",
                "Lastly, Rigging the Lottery (RigL) [7] is a recent and highly performant sparse-to-sparse method that matches or surpasses the performance of pruning-based methods.",
                "For image modelling, Top-KAST outperforms existing sparse-to-sparse training approaches, such as Sparse Evolutionary Training (SET) [26] and matches Rigging the Lottery (RigL) [7] on ImageNet across a range of floating-point operations (FLOPs) budgets.",
                "While there is a plethora of works proposing increasingly efficient ways to prune dense networks for sparse inference (dense-to-sparse training) [45, 27, 5], the field has only more recently begun to look at approaches that start training at the desired sparsity (sparse-to-sparse training) [26, 3, 28, 7]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "9c4dd36ad206ca8be96ae4000568e899f4acfa91",
                "externalIds": {
                    "DBLP": "conf/nips/JayakumarPROE20",
                    "ArXiv": "2106.03517",
                    "MAG": "3098372854",
                    "CorpusId": 227275472
                },
                "corpusId": 227275472,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9c4dd36ad206ca8be96ae4000568e899f4acfa91",
                "title": "Top-KAST: Top-K Always Sparse Training",
                "abstract": "Sparse neural networks are becoming increasingly important as the field seeks to improve the performance of existing models by scaling them up, while simultaneously trying to reduce power consumption and computational footprint. Unfortunately, most existing methods for inducing performant sparse models still entail the instantiation of dense parameters, or dense gradients in the backward-pass, during training. For very large models this requirement can be prohibitive. In this work we propose Top-KAST, a method that preserves constant sparsity throughout training (in both the forward and backward-passes). We demonstrate the efficacy of our approach by showing that it performs comparably to or better than previous works when training models on the established ImageNet benchmark, whilst fully maintaining sparsity. In addition to our ImageNet results, we also demonstrate our approach in the domain of language modeling where the current best performing architectures tend to have tens of billions of parameters and scaling up does not yet seem to have saturated performance. Sparse versions of these architectures can be run with significantly fewer resources, making them more widely accessible and applicable. Furthermore, in addition to being effective, our approach is straightforward and can easily be implemented in a wide range of existing machine learning frameworks with only a few additional lines of code. We therefore hope that our contribution will help enable the broader community to explore the potential held by massive models, without incurring massive computational cost.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35880964",
                        "name": "Siddhant M. Jayakumar"
                    },
                    {
                        "authorId": "1996134",
                        "name": "Razvan Pascanu"
                    },
                    {
                        "authorId": "34269227",
                        "name": "Jack W. Rae"
                    },
                    {
                        "authorId": "2217144",
                        "name": "Simon Osindero"
                    },
                    {
                        "authorId": "152585800",
                        "name": "Erich Elsen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Unstructured IMP (Han et al., 2015; Frankle & Carbin, 2018) serves as an effective method to find these winning tickets, and Dynamic Sparse Training (Mostafa & Wang, 2019; Mocanu et al., 2018; Evci et al., 2020) is also capable of identifying subnetworks with promising performance.",
                ", 2015; Frankle & Carbin, 2018) serves as an effective method to find these winning tickets, and Dynamic Sparse Training (Mostafa & Wang, 2019; Mocanu et al., 2018; Evci et al., 2020) is also capable of identifying subnetworks with promising performance."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6bc4681828143f5ecc49b7ecd388a86c70c7237a",
                "externalIds": {
                    "DBLP": "conf/icml/ZhangCCW21",
                    "ArXiv": "2106.03225",
                    "CorpusId": 235358439
                },
                "corpusId": 235358439,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6bc4681828143f5ecc49b7ecd388a86c70c7237a",
                "title": "Efficient Lottery Ticket Finding: Less Data is More",
                "abstract": "The lottery ticket hypothesis (LTH) reveals the existence of winning tickets (sparse but critical subnetworks) for dense networks, that can be trained in isolation from random initialization to match the latter's accuracies. However, finding winning tickets requires burdensome computations in the train-prune-retrain process, especially on large-scale datasets (e.g., ImageNet), restricting their practical benefits. This paper explores a new perspective on finding lottery tickets more efficiently, by doing so only with a specially selected subset of data, called Pruning-Aware Critical set (PrAC set), rather than using the full training set. The concept of PrAC set was inspired by the recent observation, that deep networks have samples that are either hard to memorize during training, or easy to forget during pruning. A PrAC set is thus hypothesized to capture those most challenging and informative examples for the dense model. We observe that a high-quality winning ticket can be found with training and pruning the dense network on the very compact PrAC set, which can substantially save training iterations for the ticket finding process. Extensive experiments validate our proposal across diverse datasets and network architectures. Specifically, on CIFAR-10, CIFAR-100, and Tiny ImageNet, we locate effective PrAC sets at 35.32%~78.19% of their training set sizes. On top of them, we can obtain the same competitive winning tickets for the corresponding dense networks, yet saving up to 82.85%~92.77%, 63.54%~74.92%, and 76.14%~86.56% training iterations, respectively. Crucially, we show that a PrAC set found is reusable across different network architectures, which can amortize the extra cost of finding PrAC sets, yielding a practical regime for efficient lottery ticket finding.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109338656",
                        "name": "Zhenyu (Allen) Zhang"
                    },
                    {
                        "authorId": "1753619492",
                        "name": "Xuxi Chen"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Parameters are pruned based on magnitude and grown back at random [23] or based on gradient [25] or momentum [24] information."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "bc4c65be9564abb0c40f5e754398bba450889568",
                "externalIds": {
                    "DBLP": "conf/cvpr/VemparalaFFSZKF21",
                    "DOI": "10.1109/CVPRW53098.2021.00016",
                    "CorpusId": 235703193
                },
                "corpusId": 235703193,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bc4c65be9564abb0c40f5e754398bba450889568",
                "title": "Adversarial Robust Model Compression using In-Train Pruning",
                "abstract": "Efficiently deploying learning-based systems on embedded hardware is challenging for various reasons, two of which are considered in this paper: The model\u2019s size and its robustness against attacks. Both need to be addressed even-handedly. We combine adversarial training and model pruning in a joint formulation of the fundamental learning objective during training. Unlike existing post-train pruning approaches, our method does not use heuristics and eliminates the need for a pre-trained model. This allows for a classifier which is robust against attacks and enables better compression of the model, reducing its computational effort. In comparison to prior work, our approach yields 6.21 pp higher accuracy for an 85 % reduction in parameters for ResNet20 on the CIFAR-10 dataset.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9261189",
                        "name": "M. Vemparala"
                    },
                    {
                        "authorId": "1752876377",
                        "name": "Nael Fasfous"
                    },
                    {
                        "authorId": "118715093",
                        "name": "Alexander Frickenstein"
                    },
                    {
                        "authorId": "30641664",
                        "name": "Sreetama Sarkar"
                    },
                    {
                        "authorId": "2110560568",
                        "name": "Qi Zhao"
                    },
                    {
                        "authorId": "2117318540",
                        "name": "Sabine Kuhn"
                    },
                    {
                        "authorId": "2045300041",
                        "name": "Lukas Frickenstein"
                    },
                    {
                        "authorId": "2120295001",
                        "name": "Anmol Singh"
                    },
                    {
                        "authorId": "1728721",
                        "name": "C. Unger"
                    },
                    {
                        "authorId": "40031905",
                        "name": "N. Nagaraja"
                    },
                    {
                        "authorId": "2198719",
                        "name": "Christian Wressnegger"
                    },
                    {
                        "authorId": "2040484",
                        "name": "W. Stechele"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It essentially sparsifies the network at a fine-grained level and is demonstrated to achieve an extremely high compression rate and high accuracy performance [4], [5], [6]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0338285d0ae36a1d6ce9cc3d44e132492212f7ac",
                "externalIds": {
                    "ArXiv": "2105.14713",
                    "DBLP": "journals/pami/LinZLCCWLTJ23",
                    "DOI": "10.1109/TPAMI.2022.3195774",
                    "CorpusId": 244347939,
                    "PubMed": "35917571"
                },
                "corpusId": 244347939,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0338285d0ae36a1d6ce9cc3d44e132492212f7ac",
                "title": "1xN Pattern for Pruning Convolutional Neural Networks",
                "abstract": "Though network pruning receives popularity in reducing the complexity of convolutional neural networks (CNNs), it remains an open issue to concurrently maintain model accuracy as well as achieve significant speedups on general CPUs. In this paper, we propose a novel 1\u00d7N pruning pattern to break this limitation. In particular, consecutive N output kernels with the same input channel index are grouped into one block, which serves as a basic pruning granularity of our pruning pattern. Our 1\u00d7N pattern prunes these blocks considered unimportant. We also provide a workflow of filter rearrangement that first rearranges the weight matrix in the output channel dimension to derive more influential blocks for accuracy improvements and then applies similar rearrangement to the next-layer weights in the input channel dimension to ensure correct convolutional operations. Moreover, the output computation after our 1\u00d7N pruning can be realized via a parallelized block-wise vectorized operation, leading to significant speedups on general CPUs. The efficacy of our pruning pattern is proved with experiments on ILSVRC-2012. For example, given the pruning rate of 50% and N=4, our pattern obtains about 3.0% improvements over filter pruning in the top-1 accuracy of MobileNet-V2. Meanwhile, it obtains 56.04ms inference savings on Cortex-A7 CPU over weight pruning. Our project is made available at https://github.com/lmbxmu/1xN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49352079",
                        "name": "Mingbao Lin"
                    },
                    {
                        "authorId": "2145067719",
                        "name": "Yu-xin Zhang"
                    },
                    {
                        "authorId": "2110482496",
                        "name": "Yuchao Li"
                    },
                    {
                        "authorId": "2152690044",
                        "name": "Bohong Chen"
                    },
                    {
                        "authorId": "2070266415",
                        "name": "Fei Chao"
                    },
                    {
                        "authorId": "50468734",
                        "name": "Mengdi Wang"
                    },
                    {
                        "authorId": "2153701890",
                        "name": "Shen Li"
                    },
                    {
                        "authorId": "40161651",
                        "name": "Yonghong Tian"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While some recent data-driven, unstructured approaches achieved higher levels of compression on this benchmark Evci et al. (2020), these results show the potential of RED as an efficient, portable and privacy compliant data-free, structured pruning method."
            ],
            "isInfluential": false,
            "intents": [
                "result"
            ],
            "citingPaper": {
                "paperId": "75437ae4cf8c8c04d68a9063440f802d211197d9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-14797",
                    "ArXiv": "2105.14797",
                    "CorpusId": 235253836
                },
                "corpusId": 235253836,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/75437ae4cf8c8c04d68a9063440f802d211197d9",
                "title": "RED : Looking for Redundancies for Data-Free Structured Compression of Deep Neural Networks",
                "abstract": "Deep Neural Networks (DNNs) are ubiquitous in today's computer vision land-scape, despite involving considerable computational costs. The mainstream approaches for runtime acceleration consist in pruning connections (unstructured pruning) or, better, filters (structured pruning), both often requiring data to re-train the model. In this paper, we present RED, a data-free structured, unified approach to tackle structured pruning. First, we propose a novel adaptive hashing of the scalar DNN weight distribution densities to increase the number of identical neurons represented by their weight vectors. Second, we prune the network by merging redundant neurons based on their relative similarities, as defined by their distance. Third, we propose a novel uneven depthwise separation technique to further prune convolutional layers. We demonstrate through a large variety of benchmarks that RED largely outperforms other data-free pruning methods, often reaching performance similar to unconstrained, data-driven methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1632928879",
                        "name": "Edouard Yvinec"
                    },
                    {
                        "authorId": "3190846",
                        "name": "Arnaud Dapogny"
                    },
                    {
                        "authorId": "51021910",
                        "name": "M. Cord"
                    },
                    {
                        "authorId": "2521061",
                        "name": "K\u00e9vin Bailly"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Particularly, errors can reach zero with sufficient training [21] for models that are not constrained by capacity (d \u2265 0.",
                "Some works rewire weights every training iteration [18, 22, 29\u201331], while others rewire every hundreds of training steps [21] or after an entire pass through the data [19,20]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4862141c0283502fe30d0c3b2f01c87b30fd15dd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-12920",
                    "ArXiv": "2105.12920",
                    "CorpusId": 235212519
                },
                "corpusId": 235212519,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4862141c0283502fe30d0c3b2f01c87b30fd15dd",
                "title": "Search Spaces for Neural Model Training",
                "abstract": "While larger neural models are pushing the boundaries of what deep learning can do, often more weights are needed to train models rather than to run inference for tasks. This paper seeks to understand this behavior using search spaces -- adding weights creates extra degrees of freedom that form new paths for optimization (or wider search spaces) rendering neural model training more effective. We then show how we can augment search spaces to train sparse models attaining competitive scores across dozens of deep learning workloads. They are also are tolerant of structures targeting current hardware, opening avenues for training and inference acceleration. Our work encourages research to explore beyond massive neural models being used today.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "33749574",
                        "name": "Darko Stosic"
                    },
                    {
                        "authorId": "2737605",
                        "name": "D. Stosic"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To solve this trade-off, previous works assign some parts of the model to be language specific: Language specific decoders (Dong et al., 2015), Language specific encoders and decoders (Firat et al., 2016) and Language specific hidden states and embeds (Wang et al., 2018)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6ff5ea40e0d1be8c71e5b675db64fe730018db03",
                "externalIds": {
                    "ArXiv": "2105.09259",
                    "DBLP": "conf/acl/LinWWL20",
                    "ACL": "2021.acl-long.25",
                    "DOI": "10.18653/v1/2021.acl-long.25",
                    "CorpusId": 234778222
                },
                "corpusId": 234778222,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/6ff5ea40e0d1be8c71e5b675db64fe730018db03",
                "title": "Learning Language Specific Sub-network for Multilingual Machine Translation",
                "abstract": "Multilingual neural machine translation aims at learning a single translation model for multiple languages. These jointly trained models often suffer from performance degradationon rich-resource language pairs. We attribute this degeneration to parameter interference. In this paper, we propose LaSS to jointly train a single unified multilingual MT model. LaSS learns Language Specific Sub-network (LaSS) for each language pair to counter parameter interference. Comprehensive experiments on IWSLT and WMT datasets with various Transformer architectures show that LaSS obtains gains on 36 language pairs by up to 1.2 BLEU. Besides, LaSS shows its strong generalization performance at easy adaptation to new language pairs and zero-shot translation. LaSS boosts zero-shot translation with an average of 8.3 BLEU on 30 language pairs. Codes and trained models are available at https://github.com/NLP-Playground/LaSS.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "32064536",
                        "name": "Zehui Lin"
                    },
                    {
                        "authorId": "2125109508",
                        "name": "Liwei Wu"
                    },
                    {
                        "authorId": "50468534",
                        "name": "Mingxuan Wang"
                    },
                    {
                        "authorId": "143900005",
                        "name": "Lei Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026upon the higher-order representation provided by an internal node, beyond following gradient descent to optimize the weights for connections to the node (Rumelhart et al. 1985), a reasonable next step is to use the loss gradient to help select the growth of new connections (Evci et al. 2019).",
                "\u2026cases should be expected to improve the performance of the network; even if redundant information is fed forward in some cases, denser networks are more likely to contain a \u201cwinning ticket\u201d than randomly selected sparse networks (see Evci et al. 2019 for empirical results supporting this notion).",
                "Recently, a new rewiring approach has been proposed that selects new connection growth based on the magnitude of the loss gradient for possible connections (Evci et al. 2019).",
                "However, as suggested by Evci et al. (2019), forming potentially redundant connections between adjacent layers can help knock networks out of local optima (or into a \u201cwinning lottery ticket\u201d), so restricting such connections entirely is not necessarily a favorable approach, as it would preclude\u2026"
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "781151d06b1b7e3e5ccdb8d5e312a9a6dff02d7e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-08111",
                    "ArXiv": "2105.08111",
                    "CorpusId": 234763385
                },
                "corpusId": 234763385,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/781151d06b1b7e3e5ccdb8d5e312a9a6dff02d7e",
                "title": "Livewired Neural Networks: Making Neurons That Fire Together Wire Together",
                "abstract": "Until recently, artificial neural networks were typically designed with a fixed network structure. Here, I argue that network structure is highly relevant to function, and therefore neural networks should be livewired (Eagleman 2020): dynamically rewired to reflect relationships between higher order representations of the external environment identified by coincident activations in individual neurons. I discuss how this approach may enable such networks to build compositional world models that operate on symbols and that achieve few-shot learning, capabilities thought by many to be critical to human-level cognition. Here, I also 1) discuss how such livewired neural networks maximize the information the environment provides to a model, 2) explore evidence indicating that livewiring is implemented in the brain, guided by glial cells, 3) discuss how livewiring may give rise to the associative emergent behaviors of brains, and 4) suggest paths for future research using livewired networks to understand and create human-like reasoning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2057655034",
                        "name": "Thomas Schumacher"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other approaches from literature are grow-and-prune strategies [3, 8, 11, 12, 18] which, starting from sparse networks, successively add and remove neurons or connections while training the networks."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e47da8038b5b44d6452c92e3e4641f90f3b5ecff",
                "externalIds": {
                    "ArXiv": "2105.04319",
                    "DBLP": "journals/corr/abs-2105-04319",
                    "CorpusId": 234336096
                },
                "corpusId": 234336096,
                "publicationVenue": {
                    "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                    "name": "Journal of machine learning research",
                    "type": "journal",
                    "alternate_names": [
                        "Journal of Machine Learning Research",
                        "J mach learn res",
                        "J Mach Learn Res"
                    ],
                    "issn": "1532-4435",
                    "alternate_issns": [
                        "1533-7928"
                    ],
                    "url": "http://www.ai.mit.edu/projects/jmlr/",
                    "alternate_urls": [
                        "http://jmlr.csail.mit.edu/",
                        "http://www.jmlr.org/",
                        "http://portal.acm.org/affiliated/jmlr"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e47da8038b5b44d6452c92e3e4641f90f3b5ecff",
                "title": "A Bregman Learning Framework for Sparse Neural Networks",
                "abstract": "We propose a learning framework based on stochastic Bregman iterations, also known as mirror descent, to train sparse neural networks with an inverse scale space approach. We derive a baseline algorithm called LinBreg, an accelerated version using momentum, and AdaBreg, which is a Bregmanized generalization of the Adam algorithm. In contrast to established methods for sparse training the proposed family of algorithms constitutes a regrowth strategy for neural networks that is solely optimization-based without additional heuristics. Our Bregman learning framework starts the training with very few initial parameters, successively adding only significant ones to obtain a sparse and expressive network. The proposed approach is extremely easy and efficient, yet supported by the rich mathematical theory of inverse scale space methods. We derive a statistically profound sparse parameter initialization strategy and provide a rigorous stochastic convergence analysis of the loss decay and additional convergence proofs in the convex regime. Using only 3.4% of the parameters of ResNet-18 we achieve 90.2% test accuracy on CIFAR-10, compared to 93.6% using the dense network. Our algorithm also unveils an autoencoder architecture for a denoising task. The proposed framework also has a huge potential for integrating sparse backpropagation and resource-friendly training.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9947767",
                        "name": "Leon Bungert"
                    },
                    {
                        "authorId": "2032253860",
                        "name": "Tim Roith"
                    },
                    {
                        "authorId": "1684319",
                        "name": "D. Tenbrinck"
                    },
                    {
                        "authorId": "145020801",
                        "name": "M. Burger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following Evci et al. (2019); Gale et al. (2019), we conduct experiments to compare this strong baseline with `1-norm filters pruning while employing CLR on CIFAR-10, CIFAR-100 and ImageNet."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "dfe2efeea8889a1937be16538220f5e3477d42fb",
                "externalIds": {
                    "DBLP": "conf/iclr/LeH21",
                    "ArXiv": "2105.03193",
                    "CorpusId": 234096198
                },
                "corpusId": 234096198,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/dfe2efeea8889a1937be16538220f5e3477d42fb",
                "title": "Network Pruning That Matters: A Case Study on Retraining Variants",
                "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning. One-sentence Summary: We study the effective of different retraining mechanisms while doing pruning",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2059393417",
                        "name": "Duong H. Le"
                    },
                    {
                        "authorId": "143807806",
                        "name": "Binh-Son Hua"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a5e56209623c52f3f7ddfaa92f9e44cfc6ffc972",
                "externalIds": {
                    "MAG": "3158839075",
                    "DBLP": "journals/corr/abs-2105-01571",
                    "ArXiv": "2105.01571",
                    "DOI": "10.1109/CVPR46437.2021.00360",
                    "CorpusId": 233714694
                },
                "corpusId": 233714694,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a5e56209623c52f3f7ddfaa92f9e44cfc6ffc972",
                "title": "Effective Sparsification of Neural Networks with Global Sparsity Constraint",
                "abstract": "Weight pruning is an effective technique to reduce the model size and inference time for deep neural networks in real-world deployments. However, since magnitudes and relative importance of weights are very different for different layers of a neural network, existing methods rely on either manual tuning or handcrafted heuristic rules to find appropriate pruning rates individually for each layer. This approach generally leads to suboptimal performance. In this paper, by directly working on the probability space, we propose an effective network sparsification method called probabilistic masking (ProbMask), which solves a natural sparsification formulation under global sparsity constraint. The key idea is to use probability as a global criterion for all layers to measure the weight importance. An appealing feature of ProbMask is that the amounts of weight redundancy can be learned automatically via our constraint and thus we avoid the problem of tuning pruning rates individually for different layers in a network. Extensive experimental results on CIFAR-10/100 and ImageNet demonstrate that our method is highly effective, and can outperform previous state-of-the-art methods by a significant margin, especially in the high pruning rate situation. Notably, the gap of Top-1 accuracy between our ProbMask and existing methods can be up to 10%. As a by-product, we show ProbMask is also highly effective in identifying supermasks, which are sub-networks with high performance in a randomly weighted dense neural network.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109127048",
                        "name": "Xiao Zhou"
                    },
                    {
                        "authorId": "47527753",
                        "name": "Weizhong Zhang"
                    },
                    {
                        "authorId": "47995165",
                        "name": "Hang Xu"
                    },
                    {
                        "authorId": "50728655",
                        "name": "Tong Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, it would be interesting to study more advanced pruning algorithms such as [41, 42], especially iterative or inherently sparse ones that could explore structures corresponding to prohibitively large networks."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "eca3791904a4b5d0f74cfe2300f2be0900ebeff6",
                "externalIds": {
                    "ArXiv": "2104.13343",
                    "DBLP": "journals/corr/abs-2104-13343",
                    "CorpusId": 233407723
                },
                "corpusId": 233407723,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/eca3791904a4b5d0f74cfe2300f2be0900ebeff6",
                "title": "Sifting out the features by pruning: Are convolutional networks the winning lottery ticket of fully connected ones?",
                "abstract": "Pruning methods can considerably reduce the size of artificial neural networks without harming their performance. In some cases, they can even uncover sub-networks that, when trained in isolation, match or surpass the test accuracy of their dense counterparts. Here we study the inductive bias that pruning imprints in such\"winning lottery tickets\". Focusing on visual tasks, we analyze the architecture resulting from iterative magnitude pruning of a simple fully connected network (FCN). We show that the surviving node connectivity is local in input space, and organized in patterns reminiscent of the ones found in convolutional networks (CNN). We investigate the role played by data and tasks in shaping the architecture of pruned sub-networks. Our results show that the winning lottery tickets of FCNs display the key features of CNNs. The ability of such automatic network-simplifying procedure to recover the key features\"hand-crafted\"in the design of CNNs suggests interesting applications to other datasets and tasks, in order to discover new and efficient architectural inductive biases.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "39324024",
                        "name": "F. Pellegrini"
                    },
                    {
                        "authorId": "2188423",
                        "name": "G. Biroli"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Also, lottery ticket related researches [9], [11], [14] mostly focus on finding a single winning ticket for a single task, which cannot be directly translated to the multitask problem."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "41038c23eefd2ab4bc56d465d290c8cd5e4421ed",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-12501",
                    "ArXiv": "2104.12501",
                    "DOI": "10.1109/SPAWC51858.2021.9593126",
                    "CorpusId": 233394124
                },
                "corpusId": 233394124,
                "publicationVenue": {
                    "id": "60c9a57f-2d17-4c7c-a0de-8e673a1a109a",
                    "name": "International Workshop on Signal Processing Advances in Wireless Communications",
                    "type": "conference",
                    "alternate_names": [
                        "Int Workshop Signal Process Adv Wirel Commun",
                        "SPAWC"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/41038c23eefd2ab4bc56d465d290c8cd5e4421ed",
                "title": "Communication-Efficient and Personalized Federated Lottery Ticket Learning",
                "abstract": "The lottery ticket hypothesis (LTH) claims that a deep neural network (i.e., ground network) contains a number of subnetworks (i.e., winning tickets), each of which exhibiting identically accurate inference capability as that of the ground network. Federated learning (FL) has recently been applied in LotteryFL to discover such winning tickets in a distributed way, showing higher accuracy multi-task learning than Vanilla FL. Nonetheless, LotteryFL relies on unicast transmission on the downlink, and ignores mitigating stragglers, questioning scalability. Motivated by this, in this article we propose a personalized and communication-efficient federated lottery ticket learning algorithm, coined CELL, which exploits downlink broadcast for communication efficiency. Furthermore, it utilizes a novel user grouping method, thereby alternating between FL and lottery learning to mitigate stragglers. Numerical simulations validate that CELL achieves up to 3.6% higher personalized task classification accuracy with 4.3x smaller total communication cost until convergence under the CIFAR-10 dataset.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2072586251",
                        "name": "Sejin Seo"
                    },
                    {
                        "authorId": "2490270",
                        "name": "Seung-Woo Ko"
                    },
                    {
                        "authorId": "48490823",
                        "name": "Jihong Park"
                    },
                    {
                        "authorId": "1692004",
                        "name": "Seong-Lyun Kim"
                    },
                    {
                        "authorId": "1702172",
                        "name": "M. Bennis"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "91ad4b4cf2df2c2029e860171e8dd8d5b391a40d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-08700",
                    "ArXiv": "2104.08700",
                    "DOI": "10.1109/TPAMI.2023.3311783",
                    "CorpusId": 233297054,
                    "PubMed": "37669203"
                },
                "corpusId": 233297054,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/91ad4b4cf2df2c2029e860171e8dd8d5b391a40d",
                "title": "Lottery Jackpots Exist in Pre-trained Models",
                "abstract": "Network pruning is an effective approach to reduce network complexity with acceptable performance compromise. Existing studies achieve the sparsity of neural networks via time-consuming weight training or complex searching on networks with expanded width, which greatly limits the applications of network pruning. In this paper, we show that high-performing and sparse sub-networks without the involvement of weight training, termed \"lottery jackpots\", exist in pre-trained models with unexpanded width. Our presented lottery jackpots are traceable through empirical and theoretical outcomes. For example, we obtain a lottery jackpot that has only 10% parameters and still reaches the performance of the original dense VGGNet-19 without any modifications on the pre-trained weights on CIFAR-10. Furthermore, we improve the efficiency for searching lottery jackpots from two perspectives. Firstly, we observe that the sparse masks derived from many existing pruning criteria have a high overlap with the searched mask of our lottery jackpot, among which, the magnitude-based pruning results in the most similar mask with ours. In compliance with this insight, we initialize our sparse mask using the magnitude-based pruning, resulting in at least 3\u00d7 cost reduction on the lottery jackpot searching while achieving comparable or even better performance. Secondly, we conduct an in-depth analysis of the searching process for lottery jackpots. Our theoretical result suggests that the decrease in training loss during weight searching can be disturbed by the dependency between weights in modern networks. To mitigate this, we propose a novel short restriction method to restrict change of masks that may have potential negative impacts on the training loss, which leads to a faster convergence and reduced oscillation for searching lottery jackpots. Consequently, our searched lottery jackpot removes 90% weights in ResNet-50, while it easily obtains more than 70% top-1 accuracy using only 5 searching epochs on ImageNet. Our code is available at https://github.com/zyxxmu/lottery-jackpots.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108078624",
                        "name": "Yu-xin Zhang"
                    },
                    {
                        "authorId": "49352079",
                        "name": "Mingbao Lin"
                    },
                    {
                        "authorId": "2070266415",
                        "name": "Fei Chao"
                    },
                    {
                        "authorId": "2152543905",
                        "name": "Yan Wang"
                    },
                    {
                        "authorId": "47096329",
                        "name": "Yongjian Wu"
                    },
                    {
                        "authorId": "1835006",
                        "name": "Feiyue Huang"
                    },
                    {
                        "authorId": "2285442",
                        "name": "Mingliang Xu"
                    },
                    {
                        "authorId": "40161651",
                        "name": "Yonghong Tian"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In another case [31], the training schedule was extended for some networks by 5\u00d7 in order to reach the same accuracy as the dense model."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "90d5e6f8d3b9f2617b3a3cf00fb02e730eb011cb",
                "externalIds": {
                    "ArXiv": "2104.08378",
                    "DBLP": "journals/corr/abs-2104-08378",
                    "CorpusId": 233296249
                },
                "corpusId": 233296249,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/90d5e6f8d3b9f2617b3a3cf00fb02e730eb011cb",
                "title": "Accelerating Sparse Deep Neural Networks",
                "abstract": "As neural network model sizes have dramatically increased, so has the interest in various techniques to reduce their parameter counts and accelerate their execution. An active area of research in this field is sparsity - encouraging zero values in parameters that can then be discarded from storage or computations. While most research focuses on high levels of sparsity, there are challenges in universally maintaining model accuracy as well as achieving significant speedups over modern matrix-math hardware. To make sparsity adoption practical, the NVIDIA Ampere GPU architecture introduces sparsity support in its matrix-math units, Tensor Cores. We present the design and behavior of Sparse Tensor Cores, which exploit a 2:4 (50%) sparsity pattern that leads to twice the math throughput of dense matrix units. We also describe a simple workflow for training networks that both satisfy 2:4 sparsity pattern requirements and maintain accuracy, verifying it on a wide range of common tasks and model architectures. This workflow makes it easy to prepare accurate models for efficient deployment on Sparse Tensor Cores.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35769149",
                        "name": "Asit K. Mishra"
                    },
                    {
                        "authorId": "2060797517",
                        "name": "J. Latorre"
                    },
                    {
                        "authorId": "47325862",
                        "name": "Jeff Pool"
                    },
                    {
                        "authorId": "33749574",
                        "name": "Darko Stosic"
                    },
                    {
                        "authorId": "2737605",
                        "name": "D. Stosic"
                    },
                    {
                        "authorId": "145595812",
                        "name": "Ganesh Venkatesh"
                    },
                    {
                        "authorId": "2116145799",
                        "name": "Chong Yu"
                    },
                    {
                        "authorId": "1802359",
                        "name": "P. Micikevicius"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5c0705d856eb18666db4318cf76416560764a856",
                "externalIds": {
                    "DBLP": "conf/nips/ChenCWGLW21",
                    "ArXiv": "2103.16547",
                    "CorpusId": 232417266
                },
                "corpusId": 232417266,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5c0705d856eb18666db4318cf76416560764a856",
                "title": "The Elastic Lottery Ticket Hypothesis",
                "abstract": "Lottery Ticket Hypothesis (LTH) raises keen attention to identifying sparse trainable subnetworks, or winning tickets, which can be trained in isolation to achieve similar or even better performance compared to the full models. Despite many efforts being made, the most effective method to identify such winning tickets is still Iterative Magnitude-based Pruning (IMP), which is computationally expensive and has to be run thoroughly for every different network. A natural question that comes in is: can we\"transform\"the winning ticket found in one network to another with a different architecture, yielding a winning ticket for the latter at the beginning, without re-doing the expensive IMP? Answering this question is not only practically relevant for efficient\"once-for-all\"winning ticket finding, but also theoretically appealing for uncovering inherently scalable sparse patterns in networks. We conduct extensive experiments on CIFAR-10 and ImageNet, and propose a variety of strategies to tweak the winning tickets found from different networks of the same model family (e.g., ResNets). Based on these results, we articulate the Elastic Lottery Ticket Hypothesis (E-LTH): by mindfully replicating (or dropping) and re-ordering layers for one network, its corresponding winning ticket could be stretched (or squeezed) into a subnetwork for another deeper (or shallower) network from the same family, whose performance is nearly the same competitive as the latter's winning ticket directly found by IMP. We have also extensively compared E-LTH with pruning-at-initialization and dynamic sparse training methods, as well as discussed the generalizability of E-LTH to different model families, layer types, and across datasets. Code is available at https://github.com/VITA-Group/ElasticLTH.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "66273769",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "153655416",
                        "name": "Yu Cheng"
                    },
                    {
                        "authorId": "2992833",
                        "name": "Shuohang Wang"
                    },
                    {
                        "authorId": "144702900",
                        "name": "Zhe Gan"
                    },
                    {
                        "authorId": "46700348",
                        "name": "Jingjing Liu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Sparse training is a line of work where traditional architectures are trained with sparse instead of dense layers and the number of parameters allowed during training is restricted to a percentage of the dense layers (Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Mostafa & Wang, 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b15ea460c77a4ee8aa159a30ab0331deedfcf392",
                "externalIds": {
                    "DBLP": "conf/icml/LewisBDGZ21",
                    "ArXiv": "2103.16716",
                    "CorpusId": 232428341
                },
                "corpusId": 232428341,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b15ea460c77a4ee8aa159a30ab0331deedfcf392",
                "title": "BASE Layers: Simplifying Training of Large, Sparse Models",
                "abstract": "We introduce a new balanced assignment of experts (BASE) layer for large language models that greatly simplifies existing high capacity sparse layers. Sparse layers can dramatically improve the efficiency of training and inference by routing each token to specialized expert modules that contain only a small fraction of the model parameters. However, it can be difficult to learn balanced routing functions that make full use of the available experts; existing approaches typically use routing heuristics or auxiliary expert-balancing loss functions. In contrast, we formulate token-to-expert allocation as a linear assignment problem, allowing an optimal assignment in which each expert receives an equal number of tokens. This optimal assignment scheme improves efficiency by guaranteeing balanced compute loads, and also simplifies training by not requiring any new hyperparameters or auxiliary losses. Code is publicly released at https://github.com/pytorch/fairseq/",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35084211",
                        "name": "M. Lewis"
                    },
                    {
                        "authorId": "2116473",
                        "name": "Shruti Bhosale"
                    },
                    {
                        "authorId": "3239480",
                        "name": "Tim Dettmers"
                    },
                    {
                        "authorId": "39589154",
                        "name": "Naman Goyal"
                    },
                    {
                        "authorId": "1982950",
                        "name": "Luke Zettlemoyer"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b37c4e3242122ce0d24292b9b9da90575f14003c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-16013",
                    "ArXiv": "2103.16013",
                    "CorpusId": 232417559
                },
                "corpusId": 232417559,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b37c4e3242122ce0d24292b9b9da90575f14003c",
                "title": "Training Sparse Neural Network by Constraining Synaptic Weight on Unit Lp Sphere",
                "abstract": "Sparse deep neural networks have shown their advantages over dense models with fewer parameters and higher computational efficiency. Here we demonstrate constraining the synaptic weights on unit Lp-sphere enables the flexibly control of the sparsity with p and improves the generalization ability of neural networks. Firstly, to optimize the synaptic weights constrained on unit Lp-sphere, the parameter optimization algorithm, Lp-spherical gradient descent (LpSGD) is derived from the augmented Empirical Risk Minimization condition, which is theoretically proved to be convergent. To understand the mechanism of how p affects Hoyer's sparsity, the expectation of Hoyer's sparsity under the hypothesis of gamma distribution is given and the predictions are verified at various p under different conditions. In addition, the\"semi-pruning\"and threshold adaptation are designed for topology evolution to effectively screen out important connections and lead the neural networks converge from the initial sparsity to the expected sparsity. Our approach is validated by experiments on benchmark datasets covering a wide range of domains. And the theoretical analysis pave the way to future works on training sparse neural networks with constrained optimization.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1812614631",
                        "name": "Weipeng Li"
                    },
                    {
                        "authorId": "2112080628",
                        "name": "Xiaogang Yang"
                    },
                    {
                        "authorId": "1845235",
                        "name": "Chuanxiang Li"
                    },
                    {
                        "authorId": "9359471",
                        "name": "Ruitao Lu"
                    },
                    {
                        "authorId": "2110689421",
                        "name": "Xueli Xie"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We calculate theoretical FLOP requirements in a manner similar to Evci et al. [2020] (exact details in the supplementary material).",
                "Evaluated on image classification, the central claims of Evci et al. [2020] hold true\u2014RigL outperforms existing sparse-to-sparse training methods and can also surpass other dense-to-sparse training methods with extended training.",
                "Similar to Evci et al. [2020], we assume that algorithms utilize sparsity during training."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "de178cb7d109a67449d7ba3b850e2c077c10b2ee",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-15767",
                    "ArXiv": "2103.15767",
                    "CorpusId": 232417185
                },
                "corpusId": 232417185,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/de178cb7d109a67449d7ba3b850e2c077c10b2ee",
                "title": "[Reproducibility Report] Rigging the Lottery: Making All Tickets Winners",
                "abstract": "For a fixed parameter count and compute budget, the proposed algorithm (RigL) claims to directly train sparse networks that match or exceed the performance of existing dense-to-sparse training techniques (such as pruning). RigL does so while requiring constant Floating Point Operations (FLOPs) throughout training. The technique obtains state-of-the-art performance on a variety of tasks, including image classification and character-level language-modelling.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "152894647",
                        "name": "Varun Sundar"
                    },
                    {
                        "authorId": "2061140054",
                        "name": "Rajat Vadiraj Dwaraknath"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d3c2dc510b17ad57a2a8f0a6b4ee59e977657ea4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-07674",
                    "ArXiv": "2103.07674",
                    "CorpusId": 232233641
                },
                "corpusId": 232233641,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d3c2dc510b17ad57a2a8f0a6b4ee59e977657ea4",
                "title": "Efficient Sparse Artificial Neural Networks",
                "abstract": "The brain, as the source of inspiration for Artificial Neural Networks (ANN), is based on a sparse structure. This sparse structure helps the brain to consume less energy, learn easier and generalize patterns better than any other ANN. In this paper, two evolutionary methods for adopting sparsity to ANNs are proposed. In the proposed methods, the sparse structure of a network as well as the values of its parameters are trained and updated during the learning process. The simulation results show that these two methods have better accuracy and faster convergence while they need fewer training samples compared to their sparse and non-sparse counterparts. Furthermore, the proposed methods significantly improve the generalization power and reduce the number of parameters. For example, the sparsification of the ResNet47 network by exploiting our proposed methods for the image classification of ImageNet dataset uses 40 % fewer parameters while the top-1 accuracy of the model improves by 12% and 5% compared to the dense network and their sparse counterpart, respectively. As another example, the proposed methods for the CIFAR10 dataset converge to their final structure 7 times faster than its sparse counterpart, while the final accuracy increases by 6%.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2053820248",
                        "name": "Seyed Majid Naji"
                    },
                    {
                        "authorId": "2198336",
                        "name": "A. Abtahi"
                    },
                    {
                        "authorId": "152735346",
                        "name": "F. Marvasti"
                    }
                ]
            }
        },
        {
            "contexts": [
                "RigL [Evci et al., 2020a] also uses magnitudes for pruning, yet they employ the absolute gradients for weight growing.",
                "Some researchers (such as [Evci et al., 2020a]) conjecture that dynamic and adaptive masks during training may be better, thus introduce another group of methods featured by dynamic masks.",
                "[Evci et al., 2020b] (GradFlow) present a gradient flow perspective to explain why LTH happens."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "fa3bc410e0dd057642ecc484645acdbacfbe7d2e",
                "externalIds": {
                    "DBLP": "conf/ijcai/WangQBZF22",
                    "ArXiv": "2103.06460",
                    "DOI": "10.24963/ijcai.2022/786",
                    "CorpusId": 247012121
                },
                "corpusId": 247012121,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/fa3bc410e0dd057642ecc484645acdbacfbe7d2e",
                "title": "Recent Advances on Neural Network Pruning at Initialization",
                "abstract": "Neural network pruning typically removes connections or neurons from a pretrained converged model; while a new pruning paradigm, pruning at initialization (PaI), attempts to prune a randomly initialized network. This paper offers the first survey concentrated on this emerging pruning fashion. We first introduce a generic formulation of neural network pruning, followed by the major classic pruning topics. Then, as the main body of this paper, a thorough and structured literature review of PaI methods is presented, consisting of two major tracks (sparse training and sparse selection). Finally, we summarize the surge of PaI compared to PaT and discuss the open problems. Apart from the dedicated literature review, this paper also offers a code base for easy sanity-checking and benchmarking of different PaI methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2113269100",
                        "name": "Huan Wang"
                    },
                    {
                        "authorId": "12282768",
                        "name": "Can Qin"
                    },
                    {
                        "authorId": "153802755",
                        "name": "Yue Bai"
                    },
                    {
                        "authorId": "2129519081",
                        "name": "Yulun Zhang"
                    },
                    {
                        "authorId": "2156255943",
                        "name": "Yun Fu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[10] show that if they trained with RigL sparse CNNs for a long enough time, they can reach the performance of the dense counterparts.",
                "To enhance a faster convergence, [7] and [10] introduced the idea of using momentum and gradient information (quantified in two methods named, Sparse Momentum and RigL, respectively) from non-existing connections during the regrow steps."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "5e5c79a7d3d3cdd79c2ff18359c9c2f0f695ab0b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-01636",
                    "ArXiv": "2103.01636",
                    "DOI": "10.5555/3463952.3463960",
                    "CorpusId": 232092928
                },
                "corpusId": 232092928,
                "publicationVenue": {
                    "id": "6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                    "name": "Adaptive Agents and Multi-Agent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Adapt Agent Multi-agent Syst",
                        "International Joint Conference on Autonomous Agents & Multiagent Systems",
                        "Adapt Agent Multi-agents Syst",
                        "AAMAS",
                        "Adaptive Agents and Multi-Agents Systems",
                        "Int Jt Conf Auton Agent  Multiagent Syst"
                    ],
                    "url": "http://www.ifaamas.org/"
                },
                "url": "https://www.semanticscholar.org/paper/5e5c79a7d3d3cdd79c2ff18359c9c2f0f695ab0b",
                "title": "Sparse Training Theory for Scalable and Efficient Agents",
                "abstract": "A fundamental task for artificial intelligence is learning. Deep Neural Networks have proven to cope perfectly with all learning paradigms, i.e. supervised, unsupervised, and reinforcement learning. Nevertheless, traditional deep learning approaches make use of cloud computing facilities and do not scale well to autonomous agents with low computational resources. Even in the cloud, they suffer from computational and memory limitations, and they cannot be used to model adequately large physical worlds for agents which assume networks with billions of neurons. These issues are addressed in the last few years by the emerging topic of sparse training, which trains sparse networks from scratch. This paper discusses sparse training state-of-the-art, its challenges and limitations while introducing a couple of new theoretical research directions which has the potential of alleviating sparse training limitations to push deep learning scalability well beyond its current boundaries. Nevertheless, the theoretical advancements impact in complex multi-agents settings is discussed from a real-world perspective, using the smart grid case study.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "146634864",
                        "name": "Elena Mocanu"
                    },
                    {
                        "authorId": "145786746",
                        "name": "T. Pinto"
                    },
                    {
                        "authorId": "39858495",
                        "name": "Selima Curci"
                    },
                    {
                        "authorId": "1392329556",
                        "name": "Phuong H. Nguyen"
                    },
                    {
                        "authorId": "1970654",
                        "name": "M. Gibescu"
                    },
                    {
                        "authorId": "1751167",
                        "name": "D. Ernst"
                    },
                    {
                        "authorId": "145579951",
                        "name": "Z. Vale"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Typical methods include, but are not limited to, network pruning (Lin et al., 2020c; Evci et al., 2020; Lin et al., 2020a), tensor decomposition (Jaderberg et al."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "331bbb8107b3f300f39ebeaeb263e8cd2fdc101e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-07981",
                    "ArXiv": "2102.07981",
                    "DOI": "10.1109/TPAMI.2022.3212615",
                    "CorpusId": 231934194,
                    "PubMed": "36215372"
                },
                "corpusId": 231934194,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/331bbb8107b3f300f39ebeaeb263e8cd2fdc101e",
                "title": "SiMaN: Sign-to-Magnitude Network Binarization",
                "abstract": "Binary neural networks (BNNs) have attracted broad research interest due to their efficient storage and computational ability. Nevertheless, a significant challenge of BNNs lies in handling discrete constraints while ensuring bit entropy maximization, which typically makes their weight optimization very difficult. Existing methods relax the learning using the sign function, which simply encodes positive weights into <inline-formula><tex-math notation=\"LaTeX\">$+1$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"ji-ieq1-3212615.gif\"/></alternatives></inline-formula>s, and <inline-formula><tex-math notation=\"LaTeX\">$-1$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"ji-ieq2-3212615.gif\"/></alternatives></inline-formula>s otherwise. Alternatively, we formulate an angle alignment objective to constrain the weight binarization to <inline-formula><tex-math notation=\"LaTeX\">$\\lbrace 0,+1\\rbrace$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"ji-ieq3-3212615.gif\"/></alternatives></inline-formula> to solve the challenge. In this article, we show that our weight binarization provides an analytical solution by encoding high-magnitude weights into <inline-formula><tex-math notation=\"LaTeX\">$+1$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"ji-ieq4-3212615.gif\"/></alternatives></inline-formula>s, and 0s otherwise. Therefore, a high-quality discrete solution is established in a computationally efficient manner without the sign function. We prove that the learned weights of binarized networks roughly follow a Laplacian distribution that does not allow entropy maximization, and further demonstrate that it can be effectively solved by simply removing the <inline-formula><tex-math notation=\"LaTeX\">$\\ell _{2}$</tex-math><alternatives><mml:math><mml:msub><mml:mi>\u2113</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href=\"ji-ieq5-3212615.gif\"/></alternatives></inline-formula> regularization during network training. Our method, dubbed sign-to-magnitude network binarization (SiMaN), is evaluated on CIFAR-10 and ImageNet, demonstrating its superiority over the sign-based state-of-the-arts. Our source code, experimental settings, training logs and binary models are available at <uri>https://github.com/lmbxmu/SiMaN</uri>.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49352079",
                        "name": "Mingbao Lin"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    },
                    {
                        "authorId": "48559591",
                        "name": "Zi-Han Xu"
                    },
                    {
                        "authorId": "1740430",
                        "name": "Baochang Zhang"
                    },
                    {
                        "authorId": "2070266415",
                        "name": "Fei Chao"
                    },
                    {
                        "authorId": "2285442",
                        "name": "Mingliang Xu"
                    },
                    {
                        "authorId": "46246806",
                        "name": "Chia-Wen Lin"
                    },
                    {
                        "authorId": "144082425",
                        "name": "Ling Shao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Additionally, we include comparisons to recent works on weight rewinding and dynamic sparsity, in particular SNIP (Lee et al., 2018), DSR (Mostafa and Wang, 2019), SNFS (Dettmers and Zettlemoyer, 2019), and RiGL (Evci et al., 2020).",
                "Evci et al. (2020) have shown promising results on NNs pruned at initialization where the pruning ratio across layers is adjusted by Erd\u0151s-R\u00e9nyi kernel method, as introduced by Mocanu et al. (2018).",
                ", 2018), DSR (Mostafa and Wang, 2019), SNFS (Dettmers and Zettlemoyer, 2019), and RiGL (Evci et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "08df7bc07135cc58c1f2614c5cd2cbb312915a18",
                "externalIds": {
                    "DBLP": "conf/aistats/IsikWN22",
                    "ArXiv": "2102.08329",
                    "CorpusId": 246705938
                },
                "corpusId": 246705938,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/08df7bc07135cc58c1f2614c5cd2cbb312915a18",
                "title": "An Information-Theoretic Justification for Model Pruning",
                "abstract": "We study the neural network (NN) compression problem, viewing the tension between the compression ratio and NN performance through the lens of rate-distortion theory. We choose a distortion metric that reflects the effect of NN compression on the model output and derive the tradeoff between rate (compression) and distortion. In addition to characterizing theoretical limits of NN compression, this formulation shows that \\emph{pruning}, implicitly or explicitly, must be a part of a good compression algorithm. This observation bridges a gap between parts of the literature pertaining to NN and data compression, respectively, providing insight into the empirical success of model pruning. Finally, we propose a novel pruning strategy derived from our information-theoretic formulation and show that it outperforms the relevant baselines on CIFAR-10 and ImageNet datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1707440322",
                        "name": "Berivan Isik"
                    },
                    {
                        "authorId": "4820756",
                        "name": "T. Weissman"
                    },
                    {
                        "authorId": "3268846",
                        "name": "Albert No"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[14] that requires expanding the model size), several researchers [4, 29, 30, 7, 9] tried to enable dynamic mask changes during the training process.",
                "Recently, a new line of research that aims to train sparse models from scratch [14, 7, 9] has emerged.",
                "[9] work, as it aims to solve a different issue within the same problem.",
                "[9] that aims to reduce memory footprint for sparse training from scratch, Zhou et al.",
                "While a different line of research suggested more extreme setting which restrict the memory footprint to the compressed model size [4, 29, 30, 7, 9], we argue that our method is orthogonal to it and both methods could be easily be combined."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "b4d207a2096aee4a3764933373eef6edb574c952",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-08124",
                    "ArXiv": "2102.08124",
                    "CorpusId": 231934142
                },
                "corpusId": 231934142,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b4d207a2096aee4a3764933373eef6edb574c952",
                "title": "Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N: M Transposable Masks",
                "abstract": "Unstructured pruning reduces the memory footprint in deep neural networks (DNNs). Recently, researchers proposed different types of structural pruning intending to reduce also the computation complexity. In this work, we first suggest a new measure called mask-diversity which correlates with the expected accuracy of the different types of structural pruning. We focus on the recently suggested N:M fine-grained block sparsity mask, in which for each block of M weights, we have at least N zeros. While N:M fine-grained block sparsity allows acceleration in actual modern hardware, it can be used only to accelerate the inference phase. In order to allow for similar accelerations in the training phase, we suggest a novel transposable fine-grained sparsity mask, where the same mask can be used for both forward and backward passes. Our transposable mask guarantees that both the weight matrix and its transpose follow the same sparsity pattern; thus, the matrix multiplication required for passing the error backward can also be accelerated. We formulate the problem of finding the optimal transposable-mask as a minimum-cost flow problem. Additionally, to speed up the minimum-cost flow computation, we also introduce a fast linear-time approximation that can be used when the masks dynamically change during training. Our experiments suggest a 2x speed-up in the matrix multiplications with no accuracy degradation over vision and language models. Finally, to solve the problem of switching between different structure constraints, we suggest a method to convert a pre-trained model with unstructured sparsity to an N:M fine-grained block sparsity model with little to no training. A reference implementation can be found at https://github.com/papers-submission/structured_transposable_masks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2477463",
                        "name": "Itay Hubara"
                    },
                    {
                        "authorId": "104387774",
                        "name": "Brian Chmiel"
                    },
                    {
                        "authorId": "2051020146",
                        "name": "Moshe Island"
                    },
                    {
                        "authorId": "2607278",
                        "name": "Ron Banner"
                    },
                    {
                        "authorId": "2051019587",
                        "name": "S. Naor"
                    },
                    {
                        "authorId": "1912398",
                        "name": "Daniel Soudry"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The results in Table 3 and Table 4 show that the proposed method outperform the baseline and previous methods [32, 16, 5]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "27f01c79aa141aee1cd2d5987ddbbf368bd7aecd",
                "externalIds": {
                    "ArXiv": "2102.06870",
                    "DBLP": "journals/corr/abs-2102-06870",
                    "CorpusId": 231924527
                },
                "corpusId": 231924527,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/27f01c79aa141aee1cd2d5987ddbbf368bd7aecd",
                "title": "Self-Reorganizing and Rejuvenating CNNs for Increasing Model Capacity Utilization",
                "abstract": "In this paper, we propose self-reorganizing and rejuvenating convolutional neural networks; a biologically inspired method for improving the computational resource utilization of neural networks. The proposed method utilizes the channel activations of a convolution layer in order to reorganize that layers parameters. The reorganized parameters are clustered to avoid parameter redundancies. As such, redundant neurons with similar activations are merged leaving room for the remaining parameters to rejuvenate. The rejuvenated parameters learn different features to supplement those learned by the reorganized surviving parameters. As a result, the network capacity utilization increases improving the baseline network performance without any changes to the network structure. The proposed method can be applied to various network architectures during the training stage, or applied to a pre-trained model improving its performance. Experimental results showed that the proposed method is model-agnostic and can be applied to any backbone architecture increasing its performance due to the elevated utilization of the network capacity.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2017906",
                        "name": "Wissam J. Baddar"
                    },
                    {
                        "authorId": null,
                        "name": "Seungju Han"
                    },
                    {
                        "authorId": "2059323344",
                        "name": "Seon-Min Rhee"
                    },
                    {
                        "authorId": "1764869",
                        "name": "Jae-Joon Han"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The current state-ofthe-art for DST is RigL (Evci et al., 2020), which maintains a fixed layer-wise sparsity distribution, prunes parameters with the smallest magnitude, and re-activates weights with the largest-magnitude gradients.",
                "In contrast, while RigL is also shown to be influenced by the layer-wise sparsity distribution (Evci et al., 2020), the premise of DST is precisely that the locations of the trainable parameters within each layer matter fundamentally.",
                "\u2026A.12 we compare different heuristics for distributing trainable parameters between network layers \u2013 in particular, uniform density per layer (uniform), equal number of parameters per layer (EPL), equal number of parameters per filter (EPF) and the ERK distribution used in (Evci et al., 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "0ee7dbb58be11c3583e0be47f6c5e672ffb6a819",
                "externalIds": {
                    "ArXiv": "2102.07655",
                    "DBLP": "conf/icml/PriceT21",
                    "CorpusId": 231924624
                },
                "corpusId": 231924624,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0ee7dbb58be11c3583e0be47f6c5e672ffb6a819",
                "title": "Dense for the Price of Sparse: Improved Performance of Sparsely Initialized Networks via a Subspace Offset",
                "abstract": "That neural networks may be pruned to high sparsities and retain high accuracy is well established. Recent research efforts focus on pruning immediately after initialization so as to allow the computational savings afforded by sparsity to extend to the training process. In this work, we introduce a new `DCT plus Sparse' layer architecture, which maintains information propagation and trainability even with as little as 0.01% trainable kernel parameters remaining. We show that standard training of networks built with these layers, and pruned at initialization, achieves state-of-the-art accuracy for extreme sparsities on a variety of benchmark network architectures and datasets. Moreover, these results are achieved using only simple heuristics to determine the locations of the trainable parameters in the network, and thus without having to initially store or compute with the full, unpruned network, as is required by competing prune-at-initialization algorithms. Switching from standard sparse layers to DCT plus Sparse layers does not increase the storage footprint of a network and incurs only a small additional computational overhead.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145756124",
                        "name": "Ilan Price"
                    },
                    {
                        "authorId": "144770610",
                        "name": "Jared Tanner"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In RigL (Evci et al., 2019a), the authors consider the case of SADt\u22121:t = 2k, where k is dynamically calculated for each layer during training.",
                "\u2026other is a one-stage scheme, which adopts the dynamic method to alternatively optimize parameters and prunes network architectures based on different criteria (Bellec et al., 2017; Mocanu et al., 2018; Mostafa & Wang, 2019; Evci et al., 2019b; Kusupati et al., 2020; Dettmers & Zettlemoyer, 2019).",
                "One is a two-stage scheme, which discovers a sparse neural architecture by pruning a well-trained dense network and then uses the same or even greater computational resources to retrain the sparse models (Nvidia, 2020; Evci et al., 2019b; Han et al., 2015; Frankle & Carbin, 2018).",
                "It is difficult to find the optimal sparse architecture (connections) and optimal parameters (Evci et al., 2019b) simultaneously during training sparse CNNs and Transformers although SET-MLP could easily outperform dense MLP (Bourgin et al., 2019).",
                "However, compared with training dense neural networks from scratch, to achieve the same performance, RigL needs 5\u00d7 more training time.",
                "RigL can achieve state-of-the-art results on training unsturctured sparse networks from scratch.",
                "RigL (Evci et al., 2019a) uses the magnitudebased method to prune and the periodic dense gradients to regrow connection.",
                "Before the advent of N :M fine-grained structured sparsity, there exist many state-of-the-art methods to generate sparsity models, including DSR (Mostafa & Wang, 2019), RigL (Evci et al., 2019a), GMP (Gale et al., 2019), and STR (Kusupati et al., 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "5e38dc1ccf33ac1df09b8eb6476f110cb3d1966f",
                "externalIds": {
                    "MAG": "3127067080",
                    "DBLP": "journals/corr/abs-2102-04010",
                    "ArXiv": "2102.04010",
                    "CorpusId": 231847094
                },
                "corpusId": 231847094,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5e38dc1ccf33ac1df09b8eb6476f110cb3d1966f",
                "title": "Learning N: M Fine-grained Structured Sparse Neural Networks From Scratch",
                "abstract": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and decent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing comprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9548994",
                        "name": "Aojun Zhou"
                    },
                    {
                        "authorId": "2109238929",
                        "name": "Yukun Ma"
                    },
                    {
                        "authorId": "24925751",
                        "name": "Junnan Zhu"
                    },
                    {
                        "authorId": "2124809722",
                        "name": "Jianbo Liu"
                    },
                    {
                        "authorId": "1490508571",
                        "name": "Zhijie Zhang"
                    },
                    {
                        "authorId": "50492964",
                        "name": "Kun Yuan"
                    },
                    {
                        "authorId": "8397576",
                        "name": "Wenxiu Sun"
                    },
                    {
                        "authorId": "47893312",
                        "name": "Hongsheng Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Evci et al. (2020b) found that sparse neural networks that are initialized by a dense initialization e.g., He et al. (2015), suffer from a poor gradient flow, whereas DST can improve the gradient flow during training significantly.",
                "\u2026redistribution (Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Liu et al., 2021), gradient-based weight growth (Dettmers & Zettlemoyer, 2019; Evci et al., 2020a), and extra weights update in the backward pass (Raihan & Aamodt, 2020; Jayakumar et al., 2020) to improve the sparse training\u2026",
                "\u2026expressibility of sparse training, especially for extreme sparsities, (2) in reducing training and inference costs (3) in understanding the underlying mechanism of dynamic sparse training (DST) (Mocanu et al., 2018; Evci et al., 2020a), (4) in preventing overfitting and improving generalization.",
                "It also helps to avoid the dense over-parameterization bias introduced by the gradient-based methods e.g., The Rigged Lottery (RigL) (Evci et al., 2020a) and Sparse Networks from Scratch (SNFS) (Dettmers & Zettlemoyer, 2019), as the latter utilize dense gradients in the backward pass to explore new\u2026",
                "B. Implementation Details of RigL-ITOP in Section 4.2\nIn this Appendix, we describe our replication of RigL (Evci et al., 2020a) and the hyperparameters we used for RigLITOP.",
                "We train sparse ResNet-50 for 100 epochs, the same as Dettmers & Zettlemoyer (2019); Evci et al. (2020a).",
                "More importantly, our method requires only 2\u00d7 training time to match the performance of dense ResNet-50 at 80% sparsity, far less than RigL (5\u00d7 training time) (Evci et al., 2020a)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "229a4d27d04bd3901ef0ca41942eb0cdd4f28eed",
                "externalIds": {
                    "MAG": "3171463583",
                    "DBLP": "journals/corr/abs-2102-02887",
                    "ArXiv": "2102.02887",
                    "CorpusId": 231839425
                },
                "corpusId": 231839425,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/229a4d27d04bd3901ef0ca41942eb0cdd4f28eed",
                "title": "Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training",
                "abstract": "In this paper, we introduce a new perspective on training deep neural networks capable of state-of-the-art performance without the need for the expensive over-parameterization by proposing the concept of In-Time Over-Parameterization (ITOP) in sparse training. By starting from a random sparse network and continuously exploring sparse connectivities during training, we can perform an Over-Parameterization in the space-time manifold, closing the gap in the expressibility between sparse training and dense training. We further use ITOP to understand the underlying mechanism of Dynamic Sparse Training (DST) and indicate that the benefits of DST come from its ability to consider across time all possible parameters when searching for the optimal sparse connectivity. As long as there are sufficient parameters that have been reliably explored during training, DST can outperform the dense neural network by a large margin. We present a series of experiments to support our conjecture and achieve the state-of-the-art sparse training performance with ResNet-50 on ImageNet. More impressively, our method achieves dominant performance over the overparameterization-based sparse methods at extreme sparsity levels. When trained on CIFAR-100, our method can match the performance of the dense model even at an extreme sparsity (98%). Code can be found this https URL.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "1410465360",
                        "name": "Lu Yin"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Sparse Network Optimization to study Network Dynamics Apart from being used as pruning criteria, optimization information has been used to investigate aspects of sparse networks, such as their loss landscape (Evci et al., 2019), how they are impacted by SGD noise (Frankle et al.",
                "\u2026from being used as pruning criteria, optimization information has been used to investigate aspects of sparse networks, such as their loss landscape (Evci et al., 2019), how they are impacted by SGD noise (Frankle et al., 2019a), the effect of different activation functions (Dubowski, 2020) and\u2026",
                "Pruning during Training Another branch of pruning is Dynamic Sparse Training, which uses information gathered during the training process, to dynamically update the sparsity pattern of these sparse networks (Mostafa & Wang, 2019; Bellec et al., 2017; Mocanu et al., 2018; Dettmers & Zettlemoyer, 2019; Evci et al., 2019).",
                "\u2026Another branch of pruning is Dynamic Sparse Training, which uses information gathered during the training process, to dynamically update the sparsity pattern of these sparse networks (Mostafa & Wang, 2019; Bellec et al., 2017; Mocanu et al., 2018; Dettmers & Zettlemoyer, 2019; Evci et al., 2019).",
                "However, while this work has had some success, focusing on initialization alone has proven to be inadequate (Frankle et al., 2020; Evci et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "b78d89fc34ef459f8d5e50af9a13686e14687719",
                "externalIds": {
                    "ArXiv": "2102.01670",
                    "DBLP": "journals/corr/abs-2102-01670",
                    "CorpusId": 231749949
                },
                "corpusId": 231749949,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b78d89fc34ef459f8d5e50af9a13686e14687719",
                "title": "Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network Optimization",
                "abstract": "Training sparse networks to converge to the same performance as dense neural architectures has proven to be elusive. Recent work suggests that initialization is the key. However, while this direction of research has had some success, focusing on initialization alone appears to be inadequate. In this paper, we take a broader view of training sparse networks and consider the role of regularization, optimization, and architecture choices on sparse models. We propose a simple experimental framework, Same Capacity Sparse vs Dense Comparison (SC-SDC), that allows for a fair comparison of sparse and dense networks. Furthermore, we propose a new measure of gradient flow, Effective Gradient Flow (EGF), that better correlates to performance in sparse networks. Using top-line metrics, SC-SDC and EGF, we show that default choices of optimizers, activation functions and regularizers used for dense networks can disadvantage sparse networks. Based upon these findings, we show that gradient flow in sparse networks can be improved by reconsidering aspects of the architecture design and the training regime. Our work suggests that initialization is only one piece of the puzzle and taking a wider view of tailoring optimization to sparse networks yields promising results.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2047584985",
                        "name": "Kale-ab Tessera"
                    },
                    {
                        "authorId": "50237813",
                        "name": "Sara Hooker"
                    },
                    {
                        "authorId": "2831294",
                        "name": "Benjamin Rosman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "More recently, RigL [18] introduced gradient-based regrowing to get rid of the extra computation and storage caused by SNFS.",
                "This observation contradicts the common belief that gradient-based weight regrowth achieves better performance than random-based regrowth [15, 18].",
                "More recently, RigL [18] introduced gradient-based regrowing to get rid of the extra computa-",
                "We compare ST-RNNs with strong state-of-the-art DST baselines including SET, SNFS, and RigL and a dense-to-sparse method, ISS.",
                "Only very recently, dynamic sparse training (DST) was begun to be studied to enable training sparse neural networks from scratch, with a few approaches including Sparse Evolutionary Training (SET) [47], Dynamic Sparse Reparameterization (DSR) [49], Sparse Networks from Scratch (SNFS) [15], Rigged Lottery (RigL) [18].",
                "We can see that, with 33% parameters, all gradient-based methods (SNFS and RigL) fail to match the performance of the dense-to-sparse method (ISS), whereas random-based methods (SET, ST-LSTM) can all outperform ISS and the dense model.",
                "Different from other neural networks, RNNs are relatively more challenging to be compressed [18, 57].",
                "This is the main difference between ST-RNNs with gradient-based sparse training techniques such as RigL and SNFS. Gradient-based regrowing heavily depends on the gradient of every parameter and they still require a dense forward pass at least once per D T iterations, whereas our method keeps a clearly sparse backward pass and requires smaller FLOPs."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "background"
            ],
            "citingPaper": {
                "paperId": "a2740e2bdbc3113c54e9056a4e7b107c21baf31b",
                "externalIds": {
                    "MAG": "3121852482",
                    "DBLP": "journals/nca/LiuNMMP21",
                    "DOI": "10.1007/s00521-021-05727-y",
                    "CorpusId": 234024174
                },
                "corpusId": 234024174,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a2740e2bdbc3113c54e9056a4e7b107c21baf31b",
                "title": "Efficient and effective training of sparse recurrent neural networks",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "1404570136",
                        "name": "Iftitahu Ni'mah"
                    },
                    {
                        "authorId": "2266428",
                        "name": "Vlado Menkovski"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, previous approaches are mainly for CNNs and MLPs.",
                "Note that its variant Erdo\u030bs-Re\u0301nyi-Kernel proposed by Evci et al. (2020) scales back to ER for RNNs, as no kernels are involved.",
                "For instance, while \u201cThe Rigged Lottery\u201d (RigL) achieves state-of-the-art sparse training results with various CNNs, it fails to match the performance of the iterative pruning method (Gale et al., 2019) in the RNN setting (Evci et al., 2020).",
                "iterative pruning method in the RNN setting (Evci et al., 2020).",
                "Methods that leverage gradient-based weight growth (SNFS and RigL) have shown superiority on performance over the methods using random-based weight growth for CNNs (Evci et al., 2020).",
                "It has been shown by Evci et al. (2020) that while state-ofthe-art sparse training method (RigL) achieves promising performance with various CNN models, it fails to match the performance of pruning in RNNs.",
                "Recently, some dynamic sparse training (DST) approaches (Mocanu et al., 2018; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020) have been proposed to bring efficiency to the training phase as well.",
                "However, all these works mainly focus on CNNs and MLPs, and they are not designed to match state-of-the-art performance for RNNs.",
                "It has been shown that the choice of sparse initialization (sparsity distribution) is important for sparse training in Frankle & Carbin (2019); Kusupati et al. (2020); Evci et al. (2020).",
                "Later, some works focus on designing sparse CNNs based on Expander graphs and show comparable performance against the corresponding dense models (Prabhu et al., 2018; Kepner & Robinett, 2019).",
                "The long-term dependencies and repetitive usage of recurrent cells make RNNs more difficult to be sparsified (Kalchbrenner et al., 2018; Evci et al., 2020).",
                "In line with the previous studies (Mocanu et al., 2018; Mostafa & Wang, 2019; Evci et al., 2020), both static sparse networks and small-dense networks fall short of Selfish-RNN.",
                "We follow the way of calculating training FLOPs proposed by Evci et al. (2020).",
                "In line with the previous studies (Mocanu et al., 2018; Mostafa & Wang, 2019; Evci et al., 2020), both static sparse networks and small dense networks fall short of Selfish-RNN.",
                "\u2022 Our analysis shows two surprising phenomena in the setting of RNNs contrary to CNNs (1) random-based weight growth performs better than gradient-based weight growth, and (2) uniform sparse distribution performs better than Erdo\u030bs-Re\u0301nyi (ER) sparse distribution.",
                "However, previous sparse-to-sparse methods mainly focus on Multilayer Perceptron Networks (MLPs) and Convolutional Neural Networks (CNNs), failing to match the performance of dense-to-sparse methods in the Recurrent Neural Networks (RNNs) setting.",
                "RigL (Evci et al., 2020) went one step further by activating new weights with the highest magnitude gradient."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4df2175c0daadf630623a505f623fe41a386853d",
                "externalIds": {
                    "ArXiv": "2101.09048",
                    "DBLP": "conf/icml/LiuMPP21",
                    "CorpusId": 231693152
                },
                "corpusId": 231693152,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4df2175c0daadf630623a505f623fe41a386853d",
                "title": "Selfish Sparse RNN Training",
                "abstract": "Sparse neural networks have been widely applied to reduce the computational demands of training and deploying over-parameterized deep neural networks. For inference acceleration, methods that discover a sparse network from a pre-trained dense network (dense-to-sparse training) work effectively. Recently, dynamic sparse training (DST) has been proposed to train sparse neural networks without pre-training a dense model (sparse-to-sparse training), so that the training process can also be accelerated. However, previous sparse-to-sparse methods mainly focus on Multilayer Perceptron Networks (MLPs) and Convolutional Neural Networks (CNNs), failing to match the performance of dense-to-sparse methods in the Recurrent Neural Networks (RNNs) setting. In this paper, we propose an approach to train intrinsically sparse RNNs with a fixed parameter count in one single run, without compromising performance. During training, we allow RNN layers to have a non-uniform redistribution across cell gates for better regularization. Further, we propose SNT-ASGD, a novel variant of the averaged stochastic gradient optimizer, which significantly improves the performance of all sparse training methods for RNNs. Using these strategies, we achieve state-of-the-art sparse training results, better than the dense-to-sparse methods, with various types of RNNs on Penn TreeBank and Wikitext-2 datasets. Our codes are available at https://github.com/Shiweiliuiiiiiii/Selfish-RNN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "1382535564",
                        "name": "Yulong Pei"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ff6e01975994fe68c8222dc65b568b422d7664e9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-08837",
                    "ArXiv": "2101.08837",
                    "DOI": "10.1109/ISIT45174.2021.9518221",
                    "CorpusId": 231693232
                },
                "corpusId": 231693232,
                "publicationVenue": {
                    "id": "234ccdc0-f58f-4f94-b86a-428d11a0c5ad",
                    "name": "International Symposium on Information Theory",
                    "type": "conference",
                    "alternate_names": [
                        "International Symposium on Information Technology",
                        "Int Symp Inf Theory",
                        "Int Symp Inf Technol",
                        "ISIT"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1719"
                },
                "url": "https://www.semanticscholar.org/paper/ff6e01975994fe68c8222dc65b568b422d7664e9",
                "title": "Time-Correlated Sparsification for Communication-Efficient Federated Learning",
                "abstract": "Federated learning (FL) enables multiple clients to collaboratively train a shared model, with the help of a parameter server (PS), without disclosing their local datasets. However, due to the increasing size of the trained models, the communication load due to the iterative exchanges between the clients and the PS often becomes a bottleneck in the performance. Sparse communication is often employed to reduce the communication load, where only a small subset of the model updates are communicated from the clients to the PS. In this paper, we introduce a novel time-correlated sparsification (TCS) scheme, which builds upon the notion that sparse communication framework can be considered as identifying the most significant elements of the underlying model. Hence, TCS exploits the correlation between the sparse representations at consecutive iterations in FL, so that the overhead due to encoding of the sparse representation can be significantly reduced without compromising the test accuracy. Through extensive simulations on the CIFAR-10 dataset, we show that TCS can achieve centralized training accuracy with 100 times sparsification, and up to 2000 times reduction in the communication load when employed with quantization.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "120677219",
                        "name": "Emre Ozfatura"
                    },
                    {
                        "authorId": "2008874292",
                        "name": "Kerem Ozfatura"
                    },
                    {
                        "authorId": "1727814",
                        "name": "Deniz G\u00fcnd\u00fcz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is because current sparse training algorithms typically use a fixed sparse network architecture or a fixed sparsity pattern for a number of iterations [9, 10]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "bb3367b067ef2cca2f7954bb0326e8966cc8895d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-07948",
                    "ArXiv": "2101.07948",
                    "CorpusId": 231648305
                },
                "corpusId": 231648305,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bb3367b067ef2cca2f7954bb0326e8966cc8895d",
                "title": "SparseDNN: Fast Sparse Deep Learning Inference on CPUs",
                "abstract": "The last few years have seen gigantic leaps in algorithms and systems to support efficient deep learning inference. Pruning and quantization algorithms can now consistently compress neural networks by an order of magnitude. For a compressed neural network, a multitude of inference frameworks have been designed to maximize the performance of the target hardware. While we find mature support for quantized neural networks in production frameworks such as OpenVINO and MNN, support for pruned sparse neural networks is still lacking. To tackle this challenge, we present SparseDNN, a sparse deep learning inference engine targeting CPUs. We present both kernel-level optimizations with a sparse code generator to accelerate sparse operators and novel network-level optimizations catering to sparse networks. We show that our sparse code generator can achieve significant speedups over state-of-the-art sparse and dense libraries. On end-to-end benchmarks such as Huggingface pruneBERT, SparseDNN achieves up to 5x throughput improvement over dense inference with state-of-the-art OpenVINO. Open source library at: https://github.com/marsupialtail/sparsednn.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2860279",
                        "name": "Ziheng Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "L G\n] 2\nF eb\n2 02\n1\nMostafa & Wang, 2019b; Evci et al., 2019; Anonymous, 2021a; Jayakumar et al., 2020).",
                "Very recently, based on the Lottery Ticket Hypothesis (Frankle & Carbin (2019)), RigL (Evci et al. (2019); Jayakumar et al. (2020)) was introduced as a novel method for training sparse models without the need of a \"lucky initialisation\"; it can match and sometimes exceed the performance of pruning\u2026",
                "Very recently, based on the Lottery Ticket Hypothesis (Frankle & Carbin (2019)), RigL (Evci et al. (2019); Jayakumar et al. (2020)) was introduced as a novel method for training sparse models without the need of a \"lucky initialisation\"; it can match and sometimes exceed the performance of pruning based approaches.",
                "Currently, the sparse training concept has started to be a de facto approach for e icient training of ANNs, as demonstrated in (Bellec et al., 2018; Dettmers & Zettlemoyer, 2019; Mostafa & Wang, 2019b; Evci et al., 2019; Anonymous, 2021a; Jayakumar et al., 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6723b469b5e1dd676f93ce92efcd2b1547a1eab4",
                "externalIds": {
                    "ArXiv": "2102.01732",
                    "DBLP": "journals/corr/abs-2102-01732",
                    "MAG": "3128124761",
                    "DOI": "10.21203/RS.3.RS-133395/V1",
                    "CorpusId": 231786414
                },
                "corpusId": 231786414,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6723b469b5e1dd676f93ce92efcd2b1547a1eab4",
                "title": "Truly Sparse Neural Networks at Scale",
                "abstract": "\n Recently, sparse training methods have started to be established as a de facto approach for training and inference efficiency in artificial neural networks. Yet, this efficiency is just in theory. In practice, everyone uses a binary mask to simulate sparsity since the typical deep learning software and hardware are optimized for dense matrix operations. In this paper, we take an orthogonal approach, and we show that we can train truly sparse neural networks to harvest their full potential. To achieve this goal, we introduce three novel contributions, specially designed for sparse neural networks: (1) a parallel training algorithm and its corresponding sparse implementation from scratch, (2) an activation function with non-trainable parameters to favour the gradient flow, and (3) a hidden neurons importance metric to eliminate redundancies. All in one, we are able to break the record and to train the largest neural network ever trained in terms of representational power -- reaching the bat brain size. The results show that our approach has state-of-the-art performance while opening the path for an environmentally friendly artificial intelligence era.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "39858495",
                        "name": "Selima Curci"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4b14cc34743c2f5eab3e0d53e2a0f41beb08c285",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-02338",
                    "ArXiv": "2101.02338",
                    "CorpusId": 230799602
                },
                "corpusId": 230799602,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4b14cc34743c2f5eab3e0d53e2a0f41beb08c285",
                "title": "Max-Affine Spline Insights Into Deep Network Pruning",
                "abstract": "In this paper, we study the importance of pruning in Deep Networks (DNs) and the yin&yang relationship between (1) pruning highly overparametrized DNs that have been trained from random initialization and (2) training small DNs that have been\"cleverly\"initialized. As in most cases practitioners can only resort to random initialization, there is a strong need to develop a grounded understanding of DN pruning. Current literature remains largely empirical, lacking a theoretical understanding of how pruning affects DNs' decision boundary, how to interpret pruning, and how to design corresponding principled pruning techniques. To tackle those questions, we propose to employ recent advances in the theoretical analysis of Continuous Piecewise Affine (CPA) DNs. From this perspective, we will be able to detect the early-bird (EB) ticket phenomenon, provide interpretability into current pruning techniques, and develop a principled pruning strategy. In each step of our study, we conduct extensive experiments supporting our claims and results; while our main goal is to enhance the current understanding towards DN pruning instead of developing a new pruning method, our spline pruning criteria in terms of layerwise and global pruning is on par with or even outperforms state-of-the-art pruning methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3201463",
                        "name": "Randall Balestriero"
                    },
                    {
                        "authorId": "47113848",
                        "name": "Haoran You"
                    },
                    {
                        "authorId": "9505105",
                        "name": "Zhihan Lu"
                    },
                    {
                        "authorId": "152856324",
                        "name": "Yutong Kou"
                    },
                    {
                        "authorId": "3138925",
                        "name": "Yingyan Lin"
                    },
                    {
                        "authorId": "144908066",
                        "name": "Richard Baraniuk"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Sparsification can be considered a growing trend in the training of neural networks [16, 17]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "97d3ff2055ac0fd8b829777309d991f2503d1d9d",
                "externalIds": {
                    "DOI": "10.3103/S1060992X21010033",
                    "CorpusId": 255449963
                },
                "corpusId": 255449963,
                "publicationVenue": {
                    "id": "9a2753b1-2799-4186-9139-8ec4a61170ca",
                    "name": "Optical Memory and Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Opt Mem Neural Netw"
                    ],
                    "issn": "1060-992X",
                    "url": "https://rd.springer.com/journal/12005"
                },
                "url": "https://www.semanticscholar.org/paper/97d3ff2055ac0fd8b829777309d991f2503d1d9d",
                "title": "Encoding and Decoding of Recursive Structures in Neural-Symbolic Systems",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9579304",
                        "name": "A. Demidovskij"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Evci et al. (2020) derived an algorithm for training sparse neural networks according to LTH and applied it to character-level language modeling on WikiText-103."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0c9d97d2ba489256d4f1760598dc2c7be6d90d96",
                "externalIds": {
                    "ACL": "2021.acl-long.171",
                    "DBLP": "journals/corr/abs-2101-00063",
                    "ArXiv": "2101.00063",
                    "DOI": "10.18653/v1/2021.acl-long.171",
                    "CorpusId": 230438816
                },
                "corpusId": 230438816,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/0c9d97d2ba489256d4f1760598dc2c7be6d90d96",
                "title": "EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets",
                "abstract": "Heavily overparameterized language models such as BERT, XLNet and T5 have achieved impressive success in many NLP tasks. However, their high model complexity requires enormous computation resources and extremely long training time for both pre-training and fine-tuning. Many works have studied model compression on large NLP models, but only focusing on reducing inference time while still requiring an expensive training process. Other works use extremely large batch sizes to shorten the pre-training time, at the expense of higher computational resource demands. In this paper, inspired by the Early-Bird Lottery Tickets recently studied for computer vision tasks, we propose EarlyBERT, a general computationally-efficient training algorithm applicable to both pre-training and fine-tuning of large-scale language models. By slimming the self-attention and fully-connected sub-layers inside a transformer, we are the first to identify structured winning tickets in the early stage of BERT training. We apply those tickets towards efficient BERT training, and conduct comprehensive pre-training and fine-tuning experiments on GLUE and SQuAD downstream tasks. Our results show that EarlyBERT achieves comparable performance to standard BERT, with 35 45% less training time. Code is available at https://github.com/VITA-Group/EarlyBERT.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "66273769",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "153655416",
                        "name": "Yu Cheng"
                    },
                    {
                        "authorId": "2992833",
                        "name": "Shuohang Wang"
                    },
                    {
                        "authorId": "144702900",
                        "name": "Zhe Gan"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "46700348",
                        "name": "Jingjing Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To address this research gap, we turn our attention to lottery ticket hypothesis (LTH) [20, 27, 31, 50, 76, 81], a fast-rising field that investigates the sparse trainable subnetworks within full dense networks."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "5f6fccc32953f57fe29b2316eb8351e84b0179dc",
                "externalIds": {
                    "MAG": "3111921445",
                    "DBLP": "journals/corr/abs-2012-06908",
                    "ArXiv": "2012.06908",
                    "DOI": "10.1109/CVPR46437.2021.01604",
                    "CorpusId": 229152261
                },
                "corpusId": 229152261,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5f6fccc32953f57fe29b2316eb8351e84b0179dc",
                "title": "The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models",
                "abstract": "The computer vision world has been re-gaining enthusiasm in various pre-trained models, including both classical ImageNet supervised pre-training and recently emerged self-supervised pre-training such as simCLR [10] and MoCo [40]. Pre-trained weights often boost a wide range of downstream tasks including classification, detection, and segmentation. Latest studies suggest that pre-training benefits from gigantic model capacity [11]. We are hereby curious and ask: after pre-training, does a pre-trained model indeed have to stay large for its downstream transferability? In this paper, we examine supervised and self-supervised pre-trained models through the lens of the lottery ticket hypothesis (LTH) [31]. LTH identifies highly sparse matching subnetworks that can be trained in isolation from (nearly) scratch yet still reach the full models' performance. We extend the scope of LTH and question whether matching subnetworks still exist in pre-trained computer vision models, that enjoy the same downstream transfer performance. Our extensive experiments convey an overall positive message: from all pre-trained weights obtained by ImageNet classification, simCLR, and MoCo, we are consistently able to locate such matching subnetworks at 59.04% to 96.48% sparsity that transfer universally to multiple downstream tasks, whose performance see no degradation compared to using full pre-trained weights. Further analyses reveal that subnetworks found from different pre-training tend to yield diverse mask structures and perturbation sensitivities. We conclude that the core LTH observations remain generally relevant in the pre-training paradigm of computer vision, but more delicate discussions are needed in some cases. Codes and pre-trained models will be made available at: https://github.com/VITA-Group/CV_LTH_Pre-training.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2648459",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "3307026",
                        "name": "Shiyu Chang"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "37873860",
                        "name": "Yang Zhang"
                    },
                    {
                        "authorId": "1701041",
                        "name": "Michael Carbin"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Previous works have primarily focused on efficient inference [35, 20, 24] and some on training costs [9]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9f6e03da33cf1a070826658eeddc90d676aaee8f",
                "externalIds": {
                    "MAG": "3110517527",
                    "CorpusId": 227261667
                },
                "corpusId": 227261667,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9f6e03da33cf1a070826658eeddc90d676aaee8f",
                "title": "Are We Overfitting to Experimental Setups in Recognition",
                "abstract": "Enabling robust intelligence in the real-world entails systems that offer continuous inference while learning from varying amounts of data and supervision. The machine learning community has organically broken down this challenging goal into manageable sub-tasks such as supervised, few-shot, and continual learning. In light of substantial progress on each sub-task, we pose the question, \"How well does this progress translate to more practical scenarios?\" To investigate this question, we construct a new framework, FLUID, which removes certain assumptions made by current experimental setups while integrating these sub-tasks via the following design choices -- consuming sequential data, allowing for flexible training phases, being compute aware, and working in an open-world setting. Evaluating a broad set of methods on FLUID leads to new insights including strong evidence that methods are overfitting to their experimental setup. For example, we find that representative few-shot methods are substantially worse than simple baselines, self-supervised representations from MoCo fail to learn new classes when the downstream task contains a mix of new and old classes, and pretraining largely mitigates the problem of catastrophic forgetting. Finally, we propose two new simple methods which outperform all other evaluated methods which further questions our progress towards robust, real-world systems. Project page: https://raivn.cs.washington.edu/projects/FLUID/.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1632957174",
                        "name": "Matthew Wallingford"
                    },
                    {
                        "authorId": "52207562",
                        "name": "Aditya Kusupati"
                    },
                    {
                        "authorId": "1796266802",
                        "name": "Keivan Alizadeh-Vahid"
                    },
                    {
                        "authorId": "2419744",
                        "name": "Aaron Walsman"
                    },
                    {
                        "authorId": "2684226",
                        "name": "Aniruddha Kembhavi"
                    },
                    {
                        "authorId": "143787583",
                        "name": "Ali Farhadi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A couple of methods have been proposed for training deep neural networks from scratch using sparse connections and sparse training [14, 41, 7, 42, 17, 55]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "adae9974ba53d20d4a1499d18dedcd9ef8996cee",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-00560",
                    "ArXiv": "2012.00560",
                    "MAG": "3107308507",
                    "DOI": "10.1007/s10994-021-06063-x",
                    "CorpusId": 227239047
                },
                "corpusId": 227239047,
                "publicationVenue": {
                    "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
                    "name": "Machine-mediated learning",
                    "type": "journal",
                    "alternate_names": [
                        "Mach learn",
                        "Machine Learning",
                        "Mach Learn"
                    ],
                    "issn": "0732-6718",
                    "alternate_issns": [
                        "0885-6125"
                    ],
                    "url": "http://www.springer.com/computer/artificial/journal/10994",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10994",
                        "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/adae9974ba53d20d4a1499d18dedcd9ef8996cee",
                "title": "Quick and robust feature selection: the strength of energy-efficient sparse training for autoencoders",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1768130522",
                        "name": "Zahra Atashgahi"
                    },
                    {
                        "authorId": "67102129",
                        "name": "Ghada Sokar"
                    },
                    {
                        "authorId": "30954888",
                        "name": "T. Lee"
                    },
                    {
                        "authorId": "146634864",
                        "name": "Elena Mocanu"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "39128850",
                        "name": "R. Veldhuis"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "RigL [3] maintains sparsity throughout training and does not require dense gradients during most iterations of training.",
                "In addition to IMP we analyze two dynamic sparsity methods: Discovering Neural Wirings (DNW) [19] and Rigged Lottery Tickets (RigL) [3], algorithms where the connectivity changes throughout training.",
                "In comparison to newer dynamic training algorithms including RigL and Top-KAST [3, 11], our modified DNW algorithm is likely still less computationally efficient for most applications.",
                "As in [2, 3], DNW [19] maintains sparsity throughout training.",
                "We also consider Lottery Ticket (LT) [5] graphs, DNW [19] graphs, and RigL graphs [3] which are respectively produced by the following three algorithms:",
                "Though earlier work on sparse networks focused primarily on pruning after training, researchers have recently shown interest in pruning early in training [5, 14, 17, 18] or dynamically as training progresses [2, 19, 3]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "1536206f88a2c0d2bd2c1b1b87e805232e3358aa",
                "externalIds": {
                    "ArXiv": "2012.00172",
                    "DBLP": "journals/corr/abs-2012-00172",
                    "MAG": "3110335490",
                    "CorpusId": 227238715
                },
                "corpusId": 227238715,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1536206f88a2c0d2bd2c1b1b87e805232e3358aa",
                "title": "Deconstructing the Structure of Sparse Neural Networks",
                "abstract": "Although sparse neural networks have been studied extensively, the focus has been primarily on accuracy. In this work, we focus instead on network structure, and analyze three popular algorithms. We first measure performance when structure persists and weights are reset to a different random initialization, thereby extending experiments in Deconstructing Lottery Tickets (Zhou et al., 2019). This experiment reveals that accuracy can be derived from structure alone. Second, to measure structural robustness we investigate the sensitivity of sparse neural networks to further pruning after training, finding a stark contrast between algorithms. Finally, for a recent dynamic sparsity algorithm we investigate how early in training the structure emerges. We find that even after one epoch the structure is mostly determined, allowing us to propose a more efficient algorithm which does not require dense gradients throughout training. In looking back at algorithms for sparse neural networks and analyzing their performance from a different lens, we uncover several interesting properties and promising directions for future research.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2029877602",
                        "name": "M. V. Gelder"
                    },
                    {
                        "authorId": "52193502",
                        "name": "Mitchell Wortsman"
                    },
                    {
                        "authorId": "2883417",
                        "name": "Kiana Ehsani"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e4ca4f0cf2d8dc5327c04e193064297fd63f1f60",
                "externalIds": {
                    "DBLP": "journals/jimaging/TessierGLAHB22",
                    "PubMedCentral": "8950981",
                    "ArXiv": "2011.10520",
                    "DOI": "10.3390/jimaging8030064",
                    "CorpusId": 247303139,
                    "PubMed": "35324619"
                },
                "corpusId": 247303139,
                "publicationVenue": {
                    "id": "c0fc53c7-b0ed-487d-9191-1262c8322621",
                    "name": "Journal of Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "J Imaging"
                    ],
                    "issn": "2313-433X",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-556372",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/jimaging",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-556372"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e4ca4f0cf2d8dc5327c04e193064297fd63f1f60",
                "title": "Rethinking Weight Decay for Efficient Neural Network Pruning",
                "abstract": "Introduced in the late 1980s for generalization purposes, pruning has now become a staple for compressing deep neural networks. Despite many innovations in recent decades, pruning approaches still face core issues that hinder their performance or scalability. Drawing inspiration from early work in the field, and especially the use of weight decay to achieve sparsity, we introduce Selective Weight Decay (SWD), which carries out efficient, continuous pruning throughout training. Our approach, theoretically grounded on Lagrangian smoothing, is versatile and can be applied to multiple tasks, networks, and pruning structures. We show that SWD compares favorably to state-of-the-art approaches, in terms of performance-to-parameters ratio, on the CIFAR-10, Cora, and ImageNet ILSVRC2012 datasets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "146940144",
                        "name": "Hugo Tessier"
                    },
                    {
                        "authorId": "144916029",
                        "name": "Vincent Gripon"
                    },
                    {
                        "authorId": "27532368",
                        "name": "Mathieu L\u00e9onardon"
                    },
                    {
                        "authorId": "2409852",
                        "name": "M. Arzel"
                    },
                    {
                        "authorId": "3312711",
                        "name": "T. Hannagan"
                    },
                    {
                        "authorId": "2060222860",
                        "name": "David Bertrand"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[42] and then further explored within the literature [11, 13, 44], sparse training consists in training the network with a constant level of sparsity, at first spread randomly with uniform probability and then adjusted during steps which combine 1) pruning of a certain portion of the weights, according to a certain criterion, and 2) regrowing an equivalent amount of weights, depending on another criterion."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "39f70444fb6654f7c7d106d9f8f2c453cf7f8e6a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-10520",
                    "MAG": "3109541635",
                    "CorpusId": 227119097
                },
                "corpusId": 227119097,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/39f70444fb6654f7c7d106d9f8f2c453cf7f8e6a",
                "title": "Continuous Pruning of Deep Convolutional Networks Using Selective Weight Decay",
                "abstract": "During the last decade, deep convolutional networks have become the reference for many machine learning tasks, especially in computer vision. However, large computational needs make them hard to deploy on resource-constrained hardware. Pruning has emerged as a standard way to compress such large networks. Yet, the severe perturbation caused by most pruning approaches is thought to hinder their efficacy. Drawing inspiration from Lagrangian Smoothing, we introduce a new technique, Selective Weight Decay (SWD), which achieves continuous pruning throughout training. Our approach deviates significantly from most methods of the literature as it relies on a principle that can be applied in many different ways, for any problem, network or pruning structure. We show that SWD compares favorably to other approaches in terms of performance/parameters ratio on the CIFAR-10 and ImageNet ILSVRC2012 datasets. On CIFAR-10 and unstructured pruning, with a parameters target of 0.1%, SWD attains a Top-1 accuracy of 81.32% while the reference method only reaches 27.78%. On CIFAR-10 and structured pruning, with a parameters target of 2.5%, the reference technique drops at 10% (random guess) while SWD maintains the Top-1 accuracy at 93.22%. On the ImageNet ILSVRC2012 dataset with unstructured pruning, for a parameters targer of 2.5%, SWD attains 84.6% Top-5 accuracy instead of the 77.07% reached by the reference.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "146940144",
                        "name": "Hugo Tessier"
                    },
                    {
                        "authorId": "144916029",
                        "name": "Vincent Gripon"
                    },
                    {
                        "authorId": "65982563",
                        "name": "Mathieu L'eonardon"
                    },
                    {
                        "authorId": "2409852",
                        "name": "M. Arzel"
                    },
                    {
                        "authorId": "3312711",
                        "name": "T. Hannagan"
                    },
                    {
                        "authorId": "2060222860",
                        "name": "David Bertrand"
                    }
                ]
            }
        },
        {
            "contexts": [
                "el to each layer in the network by pruning the weights with smallest magnitude in each layer [5]. The Erdosh-Renyi Kernel (ERK) baseline corresponds to a budgeted layer-wise pruning suggested in RigL [13]. Each layer\u2019s number of pruned weights is proportional to p= c o +c i +k h +k w c o c i k h k w ; (17) where c is the number of output dimensions, c i is the number of input dimensions, and k h;k w a",
                " levels of sparsity as training progresses. Several other works present magnitude-based pruning methods [18,47,19,16, 3,39]. Alternatives to magnitude-based pruning involve using gradient information [13,30,21,10,8], covariance in1 arXiv:2011.09058v1 [cs.CV] 18 Nov 2020 Layerwise Generator Layerwise Optimizer AFCLE Teacher Layer 1 Layer i \u00c9 Layer 2 \u00c9 AFCLE Student Layer 1 Layer i \u00c9 Layer 2 \u00c9 Batch-norm Stats Inp",
                "cient CNNs for on-device execution. Examples include smart-home security, factory automation, and mobile applications. One common technique for improving CNN computational ef\ufb01ciency is weight pruning [29,47,18,13]. The removal of network weights allows the network to occupy a smaller memory footprint and achieve a faster execution time. Another common technique for improving a CNN\u2019s runtime ef\ufb01ciency is quanti",
                "ittle or no loss of accuracy [27,38,26]. Most methods for CNN compression require retraining on the original training set to achieve a high compression rate. For example, compressing through sparsity [29,47,18,13] requires training on the original data. Applying post-training quantization usually results in poor network accuracy [26] unless special care is taken in adjusting network weights [38]. Handling low-",
                "[ !&quot;&amp;#&apos;  %% !&amp;#$ ! &amp; !# # 5]): a global magnitude threshold is used to prune weights. (Uniform [5]): a uniform layer-wise sparsity budget is used. (ERK [13]): a layer-wise sparsity budget weighted to prune more weights from larger convolutional layers, as described in Equation17.    !# Soft Threshold Reparameterization with data, as reported in [ "
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "404bebddcdd449301e66bf8d130555f6640ca7fb",
                "externalIds": {
                    "DBLP": "conf/icpr/HortonJFR22",
                    "ArXiv": "2011.09058",
                    "MAG": "3106178055",
                    "DOI": "10.1109/ICPR56361.2022.9956237",
                    "CorpusId": 227011888
                },
                "corpusId": 227011888,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/404bebddcdd449301e66bf8d130555f6640ca7fb",
                "title": "Layer-Wise Data-Free CNN Compression",
                "abstract": "We present a computationally efficient method for compressing a trained neural network without using real data. We break the problem of data-free network compression into independent layer-wise compressions. We show how to efficiently generate layer-wise training data using only a pretrained network. We use this data to perform independent layer-wise compressions on the pretrained network. We also show how to precondition the network to improve the accuracy of our layer-wise compression method. We present results for layer-wise compression using quantization and pruning. When quantizing, we compress with higher accuracy than related works while using orders of magnitude less compute. When compressing MobileNetV2 and evaluating on ImageNet, our method outperforms existing methods for quantization at all bit-widths, achieving a +0.34% improvement in 8-bit quantization, and a stronger improvement at lower bit-widths (up to a +28.50% improvement at 5 bits). When pruning, we outperform baselines of a similar compute envelope, achieving 1.5 times the sparsity rate at the same accuracy. We also show how to combine our efficient method with high-compute generative methods to improve upon their results.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "46211341",
                        "name": "Maxwell Horton"
                    },
                    {
                        "authorId": "30750450",
                        "name": "Yanzi Jin"
                    },
                    {
                        "authorId": "143787583",
                        "name": "Ali Farhadi"
                    },
                    {
                        "authorId": "32371083",
                        "name": "Mohammad Rastegari"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0fa13a5ef36168ff3fd08b03fd30f1f935d6a18a",
                "externalIds": {
                    "ArXiv": "2011.00241",
                    "DBLP": "journals/access/VaderaA22",
                    "MAG": "3096215947",
                    "DOI": "10.1109/ACCESS.2022.3182659",
                    "CorpusId": 226226764
                },
                "corpusId": 226226764,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0fa13a5ef36168ff3fd08b03fd30f1f935d6a18a",
                "title": "Methods for Pruning Deep Neural Networks",
                "abstract": "This paper presents a survey of methods for pruning deep neural networks. It begins by categorising over 150 studies based on the underlying approach used and then focuses on three categories: methods that use magnitude based pruning, methods that utilise clustering to identify redundancy, and methods that use sensitivity analysis to assess the effect of pruning. Some of the key influencing studies within these categories are presented to highlight the underlying approaches and results achieved. Most studies present results which are distributed in the literature as new architectures, algorithms and data sets have developed with time, making comparison across different studied difficult. The paper therefore provides a resource for the community that can be used to quickly compare the results from many different methods on a variety of data sets, and a range of architectures, including AlexNet, ResNet, DenseNet and VGG. The resource is illustrated by comparing the results published for pruning AlexNet and ResNet50 on ImageNet and ResNet56 and VGG16 on the CIFAR10 data to reveal which pruning methods work well in terms of retaining accuracy whilst achieving good compression rates. The paper concludes by identifying some research gaps and promising directions for future research.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3197685",
                        "name": "S. Vadera"
                    },
                    {
                        "authorId": "32051512",
                        "name": "Salem Ameen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There is a large body of work on the topic of sparse neural networks [17, 18, 19, 20, 21, 22, 23], and many studies derive sophisticated approaches to optimize the sparsity pattern [24, 25, 26, 27]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9e104d440540d2ffc9caaa0952a9e5f7f9344ba9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-14495",
                    "MAG": "3096695297",
                    "ArXiv": "2010.14495",
                    "CorpusId": 225075982
                },
                "corpusId": 225075982,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9e104d440540d2ffc9caaa0952a9e5f7f9344ba9",
                "title": "Are wider nets better given the same number of parameters?",
                "abstract": "Empirical studies demonstrate that the performance of neural networks improves with increasing number of parameters. In most of these studies, the number of parameters is increased by increasing the network width. This begs the question: Is the observed improvement due to the larger number of parameters, or is it due to the larger width itself? We compare different ways of increasing model width while keeping the number of parameters constant. We show that for models initialized with a random, static sparsity pattern in the weight tensors, network width is the determining factor for good performance, while the number of weights is secondary, as long as trainability is ensured. As a step towards understanding this effect, we analyze these models in the framework of Gaussian Process kernels. We find that the distance between the sparse finite-width model kernel and the infinite-width kernel at initialization is indicative of model performance.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "143999820",
                        "name": "A. Golubeva"
                    },
                    {
                        "authorId": "3007442",
                        "name": "Behnam Neyshabur"
                    },
                    {
                        "authorId": "1403749855",
                        "name": "Guy Gur-Ari"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Randomly pruning network weights typically impairs overall network performance unless special care is taken, such as intelligent sparse initialization schemes [52], [53], [55], [89] or dynamic rewiring during the training [51], [54]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a373023ed646fb6eb43087c2c9c047fbbdf7b77c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-11931",
                    "ArXiv": "2010.11931",
                    "MAG": "3094429411",
                    "DOI": "10.1109/JPROC.2020.3045625",
                    "CorpusId": 225041435
                },
                "corpusId": 225041435,
                "publicationVenue": {
                    "id": "6faaccca-1cc4-45a9-aeb6-96a4901d2606",
                    "name": "Proceedings of the IEEE",
                    "type": "journal",
                    "alternate_names": [
                        "Proc IEEE"
                    ],
                    "issn": "0018-9219",
                    "alternate_issns": [
                        "1558-2256"
                    ],
                    "url": "http://www.ieee.org/portal/pages/pubs/proceedings/",
                    "alternate_urls": [
                        "http://www.ieee.org/products/onlinepubs/pub/about_conference.html",
                        "https://ieeexplore.ieee.org/servlet/opac?punumber=5",
                        "http://proceedingsoftheieee.ieee.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a373023ed646fb6eb43087c2c9c047fbbdf7b77c",
                "title": "Brain-Inspired Learning on Neuromorphic Substrates",
                "abstract": "Neuromorphic hardware strives to emulate brain-like neural networks and thus holds the promise for scalable, low-power information processing on temporal data streams. Yet, to solve real-world problems, these networks need to be trained. However, training on neuromorphic substrates creates significant challenges due to the offline character and the required nonlocal computations of gradient-based learning algorithms. This article provides a mathematical framework for the design of practical online learning algorithms for neuromorphic substrates. Specifically, we show a direct connection between real-time recurrent learning (RTRL), an online algorithm for computing gradients in conventional recurrent neural networks (RNNs), and biologically plausible learning rules for training spiking neural networks (SNNs). Furthermore, we motivate a sparse approximation based on block-diagonal Jacobians, which reduces the algorithm\u2019s computational complexity, diminishes the nonlocal information requirements, and empirically leads to good learning performance, thereby improving its applicability to neuromorphic substrates. In summary, our framework bridges the gap between synaptic plasticity and gradient-based approaches from deep learning and lays the foundations for powerful information processing on future neuromorphic hardware systems.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "39979946",
                        "name": "Friedemann Zenke"
                    },
                    {
                        "authorId": "1734355",
                        "name": "E. Neftci"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This achieves the same objective as methods like Rigged Lottery (Evci et al., 2020), Sparse Networks From Scratch (Dettmers & Zettlemoyer, 2019), which periodically prune and grow weights based on gradient momentum.",
                "This is similar to many recent pruning during training methods that employ momentum as the importance metrics to rank weights (Ding et al., 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020).",
                "(3) Momentum Based Pruning (MoP) (Ding et al., 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020): Notice that as we have non-stationary data and thus it is also reasonable to measure the weight importance by its momentum (calculated with the exponential moving average of gradients).",
                "Besides, various methods (Dettmers & Zettlemoyer, 2019; Ding et al., 2019; Evci et al., 2020) are proposed to keep a sparse model through the training process."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "20033e4bed3955165bfac2bc3512dd6dc78d51d3",
                "externalIds": {
                    "ArXiv": "2010.08655",
                    "MAG": "3092816080",
                    "DBLP": "journals/corr/abs-2010-08655",
                    "CorpusId": 224707527
                },
                "corpusId": 224707527,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/20033e4bed3955165bfac2bc3512dd6dc78d51d3",
                "title": "Adaptive Dense-to-Sparse Paradigm for Pruning Online Recommendation System with Non-Stationary Data",
                "abstract": "Large scale deep learning provides a tremendous opportunity to improve the quality of content recommendation systems by employing both wider and deeper models, but this comes at great infrastructural cost and carbon footprint in modern data centers. Pruning is an effective technique that reduces both memory and compute demand for model inference. However, pruning for online recommendation systems is challenging due to the continuous data distribution shift (a.k.a non-stationary data). Although incremental training on the full model is able to adapt to the non-stationary data, directly applying it on the pruned model leads to accuracy loss. This is because the sparsity pattern after pruning requires adjustment to learn new patterns. To the best of our knowledge, this is the first work to provide in-depth analysis and discussion of applying pruning to online recommendation systems with non-stationary data distribution. Overall, this work makes the following contributions: 1) We present an adaptive dense to sparse paradigm equipped with a novel pruning algorithm for pruning a large scale recommendation system with non-stationary data distribution; 2) We design the pruning algorithm to automatically learn the sparsity across layers to avoid repeating hand-tuning, which is critical for pruning the heterogeneous architectures of recommendation systems trained with non-stationary data.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144785541",
                        "name": "Mao Ye"
                    },
                    {
                        "authorId": "9728430",
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "authorId": "9099711",
                        "name": "Jiecao Yu"
                    },
                    {
                        "authorId": "1999317942",
                        "name": "Ellie Wen"
                    },
                    {
                        "authorId": "2111435093",
                        "name": "Zeliang Chen"
                    },
                    {
                        "authorId": "2791531",
                        "name": "Jiyan Yang"
                    },
                    {
                        "authorId": "1686843",
                        "name": "Jongsoo Park"
                    },
                    {
                        "authorId": "47362268",
                        "name": "Qiang Liu"
                    },
                    {
                        "authorId": "2639037",
                        "name": "A. Kejariwal"
                    }
                ]
            }
        },
        {
            "contexts": [
                "An extension of Erdo\u030bs-Re\u0301nyi method (originally given by Mocanu et al. (2018)) accounting for convolutional layers, as proposed by Evci et al. (2020).",
                "Recent discoveries (Gale et al., 2019; Evci et al., 2020) demonstrate that, given an appropriate choice of layerwise sparsity, simply pruning on the basis of weight magnitude yields a surprisingly powerful unstructured pruning scheme.",
                "(2018)) accounting for convolutional layers, as proposed by Evci et al. (2020). The numbers of nonzero parameters of sparse convolutional layers are scaled proportional to 1\u2212 n l\u22121+nl+wl+hl nl\u22121\u00b7nl\u00b7wl\u00b7hl , where n denotes the number of neurons at layer l, and w, h denotes the width and height of the lth layer convolutional kernel.",
                "A recent work by Evci et al. (2020) proposes a magnitude-based dynamic sparse training method, adopting layerwise sparsity inspired from the network science approach toward neural network pruning (Mocanu et al., 2018).",
                "\u2026layer); for larger nets, authors use global MP. Morcos et al. (2019) consider transferring the \u201cwinning ticket\u201d initializations, using the global MP. Evci et al. (2020) proposes a training scheme for sparsely initialized neural networks, where the layerwise sparsity is given by the Erdo\u030bs-Re\u0301nyi\u2026"
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "9227d5897abbf297a34d447e94a802a714b8eab2",
                "externalIds": {
                    "DBLP": "conf/iclr/LeePMAS21",
                    "ArXiv": "2010.07611",
                    "CorpusId": 234358843
                },
                "corpusId": 234358843,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9227d5897abbf297a34d447e94a802a714b8eab2",
                "title": "Layer-adaptive Sparsity for the Magnitude-based Pruning",
                "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on\"how to choose,\"the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation. Under various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection. Furthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48173961",
                        "name": "Jaeho Lee"
                    },
                    {
                        "authorId": "2115258625",
                        "name": "Sejun Park"
                    },
                    {
                        "authorId": "9962692",
                        "name": "Sangwoo Mo"
                    },
                    {
                        "authorId": "70560338",
                        "name": "Sungsoo Ahn"
                    },
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "An extension of Erdo\u030bs-Re\u0301nyi method (originally given by Mocanu et al. (2018)) accounting for convolutional layers, as proposed by Evci et al. (2020).",
                "Recent discoveries (Gale et al., 2019; Evci et al., 2020) demonstrate that, given an appropriate choice of layerwise sparsity, simply pruning on the basis of weight magnitude yields a surprisingly powerful unstructured pruning scheme.",
                "(2018)) accounting for convolutional layers, as proposed by Evci et al. (2020). The numbers of nonzero parameters of sparse convolutional layers are scaled proportional to 1\u2212 n l\u22121+nl+wl+hl nl\u22121\u00b7nl\u00b7wl\u00b7hl , where n l denotes the number of neurons at layer l, and w, h denotes the width and height of the lth layer convolutional kernel.",
                "A recent work by Evci et al. (2020) proposes a magnitude-based dynamic sparse training method, adopting layerwise sparsity inspired from the network science approach toward neural network pruning (Mocanu et al., 2018).",
                "Evci et al. (2020) proposes a training scheme for sparsely initialized neural networks, where the layerwise sparsity is given by the Erdo\u030bs-Re\u0301nyi kernel method; the method generalizes the scheme initially proposed by Mocanu et al. (2018) to convolutional neural networks."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "04341104814cf8b99ed75391e7f7e0a2773208ba",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-07611",
                    "MAG": "3092972975",
                    "CorpusId": 222378502
                },
                "corpusId": 222378502,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/04341104814cf8b99ed75391e7f7e0a2773208ba",
                "title": "A Deeper Look at the Layerwise Sparsity of Magnitude-based Pruning",
                "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on \"how to choose,\" the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation. Under diverse datasets and models, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection. Furthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48173961",
                        "name": "Jaeho Lee"
                    },
                    {
                        "authorId": "2115258625",
                        "name": "Sejun Park"
                    },
                    {
                        "authorId": "9962692",
                        "name": "Sangwoo Mo"
                    },
                    {
                        "authorId": "70560338",
                        "name": "Sungsoo Ahn"
                    },
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "339be3583b4006dc9d9c54e0db60c65234dcab36",
                "externalIds": {
                    "ArXiv": "2010.07243",
                    "MAG": "3093152573",
                    "DBLP": "journals/corr/abs-2010-07243",
                    "CorpusId": 222341262
                },
                "corpusId": 222341262,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/339be3583b4006dc9d9c54e0db60c65234dcab36",
                "title": "Deep Neural Network Training with Frank-Wolfe",
                "abstract": "This paper studies the empirical efficacy and benefits of using projection-free first-order methods in the form of Conditional Gradients, a.k.a. Frank-Wolfe methods, for training Neural Networks with constrained parameters. We draw comparisons both to current state-of-the-art stochastic Gradient Descent methods as well as across different variants of stochastic Conditional Gradients. In particular, we show the general feasibility of training Neural Networks whose parameters are constrained by a convex feasible region using Frank-Wolfe algorithms and compare different stochastic variants. We then show that, by choosing an appropriate region, one can achieve performance exceeding that of unconstrained stochastic Gradient Descent and matching state-of-the-art results relying on L2-regularization. Lastly, we also demonstrate that, besides impacting performance, the particular choice of constraints can have a drastic impact on the learned representations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145729210",
                        "name": "S. Pokutta"
                    },
                    {
                        "authorId": "2064617407",
                        "name": "Christoph Spiegel"
                    },
                    {
                        "authorId": "2056708985",
                        "name": "Max Zimmer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Based on the works of their fellow researchers, a group of researchers from Google and DeepMind has come up with a Rigged Lottery (RigL) [6] method for training SNNs with fixed complexity without accuracy loss."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "75eb43eaaaacdd97cba62c420454d3ee48f1cc4b",
                "externalIds": {
                    "MAG": "3041940404",
                    "ArXiv": "2010.05943",
                    "DBLP": "journals/corr/abs-2010-05943",
                    "CorpusId": 222310732
                },
                "corpusId": 222310732,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/75eb43eaaaacdd97cba62c420454d3ee48f1cc4b",
                "title": "Activation function impact on Sparse Neural Networks",
                "abstract": "While the concept of a Sparse Neural Network has been researched for some time, researchers have only recently made notable progress in the matter. Techniques like Sparse Evolutionary Training allow for significantly lower computational complexity when compared to fully connected models by reducing redundant connections. That typically takes place in an iterative process of weight creation and removal during network training. Although there have been numerous approaches to optimize the redistribution of the removed weights, there seems to be little or no study on the effect of activation functions on the performance of the Sparse Networks. This research provides insights into the relationship between the activation function used and the network performance at various sparsity levels.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "15160656",
                        "name": "A. Dubowski"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Dettmers and Zettlemoyer (2019) proposed using momentum values, whereas Evci et al. (2020) used gradient estimates directly to guide the selection of new connections, reporting results that are on par with pruning algorithms, and has been applied to vision transformers (Chen et al. 2021), language\u2026",
                "However, we don\u2019t know how to find Lottery Tickets (LTs) efficiently; while RigL (Evci et al. 2020), a recent DST method, requires 5\u00d7 the training steps to match dense NN generalization."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "917b18b8dad23284c0a42f665f2ba1984fa360de",
                "externalIds": {
                    "DBLP": "conf/aaai/EvciIKD22",
                    "MAG": "3092446983",
                    "ArXiv": "2010.03533",
                    "DOI": "10.1609/aaai.v36i6.20611",
                    "CorpusId": 222177996
                },
                "corpusId": 222177996,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/917b18b8dad23284c0a42f665f2ba1984fa360de",
                "title": "Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win",
                "abstract": "Sparse Neural Networks (NNs) can match the generalization of dense NNs using a fraction of the compute/storage for inference, and have the potential to enable efficient training. However, naively training unstructured sparse NNs from random initialization results in significantly worse generalization, with the notable exceptions of Lottery Tickets (LTs) and Dynamic Sparse Training (DST). In this work, we attempt to answer: (1) why training unstructured sparse networks from random initialization performs poorly and; (2) what makes LTs and DST the exceptions? We show that sparse NNs have poor gradient flow at initialization and propose a modified initialization for unstructured connectivity. Furthermore, we find that DST methods significantly improve gradient flow during training over traditional sparse training methods. Finally, we show that LTs do not improve gradient flow, rather their success lies in re-learning the pruning solution they are derived from \u2014 however, this comes at the cost of learning novel solutions.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3399348",
                        "name": "Utku Evci"
                    },
                    {
                        "authorId": "3067465",
                        "name": "Yani Andrew Ioannou"
                    },
                    {
                        "authorId": "3860190",
                        "name": "Cem Keskin"
                    },
                    {
                        "authorId": "2921469",
                        "name": "Yann Dauphin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ab31e33b3620988368008ea1cf20dadc0a614983",
                "externalIds": {
                    "ArXiv": "2010.03058",
                    "MAG": "3091818438",
                    "DBLP": "journals/corr/abs-2010-03058",
                    "CorpusId": 222178157
                },
                "corpusId": 222178157,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ab31e33b3620988368008ea1cf20dadc0a614983",
                "title": "Characterising Bias in Compressed Models",
                "abstract": "The popularity and widespread use of pruning and quantization is driven by the severe resource constraints of deploying deep neural networks to environments with strict latency, memory and energy requirements. These techniques achieve high levels of compression with negligible impact on top-line metrics (top-1 and top-5 accuracy). However, overall accuracy hides disproportionately high errors on a small subset of examples; we call this subset Compression Identified Exemplars (CIE). We further establish that for CIE examples, compression amplifies existing algorithmic bias. Pruning disproportionately impacts performance on underrepresented features, which often coincides with considerations of fairness. Given that CIE is a relatively small subset but a great contributor of error in the model, we propose its use as a human-in-the-loop auditing tool to surface a tractable subset of the dataset for further inspection or annotation by a domain expert. We provide qualitative and quantitative support that CIE surfaces the most challenging examples in the data distribution for human-in-the-loop auditing.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50237813",
                        "name": "Sara Hooker"
                    },
                    {
                        "authorId": "3385736",
                        "name": "Nyalleng Moorosi"
                    },
                    {
                        "authorId": "2055187918",
                        "name": "Gregory Clark"
                    },
                    {
                        "authorId": "1751569",
                        "name": "Samy Bengio"
                    },
                    {
                        "authorId": "40081727",
                        "name": "Emily L. Denton"
                    }
                ]
            }
        },
        {
            "contexts": [
                "One way to avoid this challenge altogether is to dynamically change the mask to exploit signals from later in training (Dettmers & Zettlemoyer, 2019; Evci et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0932abfd0fb90e8a28f7bd195633c9891bfd7ecb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2009-08576",
                    "MAG": "3087194612",
                    "ArXiv": "2009.08576",
                    "CorpusId": 221802286
                },
                "corpusId": 221802286,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0932abfd0fb90e8a28f7bd195633c9891bfd7ecb",
                "title": "Pruning Neural Networks at Initialization: Why are We Missing the Mark?",
                "abstract": "Recent work has explored the possibility of pruning neural networks at initialization. We assess proposals for doing so: SNIP (Lee et al., 2019), GraSP (Wang et al., 2020), SynFlow (Tanaka et al., 2020), and magnitude pruning. Although these methods surpass the trivial baseline of random pruning, they remain below the accuracy of magnitude pruning after training, and we endeavor to understand why. We show that, unlike pruning after training, accuracy is the same or higher when randomly shuffling which weights these methods prune within each layer or sampling new initial values. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune. This property undermines the claimed justifications for these methods and suggests broader challenges with the underlying pruning heuristics, the desire to prune at initialization, or both.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "2533850",
                        "name": "G. Dziugaite"
                    },
                    {
                        "authorId": "39331522",
                        "name": "Daniel M. Roy"
                    },
                    {
                        "authorId": "1701041",
                        "name": "Michael Carbin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Current sparse training algorithms either use a fixed sparse neural network architecture, or fixes a particular architecture for a number of iterations [12, 13]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "fbb80084e253ad12fa2085eabec88f3963561254",
                "externalIds": {
                    "DBLP": "conf/IEEEpact/Wang20",
                    "MAG": "3080315965",
                    "ArXiv": "2008.11849",
                    "DOI": "10.1145/3410463.3414654",
                    "CorpusId": 221340735
                },
                "corpusId": 221340735,
                "publicationVenue": {
                    "id": "d65e8b4a-f9c2-45ae-9b87-090db7e872f1",
                    "name": "International Conference on Parallel Architectures and Compilation Techniques",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Parallel Archit Compil Tech",
                        "PACT",
                        "Parallel Comput Technol",
                        "Parallel Computing Technologies",
                        "Pan African Conference Science, Computing and Telecommunications",
                        "PaCT",
                        "Pan Afr Conf Sci Comput Telecommun"
                    ],
                    "url": "http://www.pactconf.org/",
                    "alternate_urls": [
                        "http://ssd.sscc.ru/en/conference"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fbb80084e253ad12fa2085eabec88f3963561254",
                "title": "SparseRT: Accelerating Unstructured Sparsity on GPUs for Deep Learning Inference",
                "abstract": "In recent years, there has been a flurry of research in deep neural network pruning and compression. Early approaches prune weights individually. However, it is difficult to take advantage of the resulting unstructured sparsity patterns on modern hardware like GPUs. As a result, pruning strategies which impose sparsity structures in the weights have become more popular. However,these structured pruning approaches typically lead to higher losses in accuracy than unstructured pruning. In this paper, we present SparseRT, a code generator that leverage unstructured sparsity toaccelerate sparse linear algebra operations in deep learning inference on GPUs. For 1x1 convolutions and fully connected layers, we demonstrate geometric mean of speedups of 3.4x over the equivalent dense computation at 90% sparsity and 5.4x at 95% sparsity when evaluated on hundreds of test cases in deep learning. For sparse 3x3 convolutions, we show speedups of over 5x on use casesin ResNet-50.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2860279",
                        "name": "Ziheng Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Relatedly, motivated by compression, many recent works [11, 18, 8, 6] study sparse neural networks; studying their effectiveness on learning architectural bias from data would be an interesting direction."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "dfbc867aaab9657410a970ac965acc6d1add00a6",
                "externalIds": {
                    "MAG": "3044481990",
                    "DBLP": "conf/nips/Neyshabur20",
                    "ArXiv": "2007.13657",
                    "CorpusId": 220793597
                },
                "corpusId": 220793597,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/dfbc867aaab9657410a970ac965acc6d1add00a6",
                "title": "Towards Learning Convolutions from Scratch",
                "abstract": "Convolution is one of the most essential components of architectures used in computer vision. As machine learning moves towards reducing the expert bias and learning it from data, a natural next step seems to be learning convolution-like structures from scratch. This, however, has proven elusive. For example, current state-of-the-art architecture search algorithms use convolution as one of the existing modules rather than learning it from data. In an attempt to understand the inductive bias that gives rise to convolutions, we investigate minimum description length as a guiding principle and show that in some settings, it can indeed be indicative of the performance of architectures. To find architectures with small description length, we propose $\\beta$-LASSO, a simple variant of LASSO algorithm that, when applied on fully-connected networks for image classification tasks, learns architectures with local connections and achieves state-of-the-art accuracies for training fully-connected nets on CIFAR-10 (85.19%), CIFAR-100 (59.56%) and SVHN (94.07%) bridging the gap between fully-connected and convolutional nets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3007442",
                        "name": "Behnam Neyshabur"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In parallel to this race for ever-larger models, an emerging subfield has explored the prospect of training smaller subnetworks in place of the full models without sacrificing performance [11\u201316]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "389036b1366b64579725457993c1f63a4f3370ba",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-12223",
                    "MAG": "3104263050",
                    "ArXiv": "2007.12223",
                    "CorpusId": 220768628
                },
                "corpusId": 220768628,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/389036b1366b64579725457993c1f63a4f3370ba",
                "title": "The Lottery Ticket Hypothesis for Pre-trained BERT Networks",
                "abstract": "In natural language processing (NLP), enormous pre-trained models like BERT have become the standard starting point for training on a range of downstream tasks, and similar trends are emerging in other areas of deep learning. In parallel, work on the lottery ticket hypothesis has shown that models for NLP and computer vision contain smaller matching subnetworks capable of training in isolation to full accuracy and transferring to other tasks. In this work, we combine these observations to assess whether such trainable, transferrable subnetworks exist in pre-trained BERT models. For a range of downstream tasks, we indeed find matching subnetworks at 40% to 90% sparsity. We find these subnetworks at (pre-trained) initialization, a deviation from prior NLP research where they emerge only after some amount of training. Subnetworks found on the masked language modeling task (the same task used to pre-train the model) transfer universally; those found on other tasks transfer in a limited fashion if at all. As large-scale pre-training becomes an increasingly central paradigm in deep learning, our results demonstrate that the main lottery ticket observations remain relevant in this context. Codes available at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2648459",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "3307026",
                        "name": "Shiyu Chang"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "37873860",
                        "name": "Yang Zhang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "1701041",
                        "name": "Michael Carbin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In recent years, sparse training proved its success in achieving the same performance with dense neural networks for single task standard supervised/unsupervised learning, while having much faster training speed and much lower memory requirements [30, 2, 4, 5, 14, 32].",
                "The number of specific feature maps in each hidden layer spect l is as follows: [2, 2, 5, 6, 30].",
                "Works from [5, 32] also show that the sparse training achieves better performance than iteratively pruning a pre-trained dense model and static sparse neural networks."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "dfb06177195f2ee656dd811d9df6475d6d067491",
                "externalIds": {
                    "MAG": "3042228527",
                    "DBLP": "journals/corr/abs-2007-07617",
                    "ArXiv": "2007.07617",
                    "DOI": "10.1016/j.neucom.2021.01.078",
                    "CorpusId": 220525747
                },
                "corpusId": 220525747,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/dfb06177195f2ee656dd811d9df6475d6d067491",
                "title": "SpaceNet: Make Free Space For Continual Learning",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "67102129",
                        "name": "Ghada Sokar"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Previous works have primarily focused on efficient inference [23, 28, 29, 35, 44] and some on training costs [12]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7d74a961cc2fcaaf2faab5625749af4118262aac",
                "externalIds": {
                    "DBLP": "journals/tmlr/WallingfordKAWKF23",
                    "ArXiv": "2007.02519",
                    "CorpusId": 245117005
                },
                "corpusId": 245117005,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7d74a961cc2fcaaf2faab5625749af4118262aac",
                "title": "FLUID: A Unified Evaluation Framework for Flexible Sequential Data",
                "abstract": "Modern ML methods excel when training data is IID, large-scale, and well labeled. Learning in less ideal conditions remains an open challenge. The sub-fields of few-shot, continual, transfer, and representation learning have made substantial strides in learning under adverse conditions; each affording distinct advantages through methods and insights. These methods address different challenges such as data arriving sequentially or scarce training examples, however often the difficult conditions an ML system will face over its lifetime cannot be anticipated prior to deployment. Therefore, general ML systems which can handle the many challenges of learning in practical settings are needed. To foster research towards the goal of general ML methods, we introduce a new unified evaluation framework - FLUID (Flexible Sequential Data). FLUID integrates the objectives of few-shot, continual, transfer, and representation learning while enabling comparison and integration of techniques across these subfields. In FLUID, a learner faces a stream of data and must make sequential predictions while choosing how to update itself, adapt quickly to novel classes, and deal with changing data distributions; while accounting for the total amount of compute. We conduct experiments on a broad set of methods which shed new insight on the advantages and limitations of current solutions and indicate new research problems to solve. As a starting point towards more general methods, we present two new baselines which outperform other evaluated methods on FLUID. Project page: https://raivn.cs.washington.edu/projects/FLUID/.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1632957174",
                        "name": "Matthew Wallingford"
                    },
                    {
                        "authorId": "52207562",
                        "name": "Aditya Kusupati"
                    },
                    {
                        "authorId": "1796266802",
                        "name": "Keivan Alizadeh-Vahid"
                    },
                    {
                        "authorId": "2419744",
                        "name": "Aaron Walsman"
                    },
                    {
                        "authorId": "2684226",
                        "name": "Aniruddha Kembhavi"
                    },
                    {
                        "authorId": "143787583",
                        "name": "Ali Farhadi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a9fc60d0723fac6240386dde1f50e6df1f87b832",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-03071",
                    "ArXiv": "2007.03071",
                    "MAG": "3039366294",
                    "DOI": "10.1007/978-3-031-20083-0_9",
                    "CorpusId": 220381246
                },
                "corpusId": 220381246,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/a9fc60d0723fac6240386dde1f50e6df1f87b832",
                "title": "Deep Partial Updating",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "39518318",
                        "name": "Zhongnan Qu"
                    },
                    {
                        "authorId": "21270680",
                        "name": "Cong Liu"
                    },
                    {
                        "authorId": "48605270",
                        "name": "Junfeng Guo"
                    },
                    {
                        "authorId": "143980067",
                        "name": "L. Thiele"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Pragmatic systems must measure this compute over the duration of their lifetime; not just measuring inference cost [34, 20, 24] but also update cost [10]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ec8f56e32351d4f18d9844a8ecb37c6971307299",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-02519",
                    "MAG": "3040607201",
                    "CorpusId": 220363837
                },
                "corpusId": 220363837,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ec8f56e32351d4f18d9844a8ecb37c6971307299",
                "title": "In the Wild: From ML Models to Pragmatic ML Systems",
                "abstract": "Enabling robust intelligence in the wild entails learning systems that offer uninterrupted inference while affording sustained training, with varying amounts of data & supervision. Such a pragmatic ML system should be able to cope with the openness & flexibility inherent in the real world. The machine learning community has organically broken down this challenging task into manageable sub tasks such as supervised, few-shot, continual, & self-supervised learning; each affording distinctive challenges & leading to a unique set of methods. Notwithstanding this amazing progress, the restricted & isolated nature of these tasks has resulted in methods that excel in one setting, but struggle to extend beyond them. To foster the research required to extend ML models to ML systems, we introduce a unified learning & evaluation framework - iN thE wilD (NED). NED is designed to be a more general paradigm by loosening the restrictive design decisions of past settings (e.g. closed-world assumption) & imposing fewer restrictions on learning algorithms (e.g. predefined train & test phases). The learners can infer the experimental parameters themselves by optimizing for both accuracy & compute. In NED, a learner receives a stream of data & makes sequential predictions while choosing how to update itself, adapt to data from novel categories, & deal with changing data distributions; while optimizing the total amount of compute. We evaluate a large set of existing methods across several sub fields using NED & present surprising yet revealing findings about modern day techniques. For instance, prominent few shot methods break down in NED, achieving dramatic drops of over 40% accuracy relative to simple baselines; & the SOTA self-supervised methods Momentum Contrast obtains 35% lower accuracy than supervised pretraining on novel classes. We also show that a simple baseline outperforms existing methods on NED.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1632957174",
                        "name": "Matthew Wallingford"
                    },
                    {
                        "authorId": "52207562",
                        "name": "Aditya Kusupati"
                    },
                    {
                        "authorId": "1398372702",
                        "name": "Keivan Alizadeh-Vahid"
                    },
                    {
                        "authorId": "2419744",
                        "name": "Aaron Walsman"
                    },
                    {
                        "authorId": "2684226",
                        "name": "Aniruddha Kembhavi"
                    },
                    {
                        "authorId": "143787583",
                        "name": "Ali Farhadi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In [5], it is shown that such metaheuristics approaches always lead to very-well performing sparse topologies, even if they are based on a random process, without the need of a pre-trained model and a lucky initialization as done in [6].",
                "Recently, many works have emerged to achieve both, training efficiency and inference efficiency, based on adaptive sparse connectivity [26,28,20,3,5].",
                "\u2013 In addition, with the help of our proposed distance metric, we confirm and complement the findings from [5] by being able to quantify how different are the sparse and, at the same time, similarly performing topologies obtained with adaptive sparse connectivity.",
                "Very recently, instead of using the momentum, The Rigged Lottery [5] grows the zero-weights with the highest magnitude gradients to eliminate the extra floating point operations required by Sparse Momentum.",
                "One effective way to optimize the sparse topology is adaptive sparse connectivity, a technique based on connection pruning followed by connection regrowing, which has shown good performance in the previous works [26,28,3,5]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "citingPaper": {
                "paperId": "a25a7791d34a5a15f0bc50a8a73e19f74d295f6c",
                "externalIds": {
                    "MAG": "3039421672",
                    "DBLP": "journals/corr/abs-2006-14085",
                    "ArXiv": "2006.14085",
                    "DOI": "10.1007/978-3-030-67664-3_17",
                    "CorpusId": 220055771
                },
                "corpusId": 220055771,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a25a7791d34a5a15f0bc50a8a73e19f74d295f6c",
                "title": "Topological Insights in Sparse Neural Networks",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "30954888",
                        "name": "T. Lee"
                    },
                    {
                        "authorId": "2250308",
                        "name": "Anil Yaman"
                    },
                    {
                        "authorId": "1768130522",
                        "name": "Zahra Atashgahi"
                    },
                    {
                        "authorId": "46682844",
                        "name": "David L. Ferraro"
                    },
                    {
                        "authorId": "67102129",
                        "name": "Ghada Sokar"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0b5c5571ab9dd88ebf010e8dc6898d8078d8e877",
                "externalIds": {
                    "MAG": "3034290785",
                    "DBLP": "journals/corr/abs-2006-09081",
                    "ArXiv": "2006.09081",
                    "CorpusId": 219708931
                },
                "corpusId": 219708931,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0b5c5571ab9dd88ebf010e8dc6898d8078d8e877",
                "title": "Progressive Skeletonization: Trimming more fat from a network at initialization",
                "abstract": "Recent studies have shown that skeletonization (pruning parameters) of networks at initialization provides all the practical benefits of sparsity both at inference and training time, while only marginally degrading their performance. However, we observe that beyond a certain level of sparsity (approx 95%), these approaches fail to preserve the network performance, and to our surprise, in many cases perform even worse than trivial random pruning. To this end, we propose to find a skeletonized network with maximum foresight connection sensitivity (FORCE). Intuitively, out of all possible sub-networks, we propose to find the one whose connections would have a maximum impact on the loss when perturbed. Our approximate solution to maximize the FORCE, progressively prunes connections of a given network at initialization. This allows parameters that were unimportant at earlier stages of skeletonization to become important at later stages. In many cases, our approach enables us to remove up to 99.9% parameters, while keeping networks trainable and providing significantly better performance than recent approaches. We demonstrate the effectiveness of our approach at various levels of sparsity (from medium to extreme) through extensive experiments and analysis. Code can be found in this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "66320600",
                        "name": "Pau de Jorge"
                    },
                    {
                        "authorId": "3494481",
                        "name": "Amartya Sanyal"
                    },
                    {
                        "authorId": "145560551",
                        "name": "Harkirat Singh Behl"
                    },
                    {
                        "authorId": "143635540",
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "authorId": "3321919",
                        "name": "Gr\u00e9gory Rogez"
                    },
                    {
                        "authorId": "144679302",
                        "name": "P. Dokania"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Second, by use of dynamic network rewiring rules over training time that keeps network sparsity below a given threshold (Mocanu et al., 2018; Bellec et al., 2018; Yan et al., 2019; Evci et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "021739c140c4eefaf2bbb65658e119e5251576a3",
                "externalIds": {
                    "MAG": "3035178967",
                    "DBLP": "journals/corr/abs-2006-08228",
                    "ArXiv": "2006.08228",
                    "CorpusId": 219687376
                },
                "corpusId": 219687376,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/021739c140c4eefaf2bbb65658e119e5251576a3",
                "title": "Finding trainable sparse networks through Neural Tangent Transfer",
                "abstract": "Deep neural networks have dramatically transformed machine learning, but their memory and energy demands are substantial. The requirements of real biological neural networks are rather modest in comparison, and one feature that might underlie this austerity is their sparse connectivity. In deep learning, trainable sparse networks that perform well on a specific task are usually constructed using label-dependent pruning criteria. In this article, we introduce Neural Tangent Transfer, a method that instead finds trainable sparse networks in a label-free manner. Specifically, we find sparse networks whose training dynamics, as characterized by the neural tangent kernel, mimic those of dense networks in function space. Finally, we evaluate our label-agnostic approach on several standard classification tasks and show that the resulting sparse networks achieve higher classification performance while converging faster.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "121698110",
                        "name": "Tianlin Liu"
                    },
                    {
                        "authorId": "39979946",
                        "name": "Friedemann Zenke"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[12] and Zhu and Gupta [31] prevents the use of the optimization in Equation 4, which is strictly necessary to fit the RTRL training computations on accelerators without running out of memory.",
                "The last few years have also seen a resurgence of interest in sparse neural networks \u2013 both their properties [13] and new methods for training them [12]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e9f87b1433458351edcd87a9d3c8bc2862182b8c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-07232",
                    "MAG": "3034235402",
                    "ArXiv": "2006.07232",
                    "CorpusId": 219636150
                },
                "corpusId": 219636150,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e9f87b1433458351edcd87a9d3c8bc2862182b8c",
                "title": "A Practical Sparse Approximation for Real Time Recurrent Learning",
                "abstract": "Current methods for training recurrent neural networks are based on backpropagation through time, which requires storing a complete history of network states, and prohibits updating the weights `online' (after every timestep). Real Time Recurrent Learning (RTRL) eliminates the need for history storage and allows for online weight updates, but does so at the expense of computational costs that are quartic in the state size. This renders RTRL training intractable for all but the smallest networks, even ones that are made highly sparse. \nWe introduce the Sparse n-step Approximation (SnAp) to the RTRL influence matrix, which only keeps entries that are nonzero within n steps of the recurrent core. SnAp with n=1 is no more expensive than backpropagation, and we find that it substantially outperforms other RTRL approximations with comparable costs such as Unbiased Online Recurrent Optimization. For highly sparse networks, SnAp with n=2 remains tractable and can outperform backpropagation through time in terms of learning speed when updates are done online. SnAp becomes equivalent to RTRL when n is large.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "10698483",
                        "name": "Jacob Menick"
                    },
                    {
                        "authorId": "152585800",
                        "name": "Erich Elsen"
                    },
                    {
                        "authorId": "3399348",
                        "name": "Utku Evci"
                    },
                    {
                        "authorId": "2217144",
                        "name": "Simon Osindero"
                    },
                    {
                        "authorId": "34838386",
                        "name": "K. Simonyan"
                    },
                    {
                        "authorId": "1753223",
                        "name": "Alex Graves"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, many methods to induce sparsity in neural networks have shown that it is possible to train models with an overwhelming fraction of the weights being 0 [25, 10, 9, 23, 8, 41]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "25aed5d5bffa369cfd7d483bac8a6c91488d511c",
                "externalIds": {
                    "ArXiv": "2006.07360",
                    "DBLP": "journals/corr/abs-2006-07360",
                    "CorpusId": 219635833
                },
                "corpusId": 219635833,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/25aed5d5bffa369cfd7d483bac8a6c91488d511c",
                "title": "AlgebraNets",
                "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings. However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$ and $M_{4}(\\mathbb{R})$. Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "46616544",
                        "name": "Jordan Hoffmann"
                    },
                    {
                        "authorId": "152380508",
                        "name": "Simon Schmitt"
                    },
                    {
                        "authorId": "2217144",
                        "name": "Simon Osindero"
                    },
                    {
                        "authorId": "34838386",
                        "name": "K. Simonyan"
                    },
                    {
                        "authorId": "152585800",
                        "name": "Erich Elsen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b071d907b6bf00fef44e92f19495e59b6994358c",
                "externalIds": {
                    "ArXiv": "2006.05624",
                    "DBLP": "conf/aaaiss/NathKY22",
                    "CorpusId": 238407716
                },
                "corpusId": 238407716,
                "publicationVenue": {
                    "id": "776d6c56-f19e-409f-9a23-ea13c43f02c4",
                    "name": "Make",
                    "alternate_names": [
                        "Make"
                    ],
                    "issn": "1365-8190"
                },
                "url": "https://www.semanticscholar.org/paper/b071d907b6bf00fef44e92f19495e59b6994358c",
                "title": "Adjoined Networks: A Training Paradigm With Applications to Network Compression",
                "abstract": "Compressing deep neural networks while maintaining accuracy is important when we want to deploy large, powerful models in production and/or edge devices. One common technique used to achieve this goal is knowledge distillation. Typically, the output of a static pre-defined teacher (a large base network) is used as soft labels to train and transfer information to a student (or smaller) network. In this paper, we introduce Adjoined Networks, or AN, a learning paradigm that trains both the original base network and the smaller compressed network together. In our training approach, the parameters of the smaller network are shared across both the base and the compressed networks. Using our training paradigm, we can simultaneously compress (the student network) and regularize (the teacher network) any architecture. In this paper, we focus on popular CNN-based architectures used for computer vision tasks. We conduct an extensive experimental evaluation of our training paradigm on various large-scale datasets. Using ResNet-50 as the base network, AN achieves 71.8% top-1 accuracy with only 1.8M parameters and 1.6 GFLOPs on the ImageNet data-set. We further propose Differentiable Adjoined Networks (DAN), a training paradigm that augments AN by using neural architecture search to jointly learn both the width and the weights for each layer of the smaller network. DAN achieves ResNet-50 level accuracy on ImageNet with $3.8\\times$ fewer parameters and $2.2\\times$ fewer FLOPs.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1741591626",
                        "name": "Utkarsh Nath"
                    },
                    {
                        "authorId": "1770284",
                        "name": "Shrinu Kushagra"
                    },
                    {
                        "authorId": "2680237",
                        "name": "Yingzhen Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While there are a variety of approaches to compressing neural networks, such as novel design of micro-architectures [15, 16, 17], dimensionality reduction of network parameters [18, 19], and training of dynamic sparse networks [20, 21, 22], in this work we will focus on neural network pruning."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3b0fb765716ef6861a84abffcbe40643857c613b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-05467",
                    "MAG": "3034877463",
                    "ArXiv": "2006.05467",
                    "CorpusId": 219558821
                },
                "corpusId": 219558821,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3b0fb765716ef6861a84abffcbe40643857c613b",
                "title": "Pruning neural networks without any data by iteratively conserving synaptic flow",
                "abstract": "Pruning the parameters of deep neural networks has generated intense interest due to potential savings in time, memory and energy both during training and at test time. Recent works have identified, through an expensive sequence of training and pruning cycles, the existence of winning lottery tickets or sparse trainable subnetworks at initialization. This raises a foundational question: can we identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? We provide an affirmative answer to this question through theory driven algorithm design. We first mathematically formulate and experimentally verify a conservation law that explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse, the premature pruning of an entire layer rendering a network untrainable. This theory also elucidates how layer-collapse can be entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow Pruning (SynFlow). This algorithm can be interpreted as preserving the total flow of synaptic strengths through the network at initialization subject to a sparsity constraint. Notably, this algorithm makes no reference to the training data and consistently competes with or outperforms existing state-of-the-art pruning algorithms at initialization over a range of models (VGG and ResNet), datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to 99.99 percent). Thus our data-agnostic pruning algorithm challenges the existing paradigm that, at initialization, data must be used to quantify which synapses are important.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1912151014",
                        "name": "Hidenori Tanaka"
                    },
                    {
                        "authorId": "145616412",
                        "name": "D. Kunin"
                    },
                    {
                        "authorId": "40657572",
                        "name": "Daniel L. K. Yamins"
                    },
                    {
                        "authorId": "25769960",
                        "name": "S. Ganguli"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Modern pruning techniques are quite successful at finding these sparse solutions and prune over 95% of the weights in a network, whilst leaving raw performance intact [24, 43, 18, 15, 75].",
                "This theory sparked a body of research examining the behaviour and obtainment of these winning tickets [18, 76, 54, 13, 15, 79, 17, 50], as well as some criticism [47, 19].",
                "Some methods fight this by pruning during training [48, 18, 15, 75], which reduces the problem somewhat.",
                "One way, is to initialise with a sparse distribution and then train with the periodical interchanging of which parameters are considered pruned and which are \u2018grown back \u2019 [4, 14, 15]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "ead63b8f9d335ef3f5aa7ac1f86864347d894e4c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-00896",
                    "ArXiv": "2006.00896",
                    "MAG": "3031139557",
                    "CorpusId": 219177398
                },
                "corpusId": 219177398,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ead63b8f9d335ef3f5aa7ac1f86864347d894e4c",
                "title": "Pruning via Iterative Ranking of Sensitivity Statistics",
                "abstract": "With the introduction of SNIP [arXiv:1810.02340v2], it has been demonstrated that modern neural networks can effectively be pruned before training. Yet, its sensitivity criterion has since been criticized for not propagating training signal properly or even disconnecting layers. As a remedy, GraSP [arXiv:2002.07376v1] was introduced, compromising on simplicity. However, in this work we show that by applying the sensitivity criterion iteratively in smaller steps - still before training - we can improve its performance without difficult implementation. As such, we introduce 'SNIP-it'. We then demonstrate how it can be applied for both structured and unstructured pruning, before and/or during training, therewith achieving state-of-the-art sparsity-performance trade-offs. That is, while already providing the computational benefits of pruning in the training process from the start. Furthermore, we evaluate our methods on robustness to overfitting, disconnection and adversarial attacks as well.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2067199219",
                        "name": "Stijn Verdenius"
                    },
                    {
                        "authorId": "34216125",
                        "name": "M. Stol"
                    },
                    {
                        "authorId": "51131843",
                        "name": "Patrick Forr'e"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We compare (gRDA) with the Erd\u0151s-R\u00e9nyi-Kernel of [15], variational dropout [42] and a reinforcement-learning based AutoML method [32]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "19ab5e06461aca46148696fd472767079d69c5c7",
                "externalIds": {
                    "ArXiv": "2006.09358",
                    "DBLP": "conf/nips/ChaoWXC20",
                    "MAG": "3102335301",
                    "CorpusId": 219708222
                },
                "corpusId": 219708222,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/19ab5e06461aca46148696fd472767079d69c5c7",
                "title": "Directional Pruning of Deep Neural Networks",
                "abstract": "In the light of the fact that the stochastic gradient descent (SGD) often finds a flat minimum valley in the training loss, we propose a novel directional pruning method which searches for a sparse minimizer in that flat region. The proposed pruning method is automatic in the sense that neither retraining nor expert knowledge is required. To overcome the computational formidability of estimating the flat directions, we propose to use a carefully tuned $\\ell_1$ proximal gradient algorithm which can provably achieve the directional pruning with a small learning rate after sufficient training. The empirical results show that our algorithm performs competitively in highly sparse regime (92\\% sparsity) among many existing automatic pruning methods on the ResNet50 with the ImageNet, while using only a slightly higher wall time and memory footprint than the SGD. Using the VGG16 and the wide ResNet 28x10 on the CIFAR-10 and CIFAR-100, we demonstrate that our algorithm reaches the same minima valley as the SGD, and the minima found by our algorithm and the SGD do not deviate in directions that impact the training loss.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "46233010",
                        "name": "Shih-Kang Chao"
                    },
                    {
                        "authorId": "2108404523",
                        "name": "Zhanyu Wang"
                    },
                    {
                        "authorId": "152136097",
                        "name": "Yue Xing"
                    },
                    {
                        "authorId": "152995990",
                        "name": "Guang Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, unstructured pruning (Hooker et al., 2019; Gale et al., 2019; Evci et al., 2019) and weight specific quantization (Zhen et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f8da8c55c0e7c4940a02347347dd232bc2bac0b5",
                "externalIds": {
                    "ArXiv": "2009.06489",
                    "DBLP": "journals/corr/abs-2009-06489",
                    "MAG": "3087547017",
                    "DOI": "10.1145/3467017",
                    "CorpusId": 221655745
                },
                "corpusId": 221655745,
                "publicationVenue": {
                    "id": "4d9ce1c4-dc84-46b9-903e-e3751c00c7dd",
                    "name": "Communications of the ACM",
                    "type": "journal",
                    "alternate_names": [
                        "Commun ACM",
                        "Communications of The ACM"
                    ],
                    "issn": "0001-0782",
                    "url": "http://www.acm.org/pubs/cacm/",
                    "alternate_urls": [
                        "http://portal.acm.org/cacm",
                        "http://www.acm.org/pubs/contents/journals/cacm/",
                        "https://cacm.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f8da8c55c0e7c4940a02347347dd232bc2bac0b5",
                "title": "The hardware lottery",
                "abstract": "After decades of incentivizing the isolation of hardware, software, and algorithm development, the catalysts for closer collaboration are changing the paradigm.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50237813",
                        "name": "Sara Hooker"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a68c3412e60560290400d2707596f82a914b7c00",
                "externalIds": {
                    "DBLP": "conf/iclr/DaoSGEBLRR20",
                    "ArXiv": "2012.14966",
                    "MAG": "2996641029",
                    "CorpusId": 213704197
                },
                "corpusId": 213704197,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a68c3412e60560290400d2707596f82a914b7c00",
                "title": "Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps",
                "abstract": "Modern neural network architectures use structured linear transformations, such as low-rank matrices, sparse matrices, permutations, and the Fourier transform, to improve inference speed and reduce memory usage compared to general linear maps. However, choosing which of the myriad structured transformations to use (and its associated parameterization) is a laborious task that requires trading off speed, space, and accuracy. We consider a different approach: we introduce a family of matrices called kaleidoscope matrices (K-matrices) that provably capture any structured matrix with near-optimal space (parameter) and time (arithmetic operation) complexity. We empirically validate that K-matrices can be automatically learned within end-to-end pipelines to replace hand-crafted procedures, in order to improve model quality. For example, replacing channel shuffles in ShuffleNet improves classification accuracy on ImageNet by up to 5%. Learnable K-matrices can also simplify hand-engineered pipelines---we replace filter bank feature computation in speech data preprocessing with a kaleidoscope layer, resulting in only 0.4% loss in accuracy on the TIMIT speech recognition task. K-matrices can also capture latent structure in models: for a challenging permuted image classification task, adding a K-matrix to a standard convolutional architecture can enable learning the latent permutation and improve accuracy by over 8 points. We provide a practically efficient implementation of our approach, and use K-matrices in a Transformer network to attain 36% faster end-to-end inference speed on a language translation task.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "24593911",
                        "name": "Tri Dao"
                    },
                    {
                        "authorId": "145193121",
                        "name": "N. Sohoni"
                    },
                    {
                        "authorId": "39499001",
                        "name": "Albert Gu"
                    },
                    {
                        "authorId": "41022841",
                        "name": "Matthew Eichhorn"
                    },
                    {
                        "authorId": "1581517872",
                        "name": "Amit Blonder"
                    },
                    {
                        "authorId": "37866790",
                        "name": "Megan Leszczynski"
                    },
                    {
                        "authorId": "1755572",
                        "name": "A. Rudra"
                    },
                    {
                        "authorId": "1803218",
                        "name": "Christopher R\u00e9"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[23, 24, 27, 45], which prune during regular training and can additionally allow the re-introduction of weights during training; (4) variational or regularization-based methods, e.",
                "We also find that global magnitude (GM) is quite effective, surpassing many recent dynamic pruning methods, which also adjust the sparsity distribution across layers [24, 27, 45]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "6701ae0675f344b14705c7b9dec14273a87e310e",
                "externalIds": {
                    "MAG": "3037301072",
                    "DBLP": "conf/nips/SinghA20",
                    "CorpusId": 220364055
                },
                "corpusId": 220364055,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6701ae0675f344b14705c7b9dec14273a87e310e",
                "title": "WoodFisher: Efficient Second-Order Approximation for Neural Network Compression",
                "abstract": "Second-order information, in the form of Hessian- or Inverse-Hessian-vector products, is a fundamental tool for solving optimization problems. Recently, there has been significant interest in utilizing this information in the context of deep neural networks; however, relatively little is known about the quality of existing approximations in this context. Our work examines this question, identifies issues with existing approaches, and proposes a method called WoodFisher to compute a faithful and efficient estimate of the inverse Hessian. \nOur main application is to neural network compression, where we build on the classic Optimal Brain Damage/Surgeon framework. We demonstrate that WoodFisher significantly outperforms popular state-of-the-art methods for one-shot pruning. Further, even when iterative, gradual pruning is considered, our method results in a gain in test accuracy over the state-of-the-art approaches, for pruning popular neural networks (like ResNet-50, MobileNetV1) trained on standard image classification datasets such as ImageNet ILSVRC. We examine how our method can be extended to take into account first-order information, as well as illustrate its ability to automatically set layer-wise pruning thresholds and perform compression in the limited-data regime.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "24319781",
                        "name": "Sidak Pal Singh"
                    },
                    {
                        "authorId": "3311387",
                        "name": "Dan Alistarh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b08740bf7a7a45591b89434c987f268f5ba06211",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2004-05898",
                    "ArXiv": "2004.05898",
                    "MAG": "3015465510",
                    "DOI": "10.13140/RG.2.2.31661.23527",
                    "CorpusId": 215745572
                },
                "corpusId": 215745572,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b08740bf7a7a45591b89434c987f268f5ba06211",
                "title": "Exposing Hardware Building Blocks to Machine Learning Frameworks",
                "abstract": "There are a plethora of applications that demand high throughput and low latency algorithms leveraging machine learning methods. This need for real time processing can be seen in industries ranging from developing neural network based pre-distortors for enhanced mobile broadband to designing FPGA-based triggers in major scientific efforts by CERN for particle physics. In this thesis, we explore how niche domains can benefit vastly if we look at neurons as a unique boolean function of the form $f:B^{I} \\rightarrow B^{O}$, where $B = \\{0,1\\}$. We focus on how to design topologies that complement such a view of neurons, how to automate such a strategy of neural network design, and inference of such networks on Xilinx FPGAs. Major hardware borne constraints arise when designing topologies that view neurons as unique boolean functions. Fundamentally, realizing such topologies on hardware asserts a strict limit on the 'fan-in' bits of a neuron due to the doubling of permutations possible with every increment in input bit-length. We address this limit by exploring different methods of implementing sparsity and explore activation quantization. Further, we develop a library that supports training a neural network with custom sparsity and quantization. This library also supports conversion of trained Sparse Quantized networks from PyTorch to VERILOG code which is then synthesized using Vivado, all of which is part of the LogicNet tool-flow. To aid faster prototyping, we also support calculation of the worst-case hardware cost of any given topology. We hope that our insights into the behavior of extremely sparse quantized neural networks are of use to the research community and by extension allow people to use the LogicNet design flow to deploy highly efficient neural networks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "123173562",
                        "name": "Yash Akhauri"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018) and convolutional models for computer vision (Zhu & Gupta, 2018; Elsen et al., 2019; Evci et al., 2020; Kusupati et al., 2020).",
                "\u2026findings hold for large but sparse audio synthesis models (Kalchbrenner et al., 2018) and convolutional models for computer vision (Zhu & Gupta, 2018; Elsen et al., 2019; Evci et al., 2020; Kusupati et al., 2020).\noriginal weights and the sparse weights\u2014for the 60% sparse ROBERTA models."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2356781b8a98bf94e6fc73798c6cb65ac35e5f97",
                "externalIds": {
                    "MAG": "3008851394",
                    "ArXiv": "2002.11794",
                    "DBLP": "journals/corr/abs-2002-11794",
                    "CorpusId": 211532277
                },
                "corpusId": 211532277,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2356781b8a98bf94e6fc73798c6cb65ac35e5f97",
                "title": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers",
                "abstract": "Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for NLP tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations. \nThis leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2276422",
                        "name": "Zhuohan Li"
                    },
                    {
                        "authorId": "145217343",
                        "name": "Eric Wallace"
                    },
                    {
                        "authorId": "2191455",
                        "name": "Sheng Shen"
                    },
                    {
                        "authorId": "48085802",
                        "name": "Kevin Lin"
                    },
                    {
                        "authorId": "1732330",
                        "name": "K. Keutzer"
                    },
                    {
                        "authorId": "38666915",
                        "name": "D. Klein"
                    },
                    {
                        "authorId": "144307989",
                        "name": "Joseph Gonzalez"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following this pivotal work, several studies have been carried to understand the role of initialization, the e ect of the pruning criterion used and the importance of retraining the sub-networks [3, 4, 5, 6, 7, 8] for the success of lottery tickets."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a41183d756e2078b61903b70cd606f4c0b13e2fe",
                "externalIds": {
                    "MAG": "3005249894",
                    "ArXiv": "2002.03875",
                    "DBLP": "journals/corr/abs-2002-03875",
                    "CorpusId": 211069074
                },
                "corpusId": 211069074,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a41183d756e2078b61903b70cd606f4c0b13e2fe",
                "title": "Calibrate and Prune: Improving Reliability of Lottery Tickets Through Prediction Calibration",
                "abstract": "The hypothesis that sub-network initializations (lottery) exist within the initializations of over-parameterized networks, which when trained in isolation produce highly generalizable models, has led to crucial insights into network initialization and has enabled efficient inferencing. Supervised models with uncalibrated confidences tend to be overconfident even when making wrong prediction. In this paper, for the first time, we study how explicit confidence calibration in the over-parameterized network impacts the quality of the resulting lottery tickets. More specifically, we incorporate a suite of calibration strategies, ranging from mixup regularization, variance-weighted confidence calibration to the newly proposed likelihood-based calibration and normalized bin assignment strategies. Furthermore, we explore different combinations of architectures and datasets, and make a number of key findings about the role of confidence calibration. Our empirical studies reveal that including calibration mechanisms consistently lead to more effective lottery tickets, in terms of accuracy as well as empirical calibration metrics, even when retrained using data with challenging distribution shifts with respect to the source dataset.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "153441560",
                        "name": "Bindya Venkatesh"
                    },
                    {
                        "authorId": "1744175",
                        "name": "Jayaraman J. Thiagarajan"
                    },
                    {
                        "authorId": "51149615",
                        "name": "Kowshik Thopalli"
                    },
                    {
                        "authorId": "1706272",
                        "name": "P. Sattigeri"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2019), SNFS (Dettmers & Zettlemoyer, 2019), RigL (Evci et al., 2020) and DPF (Lin et al.",
                "Sparse Networks From Scratch (SNFS) (Dettmers & Zettlemoyer, 2019) utilizes momentum of the weights to re-allocate weights across layers and the Rigged Lottery (RigL) (Evci et al., 2019) uses the magnitude to drop and the periodic dense gradients to regrow weights.",
                "The \u201c+ ERK\u201d suffix implies the usage of ERK budget (Evci et al., 2020) instead of the original sparsity budget.",
                "5\u00d7 (Gale et al., 2019) and RigL5\u00d7 (Evci et al., 2019) show that training the networks longer increases accuracy.",
                "Unstructured sparsity has been extensively studied and includes methods which use gradient, momentum, and Hessian based heuristics (Evci et al., 2020; Lee et al., 2019; LeCun et al., 1990; Hassibi & Stork, 1993; Dettmers & Zettlemoyer, 2019), and magnitude-based pruning (Han et al.",
                ", 2019) and RigL5\u00d7 (Evci et al., 2020) show that training the networks longer increases accuracy.",
                "Figure 6 also shows that the last layers through STR are denser than that of the other methods which is contrary to the understanding in the literature of non-uniform sparsity (Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Gale et al., 2019).",
                "It can be a fixed budget like the ERK (Erdos-Renyi-Kernel) heuristic described in RigL (Evci et al., 2020).",
                "RigL, SNFS, DSR, and DPF were compared in their original form.",
                "SNFS and RigL are state-of-the-art in sparse-to-sparse training but fall short of GMP for the same experimental settings.",
                "Table 1 summarizes that the non-uniform sparsity baselines like SNFS, SNFS+ERK and RigL+ERK can have up to 2-4\u00d7 higher inference cost (FLOPs) due to non-optimal layer-wise distribution of the parameter weights.",
                "Sparse Networks From Scratch (SNFS) (Dettmers & Zettlemoyer, 2019) utilizes momentum of the weights to re-allocate weights across layers and the Rigged Lottery (RigL) (Evci et al., 2020) uses the magnitude to drop and the periodic dense gradients to regrow weights.",
                "It can be a fixed budget like the ERK (Erdos-Renyi-Kernel) heuristic described in RigL (Evci et al., 2019).",
                "STR was compared against strong state-of-the-art baselines in various sparsity regimes including GMP (Gale et al., 2019), DSR (Mostafa & Wang, 2019), DNW (Wortsman et al., 2019), SNFS (Dettmers & Zettlemoyer, 2019), RigL (Evci et al., 2019) and DPF (Lin et al., 2020).",
                "This is referred to as sparse-to-sparse training and a lot of recent work (Mostafa & Wang, 2019; Bellec et al., 2018; Evci et al., 2020; Lee et al., 2019; Dettmers & Zettlemoyer, 2019) aims to do sparse-to-sparse training using techniques which include re-allocation of weights to improve accuracy."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "8eb599d5d7f1821b205e3b56fef5340b1622ba52",
                "externalIds": {
                    "MAG": "3035304835",
                    "DBLP": "journals/corr/abs-2002-03231",
                    "ArXiv": "2002.03231",
                    "CorpusId": 211069143
                },
                "corpusId": 211069143,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8eb599d5d7f1821b205e3b56fef5340b1622ba52",
                "title": "Soft Threshold Weight Reparameterization for Learnable Sparsity",
                "abstract": "Sparsity in Deep Neural Networks (DNNs) is studied extensively with the focus of maximizing prediction accuracy given an overall parameter budget. Existing methods rely on uniform or heuristic non-uniform sparsity budgets which have sub-optimal layer-wise parameter allocation resulting in a) lower prediction accuracy or b) higher inference cost (FLOPs). This work proposes Soft Threshold Reparameterization (STR), a novel use of the soft-threshold operator on DNN weights. STR smoothly induces sparsity while learning pruning thresholds thereby obtaining a non-uniform sparsity budget. Our method achieves state-of-the-art accuracy for unstructured sparsity in CNNs (ResNet50 and MobileNetV1 on ImageNet-1K), and, additionally, learns non-uniform budgets that empirically reduce the FLOPs by up to 50%. Notably, STR boosts the accuracy over existing results by up to 10% in the ultra sparse (99%) regime and can also be used to induce low-rank (structured sparsity) in RNNs. In short, STR is a simple mechanism which learns effective sparsity budgets that contrast with popular heuristics. Code, pretrained models and sparsity budgets are at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "52207562",
                        "name": "Aditya Kusupati"
                    },
                    {
                        "authorId": "5547527",
                        "name": "V. Ramanujan"
                    },
                    {
                        "authorId": "51910830",
                        "name": "Raghav Somani"
                    },
                    {
                        "authorId": "52193502",
                        "name": "Mitchell Wortsman"
                    },
                    {
                        "authorId": "48964143",
                        "name": "Prateek Jain"
                    },
                    {
                        "authorId": "144695232",
                        "name": "S. Kakade"
                    },
                    {
                        "authorId": "143787583",
                        "name": "Ali Farhadi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "52184d7a541eff0b9537e75da7327dd41daba207",
                "externalIds": {
                    "MAG": "3102093079",
                    "DBLP": "journals/corr/abs-2001-01969",
                    "ArXiv": "2001.01969",
                    "DOI": "10.14288/1.0390935",
                    "CorpusId": 210023375
                },
                "corpusId": 210023375,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/52184d7a541eff0b9537e75da7327dd41daba207",
                "title": "Sparse Weight Activation Training",
                "abstract": "Neural network training is computationally and memory intensive. Sparse training can reduce the burden on emerging hardware platforms designed to accelerate sparse computations, but it can affect network convergence. In this work, we propose a novel CNN training algorithm Sparse Weight Activation Training (SWAT). SWAT is more computation and memory-efficient than conventional training. SWAT modifies back-propagation based on the empirical insight that convergence during training tends to be robust to the elimination of (i) small magnitude weights during the forward pass and (ii) both small magnitude weights and activations during the backward pass. We evaluate SWAT on recent CNN architectures such as ResNet, VGG, DenseNet and WideResNet using CIFAR-10, CIFAR-100 and ImageNet datasets. For ResNet-50 on ImageNet SWAT reduces total floating-point operations (FLOPS) during training by 80% resulting in a 3.3$\\times$ training speedup when run on a simulated sparse learning accelerator representative of emerging platforms while incurring only 1.63% reduction in validation accuracy. Moreover, SWAT reduces memory footprint during the backward pass by 23% to 50% for activations and 50% to 90% for weights.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "9123370",
                        "name": "Md Aamir Raihan"
                    },
                    {
                        "authorId": "1742561",
                        "name": "Tor M. Aamodt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This work is among several recent papers to propose that merely sparsifying at initialization can produce high performance neural networks (Mallya et al., 2018; Zhou et al., 2019; Ramanujan et al., 2020; Evci et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3f06d02513a2763e472d2b5d5db08e9061081b9e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1912-05671",
                    "ArXiv": "1912.05671",
                    "MAG": "3035081900",
                    "CorpusId": 209324341
                },
                "corpusId": 209324341,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3f06d02513a2763e472d2b5d5db08e9061081b9e",
                "title": "Linear Mode Connectivity and the Lottery Ticket Hypothesis",
                "abstract": "We study whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise (e.g., random data order and augmentation). We find that standard vision models become stable to SGD noise in this way early in training. From then on, the outcome of optimization is determined to a linearly connected region. We use this technique to study iterative magnitude pruning (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained in isolation to full accuracy. We find that these subnetworks only reach full accuracy when they are stable to SGD noise, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (ResNet-50 and Inception-v3 on ImageNet).",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "2533850",
                        "name": "G. Dziugaite"
                    },
                    {
                        "authorId": "39331522",
                        "name": "Daniel M. Roy"
                    },
                    {
                        "authorId": "1701041",
                        "name": "Michael Carbin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Despite the widespread use of compression techniques, articulating the trade-offs of compression has overwhelming centered on change to overall accuracy for a given level of compression (Str\u00f6m, 1997; Cun et al., 1990; Evci et al., 2019; Narang et al., 2017; Gale et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "37d87f24841987ae64e871088efb6c1df6d405d4",
                "externalIds": {
                    "ArXiv": "1911.05248",
                    "MAG": "3042879175",
                    "CorpusId": 226812844
                },
                "corpusId": 226812844,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/37d87f24841987ae64e871088efb6c1df6d405d4",
                "title": "What Do Compressed Deep Neural Networks Forget",
                "abstract": "Deep neural network pruning and quantization techniques have demonstrated it is possible to achieve high levels of compression with surprisingly little degradation to test set accuracy. However, this measure of performance conceals significant differences in how different classes and images are impacted by model compression techniques. We find that models with radically different numbers of weights have comparable top-line performance metrics but diverge considerably in behavior on a narrow subset of the dataset. This small subset of data points, which we term Pruning Identified Exemplars (PIEs) are systematically more impacted by the introduction of sparsity. Compression disproportionately impacts model performance on the underrepresented long-tail of the data distribution. PIEs over-index on atypical or noisy images that are far more challenging for both humans and algorithms to classify. Our work provides intuition into the role of capacity in deep neural networks and the trade-offs incurred by compression. An understanding of this disparate impact is critical given the widespread deployment of compressed models in the wild.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "50237813",
                        "name": "Sara Hooker"
                    },
                    {
                        "authorId": "1760871",
                        "name": "Aaron C. Courville"
                    },
                    {
                        "authorId": "2055187918",
                        "name": "Gregory Clark"
                    },
                    {
                        "authorId": "2921469",
                        "name": "Yann Dauphin"
                    },
                    {
                        "authorId": "2279670",
                        "name": "Andrea Frome"
                    }
                ]
            }
        },
        {
            "contexts": [
                "llowing [14]). We run CS with s 0 2f0:0; 0:01; 0:02; 0:03; 0:05gyielding 5 tickets with varying sparsity levels. Table2summarizes the results achieved by CS, IMP, and state-of-the-art pruning methods [25, 26, 27, 28, 29], where reported sparsity levels are computed for the set of parameters that each method prunes. IMPydenotes IMP run for 12 rounds, i.e. using a larger training budget. Differences in each method\u2019s me"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "54b8fcf4cc95c0eee93910052018d6286dc78ad9",
                "externalIds": {
                    "MAG": "2995107071",
                    "DBLP": "conf/nips/SavareseSM20",
                    "ArXiv": "1912.04427",
                    "CorpusId": 209140629
                },
                "corpusId": 209140629,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/54b8fcf4cc95c0eee93910052018d6286dc78ad9",
                "title": "Winning the Lottery with Continuous Sparsification",
                "abstract": "The Lottery Ticket Hypothesis conjectures that, for a typically-sized neural network, it is possible to find small sub-networks that, when trained from scratch, match the performance of the dense counterpart given a comparable training budget. The proposed algorithm to search for winning tickets, Iterative Magnitude Pruning, consistently finds sparse sub-networks which train faster and better than the overparameterized models they were extracted from, creating potential applications to problems such as transfer learning. In this paper, we propose Continuous Sparsification, a new algorithm to search for winning tickets which continuously removes parameters from a network during training, and learns the sub-network's structure with gradient-based methods instead of relying on pruning strategies. We show empirically that our method is capable of finding tickets that are sparser than the ones found by Iterative Magnitude Pruning, while achieving higher performance when trained from scratch. Moreover, our method can be efficiently parallelized, decreasing the ticket search cost measured in wall-clock time significantly given enough parallel computing resources.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1387999547",
                        "name": "Pedro H. P. Savarese"
                    },
                    {
                        "authorId": "2064829114",
                        "name": "Hugo Silva"
                    },
                    {
                        "authorId": "145854440",
                        "name": "M. Maire"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The Rigged Lottery (RigL) [10] addressed the high computational cost by using infrequent gradient information.",
                "\u2026dynamic sparse\n1 https://github.com/zahraatashgahi/CTRE.\ntraining methods use magnitude as a pruning criterion, weight regrowing approaches are of different types, including random (Mocanu et al., 2018; Mostafa & Wang, 2019) and gradient-based regrowth (Evci et al., 2020; Jayakumar et al., 2020).",
                "While most dynamic sparse training methods use magnitude as a pruning criterion, weight regrowing approaches are of different types, including random [52, 57] and gradient-based regrowth [10, 28].",
                "\u25cf RigL Evci et al. (2020).",
                "The Rigged Lottery (RigL) (Evci et al., 2020) addressed the high computational cost by using infrequent gradient information."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "bbfd6ced1dea12b467e4797bfc35568e95326f07",
                "externalIds": {
                    "ArXiv": "1903.07138",
                    "DBLP": "journals/ml/AtashgahiPLMVP22",
                    "DOI": "10.1007/s10994-022-06266-w",
                    "CorpusId": 239885772
                },
                "corpusId": 239885772,
                "publicationVenue": {
                    "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
                    "name": "Machine-mediated learning",
                    "type": "journal",
                    "alternate_names": [
                        "Mach learn",
                        "Machine Learning",
                        "Mach Learn"
                    ],
                    "issn": "0732-6718",
                    "alternate_issns": [
                        "0885-6125"
                    ],
                    "url": "http://www.springer.com/computer/artificial/journal/10994",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10994",
                        "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bbfd6ced1dea12b467e4797bfc35568e95326f07",
                "title": "A brain-inspired algorithm for training highly sparse neural networks",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1768130522",
                        "name": "Zahra Atashgahi"
                    },
                    {
                        "authorId": "1884787",
                        "name": "Joost Pieterse"
                    },
                    {
                        "authorId": "2131166985",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "39128850",
                        "name": "R. Veldhuis"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, even if it would be possible to run FC-MLP, this comparison is outside the scope of this paper and it would be redundant as it has been shown in [6, 24, 44, 52, 53, 73] that SET-MLP typically outperforms its fully connected counterparts.",
                "recently, by modifying the sparsity distribution of Erd}os\u2013 R\u00e9nyi introduced in [52], RigL [24] can match and sometimes exceed the performance of pruning-based approaches.",
                ", sparse evolutionary training (SET) [52], DEEP-R [4], dynamic sparse reparameterization (DSR) [55], sparse momentum [20], ST-RNNs [43], and rigged lottery (RigL) [24]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "c37a1110d007a6c6a1e536527504eca5953a51f7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1901-09181",
                    "ArXiv": "1901.09181",
                    "MAG": "3038428716",
                    "DOI": "10.1007/s00521-020-05136-7",
                    "CorpusId": 59316570
                },
                "corpusId": 59316570,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c37a1110d007a6c6a1e536527504eca5953a51f7",
                "title": "Sparse evolutionary deep learning with over one million artificial neurons on commodity hardware",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "31401161",
                        "name": "A. Matavalam"
                    },
                    {
                        "authorId": "1382535564",
                        "name": "Yulong Pei"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We demonstrate that HYDRA (Sehwag et al., 2020) and Robust-ADMM (Ye et al., 2019) yield better results when used with non-uniform strategies determined by ERK (Evci et al., 2020) and LAMP (Lee et al., 2021) (cf. Section 4.3).",
                "Also in conventional network pruning, non-uniform compression strategies have been proven effective, for instance, ERK by Evci et al. (2020) and LAMP by Lee et al. (2021).",
                "Concatenated architectures such as VGG16 possess relatively high sparsity in the middle layers (Lee et al., 2021; Evci et al., 2020).",
                ", 2019) yield better results when used with non-uniform strategies determined by ERK (Evci et al., 2020) and LAMP (Lee et al."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "b6f933577c0e535ace8dcbab62a7308a2d04b980",
                "externalIds": {
                    "DBLP": "conf/iclr/ZhaoW23",
                    "CorpusId": 259298603
                },
                "corpusId": 259298603,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b6f933577c0e535ace8dcbab62a7308a2d04b980",
                "title": "Holistic Adversarially Robust Pruning",
                "abstract": "Neural networks can be drastically shrunk in size by removing redundant parameters. While crucial for the deployment on resource-constraint hardware, oftentimes, compression comes with a severe drop in accuracy and lack of adversarial robustness. Despite recent advances, counteracting both aspects has only succeeded for moderate compression rates so far. We propose a novel method, HARP, that copes with aggressive pruning significantly better than prior work. For this, we consider the network holistically. We learn a global compression strategy that optimizes how many parameters (compression rate) and which parameters (scoring connections) to prune specific to each layer individually. Our method fine-tunes an existing model with dynamic regularization, that follows a step-wise incremental function balancing the different objectives. It starts by favoring robustness before shifting focus on reaching the target compression rate and only then handles the objectives equally. The learned compression strategies allow us to maintain the pre-trained model\u2019s natural accuracy and its adversarial robustness for a reduction by 99% of the network\u2019s original size. Moreover, we observe a crucial influence of non-uniform compression across layers. The implementation of HARP is publicly available at https://intellisec.de/research/harp.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2110560568",
                        "name": "Qi Zhao"
                    },
                    {
                        "authorId": "2198719",
                        "name": "Christian Wressnegger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For link prediction in Ogbl-Collab, we adopt 28-layer ResGCNs.",
                "To test the scalability of DGLT, we further use a large-scale dataset called Ogbl-Collab Hu et al. (2020) for link prediction. Finally, we examine our algorithm for graph classification on D&D Dobson & Doig (2003) and ENZYMES Borgwardt et al.",
                "For Ogbl-Collab, in order to simulate a real collaborative recommendation application, we take the cooperation before 2017 as the training edge, the cooperation in 2018 as the validation edge and the cooperation in 2019 as the testing edge.",
                "LTH is initially observed in dense networks and is broadly found in many fields Evci et al. (2020); Frankle et al. (2020); Malach et al. (2020); Ding et al. (2021); Chen et al. (2020a; 2021); Sui et al. (2021).",
                "To answer RQ2, we conduct experiments on the Ogbl-Collab dataset using ResGCNs as the backbone.",
                "To test the scalability of DGLT, we further use a large-scale dataset called Ogbl-Collab Hu et al. (2020) for link prediction."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "f87ab24bca90dc062b7ba73dc355a41625c0652a",
                "externalIds": {
                    "DBLP": "conf/iclr/WangLWWGF023",
                    "CorpusId": 259298584
                },
                "corpusId": 259298584,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f87ab24bca90dc062b7ba73dc355a41625c0652a",
                "title": "Searching Lottery Tickets in Graph Neural Networks: A Dual Perspective",
                "abstract": "Graph Neural Networks (GNNs) have shown great promise in various graph learning tasks. However, the computational overheads of fitting GNNs to large-scale graphs grow rapidly, posing obstacles to GNNs from scaling up to real-world applications. To tackle this issue, Graph Lottery Ticket (GLT) hypothesis articulates that there always exists a sparse subnetwork/subgraph with admirable performance in GNNs with random initialization. Such a pair of core subgraph and sparse subnetwork (called graph lottery tickets) can be uncovered by iteratively applying a novel sparsification method. While GLT provides new insights for GNN compression, it requires a full pretraining process to obtain graph lottery tickets, which is not universal and friendly to real-world applications. Moreover, the graph sparsification in GLT utilizes sampling techniques, which may result in massive information loss and aggregation failure. In this paper, we explore the searching of graph lottery tickets from a complementary perspective \u2013 transforming a random ticket into a graph lottery ticket, which allows us to more comprehensively explore the relationships between the original network/graph and their sparse counterpart. Compared to GLT, our proposal helps achieve a triple-win situation of graph lottery tickets with high sparsity, admirable performance, and good explainability. More importantly, we rigorously prove that our model can eliminate noise and maintain reliable information in substructures using the graph information bottleneck theory. Extensive experimental results on various graphrelated tasks validate the effectiveness of our framework.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2121263212",
                        "name": "Kun Wang"
                    },
                    {
                        "authorId": "3431194",
                        "name": "Yuxuan Liang"
                    },
                    {
                        "authorId": "2108814780",
                        "name": "Pengkun Wang"
                    },
                    {
                        "authorId": "2108599981",
                        "name": "Xu Wang"
                    },
                    {
                        "authorId": "2056261627",
                        "name": "Pengfei Gu"
                    },
                    {
                        "authorId": "2159830802",
                        "name": "Junfeng Fang"
                    },
                    {
                        "authorId": null,
                        "name": "Yang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026finding a fixed sparse mask at the initialization as we mentioned in introduction, on the other hand, dynamic sparse training allows the sparse mask to be updated during training, e.g., (Mocanu et al., 2018; Mostafa and Wang, 2019; Evci et al., 2020; Jayakumar et al., 2020; Liu et al., 2021a,c,d)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "33ed88d985bf31fe902e757a4dbef31ceda48ff6",
                "externalIds": {
                    "CorpusId": 255372380
                },
                "corpusId": 255372380,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/33ed88d985bf31fe902e757a4dbef31ceda48ff6",
                "title": "Sharper analysis of sparsely activated wide neural networks with trainable biases",
                "abstract": "This work studies training one-hidden-layer overparameterized ReLU networks via gradient descent in the neural tangent kernel (NTK) regime, where, di\ufb00erently from the previous works, the networks\u2019 biases are trainable and are initialized to some constant rather than zero. The tantalizing bene\ufb01t of such initialization is that the neural network will provably have sparse activation pattern before, during and after training, which can enable fast training procedures and, therefore, reduce the training cost. The \ufb01rst set of results of this work characterize the convergence of the network\u2019s gradient descent dynamics. Surprisingly, we show that the network after sparsi\ufb01cation can achieve as fast convergence as the original network. Further, the required width is provided to ensure gradient descent can drive the training error towards zero at a linear rate. The contribution over previous work is that not only the bias is allowed to be updated by gradient descent under our setting but also a \ufb01ner analysis is given such that the required width to ensure the network\u2019s closeness to its NTK is improved. Secondly, the networks\u2019 generalization bound after training is provided. A width-sparsity dependence is presented which yields sparsity-dependent localized Rademacher complexity and a generalization bound matching previous analysis (up to logarithmic factors). To our knowledge, this is the \ufb01rst sparsity-dependent generalization result via localized Rademacher complexity. As a by-product, if the bias initialization is chosen to be zero, the width requirement improves the previous bound for the shallow networks\u2019 generalization. Lastly, since the generalization bound has dependence on the smallest eigenvalue of the limiting NTK and the bounds from previous works yield vacuous generalization, this work further studies the least eigenvalue of the limiting NTK. Surprisingly, while it is not shown that trainable biases are necessary, trainable bias helps to identify a nice data-dependent region where a much \ufb01ner analysis of the NTK\u2019s smallest eigenvalue can be conducted, which leads to a much sharper lower bound than the",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118571035",
                        "name": "Hongru Yang"
                    },
                    {
                        "authorId": "152420547",
                        "name": "Ziyu Jiang"
                    },
                    {
                        "authorId": null,
                        "name": "Ruizhe Zhang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "50014661",
                        "name": "Yingbin Liang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7617f49e3178c3dbeaa8ce55ab9f08eaa50f71e6",
                "externalIds": {
                    "CorpusId": 256827947
                },
                "corpusId": 256827947,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7617f49e3178c3dbeaa8ce55ab9f08eaa50f71e6",
                "title": "Regularization Compression Collapse Performance Sparsity Performance Sparsity Performance Sparsity",
                "abstract": "In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may underprune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a limit that appears to correspond to the beginning of underfitting. Subsequently, PQI decreases again when the model collapse and significant deterioration in the performance of the model start to occur. Additionally, our experiments demonstrate that the proposed adaptive pruning algorithm with proper choice of hyper-parameters is superior to the iterative pruning algorithms such as the lottery ticket-based pruning methods, in terms of both compression efficiency and robustness. Our code is available here.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51298945",
                        "name": "Enmao Diao"
                    },
                    {
                        "authorId": "2096527",
                        "name": "G. Wang"
                    },
                    {
                        "authorId": "2111183794",
                        "name": "Jie Ding"
                    }
                ]
            }
        },
        {
            "contexts": [
                "training [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], even before looking at the data, in order to reduce training cost."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "839fcb7c0b890fd46c36f48501b74a4ae4ea0ba7",
                "externalIds": {
                    "DBLP": "journals/access/GarciaAriasOHMY23",
                    "DOI": "10.1109/ACCESS.2023.3245808",
                    "CorpusId": 256971121
                },
                "corpusId": 256971121,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/839fcb7c0b890fd46c36f48501b74a4ae4ea0ba7",
                "title": "Recurrent Residual Networks Contain Stronger Lottery Tickets",
                "abstract": "Accurate neural networks can be found just by pruning a randomly initialized overparameterized model, leaving out the need for any weight optimization. The resulting subnetworks are small, sparse, and ternary, making excellent candidates for efficient hardware implementation. However, finding optimal connectivity patterns is an open challenge. Based on the evidence that residual networks may be approximating unrolled shallow recurrent neural networks, we conjecture that they contain better candidate subnetworks at inference time when explicitly transformed into recurrent architectures. This hypothesis is put to the test on image classification tasks, where we find subnetworks within the recurrent models that are more accurate and parameter-efficient than both the ones found within feedforward models and than the full models with learned weights. Furthermore, random recurrent subnetworks are tiny: under a simple compression scheme, ResNet-50 is compressed without a drastic loss in performance to $48.55\\times $ less memory size, fitting in under 2 megabytes. Code available at: https://github.com/Lopez-Angel/hidden-fold-networks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1409951890",
                        "name": "\u00c1ngel L\u00f3pez Garc\u00eda-Arias"
                    },
                    {
                        "authorId": "2159237466",
                        "name": "Yasuyuki Okoshi"
                    },
                    {
                        "authorId": "2067777534",
                        "name": "Masanori Hashimoto"
                    },
                    {
                        "authorId": "2159237569",
                        "name": "Masato Motomura"
                    },
                    {
                        "authorId": "2154957755",
                        "name": "Jaehoon Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The other line of work focuses on reducing computation through various model pruning techniques (Han et al., 2015; Frankle & Carbin, 2018; Evci et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "59b673ca75644ac112f467eb1e0374fa573b5ab0",
                "externalIds": {
                    "CorpusId": 259114343
                },
                "corpusId": 259114343,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/59b673ca75644ac112f467eb1e0374fa573b5ab0",
                "title": "L IL N ET X: L IGHTWEIGHT N ETWORKS WITH EX TREME M ODEL C OMPRESSION AND S TRUCTURED S PARSIFICATION",
                "abstract": "We introduce LilNetX, an end-to-end trainable technique for neural networks that enables learning models with specified accuracy-compression-computation tradeoff. Prior works approach these problems one at a time and often require post-processing or multistage training. Our method, on the other hand, constructs a joint training objective that penalizes the self-information of network parameters in a latent representation space to encourage small model size, while also introducing priors to increase structured sparsity in the parameter space to reduce computation. When compared with existing state-of-the-art model compression methods, we achieve up to 50% smaller model size and 98% model sparsity on ResNet-20 on the CIFAR-10 dataset as well as 31% smaller model size and 81% structured sparsity on ResNet-50 trained on ImageNet while retaining the same accuracy as these methods. The resulting sparsity can improve the inference time by a factor of almost 1 . 86 \u00d7 in comparison to a dense ResNet-50 model. Code is available at https://github.com/Sharath-girish/LilNetX .",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "143720888",
                        "name": "Sharath Girish"
                    },
                    {
                        "authorId": "145428082",
                        "name": "Kamal Gupta"
                    },
                    {
                        "authorId": "2108498897",
                        "name": "Saurabh Singh"
                    },
                    {
                        "authorId": "1781242",
                        "name": "Abhinav Shrivastava"
                    }
                ]
            }
        },
        {
            "contexts": [
                "niques have their limitations and often come at the cost of reduced accuracy [2], [5], [11]\u2013[13], [18], [21], [22], [24], [28], [29]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "32cd3af6517e69ebf3742d87de5121d1fd33fe5d",
                "externalIds": {
                    "CorpusId": 259290443
                },
                "corpusId": 259290443,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/32cd3af6517e69ebf3742d87de5121d1fd33fe5d",
                "title": "Accelerating Attention Based Models via HW-SW Co-Design using Fine-Grained Sparsification",
                "abstract": "This paper proposes FIne-Grained Sparsification (FIGS), a novel architecture for accelerating attention-based models using N:M structured sparsity. Existing hardware accelerators focus on optimizing compute to achieve ideal processing element (PE) utilization but ignore the implications of higher input bandwidth. FIGS overcomes this challenge by leveraging techniques like grouping and reusing input data to reduce required input bandwidth, achieving high PE utilization while minimizing on-chip interconnect area. The paper also proposes FIGS-Train, a sparsity training recipe that improves the accuracy of N:M structured sparse attention models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2151003017",
                        "name": "A. Bambhaniya"
                    },
                    {
                        "authorId": "2112229",
                        "name": "A. Yazdanbakhsh"
                    },
                    {
                        "authorId": "1929462",
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "authorId": "145984583",
                        "name": "T. Krishna"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In contrast, recent works propose dynamic weight training strategies where different compact subnets will be dynamically activated at each training iteration (Mocanu et al., 2018; Mostafa & Wang, 2019; Raihan & Aamodt, 2020; Evci et al., 2020; Liu et al., 2023)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8857eb437a40b9ea7bb56fa0ec06fb286286a7ea",
                "externalIds": {
                    "DBLP": "conf/icml/ZhangH0WOL0Z23",
                    "CorpusId": 259835151
                },
                "corpusId": 259835151,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8857eb437a40b9ea7bb56fa0ec06fb286286a7ea",
                "title": "When Sparsity Meets Contrastive Models: Less Graph Data Can Bring Better Class-Balanced Representations",
                "abstract": "Graph Neural Networks (GNNs) are powerful models for non-Euclidean data, but their training is often accentuated by massive unnecessary computation: On the one hand, training on non-Euclidean data has relatively high computational cost due to its irregular density properties; on the other hand, the class imbalance property often as-sociated with non-Euclidean data cannot be allevi-ated by the massiveness of the data, thus hindering the generalisation of the models. To address the above issues, theoretically, we start with a hypothesis about the effectiveness of using a subset of training data for GNNs, which is guaranteed by the gradient distance between the subset and the full set. Empirically, we also observe that a subset of the data can provide informative gradients for model optimization and which changes over time dynamically. We name this phenomenon dynamic data sparsity. Additionally, we find that pruned sparse contrastive models may \u201cmiss\u201d valuable information, leading to a large loss value on the informative subset. Motivated by the above findings, we develop a unified data model dynamic sparsity framework called Data Dec antation (DataDec) to address the above challenges. The key idea of DataDec is to identify the informative subset dynamically during the training process by applying sparse graph contrastive learning. The effectiveness of DataDec is comprehensively evaluated on graph benchmark datasets and we also verify its generalizability on image data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2155136732",
                        "name": "Chunhui Zhang"
                    },
                    {
                        "authorId": "2110926729",
                        "name": "Chao Huang"
                    },
                    {
                        "authorId": "2143725803",
                        "name": "Yijun Tian"
                    },
                    {
                        "authorId": "1818661152",
                        "name": "Qianlong Wen"
                    },
                    {
                        "authorId": "51253105",
                        "name": "Z. Ouyang"
                    },
                    {
                        "authorId": "2749392",
                        "name": "Youhuan Li"
                    },
                    {
                        "authorId": "2093920413",
                        "name": "Yanfang Ye"
                    },
                    {
                        "authorId": "2117879943",
                        "name": "Chuxu Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Evci et al. (2020b) further expand the Erdo\u030bs-Re\u0301nyi graph to convolution neural networks, demonstrating large performance improvements.",
                "\u2022 ERK (Evci et al., 2020a; Mocanu et al., 2018) initializes sparse networks with a Erdo\u030bs-Re\u0301nyi graph where small layers are usually allocated more weights.",
                "We can confirm, using our two random methods (Rand and ERK) on Figure 3, that they always yield \u03c3 \u2264 0, no matter the settings."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ed69756ce1717c1f658f52f92f81e0a4efce143c",
                "externalIds": {
                    "CorpusId": 259861290
                },
                "corpusId": 259861290,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ed69756ce1717c1f658f52f92f81e0a4efce143c",
                "title": "R EVISITING P RUNING AT I NITIALIZATION THROUGH THE L ENS OF R AMANUJAN G RAPH",
                "abstract": "Pruning neural networks at initialization (PaI) has received an upsurge of interest due to its end-to-end saving potential. PaI is able to find sparse subnetworks at initialization that can achieve comparable performance to the full networks. These methods can surpass the trivial baseline of random pruning but suffer from a significant performance gap compared to post-training pruning. Previous approaches firmly rely on weights, gradients, and sanity checks as primary signals when conducting PaI analysis. To better understand the underlying mechanism of PaI, we propose to interpret it through the lens of the Ramanujan Graph - a class of expander graphs that are sparse while being highly connected. It is often believed there should be a strong correlation between the Ramanujan graph and PaI since both are about finding sparse and well-connected neural networks. However, the finer-grained link relating highly sparse and connected networks to their relative performance ( i.e. , ranking of difference sparse structures at the same specific global sparsity) is still missing. We observe that not only the Ramanujan property for sparse networks shows no significant relationship to PaI\u2019s relative performance, but maximizing it can also lead to the formation of pseudo-random graphs with no structural meanings. We reveal the underlying cause to be Ramanujan Graph\u2019s strong assumption on the upper bound of the largest nontrivial eigenvalue ( \u02c6 \u00b5 ) of layers belonging to highly sparse networks. We hence propose Iterative Mean Difference of Bound (IMDB) as a mean to relax the \u02c6 \u00b5 upper bound. Likewise, we also show there exists a lower bound for \u02c6 \u00b5 , which we call the Normalized Random Coefficient (NaRC), that gives us an accurate assessment for when sparse but highly connected",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "1702532",
                        "name": "R. Marculescu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, sparse training (Evci et al., 2020; Yuan et al., 2021) and low-precision training (Yang et al.",
                "For example, sparse training (Evci et al., 2020; Yuan et al., 2021) and low-precision training (Yang et al., 2020; Zhao et al., 2021) are two active research areas for efficient training that can effectively reduce training costs, such as computing FLOPs and memory."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a1d2b09c099d407cb7c191a3f3f09b820d1f0cc8",
                "externalIds": {
                    "CorpusId": 259902577
                },
                "corpusId": 259902577,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a1d2b09c099d407cb7c191a3f3f09b820d1f0cc8",
                "title": "S MART FRZ: A N E FFICIENT T RAINING F RAMEWORK USING A TTENTION -B ASED L AYER F REEZING",
                "abstract": "There has been a proliferation of artificial intelligence applications, where model training is key to promising high-quality services for these applications. However, the model training process is both time-intensive and energy-intensive, inevitably affecting the user\u2019s demand for application efficiency. Layer freezing, an efficient model training technique, has been proposed to improve training efficiency. Although existing layer freezing methods demonstrate the great potential to reduce model training costs, they still remain shortcomings such as lacking generalizability and compromised accuracy. For instance, existing layer freezing methods either require the freeze configurations to be manually defined before training, which does not apply to different networks, or use heuristic freezing criteria that is hard to guarantee decent accuracy in different scenarios. Therefore, there lacks a generic and smart layer freezing method that can automatically perform \u201cinsituation\u201d layer freezing for different networks during training processes. To this end, we propose a generic and efficient training framework (SmartFRZ). The core proposed technique in SmartFRZ is attention-guided layer freezing, which can automatically select the appropriate layers to freeze without compromising accuracy. Experimental results show that SmartFRZ effectively reduces the amount of computation in training and achieves significant training acceleration, and outperforms the state-of-the-art layer freezing approaches.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "9347641",
                        "name": "Geng Yuan"
                    },
                    {
                        "authorId": "2116917804",
                        "name": "Yuezhen Dai"
                    },
                    {
                        "authorId": "2108399518",
                        "name": "Youtao Zhang"
                    },
                    {
                        "authorId": "2136922252",
                        "name": "Yanzhi Wang"
                    },
                    {
                        "authorId": "8573809",
                        "name": "Xulong Tang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent studies on unstructured pruning [96, 97] demonstrate that ResNet50 experiences significant accuracy degradation only when the pruning rate exceeds 90%."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8e67a2db1cf758fa9e49a5302e70c1c2fc0763e7",
                "externalIds": {
                    "DBLP": "phd/basesearch/Zhang23a",
                    "CorpusId": 259318075
                },
                "corpusId": 259318075,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8e67a2db1cf758fa9e49a5302e70c1c2fc0763e7",
                "title": "Software-Hardware Co-design For Deep Learning Model Acceleration",
                "abstract": "Software-Hardware Co-design For Deep Learning Model Acceleration by",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "23631324",
                        "name": "Jingchi Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While post-training quantization can be efficient and successful without any retraining (Frantar & Alistarh, 2022), in the case of pruning the gold standard is still training a separate model for every target sparsity level (Zhu & Gupta, 2017; Singh & Alistarh, 2020; Evci et al., 2020; Peste et al., 2021), which can be expensive.",
                "\u2026can be efficient and successful without any retraining (Frantar & Alistarh, 2022), in the case of pruning the gold standard is still training a separate model for every target sparsity level (Zhu & Gupta, 2017; Singh & Alistarh, 2020; Evci et al., 2020; Peste et al., 2021), which can be expensive."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "555bbc4c3d34693f5bc8e5d9de260fce2e1d8bcd",
                "externalIds": {
                    "CorpusId": 260329291
                },
                "corpusId": 260329291,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/555bbc4c3d34693f5bc8e5d9de260fce2e1d8bcd",
                "title": "C R AM: A C OMPRESSION -A WARE M INIMIZER",
                "abstract": "sparse",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3341722",
                        "name": "Alexandra Peste"
                    },
                    {
                        "authorId": "2869958",
                        "name": "Adrian Vladu"
                    },
                    {
                        "authorId": "40992614",
                        "name": "Eldar Kurtic"
                    },
                    {
                        "authorId": "1787591",
                        "name": "Christoph H. Lampert"
                    },
                    {
                        "authorId": "3311387",
                        "name": "Dan Alistarh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Therefore, it is also interesting to study how PQI are related to various pruning methods Evci et al. (2020); Hoefler et al. (2021)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "81f1c35ea36c9361d23c1c5e51eec62c9666417a",
                "externalIds": {
                    "CorpusId": 260547735
                },
                "corpusId": 260547735,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/81f1c35ea36c9361d23c1c5e51eec62c9666417a",
                "title": "P RUNING D EEP N EURAL N ETWORKS FROM A S PAR - SITY P ERSPECTIVE",
                "abstract": "In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may underprune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a limit that appears to correspond to the beginning of underfitting. Subsequently, PQI decreases again when the model collapse and significant deterioration in the performance of the model start to occur. Additionally, our experiments demonstrate that the proposed adaptive pruning algorithm with proper choice of hyper-parameters is superior to the iterative pruning algorithms such as the lottery ticket-based pruning methods, in terms of both compression efficiency and robustness. Our code is available here.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2228612435",
                        "name": "Enmao Diao"
                    },
                    {
                        "authorId": "2228132272",
                        "name": "Ganghua Wang"
                    },
                    {
                        "authorId": "2229243954",
                        "name": "Jie Ding"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is also possible to use other pruning methods such as random pruning (Evci et al., 2020; Liu et al., 2022), SNIP (Lee et al., 2018), hessian-based (Yu et al., 2021), iterative (Han et al., 2015; Frankle & Carbin, 2018) or progressive (Liu et al., 2021).",
                "It is also possible to use other pruning methods such as random pruning (Evci et al., 2020; Liu et al., 2022), SNIP (Lee et al."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "df818dcc5bd9405b50c377796ac0ed0295937125",
                "externalIds": {
                    "DBLP": "conf/icml/RoWCA23",
                    "CorpusId": 260811136
                },
                "corpusId": 260811136,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/df818dcc5bd9405b50c377796ac0ed0295937125",
                "title": "Lowering the Pre-training Tax for Gradient-based Subset Training: A Lightweight Distributed Pre-Training Toolkit",
                "abstract": "Training data and model sizes are increasing exponentially. One way to reduce training time and resources is to train with a carefully selected sub-set of the full dataset. Prior work uses the gradient signals obtained during a warm-up or \u201cpre-training\u201d phase over the full dataset, for determining the core subset; if the pre-training phase is too small, the gradients obtained are chaotic and unreliable. As a result, the pre-training phase itself incurs significant time/resource overhead, and prior work has not gone beyond hyperparam-eter search to reduce pre-training time. Our work explicitly aims to reduce this pre-training tax in gradient-based subset training. We develop a principled, scalable approach for pre-training in a distributed setup. Our approach is lightweight and minimizes communication between distributed worker nodes. It is the first to utilize the concept of model-soup based distributed training at initialization . The key idea is to minimally train an ensemble of models on small, disjointed sub-sets of the data; we further employ data-driven sparsity and data augmentation for local worker training to boost ensemble diversity. The centralized model, obtained at the end of pre-training by merging the per-worker models, is found to offer stabilized gradient signals to select subsets, on which the main model is further trained. We have validated the effectiveness of our method through extensive experiments on CIFAR-10/100, and ImageNet, using ResNet and WideResNet models. For example, our approach is shown to achieve 15.4 \u00d7 pre-training speedup and 2.8 \u00d7 end-to-end speedup on CIFAR10 and ResNet18 without loss of accuracy. The code is at https: //github.com/moonbucks/LiPT.git .",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2462957",
                        "name": "Yeonju Ro"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "2002462",
                        "name": "Vijay Chidambaram"
                    },
                    {
                        "authorId": "152426179",
                        "name": "Aditya Akella"
                    }
                ]
            }
        },
        {
            "contexts": [
                "All further hyperparameters of RigL are adopted from [4].",
                "As described in [4], RigL performes better with a longer training duration."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f51a0a0fbb0ec5c229d679f360de9f62cd99560f",
                "externalIds": {
                    "CorpusId": 261060212
                },
                "corpusId": 261060212,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f51a0a0fbb0ec5c229d679f360de9f62cd99560f",
                "title": "Supplementary Material to HyperSparse Neural Networks: Shifting Exploration to Exploitation through Adaptive Regularization",
                "abstract": "This document provides supplementary material for the paper HyperSparse Neural Networks: Shifting Exploration to Exploitation through Adaptive Regularization . At first, Sec. A gives detailed information about the implementation of our method. Subsequently, Sec. B presents more detailed results of the intersection of largest weights during training and the final pruning mask. The weight distribution after training with our introduced method shown in the main paper is analyzed for a wider set of configurations in Sec. C. Moreover, Sec. D and Sec. E elaborate the gradient and the compression behaviour during regularization presented in the main paper more into detail",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2127946406",
                        "name": "Patrick Glandorf"
                    },
                    {
                        "authorId": "2191607674",
                        "name": "Timo Kaiser"
                    },
                    {
                        "authorId": "1779035",
                        "name": "B. Rosenhahn"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "085e7f588aa922f82480a3c197f6dd80abe43350",
                "externalIds": {
                    "CorpusId": 261076565
                },
                "corpusId": 261076565,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/085e7f588aa922f82480a3c197f6dd80abe43350",
                "title": "P RUNING D EEP N EURAL N ETWORKS FROM A S PAR - SITY P ERSPECTIVE",
                "abstract": "In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may underprune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a limit that appears to correspond to the beginning of underfitting. Subsequently, PQI decreases again when the model collapse and significant deterioration in the performance of the model start to occur. Additionally, our experiments demonstrate that the proposed adaptive pruning algorithm with proper choice of hyper-parameters is superior to the iterative pruning algorithms such as the lottery ticket-based pruning methods, in terms of both compression efficiency and robustness. Our code is available here.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2228612435",
                        "name": "Enmao Diao"
                    },
                    {
                        "authorId": "2228132272",
                        "name": "Ganghua Wang"
                    },
                    {
                        "authorId": "2107988208",
                        "name": "Jiawei Zhang"
                    },
                    {
                        "authorId": "2233113808",
                        "name": "Yuhong Yang"
                    },
                    {
                        "authorId": "2229243954",
                        "name": "Jie Ding"
                    },
                    {
                        "authorId": "101661851",
                        "name": "Vahid Tarokh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "1 Comparing layerwise pruning ratios achieved on WideResNet28 (blue) and DenseNet40 (orange) using Erdos-Renyi Kernel layerwise [18], Layer-adaptive Sparsity for the Magnitude-based Pruning (LAMP) [51], and uniform (baseline) schemes.",
                "1: Comparing layerwise pruning ratios achieved on WideResNet28 (blue) and DenseNet40 (orange) using Erdos-Renyi Kernel layerwise [18], Layer-adaptive Sparsity for the Magnitude-based Pruning (LAMP) [51], and uniform (baseline) schemes."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "8c97c06ff01eeb76a9031c7975a8f38547531971",
                "externalIds": {
                    "CorpusId": 263309094
                },
                "corpusId": 263309094,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8c97c06ff01eeb76a9031c7975a8f38547531971",
                "title": "Layerwise Training of Deep Neural Networks",
                "abstract": "Layerwise Training of Deep Neural Networks",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2249329163",
                        "name": "Elicia Ye"
                    },
                    {
                        "authorId": "2249354291",
                        "name": "Tianyu Pang"
                    },
                    {
                        "authorId": "2249346772",
                        "name": "Alex Zhao"
                    },
                    {
                        "authorId": "2111405998",
                        "name": "Yefan Zhou"
                    },
                    {
                        "authorId": "2249529142",
                        "name": "Yaoqing Yang"
                    },
                    {
                        "authorId": "2249392052",
                        "name": "Michael Mahoney"
                    },
                    {
                        "authorId": "2066489854",
                        "name": "K. Ramchandran"
                    }
                ]
            }
        },
        {
            "contexts": [
                "point out that all these algorithms can be expressed with a drop-and-grow framework and propose RigL which uses the gradient magnitude as the grow criterion [8].",
                "To reduce the cost, there is a growing interest in developing sparse training algorithms [30, 2, 8, 18, 26, 27].",
                "Evci et al. point out that all these algorithms can be expressed with a drop-and-grow framework and propose RigL which uses the gradient magnitude as the grow criterion [8].",
                "Compared with nonstructured dynamic sparse training (RigL [8]), our DSB has slightly lower accuracy at 0.5 and 0.6 sparsity, and the accuracy gap widens as the sparsity gets higher.",
                "An issue with RigL is that it needs to compute the full gradient every few iterations, which may not be affordable for large models.",
                "Following previous dynamic sparse training algorithms [8, 26, 27, 18], we select the k weights with the smallest magnitudes (i.",
                "Different from previous work which selects new parameters based on dense gradients [8] or dense weights [18], we select blocks of new parameters directly based on the input value and output gradient of each layer, making our algorithm purely sparse.",
                "Compared with nonstructured dynamic sparse training (RigL [8]), our DSB has slightly lower accuracy at 0.",
                "For example, RigL [8] drops the weights with the smallest magnitudes and grows the same amount of weights with the largest gradients.",
                "Different from previous work that computes the gradients for all weights and adds the weights with the largest gradients[8, 26, 27], we use dOut and In to estimate the importance of weights."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "86ee946179119b65c57171d8a2ddaa1eebc0e7ed",
                "externalIds": {
                    "DBLP": "conf/nips/0004HS22",
                    "CorpusId": 253513780
                },
                "corpusId": 253513780,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/86ee946179119b65c57171d8a2ddaa1eebc0e7ed",
                "title": "Exposing and Exploiting Fine-Grained Block Structures for Fast and Accurate Sparse Training",
                "abstract": "Sparse training is a popular technique to reduce the overhead of training large models. Although previous work has shown promising results for nonstructured sparse models, it is still unclear whether a sparse model with structural constraints can be trained from scratch to high accuracy. In this work, we study the dynamic sparse training for a class of sparse models with shuffled block structures. Compared to nonstructured models, such fine-grained structured models are more hardware-friendly and can effectively accelerate the training process. We propose an algorithm that keeps adapting the sparse model while maintaining the active parameters in shuffled blocks. We conduct experiments on a variety of networks and datasets and obtain positive results. In particular, on ImageNet, we achieve dense accuracy for ResNet50 and ResNet18 at 0.5 sparsity. On CIFAR10/100, we show that dense accuracy can be recovered at 0.6 sparsity for various models. At higher sparsity, our algorithm can still match the accuracy of nonstructured sparse training in most cases, while reducing the training time by up to 5x due to the fine-grained block structures in the models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2061282438",
                        "name": "Peng Jiang"
                    },
                    {
                        "authorId": "73543136",
                        "name": "Lihan Hu"
                    },
                    {
                        "authorId": "8924445",
                        "name": "Shihui Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use the same sparsity initialization method ERK in the state-of-the-art sparse training method such as RigL (Evci et al. 2020) and ITOP (Liu et al.",
                "2020), SIS (Verma and Pesquet 2021) and RigL (Evci et al. 2020), respectively.",
                "To improve the flexibility, dynamic mask training has been proposed (Mocanu et al. 2018; Mostafa and Wang 2019; Evci et al. 2020; Liu et al. 2021b; Ma et al. 2021), where the sparse mask is periodically updated by drop-andgrow to search for better subnetworks with high accuracy, where in the drop\u2026",
                "RigL (Evci et al. 2020) updated the sparsity topology of the sparse network during training using the same magnitude-based weights dropping method while growing back the weights using top-k absolute largest gradients, achieving better accuracy than static mask training under same sparsity.",
                "\u2026mask training baselines while adopting DeepR (Bellec et al. 2018), SNFS (Dettmers and Zettlemoyer 2019), DSR (Mostafa and Wang 2019), SET (Mocanu et al. 2018), RigL (Evci et al. 2020), MEST (Yuan et al. 2021), RigL-ITOP (Liu et al. 2021b) as the dynamic mask training baselines as shown in Table 2.",
                "We use the same sparsity initialization method ERK in the state-of-the-art sparse training method such as RigL (Evci et al. 2020) and ITOP (Liu et al. 2021b).",
                "To improve the flexibility, dynamic mask training has been proposed (Mocanu et al. 2018; Mostafa and Wang 2019; Evci et al. 2020; Liu et al. 2021b; Ma et al. 2021), where the sparse mask is periodically updated by drop-andgrow to search for better subnetworks with high accuracy, where in the drop process we deactivate a portion of weights from active states (nonzero) to non-active states (zero), vice versa for the growing process.",
                "\u20266.5%, 6.4%, 6.5%, 4.9% and 1.4% higher accuracy performance at 98% sparsity ratio compared to SNIP (Lee, Ajanthan, and Torr 2019), GraSP (Wang, Zhang, and Grosse 2020), SynFlow (Tanaka et al. 2020), STR (Kusupati et al. 2020), SIS (Verma and Pesquet 2021) and RigL (Evci et al. 2020), respectively."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7d87152d834a8537289825aef3d612bca901e028",
                "externalIds": {
                    "CorpusId": 254636641
                },
                "corpusId": 254636641,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7d87152d834a8537289825aef3d612bca901e028",
                "title": "Dynamic Sparse Training via More Exploration",
                "abstract": "Over-parameterization of deep neural networks (DNNs) has shown high prediction accuracy for many applications. Al-though effective, the large number of parameters hinders its popularity on resource-limited devices and has an outsize en-vironmental impact. Sparse training (using a \ufb01xed number of nonzero weights in each iteration) could signi\ufb01cantly mitigate the training costs by reducing the model size. However, exist- ing sparse training methods mainly use either random-based or greedy-based drop-and-grow strategies, resulting in local min- imal and low accuracy. In this work, we consider the dynamic sparse training as a sparse connectivity search problem and design an exploitation and exploration acquisition function to escape from local optima and saddle points. We further design an acquisition function and provide the theoretical guarantees for the proposed method and clarify its convergence prop- erty. Experimental results show that sparse models (up to 98% sparsity) obtained by our proposed method outperform the SOTA sparse training methods on a wide variety of deep learning tasks. On VGG-19 / CIFAR-100, ResNet-50 / CIFAR-10, ResNet-50 / CIFAR-100, our method has even higher accuracy than dense models. On ResNet-50 / ImageNet, the proposed method has up to 8.2% accuracy improvement compared to SOTA sparse training methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2122804649",
                        "name": "Shaoyi Huang"
                    },
                    {
                        "authorId": "2144399315",
                        "name": "Bowen Lei"
                    },
                    {
                        "authorId": "2116459424",
                        "name": "Dongkuan Xu"
                    },
                    {
                        "authorId": "144490597",
                        "name": "Hongwu Peng"
                    },
                    {
                        "authorId": "2116969722",
                        "name": "Yue Sun"
                    },
                    {
                        "authorId": "3197711",
                        "name": "Mimi Xie"
                    },
                    {
                        "authorId": "2881873",
                        "name": "Caiwen Ding"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Post-training pruning methods have a high computational cost because they require training an overparameterized dense model first, while pruning from scratch methods have shown promise, but have been outperformed by dynamic sparsity methods such as Rigging the Lottery (RigL) (Evci et al., 2020).",
                "RigL (Evci et al., 2020) and Sparse Network From Scratch (SNFS) (Dettmers & Zettlemoyer, 2019) use a cosine annealing schedule, while Zhu & Gupta (2017); Mostafa & Wang (2019) use a cubic schedule.",
                "ER was adapted to Erd\u0151s R\u00e8nyiKernel (ERK) ratios by (Evci et al., 2020), to work better with Convolutional Neural Networks (CNNs).",
                "ER was adapted to Erdo\u030bs Re\u0300nyiKernel (ERK) ratios by (Evci et al., 2020), to work better with Convolutional Neural Networks (CNNs)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a2372952f39540fa3197823191c8ba4c5857c271",
                "externalIds": {
                    "CorpusId": 251733832
                },
                "corpusId": 251733832,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a2372952f39540fa3197823191c8ba4c5857c271",
                "title": "Just-in-Time Sparsity: Learning Dynamic Sparsity Schedules",
                "abstract": "Sparse neural networks have various computa-tional benefits while often being able to main-tain or improve the generalization performance of their dense counterparts. Popular sparsifica-tion methods have focused on what to sparsify , i.e. which redundant components to remove from neural networks, while when to sparsify , has re-ceived less attention and is usually handled using heuristics or simple schedules. In this work, we focus on learning sparsity schedules from scratch using reinforcement learning. In simple CNNs and ResNet-18, we show that our learned schedules are diverse across layers and training steps, while achieving competitive performance when compared to naive handcrafted schedules. Our methodology is general-purpose and can be applied to learning effective sparsity schedules across any pruning implementation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2047584985",
                        "name": "Kale-ab Tessera"
                    },
                    {
                        "authorId": "2182369023",
                        "name": "Chiratidzo Matowe"
                    },
                    {
                        "authorId": "38166712",
                        "name": "Arnu Pretorius"
                    },
                    {
                        "authorId": "2831294",
                        "name": "Benjamin Rosman"
                    },
                    {
                        "authorId": "50237813",
                        "name": "Sara Hooker"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The only existing prior work on unstructured ViT pruning is SViTE [Chen et al., 2021], which applied the general RigL pruning method [Evci et al., 2020] to the special case of ViT models.",
                "Many other criteria exist, such as gradient magnitude [Evci et al., 2020] or \u201crates of change\u201d in the weights [Sanh et al.",
                "Future work should also be able to extend our pruner to structured compression of ViT models, or employ our oViT pruner inside different, more computationally-intensive pruning algorithms such as RigL [Evci et al., 2020].",
                "Many other criteria exist, such as gradient magnitude [Evci et al., 2020] or \u201crates of change\u201d in the weights [Sanh et al., 2020].",
                ", 2021], which applied the general RigL pruning method [Evci et al., 2020] to the special case of ViT models.",
                "We compare with global magnitude (GM) following the same schedule as oViT, as well as the state-of-the-art SViTE [Chen et al., 2021] paper, which trains the sparse model from scratch using a variant of RigL [Evci et al., 2020], but for a total of 600 epochs.",
                "In addition to the sparse training from scratch with periodic updates of the sparsity weights with some salincy criterion for weight elimination and regrowth [Evci et al., 2020] one can consider alternating compressed/decompressed training (AC/DC), proposed in [Peste et al., 2021].",
                ", 2021] paper, which trains the sparse model from scratch using a variant of RigL [Evci et al., 2020], but for a total of 600 epochs."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "5c0148075627f5c5c7c0f3fd4d6041b929c3c8d0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-09223",
                    "DOI": "10.48550/arXiv.2210.09223",
                    "CorpusId": 252918510
                },
                "corpusId": 252918510,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5c0148075627f5c5c7c0f3fd4d6041b929c3c8d0",
                "title": "oViT: An Accurate Second-Order Pruning Framework for Vision Transformers",
                "abstract": "Models from the Vision Transformer (ViT) family have recently provided break-through results across image classi\ufb01cation tasks such as ImageNet. Yet, they still face barriers to deployment, notably the fact that their accuracy can be severely impacted by compression techniques such as pruning. In this paper, we take a step towards addressing this issue by introducing Optimal ViT Surgeon (oViT) , a new state-of-the-art method for the weight sparsi\ufb01cation of Vision Transformers (ViT) models. At the technical level, oViT introduces a new weight pruning algorithm which leverages second-order information, speci\ufb01cally adapted to be both highly-accurate and ef\ufb01cient in the context of ViTs. We complement this accurate one-shot pruner with an in-depth investigation of gradual pruning, augmentation, and recovery schedules for ViTs, which we show to be critical for successful ViT compression. We validate our method via extensive experiments on classical ViT and DeiT models, as well as on newer variants, such as XCiT, Ef\ufb01cientFormer and Swin. Moreover, our results are even relevant to recently-proposed highly-accurate ResNets. Our results show for the \ufb01rst time that ViT-family models can in fact be pruned to high sparsity levels (e.g. \u2265 75% ) with low impact on accuracy ( \u2264 1% relative drop), and that our approach outperforms prior methods by signi\ufb01cant margins at high sparsities. In addition, we show that our method is compatible with structured pruning methods and quantization, and that it can lead to signi\ufb01cant speedups on a sparsity-aware inference engine.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2006108901",
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "authorId": "40992614",
                        "name": "Eldar Kurtic"
                    },
                    {
                        "authorId": "1502248377",
                        "name": "Elias Frantar"
                    },
                    {
                        "authorId": "3311387",
                        "name": "Dan Alistarh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The other category is dynamic sparse training (Mocanu et al., 2018; Dettmers & Zettlemoyer, 2019; Evci et al., 2020).",
                "Inspired by RigL (Evci et al., 2020), devices only rate partial model parameters (e.",
                "Although RigL Evci et al. (2020) tries to reduce memory consumption, it needs to compute gradients for all parameters, which is computationally expensive and may lead to straggling issues in federated learning\nFederated Neural Network Pruning.",
                "After pruning, we grow the pruned parameters with the largest gradient magnitude, like RigL (Evci et al., 2020).",
                "Such negative impact becomes more challenging when pruning towards an extremely tiny subnetwork, as the biased initial subnetwork can deviate significantly from the optimal structure, resulting in poor accuracy (Evci et al., 2020).",
                "Inspired by RigL (Evci et al., 2020), devices only rate partial model parameters (e.g., a single layer) at a time, where the topK importance scores are stored locally and uploaded to the server, significantly reducing memory, computation, and communication costs."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "ef83b45faaa2b41ec439a995a145453846177d46",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-01977",
                    "DOI": "10.48550/arXiv.2212.01977",
                    "CorpusId": 254246414
                },
                "corpusId": 254246414,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ef83b45faaa2b41ec439a995a145453846177d46",
                "title": "FedTiny: Pruned Federated Learning Towards Specialized Tiny Models",
                "abstract": "Neural network pruning has been a well-established compression technique to enable deep learning models on resource-constrained devices. The pruned model is usually specialized to meet specific hardware platforms and training tasks (defined as deployment scenarios). However, existing pruning approaches rely heavily on training data to trade off model size, efficiency, and accuracy, which becomes ineffective for federated learning (FL) over distributed and confidential datasets. Moreover, the memoryand compute-intensive pruning process of most existing approaches cannot be handled by most FL devices with resource limitations. In this paper, we develop FedTiny, a novel distributed pruning framework for FL, to obtain specialized tiny models for memoryand computing-constrained participating devices with confidential local data. To alleviate biased pruning due to unseen heterogeneous data over devices, FedTiny introduces an adaptive batch normalization (BN) selection module to adaptively obtain an initially pruned model to fit deployment scenarios. Besides, to further improve the initial pruning, FedTiny develops a lightweight progressive pruning module for local finer pruning under tight memory and computational budgets, where the pruning policy for each layer is gradually determined rather than evaluating the overall deep model structure. Extensive experimental results demonstrate the effectiveness of FedTiny, which outperforms state-of-the-art baseline approaches, especially when compressing deep models to extremely sparse tiny models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2194197373",
                        "name": "Hong Huang"
                    },
                    {
                        "authorId": "2131620609",
                        "name": "Lan Zhang"
                    },
                    {
                        "authorId": "2193707949",
                        "name": "Chaoyue Sun"
                    },
                    {
                        "authorId": "2666471",
                        "name": "R. Fang"
                    },
                    {
                        "authorId": "152162529",
                        "name": "Xiaoyong Yuan"
                    },
                    {
                        "authorId": "2152704660",
                        "name": "Dapeng Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note, centralized training has shown significant benefits with sparse learning with FLOPs reduction during forward operations [8], and potential training speed-up of up to 3.",
                "FedDST [2], on the other hand, leveraged the idea of RigL [8] to perform sparse learning of the clients and relied on magnitude pruning at the server-side that does not necessarily adhere to the layer sensitivity towards a target density.",
                "More recently, sparse learning [5, 8, 19, 32], a popular form of model pruning, has gained significant traction due to its popularity in yielding FLOPs advantage and potential speed-up even during training.",
                "In particular, recently proposed sparse learning strategies [5, 8, 19, 30, 32] effectively train weights and associated binary sparse masks to allow only a fraction of model parameters to be updated during training, potentially enabling the lucrative reduction in both the training time and FLOPs [31, 32], while creating a model to meet a target parameter density denoted as d, and is able to yield accuracy close to that of the unpruned baseline."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e1ed55d37f6fa54cdf12d2b6bcb975d3adea0ed8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-13092",
                    "DOI": "10.48550/arXiv.2208.13092",
                    "CorpusId": 251903665
                },
                "corpusId": 251903665,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e1ed55d37f6fa54cdf12d2b6bcb975d3adea0ed8",
                "title": "Federated Sparse Training: Lottery Aware Model Compression for Resource Constrained Edge",
                "abstract": "Limited computation and communication capabilities of clients pose signi\ufb01cant challenges in federated learning (FL) over resource-limited edge nodes. A potential solution to this problem is to deploy off-the-shelf sparse learning algorithms that train a binary sparse mask on each client with the expectation of training a consistent sparse server mask. However, as we investigate in this paper, such naive deployments result in a signi\ufb01cant accuracy drop compared to FL with dense models, especially under low client\u2019s resource budget. In particular, our investiga-tions reveal a serious lack of consensus among the trained masks on clients, which prevents convergence on the server mask and potentially leads to a substantial drop in model performance. Based on such key observations, we propose federated lottery aware sparsity hunting (FLASH), a uni\ufb01ed sparse learning framework to make the server win a lottery in terms of a sparse sub-model, which can greatly improve performance under highly resource-limited client settings. Moreover, to address the issue of device heterogeneity, we leverage our \ufb01ndings to propose hetero-FLASH , where clients can have different target sparsity budgets based on their device resource limits. Extensive experimental evaluations with multiple models on various datasets (both IID and non-IID) show superiority of our models in yielding up to \u223c 10 . 1% improved accuracy with \u223c 10 . 26 \u00d7 fewer communication costs, compared to existing alternatives, at similar hyperparameter settings.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2098816244",
                        "name": "Sara Babakniya"
                    },
                    {
                        "authorId": "2965493",
                        "name": "Souvik Kundu"
                    },
                    {
                        "authorId": "47592370",
                        "name": "Saurav Prakash"
                    },
                    {
                        "authorId": "144829346",
                        "name": "Yue Niu"
                    },
                    {
                        "authorId": "121011351",
                        "name": "S. Avestimehr"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In particular, initial balanced or pyramidal sparsity ratios seem to be able to improve the performance of RiGL.",
                "; Evci et al., 2020a;b).",
                "Dynamical Sparse Training In order to improve the expressiveness of ER networks and achieve extremely sparse WLTs, ER networks can be rewired with the help of DST. Specifically, we use the algorithm RiGL (Evci et al., 2020a).",
                "This way, we also provide a missing theoretical foundation for dynamic sparse training approaches (Evci et al., 2020a; Liu et al., 2021.",
                "This insight presents a theoretical justification for pruning approaches that start from random ER masks like Dynamic Sparse Training (Evci et al., 2020a; Mocanu et al., 2018)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "e0ce7e0befcf7554f3db3ddb50079bf59b548906",
                "externalIds": {
                    "CorpusId": 252715754
                },
                "corpusId": 252715754,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e0ce7e0befcf7554f3db3ddb50079bf59b548906",
                "title": "H OW E RD \u00a8 OS AND R \u00b4 ENYI W IN THE L OTTERY",
                "abstract": "Random masks define surprisingly effective sparse neural network models, as has been shown empirically. The resulting Erd\u00f6s-R\u00e9nyi (ER) random graphs can often compete with dense architectures and state-of-the-art lottery ticket pruning algorithms struggle to outperform them, even though the random baselines do not rely on computationally expensive pruning-training iterations but can be drawn initially without significant computational overhead. We offer a theoretical explanation of how such ER masks can approximate arbitrary target networks if they are wider by a logarithmic factor in the inverse sparsity 1/ log(1/sparsity). While we are the first to show theoretically and experimentally that random ER source networks contain strong lottery tickets, we also prove the existence of weak lottery tickets that require a lower degree of overparametrization than strong lottery tickets. These unusual results are based on the observation that ER masks are well trainable in practice, which we verify in experiments with varied choices of random masks. Some of these data-free choices outperform previously proposed random approaches on standard image classification benchmark datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2218573299",
                        "name": "R\u00e9nyi Win"
                    },
                    {
                        "authorId": "2218578381",
                        "name": "The Lottery"
                    },
                    {
                        "authorId": "2048027747",
                        "name": "Advait Gadhikar"
                    },
                    {
                        "authorId": "79760097",
                        "name": "Sohom Mukherjee"
                    },
                    {
                        "authorId": "8813467",
                        "name": "R. Burkholz"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b63adccffb476b94ca3bec8e971fc8cd012d61fa",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-12826",
                    "CorpusId": 246430947
                },
                "corpusId": 246430947,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b63adccffb476b94ca3bec8e971fc8cd012d61fa",
                "title": "Optimizing Gradient-driven Criteria in Network Sparsity: Gradient is All You Need",
                "abstract": "Network sparsity receives popularity mostly due to its capability to reduce the network complexity. Extensive studies excavate gradient-driven sparsity. Typically, these methods are constructed upon premise of weight independence, which however, is contrary to the fact that weights are mutually in\ufb02uenced. Thus, their performance remains to be improved. In this paper, we propose to further optimize gradient-driven sparsity (OptG) by solving this independence paradox. Our motive comes from the recent advances on supermask training which shows that sparse subnetworks can be located in a randomly initialized network by simply updating mask values without modifying any weight. We prove that supermask training is to accumulate the weight gradients and can partly solve the independence paradox. Consequently, OptG integrates supermask training into gradient-driven sparsity, and a specialized mask optimizer is designed to solve the independence paradox. Experiments show that OptG can well surpass many existing state-of-the-art competitors. Our code is available at https://github.com/zyxxmu/OptG .",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108078624",
                        "name": "Yu-xin Zhang"
                    },
                    {
                        "authorId": "49352079",
                        "name": "Mingbao Lin"
                    },
                    {
                        "authorId": "2133595441",
                        "name": "Mengzhao Chen"
                    },
                    {
                        "authorId": "48559591",
                        "name": "Zi-Han Xu"
                    },
                    {
                        "authorId": "2070266415",
                        "name": "Fei Chao"
                    },
                    {
                        "authorId": "1862967",
                        "name": "Yunhang Shen"
                    },
                    {
                        "authorId": "2149140038",
                        "name": "Ke Li"
                    },
                    {
                        "authorId": "47096329",
                        "name": "Yongjian Wu"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026Training (Mocanu et al., 2018; Liu et al., 2021a) which explores the sparsity pattern in a prune-and-grow scheme according to some criteria (Mocanu et al., 2018; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020a; Ye et al., 2020; Jayakumar et al., 2021; Liu et al., 2021b).",
                "Apart from pre-specified pruning rate, pruning ratio can be varied for different layers such as Erdo\u0308-Re\u0301nyi (Mocanu et al., 2018) and Erdo\u0308-Re\u0301nyi Kernel (Evci et al., 2020a).",
                "As suggested in (Evci et al., 2020b), scaling after pruning preserves the gradient flow of the neural network."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "6291e9976067bbed346b9e50dac17225e8d1b6f1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-14328",
                    "DOI": "10.48550/arXiv.2203.14328",
                    "CorpusId": 247763112
                },
                "corpusId": 247763112,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6291e9976067bbed346b9e50dac17225e8d1b6f1",
                "title": "On the Neural Tangent Kernel Analysis of Randomly Pruned Wide Neural Networks",
                "abstract": "We study the behavior of ultra-wide neural networks when their weights are randomly pruned at the initialization, through the lens of neural tangent kernels (NTKs). We show that for fullyconnected neural networks when the network is pruned randomly at the initialization, as the width of each layer grows to infinity, the empirical NTK of the pruned neural network converges to that of the original (unpruned) network with some extra scaling factor. Further, if we apply some appropriate scaling after pruning at the initialization, the empirical NTK of the pruned network converges to the exact NTK of the original network, and we provide a non-asymptotic bound on the approximation error in terms of pruning probability. Moreover, when we apply our result to an unpruned network (i.e., we set the probability of pruning a given weight to be zero), our analysis is optimal up to a logarithmic factor in width compared with the result in (Arora et al., 2019). We conduct experiments to validate our theoretical results. We further test our theory by evaluating random pruning across different architectures via image classification on MNIST and CIFAR-10 and compare its performance with other pruning strategies.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118571035",
                        "name": "Hongru Yang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In recent studies, some sparse networks not only decrease storage and computational requirements but also achieve higher inference scores than dense networks [10], suggesting the potential utility of sparse structure in decreasing the overfit."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e8b8ccf12591cf93e3ae0d09acc04338308878f5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-02227",
                    "DOI": "10.48550/arXiv.2204.02227",
                    "CorpusId": 247958188
                },
                "corpusId": 247958188,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e8b8ccf12591cf93e3ae0d09acc04338308878f5",
                "title": "When Sparsity Meets Dynamic Convolution",
                "abstract": "Dynamic convolution achieves a substantial performance boost for ef\ufb01cient CNNs at a cost of increased convolutional weights. Contrastively, mask-based unstructured pruning obtains a lightweight network by removing redun-dancy in the heavy network at risk of performance drop. In this paper, we propose a new framework to coherently integrate these two paths so that they can complement each other compensate for the disadvantages. We \ufb01rst design a binary mask derived from a learnable threshold to prune static kernels, signi\ufb01cantly reducing the parameters and computational cost but achieving higher performance in Imagenet-1K(0.6% increase in top-1 accuracy with 0.67G fewer FLOPs). Based on this learnable mask, we further propose a novel dynamic sparse network incorporating the dynamic routine mechanism, which exerts much higher accuracy than baselines ( 2 . 63% increase in top-1 accuracy for MobileNetV1 with 90% sparsity). As a result, our method demonstrates a more ef\ufb01cient dynamic convolution with sparsity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152235390",
                        "name": "Shwai He"
                    },
                    {
                        "authorId": "1527101214",
                        "name": "Yuhang Li"
                    },
                    {
                        "authorId": "2161510458",
                        "name": "Chenbo Jiang"
                    },
                    {
                        "authorId": "2057717421",
                        "name": "Shi Gu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Google AI group proposed this modified version of Erd\u0151s\u2013R\u00e9nyi algorithm (Evci et al. 2020), originally published by Mocanu et al.",
                "Google AI group proposed this modified version of Erd\u0151s\u2013R\u00e9nyi algorithm (Evci et al. 2020), originally published by Mocanu et al. 2018) for pruning convolutional layers in neural networks.",
                "It was further improved it with a training scheme for sparsely initialized neural networks, where the layerwise sparsity is determined by the Erd\u0151s\u2013R\u00e9nyi Kernel (ERK) method (Evci et al. 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "082b0eb811d9b60f968ca8c784d0879f8afbf660",
                "externalIds": {
                    "CorpusId": 249455412
                },
                "corpusId": 249455412,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/082b0eb811d9b60f968ca8c784d0879f8afbf660",
                "title": "LSOP: Layer-Scaled One-shot Pruning A Simple and Effective Deep Pruning Framework for Neural Networks",
                "abstract": "Neural network pruning is a technique that removes unnecessary weight parameters from a network to decrease its memory and computational requirements. Many different pruning techniques have been proposed to reduce networks with over 90% shrinkage in size while minimizing accuracy loss. This paper aims to establish a framework that can generalize the mechanism among various pruning techniques, which can be used to guide users to design better deep pruning methods in the future. With some basic concepts and findings from data matrix approximation, the framework can explain the success of the state-of-the-art methods as well as a more generalized pruning design (Layer-Scaled One-shot Pruning, LSOP) proposed in this work. After pruning with different algorithms and measur-ing their accuracies, the researcher also found that those methods or algorithms aligned with the proposed framework were more accurate at sparser networks (density < 10%) than the methods that did not. This suggests that future research into neural network pruning can focus on the proposed framework, which has the potential to acceler-ate the development of pruning technology and adoption of more efficient neural networks. The LSOP framework\u2019s capability to explain strong pruning performances implies that polynomial decay and Low-Rank Matrix Approximation techniques from the field of data science can provide support for neural network pruning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39231399",
                        "name": "Oliver Wang"
                    },
                    {
                        "authorId": "144734756",
                        "name": "C. Dovrolis"
                    },
                    {
                        "authorId": null,
                        "name": "Jaeho Lee"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a33a712e686fd96cfbdd4e703493337f576aff2c",
                "externalIds": {
                    "CorpusId": 249951402
                },
                "corpusId": 249951402,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a33a712e686fd96cfbdd4e703493337f576aff2c",
                "title": "Appendix Comparison With MTL Methods",
                "abstract": "As mentioned in the Empricial Evaluation Section, DiSparse surpasses several dedicated multitask learning approaches despite the high sparsity enforced in our model. In Table 1, we show comparison of DiSparse in both static and dynamic sparse training setting with several MTL approaches including DEN [1], Sluice [10], and CrossStitch [8] applied on exactly the same model with the same optimization settings. The superiroty of DiSparse is clearly observed in the table, demonstrating that DiSparse is not only an effective compression approach but also a powerful tool for multitask learning.",
                "year": 2022,
                "authors": []
            }
        },
        {
            "contexts": [
                "Older works maintained a static graph [28] and dealt only with feedforward networks but newer methods such as dynamic sparse training (DST)[5, 22] have been proposed for both feedforward networks and RNNs which dynamically improve the sparse graph and provide better performance."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c71d00506001cbdda71b8015c61c261c32b7aed0",
                "externalIds": {
                    "CorpusId": 250408075
                },
                "corpusId": 250408075,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c71d00506001cbdda71b8015c61c261c32b7aed0",
                "title": "Winning the lottery with neurobiology: faster learning on many cognitive tasks with \ufb01xed sparse RNNs",
                "abstract": "RNNs are often used as models of biological brain circuits and can solve a variety of dif\ufb01cult problems requiring memory, error-correction, or selection [10, 26, 25]. However, fully-connected RNNs contrast structurally with their biological counterparts, which are extremely sparse ( \u223c 0 . 1 %). Practical deployment of large RNNs is often limited due to requirements of long training times and large memory requirements. Motivated by brains, where neural connectivity is constrained by distance along cortical sheets and other synaptic wiring costs, we introduce locality masked RNNs (LM-RNNs) that utilize task-agnostic predetermined graphs with sparsity as low as 4%. We make three contributions: First, we show that LM-RNNs can perform as well as their fully-connected counterparts, without a posteriori construction of the best sparse subnetwork. Second, we \ufb01nd that LM-RNNs train faster with more data-ef\ufb01ciency in a multitask setting relevant to cognitive systems neuroscience, often achieving better asymptotic performance. Third, we contribute a new cognitive multi-task battery, Mod-Cog , consisting of 132 tasks that expands by \u223c 7 -fold the number of tasks and task-complexity of an existing commonly used set of tasks [39], showing that while LM-RNNs can solve the simple [39] tasks with a small pool of unconnected autapses, the expanded task-set produces richer solutions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1749175440",
                        "name": "Mikail Khona"
                    }
                ]
            }
        },
        {
            "contexts": [
                "`1 [5] erk [2] lamp [7] Ours original [12] FLOPs 168 G 287 G PSNR 20."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8169a25cdbbd4f7c57848f6eec77e74376fc5f6f",
                "externalIds": {
                    "CorpusId": 250551718
                },
                "corpusId": 250551718,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8169a25cdbbd4f7c57848f6eec77e74376fc5f6f",
                "title": "Dreaming to Prune Image Deraining Networks Supplementary Material",
                "abstract": "In this supplementary material, we provide more detailed information to further explain our proposed method. First , we introduce the details about our framework and hyper-parameters. Next , we present the intermediate results throughout our optimizing iterations to facilitate illustrating the reasons why our method works, i.e., why we can preserve the deraining capabilities of the compressed model without original data. Then , we display more experimental results to demonstrate the superiority of our method. Finally , we extend our framework to image dehazing tasks, and we are looking forward to bringing more inspiration for future studies.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2169376207",
                        "name": "Wei Zou"
                    },
                    {
                        "authorId": "2155654785",
                        "name": "Yang Wang"
                    },
                    {
                        "authorId": "3061449",
                        "name": "Xueyang Fu"
                    },
                    {
                        "authorId": "2141058868",
                        "name": "Yang Cao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There are many other ways to obtain pruned neural networks (e.g., Janowsky, 1989; LeCun et al., 1990; Han et al., 2015; Zhu & Gupta, 2017; Evci et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "bbffbf5c75c72b35c8ffc583c709eb15b871d30c",
                "externalIds": {
                    "CorpusId": 252760166
                },
                "corpusId": 252760166,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bbffbf5c75c72b35c8ffc583c709eb15b871d30c",
                "title": "Pre-Training on a Data Diet: Identifying Sufficient Examples for Early Training \u2020",
                "abstract": "A striking observation about iterative magnitude pruning (IMP; Frankle et al. 2020a) is that\u2014after just a few hundred steps of dense training\u2014the method can find a sparse sub-network that can be trained to the same accuracy as the dense network. However, the same does not hold at step 0, i.e., random initialization. In this work, we seek to understand how this early phase of pre-training leads to a good initialization for IMP through the lens of the data distribution. Empirically we observe that, holding the number of pre-training iterations constant, training on a small fraction of (randomly chosen) data suffices to obtain an equally good initialization for IMP. We additionally observe that by pre-training only on \u201ceasy\u201d training data we can decrease the number of steps necessary to find a good initialization for IMP compared to training on the full dataset or a randomly chosen subset. Combined, these results provide new insight into the role played by data in the early phase of training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1690452",
                        "name": "Mansheej Paul"
                    },
                    {
                        "authorId": "152574768",
                        "name": "Brett W. Larsen"
                    },
                    {
                        "authorId": "25769960",
                        "name": "S. Ganguli"
                    },
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "2533850",
                        "name": "G. Dziugaite"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In spirit, our effort can be likened to methods like RigL (Evci et al., 2020), which discovers the wiring of a sparse neural network during training rather than pruning a dense network post training."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "87241e898852449235c4a83d62d41bb414d1fef0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-05974",
                    "DOI": "10.48550/arXiv.2210.05974",
                    "CorpusId": 252846675
                },
                "corpusId": 252846675,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/87241e898852449235c4a83d62d41bb414d1fef0",
                "title": "Clustering Embedding Tables, Without First Learning Them",
                "abstract": "To work with categorical features, machine learning systems employ embedding tables. These tables can become exceedingly large in modern recommendation systems, necessitating the development of new methods for fitting them in memory, even during training. Some of the most successful methods for table compression are Productand Residual Vector Quantization (Gray & Neuhoff, 1998). These methods replace table rows with references to k-means clustered \u201ccodewords.\u201d Unfortunately, this means they must first know the table before compressing it, so they can only save memory during inference, not training. Recent work has used hashing-based approaches to minimize memory usage during training, but the compression obtained is inferior to that obtained by \u201cpost-training\u201d quantization. We show that the best of both worlds may be obtained by combining techniques based on hashing and clustering. By first training a hashing-based \u201csketch\u201d, then clustering it, and then training the clustered quantization, our method achieves compression ratios close to those of post-training quantization with the training time memory reductions of hashing-based methods. We show experimentally that our method provides better compression and/or accuracy that previous methods, and we prove that our method always converges to the optimal embedding table for least-squares training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2187580005",
                        "name": "Henry Ling-Hei Tsang"
                    },
                    {
                        "authorId": "3416152",
                        "name": "Thomas Dybdahl Ahle"
                    }
                ]
            }
        },
        {
            "contexts": [
                "338 [38] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ee4961f5c882aa63d48fcc732c66ee5b9ad56d0f",
                "externalIds": {
                    "CorpusId": 253162408
                },
                "corpusId": 253162408,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ee4961f5c882aa63d48fcc732c66ee5b9ad56d0f",
                "title": "Federated Learning with Online Adaptive Heterogeneous Local Models",
                "abstract": "In Federated Learning, one of the biggest challenges is that client devices often have 1 drastically different computation and communication resources for local updates. 2 To this end, recent research efforts have focused on training heterogeneous local 3 models that are obtained by adaptively pruning a shared global model. Despite the 4 empirical success, theoretical analysis of the convergence of these heterogeneous 5 FL algorithms remains an open question. In this paper, we establish sufficient 6 conditions for any FL algorithms with heterogeneous local models to converge to a 7 neighborhood of a stationary point of standard FL at a rate of O ( 1 \u221a Q ) . For general 8 smooth cost functions and under standard assumptions, our analysis illuminates 9 two key factors impacting the optimality gap between heterogeneous and standard 10 FL: pruning-induced noise and minimum coverage index, advocating a joint design 11 strategy of local models\u2019 pruning masks in heterogeneous FL algorithms. The 12 results are numerically validated on MNIST and CIFAR-10 datasets. 13 stationary point of standard FL (with a small optimality gap that is characterized in our analysis), 36 at a rate of O ( 1 \u221a Q ) in Q communication rounds. We prove a new upperbound and show that the 37 optimality gap (between heterogeneous and standard FL) is affected by both pruning-induced noise (as identified in single-model pruning) and a new notion of minimum coverage index in FL (i.e., any 39 parameters in the global model are included in at least \u0393 min local models). Our results also motivate 40 a joint design of efficient local-model pruning strategies (e.g., leveraging [10\u201312]) for heterogeneous 41 FL to have comparable convergence with standard FL. It captures a number of existing FL algorithms 42 and provides a general convergence guarantee. We perform extensive experiments on MNIST and 43 CIFAR10 datasets. Our numerical evaluations validate the sufficient conditions established in our 44 convergence analysis. 45",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2007579020",
                        "name": "Hanhan Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Past works in network pruning have explored effective techniques to find efficient subnetworks (Lee et al., 2019; Evci et al., 2020; He et al., 2022; 2023) and zero out redundant parameters.",
                "On the other hand, LTH-based (Chen et al., 2020a; Evci et al., 2020) methods can be borrowed to find the mask by several iterations, but it is prohibitively time-consuming."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4141f53e7892120c67eafe561efaacdb0156b6e1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-05528",
                    "DOI": "10.48550/arXiv.2211.05528",
                    "CorpusId": 253447201
                },
                "corpusId": 253447201,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4141f53e7892120c67eafe561efaacdb0156b6e1",
                "title": "Cherry Hypothesis: Identifying the Cherry on the Cake for Dynamic Networks",
                "abstract": "Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model\u2019s representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert given static layers into fully dynamic ones where all parameters are dynamic (at least within a single layer) and vary with the input. Recent studies empirically show the trend that the more dynamic layers contribute to ever-increasing performance. However, such a fully dynamic setting 1) may cause redundant parameters and high deployment costs, limiting the applicability of dynamic networks to a broader range of tasks and models, and more importantly, 2) contradicts the previous discovery in the human brain that when human brains process an attention-demanding task, only partial neurons in the task-speci\ufb01c areas are activated by the input, while the rest neurons leave in a baseline state. Critically, there is no e\ufb00ort to understand and resolve the above contradic-tory \ufb01nding, leaving the primal question \u2013 whether to make the computational parameters fully dynamic or not? \u2013 unanswered. The main contributions of our work are challenging the basic commonsense in dynamic networks, and, proposing and validating the cherry hypothesis \u2013 A fully dynamic network contains a subset of dynamic parameters that when transforming other dynamic parameters into static ones, can maintain or even exceed the performance of the original network. Technically, we propose a brain-inspired partially dynamic network, namely PAD-Net, to transform the redundant dynamic parameters into static ones. Also, we further design Iterative Mode Partition to partition the dynamic- and static-subnet, which alleviates the redundancy in traditional fully dynamic networks. Our hypothesis and method are comprehensively supported by large-scale experiments with two typical advanced dynamic methods, i.e., DY-Conv and MoE, on both image classi\ufb01cation and GLUE benchmarks. Encouragingly, we surpass the fully dynamic networks by +0 . 7% top-1 acc with only 30% dynamic parameters for ResNet-50 and +1 . 9% average score in language understanding tasks with only 50% dynamic parameters for BERT-base.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152235390",
                        "name": "Shwai He"
                    },
                    {
                        "authorId": "46573238",
                        "name": "Liang Ding"
                    },
                    {
                        "authorId": "2187286687",
                        "name": "Daize Dong"
                    },
                    {
                        "authorId": "2156640875",
                        "name": "Bo-Wen Liu"
                    },
                    {
                        "authorId": "46597961",
                        "name": "Fuqiang Yu"
                    },
                    {
                        "authorId": "2140448089",
                        "name": "Dacheng Tao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Table 1 compares the inference accuracy, inference FLOPS, and model size of the proposed method with pruning (Gale et al., 2019), and with two sparsity training methods: RigL (Evci et al., 2020) and MEST+EM&S (Yuan et al., 2021).",
                ", 2019), and with two sparsity training methods: RigL (Evci et al., 2020) and MEST+EM&S (Yuan et al."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "28f382e4b4f3aa899bd045389cf2a6fec416dd0a",
                "externalIds": {
                    "DBLP": "conf/icml/OkoshiGHAKCMY22",
                    "CorpusId": 250340704
                },
                "corpusId": 250340704,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/28f382e4b4f3aa899bd045389cf2a6fec416dd0a",
                "title": "Multicoated Supermasks Enhance Hidden Networks",
                "abstract": "Hidden Networks (Ramanujan et al., 2020) showed the possibility of finding accurate subnetworks within a randomly weighted neural network by training a connectivity mask, referred to as supermask. We show that the supermask stops improving even though gradients are not zero, thus underutilizing backpropagated information. To address this issue, we propose a method that extends Hidden Networks by training an overlay of multiple hierarchical supermasks\u2014 a Multicoated Supermask. This method shows that using multiple supermasks for a single task achieves higher accuracy without additional training cost. Experiments on CIFAR-10 and ImageNet show that Multicoated Supermasks enhance the tradeoff between accuracy and model size. A ResNet- 101 using a 7-coated supermask outperforms its Hidden Networks counterpart by 4% , matching the accuracy of a dense ResNet- 50 while being an order of magnitude smaller.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2159237466",
                        "name": "Yasuyuki Okoshi"
                    },
                    {
                        "authorId": "1409951890",
                        "name": "\u00c1ngel L\u00f3pez Garc\u00eda-Arias"
                    },
                    {
                        "authorId": "4607084",
                        "name": "Kazutoshi Hirose"
                    },
                    {
                        "authorId": "5280969",
                        "name": "Kota Ando"
                    },
                    {
                        "authorId": "2966151",
                        "name": "Kazushi Kawamura"
                    },
                    {
                        "authorId": "2276843",
                        "name": "Thiem Van Chu"
                    },
                    {
                        "authorId": "34091148",
                        "name": "M. Motomura"
                    },
                    {
                        "authorId": "2154957755",
                        "name": "Jaehoon Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, it would be interesting to study more pruning algorithms such as (Evci et al., 2020; Lin et al., 2020; Wang et al., 2020; Aghasi et al., 2016; Verma & Pesquet, 2021), especially iterative or inherently sparse ones that could explore structures corresponding to prohibitively large\u2026",
                "In addition, it would be interesting to study more pruning algorithms such as (Evci et al., 2020; Lin et al., 2020; Wang et al., 2020; Aghasi et al., 2016; Verma & Pesquet, 2021), especially iterative or inherently sparse ones that could explore structures corresponding to prohibitively large networks, thus solving the problem of size when embedding in fully connected space."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c8432138866db832d44f51952da3d49633c2ea30",
                "externalIds": {
                    "DBLP": "conf/icml/PellegriniB22",
                    "CorpusId": 250340597
                },
                "corpusId": 250340597,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c8432138866db832d44f51952da3d49633c2ea30",
                "title": "Neural Network Pruning Denoises the Features and Makes Local Connectivity Emerge in Visual Tasks",
                "abstract": "Pruning methods can considerably reduce the size of arti\ufb01cial neural networks without harming their performance and in some cases they can even uncover sub-networks that, when trained in isolation, match or surpass the test accuracy of their dense counterparts. Here, we characterize the inductive bias that pruning imprints in such \u201cwinning lottery tickets\u201d: focusing on visual tasks, we analyze the architecture resulting from iterative magnitude pruning of a simple fully connected network. We show that the surviving node connectivity is local in input space, and organized in patterns reminiscent of the ones found in convolutional networks. We investigate the role played by data and tasks in shaping the architecture of the pruned sub-network. We \ufb01nd that pruning performances, and the ability to sift out the noise and make local features emerge improve by increasing the size of the training set, and the semantic value of the data. We also study different pruning procedures, and \ufb01nd that iterative magnitude pruning is particularly effective in distilling meaningful connectivity out of features present in the original task. Our results suggest the possibility to auto-matically discover new and ef\ufb01cient architectural inductive biases in other datasets and tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39324024",
                        "name": "F. Pellegrini"
                    },
                    {
                        "authorId": "2188423",
                        "name": "G. Biroli"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9fc03e03200528b3a2877fa4b801f1c3bada7e2e",
                "externalIds": {
                    "DBLP": "conf/icml/Pal0KMB22",
                    "CorpusId": 250340745
                },
                "corpusId": 250340745,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9fc03e03200528b3a2877fa4b801f1c3bada7e2e",
                "title": "A Study on the Ramanujan Graph Property of Winning Lottery Tickets",
                "abstract": "Winning lottery tickets refer to sparse subgraphs of deep neural networks which have classification accuracy close to the original dense networks. Resilient connectivity properties of such sparse networks play an important role in their performance. The attempt is to identify a sparse and yet well-connected network to guarantee unhindered information flow. Connectivity in a graph is best characterized by its spectral expansion property. Ramanujan graphs are robust expanders which lead to sparse but highly-connected networks, and thus aid in studying the winning tickets. A feed-forward neural network consists of a sequence of bipartite graphs representing its layers. We analyze the Ramanujan graph property of such bipartite layers in terms of their spectral characteristics using the Cheeger\u2019s inequality for irregular graphs. It is empirically observed that the winning ticket networks preserve the Ramanujan graph property and achieve a high accuracy even when the layers are sparse. Accuracy and robustness to noise start declining as many of the layers lose the property. Next we find a robust winning lottery ticket by pruning individual layers while retaining their respective Ramanujan graph property. This strategy is observed to improve the performance of existing network pruning algorithms.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40915587",
                        "name": "Bithika Pal"
                    },
                    {
                        "authorId": "144984407",
                        "name": "A. Biswas"
                    },
                    {
                        "authorId": "1730875",
                        "name": "Sudeshna Kolay"
                    },
                    {
                        "authorId": "144240262",
                        "name": "Pabitra Mitra"
                    },
                    {
                        "authorId": "144407488",
                        "name": "B. Basu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b547fe90d2d3ad27ee7debc36ae393ccffeb254a",
                "externalIds": {
                    "DBLP": "conf/acml/PoteGHK22",
                    "CorpusId": 259092987
                },
                "corpusId": 259092987,
                "publicationVenue": {
                    "id": "2486528b-036c-4f3c-953f-c574eb381d12",
                    "name": "Asian Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "Asian Conf Mach Learn",
                        "ACML"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=40"
                },
                "url": "https://www.semanticscholar.org/paper/b547fe90d2d3ad27ee7debc36ae393ccffeb254a",
                "title": "Dynamic Forward and Backward Sparse Training (DFBST): Accelerated Deep Learning through Completely Sparse Training Schedule",
                "abstract": "Neural network sparsification has received a lot of attention in recent years. A number of dynamic sparse training methods have been developed that achieve significant sparsity levels during training, ensuring comparable performance to their dense counterparts. However, most of these methods update all the model parameters using dense gradients. To this end, gradient sparsification is achieved either by non-dynamic (fixed) schedule or computationally expensive dynamic pruning schedule. To alleviate these drawbacks, we propose Dynamic Forward and Backward Sparse Training (DFBST), an algorithm which dynamically sparsifies both the forward and backward passes using trainable masks, leading to a completely sparse training schedule. In contrast to existing sparse training methods, we propose separate learning for forward as well as backward masks. Our approach achieves state of the art performance in terms of both accuracy and sparsity compared to existing dynamic pruning algorithms on benchmark datasets, namely MNIST, CIFAR-10 and CIFAR-100.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "98888264",
                        "name": "Tejas Pote"
                    },
                    {
                        "authorId": "2186036671",
                        "name": "Muhammad Athar Ganaie"
                    },
                    {
                        "authorId": "152109227",
                        "name": "Atif Hassan"
                    },
                    {
                        "authorId": "31452294",
                        "name": "S. Khare"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several recent works (Su et al., 2020; Evci et al., 2020) empirically verify \u201clottery ticket hypothesis\u201d, i.e., there exists sub-networks (i.e., winning tickets) that can reach comparable generalization performance as the original pre-trained DNN if re-trained.",
                "Several recent works (Su et al., 2020; Evci et al., 2020) empirically verify \u201clottery ticket hypothesis\u201d, i."
            ],
            "isInfluential": false,
            "intents": [
                "result"
            ],
            "citingPaper": {
                "paperId": "156070205d9b7f0030c5a394d56ef64ebef4eb25",
                "externalIds": {
                    "DBLP": "journals/tmlr/ZhaoZLJZ22",
                    "CorpusId": 258788213
                },
                "corpusId": 258788213,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/156070205d9b7f0030c5a394d56ef64ebef4eb25",
                "title": "Extracting Local Reasoning Chains of Deep Neural Networks",
                "abstract": "We study how to explain the main steps of inference that a pre-trained deep neural net (DNN) relies on to produce predictions for a (sub)task and its data. This problem is related to network pruning and interpretable machine learning with the following highlighted di\ufb00erences: (1) \ufb01ne-tuning of any neurons/\ufb01lters is forbidden; (2) we target a very high pruning rate, e.g., \u2265 95%, for better interpretability; (3) the interpretation is for the whole inference process on a few data of a task rather than for individual neurons/\ufb01lters or a single sample. In this paper, we introduce NeuroChains to extract the local inference chains by optimizing di\ufb00erentiable sparse scores for the \ufb01lters and layers, which re\ufb02ects their importance in preserving the outputs on a few data drawn from a given (sub)task. Thereby, NeuroChains can extract an extremely small sub-network composed of critical \ufb01lters exactly copied from the original pre-trained DNN by removing the \ufb01lters/layers with small scores. For samples from the same class, we can then visualize the inference pathway in the pre-trained DNN by applying existing interpretation techniques to the retained \ufb01lters and layers. It reveals how the inference process stitches and integrates the information layer by layer and \ufb01lter by \ufb01lter. We provide detailed and insightful case studies together with several quantitative analyses over thousands of trials to demonstrate the quality, sparsity, \ufb01delity and",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2216736132",
                        "name": "Haiyan Zhao"
                    },
                    {
                        "authorId": "2144115714",
                        "name": "Tianyi Zhou"
                    },
                    {
                        "authorId": "2062835",
                        "name": "Guodong Long"
                    },
                    {
                        "authorId": "1746594",
                        "name": "Jing Jiang"
                    },
                    {
                        "authorId": "145107889",
                        "name": "Chen Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1abc3d129e59130514e1e06530cc45e790b8c55c",
                "externalIds": {
                    "CorpusId": 261010411
                },
                "corpusId": 261010411,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1abc3d129e59130514e1e06530cc45e790b8c55c",
                "title": "ELRT: E FFICIENT L OW -R ANK T RAINING FOR C OM - PACT C ONVOLUTIONAL N EURAL N ETWORKS",
                "abstract": "Low-rank compression, a popular model compression technique that produces compact convolutional neural networks (CNNs) with low rankness, has been well studied in the literature. On the other hand, low-rank training, as an alternative way to train low-rank CNNs from scratch, is little exploited yet. Unlike low-rank compression, low-rank training does not need pre-trained full-rank models and the entire training phase is always performed on the low-rank structure, bringing attractive benefits for practical applications. However, the existing low-rank training solutions are still facing several challenges, such as considerable accuracy drop and/or still needing to update full-size models during the training. In this paper, we perform a systematic investigation on low-rank CNN training. By identifying the proper low-rank format and performance-improving strategy, we propose ELRT, an efficient low-rank training solution for high-accuracy high-compactness low-rank CNN models. Our extensive evaluation results for training various CNNs on different datasets demonstrate the effectiveness of ELRT.",
                "year": 2022,
                "authors": []
            }
        },
        {
            "contexts": [
                "Immediately after magnitude pruning, we explore the same number of p-proportion of new weights with the largest magnitude gradients as in [15]:",
                "Following RigL3 [15], the FLOPs are calculated with the total number of multiplications and additions layer by layer for a given layer S.",
                "Further, [13, 15] leverage the gradient information in the backward pass to guide the optimization of sparse connectivity and demonstrate substantial performance improvement.",
                "Following RigL3 [15], the FLOPs are calculated with the total number of multiplications and additions layer by layer for a given layer Sl. Briefly speaking, with ERK distribution, the training FLOPs of a sparse Wide ResNet28-10 at sparsity S = 0.8 and S = 0.9 are 33.7% and 16.7% of the dense model, respectively.",
                "To address this problem, we turn our attention to dynamic sparsity [3, 15, 47, 52, 55], a recently emerged sparse training area that enables training sparse neural networks from scratch by dynamically optimizing the sparse connectivities.",
                "For the convolutional layers, we use the kernel variant, Erd\u0151s-R\u00e9nyi-Kernel (ERK) as introduced in [15].",
                "Over-Parameterization [15, 47], which requires thousands of training epochs for extremely sparse models to explore sufficient parameters in the space-time manifold."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "cf1b59d11acce66875b59902688e3b1b8b0764f9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-14568",
                    "CorpusId": 235658057
                },
                "corpusId": 235658057,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cf1b59d11acce66875b59902688e3b1b8b0764f9",
                "title": "FreeTickets: Accurate, Robust and Efficient Deep Ensemble by Training with Dynamic Sparsity",
                "abstract": "Recent works on sparse neural networks have demonstrated that it is possible to train a sparse network in isolation to match the performance of the corresponding dense networks with a fraction of parameters. However, the iden-ti\ufb01cation of these performant sparse neural networks (win-ning tickets) either involves a costly iterative train-prune-retrain process (e.g., Lottery Ticket Hypothesis) or an over-extended sparse training time (e.g., Training with Dynamic Sparsity), both of which would raise \ufb01nancial and environmental concerns. In this work, we attempt to address this cost-reducing problem by introducing the FreeTickets concept, as the \ufb01rst solution which can boost the performance of sparse convolutional neural networks over their dense network equivalents by a large margin, while using for complete training only a fraction of the computational resources required by the latter. Concretely, we instantiate the FreeTickets concept, by proposing two novel ef\ufb01cient ensemble methods with dynamic sparsity, which yield in one shot many diverse and accurate tickets \u201cfor free\u201d during the sparse training process. The combination of these free tickets into an ensemble demonstrates a signi\ufb01cant improvement in accuracy, uncertainty estimation, robustness, and ef\ufb01ciency over the corresponding dense (ensemble) networks. Our results provide new",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "1768130522",
                        "name": "Zahra Atashgahi"
                    },
                    {
                        "authorId": "2116298570",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "67102129",
                        "name": "Ghada Sokar"
                    },
                    {
                        "authorId": "146634864",
                        "name": "Elena Mocanu"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In this section, we conduct various experiments to validate the effectiveness of SIS in terms of test accuracy vs. sparsity and inference time FLOPs vs. sparsity by comparing against RigL (Evci et al., 2020).",
                "(Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2020; Evci et al., 2020) computes weight magnitude and reallocates weights at every step of model training.",
                "\u2026SMAPE FLOPs\nDense 12.2 4.53G 18.6 927.73G 8.3 41.26M\nSNIP (Lee et al., 2019) 14.3 2.74G 24.6 398.92G 10.1 21.45M LRR (Renda et al., 2020) 13.7 2.61G 23.1 339.21G 9.3 14.47M RigL (Evci et al., 2020) 13.9 2.69G 22.4 326.56G 10.2 15.13M SIS (Ours) 13.1 2.34G 21.1 290.38G 9.7 14.21M\nN-BEATS on M4.",
                "sparsity by comparing against RigL (Evci et al., 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "ddac4cda451abf2377c5b536903bf31e38f5a3cc",
                "externalIds": {
                    "DBLP": "conf/icml/VermaP21",
                    "CorpusId": 235825404
                },
                "corpusId": 235825404,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ddac4cda451abf2377c5b536903bf31e38f5a3cc",
                "title": "Sparsifying Networks via Subdifferential Inclusion",
                "abstract": "Sparsifying deep neural networks is of paramount interest in many areas, especially when those networks have to be implemented on lowmemory devices. In this article, we propose a new formulation of the problem of generating sparse weights for a pre-trained neural network. By leveraging the properties of standard nonlinear activation functions, we show that the problem is equivalent to an approximate subdifferential inclusion problem. The accuracy of the approximation controls the sparsity. We show that the proposed approach is valid for a broad class of activation functions (ReLU, sigmoid, softmax). We propose an iterative optimization algorithm to induce sparsity whose convergence is guaranteed. Because of the algorithm flexibility, the sparsity can be ensured from partial training data in a minibatch manner. To demonstrate the effectiveness of our method, we perform experiments on various networks in different applicative contexts: image classification, speech recognition, natural language processing, and time-series forecasting. Project page: https://sagarverma.github.io/compression",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2114072073",
                        "name": "S. Verma"
                    },
                    {
                        "authorId": "1737505",
                        "name": "J. Pesquet"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2) Layer-wise sparsity decided by ERK only depends on the overall sparsity target and layer configurations (e.g. kernel size, input/output channels).",
                "Training epochs: Training for fine-tuning a sparse neural networks usually requires more epochs for convergence [15].",
                "In order to estimate the layer-wise redundancy, we use the Erdos-Renyi kernel (ERK) [15].",
                "Note that ri can be negative if the current sparsity (si) is larger than the sparsity estimated by ERK.",
                "3.4 16: end if 17: end if 18: end for 19: end while 20: Output Layer wise sparse scheme N iv\nIn order to estimate the layer-wise redundancy, we use the Erdos-Renyi kernel (ERK) [15].",
                "[15] propose a training scheme for unstructured sparse neural networks and present the Erdos-Renyi kernel (ERK) (extension of [17]) to heuristically select the layer-wise sparsity.",
                "ERK is a heuristic method to decide the layer-wise unstructured sparsity given an overall sparsity target (i.e. model size) which does not require design space exploration or hyper-parameter search.",
                "DominoSearch also uses a layer-wise penalty factor to balance the heuristic layer-wise redundancy of parameters[15] and layer-wise computational complexity (e.",
                "Thus, we use the layer-wise sparsity generated by ERK as a layer-wise redundancy guidance, which can be formulated as:\nri = ei \u2212 si\nmaxi=Li=1 |ei \u2212 si| (7)\nwhere ei is the sparsity of layer i decided by ERK.",
                "Recent discoveries demonstrate that by carefully selecting [14, 15] or learning [16] the layer-wise sparsity, the sparse DNNs can achieve higher accuracy than their uniform counterparts.",
                "Evci et al. [15] propose a training scheme for unstructured sparse neural networks and present the Erdos-Renyi kernel (ERK) (extension of [17]) to heuristically select the layer-wise sparsity.",
                "However, ERK cannot be directly applied to select the layer-wise N:M schemes because: 1) ERK gives layer-wise sparsity in the continuous domain while N:M sparsity is discrete."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "516903ffc5afc446bf498322d8589096d98f3767",
                "externalIds": {
                    "DBLP": "conf/nips/SunZSWNLC21",
                    "CorpusId": 245117261
                },
                "corpusId": 245117261,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/516903ffc5afc446bf498322d8589096d98f3767",
                "title": "DominoSearch: Find layer-wise fine-grained N: M sparse schemes from dense neural networks",
                "abstract": "Neural pruning is a widely-used compression technique for Deep Neural Networks (DNNs). Recent innovations in Hardware Architectures (e.g. Nvidia Ampere Sparse Tensor Core) and N:M \ufb01ne-grained Sparse Neural Network algorithms (i.e. every M-weights contains N non-zero values) reveal a promising research line of neural pruning. However, the existing N:M algorithms only address the challenge of how to train N:M sparse neural networks in a uniform fashion (i.e. every layer has the same N:M sparsity) and suffer from a signi\ufb01cant accuracy drop for high sparsity (i.e. when sparsity > 80%). To tackle this problem, we present a novel technique \u2013 DominoSearch to \ufb01nd mixed N:M sparsity schemes from pre-trained dense deep neural networks to achieve higher accuracy than the uniform-sparsity scheme with equivalent complexity constraints (e.g. model size or FLOPs). For instance, for the same model size with 2.1M parameters (87.5% sparsity), our layer-wise N:M sparse ResNet18 outperforms its uniform counterpart by 2.1% top-1 accuracy, on the large-scale ImageNet dataset. For the same computational complexity of 227M FLOPs, our layer-wise sparse ResNet18 outperforms the uniform one by 1.3% top-1 accuracy. Furthermore, our layer-wise \ufb01ne-grained N:M sparse ResNet50 achieves 76.7% top-1 accuracy with 5.0M parameters. This is competitive to the results achieved by layer-wise unstructured sparsity that is believed to be the upper-bound of Neural Network pruning with respect to the accuracy-sparsity trade-off. We believe that our work can build a strong baseline for further sparse DNN research and encourage future hardware-algorithm co-design work. Our code and models are publicly available at https://github.com/NM-sparsity/DominoSearch .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Wei Sun"
                    },
                    {
                        "authorId": "9548994",
                        "name": "Aojun Zhou"
                    },
                    {
                        "authorId": "115679894",
                        "name": "S. Stuijk"
                    },
                    {
                        "authorId": "2005109",
                        "name": "R. Wijnhoven"
                    },
                    {
                        "authorId": "31896653",
                        "name": "A. Nelson"
                    },
                    {
                        "authorId": "47893312",
                        "name": "Hongsheng Li"
                    },
                    {
                        "authorId": "1684335",
                        "name": "H. Corporaal"
                    }
                ]
            }
        },
        {
            "contexts": [
                "One pruning method removes the weights with the least magnitude and randomly makes new connections to learn a sparse ANN structure (Evci et al., 2019).",
                "Pruning methods are used to learn a sparser ANN structure (Pe\u0301rez-Sa\u0301nchez, 2018; Evci et al., 2019).",
                "Pruning methods are used to learn a sparser ANN structure (P\u00e9rez-S\u00e1nchez, 2018; Evci et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b76ebf9795b9884fd0f686e03530422155a59691",
                "externalIds": {
                    "CorpusId": 232216292
                },
                "corpusId": 232216292,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b76ebf9795b9884fd0f686e03530422155a59691",
                "title": "Toward Generate-and-Test Algorithms for Continual Feature Discovery",
                "abstract": "The backpropagation algorithm is a fundamental algorithm for training modern artificial neural networks (ANNs). However, it is known the backpropagation algorithm performs poorly on changing problems. We demonstrate the backpropagation algorithm can perform poorly on a clear, generic, changing task. The task is online meaning the agent learns from one sample at a time from a stream of samples. The task is nonstationary since the sample distribution regularly changes. We call it the generic continual feature discovery task (GCFD), as it is sufficiently difficult that the backpropagation algorithm must regularly discover new features to perform well. We propose an explanation for the poor performance of the backpropagation algorithm on the GCFD task. The backpropagation algorithm consists of two phases: initializing an ANN with small random weights, and using stochastic gradient descent to update the weights with data. It is known that the initialization step is crucial to the fast discovery of useful features with the backpropagation algorithm, and a typical initialization step sets the weights to small, random numbers. We corroborate that the small, random weight initialization step leads to conditions that speed up the discovery of useful features with the backpropagation algorithm. Then, we show that these conditions are not maintained during the GCFD task. Without the maintenance of these conditions, there is little reason to expect the backpropagation algorithm to quickly discover useful features for new sample distributions. We demonstrate that the backpropagation algorithm\u2019s performance on the ii GCFD task can be significantly improved with generate-and-test algorithms. The generate-and-test algorithms replace the least useful features of the ANN with features that have small, random weights. By regularly introducing features with small random weights, we restore conditions the backpropagation algorithm can use to quickly discover useful features for new data distributions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2065825569",
                        "name": "Parash Rahman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The method is further evolved for convolution layers considering both the magnitude and gradient of the weights(Evci et al., 2020a). Various analysis for explaining the LTH have been attempted in the past. Researchers Evci et al. (2020b) explain empirically why the LTH works through gradient flow at different stages of the training. Despite previous attempts to explain why the Lottery Ticket Hypothesis works, the underlying phenomenon associated with the hypothesis still remains ill-understood. All of these studies related to LTH identify that a sparse sub-network can be trained instead of a complete network and the network needs to be connected from input to output layers. However, none of them try to explain the LTH and the properties of the pruned network through the lens of spectral graph theory. The network connectivity can be described from the graph expansion point of view, where any subset of vertices of size less than or equal to half of the number of vertices in a graph, is adjacent to at least a fraction of the number of vertices in that set; for details, see (Lubotzky, 2010). Graphs satisfying this property are known as expander graphs. The Ramanujan Graph is a special graph in a bounded degree expander family, where the eigenbound is maximal (Nilli, 1991). This leads to a maximum possible sparsity of a network while preserving the connectivity. In this paper, we initiate a study to observe the characteristics of a pruned sub-network from the spectral properties of its adjacency matrix, which, has not been reported previously. We represent a feed-forward neural network as a series of connected bipartite graphs. Both weighted and unweighted bi-adjacency matrices are considered. The Ramanujan graph properties of each of the bipartite layers are studied. We use the results of Hoory (2005) on the bound of spectral gap for the weight matrix of a pruned network.",
                "The method is further evolved for convolution layers considering both the magnitude and gradient of the weights(Evci et al., 2020a). Various analysis for explaining the LTH have been attempted in the past. Researchers Evci et al. (2020b) explain empirically why the LTH works through gradient flow at different stages of the training.",
                "Researchers Evci et al. (2020b) explain empirically why the LTH works through gradient flow at different stages of the training.",
                "The method is further evolved for convolution layers considering both the magnitude and gradient of the weights(Evci et al., 2020a)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "d56026be1ff8a6cd7790d9d1a84c9f14e75e5e86",
                "externalIds": {
                    "CorpusId": 250981501
                },
                "corpusId": 250981501,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d56026be1ff8a6cd7790d9d1a84c9f14e75e5e86",
                "title": "R EVISITING THE L OTTERY T ICKET H YPOTHESIS : A R AMANUJAN G RAPH P ERSPECTIVE",
                "abstract": "Neural networks for machine learning applications often yield to weight pruning resulting in a sparse subnetwork that is adequate for a given task. Retraining these \u2018lottery ticket\u2019 subnetworks from their initialization minimizes the computational burden while preserving the test set accuracy of the original network. The existing literature only confirms that pruning is needed and it can be achieved up to a certain sparsity. We analyze the pruned network in the context of the properties of Ramanujan expander graphs. We consider the feed-forward network (both multi-layer perceptron and convolutional network) as a series of bipartite graphs which establish the connection from input to output. Now, as the fraction of remaining weights reduce with increasingly aggressive pruning, distinct regimes are observed: initially, no significant decrease in accuracy is demonstrated, and then the accuracy starts dropping rapidly. We empirically show that in the first regime, the pruned lottery ticket sub-network remains a Ramanujan graph. Subsequently, with the loss of Ramanujan graph property, accuracy begins to reduce sharply. This characterizes an absence of resilient connectivity in the pruned sub-network. We also propose a modified iterative pruning algorithm which removes edges in only the layers that are Ramanujan graphs thus preserving global connectivity even for heavily pruned networks. We perform experiments on MNIST and CIFAR10 datasets using different established feed-forward architectures to support the criteria for obtaining the winning ticket using the proposed algorithm.",
                "year": 2021,
                "authors": []
            }
        },
        {
            "contexts": [
                "Many follow-up works [Morcos et al., 2019; Zhou et al., 2019; Frankle et al., 2020a; Savarese et al., 2020; Wang et al., 2020; Ramanujan et al., 2020; Evci et al., 2020; Frankle et al., 2021] advance the idea and keep challenging the conventional wisdom on neural network pruning."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "bb51d822f06a320066d32f11e8c910ff2ad63db5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-06460",
                    "CorpusId": 232185631
                },
                "corpusId": 232185631,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bb51d822f06a320066d32f11e8c910ff2ad63db5",
                "title": "Emerging Paradigms of Neural Network Pruning",
                "abstract": "Over-parameterization of neural networks benefits the optimization and generalization yet brings cost in practice. Pruning is adopted as a post-processing solution to this problem, which aims to remove unnecessary parameters in a neural network with little performance compromised. It has been broadly believed the resulted sparse neural network cannot be trained from scratch to comparable accuracy. However, several recent works (e.g., [Frankle and Carbin, 2019a]) challenge this belief by discovering random sparse networks which can be trained to match the performance with their dense counterpart. This new pruning paradigm later inspires more new methods of pruning at initialization. In spite of the encouraging progress, how to coordinate these new pruning fashions with the traditional pruning has not been explored yet. This survey seeks to bridge the gap by proposing a general pruning framework so that the emerging pruning paradigms can be accommodated well with the traditional one. With it, we systematically reflect the major differences and new insights brought by these new pruning fashions, with representative works discussed at length. Finally, we summarize the open questions as worthy future directions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46507194",
                        "name": "Haiquan Wang"
                    },
                    {
                        "authorId": "12282768",
                        "name": "Can Qin"
                    },
                    {
                        "authorId": "2129519081",
                        "name": "Yulun Zhang"
                    },
                    {
                        "authorId": "46956675",
                        "name": "Y. Fu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As described in [22], the final test accuracy would decrease if the pruning interval is too big or small."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d767663f29f57b1c18245429be3c8ba3c08e2788",
                "externalIds": {
                    "DBLP": "journals/access/ZhongZQC21",
                    "DOI": "10.1109/ACCESS.2021.3065406",
                    "CorpusId": 233198080
                },
                "corpusId": 233198080,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d767663f29f57b1c18245429be3c8ba3c08e2788",
                "title": "Roulette: A Pruning Framework to Train a Sparse Neural Network From Scratch",
                "abstract": "Due to space and inference time restrictions, finding an efficient and sparse sub-network from a dense and over-parameterized network is critical for deploying neural networks on edge devices. Recent efforts explore obtaining a sparse sub-network by performing network pruning during training procedures to reduce training costs, such as memory and floating-point operations (FLOPs). However, these works take more than <inline-formula> <tex-math notation=\"LaTeX\">$1.4\\times $ </tex-math></inline-formula> the total number of iterations and try all possible pruning parameters manually to obtain sparse sub-networks. In this paper, we present a pruning framework Roulette to train a sparse network from scratch. First, we propose a novel method to train a sparse network by Pruning through the lens of Loss Landscape iteratively and automatically (PLL). We do a theoretical analysis that the curvature of the loss function is higher in the initial phase and can conduct us to start network pruning. According to our results on CIFAR-10/100 and ImageNet dataset, PLL saves up to <inline-formula> <tex-math notation=\"LaTeX\">$4\\times $ </tex-math></inline-formula> training FLOPs than prior works while maintaining comparable or even better accuracy. Then we design <bold>push</bold> and <bold>pull</bold> operations to synchronize the pruned weights on different GPUs during training, scaling PLL to multiple GPUs <bold>linearly</bold>. To our knowledge, Roulette is the first network pruning framework supporting multiple GPUs linearly.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1820947200",
                        "name": "Qiaoling Zhong"
                    },
                    {
                        "authorId": "2019868832",
                        "name": "Zhibin Zhang"
                    },
                    {
                        "authorId": "2077648",
                        "name": "Qiang Qiu"
                    },
                    {
                        "authorId": "1717004",
                        "name": "Xueqi Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The requirement of a dense gradient in Evci et al. (2019) and Zhu & Gupta (2018) prevents the use of the optimization in Equation 4, which is strictly necessary to fit the RTRL training computations on accelerators without running out of memory.",
                "The last few years have also seen a resurgence of interest in sparse neural networks \u2013 both their properties (Frankle & Carbin, 2019) and new methods for training them (Evci et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "efbcb368bdc1f27ec13584d65034321837169ae1",
                "externalIds": {
                    "DBLP": "conf/iclr/MenickEEOSG21",
                    "CorpusId": 234797942
                },
                "corpusId": 234797942,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/efbcb368bdc1f27ec13584d65034321837169ae1",
                "title": "Practical Real Time Recurrent Learning with a Sparse Approximation",
                "abstract": "Recurrent neural networks are usually trained with backpropagation through time, which requires storing a complete history of network states, and prohibits updating the weights \u2018online\u2019 (after every timestep). Real Time Recurrent Learning (RTRL) eliminates the need for history storage and allows for online weight updates, but does so at the expense of computational costs that are quartic in the state size. This renders RTRL training intractable for all but the smallest networks, even ones that are made highly sparse. We introduce the Sparse n-step Approximation (SnAp) to the RTRL influence matrix. SnAp only tracks the influence of a parameter on hidden units that are reached by the computation graph within n timesteps of the recurrent core. SnAp with n = 1 is no more expensive than backpropagation but allows training on arbitrarily long sequences. We find that it substantially outperforms other RTRL approximations with comparable costs such as Unbiased Online Recurrent Optimization. For highly sparse networks, SnAp with n = 2 remains tractable and can outperform backpropagation through time in terms of learning speed when updates are done online.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "10698483",
                        "name": "Jacob Menick"
                    },
                    {
                        "authorId": "152585800",
                        "name": "Erich Elsen"
                    },
                    {
                        "authorId": "3399348",
                        "name": "Utku Evci"
                    },
                    {
                        "authorId": "2217144",
                        "name": "Simon Osindero"
                    },
                    {
                        "authorId": "34838386",
                        "name": "K. Simonyan"
                    },
                    {
                        "authorId": "1753223",
                        "name": "Alex Graves"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It essentially sparsifies the network at a fine-grained level and is demonstrated to achieve an extremely high compression rate and high accuracy performance [11, 7, 29]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0f32fd7eb3fcb1c500546f7ee942f581358ff46d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-14713",
                    "CorpusId": 235254579
                },
                "corpusId": 235254579,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0f32fd7eb3fcb1c500546f7ee942f581358ff46d",
                "title": "1\u00d7N Block Pattern for Network Sparsity",
                "abstract": "Though network sparsity emerges as a promising direction to overcome the drastically increasing size of neural networks, it remains an open problem to concurrently maintain model accuracy as well as achieve signi\ufb01cant speedups on general CPUs. In this paper, we propose one novel concept of 1 \u00d7 N block sparsity pattern (block pruning) to break this limitation. In particular, consecutive N output kernels with the same input channel index are grouped into one block, which serves as a basic pruning granularity of our pruning pattern. Our 1 \u00d7 N sparsity pattern prunes these blocks considered unimportant. We also provide a work\ufb02ow of \ufb01lter rearrangement that \ufb01rst rearranges the weight matrix in the output channel dimension to derive more in\ufb02uential blocks for accuracy improvements, and then applies similar rearrangement to the next-layer weights in the input channel dimension to ensure correct convolutional operations. Moreover, the output computation after our 1 \u00d7 N block sparsity can be realized via a parallelized block-wise vectorized operation, leading to signi\ufb01cant speedups on general CPUs-based platforms. The ef\ufb01cacy of our pruning pattern is proved with experiments on ILSVRC-2012. For example, in the case of 50% sparsity and N = 4 , our pattern obtains about 3.0% improvements over \ufb01lter pruning in the top-1 accuracy of MobileNet-V2. Meanwhile, it obtains 56.04ms inference savings on Cortex-A7 CPU over weight pruning. Code is available at https://github.com/lmbxmu/1xN .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49352079",
                        "name": "Mingbao Lin"
                    },
                    {
                        "authorId": "2110482496",
                        "name": "Yuchao Li"
                    },
                    {
                        "authorId": "2108078624",
                        "name": "Yu-xin Zhang"
                    },
                    {
                        "authorId": "146335182",
                        "name": "Bohong Chen"
                    },
                    {
                        "authorId": "2070266415",
                        "name": "Fei Chao"
                    },
                    {
                        "authorId": "50468734",
                        "name": "Mengdi Wang"
                    },
                    {
                        "authorId": "2153701890",
                        "name": "Shen Li"
                    },
                    {
                        "authorId": "2146156715",
                        "name": "Jun Yang"
                    },
                    {
                        "authorId": "145592290",
                        "name": "R. Ji"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3f8874e01434f3f2c672b2126ff6dfb358a064fc",
                "externalIds": {
                    "CorpusId": 235377058
                },
                "corpusId": 235377058,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3f8874e01434f3f2c672b2126ff6dfb358a064fc",
                "title": "Rate-Distortion Theoretic Model Compression: Successive Refinement for Pruning",
                "abstract": "We study the neural network (NN) compression problem, viewing the tension between the compression ratio and NN performance through the lens of rate-distortion theory. We choose a distortion metric that reflects the effect of NN compression on the model output and then derive the tradeoff between rate (compression ratio) and distortion. In addition to characterizing theoretical limits of NN compression, this formulation shows that pruning, implicitly or explicitly, must be a part of a good compression algorithm. This observation bridges a gap between parts of the literature pertaining to NN and data compression, respectively, providing insight into the empirical success of pruning for NN compression. Finally, we propose a novel pruning strategy derived from our information-theoretic formulation and show that it outperforms the relevant baselines on CIFAR-10 and ImageNet datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1707440322",
                        "name": "Berivan Isik"
                    },
                    {
                        "authorId": "3268846",
                        "name": "Albert No"
                    },
                    {
                        "authorId": "4820756",
                        "name": "T. Weissman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "B Pruned CNN+ReLU [16], [21] Pruned Trasformer+ReLU [55] sparse /sparse DNN."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e5af2b093b8cf88d4a26855aa2f914ff2a386133",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-12922",
                    "CorpusId": 236447699
                },
                "corpusId": 236447699,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e5af2b093b8cf88d4a26855aa2f914ff2a386133",
                "title": "Design Space Exploration of Sparse Accelerators for Deep Neural Networks",
                "abstract": "Novel architectures for deep learning exploit both activation and weight sparsity to improve the performance of DNN inference. However, this speedup usually brings nonnegligible overheads which diminish the efficiency of such designs when running dense models. These overheads specifically are exacerbated for low precision accelerators with optimized SRAM size per core. This paper examines the design space trade-offs of such accelerators aiming to achieve competitive performance and efficiency metrics for all four combinations of dense or sparse activation/weight tensors. To do so, we systematically examine overheads of supporting sparsity on top of an optimized dense core. These overheads are modeled based on parameters that indicate how a multiplier can borrow a nonzero operation from the neighboring multipliers or future cycles. As a result of this exploration, we identify a few promising designs that perform better than prior work. Our findings suggest that even a best design targeting dual sparsity yields 20%-30% drop in power efficiency when performing on single sparse models, i.e., those with only sparse weight or sparse activation tensors. We introduce novel techniques to reuse resources of the same core to maintain high performance and efficiency when running single sparsity or dense models. We call this hybrid design Griffin. Griffin is 1.2, 3.0, 3.1, and 1.4\u00d7 more power efficient than stateof-the-art sparse architectures, for dense, weight-only sparse, activation-only sparse, and dual sparse models, respectively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116475836",
                        "name": "J. Shin"
                    },
                    {
                        "authorId": "1433007765",
                        "name": "Ali Shafiee"
                    },
                    {
                        "authorId": "9182159",
                        "name": "A. Pedram"
                    },
                    {
                        "authorId": "1403426852",
                        "name": "Hamzah Abdel-Aziz"
                    },
                    {
                        "authorId": "3353457",
                        "name": "Ling Li"
                    },
                    {
                        "authorId": "1491321888",
                        "name": "Joseph Hassoun"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7d02f87a4b8cde490523eba37f46bc3017c75874",
                "externalIds": {
                    "CorpusId": 236467673
                },
                "corpusId": 236467673,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7d02f87a4b8cde490523eba37f46bc3017c75874",
                "title": "Evaluation of Sparse Image Representations for Visual Recognition",
                "abstract": "Visual recognition tasks such as image classification and localization are some of the most common applications of deep neural networks. However, large quantities of image and video data can impose high costs for storage and transmission, which can limit the application of neural networks in resource-constrained scenarios. While there has been substantial research focus on techniques to train sparse neural network models to reduce computational costs, there has been comparatively less focus on introducing sparsity to the input data representation. I evaluated the effects of sparse image representations on the performance of MobileNetV2 and ResNet18 models trained on an image classification task. The best-performing sparse representation, color-preserved edges, attained 41% classification accuracy on the Tiny ImageNet dataset using MobileNetV2, compared to 48.6% attained with the original images.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2112839563",
                        "name": "A. Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many variants have been proposed for both structured [25, 36, 41, 65] and unstructured [16, 17, 19, 26] pruning.",
                "[16] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "45b31d499ed297927bebcd22fa7a6e2bbe191c48",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-00259",
                    "CorpusId": 236772801
                },
                "corpusId": 236772801,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/45b31d499ed297927bebcd22fa7a6e2bbe191c48",
                "title": "Provably Efficient Lottery Ticket Discovery",
                "abstract": "The lottery ticket hypothesis (LTH) [19] claims that randomly-initialized, dense neural networks contain (sparse) subnetworks that, when trained an equal amount in isolation, can match the dense network\u2019s performance. Although LTH is useful for discovering ef\ufb01cient network architectures, its three-step process\u2014pre-training, pruning, and re-training\u2014is computationally expensive, as the dense model must be fully pre-trained. Luckily, \u201cearly-bird\u201d tickets can be discovered within neural networks that are minimally pre-trained [67], allowing for the creation of ef\ufb01cient, LTH-inspired training procedures. Yet, no theoretical foundation of this phenomenon exists. We derive an analytical bound for the number of pre-training iterations that must be performed for a winning ticket to be discovered, thus providing a theoretical understanding of when and why such early-bird tickets exist. By adopting a greedy forward selection pruning strategy [65], we directly connect the pruned network\u2019s performance to the loss of the dense network from which it was derived, revealing a threshold in the number of pre-training iterations beyond which high-performing subnetworks are guaranteed to exist. We demonstrate the validity of our theoretical results across a variety of architectures and datasets, including multi-layer perceptrons (MLPs) trained on MNIST and several deep convolutional neural network (CNN) architectures trained on CIFAR10 and ImageNet.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34210029",
                        "name": "Cameron R. Wolfe"
                    },
                    {
                        "authorId": "2154497049",
                        "name": "Qihan Wang"
                    },
                    {
                        "authorId": "2120215686",
                        "name": "J. Kim"
                    },
                    {
                        "authorId": "3393746",
                        "name": "Anastasios Kyrillidis"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "38357886f6cff1a1f3902ada0aaf32d4a078c87a",
                "externalIds": {
                    "CorpusId": 237450910
                },
                "corpusId": 237450910,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/38357886f6cff1a1f3902ada0aaf32d4a078c87a",
                "title": "Efficient Weight Pruning using Pre-trained Lottery Jackpots",
                "abstract": "Network pruning is an effective approach to reduce network complexity without performance compromise. Existing studies achieve the sparsity of neural networks via timeconsuming weight tuning or complex search on networks with expanded width, which greatly limits the applications of network pruning. In this paper, we show that high-performing and sparse sub-networks without the involvement of weight tuning, termed \u201clottery jackpots\u201d, exist in pre-trained models with unexpanded width. For example, we obtain a lottery jackpot that has only 10% parameters and still reaches the performance of the original dense VGGNet-19 without any modifications on the pre-trained weights on CIFAR10. Furthermore, we observe that the sparse masks derived from many existing pruning criteria have a high overlap with the searched mask of our lottery jackpot, among which, the magnitude-based pruning results in the most similar mask with ours. Based on this insight, we initialize our sparse mask using the magnitude-based pruning, resulting in at least 3\u00d7 cost reduction on the lottery jackpot search while achieving comparable or even better performance. Specifically, our magnitude-based lottery jackpot removes 90% weights in the ResNet-50, while easily obtains more than 70% top-1 accuracy using only 10 searching epochs on ImageNet. Our project is at https://github.com/zyxxmu/lottery-jackpots.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145067719",
                        "name": "Yu-xin Zhang"
                    },
                    {
                        "authorId": "49352079",
                        "name": "Mingbao Lin"
                    },
                    {
                        "authorId": "2070266415",
                        "name": "Fei Chao"
                    },
                    {
                        "authorId": "2084534028",
                        "name": "Guannan Jiang"
                    },
                    {
                        "authorId": "2152540679",
                        "name": "Yan Wang"
                    },
                    {
                        "authorId": "47096329",
                        "name": "Yongjian Wu"
                    },
                    {
                        "authorId": "2155467272",
                        "name": "Wei Zhang"
                    },
                    {
                        "authorId": "2285442",
                        "name": "Mingliang Xu"
                    },
                    {
                        "authorId": "40161651",
                        "name": "Yonghong Tian"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Possible directions of future study include:\n\u2022 Bringing the RG framework to other sparsficiation methods, like RigL [12], which can similarly be viewed as RG schemes;\n\u2022 Bringing the RG framework to study winning tickets beyond computer vision, such as in natural language processing [3, 31, 45], reinforcement learning [45], and lifelong learning [5];\n\u2022 Computing and classifying systems by their critical exponents (such as \u03b3 in Eq.",
                "Possible directions of future study include: \u2022 Bringing the RG framework to other sparsficiation methods, like RigL [12], which can similarly be viewed as RG schemes; \u2022 Bringing the RG framework to study winning tickets beyond computer vision, such as in natural language processing [3, 31, 45], reinforcement learning [45], and lifelong learning [5]; \u2022 Computing and classifying systems by their critical exponents (such as \u03b3 in Eq."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6ee98d9b218fcace923fe5fef742cee54ebd32f3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-03210",
                    "CorpusId": 238419190
                },
                "corpusId": 238419190,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6ee98d9b218fcace923fe5fef742cee54ebd32f3",
                "title": "Universality of Deep Neural Network Lottery Tickets: A Renormalization Group Perspective",
                "abstract": "Foundational work on the Lottery Ticket Hypothesis has suggested an exciting corollary: winning tickets found in the context of one task can be transferred to similar tasks, possibly even across different architectures. While this has become of broad practical and theoretical interest, to date, there exists no detailed understanding of why winning ticket universality exists, or any way of knowing a priori whether a given ticket can be transferred to a given task. To address these outstanding open questions, we make use of renormalization group theory, one of the most successful tools in theoretical physics. We find that iterative magnitude pruning, the method used for discovering winning tickets, is a renormalization group scheme. This opens the door to a wealth of existing numerical and theoretical tools, some of which we leverage here to examine winning ticket universality in large scale lottery ticket experiments, as well as sheds new light on the success iterative magnitude pruning has found in the field of sparse machine learning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145036964",
                        "name": "William T. Redman"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": null,
                        "name": "Akshunna S. Dogra"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Evci et al. (2019) propose a reformulation of the ERDO\u030bS-R\u00c9NYI KERNEL (ERK) (Mocanu et al., 2018) to take the layer and kernel dimensions into account when determining the layerwise sparsity distribution.",
                "We note that we follow the advice of Evci et al. (2019) and Dettmers and Zettlemoyer (2019) and do not prune biases and batch-normalization parameters, since they only amount to a negligible fraction of the total weights, however keeping them has a very positive impact on the performance of the\u2026",
                "Gale et al. (2019), Evci et al. (2019) and Lin et al. (2020) consider this when presenting their results."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "46edddc44e6793824b69802eb1322dc7d504d006",
                "externalIds": {
                    "CorpusId": 240354174
                },
                "corpusId": 240354174,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/46edddc44e6793824b69802eb1322dc7d504d006",
                "title": "Back to Basics: Efficient Network Compression via IMP",
                "abstract": "Network pruning is a widely used technique for effectively compressing Deep Neural Networks with little to no degradation in performance during inference. Iterative Magnitude Pruning (IMP) (Han et al., 2015) is one of the most established approaches for network pruning, consisting of several iterative training and pruning steps, where a significant amount of the network\u2019s performance is lost after pruning and then recovered in the subsequent retraining phase. While commonly used as a benchmark reference, it is often argued that a) it reaches suboptimal states by not incorporating sparsification into the training phase, b) its global selection criterion fails to properly determine optimal layer-wise pruning rates and c) its iterative nature makes it slow and non-competitive. In light of recently proposed retraining techniques, we investigate these claims through rigorous and consistent experiments where we compare IMP to pruning-during-training algorithms, evaluate proposed modifications of its selection criterion and study the number of iterations and total training time actually required. We find that IMP with SLR (Le and Hua, 2021) for retraining can outperform state-of-the-art pruning-duringtraining approaches without or with only little computational overhead, that the global magnitude selection criterion is largely competitive with more complex approaches and that only few retraining epochs are needed in practice to achieve most of the sparsity-vs.-performance tradeoff of IMP. Our goals are both to demonstrate that basic IMP can already provide state-of-the-art pruning results on par with or even outperforming more complex or heavily parameterized approaches and also to establish a more realistic yet easily realisable baseline for future research.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2056708985",
                        "name": "Max Zimmer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although existing LTH techniques raise very intriguing observations, most of them provide only empirical evidence to verify the LTH [71, 12, 1, 47, 69, 54, 5, 53, 26, 8, 7, 11]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d5513752425a079195656580e15857d34e4d6941",
                "externalIds": {
                    "DBLP": "conf/nips/ZhangJZZZRLWJD21",
                    "CorpusId": 244958525
                },
                "corpusId": 244958525,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d5513752425a079195656580e15857d34e4d6941",
                "title": "Validating the Lottery Ticket Hypothesis with Inertial Manifold Theory",
                "abstract": "Despite achieving remarkable ef\ufb01ciency, traditional network pruning techniques often follow manually-crafted heuristics to generate pruned sparse networks. Such heuristic pruning strategies are hard to guarantee that the pruned networks achieve test accuracy comparable to the original dense ones. Recent works have empirically identi\ufb01ed and veri\ufb01ed the Lottery Ticket Hypothesis (LTH): a randomly-initialized dense neural network contains an extremely sparse subnetwork, which can be trained to achieve similar accuracy to the former. Due to the lack of theoretical evidence, they often need to run multiple rounds of expensive training and pruning over the original large networks to discover the sparse subnetworks with low accuracy loss. By leveraging dynamical systems theory and inertial manifold theory, this work theoretically veri\ufb01es the validity of the LTH. We explore the possibility of theoretically lossless pruning as well as one-time pruning, compared with existing neural network pruning and LTH techniques. We reformulate the neural network optimization problem as a gradient dynamical system and reduce this high-dimensional system onto inertial manifolds to obtain a low-dimensional system regarding pruned subnetworks. We demonstrate the precondition and existence of pruned subnetworks and prune the original networks in",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118690556",
                        "name": "Zeru Zhang"
                    },
                    {
                        "authorId": "103340106",
                        "name": "Jiayin Jin"
                    },
                    {
                        "authorId": "48806049",
                        "name": "Zijie Zhang"
                    },
                    {
                        "authorId": "2145499198",
                        "name": "Yang Zhou"
                    },
                    {
                        "authorId": "2145735267",
                        "name": "Xin Zhao"
                    },
                    {
                        "authorId": "48115953",
                        "name": "Jiaxiang Ren"
                    },
                    {
                        "authorId": "2118971193",
                        "name": "Ji Liu"
                    },
                    {
                        "authorId": "3008832",
                        "name": "Lingfei Wu"
                    },
                    {
                        "authorId": "1740308",
                        "name": "R. Jin"
                    },
                    {
                        "authorId": "1721158",
                        "name": "D. Dou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similar to our work, there exists a line of work in the area of dynamic sparse training for unstructured pruning [50, 13, 32, 9, 27] that gradually prunes the model to the target sparsity during training."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1245b5d83fac36a94d8e53045d623b80688944fb",
                "externalIds": {
                    "CorpusId": 245122875
                },
                "corpusId": 245122875,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1245b5d83fac36a94d8e53045d623b80688944fb",
                "title": "Learning Compact Representations of Neural Networks using DiscriminAtive Masking (DAM)",
                "abstract": "A central goal in deep learning is to learn compact representations of features at every layer of a neural network, which is useful for both unsupervised representation learning and structured network pruning. While there is a growing body of work in structured pruning, current state-of-the-art methods suffer from two key limitations: (i) instability during training, and (ii) need for an additional step of fine-tuning, which is resource-intensive. At the core of these limitations is the lack of a systematic approach that jointly prunes and refines weights during training in a single stage, and does not require any fine-tuning upon convergence to achieve state-of-the-art performance. We present a novel single-stage structured pruning method termed DiscriminAtive Masking (DAM). The key intuition behind DAM is to discriminatively prefer some of the neurons to be refined during the training process, while gradually masking out other neurons. We show that our proposed DAM approach has remarkably good performance over a diverse range of applications in representation learning and structured pruning, including dimensionality reduction, recommendation system, graph representation learning, and structured pruning for image classification. We also theoretically show that the learning objective of DAM is directly related to minimizing the L0 norm of the masking layer. All of our codes and datasets are available https://github.com/jayroxis/ dam-pytorch.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49221613",
                        "name": "Jie Bu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026many interesting works have addressed this training strategy by proposing different algorithms for optimizing the topology during training (Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Bellec et al., 2018; Jayakumar et al., 2020; Liu et al., 2021b; Raihan & Aamodt, 2020).",
                "We follow the method described in (Evci et al., 2020) to calculate the FLOPs."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a308aad7382396ee940adea1f53da178743f40e9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-05329",
                    "CorpusId": 238583653
                },
                "corpusId": 238583653,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a308aad7382396ee940adea1f53da178743f40e9",
                "title": "Addressing the Stability-Plasticity Dilemma via Knowledge-Aware Continual Learning",
                "abstract": "Current methods in continual learning (CL) tend to focus on alleviating catastrophic forgetting of previous tasks. This hinders balancing other CL desiderata such as forward transfer, and memory and computation efficiency. CL desiderata become much more competing when a model operates on class-Incremental Learning (class-IL) without accessing past data since it is prone to ambiguities between old and new classes. In this paper, we present Knowledge-Aware coNtinual learner (KAN) that attempts to study the stabilityplasticity dilemma to balance CL desiderata in class-IL. In particular, KAN is a new task-specific components method based on dynamic sparse training that introduces reusability and selective transfer of past knowledge in this class of methods. KAN selectively reuses relevant knowledge while addressing class ambiguity, preserves old knowledge, and utilizes the model capacity efficiently. Experiments show the effectiveness of KAN in providing models with multiple CL desirable properties, outperforming state-of-the-art methods on various challenging benchmarks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "67102129",
                        "name": "Ghada Sokar"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similar to our work, there exists a line of work in the area of dynamic sparse training for unstructured pruning [50, 13, 32, 9, 27] that gradually prunes the model to the target sparsity during training."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1234d030ba105d8fae9fe34f21474f2ff19ddaf4",
                "externalIds": {
                    "CorpusId": 247613192
                },
                "corpusId": 247613192,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1234d030ba105d8fae9fe34f21474f2ff19ddaf4",
                "title": "Learning Compact Representations of Neural Networks using DiscriminAtive Masking (DAM)",
                "abstract": "A central goal in deep learning is to learn compact representations of features at every layer of a neural network, which is useful for both unsupervised representation learning and structured network pruning. While there is a growing body of work in structured pruning, current state-of-the-art methods suffer from two key limitations: (i) instability during training, and (ii) need for an additional step of fine-tuning, which is resource-intensive. At the core of these limitations is the lack of a systematic approach that jointly prunes and refines weights during training in a single stage, and does not require any fine-tuning upon convergence to achieve state-of-the-art performance. We present a novel single-stage structured pruning method termed DiscriminAtive Masking (DAM). The key intuition behind DAM is to discriminatively prefer some of the neurons to be refined during the training process, while gradually masking out other neurons. We show that our proposed DAM approach has remarkably good performance over a diverse range of applications in representation learning and structured pruning, including dimensionality reduction, recommendation system, graph representation learning, and structured pruning for image classification. We also theoretically show that the learning objective of DAM is directly related to minimizing the L0 norm of the masking layer. All of our codes and datasets are available https://github.com/jayroxis/ dam-pytorch.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49221613",
                        "name": "Jie Bu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f054b247ba935cc95c99f72bb39600a4378862fe",
                "externalIds": {
                    "CorpusId": 250067463
                },
                "corpusId": 250067463,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f054b247ba935cc95c99f72bb39600a4378862fe",
                "title": "[Re] Rigging the Lottery: Making All Tickets Winners",
                "abstract": "For a fixed parameter count and compute budget, the proposed algorithm (RigL) claims to directly train sparse networks thatmatch or exceed the performance of existing denseto-sparse training techniques (such as pruning). RigL does so while requiring constant Floating Point Operations (FLOPs) throughout training. The technique obtains state-ofthe-art performance on a variety of tasks, including image classification and characterlevel language-modelling.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "152894647",
                        "name": "Varun Sundar"
                    },
                    {
                        "authorId": "2061140054",
                        "name": "Rajat Vadiraj Dwaraknath"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, [31] proposed a drop and grow strategy with the magnitude of the parameters and the gradients.",
                "Various dynamic methods [17]\u2013[20], [24], [28]\u2013[31] have been proposed, and they have shown better performance than static methods."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "879cfa030335cff6c3d87793174fbad3968e551b",
                "externalIds": {
                    "DBLP": "journals/access/ChoSAB21",
                    "DOI": "10.1109/ACCESS.2021.3131310",
                    "CorpusId": 244786428
                },
                "corpusId": 244786428,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/879cfa030335cff6c3d87793174fbad3968e551b",
                "title": "Dynamic Structured Pruning With Novel Filter Importance and Leaky Masking Based on Convolution and Batch Normalization Parameters",
                "abstract": "Various pruning methods have been proposed to solve the overparameterized problem in deep neural networks. Most of the structured pruning methods have used magnitude-based filter importance to remove unnecessary filters. Usually, Convolutional Neural Networks (CNN) consist of blocks that proceed with batch normalization (BN) operations after convolutional (Conv) operations. Each element is calculated through cooperation between Conv and BN, so both Conv and BN parameters must be considered together in pruning. However, previous pruning methods independently used the norm of parameters, either Conv weights or BN scales, as filter importance, ignoring these CNN structures. With this intuition, we propose a new magnitude-based filter importance method that considers both Conv weights and BN scales, and provide evidence of importance through experimental analysis on the rank of the feature map as well as mathematical analysis on the feature distortion. Furthermore, in recent works, dynamically applying a mask to recover weights has enabled more accurate pruning. However, when the weights are multiplied by zero mask values, the gradients become zero, which prevents updating the weights. We name this problem the zero gradient transferring. To solve this problem, we propose a leaky masking method that replaces the zero value of the mask with a positive constant. As a result, we solve the zero gradient transferring through the leaky masking method, enabling more accurate dynamic structured pruning. We denote our proposed method as CoBaL that incorporates Conv and BN based filter importance and Leaky mask into dynamic structured pruning. Experimental results show that our CoBaL compresses 50.09% parameters and 32.51% FLOPs on ResNet 56 with the CIFAR-10 with slightly improved accuracy. Also, we possess comparable results on ImageNet with 50.74% and 29.38% reduction in the number of parameters and FLOPs at ResNet18, respectively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2181919686",
                        "name": "Incheon Cho"
                    },
                    {
                        "authorId": "2143214909",
                        "name": "Eunseop Shin"
                    },
                    {
                        "authorId": "1935202541",
                        "name": "Muhammad Salman Ali"
                    },
                    {
                        "authorId": "40547898",
                        "name": "S. Bae"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We follow the method described in [11] to calculate the number of FLOPs required for training which is based on the total number of multiplications and additions layer by layer.",
                "In [11, 7, 25, 12], the gradient information is used to determine which connections would be changed during the evolution phase."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e7454bbba1dfd65643245d3b6087c7626e67d282",
                "externalIds": {
                    "CorpusId": 260503365
                },
                "corpusId": 260503365,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e7454bbba1dfd65643245d3b6087c7626e67d282",
                "title": "Dynamic Sparse Training for Deep Reinforcement Learning",
                "abstract": "Deep reinforcement learning has achieved significant success in many decisionmaking tasks in various fields. However, it requires a large training time of dense neural networks to obtain a good performance. This hinders its applicability on lowresource devices where memory and computation are strictly constrained. In a step towards enabling deep reinforcement learning agents to be applied to low-resource devices, in this work, we propose for the first time to dynamically train deep reinforcement learning agents with sparse neural networks from scratch. We adopt the evolution principles of dynamic sparse training in the reinforcement learning paradigm and introduce a training algorithm that optimizes the sparse topology and the weight values jointly to dynamically fit the incoming data. Our approach is easy to be integrated into existing deep reinforcement learning algorithms and has many favorable advantages. First, it allows for significant compression of the network size which reduces the memory and computation costs substantially. This would accelerate not only the agent inference but also its training process. Second, it speeds up the agent learning process and allows for reducing the number of required training steps. Third, it can achieve higher performance than training the dense counterpart network. We evaluate our approach on OpenAI gym continuous control tasks 1. The experimental results show the effectiveness of our approach in achieving higher performance than one of the state-of-art baselines with a 50% reduction in the network size and floating-point operations (FLOPs). Moreover, our proposed approach can reach the same performance achieved by the dense network with a 40-50% reduction in the number of training steps. Code available at: https://github.com/GhadaSokar/Dynamic-Sparse-Training-for-Deep-ReinforcementLearning Preprint. Under review. ar X iv :2 10 6. 04 21 7v 1 [ cs .L G ] 8 J un 2 02 1 Figure 1: An overview of our proposed method, Dynamic Sparse training of the TD3 algorithm (DS-TD3). The blue shaded blocks highlight the main modifications to the conventional algorithm.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "67102129",
                        "name": "Ghada Sokar"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "11d1b258c96ecbdc4bbc081404de2e4cee6f7189",
                "externalIds": {
                    "CorpusId": 215412771
                },
                "corpusId": 215412771,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/11d1b258c96ecbdc4bbc081404de2e4cee6f7189",
                "title": "SENTATION FOR ALL STRUCTURED LINEAR MAPS",
                "abstract": "Modern neural network architectures use structured linear transformations, such as low-rank matrices, sparse matrices, permutations, and the Fourier transform, to improve inference speed and reduce memory usage compared to general linear maps. However, choosing which of the myriad structured transformations to use (and its associated parameterization) is a laborious task that requires trading off speed, space, and accuracy. We consider a different approach: we introduce a family of matrices called kaleidoscope matrices (K-matrices) that provably capture any structured matrix with near-optimal space (parameter) and time (arithmetic operation) complexity. We empirically validate that K-matrices can be automatically learned within end-to-end pipelines to replace hand-crafted procedures, in order to improve model quality. For example, replacing channel shuffles in ShuffleNet improves classification accuracy on ImageNet by up to 5%. K-matrices can also simplify hand-engineered pipelines\u2014we replace filter bank feature computation in speech data preprocessing with a learnable kaleidoscope layer, resulting in only 0.4% loss in accuracy on the TIMIT speech recognition task. In addition, K-matrices can capture latent structure in models: for a challenging permuted image classification task, a K-matrix based representation of permutations is able to learn the right latent structure and improves accuracy of a downstream convolutional model by over 9%. We provide a practically efficient implementation of our approach, and use K-matrices in a Transformer network to attain 36% faster end-to-end inference speed on a language translation task.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "24593911",
                        "name": "Tri Dao"
                    },
                    {
                        "authorId": "145193121",
                        "name": "N. Sohoni"
                    },
                    {
                        "authorId": "39499001",
                        "name": "Albert Gu"
                    },
                    {
                        "authorId": "41022841",
                        "name": "Matthew Eichhorn"
                    },
                    {
                        "authorId": "1581517872",
                        "name": "Amit Blonder"
                    },
                    {
                        "authorId": "37866790",
                        "name": "Megan Leszczynski"
                    },
                    {
                        "authorId": "1755572",
                        "name": "A. Rudra"
                    },
                    {
                        "authorId": "1803218",
                        "name": "Christopher R\u00e9"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026uniform layer-wise magnitude pruning (Zhu & Gupta, 2017), magnitude pruning with heuristic layer-wise budgets or reallocation of weights (Dettmers & Zettlemoyer, 2019; Evci et al., 2020) and learnable sparsity methods like l1 regularization (Louizos et al., 2018) and STR (Kusupati et al., 2020).",
                ", 2016), gradual uniform layer-wise magnitude pruning (Zhu & Gupta, 2017), magnitude pruning with heuristic layer-wise budgets or reallocation of weights (Dettmers & Zettlemoyer, 2019; Evci et al., 2020) and learnable sparsity methods like l1 regularization (Louizos et al."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "578527091f8cb3313ffcebefc6acbc195ce8b70f",
                "externalIds": {
                    "CorpusId": 221493198
                },
                "corpusId": 221493198,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/578527091f8cb3313ffcebefc6acbc195ce8b70f",
                "title": "Adapting Unstructured Sparsity Techniques for Structured Sparsity",
                "abstract": "Unstructured and structured sparsities provide unique advantages in resource-efficient sparse neural networks. Unstructured sparsity can assist in obtaining highly sparse and accurate models, while structured sparsity focuses mainly on enabling fast parallelizable inference on commodity hardware (e.g. GPUs). In the recent past, these distinctive advantages led to the divergence of the sub-fields leading to a disconnect. In this report, we propose and argue that most recent advances in unstructured sparsity can be adapted for inducing structured sparsity in deep neural networks. We also note the similarities between both these two sub-fields and document how the solutions from unstructured sparsity can be leveraged in solving the issues of structured sparsity. We also showcase the ease of adaptation by proposing STR\u2212 BN which is an application of the recently proposed STR method on batch normalization to induce structured sparsity via filter/neuron pruning. Code for STR\u2212 BN can be found at https:// github.com/RAIVNLab/STR-BN.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "52207562",
                        "name": "Aditya Kusupati"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9e096337efc46fff3775ddd1db6ec20dcf2d1cd2",
                "externalIds": {
                    "CorpusId": 260633323
                },
                "corpusId": 260633323,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9e096337efc46fff3775ddd1db6ec20dcf2d1cd2",
                "title": "Privacy-preserving Learning via Deep Net Pruning",
                "abstract": "Neural network pruning has demonstrated its success in significantly improving the computational e ciency of deep models while only introducing a small reduction on final accuracy. In this paper, we explore an extra bonus of neural network pruning in terms of enhancing privacy. Specifically, we show a novel connection between magnitude-based pruning and adding di\u21b5erentially private noise to intermediate layers under the overparameterized regime. To the best of our knowledge, this is the first work that bridges pruning with the theory of di\u21b5erential privacy. The paper also presents experimental results by running the model inversion attack on two benchmark datasets, which supports the theoretical finding.",
                "year": 2020,
                "authors": []
            }
        },
        {
            "contexts": [
                "s Following the convention in (Evci et al., 2020), multiplication and addition are counted as two operations.",
                "For non-uniform sparse ResNet-50 model, the improvement over RigL5\u00d7 (Evci et al., 2020) is 1.0% and 1.5% at 80% and 90% sparsity, respectively.",
                ", 2018), DSR (Mostafa & Wang, 2019), and RigL (Evci et al., 2020) maintain the target sparsity in all layers throughout the training process and \u2020 Equal Contribution.",
                "As shown in Table 1, our implementation has slightly higher accuracy, which indicates that our code base in PyTorch is comparable with the original implementation in Evci et al. (2020), and can be used for reproducing and extending the training of RigL without causing fairness issue.",
                "We observe that the improvement over RigL5\u00d7 (Evci et al., 2020) is 1.3% (77.9% vs. 76.6",
                "Methods based on sparse mask exploration, such as DeepR (Bellec et al., 2018), SET (Mocanu et al., 2018), DSR (Mostafa & Wang, 2019), and RigL (Evci et al., 2020) maintain the target sparsity in all layers throughout the training process and\n\u2020 Equal Contribution.",
                "In Table 10, we perform additional experiments to supplement Table 1 by pruning the last FC layer using the C-GaP method and comparing them with Evci et al. (2020).",
                "In this section, we provide the implementation details of the RigL (Evci et al., 2020).",
                "We also implement the RigL and RigL5\u00d7 (Evci et al., 2020) using our code base on PyTorch (please refer to Appendix B.",
                "For the experiments with 100 training epochs, Most of the important hyper-paramters can be found in Evci et al. (2020).",
                "However, Evci et al. (2020) doesn\u2019t provide detailed hyper-parameter settings for 500-epoch training.",
                "RigL (Evci et al., 2020) and NeST (Dai et al., 2019) propose to use magnitude-based pruning and gradient-flowbased growth that update sparse model topology during training.",
                "Please note that models with the non-uniform sparsifying distribution in Table 1 already have the last FC layer pruned, thus the experiment setup is the same as the ones in Evci et al. (2020).",
                "The results in Table 10 and Table 1 indicate that the accuracy of the ResNet-50 models with sparse and dense FC layers are similar, and both of them outperform the state-of-the-art results in Evci et al. (2020).",
                "Note that previous works update weights either greedily (e.g., RigL (Evci et al., 2020)) or randomly (e.g., SET (Mocanu et al., 2018) and DSR (Mostafa & Wang, 2019)).",
                "For the uniform sparsity, the first convolutional layer with 7\u00d7 7 kernels is kept dense, the same as in Evci et al. (2020).",
                "We also implement the RigL and RigL5\u00d7 (Evci et al., 2020) using our code base on PyTorch (please refer to Appendix B.2 for details).",
                "We include the SNIP (Lee et al., 2019) and SET (Mocanu et al., 2018) results in uniform sparsity that are reproduced in Evci et al. (2020).",
                "2 OUR IMPLEMENTATION DETAILS OF RIGL In this section, we provide the implementation details of the RigL (Evci et al., 2020).",
                "We observe that the improvement over RigL5\u00d7 (Evci et al., 2020) is 1.",
                "For non-uniform sparse ResNet-50 model, the improvement over RigL5\u00d7 (Evci et al., 2020) is 1."
            ],
            "isInfluential": true,
            "intents": [],
            "citingPaper": {
                "paperId": null,
                "externalIds": null,
                "corpusId": "245124859",
                "publicationVenue": null,
                "url": null,
                "title": "Scheduled mask exploration ... Step 1 Train Step 2 Step 3 Step 4 Train Train ... ( b ) Random / Greedy mask exploration To prune Grown",
                "abstract": null,
                "year": 2021,
                "authors": []
            }
        },
        {
            "contexts": [
                "As for the uncertainty measurement, LTH and RigL show better performance than dense networks.",
                "Numerous approaches (Mocanu et al., 2016; Evci et al., 2019; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Liu et al., 2021a; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Jayakumar et al., 2020; Raihan & Aamodt, 2020; Liu et al., 2021b) study such dynamic sparsity, often matching state-of-the-art training performance (Liu et al.",
                "From Figure 11, we observe that LTH and RigL are capable of maintaining the generalization ability of dense networks at a sparsity level of 21%.",
                "And with the scope of Hessian traces for weight flatness, the subnetworks located by LTH are winning tickets on CIFAR-10 and CIFAR-100, while RigL fails to locate the flat local minima.",
                "\u2026sparsification regimes such as magnitude pruning (Han et al., 2016), lottery ticket hypothesis (Frankle & Carbin, 2019), random pruning, pruning at initialization (Lee et al., 2019; Wang et al., 2020; Tanaka et al., 2020), and dynamic sparse training (Evci et al., 2020; Liu et al., 2021b).",
                "Overall, identifying sparse neural networks from dynamic sparse training (e.g., RigL) is a great option for preserving generalization ability, interpretability, and uncertainty, but not for maintaining flat geometric of learned loss surfaces.",
                "All RigL experiments follow the recent SOTA training configurations (Liu et al., 2021b).",
                "All results and analyses about RigL are referred to Appendix A2.",
                "We choose the top-performing algorithm, RigL (Evci et al., 2020; Liu et al., 2021b), which starts from a random sparse network and encourages the connectivity to evolve dynamically based on a grow-and-prune strategy.",
                "\u2026(Mocanu et al., 2016; Evci et al., 2019; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Liu et al., 2021a; Dettmers & Zettlemoyer, 2019; Evci et al., 2020; Jayakumar et al., 2020; Raihan & Aamodt, 2020; Liu et al., 2021b) study such dynamic sparsity, often matching state-of-the-art\u2026",
                ", 2020), and dynamic sparse training (Evci et al., 2020; Liu et al., 2021b)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7c0b26c232b47b8567be1cdf675154d271209cae",
                "externalIds": {
                    "CorpusId": 250580346
                },
                "corpusId": 250580346,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7c0b26c232b47b8567be1cdf675154d271209cae",
                "title": "Can You Win Everything with A Lottery Ticket?",
                "abstract": "Lottery ticket hypothesis (LTH) has demonstrated to yield independently trainable and highly sparse neural networks (a.k.a. winning tickets ), whose test set accuracies can be surprisingly on par or even better than dense models. However, accuracy is far from the only evaluation metric, and perhaps not always the most important one. Hence it might be myopic to conclude that a sparse subnetwork can replace its dense counterpart, even if the accuracy is preserved. Spurred by that, we perform the first comprehensive assessment of lottery tickets from diverse aspects beyond test accuracy, including (i) generalization to distribution shifts, (ii) prediction uncertainty, (iii) interpretability, and (iv) geometry of loss landscapes. With extensive experiments across datasets {CIFAR-10, CIFAR-100, and ImageNet}, model architectures, as well as seven sparsification methods, we thoroughly characterize the trade-off between model sparsity and the all-dimension model capabilities. We find that an appropriate sparsity (e.g., 20% \u223c 99 . 53%) can yield the winning ticket to perform comparably or even better in all above four aspects , although some aspects (generalization to certain distribution shifts, and uncertainty) appear more sensitive to the sparsification than others. We term it as a LTH-PASS . Overall, our results endorse choosing a good sparse subnetwork of a larger dense model, over directly training a small dense model of similar parameter counts. We hope that our study can offer more in-depth insights on pruning, for researchers and engineers who seek to incorporate sparse neural networks for user-facing deployments. Codes are available in https://github.com/VITA-Group/LTH-Pass .",
                "year": null,
                "authors": [
                    {
                        "authorId": "2109338656",
                        "name": "Zhenyu (Allen) Zhang"
                    },
                    {
                        "authorId": "2147189509",
                        "name": "Jerome Friedman"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c8d6a12a973d9624699ef3e5a9e0085b4ff62533",
                "externalIds": {
                    "CorpusId": 263222223
                },
                "corpusId": 263222223,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c8d6a12a973d9624699ef3e5a9e0085b4ff62533",
                "title": "AP: Selective Activation for De-sparsifying Pruned Networks",
                "abstract": "The rectified linear unit (ReLU) is a highly successful activation function in neural networks as it allows networks to easily obtain sparse representations, which reduces overfitting in overparameterized networks. However, in the context of network pruning, we find that the sparsity introduced by ReLU, which we quantify by a term called dynamic dead neuron rate (DNR), is not beneficial for the pruned network. Interestingly, the more the network is pruned, the smaller the dynamic DNR becomes after optimization. This motivates us to propose a method to explicitly reduce the dynamic DNR for the pruned network, i.e., de-sparsify the network. We refer to our method as Activate-while-Pruning (AP). We note that AP does not function as a stand-alone method, as it does not evaluate the importance of weights. Instead, it works in tandem with existing pruning methods and aims to improve their performance by selective activation of nodes to reduce the dynamic DNR. We conduct extensive experiments using various popular networks (e.g., ResNet, VGG, DenseNet, Mo-bileNet) via two classical and three competitive pruning methods. The experimental results on public datasets (e.g., CIFAR-10, CIFAR-100) suggest that AP works well with existing pruning methods and improves the performance by 3% - 4%. For larger scale datasets (e.g., ImageNet) and competitive networks (e.g., vision transformer), we observe an improvement of 2% - 3% with AP as opposed to without. Lastly, we conduct an ablation study and a substitution study to examine the effectiveness of the components comprising AP",
                "year": null,
                "authors": [
                    {
                        "authorId": "50151902",
                        "name": "Shiyu Liu"
                    },
                    {
                        "authorId": "2978590",
                        "name": "Rohan Ghosh"
                    },
                    {
                        "authorId": "1770486",
                        "name": "M. Motani"
                    }
                ]
            }
        }
    ]
}