{
    "offset": 0,
    "data": [
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Recently, methods based on more explicit 3D representations and differentiable rendering have become popular [5, 6, 9, 11, 19, 30, 37, 39, 44, 47, 49, 50, 53, 61, 66]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5d598bfb60bc30e43c0dea491b62f5b44597837a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-02186",
                    "ArXiv": "2309.02186",
                    "DOI": "10.48550/arXiv.2309.02186",
                    "CorpusId": 261557280
                },
                "corpusId": 261557280,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5d598bfb60bc30e43c0dea491b62f5b44597837a",
                "title": "AniPortraitGAN: Animatable 3D Portrait Generation from 2D Image Collections",
                "abstract": "Previous animatable 3D-aware GANs for human generation have primarily focused on either the human head or full body. However, head-only videos are relatively uncommon in real life, and full body generation typically does not deal with facial expression control and still has challenges in generating high-quality results. Towards applicable video avatars, we present an animatable 3D-aware GAN that generates portrait images with controllable facial expression, head pose, and shoulder movements. It is a generative model trained on unstructured 2D image collections without using 3D or video data. For the new task, we base our method on the generative radiance manifold representation and equip it with learnable facial and head-shoulder deformations. A dual-camera rendering and adversarial learning scheme is proposed to improve the quality of the generated faces, which is critical for portrait images. A pose deformation processing network is developed to generate plausible deformations for challenging regions such as long hair. Experiments show that our method, trained on unstructured 2D images, can generate diverse and high-quality 3D portraits with desired control over different properties.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "72500970",
                        "name": "Yue Wu"
                    },
                    {
                        "authorId": "2110510433",
                        "name": "Sicheng Xu"
                    },
                    {
                        "authorId": "2147263432",
                        "name": "Jianfeng Xiang"
                    },
                    {
                        "authorId": "2237991050",
                        "name": "Fangyun Wei"
                    },
                    {
                        "authorId": "143832240",
                        "name": "Qifeng Chen"
                    },
                    {
                        "authorId": "2109732576",
                        "name": "Jiaolong Yang"
                    },
                    {
                        "authorId": "2147247023",
                        "name": "Xin Tong"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a9dc07473ee0f9f70a5159fbce9ee2756c68c70b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-14152",
                    "ArXiv": "2308.14152",
                    "DOI": "10.48550/arXiv.2308.14152",
                    "CorpusId": 261243016
                },
                "corpusId": 261243016,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a9dc07473ee0f9f70a5159fbce9ee2756c68c70b",
                "title": "Unaligned 2D to 3D Translation with Conditional Vector-Quantized Code Diffusion using Transformers",
                "abstract": "Generating 3D images of complex objects conditionally from a few 2D views is a difficult synthesis problem, compounded by issues such as domain gap and geometric misalignment. For instance, a unified framework such as Generative Adversarial Networks cannot achieve this unless they explicitly define both a domain-invariant and geometric-invariant joint latent distribution, whereas Neural Radiance Fields are generally unable to handle both issues as they optimize at the pixel level. By contrast, we propose a simple and novel 2D to 3D synthesis approach based on conditional diffusion with vector-quantized codes. Operating in an information-rich code space enables high-resolution 3D synthesis via full-coverage attention across the views. Specifically, we generate the 3D codes (e.g. for CT images) conditional on previously generated 3D codes and the entire codebook of two 2D views (e.g. 2D X-rays). Qualitative and quantitative results demonstrate state-of-the-art performance over specialized methods across varied evaluation criteria, including fidelity metrics such as density, coverage, and distortion metrics for two complex volumetric imagery datasets from in real-world scenarios.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1583151801",
                        "name": "Abril Corona-Figueroa"
                    },
                    {
                        "authorId": "1796272751",
                        "name": "Sam Bond-Taylor"
                    },
                    {
                        "authorId": "40336981",
                        "name": "Neelanjan Bhowmik"
                    },
                    {
                        "authorId": "2747096",
                        "name": "Y. F. A. Gaus"
                    },
                    {
                        "authorId": "1803808",
                        "name": "T. Breckon"
                    },
                    {
                        "authorId": "2840036",
                        "name": "Hubert P. H. Shum"
                    },
                    {
                        "authorId": "2240006",
                        "name": "Chris G. Willcocks"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Prior approaches utilize GANs [19, 33, 55] to synthesize data for tasks including classification [2, 8, 43, 72, 75], 3D vision [21, 52, 64, 86], 2D segmentation [44, 74, 87], dense visual alignment [54]; or diffusion models [27, 59] for data augmentation [73] or as synthetic data for few-shot learning [25]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1254f59b09b5a881ac09e49392f79af0dabff95f",
                "externalIds": {
                    "ArXiv": "2308.12288",
                    "DBLP": "journals/corr/abs-2308-12288",
                    "DOI": "10.48550/arXiv.2308.12288",
                    "CorpusId": 261076274
                },
                "corpusId": 261076274,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1254f59b09b5a881ac09e49392f79af0dabff95f",
                "title": "CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images",
                "abstract": "We present a method for teaching machines to understand and model the underlying spatial common sense of diverse human-object interactions in 3D in a self-supervised way. This is a challenging task, as there exist specific manifolds of the interactions that can be considered human-like and natural, but the human pose and the geometry of objects can vary even for similar interactions. Such diversity makes the annotating task of 3D interactions difficult and hard to scale, which limits the potential to reason about that in a supervised way. One way of learning the 3D spatial relationship between humans and objects during interaction is by showing multiple 2D images captured from different viewpoints when humans interact with the same type of objects. The core idea of our method is to leverage a generative model that produces high-quality 2D images from an arbitrary text prompt input as an\"unbounded\"data generator with effective controllability and view diversity. Despite its imperfection of the image quality over real images, we demonstrate that the synthesized images are sufficient to learn the 3D human-object spatial relations. We present multiple strategies to leverage the synthesized images, including (1) the first method to leverage a generative image model for 3D human-object spatial relation learning; (2) a framework to reason about the 3D spatial relations from inconsistent 2D cues in a self-supervised manner via 3D occupancy reasoning with pose canonicalization; (3) semantic clustering to disambiguate different types of interactions with the same object types; and (4) a novel metric to assess the quality of 3D spatial learning of interaction.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2218014742",
                        "name": "Sookwan Han"
                    },
                    {
                        "authorId": "7996087",
                        "name": "H. Joo"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "\u03c3y\u2217 along with a pre-trained recognition network [35] that is responsible for facial identity regularization.",
                "The pre-trained face embedding model [35], which uses ResNet18 [38] has been leveraged for lossid and lossseg."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "019a9150c1b851870a4e6b7a8171ee7dba6097f7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-09020",
                    "ArXiv": "2307.09020",
                    "DOI": "10.48550/arXiv.2307.09020",
                    "CorpusId": 259950871
                },
                "corpusId": 259950871,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/019a9150c1b851870a4e6b7a8171ee7dba6097f7",
                "title": "Face-PAST: Facial Pose Awareness and Style Transfer Networks",
                "abstract": "Facial style transfer has been quite popular among researchers due to the rise of emerging technologies such as eXtended Reality (XR), Metaverse, and Non-Fungible Tokens (NFTs). Furthermore, StyleGAN methods along with transfer-learning strategies have reduced the problem of limited data to some extent. However, most of the StyleGAN methods overfit the styles while adding artifacts to facial images. In this paper, we propose a facial pose awareness and style transfer (Face-PAST) network that preserves facial details and structures while generating high-quality stylized images. Dual StyleGAN inspires our work, but in contrast, our work uses a pre-trained style generation network in an external style pass with a residual modulation block instead of a transform coding block. Furthermore, we use the gated mapping unit and facial structure, identity, and segmentation losses to preserve the facial structure and details. This enables us to train the network with a very limited amount of data while generating high-quality stylized images. Our training process adapts curriculum learning strategy to perform efficient and flexible style mixing in the generative space. We perform extensive experiments to show the superiority of Face-PAST in comparison to existing state-of-the-art methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "10779576",
                        "name": "Sunder Ali Khowaja"
                    },
                    {
                        "authorId": "2280741",
                        "name": "G. Mujtaba"
                    },
                    {
                        "authorId": "2163057535",
                        "name": "Jiseok Yoon"
                    },
                    {
                        "authorId": "2149677825",
                        "name": "Ik Hyun Lee"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Other methods [7, 8, 40, 46] utilized direct 3D representations and generated images with representations of the neural radiance field."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "08088138a2fc9c6b1a54691f476ba7adb8d9e38a",
                "externalIds": {
                    "DBLP": "conf/mir/HwangKL023",
                    "DOI": "10.1145/3591106.3592273",
                    "CorpusId": 259112729
                },
                "corpusId": 259112729,
                "publicationVenue": {
                    "id": "b7d34536-73df-402d-8967-50a9cdf73c01",
                    "name": "International Conference on Multimedia Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "ICMR",
                        "Int Conf Multimedia Retr"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1424"
                },
                "url": "https://www.semanticscholar.org/paper/08088138a2fc9c6b1a54691f476ba7adb8d9e38a",
                "title": "Unlocking Potential of 3D-aware GAN for More Expressive Face Generation",
                "abstract": "As style-based image generators have achieved disentanglement in features by converting latent vector space to style vector space, numerous efforts have been made to enhance the controllability of the latent. However, existing methods for controllable models have limitations in precisely creating high-resolution faces with large expressions. The degradation is due to the dependence on the training dataset, as the high-resolution face datasets do not have sufficient expressive images. To tackle this challenge, we propose a robust training framework for 3D-aware generative adversarial networks to learn the high-quality generation of more expressive faces through a signed distance field. First, we propose a novel 3D enforcement loss to generate more expressive images in an unsupervised manner. Second, we introduce a partial training method to fine-tune the network on multiple datasets without loss of image resolution. Finally, we propose a ray-scaling scheme for the volume renderer to represent a face at arbitrary scales. Through the proposed framework, the network learns 3D face priors, such as expressional shapes of the parametric facial model, to generate detailed faces. The experimental results outperform the methods of the state of the art, showing strong benefits in the generation of high-resolution facial expressions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220600689",
                        "name": "Juheon Hwang"
                    },
                    {
                        "authorId": "143661625",
                        "name": "Jiwoo Kang"
                    },
                    {
                        "authorId": "2110246363",
                        "name": "Kyoungoh Lee"
                    },
                    {
                        "authorId": "2144570602",
                        "name": "Sanghoon Lee"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "aa296e10312e730173f116faad62ad937a2842cd",
                "externalIds": {
                    "DBLP": "conf/cvpr/0005CCTW23",
                    "DOI": "10.1109/CVPR52729.2023.00045",
                    "CorpusId": 261081253
                },
                "corpusId": 261081253,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/aa296e10312e730173f116faad62ad937a2842cd",
                "title": "Learning Neural Proto-Face Field for Disentangled 3D Face Modeling in the Wild",
                "abstract": "Generative models show good potential for recovering 3D faces beyond limited shape assumptions. While plausible details and resolutions are achieved, these models easily fail under extreme conditions of pose, shadow or appearance, due to the entangled fitting or lack of multi-view priors. To address this problem, this paper presents a novel Neural Proto-face Field (NPF) for unsupervised robust 3D face modeling. Instead of using constrained images as Neural Radiance Field (NeRF), NPF disentangles the common/specific facial cues, i.e., ID, expression and scene-specific details from in-the-wild photo collections. Specifically, NPF learns a face prototype to aggregate 3D-consistent identity via uncertainty modeling, extracting multi-image priors from a photo collection. NPF then learns to deform the prototype with the appropriate facial expressions, constrained by a loss of expression consistency and personal idiosyncrasies. Finally, NPF is optimized to fit a target image in the collection, recovering specific details of appearance and geometry. In this way, the generative model benefits from multi-image priors and meaningful facial structures. Extensive experiments on benchmarks show that NPF recovers superior or competitive facial shapes and textures, compared to state-of-the-art methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2192943489",
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "authorId": "1625895990",
                        "name": "Renwang Chen"
                    },
                    {
                        "authorId": "2075437879",
                        "name": "Weijian Cao"
                    },
                    {
                        "authorId": "144970872",
                        "name": "Ying Tai"
                    },
                    {
                        "authorId": "1978245",
                        "name": "Chengjie Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "For those methods (including ShadeGAN, LiftedStyleGAN, and Ours) that can generate albedo images, the reference images are replaced with albedo images.",
                "2021], LiftedStyleGAN [Shi et al. 2021], EG3D+Deep Portrait Relighting [Zhou et al.",
                "However, the generated samples of ShadeGAN and LiftedStyleGAN are of lower quality compared to those by ours and 2D methods.",
                ", [Pan et al. 2022, 2021; Shi et al. 2021]) which explicitly model the lighting efects through lighting models, our method can be categorized as the implicit representation learned from the dataset.",
                "We compare our method with ShadeGAN [Pan et al. 2021], LiftedStyleGAN [Shi et al. 2021], EG3D+Deep Portrait Relighting [Zhou et al. 2019], EG3D+StyleFlow [Abdal et al. 2021], and 3DFaceShop [Tang et al. 2022] as alternative 3D-aware methods."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "9a2977ea32a0fcc58b319a1104ae173b90a30779",
                "externalIds": {
                    "DBLP": "journals/tog/JiangC0023",
                    "DOI": "10.1145/3597300",
                    "CorpusId": 258688052
                },
                "corpusId": 258688052,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9a2977ea32a0fcc58b319a1104ae173b90a30779",
                "title": "NeRFFaceLighting: Implicit and Disentangled Face Lighting Representation Leveraging Generative Prior in Neural Radiance Fields",
                "abstract": "3D-aware portrait lighting control is an emerging and promising domain, thanks to the recent advance of generative adversarial networks and neural radiance fields. Existing solutions typically try to decouple the lighting from the geometry and appearance for disentangled control with an explicit lighting representation (e.g., Lambertian or Phong). However, they either are limited to a constrained lighting condition (e.g., directional light) or demand a tricky-to-fetch dataset as supervision for the intrinsic compositions (e.g., the albedo). We propose NeRFFaceLighting to explore an implicit representation for portrait lighting based on the pretrained tri-plane representation to address the above limitations. We approach this disentangled lighting-control problem by distilling the shading from the original fused representation of both appearance and lighting (i.e., one tri-plane) to their disentangled representations (i.e., two tri-planes) with the conditional discriminator to supervise the lighting effects. We further carefully design the regularization to reduce the ambiguity of such decomposition and enhance the ability of generalization to unseen lighting conditions. Moreover, our method can be extended to enable 3D-aware real portrait relighting. Through extensive quantitative and qualitative evaluations, we demonstrate the superior 3D-aware lighting control ability of our model compared to alternative and existing solutions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2217269076",
                        "name": "Kaiwen Jiang"
                    },
                    {
                        "authorId": "2999411",
                        "name": "Shu-Yu Chen"
                    },
                    {
                        "authorId": "3169698",
                        "name": "Hongbo Fu"
                    },
                    {
                        "authorId": "144614914",
                        "name": "Lin Gao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "68a6780da08bf678a2c48fb8c187626f1ea1ded1",
                "externalIds": {
                    "DBLP": "journals/icae/LiHFFY23",
                    "DOI": "10.3233/ica-230710",
                    "CorpusId": 258667946
                },
                "corpusId": 258667946,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/68a6780da08bf678a2c48fb8c187626f1ea1ded1",
                "title": "3D reconstruction based on hierarchical reinforcement learning with transferability",
                "abstract": "3D reconstruction is extremely important in CAD (computer-aided design)/CAE (computer-aided Engineering)/CAM (computer-aided manufacturing). For interpretability, reinforcement learning (RL) is used to reconstruct 3D shapes from images by a series of editing actions. However, typical applications of RL for 3D reconstruction face problems. The search space will increase exponentially with the action space due to the curse of dimensionality, which leads to low performance, especially for complex action spaces in 3D reconstruction. Additionally, most works involve training a specific agent for each shape class without learning related experiences from others. Therefore, we present a hierarchical RL approach with transferability to reconstruct 3D shapes (HRLT3D). First, actions are grouped into macro actions that can be chosen by the top-agent. Second, the task is accordingly decomposed into hierarchically simplified sub-tasks solved by sub-agents. Different from classical hierarchical RL (HRL), we propose a sub-agent based on augmented state space (ASS-Sub-Agent) to replace a set of sub-agents, which can speed up the training process due to shared learning and having fewer parameters. Furthermore, the ASS-Sub-Agent is more easily transferred to data of other classes due to the augmented diverse states and the simplified tasks. The experimental results on typical public dataset show that the proposed HRLT3D performs overwhelmingly better than recent baselines. More impressingly, the experiments also demonstrate the extreme transferability of our approach among data of different classes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2217125934",
                        "name": "Lan Li"
                    },
                    {
                        "authorId": "71340271",
                        "name": "Fazhi He"
                    },
                    {
                        "authorId": "2217137319",
                        "name": "Rubin Fan"
                    },
                    {
                        "authorId": "2217112314",
                        "name": "Bo Fan"
                    },
                    {
                        "authorId": "46580994",
                        "name": "Xiaohu Yan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dad8852fec4b4eaba8bf3c2652b13c34a0a04742",
                "externalIds": {
                    "DBLP": "journals/tog/TrevithickCSCLYKCRN23",
                    "ArXiv": "2305.02310",
                    "DOI": "10.1145/3592460",
                    "CorpusId": 258461577
                },
                "corpusId": 258461577,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/dad8852fec4b4eaba8bf3c2652b13c34a0a04742",
                "title": "Real-Time Radiance Fields for Single-Image Portrait View Synthesis",
                "abstract": "We present a one-shot method to infer and render a photorealistic 3D representation from a single unposed image (e.g., face portrait) in real-time. Given a single RGB input, our image encoder directly predicts a canonical triplane representation of a neural radiance field for 3D-aware novel view synthesis via volume rendering. Our method is fast (24 fps) on consumer hardware, and produces higher quality results than strong GAN-inversion baselines that require test-time optimization. To train our triplane encoder pipeline, we use only synthetic data, showing how to distill the knowledge from a pretrained 3D GAN into a feedforward encoder. Technical contributions include a Vision Transformer-based triplane encoder, a camera data augmentation strategy, and a well-designed loss function for synthetic data training. We benchmark against the state-of-the-art methods, demonstrating significant improvements in robustness and image quality in challenging real-world settings. We showcase our results on portraits of faces (FFHQ) and cats (AFHQ), but our algorithm can also be applied in the future to other categories with a 3D-aware image generator.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1993540993",
                        "name": "Alex Trevithick"
                    },
                    {
                        "authorId": "2147382797",
                        "name": "Matthew Chan"
                    },
                    {
                        "authorId": "2267017",
                        "name": "Michael Stengel"
                    },
                    {
                        "authorId": "121028414",
                        "name": "Eric Chan"
                    },
                    {
                        "authorId": "2152505971",
                        "name": "Chao Liu"
                    },
                    {
                        "authorId": "1751019",
                        "name": "Zhiding Yu"
                    },
                    {
                        "authorId": "2121982",
                        "name": "S. Khamis"
                    },
                    {
                        "authorId": "2099305",
                        "name": "Manmohan Chandraker"
                    },
                    {
                        "authorId": "1752236",
                        "name": "R. Ramamoorthi"
                    },
                    {
                        "authorId": "1897417",
                        "name": "Koki Nagano"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1606f818b6d78713149fb322c234caee5ff7210e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-00121",
                    "ArXiv": "2305.00121",
                    "DOI": "10.1109/CVPR52729.2023.02014",
                    "CorpusId": 258426810
                },
                "corpusId": 258426810,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1606f818b6d78713149fb322c234caee5ff7210e",
                "title": "Learning Locally Editable Virtual Humans",
                "abstract": "In this paper, we propose a novel hybrid representation and end-to-end trainable network architecture to model fully editable and customizable neural avatars. At the core of our work lies a representation that combines the modeling power of neural fields with the ease of use and inherent 3D consistency of skinned meshes. To this end, we construct a trainable feature codebook to store local geometry and texture features on the vertices of a deformable body model, thus exploiting its consistent topology under articulation. This representation is then employed in a generative auto-decoder architecture that admits fitting to unseen scans and sampling of realistic avatars with varied appearances and geometries. Furthermore, our representation allows local editing by swapping local features between 3D assets. To verify our method for avatar creation and editing, we contribute a new highquality dataset, dubbed CustomHumans, for training and evaluation. Our experiments quantitatively and qualitatively show that our method generates diverse detailed avatars and achieves better model fitting performance compared to state-of-the-art methods. Our code and dataset are available at https://ait.ethz.ch/customhumans.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "28033903",
                        "name": "Hsuan-I Ho"
                    },
                    {
                        "authorId": "2215825655",
                        "name": "Lixin Xue"
                    },
                    {
                        "authorId": "2143424560",
                        "name": "Jie Song"
                    },
                    {
                        "authorId": "1466533438",
                        "name": "Otmar Hilliges"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fe0db20c3640c54b1744f38d4f75b3e9aa3a3550",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-12455",
                    "ArXiv": "2304.12455",
                    "DOI": "10.1109/CVPRW59228.2023.00079",
                    "CorpusId": 258309158
                },
                "corpusId": 258309158,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fe0db20c3640c54b1744f38d4f75b3e9aa3a3550",
                "title": "Unsupervised Style-based Explicit 3D Face Reconstruction from Single Image",
                "abstract": "Inferring 3D object structures from a single image is an ill-posed task due to depth ambiguity and occlusion. Typical resolutions in the literature include leveraging 2D or 3D ground truth for supervised learning, as well as imposing hand-crafted symmetry priors or using an implicit representation to hallucinate novel viewpoints for unsupervised methods. In this work, we propose a general adversarial learning framework for solving Unsupervised 2D to Explicit 3D Style Transfer (UE3DST). Specifically, we merge two architectures: the unsupervised explicit 3D reconstruction network of Wu et al. and the Generative Adversarial Network (GAN) named StarGAN-v2. We experiment across three facial datasets (Basel Face Model, 3DFAW and CelebA-HQ) and show that our solution is able to outperform well established solutions such as DepthNet in 3D reconstruction and Pix2NeRF in conditional style transfer, while we also justify the individual contributions of our model components via ablation. In contrast to the aforementioned baselines, our scheme produces features for explicit 3D rendering, which can be manipulated and utilized in downstream tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2149407119",
                        "name": "Heng Yu"
                    },
                    {
                        "authorId": "2482675",
                        "name": "Z. '. Milacski"
                    },
                    {
                        "authorId": "2331345",
                        "name": "L\u00e1szl\u00f3 A. Jeni"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We also quantitatively compare texture and rendered faces by rendering it on a mesh with other methods, including LiftedGAN [49], DECA [15], and OSTeC [18].",
                "The corresponding 2D face images are used to perform 3D reconstruction using other methods[15, 18, 49]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1a91bc376aa93b8b633a35607929739fa050370b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-12483",
                    "ArXiv": "2304.12483",
                    "DOI": "10.48550/arXiv.2304.12483",
                    "CorpusId": 258309411
                },
                "corpusId": 258309411,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1a91bc376aa93b8b633a35607929739fa050370b",
                "title": "Towards Realistic Generative 3D Face Models",
                "abstract": "In recent years, there has been significant progress in 2D generative face models fueled by applications such as animation, synthetic data generation, and digital avatars. However, due to the absence of 3D information, these 2D models often struggle to accurately disentangle facial attributes like pose, expression, and illumination, limiting their editing capabilities. To address this limitation, this paper proposes a 3D controllable generative face model to produce high-quality albedo and precise 3D shape leveraging existing 2D generative models. By combining 2D face generative models with semantic face manipulation, this method enables editing of detailed 3D rendered faces. The proposed framework utilizes an alternating descent optimization approach over shape and albedo. Differentiable rendering is used to train high-quality shapes and albedo without 3D supervision. Moreover, this approach outperforms the state-of-the-art (SOTA) methods in the well-known NoW benchmark for shape reconstruction. It also outperforms the SOTA reconstruction models in recovering rendered faces' identities across novel poses by an average of 10%. Additionally, the paper demonstrates direct control of expressions in 3D faces by exploiting latent space leading to text-based editing of 3D faces.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "123611867",
                        "name": "Aashish Rai"
                    },
                    {
                        "authorId": "1381295186",
                        "name": "Hiresh Gupta"
                    },
                    {
                        "authorId": "39685357",
                        "name": "Ayush Pandey"
                    },
                    {
                        "authorId": "2215270113",
                        "name": "Francisco Vicente Carrasco"
                    },
                    {
                        "authorId": "2067051400",
                        "name": "Shingo Takagi"
                    },
                    {
                        "authorId": "3353008",
                        "name": "Amaury Aubel"
                    },
                    {
                        "authorId": "2109124642",
                        "name": "Daeil Kim"
                    },
                    {
                        "authorId": "32626817",
                        "name": "Aayush Prakash"
                    },
                    {
                        "authorId": "143867160",
                        "name": "F. D. L. Torre"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Further, StyleGAN has been extended to get novel views from images [27, 28, 44], thus making it possible to get 3D information from it."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9209ff89f3e579a104aba3206300dc0c1f5c0afd",
                "externalIds": {
                    "DBLP": "conf/cvpr/RangwaniBSKJB23",
                    "ArXiv": "2304.05866",
                    "DOI": "10.1109/CVPR52729.2023.00580",
                    "CorpusId": 258079238
                },
                "corpusId": 258079238,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9209ff89f3e579a104aba3206300dc0c1f5c0afd",
                "title": "NoisyTwins: Class-Consistent and Diverse Image Generation Through StyleGANs",
                "abstract": "StyleGANs are at the forefront of controllable image generation as they produce a latent space that is semantically disentangled, making it suitable for image editing and manipulation. However, the performance of StyleGANs severely degrades when trained via class-conditioning on large-scale long-tailed datasets. We find that one reason for degradation is the collapse of latents for each class in the $\\mathcal{W}$ latent space. With NoisyTwins, we first introduce an effective and inexpensive augmentation strategy for class embeddings, which then decorrelates the latents based on self-supervision in the $\\mathcal{W}$ space. This decorrelation mitigates collapse, ensuring that our method preserves intra-class diversity with class-consistency in image generation. We show the effectiveness of our approach on large-scale real-world long-tailed datasets of ImageNet-LT and iNaturalist 2019, where our method outperforms other methods by \u223c 19% on FID, establishing a new state-of-the-art.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46224589",
                        "name": "Harsh Rangwani"
                    },
                    {
                        "authorId": "2214202686",
                        "name": "Lavish Bansal"
                    },
                    {
                        "authorId": "1571168324",
                        "name": "Kartik Sharma"
                    },
                    {
                        "authorId": "115373370",
                        "name": "Tejan Karmali"
                    },
                    {
                        "authorId": "2131639924",
                        "name": "Varun Jampani"
                    },
                    {
                        "authorId": "144682140",
                        "name": "R. Venkatesh Babu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[33] disentangle and distill the 3D information from StyleGAN2 to 3D-aware face generation."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "410469645334b02da4b8bd206f5ffe76c71a71cb",
                "externalIds": {
                    "DBLP": "conf/cvpr/LiLWMC23",
                    "ArXiv": "2304.03526",
                    "DOI": "10.1109/CVPR52729.2023.00040",
                    "CorpusId": 258041122
                },
                "corpusId": 258041122,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/410469645334b02da4b8bd206f5ffe76c71a71cb",
                "title": "Lift3D: Synthesize 3D Training Data by Lifting 2D GAN to 3D Generative Radiance Field",
                "abstract": "This work explores the use of 3D generative models to synthesize training data for 3D vision tasks. The key requirements of the generative models are that the generated data should be photorealistic to match the real-world scenarios, and the corresponding 3D attributes should be aligned with given sampling labels. However, we find that the recent NeRF-based 3D GANs hardly meet the above requirements due to their designed generation pipeline and the lack of explicit 3D supervision. In this work, we propose Lift3D, an inverted 2D-to-3D generation framework to achieve the data generation objectives. Lift3D has several merits compared to prior methods: (1) Unlike previous 3D GANs that the output resolution is fixed after training, Lift3D can generalize to any camera intrinsic with higher resolution and photorealistic output. (2) By lifting well-disentangled 2D GAN to 3D object NeRF, Lift3D provides explicit 3D information of generated objects, thus offering accurate 3D annotations for downstream tasks. We evaluate the effectiveness of our framework by augmenting autonomous driving datasets. Experimental results demonstrate that our data generation framework can effectively improve the performance of 3D object detectors. Code: len-li.github.io/lift3d-web",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2041715720",
                        "name": "Leheng Li"
                    },
                    {
                        "authorId": "2059872750",
                        "name": "Qing Lian"
                    },
                    {
                        "authorId": "2213771562",
                        "name": "Luozhou Wang"
                    },
                    {
                        "authorId": "2068605434",
                        "name": "Ningning Ma"
                    },
                    {
                        "authorId": "104375063",
                        "name": "Yingke Chen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c1dbd8e6d8a779faa914950a2e5b3a69a76161ec",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-01172",
                    "ArXiv": "2304.01172",
                    "DOI": "10.48550/arXiv.2304.01172",
                    "CorpusId": 257913354
                },
                "corpusId": 257913354,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c1dbd8e6d8a779faa914950a2e5b3a69a76161ec",
                "title": "Generative Multiplane Neural Radiance for 3D-Aware Image Generation",
                "abstract": "We present a method to efficiently generate 3D-aware high-resolution images that are view-consistent across multiple target views. The proposed multiplane neural radiance model, named GMNR, consists of a novel {\\alpha}-guided view-dependent representation ({\\alpha}-VdR) module for learning view-dependent information. The {\\alpha}-VdR module, faciliated by an {\\alpha}-guided pixel sampling technique, computes the view-dependent representation efficiently by learning viewing direction and position coefficients. Moreover, we propose a view-consistency loss to enforce photometric similarity across multiple views. The GMNR model can generate 3D-aware high-resolution images that are viewconsistent across multiple camera poses, while maintaining the computational efficiency in terms of both training and inference time. Experiments on three datasets demonstrate the effectiveness of the proposed modules, leading to favorable results in terms of both generation quality and inference time, compared to existing approaches. Our GMNR model generates 3D-aware images of 1024 X 1024 pixels with 17.6 FPS on a single V100. Code : https://github.com/VIROBO-15/GMNR",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2123129131",
                        "name": "Amandeep Kumar"
                    },
                    {
                        "authorId": "26418979",
                        "name": "A. Bhunia"
                    },
                    {
                        "authorId": "3347447",
                        "name": "Sanath Narayan"
                    },
                    {
                        "authorId": "2951229",
                        "name": "Hisham Cholakkal"
                    },
                    {
                        "authorId": "3288214",
                        "name": "R. Anwer"
                    },
                    {
                        "authorId": "2111180748",
                        "name": "Salman Siddique Khan"
                    },
                    {
                        "authorId": "152790163",
                        "name": "Ming Yang"
                    },
                    {
                        "authorId": "2358803",
                        "name": "F. Khan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "result"
            ],
            "contexts": [
                "This differs from baseline methods [6, 7, 31, 36] that use Deng et al.",
                "\u2022 Identity consistency (ID): As in [6, 7, 31, 36], we compute ID with the mean Arcface [8] cosine similarity, after rendering a face with two random camera views.",
                "For depth consistency, following previous work [6, 31, 36], we estimate pseudo ground truth depth from Deng et al."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c0ec788894c22c5356a00c1ec57da49c58228407",
                "externalIds": {
                    "DBLP": "conf/cvpr/RanjanYCT23",
                    "ArXiv": "2303.15437",
                    "DOI": "10.1109/CVPR52729.2023.00833",
                    "CorpusId": 257767004
                },
                "corpusId": 257767004,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c0ec788894c22c5356a00c1ec57da49c58228407",
                "title": "FaceLit: Neural 3D Relightable Faces",
                "abstract": "We propose a generative framework, FaceLit, capable of generating a 3D face that can be rendered at various user-defined lighting conditions and views, learned purely from 2D images in-the-wild without any manual annotation. Unlike existing works that require careful capture setup or human labor, we rely on off-the-shelf pose and illumination estimators. With these estimates, we incorporate the Phong reflectance model in the neural volume rendering framework. Our model learns to generate shape and material properties of a face such that, when rendered according to the natural statistics of pose and illumination, produces photorealistic face images with multiview 3D and illumination consistency. Our method enables photorealistic generation of faces with explicit illumination and view controls on multiple datasets - FFHQ, MetFaces and CelebA-HQ. We show state-of-the-art photorealism among 3D aware GANs on FFHQ dataset achieving an FID score of 3.5.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1952002",
                        "name": "Anurag Ranjan"
                    },
                    {
                        "authorId": "2906716",
                        "name": "K. M. Yi"
                    },
                    {
                        "authorId": "2148969927",
                        "name": "Jen-Hao Rick Chang"
                    },
                    {
                        "authorId": "2577513",
                        "name": "Oncel Tuzel"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Recently, a line of work has explored neural 3D representations by unsupervised training of 3D-aware GANs from in-the-wild unstructured images [7, 8, 11, 17, 37, 44, 45, 48, 61, 62, 67]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6cdbb7efd6242a85ac3ed3fe0f50e6bf081ae7ff",
                "externalIds": {
                    "ArXiv": "2303.15539",
                    "DBLP": "journals/corr/abs-2303-15539",
                    "DOI": "10.1109/CVPR52729.2023.01232",
                    "CorpusId": 257771568
                },
                "corpusId": 257771568,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6cdbb7efd6242a85ac3ed3fe0f50e6bf081ae7ff",
                "title": "OmniAvatar: Geometry-Guided Controllable 3D Head Synthesis",
                "abstract": "We present OmniAvatar, a novel geometry-guided 3D head synthesis model trained from in-the-wild unstructured images that is capable of synthesizing diverse identity-preserved 3D heads with compelling dynamic details under full disentangled control over camera poses, facial expressions, head shapes, articulated neck and jaw poses. To achieve such high level of disentangled control, we first explicitly define a novel semantic signed distance function (SDF) around a head geometry (FLAME) conditioned on the control parameters. This semantic SDF allows us to build a differentiable volumetric correspondence map from the observation space to a disentangled canonical space from all the control parameters. We then leverage the 3D-aware GAN framework (EG3D) to synthesize detailed shape and appearance of 3D full heads in the canonical space, followed by a volume rendering step guided by the volumetric correspondence map to output into the observation space. To ensure the control accuracy on the synthesized head shapes and expressions, we introduce a geometry prior loss to conform to head SDF and a control loss to conform to the expression code. Further, we enhance the temporal realism with dynamic details conditioned upon varying expressions and joint poses. Our model can synthesize more preferable identity-preserved 3D heads with compelling dynamic details compared to the state-of-the-art methods both qualitatively and quantitatively. We also provide an ablation study to justify many of our system design choices.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47995443",
                        "name": "Hongyi Xu"
                    },
                    {
                        "authorId": "7436326",
                        "name": "Guoxian Song"
                    },
                    {
                        "authorId": "145062296",
                        "name": "Zihang Jiang"
                    },
                    {
                        "authorId": "2107968449",
                        "name": "Jianfeng Zhang"
                    },
                    {
                        "authorId": "9644181",
                        "name": "Yichun Shi"
                    },
                    {
                        "authorId": "2153468093",
                        "name": "Jing Liu"
                    },
                    {
                        "authorId": "2249281630",
                        "name": "Wan-Chun Ma"
                    },
                    {
                        "authorId": "1698982",
                        "name": "Jiashi Feng"
                    },
                    {
                        "authorId": "40359898",
                        "name": "Linjie Luo"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[39] proposed a self-supervised framework to convert 2D StyleGANs [21] into 3D generative models, although its generalizability is bounded by its base 2D StyleGAN."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6f59f7199656da81a0eb165b1df7b7d64d0ac0d0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-13071",
                    "ArXiv": "2303.13071",
                    "DOI": "10.1109/CVPR52729.2023.02007",
                    "CorpusId": 257687701
                },
                "corpusId": 257687701,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6f59f7199656da81a0eb165b1df7b7d64d0ac0d0",
                "title": "PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360\u00b0",
                "abstract": "Synthesis and reconstruction of 3D human head has gained increasing interests in computer vision and computer graphics recently. Existing state-of-the-art 3D generative adversarial networks (GANs) for 3D human head synthesis are either limited to near-frontal views or hard to preserve 3D consistency in large view angles. We propose PanoHead, the first 3D-aware generative model that enables high-quality view-consistent image synthesis of full heads in 360\u00b0 with diverse appearance and detailed geometry using only in-the-wild unstructured images for training. At its core, we lift up the representation power of recent 3D GANs and bridge the data alignment gap when training from in-the-wild images with widely distributed views. Specifically, we propose a novel two-stage self-adaptive image alignment for robust 3D GAN training. We further introduce a tri-grid neural volume representation that effectively addresses front-face and back-head feature entanglement rooted in the widely-adopted tri-plane formulation. Our method instills prior knowledge of 2D image segmentation in adversarial learning of 3D neural scene structures, enabling compositable head synthesis in diverse backgrounds. Benefiting from these designs, our method significantly outperforms previous 3D GANs, generating high-quality 3D heads with accurate geometry and diverse appearances, even with long wavy and afro hairstyles, renderable from arbitrary poses. Furthermore, we show that our system can reconstruct full 3D heads from single input images for personalized realistic 3D avatars.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "122369339",
                        "name": "Sizhe An"
                    },
                    {
                        "authorId": "47995443",
                        "name": "Hongyi Xu"
                    },
                    {
                        "authorId": "9644181",
                        "name": "Yichun Shi"
                    },
                    {
                        "authorId": "7436326",
                        "name": "Guoxian Song"
                    },
                    {
                        "authorId": "117025233",
                        "name": "U. Ogras"
                    },
                    {
                        "authorId": "40359898",
                        "name": "Linjie Luo"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "To measure the geometry properties, we followed [5, 41], utilizing a pre-trained 3D face reconstruction model to extract a \u201cpseudo\u201d ground truth depth map from the source image."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dab3c50a440247990b1ab3ae793828567c66ab18",
                "externalIds": {
                    "ArXiv": "2303.12326",
                    "DBLP": "journals/corr/abs-2303-12326",
                    "DOI": "10.48550/arXiv.2303.12326",
                    "CorpusId": 257663322
                },
                "corpusId": 257663322,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dab3c50a440247990b1ab3ae793828567c66ab18",
                "title": "Make Encoder Great Again in 3D GAN Inversion through Geometry and Occlusion-Aware Encoding",
                "abstract": "3D GAN inversion aims to achieve high reconstruction fidelity and reasonable 3D geometry simultaneously from a single image input. However, existing 3D GAN inversion methods rely on time-consuming optimization for each individual case. In this work, we introduce a novel encoder-based inversion framework based on EG3D, one of the most widely-used 3D GAN models. We leverage the inherent properties of EG3D's latent space to design a discriminator and a background depth regularization. This enables us to train a geometry-aware encoder capable of converting the input image into corresponding latent code. Additionally, we explore the feature space of EG3D and develop an adaptive refinement stage that improves the representation ability of features in EG3D to enhance the recovery of fine-grained textural details. Finally, we propose an occlusion-aware fusion operation to prevent distortion in unobserved regions. Our method achieves impressive results comparable to optimization-based methods while operating up to 500 times faster. Our framework is well-suited for applications such as semantic editing.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "8591253",
                        "name": "Ziyang Yuan"
                    },
                    {
                        "authorId": "2118321150",
                        "name": "Yiming Zhu"
                    },
                    {
                        "authorId": "2116609876",
                        "name": "Yu Li"
                    },
                    {
                        "authorId": "2115669461",
                        "name": "Hongyu Liu"
                    },
                    {
                        "authorId": "2175625059",
                        "name": "Chun Yuan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "03209edc230adfcf4ffc056f5b6e2cc7d0f946a4",
                "externalIds": {
                    "ArXiv": "2303.12865",
                    "DBLP": "journals/corr/abs-2303-12865",
                    "DOI": "10.48550/arXiv.2303.12865",
                    "CorpusId": 257687857
                },
                "corpusId": 257687857,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/03209edc230adfcf4ffc056f5b6e2cc7d0f946a4",
                "title": "NeRF-GAN Distillation for Efficient 3D-Aware Generation with Convolutions",
                "abstract": "Pose-conditioned convolutional generative models struggle with high-quality 3D-consistent image generation from single-view datasets, due to their lack of sufficient 3D priors. Recently, the integration of Neural Radiance Fields (NeRFs) and generative models, such as Generative Adversarial Networks (GANs), has transformed 3D-aware generation from single-view images. NeRF-GANs exploit the strong inductive bias of neural 3D representations and volumetric rendering at the cost of higher computational complexity. This study aims at revisiting pose-conditioned 2D GANs for efficient 3D-aware generation at inference time by distilling 3D knowledge from pretrained NeRF-GANs. We propose a simple and effective method, based on re-using the well-disentangled latent space of a pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly generate 3D-consistent images corresponding to the underlying 3D representations. Experiments on several datasets demonstrate that the proposed method obtains results comparable with volumetric rendering in terms of quality and 3D consistency while benefiting from the computational advantage of convolutional networks. The code will be available at: https://github.com/mshahbazi72/NeRF-GAN-Distillation",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "73774192",
                        "name": "Mohamad Shahbazi"
                    },
                    {
                        "authorId": "1628458093",
                        "name": "Evangelos Ntavelis"
                    },
                    {
                        "authorId": "20406113",
                        "name": "A. Tonioni"
                    },
                    {
                        "authorId": "33942393",
                        "name": "Edo Collins"
                    },
                    {
                        "authorId": "35268081",
                        "name": "D. Paudel"
                    },
                    {
                        "authorId": "2129520569",
                        "name": "Martin Danelljan"
                    },
                    {
                        "authorId": "1681236",
                        "name": "L. Gool"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7eb2afee6823eebdf37f379a7eb1a2d1fb0a98a5",
                "externalIds": {
                    "ArXiv": "2303.09036",
                    "DBLP": "journals/corr/abs-2303-09036",
                    "DOI": "10.48550/arXiv.2303.09036",
                    "CorpusId": 257557618
                },
                "corpusId": 257557618,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7eb2afee6823eebdf37f379a7eb1a2d1fb0a98a5",
                "title": "Mimic3D: Thriving 3D-Aware GANs via 3D-to-2D Imitation",
                "abstract": "Generating images with both photorealism and multiview 3D consistency is crucial for 3D-aware GANs, yet existing methods struggle to achieve them simultaneously. Improving the photorealism via CNN-based 2D super-resolution can break the strict 3D consistency, while keeping the 3D consistency by learning high-resolution 3D representations for direct rendering often compromises image quality. In this paper, we propose a novel learning strategy, namely 3D-to-2D imitation, which enables a 3D-aware GAN to generate high-quality images while maintaining their strict 3D consistency, by letting the images synthesized by the generator's 3D rendering branch to mimic those generated by its 2D super-resolution branch. We also introduce 3D-aware convolutions into the generator for better 3D representation learning, which further improves the image generation quality. With the above strategies, our method reaches FID scores of 5.4 and 4.3 on FFHQ and AFHQ-v2 Cats, respectively, at 512x512 resolution, largely outperforming existing 3D-aware GANs using direct 3D rendering and coming very close to the previous state-of-the-art method that leverages 2D super-resolution. Project website: https://seanchenxy.github.io/Mimic3DWeb.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143792314",
                        "name": "Xingyu Chen"
                    },
                    {
                        "authorId": "152710186",
                        "name": "Yu Deng"
                    },
                    {
                        "authorId": "2450889",
                        "name": "Baoyuan Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "To this end, we use the pre-trained model of Unsup3D [45] since it has been widely applied in unsupervised face reconstruction [50, 30, 35, 49] in recent years."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "676005e85a0e3956bf0778a01ee41b887f301fad",
                "externalIds": {
                    "ArXiv": "2303.07709",
                    "DBLP": "journals/corr/abs-2303-07709",
                    "DOI": "10.48550/arXiv.2303.07709",
                    "CorpusId": 257505090
                },
                "corpusId": 257505090,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/676005e85a0e3956bf0778a01ee41b887f301fad",
                "title": "3D Face Arbitrary Style Transfer",
                "abstract": "Style transfer of 3D faces has gained more and more attention. However, previous methods mainly use images of artistic faces for style transfer while ignoring arbitrary style images such as abstract paintings. To solve this problem, we propose a novel method, namely Face-guided Dual Style Transfer (FDST). To begin with, FDST employs a 3D decoupling module to separate facial geometry and texture. Then we propose a style fusion strategy for facial geometry. Subsequently, we design an optimization-based DDSG mechanism for textures that can guide the style transfer by two style images. Besides the normal style image input, DDSG can utilize the original face input as another style input as the face prior. By this means, high-quality face arbitrary style transfer results can be obtained. Furthermore, FDST can be applied in many downstream tasks, including region-controllable style transfer, high-fidelity face texture reconstruction, large-pose face reconstruction, and artistic face reconstruction. Comprehensive quantitative and qualitative results show that our method can achieve comparable performance. All source codes and pre-trained weights will be released to the public.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2087027693",
                        "name": "Xiangwen Deng"
                    },
                    {
                        "authorId": "2113958213",
                        "name": "Ying Zou"
                    },
                    {
                        "authorId": null,
                        "name": "Yuanhao Cai"
                    },
                    {
                        "authorId": "2152588546",
                        "name": "Chendong Zhao"
                    },
                    {
                        "authorId": "1614039034",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "2203016405",
                        "name": "Zhifang Liu"
                    },
                    {
                        "authorId": "2211723905",
                        "name": "Yuxiao Liu"
                    },
                    {
                        "authorId": "2112249764",
                        "name": "Jia-wei Zhou"
                    },
                    {
                        "authorId": "2143410777",
                        "name": "Haoqian Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We address this concern by analyzing both the FID score and the depth accuracy metric used in [5, 32]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2a7525357f1bb41300179550c47a7d6424b2be48",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-06833",
                    "ArXiv": "2302.06833",
                    "DOI": "10.48550/arXiv.2302.06833",
                    "CorpusId": 256846382
                },
                "corpusId": 256846382,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2a7525357f1bb41300179550c47a7d6424b2be48",
                "title": "VQ3D: Learning a 3D-Aware Generative Model on ImageNet",
                "abstract": "Recent work has shown the possibility of training generative models of 3D content from 2D image collections on small datasets corresponding to a single object class, such as human faces, animal faces, or cars. However, these models struggle on larger, more complex datasets. To model diverse and unconstrained image collections such as ImageNet, we present VQ3D, which introduces a NeRF-based decoder into a two-stage vector-quantized autoencoder. Our Stage 1 allows for the reconstruction of an input image and the ability to change the camera position around the image, and our Stage 2 allows for the generation of new 3D scenes. VQ3D is capable of generating and reconstructing 3D-aware images from the 1000-class ImageNet dataset of 1.2 million training images. We achieve an ImageNet generation FID score of 16.8, compared to 69.8 for the next best baseline method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2121366367",
                        "name": "Kyle Sargent"
                    },
                    {
                        "authorId": "23978705",
                        "name": "Jing Yu Koh"
                    },
                    {
                        "authorId": "2146204239",
                        "name": "Han Zhang"
                    },
                    {
                        "authorId": "2914394",
                        "name": "Huiwen Chang"
                    },
                    {
                        "authorId": "144382278",
                        "name": "Charles Herrmann"
                    },
                    {
                        "authorId": "2179732",
                        "name": "Pratul P. Srinivasan"
                    },
                    {
                        "authorId": "3045089",
                        "name": "Jiajun Wu"
                    },
                    {
                        "authorId": "3232265",
                        "name": "Deqing Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Previously, some approaches attempt to extract 3D structure from pre-trained 2D-GANs [42, 52]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "12f99b597fd65c9eb730cfef498b47f3fb3a5ec8",
                "externalIds": {
                    "DBLP": "conf/cvpr/AbdalL0CSWT23",
                    "ArXiv": "2301.02700",
                    "DOI": "10.1109/CVPR52729.2023.00442",
                    "CorpusId": 255546292
                },
                "corpusId": 255546292,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/12f99b597fd65c9eb730cfef498b47f3fb3a5ec8",
                "title": "3DAvatarGAN: Bridging Domains for Personalized Editable Avatars",
                "abstract": "Modern 3D-GANs synthesize geometry and texture by training on large-scale datasets with a consistent structure. Training such models on stylized, artistic data, with often unknown, highly variable geometry, and camera information has not yet been shown possible. Can we train a 3D GAN on such artistic data, while maintaining multi-view consistency and texture quality? To this end, we propose an adaptation framework, where the source domain is a pre-trained 3D-GAN, while the target domain is a 2D-GAN trained on artistic datasets. We, then, distill the knowledge from a 2D generator to the source 3D generator. To do that, we first propose an optimization-based method to align the distributions of camera parameters across domains. Second, we propose regularizations necessary to learn high-quality texture, while avoiding degenerate geometric solutions, such as flat shapes. Third, we show a deformation-based technique for modeling exaggerated geometry of artistic domains, enabling-as a byproduct- personalized geometric editing. Finally, we propose a novel inversion method for 3D-GANs linking the latent spaces of the source and the target domains. Our contributions-for the first time-allow for the generation, editing, and animation of personalized artistic 3D avatars on artistic datasets. Project Page: https:/rameenabdal.github.io/3DAvatarGAN",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "94395014",
                        "name": "Rameen Abdal"
                    },
                    {
                        "authorId": "49923155",
                        "name": "Hsin-Ying Lee"
                    },
                    {
                        "authorId": "100516590",
                        "name": "Peihao Zhu"
                    },
                    {
                        "authorId": "1752091",
                        "name": "Menglei Chai"
                    },
                    {
                        "authorId": "10753214",
                        "name": "Aliaksandr Siarohin"
                    },
                    {
                        "authorId": "1798011",
                        "name": "Peter Wonka"
                    },
                    {
                        "authorId": "145582202",
                        "name": "S. Tulyakov"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "\u2026implicit neural representations (Mildenhall et al., 2020; Sitzmann et al., 2020) or differentiable neural rendering (Kato et al., 2018) into GANs, recent works (Schwarz et al., 2020; Niemeyer & Geiger, 2021; Chan et al., 2021; Shi et al., 2021) can produce multi-view consistent images.",
                ", 2018), Multi-view Identity Consistency (ID) (Shi et al., 2021), Chamfer Distance (CD), and Multi-view Image Warping Errors (WE) (Zhang et al.",
                ", 2018) into GANs, recent works (Schwarz et al., 2020; Niemeyer & Geiger, 2021; Chan et al., 2021; Shi et al., 2021) can produce multi-view consistent images.",
                "We evaluate PV3D and baseline models by Frechet Video Distance (FVD) (Unterthiner et al., 2018), Multi-view Identity Consistency (ID) (Shi et al., 2021), Chamfer Distance (CD), and Multi-view Image Warping Errors (WE) (Zhang et al., 2022a;b)."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5552e4acb67c845075836749691c3ef29bdecb89",
                "externalIds": {
                    "DBLP": "conf/iclr/XuZLZBFS23",
                    "ArXiv": "2212.06384",
                    "DOI": "10.48550/arXiv.2212.06384",
                    "CorpusId": 254591805
                },
                "corpusId": 254591805,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5552e4acb67c845075836749691c3ef29bdecb89",
                "title": "PV3D: A 3D Generative Model for Portrait Video Generation",
                "abstract": "Recent advances in generative adversarial networks (GANs) have demonstrated the capabilities of generating stunning photo-realistic portrait images. While some prior works have applied such image GANs to unconditional 2D portrait video generation and static 3D portrait synthesis, there are few works successfully extending GANs for generating 3D-aware portrait videos. In this work, we propose PV3D, the first generative framework that can synthesize multi-view consistent portrait videos. Specifically, our method extends the recent static 3D-aware image GAN to the video domain by generalizing the 3D implicit neural representation to model the spatio-temporal space. To introduce motion dynamics to the generation process, we develop a motion generator by stacking multiple motion layers to generate motion features via modulated convolution. To alleviate motion ambiguities caused by camera/human motions, we propose a simple yet effective camera condition strategy for PV3D, enabling both temporal and multi-view consistent video generation. Moreover, PV3D introduces two discriminators for regularizing the spatial and temporal domains to ensure the plausibility of the generated portrait videos. These elaborated designs enable PV3D to generate 3D-aware motion-plausible portrait videos with high-quality appearance and geometry, significantly outperforming prior works. As a result, PV3D is able to support many downstream applications such as animating static portraits and view-consistent video motion editing. Code and models are released at https://showlab.github.io/pv3d.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2065723181",
                        "name": "Eric Xu"
                    },
                    {
                        "authorId": "2107968449",
                        "name": "Jianfeng Zhang"
                    },
                    {
                        "authorId": "123200208",
                        "name": "J. Liew"
                    },
                    {
                        "authorId": "2108126147",
                        "name": "Wenqing Zhang"
                    },
                    {
                        "authorId": "2052830285",
                        "name": "Song Bai"
                    },
                    {
                        "authorId": "1698982",
                        "name": "Jiashi Feng"
                    },
                    {
                        "authorId": "2047358650",
                        "name": "Mike Zheng Shou"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[69] Yichun Shi, Divyansh Aggarwal, and Anil K Jain."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8f80620a8bf12fc6001248ca0cc3d2449db10a8f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-16374",
                    "ArXiv": "2211.16374",
                    "DOI": "10.1109/CVPR52729.2023.01365",
                    "CorpusId": 254069418
                },
                "corpusId": 254069418,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8f80620a8bf12fc6001248ca0cc3d2449db10a8f",
                "title": "DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model",
                "abstract": "Recent 3D generative models have achieved remarkable performance in synthesizing high resolution photorealistic images with view consistency and detailed 3D shapes, but training them for diverse domains is challenging since it requires massive training images and their camera distribution information. Text-guided domain adaptation methods have shown impressive performance on converting the 2D generative model on one domain into the models on other domains with different styles by leveraging the CLIP (Contrastive Language-Image Pre-training), rather than collecting massive datasets for those domains. However, one drawback of them is that the sample diversity in the original generative model is not well-preserved in the domain-adapted generative models due to the deterministic nature of the CLIP text encoder. Text-guided domain adaptation will be even more challenging for 3D generative models not only because of catastrophic diversity loss, but also because of inferior text-image correspondence and poor image quality. Here we propose DATID-3D, a domain adaptation method tailored for 3D generative models using text-to-image diffusion models that can synthesize diverse images per text prompt without collecting additional images and camera information for the target domain. Unlike 3D extensions of prior text-guided domain adaptation methods, our novel pipeline was able to fine-tune the state-of-the-art 3D generator of the source domain to synthesize high resolution, multi-view consistent images in text-guided targeted domains without additional data, outperforming the existing text-guided domain adaptation methods in diversity and text-image correspondence. Furthermore, we propose and demonstrate diverse 3D image manipulations such as one-shot instance-selected adaptation and single-view manipulated 3D reconstruction to fully enjoy diversity in text.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109334279",
                        "name": "Gwanghyun Kim"
                    },
                    {
                        "authorId": "34971370",
                        "name": "S. Chun"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Earlier works [38, 52, 58] utilize voxel or mesh as the intermediate representation."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "408248c1614254cd8a6967e7faaabf18754fead6",
                "externalIds": {
                    "ArXiv": "2211.13901",
                    "DBLP": "journals/corr/abs-2211-13901",
                    "DOI": "10.1109/CVPR52729.2023.00430",
                    "CorpusId": 254017882
                },
                "corpusId": 254017882,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/408248c1614254cd8a6967e7faaabf18754fead6",
                "title": "Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent Portrait Synthesis from Monocular Image",
                "abstract": "A key challenge for novel view synthesis of monocular portrait images is 3D consistency under continuous pose variations. Most existing methods rely on 2D generative models which often leads to obvious 3D inconsistency artifacts. We present a 3D-consistent novel view synthesis approach for monocular portrait images based on a recent proposed 3D-aware GAN, namely Generative Radiance Manifolds (GRAM) [13], which has shown strong 3D consistency at multiview image generation of virtual subjects via the radiance manifolds representation. However, simply learning an encoder to map a real image into the latent space of GRAM can only reconstruct coarse radiance manifolds without faithful fine details, while improving the reconstruction fidelity via instance-specific optimization is time-consuming. We introduce a novel detail manifolds reconstructor to learn 3D-consistent fine details on the radiance manifolds from monocular images, and combine them with the coarse radiance manifolds for high-fidelity reconstruction. The 3D priors derived from the coarse radiance manifolds are used to regulate the learned details to ensure reasonable synthesized results at novel views. Trained on in-the-wild 2D images, our method achieves high-fidelity and 3D-consistent portrait synthesis largely outperforming the prior art. Project page: https://yudeng.github.io/GRAMInverter/",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152710186",
                        "name": "Yu Deng"
                    },
                    {
                        "authorId": "2450889",
                        "name": "Baoyuan Wang"
                    },
                    {
                        "authorId": "93596028",
                        "name": "H. Shum"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Compared to other methods, our approach is fast compared to existing frameworks for training explicitly 3D aware GANs [17, 31, 50] and compared to [44] our method is lightweight and able to perform rotations and edits to face shape without the need for a 3D morphable model.",
                "Several works have investigated incorporating explicit 3D understanding into GANs [17, 31, 50]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f5e76bbd00e910c9d5698efd01bd63ea6f9a85c7",
                "externalIds": {
                    "DBLP": "conf/cvpr/HaasGB23",
                    "ArXiv": "2211.07195",
                    "DOI": "10.1109/CVPRW59228.2023.00075",
                    "CorpusId": 253511090
                },
                "corpusId": 253511090,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f5e76bbd00e910c9d5698efd01bd63ea6f9a85c7",
                "title": "Controllable GAN Synthesis Using Non-Rigid Structure-from-Motion",
                "abstract": "In this paper, we present an approach for combining non-rigid structure-from-motion (NRSfM) with deep generative models, and propose an efficient framework for discovering trajectories in the latent space of 2D GANs corresponding to changes in 3D geometry. Our approach uses recent advances in NRSfM and enables editing of the camera and non-rigid shape information associated with the latent codes without needing to retrain the generator. This formulation provides an implicit dense 3D reconstruction as it enables the image synthesis of novel shapes from arbitrary view angles and non-rigid structure. The method is built upon a sparse backbone, where a neural regressor is first trained to regress parameters describing the cameras and sparse non-rigid structure directly from the latent codes. The latent trajectories associated with changes in the camera and structure parameters are then identified by estimating the local inverse of the regressor in the neighborhood of a given latent code. The experiments show that our approach provides a versatile, systematic way to model, analyze, and edit the geometry and non-rigid structures of faces.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2140280426",
                        "name": "Ren\u00e9 Haas"
                    },
                    {
                        "authorId": "23620370",
                        "name": "Stella Grasshof"
                    },
                    {
                        "authorId": "120414984",
                        "name": "Sami S. Brandt"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "This category of studies facilitate the learning of 3D consistency by utilizing one or more kinds of 3D prior knowledge as constraints, such as shape [16, 130, 153], albedo [2, 99], normal [2], and depth [79, 99, 102].",
                "NGP [16] CGF 2021 Chair, Car user controls \u2713 \u2713 3D shape, relectance map LiftedGAN [99] CVPR 2021 Face Uncon.",
                "More recently, a few methods [99, 102] are build on top of StyleGAN architectures, either using a pretrained StyleGAN or adapting the vanilla design to their setting.",
                "LiftedGAN [99] (Nov 2020) equips a pre-trained StyleGAN2 generator with ive additional 3D-aware networks, which disentangle the latent space of StyleGAN2 into texture, shape, viewpoint, and lighting."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "bbe393d68defa7cb03b41d6a751726374020bb12",
                "externalIds": {
                    "ArXiv": "2210.14267",
                    "DOI": "10.1145/3626193",
                    "CorpusId": 253116603
                },
                "corpusId": 253116603,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bbe393d68defa7cb03b41d6a751726374020bb12",
                "title": "A Survey on Deep Generative 3D-aware Image Synthesis",
                "abstract": "Recent years have seen remarkable progress in deep learning powered visual content creation. This includes deep generative 3D-aware image synthesis, which produces high-fidelity images in a 3D-consistent manner while simultaneously capturing compact surfaces of objects from pure image collections without the need for any 3D supervision, thus bridging the gap between 2D imagery and 3D reality. The field of computer vision has been recently captivated by the task of deep generative 3D-aware image synthesis, with hundreds of papers appearing in top-tier journals and conferences over the past few years (mainly the past two years), but there lacks a comprehensive survey of this remarkable and swift progress. Our survey aims to introduce new researchers to this topic, provide a useful reference for related works, and stimulate future research directions through our discussion section. Apart from the presented papers, we aim to constantly update the latest relevant papers along with corresponding implementations at https://weihaox.github.io/3D-aware-Gen.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50875615",
                        "name": "Weihao Xia"
                    },
                    {
                        "authorId": "2067730921",
                        "name": "Jing Xue"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0f44890c6e4c6b6dac1396359143bac37657e811",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-12050",
                    "ArXiv": "2209.12050",
                    "DOI": "10.48550/arXiv.2209.12050",
                    "CorpusId": 252531206
                },
                "corpusId": 252531206,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0f44890c6e4c6b6dac1396359143bac37657e811",
                "title": "Controllable Face Manipulation and UV Map Generation by Self-supervised Learning",
                "abstract": "Although manipulating facial attributes by Generative Adversarial Networks (GANs) has been remarkably successful recently, there are still some challenges in explicit control of features such as pose, expression, lighting, etc. Recent methods achieve explicit control over 2D images by combining 2D generative model and 3DMM. However, due to the lack of realism and clarity in texture reconstruction by 3DMM, there is a domain gap between the synthetic image and the rendered image of 3DMM. Since rendered 3DMM images contain facial region only without the background, directly computing the loss between these two domains is not ideal and the resultant trained model will be biased. In this study, we propose to explicitly edit the latent space of the pretrained StyleGAN by controlling the parameters of the 3DMM. To address the domain gap problem, we propose a noval network called 'Map and edit' and a simple but effective attribute editing method to avoid direct loss computation between rendered and synthesized images. Furthermore, since our model can accurately generate multi-view face images while the identity remains unchanged. As a by-product, combined with visibility masks, our proposed model can also generate texture-rich and high-resolution UV facial textures. Our model relies on pretrained StyleGAN, and the proposed model is trained in a self-supervised manner without any manual annotations or datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111164052",
                        "name": "Yuanming Li"
                    },
                    {
                        "authorId": "2016481042",
                        "name": "Jeong-gi Kwak"
                    },
                    {
                        "authorId": "2757702",
                        "name": "D. Han"
                    },
                    {
                        "authorId": "144878703",
                        "name": "Hanseok Ko"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In order to generate high-quality multi-view consistent images, neural scene representation using differentiable rendering [61], [85], [86], [87], [88], [89], [90], [91] that can be optimized on a training set of only 2D multi-view images has gained popularity in the past few years."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4d0b528bc9f7e7e373dbba62bea8d9df43cffcca",
                "externalIds": {
                    "ArXiv": "2209.05434",
                    "CorpusId": 252917593
                },
                "corpusId": 252917593,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4d0b528bc9f7e7e373dbba62bea8d9df43cffcca",
                "title": "3DFaceShop: Explicitly Controllable 3D-Aware Portrait Generation",
                "abstract": "In contrast to the traditional avatar creation pipeline which is a costly process, contemporary generative approaches directly learn the data distribution from photographs. While plenty of works extend unconditional generative models and achieve some levels of controllability, it is still challenging to ensure multi-view consistency, especially in large poses. In this work, we propose a network that generates 3D-aware portraits while being controllable according to semantic parameters regarding pose, identity, expression and illumination. Our network uses neural scene representation to model 3D-aware portraits, whose generation is guided by a parametric face model that supports explicit control. While the latent disentanglement can be further enhanced by contrasting images with partially different attributes, there still exists noticeable inconsistency in non-face areas, e.g., hair and background, when animating expressions. Wesolve this by proposing a volume blending strategy in which we form a composite output by blending dynamic and static areas, with two parts segmented from the jointly learned semantic field. Our method outperforms prior arts in extensive experiments, producing realistic portraits with vivid expression in natural lighting when viewed from free viewpoints. It also demonstrates generalization ability to real images as well as out-of-domain data, showing great promise in real applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152949334",
                        "name": "Junshu Tang"
                    },
                    {
                        "authorId": "145803569",
                        "name": "Bo Zhang"
                    },
                    {
                        "authorId": "2157857267",
                        "name": "Binxin Yang"
                    },
                    {
                        "authorId": "2146320438",
                        "name": "Ting Zhang"
                    },
                    {
                        "authorId": "47514557",
                        "name": "Dong Chen"
                    },
                    {
                        "authorId": "2149343311",
                        "name": "Lizhuang Ma"
                    },
                    {
                        "authorId": "1716835",
                        "name": "Fang Wen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Other more descriptive and flexible 3D representations include depth maps [40, 48, 41], regular meshes [43, 38], and volumetric grids [19], although many of these approaches still rely on 3DMM in their intermediate steps.",
                "Two rare examples of completely modelfree methods that also reconstruct hair [41, 43] are selfsupervised GANs [21] that learn from unlabeled collections of images."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "19679ce6c7130c3de726897579d83655c9fe7abb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-04436",
                    "ArXiv": "2209.04436",
                    "DOI": "10.1109/ACCESS.2023.3309412",
                    "CorpusId": 252185485
                },
                "corpusId": 252185485,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/19679ce6c7130c3de726897579d83655c9fe7abb",
                "title": "Multi-NeuS: 3D Head Portraits From Single Image With Neural Implicit Functions",
                "abstract": "We present an approach for the reconstruction of textured 3D meshes of human heads from one or few views. Since such few-shot reconstruction is underconstrained, it requires prior knowledge which is hard to impose on traditional 3D reconstruction algorithms. In this work, we rely on the recently introduced 3D representation\u2013 neural implicit functions\u2013 which, being based on neural networks, allows to naturally learn priors about human heads from data, and is directly convertible to textured mesh. Namely, we extend NeuS, a state-of-the-art neural implicit function formulation, to represent multiple objects of a class (human heads in our case) simultaneously. The underlying neural net architecture is designed to learn the commonalities among these objects and to generalize to unseen ones. Our model is trained on just a hundred smartphone videos and does not require any scanned 3D data. Afterwards, the model can fit novel heads in the few-shot or one-shot modes with good results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "52225338",
                        "name": "Egor Burkov"
                    },
                    {
                        "authorId": "2075465659",
                        "name": "Ruslan Rakhimov"
                    },
                    {
                        "authorId": "1562067853",
                        "name": "Aleksandr Safin"
                    },
                    {
                        "authorId": "51139941",
                        "name": "Evgeny Burnaev"
                    },
                    {
                        "authorId": "1740145",
                        "name": "V. Lempitsky"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Besides, we provide the FID, aMP, and FRS scores of LiftedGAN by using their randomly generated samples instead of the attribute-edited samples.",
                "However, LiftedGAN cannot control individual attributes.",
                "Finally, Shi et al. [Shi et al. 2021] presents a LiftedGAN model, which lifts the pretrained StyleGAN2 in 3D.",
                "They either directly base on an image-to-image translation models [Choi et al. 2018; Liu et al. 2019], or utilize the disentangling abilities [Abdal et al. 2021; Shi et al. 2021; Tewari et al. 2020] of StyleGAN [Karras et al. ar X\niv :2\n20 8.",
                "We notice that LiftedGAN and DiscoFaceGAN cannot directly perform editing tasks, such as \u201cGender\u201d and \u201cAge\u201d.",
                "Finally, we also adopt a 3D-aware LiftedGAN [Shi et al. 2021] to compare multiple-view generation.",
                "[Shi et al. 2021] presents a LiftedGAN model, which lifts the pretrained StyleGAN2 in 3D.",
                "Specifically, for the \u201cSmile\u201d attribute, our TT-GNeRF (S) achieve 1899.6 aMP and 0.812 FRS scores, which are better than 1484.0 aMP and 0.464 FRS scores of LiftedGAN, 1347.4 aMP, and 0.587 FRS scores of DiscoFaceGAN.",
                "Many prior works [Abdal et al. 2021; Choi et al. 2018; Shi et al. 2021; Tewari et al. 2020] focus on realistic face editing.",
                "On the other hand, LiftedGAN has improved 3D consistency but has the limited quality and cannot perform facial attribute editing.",
                "Recently, some works [Deng et al. 2020; Geng et al. 2019; Lin et al. 2022; Shi et al. 2021; Tewari et al. 2020] demonstrate high-quality control over GAN generation via a 3DMM [Paysan et al. 2009].",
                "2019], or utilize the disentangling abilities [Abdal et al. 2021; Shi et al. 2021; Tewari et al. 2020] of StyleGAN [Karras et al.",
                "Recently, some works [Deng et al. 2020; Geng et al. 2019; Lin et al. 2022; Shi et al. 2021; Tewari et al. 2020] demonstrate high-quality control over GAN generation via a 3DMM [Paysan et al."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c21709f3f6671c6b417e7d72778d998a29a575f9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-12550",
                    "ArXiv": "2208.12550",
                    "DOI": "10.48550/arXiv.2208.12550",
                    "CorpusId": 251881341
                },
                "corpusId": 251881341,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c21709f3f6671c6b417e7d72778d998a29a575f9",
                "title": "Training and Tuning Generative Neural Radiance Fields for Attribute-Conditional 3D-Aware Face Generation",
                "abstract": "(GNeRF) have achieved impressive high-quality image generation, while preserving strong 3D consistency. The most notable achievements are made in the face gen- eration domain. However, most of these models focus on improving view consistency but neglect a disentanglement aspect, thus these models cannot provide high-quality semantic/attribute control over generation. To this end, we introduce a conditional GNeRF model uses specific (DAEM), generation. Moreover, we propose a TRIOT (TRaining as Init, and Optimizing for Tuning) method to optimize the latent vector to improve the precision further. Extensive experiments on the widely used show that our model yields high-quality editing with better view consis- tency while preserving the non-target regions. The code is available at",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50560752",
                        "name": "Jichao Zhang"
                    },
                    {
                        "authorId": "10753214",
                        "name": "Aliaksandr Siarohin"
                    },
                    {
                        "authorId": "1646872838",
                        "name": "Yahui Liu"
                    },
                    {
                        "authorId": "2109238637",
                        "name": "Hao Tang"
                    },
                    {
                        "authorId": "1429806753",
                        "name": "N. Sebe"
                    },
                    {
                        "authorId": "40397893",
                        "name": "Wei Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "Moreover, photo-realistic synthesis remains challenging [26, 42].",
                "We also follow the similar strategy in [42] to measure identity preservation, where we use all frontal images from the held-out FFHQ set and perform pose editing at different angles to compute the identity cosine similarity between the edited faces and the original ones.",
                "We compare with prior 3D-controllable GANs [10, 45, 46, 42, 26, 28, 6], and show more results in Supplementary.",
                "More recently, several works introduced 3D priors into GANs [46, 10, 14, 42] for controllable synthesis.",
                "In line with our work, several prior methods [28, 10, 26, 46, 45, 14, 42, 6] introduce 3D priors into GANs to achieve 3D controllability over face attributes of expression, pose, and illumination."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c35a3bacc1b4bb842cd1be853e1e456e5bd1feb3",
                "externalIds": {
                    "DBLP": "conf/eccv/LiuSL00K22",
                    "ArXiv": "2208.11257",
                    "DOI": "10.48550/arXiv.2208.11257",
                    "CorpusId": 251765502
                },
                "corpusId": 251765502,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/c35a3bacc1b4bb842cd1be853e1e456e5bd1feb3",
                "title": "3D-FM GAN: Towards 3D-Controllable Face Manipulation",
                "abstract": "3D-controllable portrait synthesis has significantly advanced, thanks to breakthroughs in generative adversarial networks (GANs). However, it is still challenging to manipulate existing face images with precise 3D control. While concatenating GAN inversion and a 3D-aware, noise-to-image GAN is a straight-forward solution, it is inefficient and may lead to noticeable drop in editing quality. To fill this gap, we propose 3D-FM GAN, a novel conditional GAN framework designed specifically for 3D-controllable face manipulation, and does not require any tuning after the end-to-end learning phase. By carefully encoding both the input face image and a physically-based rendering of 3D edits into a StyleGAN's latent spaces, our image generator provides high-quality, identity-preserved, 3D-controllable face manipulation. To effectively learn such novel framework, we develop two essential training strategies and a novel multiplicative co-modulation architecture that improves significantly upon naive schemes. With extensive evaluations, we show that our method outperforms the prior arts on various tasks, with better editability, stronger identity preservation, and higher photo-realism. In addition, we demonstrate a better generalizability of our design on large pose editing and out-of-domain images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Yuchen Liu"
                    },
                    {
                        "authorId": "2496409",
                        "name": "Zhixin Shu"
                    },
                    {
                        "authorId": "152998391",
                        "name": "Yijun Li"
                    },
                    {
                        "authorId": "2112754968",
                        "name": "Zhe Lin"
                    },
                    {
                        "authorId": "2844849",
                        "name": "Richard Zhang"
                    },
                    {
                        "authorId": "153574814",
                        "name": "S. Kung"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "[33] propose to train a 3D generator to disentangle the latent codes into 3D components, which are used as the input for the renderer.",
                "results of Unsup3d [34], LiftedGAN [33] and our proposed method.",
                "We adopt the pretrained face embedding model used in [33] for Lid and Llow, which is an ResNet-18 [47].",
                "We observe our method performs better than Unsup3d [34] and LiftedGAN [33].",
                "We then adopt a pre-trained face recognition network [33] f(\u00b7) to regularize the face identities, which can be denoted as",
                "In Table I, we show the quantitative results of Unsup3d [34], LiftedGAN [33] and our proposed method.",
                "Comparisons of qualitative results between ours and two related works: (a) Unsup3d [34] and (b) LiftedGAN [33].",
                "However, they [33] try to infer the depth information without",
                "LiftedGAN [33] performs well at generating the front cartoon faces only, while at other viewpoints, it renders low-quality images with large distortions, since LiftedGAN simply infers the depth from latent codes and trains the model without any constraint on the depth.",
                "Since the 3D shape reconstruction requires images of consistent multiple views and lighting, recent works [6], [7], [20], [33], [34] attempt to uncover extra cues to guide the learning process.",
                "[20], [33], [35] aim to manipulate the latent codes of StyleGAN [8], [9] to generate synthetic data for 3D shape learning.",
                "Both LiftedGAN [33] and our method set the resolution as 256.",
                "In Figure 8, we show the rotated results of related works [33], [34] using the unsupervised method to reconstruct the 3D shapes."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "161c7f7c41e73513ecb1a99bfc00a560a1b3c625",
                "externalIds": {
                    "ArXiv": "2207.14425",
                    "DBLP": "journals/corr/abs-2207-14425",
                    "DOI": "10.48550/arXiv.2207.14425",
                    "CorpusId": 251196845
                },
                "corpusId": 251196845,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/161c7f7c41e73513ecb1a99bfc00a560a1b3c625",
                "title": "3D Cartoon Face Generation with Controllable Expressions from a Single GAN Image",
                "abstract": "In this paper, we investigate an open research task of generating 3D cartoon face shapes from single 2D GAN generated human faces and without 3D supervision, where we can also manipulate the facial expressions of the 3D shapes. To this end, we discover the semantic meanings of StyleGAN latent space, such that we are able to produce face images of various expressions, poses, and lighting by controlling the latent codes. Specifically, we first finetune the pretrained StyleGAN face model on the cartoon datasets. By feeding the same latent codes to face and cartoon generation models, we aim to realize the translation from 2D human face images to cartoon styled avatars. We then discover semantic directions of the GAN latent space, in an attempt to change the facial expressions while preserving the original identity. As we do not have any 3D annotations for cartoon faces, we manipulate the latent codes to generate images with different poses and lighting, such that we can reconstruct the 3D cartoon face shapes. We validate the efficacy of our method on three cartoon datasets qualitatively and quantitatively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2359832",
                        "name": "Hongya Wang"
                    },
                    {
                        "authorId": "2604251",
                        "name": "Guosheng Lin"
                    },
                    {
                        "authorId": "1741126",
                        "name": "S. Hoi"
                    },
                    {
                        "authorId": "2158509654",
                        "name": "Chun Miao"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "result"
            ],
            "contexts": [
                "Unit GIRAFFE [54] pi-GAN [10] LiftedGAN [61] EG3D\u2020 [9] GRAM\u2020 [16] StyleNeRF\u2020 [23] GMPI Train - Time\u2193 \u2013 56h \u2013 8.",
                "LiftedGAN [61] serves as this ablation.",
                "Following [9, 61], we also study the 3D geometry\u2019s pose accuracy.",
                "Row FID\u2193 KID\u00d7100 \u2193 ID\u2191 Depth\u2193 Pose\u2193\n2 5 6 2\n1 LiftedGAN [58] 29.8 \u2013 0.58 0.40 0.023 2-1 D2A (\u03f5 = 1/64) 13.4 0.920 0.69 0.60 0.004 2-2 D2A (\u03f5 = 1/128) 13.5 0.867 0.70 0.60 0.004 2-3 D2A (\u03f5 = 1/256) 11.7 0.644 0.70 0.63 0.005 2-4 D2A (\u03f5 = 1/512) 12.6 0.684 0.69 0.62 0.005 3 GMPI 11.4 0.738 0.70 0.53 0.004\nSimilarly, we approximate depth Dvtgt via\nDvtgt = L\u2211 i=1 bi \u00b7 \u03b1\u2032i \u00b7 i\u22121\u220f j=1 (1\u2212 \u03b1\u2032j)  , (S4) where bi is the distance mentioned in Eq.",
                "At a resolution of 2562, 1) GMPI outperforms GIRAFFE, pi-GAN, LiftedGAN, and GRAM on FID/KID while outperforming StyleSDF on FID; 2) GMPI demonstrates better identity similarity (ID) than GIRAFFE, pi-GAN, and LiftedGAN; 3) GMPI outperforms GIRAFFE regarding depth; 4) GMPI performs best among all baselines on pose accuracy.",
                "Similar to [9, 61], we also assess geometry and depth accuracy.",
                "Different from our proposed approach, because of the transformation map, LiftedGAN is not strictly view-consistent.",
                "LiftedGAN reconstructs the geometry of an image by distilling intermediate representations from a fixed StyleGANv2 to a separate 3D generator which produces a depth map and a transformation map in addition to an image.",
                "Most related to our work are GRAM [16] and LiftedGAN [61]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a951da634637181031ba6b231e44522e4bfb13ba",
                "externalIds": {
                    "ArXiv": "2207.10642",
                    "DBLP": "conf/eccv/ZhaoMGRSC22",
                    "DOI": "10.48550/arXiv.2207.10642",
                    "CorpusId": 250918197
                },
                "corpusId": 250918197,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/a951da634637181031ba6b231e44522e4bfb13ba",
                "title": "Generative Multiplane Images: Making a 2D GAN 3D-Aware",
                "abstract": "What is really needed to make an existing 2D GAN 3D-aware? To answer this question, we modify a classical GAN, i.e., StyleGANv2, as little as possible. We find that only two modifications are absolutely necessary: 1) a multiplane image style generator branch which produces a set of alpha maps conditioned on their depth; 2) a pose-conditioned discriminator. We refer to the generated output as a 'generative multiplane image' (GMPI) and emphasize that its renderings are not only high-quality but also guaranteed to be view-consistent, which makes GMPIs different from many prior works. Importantly, the number of alpha maps can be dynamically adjusted and can differ between training and inference, alleviating memory concerns and enabling fast training of GMPIs in less than half a day at a resolution of $1024^2$. Our findings are consistent across three challenging and common high-resolution datasets, including FFHQ, AFHQv2, and MetFaces.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144306665",
                        "name": "Xiaoming Zhao"
                    },
                    {
                        "authorId": "3211133",
                        "name": "Fangchang Ma"
                    },
                    {
                        "authorId": "9522576",
                        "name": "David Guera"
                    },
                    {
                        "authorId": "2521387",
                        "name": "Zhile Ren"
                    },
                    {
                        "authorId": "2068227",
                        "name": "A. Schwing"
                    },
                    {
                        "authorId": "143792576",
                        "name": "Alex Colburn"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "3 Portrait image generation with 3D control To evaluate the performance of the proposed 3D-controllable StyleGAN, we report the qualitative and quantitative comparison with state-of-the-art models [34,9,72,57] whose generator allows explicit control over pose.",
                "Recently, a few methods [57,47] have incorporated a pre-trained StyleGAN with a differentiable renderer, but they struggle with photorealism, high-resolution [47] and real image editing [57].",
                "In addition, a few unsupervised approaches have been proposed by adopting implicit 3D feature [42,43] or differentiable renderer [57,47] in generation."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1a47926f35211ba5aeeaff0e5d373d1f1e44f6d0",
                "externalIds": {
                    "DBLP": "conf/eccv/KwakLYKHK22",
                    "ArXiv": "2207.10257",
                    "DOI": "10.48550/arXiv.2207.10257",
                    "CorpusId": 250921216
                },
                "corpusId": 250921216,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/1a47926f35211ba5aeeaff0e5d373d1f1e44f6d0",
                "title": "Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for Editable Portrait Image Synthesis",
                "abstract": "Over the years, 2D GANs have achieved great successes in photorealistic portrait generation. However, they lack 3D understanding in the generation process, thus they suffer from multi-view inconsistency problem. To alleviate the issue, many 3D-aware GANs have been proposed and shown notable results, but 3D GANs struggle with editing semantic attributes. The controllability and interpretability of 3D GANs have not been much explored. In this work, we propose two solutions to overcome these weaknesses of 2D GANs and 3D-aware GANs. We first introduce a novel 3D-aware GAN, SURF-GAN, which is capable of discovering semantic attributes during training and controlling them in an unsupervised manner. After that, we inject the prior of SURF-GAN into StyleGAN to obtain a high-fidelity 3D-controllable generator. Unlike existing latent-based methods allowing implicit pose control, the proposed 3D-controllable StyleGAN enables explicit pose control over portrait generation. This distillation allows direct compatibility between 3D control and many StyleGAN-based techniques (e.g., inversion and stylization), and also brings an advantage in terms of computational resources. Our codes are available at https://github.com/jgkwak95/SURF-GAN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2016481042",
                        "name": "Jeong-gi Kwak"
                    },
                    {
                        "authorId": "2111164052",
                        "name": "Yuanming Li"
                    },
                    {
                        "authorId": "2146861723",
                        "name": "Dongsik Yoon"
                    },
                    {
                        "authorId": "2145183568",
                        "name": "Donghyeon Kim"
                    },
                    {
                        "authorId": "2757702",
                        "name": "D. Han"
                    },
                    {
                        "authorId": "144878703",
                        "name": "Hanseok Ko"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Recently, LiftedGAN [52] lifts a pre-trained StyleGAN and distill it into a 3D aware generator, producing depth maps as a by-product.",
                "Early attempts [44,52,63] are made to mine 3D geometric cues from the pretrained 2D GAN models in an unsupervised manner."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "762d9b045fd2e6fe521720de57d8fc441fff7746",
                "externalIds": {
                    "ArXiv": "2207.10183",
                    "DBLP": "journals/corr/abs-2207-10183",
                    "DOI": "10.48550/arXiv.2207.10183",
                    "CorpusId": 250920692
                },
                "corpusId": 250920692,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/762d9b045fd2e6fe521720de57d8fc441fff7746",
                "title": "2D GANs Meet Unsupervised Single-view 3D Reconstruction",
                "abstract": "Recent research has shown that controllable image generation based on pre-trained GANs can benefit a wide range of computer vision tasks. However, less attention has been devoted to 3D vision tasks. In light of this, we propose a novel image-conditioned neural implicit field, which can leverage 2D supervisions from GAN-generated multi-view images and perform the single-view reconstruction of generic objects. Firstly, a novel offline StyleGAN-based generator is presented to generate plausible pseudo images with full control over the viewpoint. Then, we propose to utilize a neural implicit function, along with a differentiable renderer to learn 3D geometry from pseudo images with object masks and rough pose initializations. To further detect the unreliable supervisions, we introduce a novel uncertainty module to predict uncertainty maps, which remedy the negative effect of uncertain regions in pseudo images, leading to a better reconstruction performance. The effectiveness of our approach is demonstrated through superior single-view 3D reconstruction results of generic objects.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152943473",
                        "name": "Feng Liu"
                    },
                    {
                        "authorId": "2111119747",
                        "name": "Xiaoming Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "GAN2Shape [3] and LiftedGAN [24] use neural networks and a differentiable renderer [25] for reasoning the mapping process by semantics-embedding-semantics selfmapping.",
                "Differently, the semantics-embedding networks of GAN2Shape use the rendered images as a intermediary, while LiftedGAN does not and achieves high-fidelity rotation results in a large angle range.",
                "In contrast, our ASRMM focuses on a light way to improve the reconstruction accuracy, without relying on heavy prior models for view changing or relighting, but our ASRMM is also inspired by [3, 22, 24] that style-transferred images can improve the diversity of the input, and we transfer image style by making the material monotonous."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bd7497c11a8c2e41bdcf910e8145ede3bdd7f07c",
                "externalIds": {
                    "DBLP": "journals/apin/FangX23",
                    "DOI": "10.1007/s10489-022-03724-9",
                    "CorpusId": 250545945
                },
                "corpusId": 250545945,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bd7497c11a8c2e41bdcf910e8145ede3bdd7f07c",
                "title": "Self-supervised reflectance-guided 3d shape reconstruction from single-view images",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2176301850",
                        "name": "Binbin Fang"
                    },
                    {
                        "authorId": "1713125",
                        "name": "N. Xiao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Modern GANs are a lot of engineering and it often takes a lot of futile experiments to get to a point where the obtained performance is acceptable.",
                "We believe that the future of 3D GANs is a combination of efficient volumetric representations, regularized 2D upsamplers, and patch-wise training.",
                "In contrast to classical NeRF [38], we do not utilize view direction conditioning since it worsens multi-view consistency [7] in GANs which are trained on RGB datasets with a single view per instance.",
                "Note that this high training efficiency is achieved without the use of an upsampler, which initially enabled high-resolution synthesis of 3D-aware GANs.",
                "Also, in contrast to upsampler-based 3D GANs, our generator can naturally incorporate the techniques from the traditional NeRF literature.",
                "NeRF-based GANs.",
                "Compared to upsampler-based 3D GANs [15, 43, 72, 79, 6, 78], we use a pure NeRF [38] as our generator G and utilize the tri-plane representation [6, 8] as the backbone.",
                "Recently, there appeared works which train from single-view RGB only, including mesh-generation methods [19, 73, 53] and methods that extract 3D structure from pretrained 2D GANs [58, 48].",
                "Apart from that, we also compare to pi-GAN [7] and GRAM [12], which are non-upsampler-based GANs.",
                "Finally, 3D GANs generating faces and humans may have negative societal impact as discussed in Appx G.",
                "Patch-wise training of NeRF-based GANs was originally proposed by GRAF [56] and got largely neglected by the community since then.",
                "But for NeRF-based GANs, it becomes prohibitively expensive for high resolutions since convolutional discriminators operate on dense full-size images.",
                "Training NeRF-based GANs is computationally expensive, because rendering each pixel via volumetric rendering requires many evaluations (e.g., in our case, 96) of the underlying MLP.",
                "People address these scaling issues of NeRF-based GANs in different ways, but the dominating approach is to train a separate 2D decoder to produce a high-resolution image from a low-resolution image or feature grid rendered from a NeRF backbone [43]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "89a856845a3640f414eea896e088c9bf92466c75",
                "externalIds": {
                    "DBLP": "conf/nips/SkorokhodovT0W22",
                    "ArXiv": "2206.10535",
                    "DOI": "10.48550/arXiv.2206.10535",
                    "CorpusId": 249889658
                },
                "corpusId": 249889658,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/89a856845a3640f414eea896e088c9bf92466c75",
                "title": "EpiGRAF: Rethinking training of 3D GANs",
                "abstract": "A very recent trend in generative modeling is building 3D-aware generators from 2D image collections. To induce the 3D bias, such models typically rely on volumetric rendering, which is expensive to employ at high resolutions. During the past months, there appeared more than 10 works that address this scaling issue by training a separate 2D decoder to upsample a low-resolution image (or a feature tensor) produced from a pure 3D generator. But this solution comes at a cost: not only does it break multi-view consistency (i.e. shape and texture change when the camera moves), but it also learns the geometry in a low fidelity. In this work, we show that it is possible to obtain a high-resolution 3D generator with SotA image quality by following a completely different route of simply training the model patch-wise. We revisit and improve this optimization scheme in two ways. First, we design a location- and scale-aware discriminator to work on patches of different proportions and spatial positions. Second, we modify the patch sampling strategy based on an annealed beta distribution to stabilize training and accelerate the convergence. The resulted model, named EpiGRAF, is an efficient, high-resolution, pure 3D generator, and we test it on four datasets (two introduced in this work) at $256^2$ and $512^2$ resolutions. It obtains state-of-the-art image quality, high-fidelity geometry and trains ${\\approx} 2.5 \\times$ faster than the upsampler-based counterparts. Project website: https://universome.github.io/epigraf.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51118864",
                        "name": "Ivan Skorokhodov"
                    },
                    {
                        "authorId": "145582202",
                        "name": "S. Tulyakov"
                    },
                    {
                        "authorId": "2155345360",
                        "name": "Yiqun Wang"
                    },
                    {
                        "authorId": "1798011",
                        "name": "Peter Wonka"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "With the advances in differentiable rendering and neural 3D representations, a recent line of work has explored photo-realistic 3D face generation using only 2D image collections as training data [1], [21], [22], [23], [24], [25], [26], [27], [28], where they generate not only 2D images, but the implicit 3D geometry as well."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "55fe382924fa31ca8405554ed5f3073f8277e300",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-13251",
                    "ArXiv": "2206.08361",
                    "DOI": "10.48550/arXiv.2206.08361",
                    "CorpusId": 249712197
                },
                "corpusId": 249712197,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/55fe382924fa31ca8405554ed5f3073f8277e300",
                "title": "Controllable 3D Face Synthesis with Conditional Generative Occupancy Fields",
                "abstract": "Capitalizing on the recent advances in image generation models, existing controllable face image synthesis methods are able to generate high-fidelity images with some levels of controllability, e.g., controlling the shapes, expressions, textures, and poses of the generated face images. However, previous methods focus on controllable 2D image generative models, which are prone to producing inconsistent face images under large expression and pose changes. In this paper, we propose a new NeRF-based conditional 3D face synthesis framework, which enables 3D controllability over the generated face images by imposing explicit 3D conditions from 3D face priors. At its core is a conditional Generative Occupancy Field (cGOF++) that effectively enforces the shape of the generated face to conform to a given 3D Morphable Model (3DMM) mesh, built on top of EG3D [1], a recent tri-plane-based generative model. To achieve accurate control over fine-grained 3D face shapes of the synthesized images, we additionally incorporate a 3D landmark loss as well as a volume warping loss into our synthesis framework. Experiments validate the effectiveness of the proposed method, which is able to generate high-fidelity face images and shows more precise 3D controllability than state-of-the-art 2D-based controllable face synthesis methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "11837959",
                        "name": "Keqiang Sun"
                    },
                    {
                        "authorId": "2146648",
                        "name": "Shangzhe Wu"
                    },
                    {
                        "authorId": "1830448350",
                        "name": "Zhaoyang Huang"
                    },
                    {
                        "authorId": null,
                        "name": "Ning Zhang"
                    },
                    {
                        "authorId": "2145348695",
                        "name": "Quan Wang"
                    },
                    {
                        "authorId": "47893312",
                        "name": "Hongsheng Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Generative 3D-aware image synthesis 3D-aware generative models [42, 30, 53, 51, 8, 44] aim to learn multiview image synthesis of an object category given uncontrolled 2D images collections."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b3a36a0fac0b324e68e7c4b7b829de4b73fe303a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-07255",
                    "ArXiv": "2206.07255",
                    "DOI": "10.48550/arXiv.2206.07255",
                    "CorpusId": 249674797
                },
                "corpusId": 249674797,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b3a36a0fac0b324e68e7c4b7b829de4b73fe303a",
                "title": "GRAM-HD: 3D-Consistent Image Generation at High Resolution with Generative Radiance Manifolds",
                "abstract": "Recent works have shown that 3D-aware GANs trained on unstructured single image collections can generate multiview images of novel instances. The key underpinnings to achieve this are a 3D radiance field generator and a volume rendering process. However, existing methods either cannot generate high-resolution images (e.g., up to 256X256) due to the high computation cost of neural volume rendering, or rely on 2D CNNs for image-space upsampling which jeopardizes the 3D consistency across different views. This paper proposes a novel 3D-aware GAN that can generate high resolution images (up to 1024X1024) while keeping strict 3D consistency as in volume rendering. Our motivation is to achieve super-resolution directly in the 3D space to preserve 3D consistency. We avoid the otherwise prohibitively-expensive computation cost by applying 2D convolutions on a set of 2D radiance manifolds defined in the recent generative radiance manifold (GRAM) approach, and apply dedicated loss functions for effective GAN training at high resolution. Experiments on FFHQ and AFHQv2 datasets show that our method can produce high-quality 3D-consistent results that significantly outperform existing methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2147263432",
                        "name": "Jianfeng Xiang"
                    },
                    {
                        "authorId": "2109732576",
                        "name": "Jiaolong Yang"
                    },
                    {
                        "authorId": "2111233530",
                        "name": "Yu Deng"
                    },
                    {
                        "authorId": "2241956709",
                        "name": "Xin Tong"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "While the disentangled style-space of StyleGANs [20\u201322] allows for control over the viewpoint of the generated images to some extent [13, 26, 42, 51], gaining precise 3D-consistent control is still non-trivial due to its lack of physical interpretation and operation in 2D."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "05f31f8d11629bec66e99b72862da48bbcb03ff7",
                "externalIds": {
                    "DBLP": "conf/nips/SchwarzSNL022",
                    "ArXiv": "2206.07695",
                    "DOI": "10.48550/arXiv.2206.07695",
                    "CorpusId": 249674609
                },
                "corpusId": 249674609,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/05f31f8d11629bec66e99b72862da48bbcb03ff7",
                "title": "VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids",
                "abstract": "State-of-the-art 3D-aware generative models rely on coordinate-based MLPs to parameterize 3D radiance fields. While demonstrating impressive results, querying an MLP for every sample along each ray leads to slow rendering. Therefore, existing approaches often render low-resolution feature maps and process them with an upsampling network to obtain the final image. Albeit efficient, neural rendering often entangles viewpoint and content such that changing the camera pose results in unwanted changes of geometry or appearance. Motivated by recent results in voxel-based novel view synthesis, we investigate the utility of sparse voxel grid representations for fast and 3D-consistent generative modeling in this paper. Our results demonstrate that monolithic MLPs can indeed be replaced by 3D convolutions when combining sparse voxel grids with progressive growing, free space pruning and appropriate regularization. To obtain a compact representation of the scene and allow for scaling to higher voxel resolutions, our model disentangles the foreground object (modeled in 3D) from the background (modeled in 2D). In contrast to existing approaches, our method requires only a single forward pass to generate a full 3D scene. It hence allows for efficient rendering from arbitrary viewpoints while yielding 3D consistent results with high visual fidelity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40502376",
                        "name": "Katja Schwarz"
                    },
                    {
                        "authorId": "40562186",
                        "name": "Axel Sauer"
                    },
                    {
                        "authorId": "145048708",
                        "name": "Michael Niemeyer"
                    },
                    {
                        "authorId": "2699340",
                        "name": "Yiyi Liao"
                    },
                    {
                        "authorId": "47237027",
                        "name": "Andreas Geiger"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "LiftedGAN [51] transforms the framework to a generative model but also needs optimization to address real-world images."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1ba27595f9272490cb7bcea360978658b38bf7d2",
                "externalIds": {
                    "DBLP": "conf/cvpr/0005GTHWTHX22",
                    "DOI": "10.1109/CVPR52688.2022.00420",
                    "CorpusId": 250551746
                },
                "corpusId": 250551746,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1ba27595f9272490cb7bcea360978658b38bf7d2",
                "title": "Learning to Restore 3D Face from In-the-Wild Degraded Images",
                "abstract": "In-the-wild 3D face modelling is a challenging problem as the predicted facial geometry and texture suffer from a lack of reliable clues or priors, when the input images are degraded. To address such a problem, in this paper we propose a novel Learning to Restore (L2R) 3D face framework for unsupervised high-quality face reconstruction from low-resolution images. Rather than directly refining 2D image appearance, L2R learns to recover fine-grained 3D details on the proxy against degradation via extracting generative facial priors. Concretely, L2R proposes a novel albedo restoration network to model high-quality 3D facial texture, in which the diverse guidance from the pre-trained Generative Adversarial Networks (GANs) is leveraged to complement the lack of input facial clues. With the finer details of the restored 3D texture, L2R then learns displacement maps from scratch to enhance the significant facial structure and geometry. Both of the procedures are mutually optimized with a novel 3D-aware adversarial loss, which further improves the modelling performance and suppresses the potential uncertainty. Extensive experiments on benchmarks show that L2R outperforms state-of-the-art methods under the condition of low-quality inputs, and obtains superior performances than 2D pre-processed modelling approaches with limited 3D proxy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144399891",
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "authorId": "83103152",
                        "name": "Yanhao Ge"
                    },
                    {
                        "authorId": "144970872",
                        "name": "Ying Tai"
                    },
                    {
                        "authorId": "2124765851",
                        "name": "Xiao-Ying Huang"
                    },
                    {
                        "authorId": "1978245",
                        "name": "Chengjie Wang"
                    },
                    {
                        "authorId": "2112388851",
                        "name": "H. Tang"
                    },
                    {
                        "authorId": "3034852",
                        "name": "Dongjin Huang"
                    },
                    {
                        "authorId": "1724454",
                        "name": "Zhifeng Xie"
                    },
                    {
                        "authorId": "2176289497",
                        "name": "Tencent Youtu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Theoretically, larger modeling sizes are feasible, but we use a similar setting as [7, 51, 75] due to the time and memory cost.",
                "Gan2Shape [40] and LiftedGAN [51] try to distill knowledge from 2D GANs for 3D reconstruction."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9db416c089d917e7ab489a4fb5829928432c5d6f",
                "externalIds": {
                    "DBLP": "conf/cvpr/0005GTCCLTHWXH22",
                    "DOI": "10.1109/CVPR52688.2022.01971",
                    "CorpusId": 250647716
                },
                "corpusId": 250647716,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9db416c089d917e7ab489a4fb5829928432c5d6f",
                "title": "Physically-guided Disentangled Implicit Rendering for 3D Face Modeling",
                "abstract": "This paper presents a novel Physically-guided Disentangled Implicit Rendering (PhyDIR) framework for highfidelity 3D face modeling. The motivation comes from two observations: Widely-used graphics renderers yield excessive approximations against photo-realistic imaging, while neural rendering methods produce superior appearances but are highly entangled to perceive 3D-aware operations. Hence, we learn to disentangle the implicit rendering via explicit physical guidance, while guaranteeing the properties of: (1) 3D-aware comprehension and (2) high-reality image formation. For the former one, PhyDIR explicitly adopts 3D shading and rasterizing modules to control the renderer, which disentangles the light, facial shape, and viewpoint from neural reasoning. Specifically, PhyDIR proposes a novel multi-image shading strategy to compensate for the monocular limitation, so that the lighting variations are accessible to the neural renderer. For the latter, PhyDIR learns the face-collection implicit texture to avoid ill-posed intrinsic factorization, then leverages a series of consistency losses to constrain the rendering robustness. With the disentangled method, we make 3D face modeling benefit from both kinds of rendering strategies. Extensive experiments on benchmarks show that PhyDIR obtains superior performance than state-of-the-art explicit/implicit methods on geometry/texture modeling.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144470702",
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "authorId": "83103152",
                        "name": "Yanhao Ge"
                    },
                    {
                        "authorId": "144970872",
                        "name": "Ying Tai"
                    },
                    {
                        "authorId": "2075437879",
                        "name": "Weijian Cao"
                    },
                    {
                        "authorId": "1625895990",
                        "name": "Renwang Chen"
                    },
                    {
                        "authorId": "2118800472",
                        "name": "Kunlin Liu"
                    },
                    {
                        "authorId": "2178852343",
                        "name": "Hao Tang"
                    },
                    {
                        "authorId": "2124765851",
                        "name": "Xiao-Ying Huang"
                    },
                    {
                        "authorId": "1978245",
                        "name": "Chengjie Wang"
                    },
                    {
                        "authorId": "1724454",
                        "name": "Zhifeng Xie"
                    },
                    {
                        "authorId": "3034852",
                        "name": "Dongjin Huang"
                    },
                    {
                        "authorId": "2176289497",
                        "name": "Tencent Youtu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "For example, mesh-based 3D GANs are limited in viewing angle and detail [Liao et al. 2020; Shi et al. 2021; Szab\u00f3 et al. 2019]; voxel-based 3D GANs are limited in their resolution due to extensive memory requirements [Gadelha et al. 2017; Hao et al. 2021; Henzler et al. 2019; Nguyen-Phuoc et al.\u2026",
                "For example, mesh-based 3D GANs are limited in viewing angle and detail [Liao et al. 2020; Shi et al. 2021; Szab\u00f3 et al. 2019]; voxel-based 3D GANs are limited in their resolution due to extensive memory requirements [Gadelha et al."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "138d01f3f699f99ea477c4192b8734b1bbff642b",
                "externalIds": {
                    "ArXiv": "2203.13441",
                    "DBLP": "journals/corr/abs-2203-13441",
                    "DOI": "10.48550/arXiv.2203.13441",
                    "CorpusId": 247748597
                },
                "corpusId": 247748597,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/138d01f3f699f99ea477c4192b8734b1bbff642b",
                "title": "3D GAN Inversion for Controllable Portrait Image Animation",
                "abstract": "Millions of images of human faces are captured every single day; but these photographs portray the likeness of an individual with a fixed pose, expression, and appearance. Portrait image animation enables the post-capture adjustment of these attributes from a single image while maintaining a photorealistic reconstruction of the subject's likeness or identity. Still, current methods for portrait image animation are typically based on 2D warping operations or manipulations of a 2D generative adversarial network (GAN) and lack explicit mechanisms to enforce multi-view consistency. Thus these methods may significantly alter the identity of the subject, especially when the viewpoint relative to the camera is changed. In this work, we leverage newly developed 3D GANs, which allow explicit control over the pose of the image subject with multi-view consistency. We propose a supervision strategy to flexibly manipulate expressions with 3D morphable models, and we show that the proposed method also supports editing appearance attributes, such as age or hairstyle, by interpolating within the latent space of the GAN. The proposed technique for portrait image animation outperforms previous methods in terms of image quality, identity preservation, and pose transfer while also supporting attribute editing.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108720546",
                        "name": "Connor Z. Lin"
                    },
                    {
                        "authorId": "2202838",
                        "name": "David B. Lindell"
                    },
                    {
                        "authorId": "121028414",
                        "name": "Eric Chan"
                    },
                    {
                        "authorId": "1731170",
                        "name": "Gordon Wetzstein"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "007257c7a8caf7260f756324ba37e47f363ca3c0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-09301",
                    "ArXiv": "2203.09301",
                    "DOI": "10.1109/TPAMI.2023.3283551",
                    "CorpusId": 247519189,
                    "PubMed": "37352089"
                },
                "corpusId": 247519189,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/007257c7a8caf7260f756324ba37e47f363ca3c0",
                "title": "One-Shot Adaptation of GAN in Just One CLIP",
                "abstract": "There are many recent research efforts to fine-tune a pre-trained generator with a few target images to generate images of a novel domain. Unfortunately, these methods often suffer from overfitting or under-fitting when fine-tuned with a single target image. To address this, here we present a novel single-shot GAN adaptation method through unified CLIP space manipulations. Specifically, our model employs a two-step training strategy: reference image search in the source generator using a CLIP-guided latent optimization, followed by generator fine-tuning with a novel loss function that imposes CLIP space consistency between the source and adapted generators. To further improve the adapted model to produce spatially consistent samples with respect to the source generator, we also propose contrastive regularization for patchwise relationships in the CLIP space. Experimental results show that our model generates diverse outputs with the target texture and outperforms the baseline models both qualitatively and quantitatively. Furthermore, we show that our CLIP space manipulation strategy allows more effective attribute editing.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "116153377",
                        "name": "Gihyun Kwon"
                    },
                    {
                        "authorId": "30547794",
                        "name": "Jong-Chul Ye"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "[8, 48] I, 3DMM MI, LI, 3DMM [10] SI 3DV (w/o T) [49] I, KP 3DM (w/o T) Ellipsoid [16] I, KP, BG, SI 3DM Ellipsoid [37] I, SI, KP 3DM Ellipsoid [44] I MI, LI, A, D, N Symmetry"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d27eb6c93f83badb60376d6f76552fa258d63b9e",
                "externalIds": {
                    "ArXiv": "2203.06457",
                    "DBLP": "journals/corr/abs-2203-06457",
                    "DOI": "10.48550/arXiv.2203.06457",
                    "CorpusId": 247446706
                },
                "corpusId": 247446706,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d27eb6c93f83badb60376d6f76552fa258d63b9e",
                "title": "3D-GIF: 3D-Controllable Object Generation via Implicit Factorized Representations",
                "abstract": "While NeRF-based 3D-aware image generation methods enable viewpoint control, limitations still remain to be adopted to various 3D applications. Due to their view-dependent and light-entangled volume representation, the 3D geometry presents unrealistic quality and the color should be re-rendered for every desired viewpoint. To broaden the 3D applicability from 3D-aware image generation to 3D-controllable object generation, we propose the factorized representations which are view-independent and light-disentangled, and training schemes with randomly sampled light conditions. We demonstrate the superiority of our method by visualizing factorized representations, re-lighted images, and albedo-textured meshes. In addition, we show that our approach improves the quality of the generated geometry via visualization and quantitative comparison. To the best of our knowledge, this is the first work that extracts albedo-textured meshes with unposed 2D images without any additional labels or assumptions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109518360",
                        "name": "M. Lee"
                    },
                    {
                        "authorId": "2049404784",
                        "name": "Chaeyeon Chung"
                    },
                    {
                        "authorId": "2158831749",
                        "name": "Hojun Cho"
                    },
                    {
                        "authorId": null,
                        "name": "Minjung Kim"
                    },
                    {
                        "authorId": "2110428041",
                        "name": "Sanghun Jung"
                    },
                    {
                        "authorId": "1795455",
                        "name": "J. Choo"
                    },
                    {
                        "authorId": "46461051",
                        "name": "Minhyuk Sung"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "In this paper, we adopt the state-of-the-art pre-trained 3D generator of G3D [27] for 3D face modeling, which can disentangle the generation process of a 2D generator G2D instantiated by StyleGAN [17] into different 3D modules for a 3D shape representation.",
                "(2)\nBy optimizing the objective function (1), we can obtain the optimal w\u2217 and get the 3D face as {s, t} = G3D(w\u2217).",
                "Given a face recognition model f(x) : X \u2192 Rd, we optimize the parameter of w for the generator by minimizing the distance between the original face image and the rendered image of x\u2032 as\nmin w\nDf (x\u2032,x) + \u03bb\u2225x\u2032 \u2212 x\u22251, (1)\nwhere x\u2032 := R(G3D(w);V0, L0) with R being a differentiable renderer, and V0 and L0 are corresponding parameters of neutralized viewpoint and lighting; and \u03bb is a balancing hyperparameter.",
                "Specifically, we adopt a 3D generator [27] to synthesize 3D face information, including texture, shape, viewpoint, and lighting, using only a single-view face image.",
                "(1); 4: w \u2190 w \u2212 \u03b7\u2207wJ ; 5: end for 6: Forward pass the optimal w\u2217 into G3D to the 3D face {sa, ta}; 7: Initializing t\u22170 = x b; \u25b7 Stage II: Optimize t\u2217\n8: for k in MaxIterations N2 do 9: t\u2217k = t\na \u2299 (1\u2212M) + t\u2217k \u2299M; 10: Construct 3D adversarial face {sa, t\u2217k}; 11: Get importance probability P\u0302i,j from Eq.",
                "(5) as\nPi,j = 1\nZ eJf (R(s a,t\u2217;Vi,Lj),x b), (7)\nAlgorithm 1 Face3DAdv\nRequire: A pre-trained 3D generative model G3D, a FR model f , a real face image xa, a target face image xb, 2D transformation function T .",
                "On the other hand, some face representation methods leverage 3D position maps [27,13] to represent and output the mesh of the target, and achieve the controllable parametric nature of existing face models."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "f6a17c2dcb4e7b6df175eb3d044b3be93d65721d",
                "externalIds": {
                    "ArXiv": "2203.04623",
                    "DBLP": "journals/corr/abs-2203-04623",
                    "DOI": "10.48550/arXiv.2203.04623",
                    "CorpusId": 247319100
                },
                "corpusId": 247319100,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f6a17c2dcb4e7b6df175eb3d044b3be93d65721d",
                "title": "Controllable Evaluation and Generation of Physical Adversarial Patch on Face Recognition",
                "abstract": "Recent studies have revealed the vulnerability of face recognition models against physical adversarial patches, which raises security concerns about the deployed face recognition systems. However, it is still challenging to ensure the reproducibility for most attack algorithms under complex physical conditions, which leads to the lack of a systematic evaluation of the existing methods. It is therefore imperative to develop a framework that can enable a comprehensive evaluation of the vulnerability of face recognition in the physical world. To this end, we propose to simulate the complex transformations of faces in the physical world via 3D-face modeling, which serves as a digital counterpart of physical faces. The generic framework allows us to control different face variations and physical conditions to conduct reproducible evaluations comprehensively. With this digital simulator, we further propose a Face3DAdv method considering the 3D face transformations and realistic physical variations. Extensive experiments validate that Face3DAdv can significantly improve the effectiveness of diverse physically realizable adversarial patches in both simulated and physical environments, against various white-box and black-box face recognition models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48520620",
                        "name": "X. Yang"
                    },
                    {
                        "authorId": "3431029",
                        "name": "Yinpeng Dong"
                    },
                    {
                        "authorId": "19201674",
                        "name": "Tianyu Pang"
                    },
                    {
                        "authorId": "9381483",
                        "name": "Zihao Xiao"
                    },
                    {
                        "authorId": "2093561216",
                        "name": "Hang Su"
                    },
                    {
                        "authorId": "2146280930",
                        "name": "Junyi Zhu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[61] and [56] adopt a meshbased representation and generate images via rasterization.",
                "Another group of works [10, 12, 43, 55, 56, 61] seek to learn direct 3D representation of scenes and synthesize images under physical-based rendering process to achieve more strict 3D consistency."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c3ec9c6f4e82e90c0a3fea0802cc5b33f66e1b9b",
                "externalIds": {
                    "DBLP": "conf/cvpr/DengYX022",
                    "ArXiv": "2112.08867",
                    "DOI": "10.1109/CVPR52688.2022.01041",
                    "CorpusId": 245218753
                },
                "corpusId": 245218753,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c3ec9c6f4e82e90c0a3fea0802cc5b33f66e1b9b",
                "title": "GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation",
                "abstract": "3D-aware image generative modeling aims to generate 3D-consistent images with explicitly controllable camera poses. Recent works have shown promising results by training neural radiance field (NeRF) generators on unstructured 2D images, but still cannot generate highly-realistic images with fine details. A critical reason is that the high memory and computation cost of volumetric representation learning greatly restricts the number of point samples for radiance integration during training. Deficient sampling not only limits the expressive power of the generator to handle fine details but also impedes effective GAN training due to the noise caused by unstable Monte Carlo sampling. We propose a novel approach that regulates point sampling and radiance field learning on 2D manifolds, embodied as a set of learned implicit surfaces in the 3D volume. For each viewing ray, we calculate ray-surface intersections and accumulate their radiance generated by the network. By training and rendering such radiance mani folds, our generator can produce high quality images with realistic fine details and strong visual 3D consistency. 11Project page: https://yudeng.github.io/GRAM/",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2111233530",
                        "name": "Yu Deng"
                    },
                    {
                        "authorId": "2109732576",
                        "name": "Jiaolong Yang"
                    },
                    {
                        "authorId": "2147263432",
                        "name": "Jianfeng Xiang"
                    },
                    {
                        "authorId": "2241956709",
                        "name": "Xin Tong"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Related work has been successful at being view consistent [4, 58, 59] or modeling pose-appearance correlations [47, 49], but cannot achieve both simultaneously.",
                "Although it is not as fast as Lifting StyleGAN [59] and GIRAFFE [49], we believe major improvements in image quality, geometry quality, and viewconsistency outweigh the increased compute cost.",
                "We evaluate shape quality by calculating MSE against pseudo-ground-truth depth-maps (Depth) and poses (Pose) estimated from synthesized images by [10]; a similar evaluation was introduced by [59].",
                "5D GANs, which generate images and depth maps [59], our method works naturally for steep camera angles and in 360\u25e6 viewing conditions.",
                "We compare our methods against three stateof-the-art methods for 3D-aware image synthesis: \u03c0GAN [4], GIRAFFE [49], and Lifting StyleGAN [59].",
                "While GIRAFFE synthesizes high-quality images, reliance on view-inconsistent convolutions produces poor-quality shapes and identity shift\u2014note the hairline inconsistency between rendered views. \u03c0-GAN and Lifting StyleGAN generate adequate shapes and images but both struggle with photorealism and in capturing detailed shapes."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "3007b30891714b5b0cc0a41eb06f8a194d74993a",
                "externalIds": {
                    "DBLP": "conf/cvpr/ChanLCNPMGGTKKW22",
                    "ArXiv": "2112.07945",
                    "DOI": "10.1109/CVPR52688.2022.01565",
                    "CorpusId": 245144673
                },
                "corpusId": 245144673,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3007b30891714b5b0cc0a41eb06f8a194d74993a",
                "title": "Efficient Geometry-aware 3D Generative Adversarial Networks",
                "abstract": "Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. We introduce an expressive hybrid explicit implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "121028414",
                        "name": "Eric Chan"
                    },
                    {
                        "authorId": "2108720546",
                        "name": "Connor Z. Lin"
                    },
                    {
                        "authorId": "2147382797",
                        "name": "Matthew Chan"
                    },
                    {
                        "authorId": "1897417",
                        "name": "Koki Nagano"
                    },
                    {
                        "authorId": "52170427",
                        "name": "Boxiao Pan"
                    },
                    {
                        "authorId": "24817039",
                        "name": "Shalini De Mello"
                    },
                    {
                        "authorId": "39775678",
                        "name": "Orazio Gallo"
                    },
                    {
                        "authorId": "51352814",
                        "name": "L. Guibas"
                    },
                    {
                        "authorId": "31943350",
                        "name": "Jonathan Tremblay"
                    },
                    {
                        "authorId": "2121982",
                        "name": "S. Khamis"
                    },
                    {
                        "authorId": "2976930",
                        "name": "Tero Karras"
                    },
                    {
                        "authorId": "1731170",
                        "name": "Gordon Wetzstein"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[73] Yichun Shi, Divyansh Aggarwal, and Anil K Jain.",
                "Prior work has explored the use of GANs [27, 68] in vision tasks such as classification [10, 12, 55, 75, 85], segmentation [57, 80, 83, 91] and representation learning [7, 20, 21, 23, 36], as well as 3D vision and graphics tasks [28, 65, 73, 90].",
                "We do not rely on hand-crafted pixel space augmentations [12, 36], human-labeled data [28, 73, 80, 90, 91] or post-processing of GAN-generated datasets using domain knowledge [10, 57, 83, 90]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "03871045478e9a5062c336b16230e4a79d488052",
                "externalIds": {
                    "DBLP": "conf/cvpr/PeeblesZ00ES22",
                    "ArXiv": "2112.05143",
                    "DOI": "10.1109/CVPR52688.2022.01311",
                    "CorpusId": 245005830
                },
                "corpusId": 245005830,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/03871045478e9a5062c336b16230e4a79d488052",
                "title": "GAN-Supervised Dense Visual Alignment",
                "abstract": "We propose GAN-Supervised Learning, a framework for learning discriminative models and their GAN-generated training data jointly end-to-end. We apply our framework to the dense visual alignment problem. Inspired by the classic Congealing method, our GAN gealing algorithm trains a Spatial Transformer to map random samples from a GAN trained on unaligned data to a common, jointly-learned target mode. We show results on eight datasets, all of which demonstrate our method successfully aligns complex data and discovers dense correspondences. GANgealing significantly outperforms past self-supervised correspondence algorithms and performs on-par with (and sometimes exceeds) state-of-the-art supervised correspondence algorithms on several datasets-without making use of any correspondence supervision or data augmentation and despite being trained exclusively on GAN-generated data. For precise correspondence, we improve upon state-of-the-art supervised methods by as much as 3 \u00d7. We show applications of our method for augmented reality, image editing and automated pre-processing of image datasets for downstream GAN training.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35235273",
                        "name": "William S. Peebles"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "authorId": "2109976035",
                        "name": "Richard Zhang"
                    },
                    {
                        "authorId": "143805211",
                        "name": "A. Torralba"
                    },
                    {
                        "authorId": "1763086",
                        "name": "Alexei A. Efros"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Recent advances in controllable face image synthesis have yielded many impressive applications [2, 4, 7, 10, 15, 23, 41, 43, 44, 48]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2ec5aa9534cac0ec0e4673bad26dc118b0cc53ea",
                "externalIds": {
                    "ArXiv": "2110.01571",
                    "CorpusId": 244347896
                },
                "corpusId": 244347896,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2ec5aa9534cac0ec0e4673bad26dc118b0cc53ea",
                "title": "Causal Representation Learning for Context-Aware Face Transfer",
                "abstract": "Human face synthesis involves transferring knowledge about the identity and identity-dependent face shape (IDFS) of a human face to target face images where the context (e.g., facial expressions, head poses, and other background factors) may change dramatically. Human faces are non-rigid, so facial expression leads to deformation of face shape, and head pose also affects the face observed in 2D images. A key challenge in face transfer is to match the face with unobserved new contexts, adapting the face appearance to different poses and expressions accordingly. In this work, we find a way to provide prior knowledge for generative models to reason about the appropriate appearance of a human face in response to various expressions and poses. We propose a novel context-aware face transfer method, called CarTrans, that incorporates causal effects of contextual factors into face representation, and thus is able to be aware of the uncertainty of new contexts. We estimate the effect of facial expression and head pose in terms of counterfactual inference by designing a controlled intervention trial, thus avoiding the requirement of a large number of observations to cover the pose-expression space well. Moreover, we propose a kernel regression-based encoder that eliminates the identity specificity of target faces when encoding contextual information from target images. The resulting method shows impressive performance, allowing fine-grained control over face shape and appearance under various contextual conditions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "32855918",
                        "name": "Gege Gao"
                    },
                    {
                        "authorId": "32885778",
                        "name": "Huaibo Huang"
                    },
                    {
                        "authorId": "50876245",
                        "name": "Chaoyou Fu"
                    },
                    {
                        "authorId": "143712929",
                        "name": "R. He"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8bf5ede037959c3bec9857288753c945e6f55143",
                "externalIds": {
                    "ArXiv": "2109.09378",
                    "DBLP": "journals/corr/abs-2109-09378",
                    "DOI": "10.1145/3478513.3480538",
                    "CorpusId": 237510393
                },
                "corpusId": 237510393,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8bf5ede037959c3bec9857288753c945e6f55143",
                "title": "FreeStyleGAN",
                "abstract": "Current Generative Adversarial Networks (GANs) produce photorealistic renderings of portrait images. Embedding real images into the latent space of such models enables high-level image editing. While recent methods provide considerable semantic control over the (re-)generated images, they can only generate a limited set of viewpoints and cannot explicitly control the camera. Such 3D camera control is required for 3D virtual and mixed reality applications. In our solution, we use a few images of a face to perform 3D reconstruction, and we introduce the notion of the GAN camera manifold, the key element allowing us to precisely define the range of images that the GAN can reproduce in a stable manner. We train a small face-specific neural implicit representation network to map a captured face to this manifold and complement it with a warping scheme to obtain free-viewpoint novel-view synthesis. We show how our approach - due to its precise camera control - enables the integration of a pre-trained StyleGAN into standard 3D rendering pipelines, allowing e.g., stereo rendering or consistent insertion of faces in synthetic 3D environments. Our solution proposes the first truly free-viewpoint rendering of realistic faces at interactive rates, using only a small number of casual photos as input, while simultaneously allowing semantic editing capabilities, such as facial expression or lighting changes.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2422386",
                        "name": "Thomas Leimk\u00fchler"
                    },
                    {
                        "authorId": "1721779",
                        "name": "G. Drettakis"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We do not rely on hand-crafted pixel space augmentations [144, 155], human-labeled data [151,152,158,160,161] or post-processing of GAN-generated datasets using domain knowledge [146,149,150,160]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a8d6467e4126d0ada4e824b30a6098056b843242",
                "externalIds": {
                    "CorpusId": 259114449
                },
                "corpusId": 259114449,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a8d6467e4126d0ada4e824b30a6098056b843242",
                "title": "Generative Models of Images and Neural Networks",
                "abstract": "Generative Models of Images and Neural Networks",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219729481",
                        "name": "Bill Peebles"
                    },
                    {
                        "authorId": "35235273",
                        "name": "William S. Peebles"
                    },
                    {
                        "authorId": "1763086",
                        "name": "Alexei A. Efros"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "As a result, later methods moved away from fully convolutional GANs by incorporating 3D inductive biases in the architecture and training pipeline, such as 3D neural representations and differentiable rendering methods [34, 35, 47, 38].",
                "LiftGAN [47]: a method predating EG3D and SURF baselines, based on differentiable rendering for distilling 2D GANs to train a 3D generator.",
                "As a result, later works aimed at unsupervised methods by introducing 3D inductive biases in GANs, including 3D neural representations and differentiable rendering [34, 38, 47, 35] These methods, although promising, lag far behind 2D GANs in terms of image quality or struggle with high-resolution generation due to the additional computational complexity compared to the convolutional generators."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ad4ca90be642c80e48865adb1fba84718d250217",
                "externalIds": {
                    "CorpusId": 259138647
                },
                "corpusId": 259138647,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ad4ca90be642c80e48865adb1fba84718d250217",
                "title": "NeRF-GAN Distillation for Memory-Efficient 3D-Aware Generation with Convolutions",
                "abstract": "Pose-conditioned convolutional generative models struggle with high-quality 3D-consistent image generation from single-view datasets, due to their lack of sufficient 3D priors. Recently, the integration of Neural Radiance Fields (NeRFs) and generative models, such as Generative Adversarial Networks (GANs), has transformed 3D-aware generation from single-view images. NeRF-GANs exploit the strong inductive bias of 3D neural representations and volumetric rendering at the cost of higher computational complexity. This study aims at revisiting pose-conditioned 2D GANs for memory-efficient 3D-aware generation at inference time by distilling 3D knowledge from pretrained NeRF-GANS. We propose a simple and effective method, based on re-using the well-disentangled latent space of a pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly generate 3D-consistent images corresponding to the underlying 3D representations. Experiments on several datasets demonstrate that the proposed method obtains results comparable with volumetric rendering in terms of quality and 3D consistency while benefiting from the superior computational advantage of convolutional networks. The code will be available at:",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "73774192",
                        "name": "Mohamad Shahbazi"
                    },
                    {
                        "authorId": "2219926024",
                        "name": "Evangelos Ntavelis"
                    },
                    {
                        "authorId": "20406113",
                        "name": "A. Tonioni"
                    },
                    {
                        "authorId": "33942393",
                        "name": "Edo Collins"
                    },
                    {
                        "authorId": "35268081",
                        "name": "D. Paudel"
                    },
                    {
                        "authorId": "2129520569",
                        "name": "Martin Danelljan"
                    },
                    {
                        "authorId": "1681236",
                        "name": "L. Gool"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[23] proposed LiftedGAN, a framework that maps a latent code of StyleGAN2 to various maps as representations of shape and appearance.",
                "2 Qualitative results Figure 4 compares our method with other controllable and 3D-aware face synthesis methods based on StyleGAN2, including LiftedGAN [23], InterfaceGAN [22], StyleFlow [1], and EG3D [4].",
                "The FID scores of our method, LiftedGAN [23], and StyleGAN2 [13] are 11.",
                "LiftedGAN [23] utilizes the depth map as the shape representation and trains a couple of modules to render stably in various perspectives.",
                "Figure 4: Visual comparison of our generated faces with InterfaceGAN [21, 22], LiftedGAN [23], StyleFlow [1], EG3D [4] in different yaw angles."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "eabaf0003948824e3817b076bb75a3cc5eb0efa5",
                "externalIds": {
                    "DBLP": "conf/bmvc/ChungZSWC22",
                    "CorpusId": 253736279
                },
                "corpusId": 253736279,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/eabaf0003948824e3817b076bb75a3cc5eb0efa5",
                "title": "StyleFaceUV: a 3D Face UV Map Generator for View-Consistent Face Image Synthesis",
                "abstract": "Recent deep image generation models, such as StyleGAN2, face challenges to produce high-quality 2D face images with multi-view consistency. We address this issue by proposing an approach for generating detailed 3D faces using a pre-trained StyleGAN2 model. Our method estimates the 3D Morphable Model (3DMM) coefficients directly from the StyleGAN2\u2019s stylecode. To add more details to the produced 3D face models, we train a generator to produce two UV maps: a diffuse map to give the model a more faithful appearance and a generalized displacement map to add geometric details to the model. To achieve multi-view consistency, we also add a symmetric view image to recover information regarding the invisible side of a single image. The generated detailed 3D face models allow for consistent changes in viewing angles, expressions, and lighting conditions. Experimental results indicate that our method outperforms previous approaches both qualitatively and quantitatively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2191617799",
                        "name": "Wei-Chieh Chung"
                    },
                    {
                        "authorId": "5415498",
                        "name": "Yu-Ting Wu"
                    },
                    {
                        "authorId": "143708263",
                        "name": "Yung-Yu Chuang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In order to generate high-quality multi-view consistent images, neural scene representation using differentiable rendering [61], [85], [86], [87], [88], [89], [90], [91] that can be optimized on a training set of only 2D multi-view images has gained popularity in the past few years."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "25c6c02f2db2e90e5660a92fcd134cd33addc8ce",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-05434",
                    "DOI": "10.48550/arXiv.2209.05434",
                    "CorpusId": 252199614
                },
                "corpusId": 252199614,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/25c6c02f2db2e90e5660a92fcd134cd33addc8ce",
                "title": "Explicitly Controllable 3D-Aware Portrait Generation",
                "abstract": "\u2014In contrast to the traditional avatar creation pipeline which is a costly process, contemporary generative approaches directly learn the data distribution from photographs. While plenty of works extend unconditional generative models and achieve some levels of controllability, it is still challenging to ensure multi-view consistency, especially in large poses. In this work, we propose a network that generates 3D-aware portraits while being controllable according to semantic parameters regarding pose, identity, expression and illumination. Our network uses neural scene representation to model 3D-aware portraits, whose generation is guided by a parametric face model that supports explicit control. While the latent disentanglement can be further enhanced by contrasting images with partially different attributes, there still exists noticeable inconsistency in non-face areas, e.g. , hair and background, when animating expressions. We solve this by proposing a volume blending strategy in which we form a composite output by blending dynamic and static areas, with two parts segmented from the jointly learned semantic \ufb01eld. Our method outperforms prior arts in extensive experiments, producing realistic portraits with vivid expression in natural lighting when viewed from free viewpoints. It also demonstrates generalization ability to real images as well as out-of-domain data, showing great promise in real applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152949334",
                        "name": "Junshu Tang"
                    },
                    {
                        "authorId": "145803569",
                        "name": "Bo Zhang"
                    },
                    {
                        "authorId": "2157857267",
                        "name": "Binxin Yang"
                    },
                    {
                        "authorId": "2146320438",
                        "name": "Ting Zhang"
                    },
                    {
                        "authorId": "47514557",
                        "name": "Dong Chen"
                    },
                    {
                        "authorId": "2149343311",
                        "name": "Lizhuang Ma"
                    },
                    {
                        "authorId": "1716835",
                        "name": "Fang Wen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "3 Transcoder It has been observed in prior works that altering viewpoint by directly manipulating the style code is possible [28, 33].",
                "For instance, StyleGAN [13] representations have been shown to disentangle pose, shape and fine detail naturally, a property which has been used to help lift objects to 3D [12, 17, 28, 33, 39], these methods are 3D aware, but lack multi-view consistency."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c8706740814131eeee90b5c279baa6a40de7ca63",
                "externalIds": {
                    "DBLP": "conf/bmvc/CharlesARC22",
                    "CorpusId": 256902308
                },
                "corpusId": 256902308,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/c8706740814131eeee90b5c279baa6a40de7ca63",
                "title": "Style2NeRF: An Unsupervised One-Shot NeRF for Semantic 3D Reconstruction",
                "abstract": "We present Style2NeRF, an unsupervised model for one-shot recovery of 3D pose, shape and appearance of symmetric objects. Style2NeRF contains a transcoder which disentangles 2D representations from pretrained StyleGANs, then maps them to a semantically editable 3D NeRF generator. As such, the generative NeRF inherits Style-GAN\u2019s expressiveness and image editing properties, translating them to 3D. We make four key contributions: (i) We provide a novel model to accurately estimate an object\u2019s 3D pose, shape and appearance without any human supervision during training; (ii) We show how to map between semantically meaningful 2D and 3D representations using a novel disentangled generative NeRF; (iii) we introduce the pose and viewpoint ambiguity problem (suffered by existing 3D GAN methods) and propose a solution improving pose estimation accuracy; (iv) Finally, via transfer learning, we show our model can be trained on real car images where the pose distribution is unknown. Style2NeRF outperforms the state-of-the-art on the CARLA cars dataset as well as a fully supervised model for the task of car pose estimation on ShapeNet-cars and a new dataset of real car images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2055745260",
                        "name": "James Charles"
                    },
                    {
                        "authorId": "2461829",
                        "name": "Wim Abbeloos"
                    },
                    {
                        "authorId": "113231027",
                        "name": "Daniel Olmeda Reino"
                    },
                    {
                        "authorId": "1745672",
                        "name": "R. Cipolla"
                    }
                ]
            }
        }
    ]
}