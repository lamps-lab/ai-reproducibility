{
    "offset": 0,
    "data": [
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "7dbb4040b149a526003ed1b5c5e4141008f8d1eb",
                "externalIds": {
                    "ArXiv": "2310.01766",
                    "CorpusId": 263608346
                },
                "corpusId": 263608346,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7dbb4040b149a526003ed1b5c5e4141008f8d1eb",
                "title": "Exploring Counterfactual Alignment Loss towards Human-centered AI",
                "abstract": "Deep neural networks have demonstrated impressive accuracy in supervised learning tasks. However, their lack of transparency makes it hard for humans to trust their results, especially in safe-critic domains such as healthcare. To address this issue, recent explanation-guided learning approaches proposed to align the gradient-based attention map to image regions annotated by human experts, thereby obtaining an intrinsically human-centered model. However, the attention map these methods are based on may fail to causally attribute the model predictions, thus compromising their validity for alignment. To address this issue, we propose a novel human-centered framework based on counterfactual generation. In particular, we utilize the counterfactual generation's ability for causal attribution to introduce a novel loss called the CounterFactual Alignment (CF-Align) loss. This loss guarantees that the features attributed by the counterfactual generation for the classifier align with the human annotations. To optimize the proposed loss that entails a counterfactual generation with an implicit function form, we leverage the implicit function theorem for backpropagation. Our method is architecture-agnostic and, therefore can be applied to any neural network. We demonstrate the effectiveness of our method on a lung cancer diagnosis dataset, showcasing faithful alignment to humans.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2112345041",
                        "name": "Mingzhou Liu"
                    },
                    {
                        "authorId": "2253975325",
                        "name": "Xinwei Sun"
                    },
                    {
                        "authorId": "2255327893",
                        "name": "Ching-Wen Lee"
                    },
                    {
                        "authorId": "2253468866",
                        "name": "Yu Qiao"
                    },
                    {
                        "authorId": "2249533963",
                        "name": "Yizhou Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "To accurately measure the model\u2019s dependence on shortcut features and guide its reliance on them, we borrow and revise the feature attribution strategy based on counterfactual analysis [18, 46], which measures the importance of shortcut features by counterfactually changing them:"
            ],
            "citingPaper": {
                "paperId": "60ce57ab67d78d1dd8bf0b72b64b37da56fc8b5e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-08482",
                    "ArXiv": "2308.08482",
                    "DOI": "10.1145/3581783.3612317",
                    "CorpusId": 260925528
                },
                "corpusId": 260925528,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/60ce57ab67d78d1dd8bf0b72b64b37da56fc8b5e",
                "title": "Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features",
                "abstract": "Machine learning models often learn to make predictions that rely on sensitive social attributes like gender and race, which poses significant fairness risks, especially in societal applications, such as hiring, banking, and criminal justice. Existing work tackles this issue by minimizing the employed information about social attributes in models for debiasing. However, the high correlation between target task and these social attributes makes learning on the target task incompatible with debiasing. Given that model bias arises due to the learning of bias features (\\emph{i.e}., gender) that help target task optimization, we explore the following research question: \\emph{Can we leverage shortcut features to replace the role of bias feature in target task optimization for debiasing?} To this end, we propose \\emph{Shortcut Debiasing}, to first transfer the target task's learning of bias attributes from bias features to shortcut features, and then employ causal intervention to eliminate shortcut features during inference. The key idea of \\emph{Shortcut Debiasing} is to design controllable shortcut features to on one hand replace bias features in contributing to the target task during the training stage, and on the other hand be easily removed by intervention during the inference stage. This guarantees the learning of the target task does not hinder the elimination of bias features. We apply \\emph{Shortcut Debiasing} to several benchmark datasets, and achieve significant improvements over the state-of-the-art debiasing methods in both accuracy and fairness.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153912315",
                        "name": "Yi Zhang"
                    },
                    {
                        "authorId": "1798398",
                        "name": "J. Sang"
                    },
                    {
                        "authorId": "2110125710",
                        "name": "Junyan Wang"
                    },
                    {
                        "authorId": "2113406398",
                        "name": "D. Jiang"
                    },
                    {
                        "authorId": "2159616786",
                        "name": "Yaowei Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "0a429980acbd9b24b2b88740360e9da3ba7cb613",
                "externalIds": {
                    "DBLP": "journals/cogsr/LindsayB23",
                    "DOI": "10.1016/j.cogsys.2023.101156",
                    "CorpusId": 263421763
                },
                "corpusId": 263421763,
                "publicationVenue": {
                    "id": "6051439f-768c-4aae-a75c-3c82be5fa675",
                    "name": "Cognitive Systems Research",
                    "type": "journal",
                    "alternate_names": [
                        "Cogn Syst Res"
                    ],
                    "issn": "1389-0417",
                    "url": "http://www.elsevier.com/locate/cogsys",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/620288/description#description",
                        "http://www.sciencedirect.com/science/journal/13890417"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0a429980acbd9b24b2b88740360e9da3ba7cb613",
                "title": "Testing methods of neural systems understanding",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2250336880",
                        "name": "Grace W. Lindsay"
                    },
                    {
                        "authorId": "2250984340",
                        "name": "David Bau"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Second, most previous methods focus on images, where the resulting canonical inputs are individually interpretable and difficult to aggregate [29]."
            ],
            "citingPaper": {
                "paperId": "b0ea65fbcb35230ebb8dec360806e43135bbfa24",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-15466",
                    "ArXiv": "2307.15466",
                    "DOI": "10.2139/ssrn.4289597",
                    "CorpusId": 254668894
                },
                "corpusId": 254668894,
                "publicationVenue": {
                    "id": "75d7a8c1-d871-42db-a8e4-7cf5146fdb62",
                    "name": "Social Science Research Network",
                    "type": "journal",
                    "alternate_names": [
                        "SSRN, Social Science Research Network (SSRN) home page",
                        "SSRN Electronic Journal",
                        "Soc Sci Res Netw",
                        "SSRN",
                        "SSRN Home Page",
                        "SSRN Electron J",
                        "Social Science Electronic Publishing presents Social Science Research Network"
                    ],
                    "issn": "1556-5068",
                    "url": "http://www.ssrn.com/",
                    "alternate_urls": [
                        "www.ssrn.com/",
                        "https://fatcat.wiki/container/tol7woxlqjeg5bmzadeg6qrg3e",
                        "https://www.wikidata.org/wiki/Q53949192",
                        "www.ssrn.com/en",
                        "http://www.ssrn.com/en/",
                        "http://umlib.nl/ssrn",
                        "umlib.nl/ssrn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b0ea65fbcb35230ebb8dec360806e43135bbfa24",
                "title": "LUCID-GAN: Conditional Generative Models to Locate Unfairness",
                "abstract": "Most group fairness notions detect unethical biases by computing statistical parity metrics on a model's output. However, this approach suffers from several shortcomings, such as philosophical disagreement, mutual incompatibility, and lack of interpretability. These shortcomings have spurred the research on complementary bias detection methods that offer additional transparency into the sources of discrimination and are agnostic towards an a priori decision on the definition of fairness and choice of protected features. A recent proposal in this direction is LUCID (Locating Unfairness through Canonical Inverse Design), where canonical sets are generated by performing gradient descent on the input space, revealing a model's desired input given a preferred output. This information about the model's mechanisms, i.e., which feature values are essential to obtain specific outputs, allows exposing potential unethical biases in its internal logic. Here, we present LUCID-GAN, which generates canonical inputs via a conditional generative model instead of gradient-based inverse design. LUCID-GAN has several benefits, including that it applies to non-differentiable models, ensures that canonical sets consist of realistic inputs, and allows to assess proxy and intersectional discrimination. We empirically evaluate LUCID-GAN on the UCI Adult and COMPAS data sets and show that it allows for detecting unethical biases in black-box models without requiring access to the training data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "119626032",
                        "name": "Andres Algaba"
                    },
                    {
                        "authorId": "2121504247",
                        "name": "Carmen Mazijn"
                    },
                    {
                        "authorId": "50728331",
                        "name": "Carina E. A. Prunkl"
                    },
                    {
                        "authorId": "1785970",
                        "name": "J. Danckaert"
                    },
                    {
                        "authorId": "9109493",
                        "name": "V. Ginis"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4cac9da053b14ce5ad036d22bbff533ca97a7dcf",
                "externalIds": {
                    "ArXiv": "2307.13390",
                    "DBLP": "journals/corr/abs-2307-13390",
                    "DOI": "10.48550/arXiv.2307.13390",
                    "CorpusId": 260154908
                },
                "corpusId": 260154908,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4cac9da053b14ce5ad036d22bbff533ca97a7dcf",
                "title": "Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space",
                "abstract": "Counterfactual Explanations (CEs) are an important tool in Algorithmic Recourse for addressing two questions: 1. What are the crucial factors that led to an automated prediction/decision? 2. How can these factors be changed to achieve a more favorable outcome from a user's perspective? Thus, guiding the user's interaction with AI systems by proposing easy-to-understand explanations and easy-to-attain feasible changes is essential for the trustworthy adoption and long-term acceptance of AI systems. In the literature, various methods have been proposed to generate CEs, and different quality measures have been suggested to evaluate these methods. However, the generation of CEs is usually computationally expensive, and the resulting suggestions are unrealistic and thus non-actionable. In this paper, we introduce a new method to generate CEs for a pre-trained binary classifier by first shaping the latent space of an autoencoder to be a mixture of Gaussian distributions. CEs are then generated in latent space by linear interpolation between the query sample and the centroid of the target class. We show that our method maintains the characteristics of the input sample during the counterfactual search. In various experiments, we show that the proposed method is competitive based on different quality measures on image and tabular datasets -- efficiently returns results that are closer to the original data manifold compared to three state-of-the-art methods, which are essential for realistic high-dimensional machine learning applications.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2145742718",
                        "name": "Xuan Zhao"
                    },
                    {
                        "authorId": "2102011",
                        "name": "Klaus Broelemann"
                    },
                    {
                        "authorId": "1686448",
                        "name": "G. Kasneci"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "The work of (Lang et al. 2021) proposes a training procedure, which incorporates the classifier model for a StyleGAN to learn a classifier-specific StyleSpace to explain a classifier."
            ],
            "citingPaper": {
                "paperId": "5ea465cc8b8715c8aaa70c300e50ccee3c742f52",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-12219",
                    "ArXiv": "2307.12219",
                    "DOI": "10.48550/arXiv.2307.12219",
                    "CorpusId": 260125740
                },
                "corpusId": 260125740,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5ea465cc8b8715c8aaa70c300e50ccee3c742f52",
                "title": "Improving Out-of-Distribution Robustness of Classifiers via Generative Interpolation",
                "abstract": "Deep neural networks achieve superior performance for learning from independent and identically distributed (i.i.d.) data. However, their performance deteriorates significantly when handling out-of-distribution (OoD) data, where the training and test are drawn from different distributions. In this paper, we explore utilizing the generative models as a data augmentation source for improving out-of-distribution robustness of neural classifiers. Specifically, we develop a simple yet effective method called Generative Interpolation to fuse generative models trained from multiple domains for synthesizing diverse OoD samples. Training a generative model directly on the source domains tends to suffer from mode collapse and sometimes amplifies the data bias. Instead, we first train a StyleGAN model on one source domain and then fine-tune it on the other domains, resulting in many correlated generators where their model parameters have the same initialization thus are aligned. We then linearly interpolate the model parameters of the generators to spawn new sets of generators. Such interpolated generators are used as an extra data augmentation source to train the classifiers. The interpolation coefficients can flexibly control the augmentation direction and strength. In addition, a style-mixing mechanism is applied to further improve the diversity of the generated OoD samples. Our experiments show that the proposed method explicitly increases the diversity of training domains and achieves consistent improvements over baselines across datasets and multiple different distribution shifts.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1930797",
                        "name": "Haoyue Bai"
                    },
                    {
                        "authorId": "49984891",
                        "name": "Ceyuan Yang"
                    },
                    {
                        "authorId": "121983635",
                        "name": "Yinghao Xu"
                    },
                    {
                        "authorId": "2118784964",
                        "name": "Shueng-Han Gary Chan"
                    },
                    {
                        "authorId": "145291669",
                        "name": "Bolei Zhou"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "d5e0c4bf2018e6ba59918878fe7e2b376c688b8d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-13531",
                    "ArXiv": "2306.13531",
                    "DOI": "10.48550/arXiv.2306.13531",
                    "CorpusId": 259243922
                },
                "corpusId": 259243922,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d5e0c4bf2018e6ba59918878fe7e2b376c688b8d",
                "title": "WBCAtt: A White Blood Cell Dataset Annotated with Detailed Morphological Attributes",
                "abstract": "The examination of blood samples at a microscopic level plays a fundamental role in clinical diagnostics, influencing a wide range of medical conditions. For instance, an in-depth study of White Blood Cells (WBCs), a crucial component of our blood, is essential for diagnosing blood-related diseases such as leukemia and anemia. While multiple datasets containing WBC images have been proposed, they mostly focus on cell categorization, often lacking the necessary morphological details to explain such categorizations, despite the importance of explainable artificial intelligence (XAI) in medical domains. This paper seeks to address this limitation by introducing comprehensive annotations for WBC images. Through collaboration with pathologists, a thorough literature review, and manual inspection of microscopic images, we have identified 11 morphological attributes associated with the cell and its components (nucleus, cytoplasm, and granules). We then annotated ten thousand WBC images with these attributes. Moreover, we conduct experiments to predict these attributes from images, providing insights beyond basic WBC classification. As the first public dataset to offer such extensive annotations, we also illustrate specific applications that can benefit from our attribute annotations. Overall, our dataset paves the way for interpreting WBC recognition models, further advancing XAI in the fields of pathology and hematology.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3727644",
                        "name": "Satoshi Tsutsui"
                    },
                    {
                        "authorId": "33499770",
                        "name": "Winnie Pang"
                    },
                    {
                        "authorId": "1766554",
                        "name": "B. Wen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[21] trained a GAN to explain attributes that underlie classifier decisions.",
                "Moreover, as opposed to [21] and [31], our method does not require GAN training and can be applied to any off-the-shelf GAN and discriminative model."
            ],
            "citingPaper": {
                "paperId": "bd04e8b6d726bdfa51f4d10b9c6693075c99afee",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-09346",
                    "ArXiv": "2306.09346",
                    "DOI": "10.48550/arXiv.2306.09346",
                    "CorpusId": 259171774
                },
                "corpusId": 259171774,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bd04e8b6d726bdfa51f4d10b9c6693075c99afee",
                "title": "Rosetta Neurons: Mining the Common Units in a Model Zoo",
                "abstract": "Do different neural networks, trained for various vision tasks, share some common representations? In this paper, we demonstrate the existence of common features we call\"Rosetta Neurons\"across a range of models with different architectures, different tasks (generative and discriminative), and different types of supervision (class-supervised, text-supervised, self-supervised). We present an algorithm for mining a dictionary of Rosetta Neurons across several popular vision models: Class Supervised-ResNet50, DINO-ResNet50, DINO-ViT, MAE, CLIP-ResNet50, BigGAN, StyleGAN-2, StyleGAN-XL. Our findings suggest that certain visual concepts and structures are inherently embedded in the natural world and can be learned by different models regardless of the specific task or architecture, and without the use of semantic labels. We can visualize shared concepts directly due to generative models included in our analysis. The Rosetta Neurons facilitate model-to-model translation enabling various inversion-based manipulations, including cross-class alignments, shifting, zooming, and more, without the need for specialized training.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "116990729",
                        "name": "Amil Dravid"
                    },
                    {
                        "authorId": "52164591",
                        "name": "Yossi Gandelsman"
                    },
                    {
                        "authorId": "1763086",
                        "name": "Alexei A. Efros"
                    },
                    {
                        "authorId": "31114823",
                        "name": "Assaf Shocher"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "b80ba53d447429b2431decbcfed6b8f49192c1bd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-00985",
                    "ArXiv": "2306.00985",
                    "DOI": "10.48550/arXiv.2306.00985",
                    "CorpusId": 258999552
                },
                "corpusId": 258999552,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b80ba53d447429b2431decbcfed6b8f49192c1bd",
                "title": "Using generative AI to investigate medical imagery models and datasets",
                "abstract": "AI models have shown promise in many medical imaging tasks. However, our ability to explain what signals these models have learned is severely lacking. Explanations are needed in order to increase the trust in AI-based models, and could enable novel scientific discovery by uncovering signals in the data that are not yet known to experts. In this paper, we present a method for automatic visual explanations leveraging team-based expertise by generating hypotheses of what visual signals in the images are correlated with the task. We propose the following 4 steps: (i) Train a classifier to perform a given task (ii) Train a classifier guided StyleGAN-based image generator (StylEx) (iii) Automatically detect and visualize the top visual attributes that the classifier is sensitive towards (iv) Formulate hypotheses for the underlying mechanisms, to stimulate future research. Specifically, we present the discovered attributes to an interdisciplinary panel of experts so that hypotheses can account for social and structural determinants of health. We demonstrate results on eight prediction tasks across three medical imaging modalities: retinal fundus photographs, external eye photographs, and chest radiographs. We showcase examples of attributes that capture clinically known features, confounders that arise from factors beyond physiological mechanisms, and reveal a number of physiologically plausible novel attributes. Our approach has the potential to enable researchers to better understand, improve their assessment, and extract new knowledge from AI-based models. Importantly, we highlight that attributes generated by our framework can capture phenomena beyond physiology or pathophysiology, reflecting the real world nature of healthcare delivery and socio-cultural factors. Finally, we intend to release code to enable researchers to train their own StylEx models and analyze their predictive tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "49618488",
                        "name": "Oran Lang"
                    },
                    {
                        "authorId": "2219323970",
                        "name": "Doron Yaya-Stupp"
                    },
                    {
                        "authorId": "5815721",
                        "name": "I. Traynis"
                    },
                    {
                        "authorId": "1396212229",
                        "name": "H. Cole-Lewis"
                    },
                    {
                        "authorId": "2218963856",
                        "name": "Chloe R. Bennett"
                    },
                    {
                        "authorId": "2081156465",
                        "name": "Courtney R. Lyles"
                    },
                    {
                        "authorId": "2054812727",
                        "name": "Charles Lau"
                    },
                    {
                        "authorId": "52326632",
                        "name": "Christopher Semturs"
                    },
                    {
                        "authorId": "47191829",
                        "name": "D. Webster"
                    },
                    {
                        "authorId": "102388768",
                        "name": "G. Corrado"
                    },
                    {
                        "authorId": "1809983",
                        "name": "A. Hassidim"
                    },
                    {
                        "authorId": "2162722165",
                        "name": "Y. Matias"
                    },
                    {
                        "authorId": "2118113191",
                        "name": "Yun Liu"
                    },
                    {
                        "authorId": "12809519",
                        "name": "N. Hammel"
                    },
                    {
                        "authorId": "2490700",
                        "name": "Boris Babenko"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Domain Adaptation: Our approach builds on advances in domain-adaptation and style-transfer, developed to allow for the differential manipulating a style while preserving other content [4, 5, 18, 20]."
            ],
            "citingPaper": {
                "paperId": "ade6cfe42892f4a9621c6a10e632ae283cf6885b",
                "externalIds": {
                    "DBLP": "conf/cvpr/PerniceDQPKVHC23",
                    "ArXiv": "2306.11890",
                    "DOI": "10.1109/CVPRW59228.2023.00455",
                    "CorpusId": 259212312
                },
                "corpusId": 259212312,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ade6cfe42892f4a9621c6a10e632ae283cf6885b",
                "title": "Out of Distribution Generalization via Interventional Style Transfer in Single-Cell Microscopy",
                "abstract": "Real-world deployment of computer vision systems, including in the discovery processes of biomedical research, requires causal representations that are invariant to contextual nuisances and generalize to new data. Leveraging the internal replicate structure of two novel single-cell fluorescent microscopy datasets, we propose generally applicable tests to assess the extent to which models learn causal representations across increasingly challenging levels of OOD-generalization. We show that despite seemingly strong performance as assessed by other established metrics, both naive and contemporary baselines designed to ward against confounding, collapse to random on these tests. We introduce a new method, Interventional Style Transfer (IST), that substantially improves OOD generalization by generating interventional training distributions in which spurious correlations between biological causes and nuisances are mitigated. We publish our code 1 and datasets 2.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "49152650",
                        "name": "Wolfgang M. Pernice"
                    },
                    {
                        "authorId": "40452705",
                        "name": "Michael Doron"
                    },
                    {
                        "authorId": "49742134",
                        "name": "A. Quach"
                    },
                    {
                        "authorId": "2170609806",
                        "name": "A. Pratapa"
                    },
                    {
                        "authorId": "2220344709",
                        "name": "Sultan Kenjeyev"
                    },
                    {
                        "authorId": "91488028",
                        "name": "N. Veaux"
                    },
                    {
                        "authorId": "2220403566",
                        "name": "Michio Hirano"
                    },
                    {
                        "authorId": "145507543",
                        "name": "Juan C. Caicedo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "12358d13b6a4f6f68b57411a286f2a1e9a77d309",
                "externalIds": {
                    "DBLP": "conf/cvpr/ZhengLZXH23",
                    "DOI": "10.1109/CVPR52729.2023.00319",
                    "CorpusId": 259255706
                },
                "corpusId": 259255706,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/12358d13b6a4f6f68b57411a286f2a1e9a77d309",
                "title": "Where is My Spot? Few-shot Image Generation via Latent Subspace Optimization",
                "abstract": "Image generation relies on massive training data that can hardly produce diverse images of an unseen category according to a few examples. In this paper, we address this dilemma by projecting sparse few-shot samples into a continuous latent space that can potentially generate infinite unseen samples. The rationale behind is that we aim to locate a centroid latent position in a conditional StyleGAN, where the corresponding output image on that centroid can maximize the similarity with the given samples. Although the given samples are unseen for the conditional StyleGAN, we assume the neighboring latent subspace around the centroid belongs to the novel category, and therefore introduce two latent subspace optimization objectives. In the first one we use few-shot samples as positive anchors of the novel class, and adjust the StyleGAN to produce the corresponding results with the new class label condition. The second objective is to govern the generation process from the other way around, by altering the centroid and its surrounding latent subspace for a more precise generation of the novel class. These reciprocal optimization objectives inject a novel class into the StyleGAN latent subspace, and therefore new unseen samples can be easily produced by sampling images from it. Extensive experiments demonstrate superior few-shot generation performances compared with state-of-the-art methods, especially in terms of diversity and generation quality. Code is available at https://github.com/chansey0529/LSO.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220800052",
                        "name": "Chenxi Zheng"
                    },
                    {
                        "authorId": "2220596660",
                        "name": "Bangzhen Liu"
                    },
                    {
                        "authorId": "2108909080",
                        "name": "Huaidong Zhang"
                    },
                    {
                        "authorId": "2884662",
                        "name": "Xuemiao Xu"
                    },
                    {
                        "authorId": "2115300590",
                        "name": "Shengfeng He"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "44edd9500af80e3870bc6876abd35cb7c62a2c0b",
                "externalIds": {
                    "DBLP": "conf/cvpr/KimOLYDT23",
                    "DOI": "10.1109/CVPR52729.2023.01053",
                    "CorpusId": 261080904
                },
                "corpusId": 261080904,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/44edd9500af80e3870bc6876abd35cb7c62a2c0b",
                "title": "Grounding Counterfactual Explanation of Image Classifiers to Textual Concept Space",
                "abstract": "Concept-based explanation aims to provide concise and human-understandable explanations of an image classifier. However, existing concept-based explanation methods typically require a significant amount of manually collected concept-annotated images. This is costly and runs the risk of human biases being involved in the explanation. In this paper, we propose Counterfactual explanation with text-driven concepts (CounTEX), where the concepts are defined only from text by leveraging a pretrained multimodal joint embedding space without additional concept-annotated datasets. A conceptual counterfactual explanation is generated with text-driven concepts. To utilize the text-driven concepts defined in the joint embedding space to interpret target classifier outcome, we present a novel projection scheme for mapping the two spaces with a simple yet effective implementation. We show that CounTEX generates faithful explanations that provide a semantic understanding of model decision rationale robust to human bias.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48388878",
                        "name": "Siwon Kim"
                    },
                    {
                        "authorId": "2031932",
                        "name": "Jinoh Oh"
                    },
                    {
                        "authorId": "2108230831",
                        "name": "Sungjin Lee"
                    },
                    {
                        "authorId": "1885974",
                        "name": "Seunghak Yu"
                    },
                    {
                        "authorId": "36733784",
                        "name": "Jaeyoung Do"
                    },
                    {
                        "authorId": "2221288115",
                        "name": "Tara Taghavi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "b9c99706b63cc0be3d8b91200a4dd3693edd09b4",
                "externalIds": {
                    "ArXiv": "2306.00219",
                    "DBLP": "journals/corr/abs-2306-00219",
                    "DOI": "10.48550/arXiv.2306.00219",
                    "CorpusId": 258999925
                },
                "corpusId": 258999925,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b9c99706b63cc0be3d8b91200a4dd3693edd09b4",
                "title": "Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images",
                "abstract": "Text-to-image generative models have made remarkable advancements in generating high-quality images. However, generated images often contain undesirable artifacts or other errors due to model limitations. Existing techniques to fine-tune generated images are time-consuming (manual editing), produce poorly-integrated results (inpainting), or result in unexpected changes across the entire image (variation selection and prompt fine-tuning). In this work, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to efficiently fine-tune desired regions within an AI-synthesized image. Our method introduces new random noise patterns at targeted regions during the reverse diffusion process, enabling the model to efficiently make changes to the specified regions while preserving the original context for the rest of the image. We evaluate our method's usability and effectiveness through a user study with artists, comparing our technique against other state-of-the-art image inpainting techniques and editing software for fine-tuning AI-generated imagery.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3276489",
                        "name": "P. Gholami"
                    },
                    {
                        "authorId": "144742880",
                        "name": "R. Xiao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4e50df38414c5309091258d8660c5446c6bd1f38",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-00650",
                    "ArXiv": "2305.00650",
                    "DOI": "10.48550/arXiv.2305.00650",
                    "CorpusId": 258426292
                },
                "corpusId": 258426292,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4e50df38414c5309091258d8660c5446c6bd1f38",
                "title": "Discover and Cure: Concept-aware Mitigation of Spurious Correlation",
                "abstract": "Deep neural networks often rely on spurious correlations to make predictions, which hinders generalization beyond training environments. For instance, models that associate cats with bed backgrounds can fail to predict the existence of cats in other environments without beds. Mitigating spurious correlations is crucial in building trustworthy models. However, the existing works lack transparency to offer insights into the mitigation process. In this work, we propose an interpretable framework, Discover and Cure (DISC), to tackle the issue. With human-interpretable concepts, DISC iteratively 1) discovers unstable concepts across different environments as spurious attributes, then 2) intervenes on the training data using the discovered concepts to reduce spurious correlation. Across systematic experiments, DISC provides superior generalization ability and interpretability than the existing approaches. Specifically, it outperforms the state-of-the-art methods on an object recognition task and a skin-lesion classification task by 7.5% and 9.6%, respectively. Additionally, we offer theoretical analysis and guarantees to understand the benefits of models trained by DISC. Code and data are available at https://github.com/Wuyxin/DISC.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2188774538",
                        "name": "Shirley Wu"
                    },
                    {
                        "authorId": "2186981598",
                        "name": "Mert Yuksekgonul"
                    },
                    {
                        "authorId": "10537441",
                        "name": "Linjun Zhang"
                    },
                    {
                        "authorId": "145085305",
                        "name": "James Y. Zou"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Interpretation and manipulation of the GANs input space, or the latent style space, has been a subject of extensive research [15, 49, 42, 45, 22].",
                "Moreover, [22] uses style space to explaining and interpret the decisions made by attribute classifiers."
            ],
            "citingPaper": {
                "paperId": "98703f4beaee68721b44dfadc56e3d3bbe54fa44",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-00599",
                    "ArXiv": "2305.00599",
                    "DOI": "10.48550/arXiv.2305.00599",
                    "CorpusId": 258426822
                },
                "corpusId": 258426822,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/98703f4beaee68721b44dfadc56e3d3bbe54fa44",
                "title": "StyleGenes: Discrete and Efficient Latent Distributions for GANs",
                "abstract": "We propose a discrete latent distribution for Generative Adversarial Networks (GANs). Instead of drawing latent vectors from a continuous prior, we sample from a finite set of learnable latents. However, a direct parametrization of such a distribution leads to an intractable linear increase in memory in order to ensure sufficient sample diversity. We address this key issue by taking inspiration from the encoding of information in biological organisms. Instead of learning a separate latent vector for each sample, we split the latent space into a set of genes. For each gene, we train a small bank of gene variants. Thus, by independently sampling a variant for each gene and combining them into the final latent vector, our approach can represent a vast number of unique latent samples from a compact set of learnable parameters. Interestingly, our gene-inspired latent encoding allows for new and intuitive approaches to latent-space exploration, enabling conditional sampling from our unconditionally trained model. Moreover, our approach preserves state-of-the-art photo-realism while achieving better disentanglement than the widely-used StyleMapping network.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1628458093",
                        "name": "Evangelos Ntavelis"
                    },
                    {
                        "authorId": "73774192",
                        "name": "Mohamad Shahbazi"
                    },
                    {
                        "authorId": "3274768",
                        "name": "I. Kastanis"
                    },
                    {
                        "authorId": "1732855",
                        "name": "R. Timofte"
                    },
                    {
                        "authorId": "2129520569",
                        "name": "Martin Danelljan"
                    },
                    {
                        "authorId": "1681236",
                        "name": "L. Gool"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "14a4df97253c3d0e614b1cda9fb1dd493d183671",
                "externalIds": {
                    "DOI": "10.1007/s12065-023-00850-2",
                    "CorpusId": 258425619
                },
                "corpusId": 258425619,
                "publicationVenue": {
                    "id": "283554cb-7ba2-4666-bb9b-ce759f2ee4fd",
                    "name": "Evolutionary Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "Evol Intell"
                    ],
                    "issn": "1864-5909",
                    "url": "https://www.springer.com/engineering/computational+intelligence+and+complexity/journal/12065",
                    "alternate_urls": [
                        "https://link.springer.com/journal/12065"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/14a4df97253c3d0e614b1cda9fb1dd493d183671",
                "title": "Fingerprint image denoising and inpainting using generative adversarial networks",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2112647515",
                        "name": "W. Zhong"
                    },
                    {
                        "authorId": "2209362269",
                        "name": "Li Mao"
                    },
                    {
                        "authorId": "2216107656",
                        "name": "Yang Ning"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Example-based explanation techniques have recently become popular within visual decision-making for image classification [20, 35]."
            ],
            "citingPaper": {
                "paperId": "a470882bf9798239705f9fd40c3f86a53e367fc9",
                "externalIds": {
                    "DBLP": "journals/pacmhci/MorrisonSHP23",
                    "DOI": "10.1145/3579481",
                    "CorpusId": 258171595
                },
                "corpusId": 258171595,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a470882bf9798239705f9fd40c3f86a53e367fc9",
                "title": "Evaluating the Impact of Human Explanation Strategies on Human-AI Visual Decision-Making",
                "abstract": "Artificial intelligence (AI) is increasingly being deployed in high-stakes domains, such as disaster relief and radiology, to aid practitioners during the decision-making process. Explainable AI techniques have been developed and deployed to provide users insights into why the AI made certain predictions. However, recent research suggests that these techniques may confuse or mislead users. We conducted a series of two studies to uncover strategies that humans use to explain decisions and then understand how those explanation strategies impact visual decision-making. In our first study, we elicit explanations from humans when assessing and localizing damaged buildings after natural disasters from satellite imagery and identify four core explanation strategies that humans employed. We then follow up by studying the impact of these explanation strategies by framing the explanations from Study 1 as if they were generated by AI and showing them to a different set of decision-makers performing the same task. We provide initial insights on how causal explanation strategies improve humans' accuracy and calibrate humans' reliance on AI when the AI is incorrect. However, we also find that causal explanation strategies may lead to incorrect rationalizations when AI presents a correct assessment with incorrect localization. We explore the implications of our findings for the design of human-centered explainable AI and address directions for future work.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2054095978",
                        "name": "Katelyn Morrison"
                    },
                    {
                        "authorId": "48224632",
                        "name": "Donghoon Shin"
                    },
                    {
                        "authorId": "2257481",
                        "name": "Kenneth Holstein"
                    },
                    {
                        "authorId": "2912842",
                        "name": "Adam Perer"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "28ac6312b46c8341df9f8406d2417d16468346fe",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-15441",
                    "ArXiv": "2303.15441",
                    "DOI": "10.1109/CVPR52729.2023.01119",
                    "CorpusId": 257766385
                },
                "corpusId": 257766385,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/28ac6312b46c8341df9f8406d2417d16468346fe",
                "title": "Zero-Shot Model Diagnosis",
                "abstract": "When it comes to deploying deep vision models, the behavior of these systems must be explicable to ensure confidence in their reliability and fairness. A common approach to evaluate deep learning models is to build a labeled test set with attributes of interest and assess how well it performs. However, creating a balanced test set (i.e., one that is uniformly sampled over all the important traits) is often time-consuming, expensive, and prone to mistakes. The question we try to address is: can we evaluate the sensitivity of deep learning models to arbitrary visual attributes without an annotated test set? This paper argues the case that Zero-shot Model Diagnosis (ZOOM) is possible without the need for a test set nor labeling. To avoid the need for test sets, our system relies on a generative model and CLIP. The key idea is enabling the user to select a set of prompts (relevant to the problem) and our system will automatically search for semantic counterfactual images (i.e., synthesized images that flip the prediction in the case of a binary classifier) using the generative model. We evaluate several visual tasks (classification, key-point detection, and segmentation) in multiple visual domains to demonstrate the viability of our methodology. Extensive experiments demonstrate that our method is capable of producing counterfactual images and offering sensitivity analysis for model diagnosis without the need for a test set.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "32020492",
                        "name": "Jinqi Luo"
                    },
                    {
                        "authorId": "2156070663",
                        "name": "Zhaoning Wang"
                    },
                    {
                        "authorId": "114621402",
                        "name": "Chen Henry Wu"
                    },
                    {
                        "authorId": "145252513",
                        "name": "Dong Huang"
                    },
                    {
                        "authorId": "143867160",
                        "name": "F. D. L. Torre"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Recently, [13] used StyleGAN [12] to learn a target-model-dependent style/attribute space, which allows a human to interpret the target models\u2019 behavior in terms of attributes.",
                "We also did not compare with [13] because their method requires training a separate model on StyleGAN\u2019s original training data for each target model."
            ],
            "citingPaper": {
                "paperId": "d3f130f960110f251b7f434556853a66c0cfd082",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-13010",
                    "ArXiv": "2303.13010",
                    "DOI": "10.48550/arXiv.2303.13010",
                    "CorpusId": 257687848
                },
                "corpusId": 257687848,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d3f130f960110f251b7f434556853a66c0cfd082",
                "title": "Semantic Image Attack for Visual Model Diagnosis",
                "abstract": "In practice, metric analysis on a specific train and test dataset does not guarantee reliable or fair ML models. This is partially due to the fact that obtaining a balanced, diverse, and perfectly labeled dataset is typically expensive, time-consuming, and error-prone. Rather than relying on a carefully designed test set to assess ML models' failures, fairness, or robustness, this paper proposes Semantic Image Attack (SIA), a method based on the adversarial attack that provides semantic adversarial images to allow model diagnosis, interpretability, and robustness. Traditional adversarial training is a popular methodology for robustifying ML models against attacks. However, existing adversarial methods do not combine the two aspects that enable the interpretation and analysis of the model's flaws: semantic traceability and perceptual quality. SIA combines the two features via iterative gradient ascent on a predefined semantic attribute space and the image space. We illustrate the validity of our approach in three scenarios for keypoint detection and classification. (1) Model diagnosis: SIA generates a histogram of attributes that highlights the semantic vulnerability of the ML model (i.e., attributes that make the model fail). (2) Stronger attacks: SIA generates adversarial examples with visually interpretable attributes that lead to higher attack success rates than baseline methods. The adversarial training on SIA improves the transferable robustness across different gradient-based attacks. (3) Robustness to imbalanced datasets: we use SIA to augment the underrepresented classes, which outperforms strong augmentation and re-balancing baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "32020492",
                        "name": "Jinqi Luo"
                    },
                    {
                        "authorId": "2156070663",
                        "name": "Zhaoning Wang"
                    },
                    {
                        "authorId": "2151102768",
                        "name": "Chenhuan Wu"
                    },
                    {
                        "authorId": "47817658",
                        "name": "Dong Huang"
                    },
                    {
                        "authorId": "143867160",
                        "name": "F. D. L. Torre"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Counterfactual explanations may reveal the underlying factors of variation that a DL model considers for its decisions.(5) Although different generative DL models exist (see Bond-Tayler et al.",
                "performed a user study using Amazon Mechanical Turk to evaluate the fidelity of semantics learned by their StylEx model.(5) However, an approach including human judgment is usually subjective, not reproducible, costly, and there are rarely enough participants for significant results.",
                "However, an approach including human judgment is usually subjective, not reproducible, costly, and there are rarely enough participants for significant results.(5) For instance, Lang et al."
            ],
            "citingPaper": {
                "paperId": "c6451072cb8f3a90dd9d5268e1df62cc824d738f",
                "externalIds": {
                    "DOI": "10.1117/12.2646326",
                    "CorpusId": 257563077
                },
                "corpusId": 257563077,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c6451072cb8f3a90dd9d5268e1df62cc824d738f",
                "title": "Identifying the advantageous latent space dimensionality for StyleGANs used in industrial machine vision applications",
                "abstract": "State-of-the-art style-based generative adversarial networks (StyleGANs) synthesize high-quality images by learning a mapping from a disentangled latent space onto the image manifold. Thereby, learned representations can be analyzed by interpreting the latent space and used subsequently to control the properties of the synthesized industrial machine vision data. StyleGANs in combination with an embedding into the latent space enable the assessment of the properties of embedded images by means of their latent space representations, however, a trade-off between the dimensionality of the StyleGAN\u2019s latent space and the quality of generated images must be found. While a smaller latent space is easier to interpret, it might not capture all quality characteristics if lossless compression cannot be achieved. This work presents an evaluation scheme that uses statistical hypothesis testing to identify an advantageous dimensionality of the latent space for industrial machine vision applications. As quality measure of images synthesized by GANs, often the Fr\u00b4echet Inception distance (FID) based on features learned from the ImageNet dataset is used. However, the features of the underlying Inception network are opaque and might not be representative for application specific quality characteristics. Herein, synthetic data is evaluated instead by means of a Fr\u00b4echet distance based on selected and application specific features extracted from the used industrial machine vision dataset. With these application specific features, the image quality of multiple StyleGANs trained with different latent space dimensionalities is compared using statistical tests to select an advantageous latent space dimension.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "103763981",
                        "name": "Dominik Wolfschl\u00e4ger"
                    },
                    {
                        "authorId": "2125686884",
                        "name": "Ruslan Yermakov"
                    },
                    {
                        "authorId": "31198773",
                        "name": "B. Montavon"
                    },
                    {
                        "authorId": "2060268",
                        "name": "B. Berkels"
                    },
                    {
                        "authorId": "2146132206",
                        "name": "R. H. Schmitt"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "8cb08ec85ab454f9564b7f906e4d84e665ec2443",
                "externalIds": {
                    "ArXiv": "2303.05102",
                    "DBLP": "journals/corr/abs-2303-05102",
                    "DOI": "10.1016/j.imavis.2023.104808",
                    "CorpusId": 257427459
                },
                "corpusId": 257427459,
                "publicationVenue": {
                    "id": "6cc36eeb-d056-42c4-a306-7bcb239cc442",
                    "name": "Image and Vision Computing",
                    "type": "journal",
                    "alternate_names": [
                        "Image Vis Comput"
                    ],
                    "issn": "0262-8856",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525443/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/02628856",
                        "https://www.journals.elsevier.com/image-and-vision-computing"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8cb08ec85ab454f9564b7f906e4d84e665ec2443",
                "title": "StyleDiff: Attribute Comparison Between Unlabeled Datasets in Latent Disentangled Space",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1677093614",
                        "name": "Keisuke Kawano"
                    },
                    {
                        "authorId": "34687538",
                        "name": "Takuro Kutsuna"
                    },
                    {
                        "authorId": "3450697",
                        "name": "Ryoko Tokuhisa"
                    },
                    {
                        "authorId": "2187455748",
                        "name": "Akihiro Nakamura"
                    },
                    {
                        "authorId": "101368663",
                        "name": "Yasushi Esaki"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "many practical applications such as Image Synthesis [39], Domain Adaptation [20], Style Transfer [21, 32] and Interpretability [31] to name a few."
            ],
            "citingPaper": {
                "paperId": "729919e6e764112a66bbe2fcea0cc1224d45d145",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-14368",
                    "ArXiv": "2302.14368",
                    "DOI": "10.48550/arXiv.2302.14368",
                    "CorpusId": 257232777
                },
                "corpusId": 257232777,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/729919e6e764112a66bbe2fcea0cc1224d45d145",
                "title": "Towards Enhanced Controllability of Diffusion Models",
                "abstract": "Denoising Diffusion models have shown remarkable capabilities in generating realistic, high-quality and diverse images. However, the extent of controllability during generation is underexplored. Inspired by techniques based on GAN latent space for image manipulation, we train a diffusion model conditioned on two latent codes, a spatial content mask and a flattened style embedding. We rely on the inductive bias of the progressive denoising process of diffusion models to encode pose/layout information in the spatial structure mask and semantic/style information in the style code. We propose two generic sampling techniques for improving controllability. We extend composable diffusion models to allow for some dependence between conditional inputs, to improve the quality of generations while also providing control over the amount of guidance from each latent code and their joint distribution. We also propose timestep dependent weight scheduling for content and style latents to further improve the translations. We observe better controllability compared to existing methods and show that without explicit training objectives, diffusion models can be used for effective image manipulation and image translation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "41021457",
                        "name": "Wonwoong Cho"
                    },
                    {
                        "authorId": "39936314",
                        "name": "Hareesh Ravi"
                    },
                    {
                        "authorId": "2206112624",
                        "name": "Midhun Harikumar"
                    },
                    {
                        "authorId": "2204801",
                        "name": "V. Khuc"
                    },
                    {
                        "authorId": "50339742",
                        "name": "Krishna Kumar Singh"
                    },
                    {
                        "authorId": "2054975",
                        "name": "Jingwan Lu"
                    },
                    {
                        "authorId": "2055998168",
                        "name": "David I. Inouye"
                    },
                    {
                        "authorId": "37493415",
                        "name": "Ajinkya Kale"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "cfeefcc801fd0dd1b16eb31e87f728db1abadb59",
                "externalIds": {
                    "ArXiv": "2302.12689",
                    "DBLP": "conf/atal/HuberDMOA23",
                    "DOI": "10.48550/arXiv.2302.12689",
                    "CorpusId": 257205945
                },
                "corpusId": 257205945,
                "publicationVenue": {
                    "id": "6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                    "name": "Adaptive Agents and Multi-Agent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Adapt Agent Multi-agent Syst",
                        "International Joint Conference on Autonomous Agents & Multiagent Systems",
                        "Adapt Agent Multi-agents Syst",
                        "AAMAS",
                        "Adaptive Agents and Multi-Agents Systems",
                        "Int Jt Conf Auton Agent  Multiagent Syst"
                    ],
                    "url": "http://www.ifaamas.org/"
                },
                "url": "https://www.semanticscholar.org/paper/cfeefcc801fd0dd1b16eb31e87f728db1abadb59",
                "title": "GANterfactual-RL: Understanding Reinforcement Learning Agents' Strategies through Visual Counterfactual Explanations",
                "abstract": "Counterfactual explanations are a common tool to explain artificial intelligence models. For Reinforcement Learning (RL) agents, they answer\"Why not?\"or\"What if?\"questions by illustrating what minimal change to a state is needed such that an agent chooses a different action. Generating counterfactual explanations for RL agents with visual input is especially challenging because of their large state spaces and because their decisions are part of an overarching policy, which includes long-term decision-making. However, research focusing on counterfactual explanations, specifically for RL agents with visual input, is scarce and does not go beyond identifying defective agents. It is unclear whether counterfactual explanations are still helpful for more complex tasks like analyzing the learned strategies of different agents or choosing a fitting agent for a specific task. We propose a novel but simple method to generate counterfactual explanations for RL agents by formulating the problem as a domain transfer problem which allows the use of adversarial learning techniques like StarGAN. Our method is fully model-agnostic and we demonstrate that it outperforms the only previous method in several computational metrics. Furthermore, we show in a user study that our method performs best when analyzing which strategies different agents pursue.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1798294837",
                        "name": "Tobias Huber"
                    },
                    {
                        "authorId": "1396126293",
                        "name": "Maximilian Demmler"
                    },
                    {
                        "authorId": "1456101466",
                        "name": "Silvan Mertes"
                    },
                    {
                        "authorId": "2058026244",
                        "name": "Matthew Lyle Olson"
                    },
                    {
                        "authorId": "2202220241",
                        "name": "Elisabeth Andr'e"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[25] train a transformation network to combine the latent code with the semantic attributes."
            ],
            "citingPaper": {
                "paperId": "d4a05ac079679055f70abdd9b45e11a76c179a93",
                "externalIds": {
                    "ArXiv": "2302.09260",
                    "DBLP": "journals/corr/abs-2302-09260",
                    "DOI": "10.48550/arXiv.2302.09260",
                    "CorpusId": 257038565
                },
                "corpusId": 257038565,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d4a05ac079679055f70abdd9b45e11a76c179a93",
                "title": "Attribute-Specific Manipulation Based on Layer-Wise Channels",
                "abstract": "Image manipulation on the latent space of the pre-trained StyleGAN can control the semantic attributes of the generated images. Recently, some studies have focused on detecting channels with specific properties to directly manipulate the latent code, which is limited by the entanglement of the latent space. To detect the attribute-specific channels, we propose a novel detection method in the context of pre-trained classifiers. We analyse the gradients layer by layer on the style space. The intensities of the gradients indicate the channel's responses to specific attributes. The latent style codes of channels control separate attributes in the layers. We choose channels with top-$k$ gradients to control specific attributes in the maximum response layer. We implement single-channel and multi-channel manipulations with a certain attribute. Our methods can accurately detect relevant channels for a large number of face attributes. Extensive qualitative and quantitative results demonstrate that the proposed methods outperform state-of-the-art methods in generalization and scalability.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "102365723",
                        "name": "Yuanjie Yan"
                    },
                    {
                        "authorId": "2179595489",
                        "name": "Jian Zhao"
                    },
                    {
                        "authorId": "1728090",
                        "name": "S. Furao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "38666ba5839167ca9cca87e54a5dd02847b8d440",
                "externalIds": {
                    "ArXiv": "2302.04246",
                    "DBLP": "journals/corr/abs-2302-04246",
                    "DOI": "10.48550/arXiv.2302.04246",
                    "CorpusId": 256662215
                },
                "corpusId": 256662215,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/38666ba5839167ca9cca87e54a5dd02847b8d440",
                "title": "Shortcut Detection with Variational Autoencoders",
                "abstract": "For real-world applications of machine learning (ML), it is essential that models make predictions based on well-generalizing features rather than spurious correlations in the data. The identification of such spurious correlations, also known as shortcuts, is a challenging problem and has so far been scarcely addressed. In this work, we present a novel approach to detect shortcuts in image and audio datasets by leveraging variational autoencoders (VAEs). The disentanglement of features in the latent space of VAEs allows us to discover feature-target correlations in datasets and semi-automatically evaluate them for ML shortcuts. We demonstrate the applicability of our method on several real-world datasets and identify shortcuts that have not been discovered before.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2187302090",
                        "name": "Nicolas M. Muller"
                    },
                    {
                        "authorId": "2075341377",
                        "name": "Simon Roschmann"
                    },
                    {
                        "authorId": "2204755298",
                        "name": "Shahbaz Khan"
                    },
                    {
                        "authorId": "87891613",
                        "name": "Philip Sperl"
                    },
                    {
                        "authorId": "1519535255",
                        "name": "Konstantin Bottinger"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "StylEx [59] and C3LT [66] are two recent methods, leveraging a GAN to produce the explanations.",
                "Counterfactual visual explanations transform an image of class A so as to elicit its classification into the counter class B [38], [56], [57], [58], [59], [60]."
            ],
            "citingPaper": {
                "paperId": "010ec3fe2edfaf751b26f0aef03fd01fceeb5f58",
                "externalIds": {
                    "DBLP": "journals/pami/WangV23",
                    "DOI": "10.1109/TPAMI.2023.3241106",
                    "CorpusId": 256542511,
                    "PubMed": "37022375"
                },
                "corpusId": 256542511,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/010ec3fe2edfaf751b26f0aef03fd01fceeb5f58",
                "title": "A Generalized Explanation Framework for Visualization of Deep Learning Model Predictions",
                "abstract": "Attribution-based explanations are popular in computer vision but of limited use for fine-grained classification problems typical of expert domains, where classes differ by subtle details. In these domains, users also seek understanding of \u201cwhy\u201d a class was chosen and \u201cwhy not\u201d an alternative class. A new GenerAlized expLanatiOn fRamEwork (GALORE) is proposed to satisfy all these requirements, by unifying attributive explanations with explanations of two other types. The first is a new class of explanations, denoted deliberative, proposed to address the \u201cwhy\u201d question, by exposing the network insecurities about a prediction. The second is the class of counterfactual explanations, which have been shown to address the \u201cwhy not\u201d question but are now more efficiently computed. GALORE unifies these explanations by defining them as combinations of attribution maps with respect to various classifier predictions and a confidence score. An evaluation protocol that leverages object recognition (CUB200) and scene classification (ADE20 K) datasets combining part and attribute annotations is also proposed. Experiments show that confidence scores can improve explanation accuracy, deliberative explanations provide insight into the network deliberation process, the latter correlates with that performed by humans, and counterfactual explanations enhance the performance of human students in machine teaching experiments.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118951664",
                        "name": "Pei Wang"
                    },
                    {
                        "authorId": "1699559",
                        "name": "N. Vasconcelos"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Similarly, [Lang et al., 2021] trains an external StyleGAN as an explainer for a visual classifier."
            ],
            "citingPaper": {
                "paperId": "8b9b9db68ed03d1cff19acdb98420d5ece1d30e1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-13445",
                    "ArXiv": "2301.13445",
                    "DOI": "10.48550/arXiv.2301.13445",
                    "CorpusId": 256416377
                },
                "corpusId": 256416377,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8b9b9db68ed03d1cff19acdb98420d5ece1d30e1",
                "title": "A Survey of Explainable AI in Deep Visual Modeling: Methods and Metrics",
                "abstract": "Deep visual models have widespread applications in high-stake domains. Hence, their black-box nature is currently attracting a large interest of the research community. We present the first survey in Explainable AI that focuses on the methods and metrics for interpreting deep visual models. Covering the landmark contributions along the state-of-the-art, we not only provide a taxonomic organization of the existing techniques, but also excavate a range of evaluation metrics and collate them as measures of different properties of model explanations. Along the insightful discussion on the current trends, we also discuss the challenges and future avenues for this research direction.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47398812",
                        "name": "Naveed Akhtar"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a9cd05d3421801ba94f76fafc74573cbf81d9a50",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-06324",
                    "ArXiv": "2301.06324",
                    "DOI": "10.48550/arXiv.2301.06324",
                    "CorpusId": 255942098
                },
                "corpusId": 255942098,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a9cd05d3421801ba94f76fafc74573cbf81d9a50",
                "title": "Img2Tab: Automatic Class Relevant Concept Discovery from StyleGAN Features for Explainable Image Classification",
                "abstract": "Traditional tabular classifiers provide explainable decision-making with interpretable features(concepts). However, using their explainability in vision tasks has been limited due to the pixel representation of images. In this paper, we design Img2Tabs that classify images by concepts to harness the explainability of tabular classifiers. Img2Tabs encode image pixels into tabular features by StyleGAN inversion. Since not all of the resulting features are class-relevant or interpretable due to their generative nature, we expect Img2Tab classifiers to discover class-relevant concepts automatically from the StyleGAN features. Thus, we propose a novel method using the Wasserstein-1 metric to quantify class-relevancy and interpretability simultaneously. Using this method, we investigate whether important features extracted by tabular classifiers are class-relevant concepts. Consequently, we determine the most effective classifier for Img2Tabs in terms of discovering class-relevant concepts automatically from StyleGAN features. In evaluations, we demonstrate concept-based explanations through importance and visualization. Img2Tab achieves top-1 accuracy that is on par with CNN classifiers and deep feature learning baselines. Additionally, we show that users can easily debug Img2Tab classifiers at the concept level to ensure unbiased and fair decision-making without sacrificing accuracy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2138148717",
                        "name": "Y. Song"
                    },
                    {
                        "authorId": "2125878653",
                        "name": "S. K. Shyn"
                    },
                    {
                        "authorId": "2201430050",
                        "name": "Kwang-su Kim"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "b10ca22eeedafdc3f273a99cbda6fbf159354d18",
                "externalIds": {
                    "DBLP": "journals/ci/WangZHLL23",
                    "DOI": "10.1111/coin.12564",
                    "CorpusId": 254660121
                },
                "corpusId": 254660121,
                "publicationVenue": {
                    "id": "03e270f3-9983-44e1-9d91-754044085687",
                    "name": "International Conference on Climate Informatics",
                    "type": "conference",
                    "alternate_names": [
                        "CI",
                        "Comput Intell",
                        "Computational Intelligence",
                        "Int Conf Clim Informatics",
                        "International Workshop Climate Informatics",
                        "Computational intelligence",
                        "Int Workshop Clim Informatics",
                        "Comput intell"
                    ],
                    "issn": "0824-7935",
                    "url": "http://www.wiley.com/WileyCDA/WileyTitle/productCd-COIN.html",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/14678640"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b10ca22eeedafdc3f273a99cbda6fbf159354d18",
                "title": "Current status, application, and challenges of the interpretability of generative adversarial network models",
                "abstract": "The generative adversarial network (GAN) is one of the most promising methods in the field of unsupervised learning. Model developers, users, and other interested people are highly concerned about the GAN mechanism where the generative model and the discriminative model learn from each other in a gameplay manner, which generates a causal relationship among output features, internal network structure, feature extraction process, and output results. Through the study of the interpretability of GANs, the validity, reliability, and robustness of the application of GANs can be verified, and the weaknesses of the GANs in specific applications can be diagnosed, which can provide support for designing better network structures. It can also improve security and reduce the decision\u2010making and prediction risks brought by GANs. In this article, the study of the interpretability of GANs is explored, and ways of the evaluation of the application effect of GAN interpretability techniques are analyzed. Besides, the effect of interpretable GANs in fields such as medical treatment and military is discussed, and current limitations and future challenges are demonstrated.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2196113926",
                        "name": "Sulin Wang"
                    },
                    {
                        "authorId": "2195744512",
                        "name": "Chengqiang Zhao"
                    },
                    {
                        "authorId": "2111400588",
                        "name": "Lingling Huang"
                    },
                    {
                        "authorId": "33232556",
                        "name": "Yuanwei Li"
                    },
                    {
                        "authorId": "2112079714",
                        "name": "Ruochen Li"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "290ad057e7c36901201c537e863bef5ff1d80d5c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-04825",
                    "ArXiv": "2212.04825",
                    "DOI": "10.1109/CVPR52729.2023.01922",
                    "CorpusId": 254536007
                },
                "corpusId": 254536007,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/290ad057e7c36901201c537e863bef5ff1d80d5c",
                "title": "A Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others",
                "abstract": "Machine learning models have been found to learn shortcuts\u2014unintended decision rules that are unable to generalize\u2014undermining models' reliability. Previous works address this problem under the tenuous assumption that only a single shortcut exists in the training data. Real-world images are rife with multiple visual cues from background to texture. Key to advancing the reliability of vision systems is understanding whether existing methods can overcome multiple shortcuts or struggle in a Whac-A-Mole game, i.e., where mitigating one shortcut amplifies reliance on others. To address this shortcoming, we propose two benchmarks: 1) UrbanCars, a dataset with precisely controlled spurious cues, and 2) ImageNet-W, an evaluation set based on ImageNet for watermark, a shortcut we discovered affects nearly every modern vision model. Along with texture and background, ImageNet-W allows us to study multiple shortcuts emerging from training on natural images. We find computer vision models, including large foundation models\u2014regardless of training set, architecture, and supervision\u2014struggle when multiple shortcuts are present. Even methods explicitly designed to combat shortcuts struggle in a Whac-A-Mole dilemma. To tackle this challenge, we propose Last Layer Ensemble, a simple-yet-effective method to mitigate multiple shortcuts without Whac-A-Mole behavior. Our results surface multi-shortcut mitigation as an overlooked challenge critical to advancing the reliability of vision systems. The datasets and code are released: https://github.com/facebookresearch/Whac-A-Mole.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48458657",
                        "name": "Zhiheng Li"
                    },
                    {
                        "authorId": "22229139",
                        "name": "I. Evtimov"
                    },
                    {
                        "authorId": "1821267",
                        "name": "Albert Gordo"
                    },
                    {
                        "authorId": "3322806",
                        "name": "C. Hazirbas"
                    },
                    {
                        "authorId": "1756099",
                        "name": "Tal Hassner"
                    },
                    {
                        "authorId": "66286536",
                        "name": "Cristian Cant\u00f3n Ferrer"
                    },
                    {
                        "authorId": "2026123",
                        "name": "Chenliang Xu"
                    },
                    {
                        "authorId": "3407874",
                        "name": "Mark Ibrahim"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "3877a045d853a8b938f039750cb6e0fb4aa9f83f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-12380",
                    "ArXiv": "2211.12380",
                    "DOI": "10.1109/CVPR52729.2023.01446",
                    "CorpusId": 253761301
                },
                "corpusId": 253761301,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3877a045d853a8b938f039750cb6e0fb4aa9f83f",
                "title": "OCTET: Object-aware Counterfactual Explanations",
                "abstract": "Nowadays, deep vision models are being widely deployed in safety-critical applications, e.g., autonomous driving, and explainability of such models is becoming a pressing concern. Among explanation methods, counter-factual explanations aim to find minimal and interpretable changes to the input image that would also change the output of the model to be explained. Such explanations point end-users at the main factors that impact the decision of the model. However, previous methods struggle to explain decision models trained on images with many objects, e.g., urban scenes, which are more difficult to work with but also arguably more critical to explain. In this work, we propose to tackle this issue with an object-centric framework for counterfactual explanation generation. Our method, inspired by recent generative modeling works, encodes the query image into a latent space that is structured in a way to ease object-level manipulations. Doing so, it provides the end-user with control over which search directions (e.g., spatial displacement of objects, style modification, etc.) are to be explored during the counterfactual generation. We conduct a set of experiments on counterfactual explanation benchmarks for driving scenes, and we show that our method can be adapted beyond classification, e.g., to explain semantic segmentation models. To complete our analysis, we design and run a user study that measures the usefulness of counterfactual explanations in understanding a decision model. Code is available at https://github.com/valeoai/OCTET.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2191690100",
                        "name": "Mehdi Zemni"
                    },
                    {
                        "authorId": "8408521",
                        "name": "Micka\u00ebl Chen"
                    },
                    {
                        "authorId": "39541096",
                        "name": "\u00c9loi Zablocki"
                    },
                    {
                        "authorId": "1405301761",
                        "name": "H. Ben-younes"
                    },
                    {
                        "authorId": "2173636176",
                        "name": "Patrick P'erez"
                    },
                    {
                        "authorId": "51021910",
                        "name": "M. Cord"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Another limitation of our method is the fact that the level of detail, and thus, the level of realism, of the produced counterfactuals rely on the quality of the generative model of the VAE, which is known to be inferior to other types of generative models such as GANs.",
                "The most widely used generative models are Generative Adversarial Networks\n(GANs) (Goodfellow et al., 2014) and Variational AutoEncoders (VAEs) (Kingma and Welling, 2014).",
                "This last remark leads us to a perspective: all of the techniques presented on this paper could be adapted to any generative model, such as GANs or Normalizing Flows (Rezende and Blei, 2015) or more sophisticated VAE architectures (Dai and Wipf, 2019; Larsen et al., 2016).",
                "Several counterfactual explanation methods already use GANs (Lang et al., 2021; Liu et al., 2019) and even diffusion models (Sanchez and Tsaftaris, 2022)."
            ],
            "citingPaper": {
                "paperId": "8fa0cb31923dcfbe7f2b45565b307543314b15e2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-15370",
                    "ArXiv": "2211.15370",
                    "DOI": "10.48550/arXiv.2211.15370",
                    "CorpusId": 254043949
                },
                "corpusId": 254043949,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8fa0cb31923dcfbe7f2b45565b307543314b15e2",
                "title": "Clarity: an improved gradient method for producing quality visual counterfactual explanations",
                "abstract": "Visual counterfactual explanations identify modifications to an image that would change the prediction of a classifier. We propose a set of techniques based on generative models (VAE) and a classifier ensemble directly trained in the latent space, which all together, improve the quality of the gradient required to compute visual counterfactuals. These improvements lead to a novel classification model, Clarity, which produces realistic counterfactual explanations over all images. We also present several experiments that give insights on why these techniques lead to better quality results than those in the literature. The explanations produced are competitive with the state-of-the-art and emphasize the importance of selecting a meaningful input space for training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2047712457",
                        "name": "Claire Theobald"
                    },
                    {
                        "authorId": "3045528",
                        "name": "Fr\u00e9d\u00e9ric Pennerath"
                    },
                    {
                        "authorId": "1403862587",
                        "name": "B. Conan-Guez"
                    },
                    {
                        "authorId": "2067756",
                        "name": "Miguel Couceiro"
                    },
                    {
                        "authorId": "2064925708",
                        "name": "Amedeo Napoli"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "04ae0e78e6a05de71b193f83eeebb33efbf946e6",
                "externalIds": {
                    "PubMedCentral": "10227067",
                    "DBLP": "journals/corr/abs-2211-06522",
                    "ArXiv": "2211.06522",
                    "DOI": "10.48550/arXiv.2211.06522",
                    "CorpusId": 253511066,
                    "PubMed": "37248379"
                },
                "corpusId": 253511066,
                "publicationVenue": {
                    "id": "adf0b60b-d3a4-4ec1-b1a1-04313d832403",
                    "name": "npj Precision Oncology",
                    "alternate_names": [
                        "npj Precis Oncol"
                    ],
                    "issn": "2397-768X",
                    "url": "http://www.nature.com/npjprecisiononcology/"
                },
                "url": "https://www.semanticscholar.org/paper/04ae0e78e6a05de71b193f83eeebb33efbf946e6",
                "title": "Deep learning generates synthetic cancer histology for explainability and education",
                "abstract": "Artificial intelligence methods including deep neural networks (DNN) can provide rapid molecular classification of tumors from routine histology with accuracy that matches or exceeds human pathologists. Discerning how neural networks make their predictions remains a significant challenge, but explainability tools help provide insights into what models have learned when corresponding histologic features are poorly defined. Here, we present a method for improving explainability of DNN models using synthetic histology generated by a conditional generative adversarial network (cGAN). We show that cGANs generate high-quality synthetic histology images that can be leveraged for explaining DNN models trained to classify molecularly-subtyped tumors, exposing histologic features associated with molecular state. Fine-tuning synthetic histology through class and layer blending illustrates nuanced morphologic differences between tumor subtypes. Finally, we demonstrate the use of synthetic histology for augmenting pathologist-in-training education, showing that these intuitive visualizations can reinforce and improve understanding of histologic manifestations of tumor biology.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40115385",
                        "name": "J. Dolezal"
                    },
                    {
                        "authorId": "2179556910",
                        "name": "Rachelle Wolk"
                    },
                    {
                        "authorId": "1573786429",
                        "name": "Hanna M. Hieromnimon"
                    },
                    {
                        "authorId": "1737779229",
                        "name": "Frederick M. Howard"
                    },
                    {
                        "authorId": "46307477",
                        "name": "Andrew Srisuwananukorn"
                    },
                    {
                        "authorId": "48275774",
                        "name": "D. Karpeyev"
                    },
                    {
                        "authorId": "144972037",
                        "name": "S. Ramesh"
                    },
                    {
                        "authorId": "52033954",
                        "name": "S. Kochanny"
                    },
                    {
                        "authorId": "2151131089",
                        "name": "J. Kwon"
                    },
                    {
                        "authorId": "9508303",
                        "name": "M. Agni"
                    },
                    {
                        "authorId": "2190753380",
                        "name": "Richard C. Simon"
                    },
                    {
                        "authorId": "46934575",
                        "name": "Chandni Desai"
                    },
                    {
                        "authorId": "2123476693",
                        "name": "Raghad Kherallah"
                    },
                    {
                        "authorId": "2190926671",
                        "name": "Tung D. Nguyen"
                    },
                    {
                        "authorId": "37924151",
                        "name": "Jefree J. Schulte"
                    },
                    {
                        "authorId": "1633559641",
                        "name": "K. Cole"
                    },
                    {
                        "authorId": "51232575",
                        "name": "G. Khramtsova"
                    },
                    {
                        "authorId": "8465724",
                        "name": "M. Garassino"
                    },
                    {
                        "authorId": "3629763",
                        "name": "A. Husain"
                    },
                    {
                        "authorId": "2190793055",
                        "name": "Huihua Li"
                    },
                    {
                        "authorId": "1392208636",
                        "name": "R. Grossman"
                    },
                    {
                        "authorId": "8380418",
                        "name": "N. Cipriani"
                    },
                    {
                        "authorId": "3833069",
                        "name": "A. Pearson"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Then, we borrow and revise the feature attribution strategy of counterfactual analysis (Lang et al. 2021; Zhang, Wang, and Sang 2022) to measure the importance of proxy features by counterfactually changing the proxy features:\n\u03b1c = Yc(x, pb)\u2212 Yc(x, anchor) (6) Where \u03b1c indicates the importance of\u2026",
                "Then, we borrow and revise the feature attribution strategy of counterfactual analysis (Lang et al. 2021; Zhang, Wang, and Sang 2022) to measure the importance of proxy features by counterfactually changing the proxy features:"
            ],
            "citingPaper": {
                "paperId": "79736f7b5a493547249866cf2a3372af635d51c1",
                "externalIds": {
                    "ArXiv": "2211.01253",
                    "DBLP": "journals/corr/abs-2211-01253",
                    "DOI": "10.48550/arXiv.2211.01253",
                    "CorpusId": 253255247
                },
                "corpusId": 253255247,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/79736f7b5a493547249866cf2a3372af635d51c1",
                "title": "Fair Visual Recognition via Intervention with Proxy Features",
                "abstract": "Deep learning models often learn to make predictions that rely on sensitive social attributes like gender and race, which poses signi\ufb01cant fairness risks, especially in societal appli- cations, e.g., hiring, banking, and criminal justice. Existing work tackles this issue by minimizing information about so- cial attributes in models for debiasing. However, the high correlation between target task and social attributes makes bias mitigation incompatible with target task accuracy. Recalling that model bias arises because the learning of features in re- gard to bias attributes (i.e., bias features) helps target task optimization, we explore the following research question: Can we leverage proxy features to replace the role of bias feature in target task optimization for debiasing? To this end, we propose Proxy Debiasing , to \ufb01rst transfer the target task\u2019s learning of bias information from bias features to arti\ufb01cial proxy features, and then employ causal intervention to eliminate proxy features in inference. The key idea of Proxy Debi- asing is to design controllable proxy features to on one hand replace bias features in contributing to target task during the training stage, and on the other hand easily to be removed by intervention during the inference stage. This guarantees the elimination of bias features without affecting the target information, thus addressing the fairness-accuracy paradox in previous debiasing solutions. We apply Proxy Debiasing to several benchmark datasets, and achieve signi\ufb01cant improve- ments over the state-of-the-art debiasing methods in both of accuracy and fairness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2153912315",
                        "name": "Yi Zhang"
                    },
                    {
                        "authorId": "1798398",
                        "name": "J. Sang"
                    },
                    {
                        "authorId": "2110125710",
                        "name": "Junyan Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "In contrast, we train a GAN-based counterfactual explainer [58, 33] to derive CAD.",
                "Following previous work [33, 57, 58], we design the PCE to satisfy the following three properties:",
                "We derived counterfactuals using a progressive counterfactual explainer (PCE) that create a series of perturbations of an input image, such that the classification decision is changed to a different class [57, 33]."
            ],
            "citingPaper": {
                "paperId": "14f7a95eb015f30053f889f536a4c6efdf6e2ea4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-12196",
                    "ArXiv": "2210.12196",
                    "DOI": "10.1109/WACV56688.2023.00470",
                    "CorpusId": 253098609,
                    "PubMed": "37724183"
                },
                "corpusId": 253098609,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/14f7a95eb015f30053f889f536a4c6efdf6e2ea4",
                "title": "Augmentation by Counterfactual Explanation -Fixing an Overconfident Classifier",
                "abstract": "A highly accurate but overconfident model is ill-suited for deployment in critical applications such as healthcare and autonomous driving. The classification outcome should reflect a high uncertainty on ambiguous in-distribution samples that lie close to the decision boundary. The model should also refrain from making overconfident decisions on samples that lie far outside its training distribution, far-out-of-distribution (far-OOD), or on unseen samples from novel classes that lie near its training distribution (near-OOD). This paper proposes an application of counterfactual explanations in fixing an over-confident classifier. Specifically, we propose to fine-tune a given pre-trained classifier using augmentations from a counterfactual explainer (ACE) to fix its uncertainty characteristics while retaining its predictive performance. We perform extensive experiments with detecting far-OOD, near-OOD, and ambiguous samples. Our empirical results show that the revised model have improved uncertainty measures, and its performance is competitive to the state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51163902",
                        "name": "Sumedha Singla"
                    },
                    {
                        "authorId": "145959106",
                        "name": "Nihal Murali"
                    },
                    {
                        "authorId": "1803981",
                        "name": "Forough Arabshahi"
                    },
                    {
                        "authorId": "1756441678",
                        "name": "Sofia Triantafyllou"
                    },
                    {
                        "authorId": "3443176",
                        "name": "K. Batmanghelich"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recently, [26] trained a StyleGAN2 [24] model to discover and manipulate class attributes."
            ],
            "citingPaper": {
                "paperId": "39be6e8eaf20d17e42ba5ad3bc04127704aad6b3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-11841",
                    "ArXiv": "2210.11841",
                    "DOI": "10.48550/arXiv.2210.11841",
                    "CorpusId": 253080402
                },
                "corpusId": 253080402,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/39be6e8eaf20d17e42ba5ad3bc04127704aad6b3",
                "title": "Diffusion Visual Counterfactual Explanations",
                "abstract": "Visual Counterfactual Explanations (VCEs) are an important tool to understand the decisions of an image classifier. They are 'small' but 'realistic' semantic changes of the image changing the classifier decision. Current approaches for the generation of VCEs are restricted to adversarially robust models and often contain non-realistic artefacts, or are limited to image classification problems with few classes. In this paper, we overcome this by generating Diffusion Visual Counterfactual Explanations (DVCEs) for arbitrary ImageNet classifiers via a diffusion process. Two modifications to the diffusion process are key for our DVCEs: first, an adaptive parameterization, whose hyperparameters generalize across images and models, together with distance regularization and late start of the diffusion process, allow us to generate images with minimal semantic changes to the original ones but different classification. Second, our cone regularization via an adversarially robust model ensures that the diffusion process does not converge to trivial non-semantic changes, but instead produces realistic images of the target class which achieve high confidence by the classifier.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49799275",
                        "name": "Maximilian Augustin"
                    },
                    {
                        "authorId": "2165469787",
                        "name": "Valentyn Boreiko"
                    },
                    {
                        "authorId": "39171784",
                        "name": "Francesco Croce"
                    },
                    {
                        "authorId": "143610806",
                        "name": "Matthias Hein"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Consequently, a disentangled StyleSpace was proposed for finding the attributes that determine classification (Lang et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "5808bda70e06a414e9970dff6946f007e942ac8e",
                "externalIds": {
                    "ArXiv": "2210.12112",
                    "DBLP": "conf/emnlp/HupertSW22",
                    "DOI": "10.48550/arXiv.2210.12112",
                    "CorpusId": 253080390
                },
                "corpusId": 253080390,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/5808bda70e06a414e9970dff6946f007e942ac8e",
                "title": "Describing Sets of Images with Textual-PCA",
                "abstract": "We seek to semantically describe a set of images, capturing both the attributes of single images and the variations within the set. Our procedure is analogous to Principle Component Analysis, in which the role of projection vectors is replaced with generated phrases. First, a centroid phrase that has the largest average semantic similarity to the images in the set is generated, where both the computation of the similarity and the generation are based on pretrained vision-language models. Then, the phrase that generates the highest variation among the similarity scores is generated, using the same models. The next phrase maximizes the variance subject to being orthogonal, in the latent space, to the highest-variance phrase, and the process continues. Our experiments show that our method is able to convincingly capture the essence of image sets and describe the individual elements in a semantically meaningful way within the context of the entire set. Our code is available at: https://github.com/OdedH/textual-pca.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2188651208",
                        "name": "Oded Hupert"
                    },
                    {
                        "authorId": "38211837",
                        "name": "Idan Schwartz"
                    },
                    {
                        "authorId": "145128145",
                        "name": "Lior Wolf"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Following recent advancements in image synthesis quality [21\u201324], many works utilized the latent space of pretrained generative adversarial networks (GANs) to perform a variety of image manipulations [3, 15, 30, 36, 49, 50]."
            ],
            "citingPaper": {
                "paperId": "23e261a20a315059b4de5492ed071c97a20c12e7",
                "externalIds": {
                    "ArXiv": "2210.09276",
                    "DBLP": "conf/cvpr/KawarZLTCDMI23",
                    "DOI": "10.1109/CVPR52729.2023.00582",
                    "CorpusId": 252918469
                },
                "corpusId": 252918469,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/23e261a20a315059b4de5492ed071c97a20c12e7",
                "title": "Imagic: Text-Based Real Image Editing with Diffusion Models",
                "abstract": "Text-conditioned image editing has recently attracted considerable interest. However, most methods are currently limited to one of the following: specific editing types (e.g., object overlay, style transfer), synthetically generated images, or requiring multiple input images of a common object. In this paper we demonstrate, for the very first time, the ability to apply complex (e.g., non-rigid) text-based semantic edits to a single real image. For example, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. Our method can make a standing dog sit down, cause a bird to spread its wings, etc. \u2013 each within its single high-resolution user-provided natural image. Contrary to previous work, our proposed method requires only a single input image and a target text (the desired edit). It operates on real images, and does not require any additional inputs (such as image masks or additional views of the object). Our method, called Imagic, leverages a pre-trained text-to-image diffusion model for this task. It produces a text embedding that aligns with both the input image and the target text, while fine-tuning the diffusion model to capture the image-specific appearance. We demonstrate the quality and versatility of Imagic on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single unified framework. To better assess performance, we introduce TEdBench, a highly challenging image editing benchmark. We conduct a user study, whose findings show that human raters prefer Imagic to previous leading editing methods on TEdBench.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2047309422",
                        "name": "Bahjat Kawar"
                    },
                    {
                        "authorId": "2145761298",
                        "name": "Shiran Zada"
                    },
                    {
                        "authorId": "49618488",
                        "name": "Oran Lang"
                    },
                    {
                        "authorId": "2047833213",
                        "name": "Omer Tov"
                    },
                    {
                        "authorId": "2146380",
                        "name": "Hui-Tang Chang"
                    },
                    {
                        "authorId": "2112779",
                        "name": "Tali Dekel"
                    },
                    {
                        "authorId": "2138834",
                        "name": "Inbar Mosseri"
                    },
                    {
                        "authorId": "144611617",
                        "name": "M. Irani"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": true,
            "contexts": [
                "From figure 1, it is evident that GANs trained with the FFHQ dataset are biased towards generating more faces in the age group \u201d20-29\u201d and mostly \u201dWhite\u201d faces.",
                "Our observations from the analysis of the results are as follows : (Observation-1 is drawn from experiment-1 and observations-2,3,4 were drawn from experiment-2)\n\u2022 Observation-1: GANs are biased towards age group \u201d20-29\u201d and \u201dWhite\u201d faces.",
                "Our main contributions in this research are as follows :\n\u2022 Result-1: We observed that GANs trained on FFHQ dataset exhibit bias for the \u201dage\u201d and \u201drace\u201d protected attributes.",
                "However, existing models (GANs) trained with FFHQ dataset [17] are prone to bias and fairness issues.",
                "Hence, it is important to debiase GANs before using them in any application.",
                "Keywords: Bias, Fairness, GANs, Face Verification, Synthetic Data",
                "In this work, we analyze bias and fairness of GANs [18] and their impact on face verification systems.",
                "Generative Models such as Generative Adversarial Networks (GANs) [9,29,2] are basic building blocks in most of image recognition architectures.",
                "In future, we aim to investigate methods and techniques for debiasing GANs with respect to different critical attributes.",
                "GANs are popular networks that are very successful in generating faces of good perpetual quality.",
                "GANs can be used to obtain synthetic data where data is scarce and in scenarios where privacy is important."
            ],
            "citingPaper": {
                "paperId": "1ace17eac18c188ec7becbfe91f8084defbc12ce",
                "externalIds": {
                    "ArXiv": "2208.13061",
                    "CorpusId": 251979862
                },
                "corpusId": 251979862,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1ace17eac18c188ec7becbfe91f8084defbc12ce",
                "title": "On Biased Behavior of GANs for Face Verification",
                "abstract": ". Deep Learning systems need large data for training. Datasets for training face verification systems are difficult to obtain and prone to privacy issues. Synthetic data generated by generative models such as GANs can be a good alternative. However, we show that data generated from GANs are prone to bias and fairness issues. Specifically, GANs trained on FFHQ dataset show biased behavior towards generating white faces in the age group of 20-29. We also demonstrate that synthetic faces cause disparate impact, specifically for race attribute, when used for fine tuning face verification systems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2114891897",
                        "name": "Sasikanth Kotti"
                    },
                    {
                        "authorId": "2338122",
                        "name": "Mayank Vatsa"
                    },
                    {
                        "authorId": "2041134713",
                        "name": "Richa Singh"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", StyleGAN2 [19] and StyleGAN3 [17], which is one of the most applied unconditional GANs in various downstream tasks [33,20,2]."
            ],
            "citingPaper": {
                "paperId": "03c5710f582c80cfd1ec0b08744bbbce1d42b79d",
                "externalIds": {
                    "DBLP": "conf/eccv/XuH0L22",
                    "ArXiv": "2208.08840",
                    "DOI": "10.48550/arXiv.2208.08840",
                    "CorpusId": 251643370
                },
                "corpusId": 251643370,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/03c5710f582c80cfd1ec0b08744bbbce1d42b79d",
                "title": "Mind the Gap in Distilling StyleGANs",
                "abstract": "StyleGAN family is one of the most popular Generative Adversarial Networks (GANs) for unconditional generation. Despite its impressive performance, its high demand on storage and computation impedes their deployment on resource-constrained devices. This paper provides a comprehensive study of distilling from the popular StyleGAN-like architecture. Our key insight is that the main challenge of StyleGAN distillation lies in the output discrepancy issue, where the teacher and student model yield different outputs given the same input latent code. Standard knowledge distillation losses typically fail under this heterogeneous distillation scenario. We conduct thorough analysis about the reasons and effects of this discrepancy issue, and identify that the mapping network plays a vital role in determining semantic information of generated images. Based on this finding, we propose a novel initialization strategy for the student model, which can ensure the output consistency to the maximum extent. To further enhance the semantic consistency between the teacher and student model, we present a latent-direction-based distillation loss that preserves the semantic relations in latent space. Extensive experiments demonstrate the effectiveness of our approach in distilling StyleGAN2 and StyleGAN3, outperforming existing GAN distillation methods by a large margin.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46538811",
                        "name": "Guodong Xu"
                    },
                    {
                        "authorId": "29976076",
                        "name": "Yuenan Hou"
                    },
                    {
                        "authorId": "2117940996",
                        "name": "Ziwei Liu"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "[17] talks about explainability on images using a classifier."
            ],
            "citingPaper": {
                "paperId": "0a372cf9dd21a887f23ad88b7938ba4af184e84c",
                "externalIds": {
                    "DBLP": "conf/kdd/TalwadkerCPMS22",
                    "DOI": "10.1145/3534678.3539179",
                    "CorpusId": 251518410
                },
                "corpusId": 251518410,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/0a372cf9dd21a887f23ad88b7938ba4af184e84c",
                "title": "CognitionNet: A Collaborative Neural Network for Play Style Discovery in Online Skill Gaming Platform",
                "abstract": "Games are one of the safest source of realizing self-esteem and relaxation at the same time. An online gaming platform typically has massive data coming in, e.g., in-game actions, player moves, clickstreams, transactions etc. It is rather interesting, as something as simple as data on gaming moves can help create a psychological imprint of the user at that moment, based on her impulsive reactions and response to a situation in the game. Mining this knowledge can: (a) immediately help better explain observed and predicted player behavior; and (b) consequently propel deeper understanding towards players' experience, growth and protection. To this effect, we focus on discovery of the \"game behaviours\" as micro-patterns formed by continuous sequence of games and the persistent \"play styles\" of the players' as a sequence of such sequences on an online skill gaming platform for Rummy. The complex sequences of intricate sequences is analysed through a novel collaborative two stage deep neural network, CognitionNet. The first stage focuses on mining game behaviours as cluster representations in a latent space while the second aggregates over these micro patterns (e.g., transitions across patterns) to discover play styles via a supervised classification objective around player engagement. The dual objective allows CognitionNet to reveal several player psychology inspired decision making and tactics. To our knowledge, this is the first and one-of-its-kind research to fully automate the discovery of: (i) player psychology and game tactics from telemetry data; and (ii) relevant diagnostic explanations to players' engagement predictions. The collaborative training of the two networks with differential input dimensions is enabled using a novel formulation of \"bridge loss\". The network plays pivotal role in obtaining homogeneous and consistent play style definitions and significantly outperforms the SOTA baselines wherever applicable.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3099404",
                        "name": "Rukma Talwadker"
                    },
                    {
                        "authorId": "46799516",
                        "name": "Surajit Chakrabarty"
                    },
                    {
                        "authorId": "2181310910",
                        "name": "Aditya Pareek"
                    },
                    {
                        "authorId": "101181945",
                        "name": "Tridib Mukherjee"
                    },
                    {
                        "authorId": "2059419780",
                        "name": "Deepak Saini"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In the face image domain, a recent study [37] shows that gender classifier is biased by multiple attributes, such as Heavy Makeup and Wearing Lipstick.",
                "[37] find that gender classifiers are biased with multiple independent bias attributes, including wearing lipsticks, eyebrow thickness, nose width, etc.",
                "[37,42] discovers unknown biases without labels."
            ],
            "citingPaper": {
                "paperId": "c2fbcd0f7fd1edb4616c435b5e92106eb5c4945a",
                "externalIds": {
                    "DBLP": "conf/eccv/LiHX22",
                    "ArXiv": "2207.10077",
                    "DOI": "10.48550/arXiv.2207.10077",
                    "CorpusId": 250698948
                },
                "corpusId": 250698948,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/c2fbcd0f7fd1edb4616c435b5e92106eb5c4945a",
                "title": "Discover and Mitigate Unknown Biases with Debiasing Alternate Networks",
                "abstract": "Deep image classifiers have been found to learn biases from datasets. To mitigate the biases, most previous methods require labels of protected attributes (e.g., age, skin tone) as full-supervision, which has two limitations: 1) it is infeasible when the labels are unavailable; 2) they are incapable of mitigating unknown biases -- biases that humans do not preconceive. To resolve those problems, we propose Debiasing Alternate Networks (DebiAN), which comprises two networks -- a Discoverer and a Classifier. By training in an alternate manner, the discoverer tries to find multiple unknown biases of the classifier without any annotations of biases, and the classifier aims at unlearning the biases identified by the discoverer. While previous works evaluate debiasing results in terms of a single bias, we create Multi-Color MNIST dataset to better benchmark mitigation of multiple biases in a multi-bias setting, which not only reveals the problems in previous methods but also demonstrates the advantage of DebiAN in identifying and mitigating multiple biases simultaneously. We further conduct extensive experiments on real-world datasets, showing that the discoverer in DebiAN can identify unknown biases that may be hard to be found by humans. Regarding debiasing, DebiAN achieves strong bias mitigation performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48458657",
                        "name": "Zhiheng Li"
                    },
                    {
                        "authorId": "1397590190",
                        "name": "Anthony J. Hoogs"
                    },
                    {
                        "authorId": "2026123",
                        "name": "Chenliang Xu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "67add6a624cd81dfc6fee7f094bcd4b39dab8743",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-07553",
                    "ArXiv": "2207.07553",
                    "DOI": "10.48550/arXiv.2207.07553",
                    "CorpusId": 250607445
                },
                "corpusId": 250607445,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/67add6a624cd81dfc6fee7f094bcd4b39dab8743",
                "title": "CheXplaining in Style: Counterfactual Explanations for Chest X-rays using StyleGAN",
                "abstract": "Deep learning models used in medical image anal-ysis are prone to raising reliability concerns due to their black-box nature. To shed light on these black-box models, previous works predominantly focus on identifying the contribution of input features to the diagnosis, i.e., feature attribution. In this work, we explore counterfactual explanations to identify what patterns the models rely on for diagnosis. Speci\ufb01cally, we investigate the effect of changing features within chest X-rays on the classi\ufb01er\u2019s output to understand its decision mech-anism. We leverage a StyleGAN-based approach (StyleEx) to create counterfactual explanations for chest X-rays by manipulating speci\ufb01c latent directions in their latent space. In addition, we propose EigenFind to signi\ufb01cantly reduce the computation time of generated explanations. We clinically evaluate the relevancy of our counterfactual explanations with the help of radiologists. Our code is publicly available. 1",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2176642016",
                        "name": "Matan Atad"
                    },
                    {
                        "authorId": "2136964650",
                        "name": "V. Dmytrenko"
                    },
                    {
                        "authorId": "50024168",
                        "name": "Yitong Li"
                    },
                    {
                        "authorId": "2108266228",
                        "name": "Xinyue Zhang"
                    },
                    {
                        "authorId": "2372357",
                        "name": "Matthias Keicher"
                    },
                    {
                        "authorId": "38095391",
                        "name": "J. Kirschke"
                    },
                    {
                        "authorId": "2364183",
                        "name": "B. Wiestler"
                    },
                    {
                        "authorId": "16805480",
                        "name": "Ashkan Khakzar"
                    },
                    {
                        "authorId": "145587209",
                        "name": "N. Navab"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2022 The ability to produce counterfactual images (e.g., Shetty et al., 2019; Singla et al., 2020; Xiao et al., 2021; Leclerc et al., 2021; Li & Xu, 2021; Lang et al., 2021; Plumb et al., 2022; Wiles et al., 2023)."
            ],
            "citingPaper": {
                "paperId": "1b693d569432efc4a7d56662a86ab28bd4832358",
                "externalIds": {
                    "ArXiv": "2207.04104",
                    "CorpusId": 259833461
                },
                "corpusId": 259833461,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1b693d569432efc4a7d56662a86ab28bd4832358",
                "title": "Towards a More Rigorous Science of Blindspot Discovery in Image Classification Models",
                "abstract": "A growing body of work studies Blindspot Discovery Methods (\"BDM\"s): methods that use an image embedding to find semantically meaningful (i.e., united by a human-understandable concept) subsets of the data where an image classifier performs significantly worse. Motivated by observed gaps in prior work, we introduce a new framework for evaluating BDMs, SpotCheck, that uses synthetic image datasets to train models with known blindspots and a new BDM, PlaneSpot, that uses a 2D image representation. We use SpotCheck to run controlled experiments that identify factors that influence BDM performance (e.g., the number of blindspots in a model, or features used to define the blindspot) and show that PlaneSpot is competitive with and in many cases outperforms existing BDMs. Importantly, we validate these findings by designing additional experiments that use real image data from MS-COCO, a large image benchmark dataset. Our findings suggest several promising directions for future work on BDM design and evaluation. Overall, we hope that the methodology and analyses presented in this work will help facilitate a more rigorous science of blindspot discovery.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "31929250",
                        "name": "Gregory Plumb"
                    },
                    {
                        "authorId": "2142541079",
                        "name": "Nari Johnson"
                    },
                    {
                        "authorId": "2128108979",
                        "name": "'Angel Alexander Cabrera"
                    },
                    {
                        "authorId": "145532827",
                        "name": "Ameet Talwalkar"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Also, VCEs have been generated using GANs [20] (no models/code is available) but the advantage of our VCE is that they depend only on the classifier and thus there is no danger that the prior of the GAN \u201chides\u201d undesired behavior of the classifier."
            ],
            "citingPaper": {
                "paperId": "4ea40430f5e8dcff36aeacd57476040eb0eb8caa",
                "externalIds": {
                    "DBLP": "conf/miccai/BoreikoIAMKFBH22",
                    "DOI": "10.1101/2022.07.06.22276633",
                    "CorpusId": 250332002
                },
                "corpusId": 250332002,
                "publicationVenue": {
                    "id": "d5e5b5e7-54b1-4f53-82fc-4853f3e71c58",
                    "name": "medRxiv",
                    "type": "journal",
                    "url": "https://www.medrxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4ea40430f5e8dcff36aeacd57476040eb0eb8caa",
                "title": "Visual explanations for the detection of diabetic retinopathy from retinal fundus images",
                "abstract": "In medical image classification tasks like the detection of diabetic retinopathy from retinal fundus images, it is highly desirable to get visual explanations for the decisions of black-box deep neural networks (DNNs). However, gradient-based saliency methods often fail to highlight the diseased image regions reliably. On the other hand, adversarially robust models have more interpretable gradients than plain models but suffer typically from a significant drop in accuracy, which is unacceptable for clinical practice. Here, we show that one can get the best of both worlds by ensembling a plain and an adversarially robust model: maintaining high accuracy but having improved visual explanations. Also, our ensemble produces meaningful visual counterfactuals which are complementary to existing saliency-based techniques. Code is available under url{https://github.com/valentyn1boreiko/Fundus_VCEs}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "5597964",
                        "name": "V. Bore\u012dko"
                    },
                    {
                        "authorId": "89918933",
                        "name": "I. Ilanchezian"
                    },
                    {
                        "authorId": "145366222",
                        "name": "M. Ayhan"
                    },
                    {
                        "authorId": "1958850145",
                        "name": "S. Mu\u00cc\u0088ller"
                    },
                    {
                        "authorId": "36284274",
                        "name": "Lisa M. Koch"
                    },
                    {
                        "authorId": "1641091270",
                        "name": "H. Faber"
                    },
                    {
                        "authorId": "1689077",
                        "name": "Philipp Berens"
                    },
                    {
                        "authorId": "49165189",
                        "name": "M. Hein"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Counterfactual explanation methods [18, 19, 20, 21] help in analysing a classifier by creating several carefully constructed what-if scenarios by perturbing specific features, but are also example-based."
            ],
            "citingPaper": {
                "paperId": "514eec7a2c3c9445b4a8a8408c47594709cf9be1",
                "externalIds": {
                    "ArXiv": "2207.01916",
                    "DBLP": "journals/corr/abs-2207-01916",
                    "DOI": "10.48550/arXiv.2207.01916",
                    "CorpusId": 250279756
                },
                "corpusId": 250279756,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/514eec7a2c3c9445b4a8a8408c47594709cf9be1",
                "title": "Hierarchical Symbolic Reasoning in Hyperbolic Space for Deep Discriminative Models",
                "abstract": "Explanations for \\emph{black-box} models help us understand model decisions as well as provide information on model biases and inconsistencies. Most of the current explainability techniques provide a single level of explanation, often in terms of feature importance scores or feature attention maps in input space. Our focus is on explaining deep discriminative models at \\emph{multiple levels of abstraction}, from fine-grained to fully abstract explanations. We achieve this by using the natural properties of \\emph{hyperbolic geometry} to more efficiently model a hierarchy of symbolic features and generate \\emph{hierarchical symbolic rules} as part of our explanations. Specifically, for any given deep discriminative model, we distill the underpinning knowledge by discretisation of the continuous latent space using vector quantisation to form symbols, followed by a \\emph{hyperbolic reasoning block} to induce an \\emph{abstraction tree}. We traverse the tree to extract explanations in terms of symbolic rules and its corresponding visual semantics. We demonstrate the effectiveness of our method on the MNIST and AFHQ high-resolution animal faces dataset. Our framework is available at \\url{https://github.com/koriavinash1/SymbolicInterpretability}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9696089",
                        "name": "Ainkaran Santhirasekaram"
                    },
                    {
                        "authorId": "35982249",
                        "name": "A. Kori"
                    },
                    {
                        "authorId": "2064337286",
                        "name": "A. Rockall"
                    },
                    {
                        "authorId": "2174812191",
                        "name": "Mathias Winkler"
                    },
                    {
                        "authorId": "49973505",
                        "name": "Francesca Toni"
                    },
                    {
                        "authorId": "1709824",
                        "name": "Ben Glocker"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Among the different forms of explanations, counterfactual explanations are recently gaining attention [9, 10, 16, 17].",
                "In case of generating counterfactual explanations for images, black-box models are usually explained via twin-surrogate models to provide visual explanations with desired latent properties [27, 26, 25, 28, 29, 17]."
            ],
            "citingPaper": {
                "paperId": "299d9193e2843c0bed4513d4a647f19206bebb2d",
                "externalIds": {
                    "ArXiv": "2207.01917",
                    "DBLP": "journals/corr/abs-2207-01917",
                    "DOI": "10.48550/arXiv.2207.01917",
                    "CorpusId": 250280171
                },
                "corpusId": 250280171,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/299d9193e2843c0bed4513d4a647f19206bebb2d",
                "title": "GLANCE: Global to Local Architecture-Neutral Concept-based Explanations",
                "abstract": "Most of the current explainability techniques focus on capturing the importance of features in input space. However, given the complexity of models and data-generating processes, the resulting explanations are far from being \u2018complete\u2019, in that they lack an indication of feature interactions and visualization of their \u2018effect\u2019. In this work, we propose a novel twin-surrogate explainability framework to explain the decisions made by any CNN-based image classi\ufb01er (irrespective of the architec-ture). For this, we \ufb01rst disentangle latent features from the classi\ufb01er, followed by aligning these features to observed/human-de\ufb01ned \u2018context\u2019 features. These aligned features form semantically meaningful concepts that are used for extracting a causal graph depicting the \u2018perceived\u2019 data-generating process, describing the inter- and intra-feature interactions between unobserved latent features and observed \u2018context\u2019 features. This causal graph serves as a global model from which local explanations of different forms can be extracted. Speci\ufb01cally, we provide a generator to visualize the \u2018effect\u2019 of interactions among features in latent space and draw feature importance therefrom as local explanations. Our framework utilizes adversarial knowledge distillation to faithfully learn a representation from the classi\ufb01ers\u2019 latent space and use it for extracting visual explanations. We use the styleGAN-v2 architecture with an additional regularization term to enforce disentanglement and alignment. We demonstrate and evaluate explanations obtained with our framework on Morpho-MNIST and on the FFHQ human faces dataset. Our framework is available at https://github.com/koriavinash1/GLANCE-Explanations",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35982249",
                        "name": "A. Kori"
                    },
                    {
                        "authorId": "1709824",
                        "name": "Ben Glocker"
                    },
                    {
                        "authorId": "49973505",
                        "name": "Francesca Toni"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "d7e13c15c263f51580a1089967618da8747cc43b",
                "externalIds": {
                    "DBLP": "journals/frai/ZhangPSPMS22",
                    "PubMedCentral": "9289439",
                    "DOI": "10.3389/frai.2022.872858",
                    "CorpusId": 250275474,
                    "PubMed": "35860344"
                },
                "corpusId": 250275474,
                "publicationVenue": {
                    "id": "6a8c0041-d0b7-4e32-b52c-33adef005c7e",
                    "name": "Frontiers in Artificial Intelligence",
                    "alternate_names": [
                        "Front Artif Intell"
                    ],
                    "issn": "2624-8212",
                    "url": "https://www.frontiersin.org/journals/artificial-intelligence#"
                },
                "url": "https://www.semanticscholar.org/paper/d7e13c15c263f51580a1089967618da8747cc43b",
                "title": "Comparing Deep Learning Approaches for Understanding Genotype \u00d7 Phenotype Interactions in Biomass Sorghum",
                "abstract": "We explore the use of deep convolutional neural networks (CNNs) trained on overhead imagery of biomass sorghum to ascertain the relationship between single nucleotide polymorphisms (SNPs), or groups of related SNPs, and the phenotypes they control. We consider both CNNs trained explicitly on the classification task of predicting whether an image shows a plant with a reference or alternate version of various SNPs as well as CNNs trained to create data-driven features based on learning features so that images from the same plot are more similar than images from different plots, and then using the features this network learns for genetic marker classification. We characterize how efficient both approaches are at predicting the presence or absence of a genetic markers, and visualize what parts of the images are most important for those predictions. We find that the data-driven approaches give somewhat higher prediction performance, but have visualizations that are harder to interpret; and we give suggestions of potential future machine learning research and discuss the possibilities of using this approach to uncover unknown genotype \u00d7 phenotype relationships.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155280358",
                        "name": "Zeyu Zhang"
                    },
                    {
                        "authorId": "2174781509",
                        "name": "Madison Pope"
                    },
                    {
                        "authorId": "34892854",
                        "name": "N. Shakoor"
                    },
                    {
                        "authorId": "143857761",
                        "name": "Robert Pless"
                    },
                    {
                        "authorId": "2491793",
                        "name": "T. Mockler"
                    },
                    {
                        "authorId": "15017879",
                        "name": "Abby Stylianou"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "\u2026models to explain what a classifier learnt still by pointing out already perceptible or known features (that were actually used to annotate the pictures), but, to our knowledge, never evaluated on invisible cell phenotypes in the context of various assays (Lang et al. 2021; Singla et al. 2021).",
                "\u2026in image generation and translation have been proposed, including recent work explaining black box classifiers, but, to our knowledge, never with the aim of explaining invisible changes between conditions (Choi et al. 2018; Baek et al. 2020; Zhu et al. 2017; Lang et al. 2021; Singla et al. 2021).",
                "Some recent work has used generative models to explain what a classifier learnt still by pointing out already perceptible or known features (that were actually used to annotate the pictures), but, to our knowledge, never evaluated on invisible cell phenotypes in the context of various assays (Lang et al. 2021; Singla et al. 2021).",
                "aim of explaining invisible changes between conditions (Choi et al. 2018; Baek et al. 2020; Zhu et al. 2017; Lang et al. 2021; Singla et al. 2021)."
            ],
            "citingPaper": {
                "paperId": "68c33d946a38f620ff019456082b202d5aa94b85",
                "externalIds": {
                    "DOI": "10.1101/2022.06.16.496413",
                    "CorpusId": 249873188
                },
                "corpusId": 249873188,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/68c33d946a38f620ff019456082b202d5aa94b85",
                "title": "Revealing invisible cell phenotypes with conditional generative modeling",
                "abstract": "Biological sciences, drug discovery and medicine rely heavily on cell phenotype perturbation and observation. Aside from dramatic events such as cell division or cell death, most cell phenotypic changes that keep cells alive are subtle and thus hidden from us by natural cell variability: two cells in the same condition already look different. While we show that deep learning models can leverage invisible features from microscopy images, to discriminate between close conditions, these features can yet hardly be observed and therefore interpreted. In this work, we show that conditional generative models can be used to transform an image of cells from any one condition to another, thus canceling cell variability. We visually and quantitatively validate that the principle of synthetic cell perturbation works on discernible cases such as high concentration drug treatments, nuclear translocation and golgi apparatus assays. We then illustrate its effectiveness in displaying otherwise invisible cell phenotypes triggered by blood cells under parasite infection, the presence of a disease-causing pathological mutation in differentiated neurons derived from iPSCs or low concentration drug treatments. The proposed approach, easy to use and robust, opens the door to the accessible discovery of biological and disease biomarkers.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2374598",
                        "name": "Alexis Lamiable"
                    },
                    {
                        "authorId": "90921790",
                        "name": "T. Champetier"
                    },
                    {
                        "authorId": "2171360955",
                        "name": "Francesco Leonardi"
                    },
                    {
                        "authorId": "2064656900",
                        "name": "E. Cohen"
                    },
                    {
                        "authorId": "2157444731",
                        "name": "Peter Sommer"
                    },
                    {
                        "authorId": "2052563470",
                        "name": "David Hardy"
                    },
                    {
                        "authorId": "3488319",
                        "name": "N. Argy"
                    },
                    {
                        "authorId": "4018826",
                        "name": "A. Massougbodji"
                    },
                    {
                        "authorId": "6369832",
                        "name": "E. Del Nery"
                    },
                    {
                        "authorId": "40171391",
                        "name": "G. Cottrell"
                    },
                    {
                        "authorId": "2319017",
                        "name": "Yong-Jun Kwon"
                    },
                    {
                        "authorId": "2186053",
                        "name": "Auguste Genovesio"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Moreover, some prior work experiment with incorporating the classifier [22] or contrastive language-image models [39] into GAN to accommodate attributes into the latent space."
            ],
            "citingPaper": {
                "paperId": "da135c5294aca324ce7ff3e0d30d7a1e1ed74f83",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-05257",
                    "ArXiv": "2206.05257",
                    "DOI": "10.48550/arXiv.2206.05257",
                    "CorpusId": 249605335
                },
                "corpusId": 249605335,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/da135c5294aca324ce7ff3e0d30d7a1e1ed74f83",
                "title": "Explaining Image Classifiers Using Contrastive Counterfactuals in Generative Latent Spaces",
                "abstract": "Despite their high accuracies, modern complex image classifiers cannot be trusted for sensitive tasks due to their unknown decision-making process and potential biases. Counterfactual explanations are very effective in providing transparency for these black-box algorithms. Nevertheless, generating counterfactuals that can have a consistent impact on classifier outputs and yet expose interpretable feature changes is a very challenging task. We introduce a novel method to generate causal and yet interpretable counterfactual explanations for image classifiers using pretrained generative models without any re-training or conditioning. The generative models in this technique are not bound to be trained on the same data as the target classifier. We use this framework to obtain contrastive and causal sufficiency and necessity scores as global explanations for black-box classifiers. On the task of face attribute classification, we show how different attributes influence the classifier output by providing both causal and contrastive feature attributions, and the corresponding counterfactual images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46650151",
                        "name": "Kamran Alipour"
                    },
                    {
                        "authorId": "2003163970",
                        "name": "Aditya Lahiri"
                    },
                    {
                        "authorId": "3419364",
                        "name": "E. Adeli"
                    },
                    {
                        "authorId": "2124624117",
                        "name": "Babak Salimi"
                    },
                    {
                        "authorId": "1694780",
                        "name": "M. Pazzani"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Nevertheless, a few works [77, 81, 86, 89] undertake small user studies (9 \u2264 N \u2264 60) on a relatively limited set of generated counterfactuals.",
                "[86] train a StyleGAN with a classifier specific style space."
            ],
            "citingPaper": {
                "paperId": "2429e2635d65e8b5929fbb43af102ad79d109b79",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-05075",
                    "ArXiv": "2206.05075",
                    "DOI": "10.48550/arXiv.2206.05075",
                    "CorpusId": 249605307
                },
                "corpusId": 249605307,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2429e2635d65e8b5929fbb43af102ad79d109b79",
                "title": "Diffeomorphic Counterfactuals with Generative Models",
                "abstract": "Counterfactuals can explain classification decisions of neural networks in a human interpretable way. We propose a simple but effective method to generate such counterfactuals. More specifically, we perform a suitable diffeomorphic coordinate transformation and then perform gradient ascent in these coordinates to find counterfactuals which are classified with great confidence as a specified target class. We propose two methods to leverage generative models to construct such suitable coordinate systems that are either exactly or approximately diffeomorphic. We analyze the generation process theoretically using Riemannian differential geometry and validate the quality of the generated counterfactuals using various qualitative and quantitative measures.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46658900",
                        "name": "Ann-Kathrin Dombrowski"
                    },
                    {
                        "authorId": "3315379",
                        "name": "Jan E. Gerken"
                    },
                    {
                        "authorId": "2113612432",
                        "name": "Klaus-Robert M\u00fcller"
                    },
                    {
                        "authorId": "52029112",
                        "name": "P. Kessel"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "48d7402aee86e6e9832b7b628a2aa52207822ee5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-03208",
                    "ArXiv": "2206.03208",
                    "DOI": "10.48550/arXiv.2206.03208",
                    "CorpusId": 249431627
                },
                "corpusId": 249431627,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/48d7402aee86e6e9832b7b628a2aa52207822ee5",
                "title": "From \"Where\" to \"What\": Towards Human-Understandable Explanations through Concept Relevance Propagation",
                "abstract": "The emerging field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to today\u2019s powerful but opaque deep learning models. While local XAI methods explain individual predictions in form of attribution maps, thereby identifying where important features occur (but not providing information about what they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of methods thus only provide partial insights and leave the burden of interpreting the model\u2019s reasoning to the user. Only few contemporary techniques aim at combining the principles behind both local and global XAI for obtaining more informative explanations. Those methods, however, are often limited to specific model architectures or impose additional requirements on training regimes or data and label availability, which renders the post-hoc application to arbitrarily pre-trained models practically impossible. In this work we introduce the Concept Relevance Propagation (CRP) approach, which combines the local and global perspectives of XAI and thus allows answering both the \u201cwhere\u201d and \u201cwhat\u201d questions for individual predictions, without additional constraints imposed. We further introduce the principle of Relevance Maximization for finding representative examples of encoded concepts based on their usefulness to the model. We thereby lift the dependency on the common practice of Activation Maximization and its limitations. We demonstrate the capabilities of our methods in various settings, showcasing that Concept Relevance Propagation and Relevance Maximization lead to more human interpretable explanations and provide deep insights into the model\u2019s representations and reasoning through concept atlases, concept composition analyses, and quantitative investigations of concept subspaces and their role in fine-grained decision making.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2168455698",
                        "name": "R. Achtibat"
                    },
                    {
                        "authorId": "2126051719",
                        "name": "Maximilian Dreyer"
                    },
                    {
                        "authorId": "2168462352",
                        "name": "Ilona Eisenbraun"
                    },
                    {
                        "authorId": "1774736",
                        "name": "S. Bosse"
                    },
                    {
                        "authorId": "48531638",
                        "name": "Thomas Wiegand"
                    },
                    {
                        "authorId": "1699054",
                        "name": "W. Samek"
                    },
                    {
                        "authorId": "3633358",
                        "name": "S. Lapuschkin"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "These works mostly use a set of human-specified concepts to analyze model behavior, however, there is an increasing interest in automatically discovering the concepts that are used by a model (Yeh et al., 2020; Ghorbani et al., 2019; Lang et al., 2021).",
                "There is further increasing interest in automatically discovering the concepts that are used by a model (Yeh et al., 2020; Ghorbani et al., 2019; Lang et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "8545e249ab7a49f4a5abcfade395b90ffadb687a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-15480",
                    "ArXiv": "2205.15480",
                    "DOI": "10.48550/arXiv.2205.15480",
                    "CorpusId": 249209990
                },
                "corpusId": 249209990,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8545e249ab7a49f4a5abcfade395b90ffadb687a",
                "title": "Post-hoc Concept Bottleneck Models",
                "abstract": "Concept Bottleneck Models (CBMs) map the inputs onto a set of interpretable concepts (``the bottleneck'') and use the concepts to make predictions. A concept bottleneck enhances interpretability since it can be investigated to understand what concepts the model\"sees\"in an input and which of these concepts are deemed important. However, CBMs are restrictive in practice as they require dense concept annotations in the training data to learn the bottleneck. Moreover, CBMs often do not match the accuracy of an unrestricted neural network, reducing the incentive to deploy them in practice. In this work, we address these limitations of CBMs by introducing Post-hoc Concept Bottleneck models (PCBMs). We show that we can turn any neural network into a PCBM without sacrificing model performance while still retaining the interpretability benefits. When concept annotations are not available on the training data, we show that PCBM can transfer concepts from other datasets or from natural language descriptions of concepts via multimodal models. A key benefit of PCBM is that it enables users to quickly debug and update the model to reduce spurious correlations and improve generalization to new distributions. PCBM allows for global model edits, which can be more efficient than previous works on local interventions that fix a specific prediction. Through a model-editing user study, we show that editing PCBMs via concept-level feedback can provide significant performance gains without using data from the target domain or model retraining.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2186981598",
                        "name": "Mert Yuksekgonul"
                    },
                    {
                        "authorId": "2027032530",
                        "name": "Maggie Wang"
                    },
                    {
                        "authorId": "145085305",
                        "name": "James Y. Zou"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[34] Oran Lang, Yossi Gandelsman, Michal Yarom, Yoav Wald, Gal Elidan, Avinatan Hassidim, 262 William T Freeman, Phillip Isola, Amir Globerson, Michal Irani, et al.",
                "On the other hand, there have been studies (Li & Xu, 2021; Lang et al., 2021; Krishnakumar et al., 2021) that identify the bias attribute of the training dataset without human supervision.",
                "On the other hand, there have been studies [42, 34, 32] that identify the bias attribute 402 of the training dataset without human supervision."
            ],
            "citingPaper": {
                "paperId": "35cd12a6fe30e8fc532c37396f581bc8dc0c2a09",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-15704",
                    "ArXiv": "2205.15704",
                    "DOI": "10.48550/arXiv.2205.15704",
                    "CorpusId": 249209690
                },
                "corpusId": 249209690,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/35cd12a6fe30e8fc532c37396f581bc8dc0c2a09",
                "title": "Mitigating Dataset Bias by Using Per-sample Gradient",
                "abstract": "The performance of deep neural networks is strongly influenced by the training dataset setup. In particular, when attributes having a strong correlation with the target attribute are present, the trained model can provide unintended prejudgments and show significant inference errors (i.e., the dataset bias problem). Various methods have been proposed to mitigate dataset bias, and their emphasis is on weakly correlated samples, called bias-conflicting samples. These methods are based on explicit bias labels involving human or empirical correlation metrics (e.g., training loss). However, such metrics require human costs or have insufficient theoretical explanation. In this study, we propose a debiasing algorithm, called PGD (Per-sample Gradient-based Debiasing), that comprises three steps: (1) training a model on uniform batch sampling, (2) setting the importance of each sample in proportion to the norm of the sample gradient, and (3) training the model using importance-batch sampling, whose probability is obtained in step (2). Compared with existing baselines for various synthetic and real-world datasets, the proposed method showed state-of-the-art accuracy for a the classification task. Furthermore, we describe theoretical understandings about how PGD can mitigate dataset bias.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40917250",
                        "name": "Sumyeong Ahn"
                    },
                    {
                        "authorId": "2109600198",
                        "name": "Seongyoon Kim"
                    },
                    {
                        "authorId": "70509252",
                        "name": "Se-Young Yun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[43] to test what characteristics of an image directly influence hypodescent and valence bias.",
                "[43] train a GAN to explain the decisions of an image classifier by discovering the visual attributes"
            ],
            "citingPaper": {
                "paperId": "3cfea2a4291ea4e5e0061d8a626e414e27ec5ac5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-10764",
                    "ArXiv": "2205.10764",
                    "DOI": "10.1145/3531146.3533185",
                    "CorpusId": 248986266
                },
                "corpusId": 248986266,
                "publicationVenue": {
                    "id": "cdc83875-a82d-445c-b097-cbe91afe99a8",
                    "name": "Conference on Fairness, Accountability and Transparency",
                    "type": "conference",
                    "alternate_names": [
                        "FAccT",
                        "Conf Fairness Account Transpar"
                    ],
                    "url": "https://facctconference.org/"
                },
                "url": "https://www.semanticscholar.org/paper/3cfea2a4291ea4e5e0061d8a626e414e27ec5ac5",
                "title": "Evidence for Hypodescent in Visual Semantic AI",
                "abstract": "We examine the state-of-the-art multimodal \u201dvisual semantic\u201d model CLIP (\u201dContrastive Language Image Pretraining\u201d) for the rule of hypodescent, or one-drop rule, whereby multiracial people are more likely to be assigned a racial or ethnic label corresponding to a minority or disadvantaged racial or ethnic group than to the equivalent majority or advantaged group. A face morphing experiment grounded in psychological research demonstrating hypodescent indicates that, at the midway point of 1,000 series of morphed images, CLIP associates 69.7% of Black-White female images with a Black text label over a White text label, and similarly prefers Latina (75.8%) and Asian (89.1%) text labels at the midway point for Latina-White female and Asian-White female morphs, reflecting hypodescent. Additionally, assessment of the underlying cosine similarities in the model reveals that association with White is correlated with association with \u201dperson,\u201d with Pearson\u2019s \u03c1 as high as 0.82, p < 10\u2212 90 over a 21,000-image morph series, indicating that a White person corresponds to the default representation of a person in CLIP. Finally, we show that the stereotype-congruent pleasantness association of an image correlates with association with the Black text label in CLIP, with Pearson\u2019s \u03c1 = 0.48, p < 10\u2212 90 for 21,000 Black-White multiracial male images, and \u03c1 = 0.41, p < 10\u2212 90 for Black-White multiracial female images. CLIP is trained on English-language text gathered using data collected from an American website (Wikipedia), and our findings demonstrate that CLIP embeds the values of American racial hierarchy, reflecting the implicit and explicit beliefs that are present in human minds. We contextualize these findings within the history of and psychology of hypodescent. Overall, the data suggests that AI supervised using natural language will, unless checked, learn biases that reflect racial hierarchies.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "100508901",
                        "name": "R. Wolfe"
                    },
                    {
                        "authorId": "1968771",
                        "name": "M. Banaji"
                    },
                    {
                        "authorId": "144537437",
                        "name": "Aylin Caliskan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "340535d68dedefd64b530154eeda5c4569da4a36",
                "externalIds": {
                    "DBLP": "conf/iui/Sarkar22",
                    "ArXiv": "2205.10119",
                    "DOI": "10.48550/arXiv.2205.10119",
                    "CorpusId": 248302066
                },
                "corpusId": 248302066,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/340535d68dedefd64b530154eeda5c4569da4a36",
                "title": "Is Explainable AI a Race Against Model Complexity? 192-199",
                "abstract": "Explaining the behaviour of intelligent systems will get increasingly and perhaps intractably challenging as models grow in size and complexity. We may not be able to expect an explanation for every prediction made by a brain-scale model, nor can we expect explanations to remain objective or apolitical. Our functionalist understanding of these models is of less advantage than we might assume. Models precede explanations, and can be useful even when both model and explanation are incorrect. Explainability may never win the race against complexity, but this is less problematic than it seems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40766061",
                        "name": "Advait Sarkar"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Very recently visual counterfactuals based on generative models have been proposed [36, 47, 60] but no code has been released so far."
            ],
            "citingPaper": {
                "paperId": "8ed6e33483f302f372199fa037061d835f8ed5b6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-07972",
                    "ArXiv": "2205.07972",
                    "DOI": "10.1007/978-3-031-16788-1_9",
                    "CorpusId": 248834482
                },
                "corpusId": 248834482,
                "publicationVenue": {
                    "id": "4bdc459e-68eb-4596-8e24-85dd8a047952",
                    "name": "German Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Ger Conf Pattern Recognit",
                        "GCPR"
                    ],
                    "url": "http://www.dagm.de/"
                },
                "url": "https://www.semanticscholar.org/paper/8ed6e33483f302f372199fa037061d835f8ed5b6",
                "title": "Sparse Visual Counterfactual Explanations in Image Space",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2165469787",
                        "name": "Valentyn Boreiko"
                    },
                    {
                        "authorId": "49799275",
                        "name": "Maximilian Augustin"
                    },
                    {
                        "authorId": "39171784",
                        "name": "Francesco Croce"
                    },
                    {
                        "authorId": "1689077",
                        "name": "Philipp Berens"
                    },
                    {
                        "authorId": "143610806",
                        "name": "Matthias Hein"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Generative models can also be used for explaining the attributes that are highly correlated with the model\u2019s decision; for example, [77]",
                "As an example, [77] uses selfattention mechanisms of transformers which enables the models to selectively focus on certain parts of their input and thus reason more effectively."
            ],
            "citingPaper": {
                "paperId": "7ac0aaa2a89b8c157b06fb8c62af4b1badc25b0d",
                "externalIds": {
                    "PubMedCentral": "9141387",
                    "DOI": "10.3390/diagnostics12051278",
                    "CorpusId": 248972153,
                    "PubMed": "35626433"
                },
                "corpusId": 248972153,
                "publicationVenue": {
                    "id": "1944b6e1-2c1d-4f42-88e3-9f8a52f57e47",
                    "name": "Diagnostics",
                    "issn": "2075-4418",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217965",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/diagnostics",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217965"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7ac0aaa2a89b8c157b06fb8c62af4b1badc25b0d",
                "title": "Artificial Intelligence for Upper Gastrointestinal Endoscopy: A Roadmap from Technology Development to Clinical Practice",
                "abstract": "Stomach cancer is the third deadliest type of cancer in the world (0.86 million deaths in 2017). In 2035, a 20% increase will be observed both in incidence and mortality due to demographic effects if no interventions are foreseen. Upper GI endoscopy (UGIE) plays a paramount role in early diagnosis and, therefore, improved survival rates. On the other hand, human and technical factors can contribute to misdiagnosis while performing UGIE. In this scenario, artificial intelligence (AI) has recently shown its potential in compensating for the pitfalls of UGIE, by leveraging deep learning architectures able to efficiently recognize endoscopic patterns from UGIE video data. This work presents a review of the current state-of-the-art algorithms in the application of AI to gastroscopy. It focuses specifically on the threefold tasks of assuring exam completeness (i.e., detecting the presence of blind spots) and assisting in the detection and characterization of clinical findings, both gastric precancerous conditions and neoplastic lesion changes. Early and promising results have already been obtained using well-known deep learning architectures for computer vision, but many algorithmic challenges remain in achieving the vision of AI-assisted UGIE. Future challenges in the roadmap for the effective integration of AI tools within the UGIE clinical practice are discussed, namely the adoption of more robust deep learning architectures and methods able to embed domain knowledge into image/video classifiers as well as the availability of large, annotated datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1797591",
                        "name": "F. Renna"
                    },
                    {
                        "authorId": "2147405607",
                        "name": "Miguel Martins"
                    },
                    {
                        "authorId": "2149889247",
                        "name": "Alexandre Neto"
                    },
                    {
                        "authorId": "152569543",
                        "name": "Ant\u00f3nio Cunha"
                    },
                    {
                        "authorId": "13029112",
                        "name": "D. Lib\u00e2nio"
                    },
                    {
                        "authorId": "1388888559",
                        "name": "M. Dinis-Ribeiro"
                    },
                    {
                        "authorId": "40606287",
                        "name": "M. Coimbra"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Generative models have been proposed to visualize classifiers [26, 28, 32, 37, 44].",
                "The methods in [26, 44] rely on StyleGAN [22] that generate high quality explanations, but lack substantial quantitative experiments on common baselines, and rely on search algorithms to find the relevant latent codes."
            ],
            "citingPaper": {
                "paperId": "3e4851a761c6cb3b72ae7e04e6c52fdead6d4b35",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-05376",
                    "ArXiv": "2204.05376",
                    "DOI": "10.1109/CVPRW56347.2022.00331",
                    "CorpusId": 248118862
                },
                "corpusId": 248118862,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3e4851a761c6cb3b72ae7e04e6c52fdead6d4b35",
                "title": "medXGAN: Visual Explanations for Medical Classifiers through a Generative Latent Space",
                "abstract": "Despite the surge of deep learning in the past decade, some users are skeptical to deploy these models in practice due to their black-box nature. Specifically, in the medical space where there are severe potential repercussions, we need to develop methods to gain confidence in the models\u2019 decisions. To this end, we propose a novel medical imaging generative adversarial framework, medXGAN (medical eXplanation GAN), to visually explain what a medical classifier focuses on in its binary predictions. By encoding domain knowledge of medical images, we are able to disentangle anatomical structure and pathology, leading to fine-grained visualization through latent interpolation. Furthermore, we optimize the latent space such that interpolation explains how the features contribute to the classifier\u2019s output. Our method outperforms baselines such as Gradient-Weighted Class Activation Mapping (Grad-CAM) and Integrated Gradients in localization and explanatory ability. Additionally, a combination of the medXGAN with Integrated Gradients can yield explanations more robust to noise. The project page with code is available at: https://avdravid.github.io/medXGANpage/.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "116990729",
                        "name": "Amil Dravid"
                    },
                    {
                        "authorId": "16766276",
                        "name": "Florian Schiffers"
                    },
                    {
                        "authorId": "40206014",
                        "name": "Boqing Gong"
                    },
                    {
                        "authorId": "144842935",
                        "name": "A. Katsaggelos"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Having a single model that provides control over different object attributes has received substantial attention from the research community [7, 18, 20]."
            ],
            "citingPaper": {
                "paperId": "517f943d0d97d9b6eff7c3a5975649bce8115b7a",
                "externalIds": {
                    "ArXiv": "2204.02273",
                    "DBLP": "journals/corr/abs-2204-02273",
                    "DOI": "10.1109/CVPR52688.2022.01124",
                    "CorpusId": 247958379
                },
                "corpusId": 247958379,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/517f943d0d97d9b6eff7c3a5975649bce8115b7a",
                "title": "Arbitrary-Scale Image Synthesis",
                "abstract": "Positional encodings have enabled recent works to train a single adversarial network that can generate images of different scales. However, these approaches are either limited to a set of discrete scales or struggle to maintain good perceptual quality at the scales for which the model is not trained explicitly. We propose the design of scale-consistent positional encodings invariant to our generator's layers transformations. This enables the generation of arbitrary-scale images even at scales unseen during training. Moreover, we incorporate novel inter-scale augmentations into our pipeline and partial generation training to facilitate the synthesis of consistent images at arbitrary scales. Lastly, we show competitive results for a continuum of scales on various commonly used datasets for image synthesis.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1628458093",
                        "name": "Evangelos Ntavelis"
                    },
                    {
                        "authorId": "73774192",
                        "name": "Mohamad Shahbazi"
                    },
                    {
                        "authorId": "3274768",
                        "name": "I. Kastanis"
                    },
                    {
                        "authorId": "1732855",
                        "name": "R. Timofte"
                    },
                    {
                        "authorId": "2129520569",
                        "name": "Martin Danelljan"
                    },
                    {
                        "authorId": "1681236",
                        "name": "L. Gool"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "901f91006e113db7df64e155c40ed207f0056980",
                "externalIds": {
                    "DOI": "10.1016/j.cirp.2022.03.016",
                    "CorpusId": 248194174
                },
                "corpusId": 248194174,
                "publicationVenue": {
                    "id": "64fd23a1-3c84-4991-afdb-a731ad1abf24",
                    "name": "CIRP annals",
                    "type": "journal",
                    "alternate_names": [
                        "CIRP Annals",
                        "CIRP Ann",
                        "CIRP ann"
                    ],
                    "issn": "0007-8506",
                    "url": "https://www.journals.elsevier.com/cirp-annals",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00078506"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/901f91006e113db7df64e155c40ed207f0056980",
                "title": "Metrologically interpretable feature extraction for industrial machine vision using generative deep learning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146132206",
                        "name": "R. H. Schmitt"
                    },
                    {
                        "authorId": "103763981",
                        "name": "Dominik Wolfschl\u00e4ger"
                    },
                    {
                        "authorId": "1397267970",
                        "name": "Evelina Masliankova"
                    },
                    {
                        "authorId": "31198773",
                        "name": "B. Montavon"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Disentangled Representation Unsupervised disentangled representation learning focuses on training generative models [11,24] with different latent dimensions interpreting independent factors of data variations, and most of such models are based on VAE [5,14,21,23,26] and GAN [43,63], enabling many downstream applications [27,31,55]."
            ],
            "citingPaper": {
                "paperId": "92a2c42e9ad5c1bce57efc347f130667e261e7bf",
                "externalIds": {
                    "ArXiv": "2203.15799",
                    "DBLP": "conf/cvpr/LiM0X22",
                    "DOI": "10.1109/CVPR52688.2022.01766",
                    "CorpusId": 247778704
                },
                "corpusId": 247778704,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/92a2c42e9ad5c1bce57efc347f130667e261e7bf",
                "title": "StyleT2I: Toward Compositional and High-Fidelity Text-to-Image Synthesis",
                "abstract": "Although progress has been made for text-to-image synthesis, previous methods fall short of generalizing to unseen or underrepresented attribute compositions in the input text. Lacking compositionality could have severe implications for robustness and fairness, e.g., inability to synthesize the face images of underrepresented demographic groups. In this paper, we introduce a new framework, StyleT2I, to improve the compositionality of text-to-image synthesis. Specifically, we propose a CLIP-guided Contrastive Loss to better distinguish different compositions among different sentences. To further improve the compositionality, we design a novel Semantic Matching Loss and a Spatial Constraint to identify attributes' latent directions for intended spatial region manipulations, leading to better disentangled latent representations of attributes. Based on the identified latent directions of attributes, we propose Compositional Attribute Adjustment to adjust the latent code, resulting in better compositionality of image synthesis. In addition, we leverage the $l_{2}$-norm regularization of identified latent directions (norm penalty) to strike a nice balance between image-text alignment and image fidelity. In the experiments, we devise a new dataset split and an evaluation metric to evaluate the compositionality of text-to-image synthesis models. The results show that StyleT2I outperforms previous approaches in terms of the consistency between the input text and synthesized images and achieves higher fidelity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48458657",
                        "name": "Zhiheng Li"
                    },
                    {
                        "authorId": "5477477",
                        "name": "Martin Renqiang Min"
                    },
                    {
                        "authorId": "94451829",
                        "name": "K. Li"
                    },
                    {
                        "authorId": "2026123",
                        "name": "Chenliang Xu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "StyleEx [30] uses the latent space of a StyleGAN [26] to identify the visual attributes that underlie the classifier\u2019s decision.",
                "In computer vision, several works [5,24,25,30,31,41,46,47] used a generative model to synthesize counterfactual examples."
            ],
            "citingPaper": {
                "paperId": "be9f140bcb8b653d7d204fc4bb57274ea156c523",
                "externalIds": {
                    "DBLP": "conf/eccv/VandenhendeMRG22",
                    "ArXiv": "2203.12892",
                    "DOI": "10.48550/arXiv.2203.12892",
                    "CorpusId": 247628219
                },
                "corpusId": 247628219,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/be9f140bcb8b653d7d204fc4bb57274ea156c523",
                "title": "Making Heads or Tails: Towards Semantically Consistent Visual Counterfactuals",
                "abstract": "A visual counterfactual explanation replaces image regions in a query image with regions from a distractor image such that the system's decision on the transformed image changes to the distractor class. In this work, we present a novel framework for computing visual counterfactual explanations based on two key ideas. First, we enforce that the replaced and replacer regions contain the same semantic part, resulting in more semantically consistent explanations. Second, we use multiple distractor images in a computationally efficient way and obtain more discriminative explanations with fewer region replacements. Our approach is 27 % more semantically consistent and an order of magnitude faster than a competing method on three fine-grained image recognition datasets. We highlight the utility of our counterfactuals over existing works through machine teaching experiments where we teach humans to classify different bird species. We also complement our explanations with the vocabulary of parts and attributes that contributed the most to the system's decision. In this task as well, we obtain state-of-the-art results when using our counterfactual explanations relative to existing works, reinforcing the importance of semantically consistent explanations. Source code is available at https://github.com/facebookresearch/visual-counterfactuals.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "83754395",
                        "name": "Simon Vandenhende"
                    },
                    {
                        "authorId": "144542135",
                        "name": "D. Mahajan"
                    },
                    {
                        "authorId": "2708577",
                        "name": "Filip Radenovic"
                    },
                    {
                        "authorId": "2028234",
                        "name": "Deepti Ghadiyaram"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Further research on this architecture can be found in [Karras et al., 2020] and [Lang et al., 2021]."
            ],
            "citingPaper": {
                "paperId": "7d407c2299fa1beee78fc5c80323c7b866bf2280",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-11132",
                    "ArXiv": "2203.11132",
                    "DOI": "10.48550/arXiv.2203.11132",
                    "CorpusId": 247594759
                },
                "corpusId": 247594759,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7d407c2299fa1beee78fc5c80323c7b866bf2280",
                "title": "Review of Disentanglement Approaches for Medical Applications - Towards Solving the Gordian Knot of Generative Models in Healthcare",
                "abstract": "Deep neural networks are commonly used for medical purposes such as image generation, segmentation, or classification. Besides this, they are often criticized as black boxes as their decision process is often not human interpretable. Encouraging the latent representation of a generative model to be disentangled offers new perspectives of control and interpretability. Understanding the data generation process could help to create artificial medical data sets without violating patient privacy, synthesizing different data modalities, or discovering data generating characteristics. These characteristics might unravel novel relationships that can be related to genetic traits or patient outcomes. In this paper, we give a comprehensive overview of popular generative models, like Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Flow-basedModels. Furthermore, we summarize the different notions of disentanglement, review approaches to disentangle latent space representations and metrics to evaluate the degree of disentanglement. After introducing the theoretical frameworks, we give an overview of recent medical applications and discuss the impact and importance of disentanglement approaches for medical applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2159543555",
                        "name": "Jana Fragemann"
                    },
                    {
                        "authorId": "35875116",
                        "name": "Lynton Ardizzone"
                    },
                    {
                        "authorId": "145629072",
                        "name": "J. Egger"
                    },
                    {
                        "authorId": "2239665",
                        "name": "J. Kleesiek"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[29] present an attractive alternative that generates new realistic examples from a style space learned with a GAN-based approach."
            ],
            "citingPaper": {
                "paperId": "d6c90d8a052354038726c4c05df7d00a5395c504",
                "externalIds": {
                    "DBLP": "journals/natmi/FriedrichSSK23",
                    "ArXiv": "2203.03668",
                    "DOI": "10.1038/s42256-023-00612-w",
                    "CorpusId": 257019571
                },
                "corpusId": 257019571,
                "publicationVenue": {
                    "id": "6457124b-39bf-4d02-bff4-73752ff21562",
                    "name": "Nature Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "Nat Mach Intell"
                    ],
                    "issn": "2522-5839",
                    "url": "https://www.nature.com/natmachintell/"
                },
                "url": "https://www.semanticscholar.org/paper/d6c90d8a052354038726c4c05df7d00a5395c504",
                "title": "A typology for exploring the mitigation of shortcut behaviour",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2055616945",
                        "name": "Felix Friedrich"
                    },
                    {
                        "authorId": "1486503614",
                        "name": "Wolfgang Stammer"
                    },
                    {
                        "authorId": "40896023",
                        "name": "P. Schramowski"
                    },
                    {
                        "authorId": "2066493115",
                        "name": "K. Kersting"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "36714983e9e70eaafe8d0f77d591e39e0c49eb0b",
                "externalIds": {
                    "DBLP": "journals/cgf/BermanoGAMNTPC22",
                    "ArXiv": "2202.14020",
                    "DOI": "10.1111/cgf.14503",
                    "CorpusId": 247158728
                },
                "corpusId": 247158728,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/36714983e9e70eaafe8d0f77d591e39e0c49eb0b",
                "title": "State\u2010of\u2010the\u2010Art in the Architecture, Methods and Applications of StyleGAN",
                "abstract": "Generative Adversarial Networks (GANs) have established themselves as a prevalent approach to image synthesis. Of these, StyleGAN offers a fascinating case study, owing to its remarkable visual quality and an ability to support a large array of downstream tasks. This state\u2010of\u2010the\u2010art report covers the StyleGAN architecture, and the ways it has been employed since its conception, while also analyzing its severe limitations. It aims to be of use for both newcomers, who wish to get a grasp of the field, and for more experienced readers that might benefit from seeing current research trends and existing tools laid out. Among StyleGAN's most interesting aspects is its learned latent space. Despite being learned with no supervision, it is surprisingly well\u2010behaved and remarkably disentangled. Combined with StyleGAN's visual quality, these properties gave rise to unparalleled editing capabilities. However, the control offered by StyleGAN is inherently limited to the generator's learned distribution, and can only be applied to images generated by StyleGAN itself. Seeking to bring StyleGAN's latent control to real\u2010world scenarios, the study of GAN inversion and latent space embedding has quickly gained in popularity. Meanwhile, this same study has helped shed light on the inner workings and limitations of StyleGAN. We map out StyleGAN's impressive story through these investigations, and discuss the details that have made StyleGAN the go\u2010to generator. We further elaborate on the visual priors StyleGAN constructs, and discuss their use in downstream discriminative tasks. Looking forward, we point out StyleGAN's limitations and speculate on current trends and promising directions for future research, such as task and target specific fine\u2010tuning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1755628",
                        "name": "Amit H. Bermano"
                    },
                    {
                        "authorId": "134639223",
                        "name": "Rinon Gal"
                    },
                    {
                        "authorId": "1850630812",
                        "name": "Yuval Alaluf"
                    },
                    {
                        "authorId": "147940380",
                        "name": "Ron Mokady"
                    },
                    {
                        "authorId": "1702992975",
                        "name": "Yotam Nitzan"
                    },
                    {
                        "authorId": "2047833213",
                        "name": "Omer Tov"
                    },
                    {
                        "authorId": "2819477",
                        "name": "Or Patashnik"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "56fd6fdc46e76cae7441e3abc6171d2af3b58e0c",
                "externalIds": {
                    "ArXiv": "2202.12211",
                    "DBLP": "journals/corr/abs-2202-12211",
                    "DOI": "10.1145/3528233.3530708",
                    "CorpusId": 247084382
                },
                "corpusId": 247084382,
                "publicationVenue": {
                    "id": "cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
                    "name": "International Conference on Computer Graphics and Interactive Techniques",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Graph Interact Tech",
                        "SIGGRAPH"
                    ],
                    "url": "http://www.siggraph.org/"
                },
                "url": "https://www.semanticscholar.org/paper/56fd6fdc46e76cae7441e3abc6171d2af3b58e0c",
                "title": "Self-Distilled StyleGAN: Towards Generation from Internet Photos",
                "abstract": "StyleGAN is known to produce high-fidelity images, while also offering unprecedented semantic editing. However, these fascinating abilities have been demonstrated only on a limited set of datasets, which are usually structurally aligned and well curated. In this paper, we show how StyleGAN can be adapted to work on raw uncurated images collected from the Internet. Such image collections impose two main challenges to StyleGAN: they contain many outlier images, and are characterized by a multi-modal distribution. Training StyleGAN on such raw image collections results in degraded image synthesis quality. To meet these challenges, we proposed a StyleGAN-based self-distillation approach, which consists of two main components: (i) A generative-based self-filtering of the dataset to eliminate outlier images, in order to generate an adequate training set, and (ii) Perceptual clustering of the generated images to detect the inherent data modalities, which are then employed to improve StyleGAN\u2019s \u201ctruncation trick\u201d in the image synthesis process. The presented technique enables the generation of high-quality images, while minimizing the loss in diversity of the data. Through qualitative and quantitative evaluation, we demonstrate the power of our approach to new challenging and diverse domains collected from the Internet. New datasets and pre-trained models are provided in our project website https://self-distilled-stylegan.github.io/.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "147940380",
                        "name": "Ron Mokady"
                    },
                    {
                        "authorId": "2065978700",
                        "name": "Michal Yarom"
                    },
                    {
                        "authorId": "2047833213",
                        "name": "Omer Tov"
                    },
                    {
                        "authorId": "49618488",
                        "name": "Oran Lang"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    },
                    {
                        "authorId": "2112779",
                        "name": "Tali Dekel"
                    },
                    {
                        "authorId": "144611617",
                        "name": "M. Irani"
                    },
                    {
                        "authorId": "2138834",
                        "name": "Inbar Mosseri"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "It has recently been shown that some latent subspaces of GANs can be directly used for image local editing without operating the feature maps [27, 29, 42, 50]."
            ],
            "citingPaper": {
                "paperId": "571a237019c3f796eef20a1252e76cf7beea9a9d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-09649",
                    "ArXiv": "2202.09649",
                    "CorpusId": 247011343
                },
                "corpusId": 247011343,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/571a237019c3f796eef20a1252e76cf7beea9a9d",
                "title": "Region-Based Semantic Factorization in GANs",
                "abstract": "Despite the rapid advancement of semantic discovery in the latent space of Generative Adversarial Networks (GANs), existing approaches either are limited to finding global attributes or rely on a number of segmentation masks to identify local attributes. In this work, we present a highly efficient algorithm to factorize the latent semantics learned by GANs concerning an arbitrary image region. Concretely, we revisit the task of local manipulation with pre-trained GANs and formulate region-based semantic discovery as a dual optimization problem. Through an appropriately defined generalized Rayleigh quotient, we manage to solve such a problem without any annotations or training. Experimental results on various state-of-the-art GAN models demonstrate the effectiveness of our approach, as well as its superiority over prior arts regarding precise control, region robustness, speed of implementation, and simplicity of use.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47054925",
                        "name": "Jiapeng Zhu"
                    },
                    {
                        "authorId": "2117687899",
                        "name": "Yujun Shen"
                    },
                    {
                        "authorId": "121983635",
                        "name": "Yinghao Xu"
                    },
                    {
                        "authorId": "1678783",
                        "name": "Deli Zhao"
                    },
                    {
                        "authorId": "143832240",
                        "name": "Qifeng Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "[17] proposes to use attribute-specific classifiers and train a generative model to specifically explain which style channels of StyleGAN2 contribute to the underlying classifier decisions."
            ],
            "citingPaper": {
                "paperId": "6ec7a6c0cd8679e64792f38c61f79a624a1268d4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-06240",
                    "ArXiv": "2202.06240",
                    "DOI": "10.1007/978-3-031-19778-9_33",
                    "CorpusId": 246823309
                },
                "corpusId": 246823309,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/6ec7a6c0cd8679e64792f38c61f79a624a1268d4",
                "title": "FairStyle: Debiasing StyleGAN2 with Style Channel Manipulations",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2154417321",
                        "name": "Cemre Karakas"
                    },
                    {
                        "authorId": "2145260636",
                        "name": "Alara Dirik"
                    },
                    {
                        "authorId": "2134272652",
                        "name": "Eyl\u00fcl Yal\u00e7\u0131nkaya"
                    },
                    {
                        "authorId": "3137679",
                        "name": "Pinar Yanardag"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "To address this difficulty, some recent studies consider to exploit image generation techniques to produce counterfactuals [7, 19, 30].",
                "StylEx [19] proposes to incorporate the classifier into the training process of StyleGAN and learn a classifier-specific StyleSpace."
            ],
            "citingPaper": {
                "paperId": "85b029777a4e87e8bdd9a692be1cb520bd4f3214",
                "externalIds": {
                    "ArXiv": "2201.09689",
                    "DBLP": "journals/corr/abs-2201-09689",
                    "CorpusId": 246240028
                },
                "corpusId": 246240028,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/85b029777a4e87e8bdd9a692be1cb520bd4f3214",
                "title": "Which Style Makes Me Attractive? Interpretable Control Discovery and Counterfactual Explanation on StyleGAN",
                "abstract": "The semantically disentangled latent subspace in GAN provides rich interpretable controls in image generation. This paper includes two contributions on semantic latent subspace analysis in the scenario of face generation using StyleGAN2. First, we propose a novel approach to disentangle latent subspace semantics by exploiting existing face analysis models, e.g., face parsers and face landmark detectors. These models provide the flexibility to construct various criterions with very concrete and interpretable semantic meanings (e.g., change face shape or change skin color) to restrict latent subspace disentanglement. Rich latent space controls unknown previously can be discovered using the constructed criterions. Second, we propose a new perspective to explain the behavior of a CNN classifier by generating counterfactuals in the interpretable latent subspaces we discovered. This explanation helps reveal whether the classifier learns semantics as intended. Experiments on various disentanglement criterions demonstrate the effectiveness of our approach. We believe this approach contributes to both areas of image manipulation and counterfactual explainability of CNNs. The code is available at \\url{https://github.com/prclibo/ice}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48218911",
                        "name": "B. Li"
                    },
                    {
                        "authorId": "1684884626",
                        "name": "Qiuli Wang"
                    },
                    {
                        "authorId": "2143385094",
                        "name": "Jiquan Pei"
                    },
                    {
                        "authorId": "2118809263",
                        "name": "Yu Yang"
                    },
                    {
                        "authorId": "7807689",
                        "name": "Xiangyang Ji"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Thus, prior work has often relied on generative models [2,12,13,33,39,40,47,48,67,70] or computer simulations [59] to manipulate these sensitive attributes and check whether the perturbed instances are classified the same."
            ],
            "citingPaper": {
                "paperId": "1699d664ae8d28d74083ef477f88f8c2d9efef53",
                "externalIds": {
                    "ArXiv": "2111.13650",
                    "DBLP": "conf/eccv/PeychevRBBV22",
                    "DOI": "10.1007/978-3-031-19778-9_31",
                    "CorpusId": 244709148
                },
                "corpusId": 244709148,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/1699d664ae8d28d74083ef477f88f8c2d9efef53",
                "title": "Latent Space Smoothing for Individually Fair Representations",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "30122763",
                        "name": "Momchil Peychev"
                    },
                    {
                        "authorId": "12114187",
                        "name": "A. Ruoss"
                    },
                    {
                        "authorId": "2138580250",
                        "name": "Mislav Balunovi'c"
                    },
                    {
                        "authorId": "1389559318",
                        "name": "Maximilian Baader"
                    },
                    {
                        "authorId": "1736447",
                        "name": "Martin T. Vechev"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "As for PE and StylEx, they perform style-based manipulations that are not well-suited to deal with images that have multiple small independent objects of interest.",
                "Very recently, StylEx [24] trains a variant of StyleGAN2 [22] to obtain a classifier-specific disentangled style space."
            ],
            "citingPaper": {
                "paperId": "b43fff2f88175fb4e22976697614f165ee1acf72",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-09094",
                    "ArXiv": "2111.09094",
                    "DOI": "10.1007/978-3-031-19775-8_23",
                    "CorpusId": 244269857
                },
                "corpusId": 244269857,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/b43fff2f88175fb4e22976697614f165ee1acf72",
                "title": "STEEX: Steering Counterfactual Explanations with Semantics",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145448899",
                        "name": "P. Jacob"
                    },
                    {
                        "authorId": "39541096",
                        "name": "\u00c9loi Zablocki"
                    },
                    {
                        "authorId": "1405301761",
                        "name": "H. Ben-younes"
                    },
                    {
                        "authorId": "8408521",
                        "name": "Micka\u00ebl Chen"
                    },
                    {
                        "authorId": "2066449716",
                        "name": "P. P'erez"
                    },
                    {
                        "authorId": "51021910",
                        "name": "M. Cord"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Methods of this stream (Shen et al. 2020; H\u00e4rk\u00f6nen et al. 2020; Shen and Zhou 2020; Hou et al. 2020; Tewari et al. 2020; Abdal et al. 2020; Wang, Yu, and Fritz 2021; Xia et al. 2021; Roich et al. 2021; Alaluf, Patashnik, and CohenOr 2021b; Ren et al. 2021; Lang et al. 2021; Wu, Lischinski, and Shechtman 2021; Patashnik et al. 2021) attempt to achieve controlled image synthesis by exploring the semantics in the latent space of well-trained GANs."
            ],
            "citingPaper": {
                "paperId": "764a7148ed00a5e202e20d7bebcefbfbd31fb0b7",
                "externalIds": {
                    "ArXiv": "2109.10737",
                    "DBLP": "conf/wacv/LiCLZHHY23",
                    "DOI": "10.1109/WACV56688.2023.00027",
                    "CorpusId": 237592637
                },
                "corpusId": 237592637,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/764a7148ed00a5e202e20d7bebcefbfbd31fb0b7",
                "title": "DyStyle: Dynamic Neural Network for Multi-Attribute-Conditioned Style Editings",
                "abstract": "The semantic controllability of StyleGAN is enhanced by unremitting research. Although the existing weak supervision methods work well in manipulating the style codes along one attribute, the accuracy of manipulating multiple attributes is neglected. Multi-attribute representations are prone to entanglement in the StyleGAN latent space, while sequential editing leads to error accumulation. To address these limitations, we design a Dynamic Style Manipulation Network (DyStyle) whose structure and parameters vary by input samples, to perform nonlinear and adaptive manipulation of latent codes for flexible and precise attribute control. In order to efficient and stable optimization of the DyStyle network, we propose a Dynamic Multi-Attribute Contrastive Learning (DmaCL) method: including dynamic multi-attribute contrastor and dynamic multi-attribute contrastive loss, which simultaneously disentangle a variety of attributes from the generative image and latent space of model. As a result, our approach demonstrates fine-grained disentangled edits along multiple numeric and binary attributes. Qualitative and quantitative comparisons with existing style manipulation methods verify the superiority of our method in terms of the multi-attribute control accuracy and identity preservation without compromising photorealism.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145726957",
                        "name": "Bingchuan Li"
                    },
                    {
                        "authorId": "1993661033",
                        "name": "Shaofei Cai"
                    },
                    {
                        "authorId": null,
                        "name": "Wei Liu"
                    },
                    {
                        "authorId": "2151331126",
                        "name": "Peng Zhang"
                    },
                    {
                        "authorId": "2128319758",
                        "name": "Miao Hua"
                    },
                    {
                        "authorId": "2152880412",
                        "name": "Qian He"
                    },
                    {
                        "authorId": "39737792",
                        "name": "Zili Yi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "As inversion is significantly more difficult for multiclass GANs [5, 9, 32] such as BigGAN [6], we follow prior work [9,29,65] and focus our attention on the state-of-the-art single-object GAN - StyleGAN.",
                "[29] used StyleGAN to visualize counterfactual examples for explaining a pretrained classifier\u2019s predictions."
            ],
            "citingPaper": {
                "paperId": "f312db26053b39c90d5220a669d15ccea0635363",
                "externalIds": {
                    "DBLP": "conf/cvpr/NitzanGBC22",
                    "ArXiv": "2107.11186",
                    "DOI": "10.1109/CVPR52688.2022.01864",
                    "CorpusId": 236318260
                },
                "corpusId": 236318260,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f312db26053b39c90d5220a669d15ccea0635363",
                "title": "LARGE: Latent-Based Regression through GAN Semantics",
                "abstract": "We propose a novel method for solving regression tasks using few-shot or weak supervision. At the core of our method is the fundamental observation that GANs are incredibly successful at encoding semantic information within their latent space, even in a completely unsupervised setting. For modern generative frameworks, this semantic encoding manifests as smooth, linear directions which affect image attributes in a disentangled manner. These directions have been widely used in GAN-based image editing. In this work, we leverage them for few-shot regression. Specifically, we make the simple observation that distances traversed along such directions are good features for downstream tasks - reliably gauging the magnitude of a property in an image. In the absence of explicit supervision, we use these distances to solve tasks such as sorting a collection of images, and ordinal regression. With a few labels - as little as two - we calibrate these distances to real-world values and convert a pre-trained GAN into a state-of-the-art few-shot regression model. This enables solving regression tasks on datasets and attributes which are difficult to produce quality supervision for. Extensive experimental evaluations demonstrate that our method can be applied across a wide range of domains, leverage multiple latent direction discovery frame-works, and achieve state-of-the-art results in few-shot and low-supervision settings, even when compared to methods designed to tackle a single task. Code is available on our project website.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1702992975",
                        "name": "Yotam Nitzan"
                    },
                    {
                        "authorId": "134639223",
                        "name": "Rinon Gal"
                    },
                    {
                        "authorId": "2120835121",
                        "name": "Ofir Brenner"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "We note that relying on generative models has recently gained traction for interpretability and score attribution purposes [Lang et al., 2021].",
                "Generative adversarial networks have gained relevance as a means to facilitate interpretability in classification tasks [Lang et al., 2021], however, training can be unstable and identifying counterfactual references is infeasible."
            ],
            "citingPaper": {
                "paperId": "77d3c384b2fe805fa31adbe060ac27c014fd012b",
                "externalIds": {
                    "ArXiv": "2107.08756",
                    "DBLP": "conf/uai/PerezSBWS22",
                    "CorpusId": 249494473
                },
                "corpusId": 249494473,
                "publicationVenue": {
                    "id": "f9af8000-42f8-410d-a622-e8811e41660a",
                    "name": "Conference on Uncertainty in Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Uncertainty in Artificial Intelligence",
                        "UAI",
                        "Conf Uncertain Artif Intell",
                        "Uncertain Artif Intell"
                    ],
                    "url": "http://www.auai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/77d3c384b2fe805fa31adbe060ac27c014fd012b",
                "title": "Attribution of predictive uncertainties in classification models",
                "abstract": "Predictive uncertainties in classi\ufb01cation tasks are often a consequence of model inadequacy or in-suf\ufb01cient training data. In popular applications, such as image processing, we are often required to scrutinise these uncertainties by meaningfully attributing them to input features. This helps to improve interpretability assessments. However, there exist few effective frameworks for this purpose. Vanilla forms of popular methods for the provision of saliency masks, such as SHAP or integrated gradients, adapt poorly to target measures of uncertainty. Thus, state-of-the-art tools instead proceed by creating counterfactual or adversarial feature vectors, and assign attributions by direct comparison to original images. In this paper, we present a novel framework that combines path integrals, counterfactual explanations and generative models, in order to procure attributions that contain few observable artefacts or noise. We evidence that this outperforms existing alternatives through quantitative evaluations with popular benchmarking methods and data sets of varying complexity.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143825001",
                        "name": "Iker Perez"
                    },
                    {
                        "authorId": "1995058626",
                        "name": "Piotr Skalski"
                    },
                    {
                        "authorId": "1422541656",
                        "name": "Alec E. Barns-Graham"
                    },
                    {
                        "authorId": "2113386549",
                        "name": "Jason Wong"
                    },
                    {
                        "authorId": "145416008",
                        "name": "D. Sutton"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "93a55b04045ff7fff78de473f5ff52cbcfb9a948",
                "externalIds": {
                    "DBLP": "journals/datamine/BodriaGGNPR23",
                    "ArXiv": "2102.13076",
                    "DOI": "10.1007/s10618-023-00933-9",
                    "CorpusId": 232046272
                },
                "corpusId": 232046272,
                "publicationVenue": {
                    "id": "d263025a-9eaf-443f-9bbf-72377e8d22a6",
                    "name": "Data mining and knowledge discovery",
                    "type": "journal",
                    "alternate_names": [
                        "Data Mining and Knowledge Discovery",
                        "Data Min Knowl Discov",
                        "Data min knowl discov"
                    ],
                    "issn": "1384-5810",
                    "url": "https://www.springer.com/computer/database+management+&+information+retrieval/journal/10618",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10618",
                        "http://www.springer.com/computer/database+management+&+information+retrieval/journal/10618"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/93a55b04045ff7fff78de473f5ff52cbcfb9a948",
                "title": "Benchmarking and survey of explanation methods for black box models",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1885229792",
                        "name": "F. Bodria"
                    },
                    {
                        "authorId": "1685102",
                        "name": "F. Giannotti"
                    },
                    {
                        "authorId": "1704327",
                        "name": "Riccardo Guidotti"
                    },
                    {
                        "authorId": "1998031158",
                        "name": "Francesca Naretto"
                    },
                    {
                        "authorId": "1693341",
                        "name": "D. Pedreschi"
                    },
                    {
                        "authorId": "2120595",
                        "name": "S. Rinzivillo"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Although some methods [31], [32], [33], [96] use additive",
                "Some methods [31], [32], [33], [34] use additional encoder networks to learn the inverse mapping of GANs, but their goals are to jointly train Fig."
            ],
            "citingPaper": {
                "paperId": "a3ef5a321876738a6b257de5e1eebc4a8aa5b907",
                "externalIds": {
                    "DBLP": "journals/pami/XiaZYXZY23",
                    "ArXiv": "2101.05278",
                    "DOI": "10.1109/TPAMI.2022.3181070",
                    "CorpusId": 231603119,
                    "PubMed": "37022469"
                },
                "corpusId": 231603119,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a3ef5a321876738a6b257de5e1eebc4a8aa5b907",
                "title": "GAN Inversion: A Survey",
                "abstract": "GAN inversion aims to invert a given image back into the latent space of a pretrained GAN model so that the image can be faithfully reconstructed from the inverted code by the generator. As an emerging technique to bridge the real and fake image domains, GAN inversion plays an essential role in enabling pretrained GAN models, such as StyleGAN and BigGAN, for applications of real image editing. Moreover, GAN inversion interprets GAN's latent space and examines how realistic images can be generated. In this paper, we provide a survey of GAN inversion with a focus on its representative algorithms and its applications in image restoration and image manipulation. We further discuss the trends and challenges for future research. A curated list of GAN inversion methods, datasets, and other related information can be found at https://github.com/weihaox/awesome-gan-inversion.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50875615",
                        "name": "Weihao Xia"
                    },
                    {
                        "authorId": "2129519081",
                        "name": "Yulun Zhang"
                    },
                    {
                        "authorId": "3001727",
                        "name": "Yujiu Yang"
                    },
                    {
                        "authorId": "1891766",
                        "name": "Jing-Hao Xue"
                    },
                    {
                        "authorId": "145291669",
                        "name": "Bolei Zhou"
                    },
                    {
                        "authorId": "1715634",
                        "name": "Ming-Hsuan Yang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "\u2026the dimensions with small changes in e with respect to z are discarded through a feature selection process, maximizing the sparsity of e\u2212 z. StylEx (Lang et al., 2021)5: They find a latent perturbation in a direction that maximizes the difference in the output of the classifier for the original\u2026",
                "For a detailed description of the role of these hyper-parameters see (Lang et al., 2021).",
                "This last parameter is not mentioned in the Stylex (Lang et al., 2021) paper, but it can found as a parameter under the name shift_size in the implementation they provide.",
                "StylEx (Lang et al., 2021)5: They find a latent perturbation in a direction that maximizes the difference in the output of the classifier for the original sample and its perturbed counterpart.",
                "Table 3 shows that DiCE (Mothilal et al., 2020) and StylEx (Lang et al., 2021) produce a high amount of these counterfactuals, while GS (Laugel et al., 2017) and Latent-CF (Balasubramanian et al., 2020) always change the classifiers prediction and thus produce none.",
                ", 2017) \u2717 \u2717 \u2713 StylEx (Lang et al., 2021) \u2717 \u2717 \u2713 Latent-CF (Balasubramanian et al.",
                ", 2020) and StylEx (Lang et al., 2021) produce a high amount of these counterfactuals, while GS (Laugel et al.",
                "StylEx (Lang et al., 2021) For this method we found the best configuration was setting threshold t to 0.3 using the \u201cIndependent\u201d selection strategy and the amount of shift applied to each coordinate to 0.8.",
                "Many explainability methods in the literature are designed for the image domain (Rodr\u00edguez et al., 2021; Joshi et al., 2018; Lang et al., 2021; Singla et al., 2019; Chang et al., 2018)."
            ],
            "citingPaper": {
                "paperId": "cb210e14dff0351c35d58a97cf43703d99becc37",
                "externalIds": {
                    "CorpusId": 259372993
                },
                "corpusId": 259372993,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cb210e14dff0351c35d58a97cf43703d99becc37",
                "title": "Evaluating Visual Counterfactual Explainers",
                "abstract": "Explainability methods have been widely used to provide insight into the decisions made by statistical models, thus facilitating their adoption in various domains within the industry. Counterfactual explanation methods aim to improve our understanding of a model by perturbing samples in a way that would alter its response in an unexpected manner. This information is helpful for users and for machine learning practitioners to understand and improve their models. Given the value provided by counterfactual explanations, there is a growing interest in the research community to investigate and propose new methods. However, we identify two issues that could hinder the progress in this field. (1) Existing metrics do not accurately reflect the value of an explainability method for the users. (2) Comparisons between methods are usually performed with datasets like CelebA, where images are annotated with attributes that do not fully describe them and with subjective attributes such as \u201cAttractive\u201d. In this work, we address these problems by proposing an evaluation method with a principled metric to evaluate and compare different counterfactual explanation methods. The evaluation is based on a synthetic dataset where images are fully described by their annotated attributes. As a result, we are able to perform a fair comparison of multiple explainability methods in the recent literature, obtaining insights about their performance. We make the code 1 and data public to the research community.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1820967551",
                        "name": "D. Velazquez"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "c1553e864ec182d96c3d1c2fe7a99f4a7c589039",
                "externalIds": {
                    "CorpusId": 259318955
                },
                "corpusId": 259318955,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c1553e864ec182d96c3d1c2fe7a99f4a7c589039",
                "title": "Modular Control and Services to Operate Lineless Mobile Assembly Systems",
                "abstract": "The increasing product variability and lack of skilled workers demand for autonomous, flexible production. Since assembly is considered a main cost driver and accounts for a major part of production time, research focuses on new technologies in assembly. The paradigm of Line-less Mobile Assembly Systems (LMAS) provides a solution for the future of assembly by mobilizing all resources. Thus, dynamic product routes through spatiotemporally configured assembly stations on a shop floor free of fixed obstacles are enabled. In this chapter, we present research focal points on different levels of LMAS, starting with the macroscopic level of formation planning, followed by the mesoscopic level of mobile robot control and multipurpose input devices and the microscopic level of services, such as interpreting autonomous decisions and in-network computing. We provide cross-level data and knowledge transfer through a novel ontology-based knowledge management. Overall, our work contributes to future safe and predictable human-robot collaboration in dynamic LMAS stations based on accurate online formation and motion planning of mobile robots, novel human-machine interfaces and networking technologies, as well as trustworthy",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1405049044",
                        "name": "A. Kluge-Wilkes"
                    },
                    {
                        "authorId": "48861700",
                        "name": "Ralph Baier"
                    },
                    {
                        "authorId": "20543442",
                        "name": "Ike Kunze"
                    },
                    {
                        "authorId": "2120175832",
                        "name": "Aleksandra M\u00fcller"
                    },
                    {
                        "authorId": "2000456366",
                        "name": "Amir Shahidi"
                    },
                    {
                        "authorId": "103763981",
                        "name": "Dominik Wolfschl\u00e4ger"
                    },
                    {
                        "authorId": "1735841",
                        "name": "C. Brecher"
                    },
                    {
                        "authorId": "2754908",
                        "name": "B. Corves"
                    },
                    {
                        "authorId": "2586116",
                        "name": "M. H\u00fcsing"
                    },
                    {
                        "authorId": "3070800",
                        "name": "V. Nitsch"
                    },
                    {
                        "authorId": "46253301",
                        "name": "R. Schmitt"
                    },
                    {
                        "authorId": "1719689",
                        "name": "Klaus Wehrle"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "StyleEx [31] uses the latent space of a StyleGAN [27] to identify the visual attributes that underlie the classifier\u2019s decision.",
                "In computer vision, several works [5,25,26,31,32,43,48,49] used a generative model to synthesize counterfactual examples."
            ],
            "citingPaper": {
                "paperId": "6b973f205aa4aaedf79d64117bf22276a710a11a",
                "externalIds": {
                    "CorpusId": 259377977
                },
                "corpusId": 259377977,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6b973f205aa4aaedf79d64117bf22276a710a11a",
                "title": "Consistent Visual Counterfactuals",
                "abstract": ". A visual counterfactual explanation replaces image regions in a query image with regions from a distractor image such that the system\u2019s decision on the transformed image changes to the distractor class. In this work, we present a novel framework for computing visual counterfactual explanations based on two key ideas. First, we enforce that the replaced and replacer regions contain the same semantic part, resulting in more semantically consistent explanations. Second, we use multiple distractor images in a computationally efficient way and obtain more discriminative explanations with fewer region replacements. Our approach is 27 % more semantically consistent and an order of magnitude faster than a competing method on three fine-grained image recognition datasets. We highlight the utility of our counterfactuals over existing works through machine teaching experiments where we teach humans to classify different bird species. We also complement our explanations with the vocabulary of parts and attributes that contributed the most to the system\u2019s decision. In this task as well, we obtain state-of-the-art results when using our counterfactual explanations relative to existing works, re-inforcing the importance of semantically consistent explanations. Source code is available at github.com/facebookresearch/visual-counterfactuals.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2221325064",
                        "name": "Simon Vandenhende Dhruv Mahajan"
                    },
                    {
                        "authorId": "2708577",
                        "name": "Filip Radenovic"
                    },
                    {
                        "authorId": "2028234",
                        "name": "Deepti Ghadiyaram"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "This is why, for both faces models, a ResNet classifier was used to perform the AttFind algorithm.96\nThis expanded latent vector w, either obtained by the encoder or StyleVectorizer, is passed on to the StyleGAN2, where97 it is transformed into the StyleSpace by a set of concurrent affine transformations to style vectors s0, ..., sn.",
                "40 In this work, we reproduce the paper \u2018Explaining in Style: Explaining a GAN in StyleSpace\u2019 [8].",
                "[Re] Explaining in Style: Training a GAN to explain a classifier in StyleSpace\nAnonymous Author(s) Affiliation Address email\nReproducibility Summary1\nScope of Reproducibility2\nStylEx is an approach for classifier-conditioned training of a StyleGAN2 [6], intending to capture classifier-specific3 attributes in its disentangled StyleSpace [15].",
                "Secondary objectives involve the39 visualization and control of the impact of these features on the classifier output.40\nIn this work, we reproduce the paper \u2018Explaining in Style: Explaining a GAN in StyleSpace\u2019 [8].",
                "A recent observation by [14] highlighted the disentanglement of this space (appropriately called, the86 StyleSpace) that is used to extract classifier-specific attributes.",
                "\u2026seem to slightly outperform the results by Wu et al. (2021) on the perceived age classifier, it does180 not seem to outperform the method posed by Lang et al. (2021).181\n4.2 Results beyond original paper182\nTo investigate the impact of attribute perturbation on the quality of the183 generated\u2026"
            ],
            "citingPaper": {
                "paperId": "2009ed2e28d8255c2a788be585b8fe4d809cffc3",
                "externalIds": {
                    "CorpusId": 247741149
                },
                "corpusId": 247741149,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2009ed2e28d8255c2a788be585b8fe4d809cffc3",
                "title": "[Re] Explaining in Style: Training a GAN to explain a classifier in StyleSpace",
                "abstract": "StylEx is an approach for classifier-conditioned training of a StyleGAN2 [6], intending to capture classifier-specific 3 attributes in its disentangled StyleSpace [15]. Attributes can be adjusted to generate counterfactual explanations of 4 the classifier decisions. StylEx is domain and classifier-agnostic, while its explanations are claimed to be human5 interpretable, distinct, coherent and sufficient to produce flipped classifier decisions. We verify these claims by 6 reproducing a selection of the experiments in the paper. 7",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1955694",
                        "name": "Sherjil Ozair"
                    },
                    {
                        "authorId": "2741985",
                        "name": "Dmitry Kalenichenko"
                    },
                    {
                        "authorId": "2161320002",
                        "name": "Weijun Wang"
                    },
                    {
                        "authorId": "47447630",
                        "name": "Tobias Weyand"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Instead of using noise to augment an example, Lang et al. (2021) present an attractive alternative that generates new realistic examples from a style space learned with a GAN-based approach."
            ],
            "citingPaper": {
                "paperId": "110730b11d578e7c70711949b0fae583c6cf1904",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-03668",
                    "DOI": "10.48550/arXiv.2203.03668",
                    "CorpusId": 247315329
                },
                "corpusId": 247315329,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/110730b11d578e7c70711949b0fae583c6cf1904",
                "title": "A Typology to Explore and Guide Explanatory Interactive Machine Learning",
                "abstract": "Recently, more and more eXplanatory Interactive machine Learning (XIL) methods have been proposed with the goal of extending a model\u2019s learning process by integrating human user supervision on the model\u2019s explanations. These methods were often developed independently, provide different motivations and stem from different applications. Notably, up to now, there has not been a comprehensive evaluation of these works. By identifying a common set of basic modules and providing a thorough discussion of these modules, our work, for the first time, comes up with a unification of the various methods into a single typology. This typology can thus be used to categorize existing and future XIL methods based on the identified modules. Moreover, our work contributes by surveying six existing XIL methods. In addition to benchmarking these methods on their overall ability to revise a model, we perform additional benchmarks regarding wrong reason revision, interaction efficiency, robustness to feedback quality, and the ability to revise a strongly corrupted model. Apart from introducing these novel benchmarking tasks, for improved quantitative evaluations, we further introduce a novel Wrong Reason (wr) metric which measures the average wrong reason activation in a model\u2019s explanations to complement a qualitative inspection. In our evaluations, all methods prove to revise a model successfully. However, we found significant differences between the methods on individual benchmark tasks, revealing valuable application-relevant aspects not only for comparing current methods but also to motivate the necessity of incorporating these benchmarks in the development of future XIL methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2055616945",
                        "name": "Felix Friedrich"
                    },
                    {
                        "authorId": "1486503614",
                        "name": "Wolfgang Stammer"
                    },
                    {
                        "authorId": "40896023",
                        "name": "P. Schramowski"
                    },
                    {
                        "authorId": "2066493115",
                        "name": "K. Kersting"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u20262014, Bach et al., 2015, Selvaraju et al., 2017], counterfactual explanations [Chang et al., 2019, Antoran et al., 2021], explanations based on pre-defined concepts [Rezende et al., 2014, Kazhdan et al., 2020, Yeh et al., 2020], and recently developed StyleGANs [Wu et al., 2021, Lang et al., 2021]."
            ],
            "citingPaper": {
                "paperId": "8b561ecc498a088d68f2a841b91ac2572424a600",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-00492",
                    "DOI": "10.48550/arXiv.2204.00492",
                    "CorpusId": 247922743
                },
                "corpusId": 247922743,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8b561ecc498a088d68f2a841b91ac2572424a600",
                "title": "Provable concept learning for interpretable predictions using variational inference",
                "abstract": "In safety critical applications, practitioners are reluctant to trust neural networks when no interpretable explanations are available. Many attempts to provide such explanations revolve around pixel level attributions or use previously known concepts. In this paper we aim to provide explanations by provably identifying high-level, previously unknown concepts . To this end, we propose a probabilistic modeling framework to derive (C)oncept (L)earning and (P)rediction (CLAP) \u2013 a VAE-based classi\ufb01er that uses visually interpretable concepts as linear predictors. Assuming that the data generating mechanism involves predictive concepts, we prove that our method is able to identify them while attaining optimal classi\ufb01cation accuracy. We use synthetic experiments for validation, and also show that on real-world (PlantVillage and ChestXRay) datasets, CLAP e\ufb00ectively discovers interpretable factors for classifying diseases. S (1) . Our objective is to show that any optimal encoder produces core features that are",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2106880",
                        "name": "Armeen Taeb"
                    },
                    {
                        "authorId": "151453356",
                        "name": "Nicol\u00f2 Ruggeri"
                    },
                    {
                        "authorId": "2161242464",
                        "name": "Carina Schnuck"
                    },
                    {
                        "authorId": "2151252957",
                        "name": "Fanny Yang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "These works mostly use a set of human-specified concepts to analyze model behavior, however, there is an increasing interest in automatically discovering the concepts that are used by a model (Yeh et al., 2020; Ghorbani et al., 2019; Lang et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "df2c2752a77c1d0c9d052c9b11d8c87e01525cd1",
                "externalIds": {
                    "CorpusId": 248913170
                },
                "corpusId": 248913170,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/df2c2752a77c1d0c9d052c9b11d8c87e01525cd1",
                "title": "P OST - HOC C ONCEPT B OTTLENECK M ODELS",
                "abstract": "Concept Bottleneck Models (CBMs) map the inputs onto a concept bottleneck and use the bottleneck to make a prediction. A concept bottleneck enhances interpretability since it can be investigated to understand what concepts the model \u201dsees\u201d in an input and which of these concepts are deemed important. However, CBMs are restrictive in practice as they require concept labels during training to learn the bottleneck. Additionally, it is questionable if CBMs can match the accuracy of an unrestricted neural network trained on a given domain, potentially reducing the incentive to deploy them in practice. In this work, we address these two key limitations by introducing Post-hoc Concept Bottleneck models (P-CBMs). We show that we can turn any neural network into a P-CBM without sacrificing model performance while still retaining interpretability benefits. Finally, we show that editing P-CBMs without any fine-tuning or use of data from the target domain can provide significant performance gains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2027032530",
                        "name": "Maggie Wang"
                    },
                    {
                        "authorId": "145085305",
                        "name": "James Y. Zou"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "be6516dda2fd5ca59c0a182e49dc95303a8dd2e0",
                "externalIds": {
                    "DBLP": "conf/ijcai/Le-CozHA22",
                    "CorpusId": 252760530
                },
                "corpusId": 252760530,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/be6516dda2fd5ca59c0a182e49dc95303a8dd2e0",
                "title": "Leveraging generative models to characterize the failure conditions of image classifiers",
                "abstract": "We address in this work the question of identifying the failure conditions of a given image classifier. To do so, we exploit the capacity of producing controllable distributions of high quality image data made available by recent Generative Adversarial Networks (StyleGAN2): the failure conditions are expressed as directions of strong performance degradation in the generative model latent space. This strategy of analysis is used to discover corner cases that combine multiple sources of corruption, and to compare in more details the behavior of different classifiers. The directions of degradation can also be rendered visually by generating data for better interpretability. Some degradations such as image quality can affect all classes, whereas other ones such as shape are more class-specific. The approach is demonstrated on the MNIST dataset that has been completed by two sources of corruption: noise and blur, and shows a promising way to better understand and control the risks of exploiting Artificial Intelligence components for safety-critical applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2039553828",
                        "name": "Adrien Le Coz"
                    },
                    {
                        "authorId": "1924996",
                        "name": "S. Herbin"
                    },
                    {
                        "authorId": "7167973",
                        "name": "F. Adjed"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[29] present an attractive alternative that generates new realistic examples from a style space learned with a GAN-based approach."
            ],
            "citingPaper": {
                "paperId": "45edaa7e3ff7c46d5a5f0cfcbad2532426c5a8b3",
                "externalIds": {
                    "CorpusId": 253244644
                },
                "corpusId": 253244644,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/45edaa7e3ff7c46d5a5f0cfcbad2532426c5a8b3",
                "title": "A Typology to Explore the Mitigation of Shortcut Behavior",
                "abstract": "As machine learning models become increasingly larger, trained weakly supervised on large, possibly uncurated data sets, it becomes increasingly important to establish mechanisms for inspecting, interacting, and revising models to mitigate learning shortcuts and guarantee their learned knowledge is aligned with human knowledge. The recently proposed XIL framework was developed for this purpose, and several such methods have been introduced, each with individual motivations and methodological details. In this work, we provide a uni\ufb01cation of various XIL methods into a single typology by establishing a common set of basic modules. In doing so, we pave the way for a principled comparison of existing, but, importantly, also future XIL approaches. In addition, we discuss existing and introduce novel measures and benchmarks for evaluating the overall abilities of a XIL method. Given this extensive toolbox, including our typology, measures, and benchmarks, we \ufb01nally compare several recent XIL methods methodologically and quantitatively. In our evaluations, all methods prove to revise a model successfully. However, we found remarkable di\ufb00erences in individual benchmark tasks, revealing valuable application-relevant aspects for integrating these benchmarks in developing future methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2055616945",
                        "name": "Felix Friedrich"
                    },
                    {
                        "authorId": "1486503614",
                        "name": "Wolfgang Stammer"
                    },
                    {
                        "authorId": "40896023",
                        "name": "P. Schramowski"
                    },
                    {
                        "authorId": "2066493115",
                        "name": "K. Kersting"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "229bcba7473a82a23744884dc3c5136b873e7b39",
                "externalIds": {
                    "CorpusId": 253525601
                },
                "corpusId": 253525601,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/229bcba7473a82a23744884dc3c5136b873e7b39",
                "title": "A Binary Classifier Architecture",
                "abstract": "For all experiments, we train and use binary classifiers with the following architecture: We implement feedforward neural networks with 10 main layers, where each main layer consists of a convolution layer followed by batch normalization and ReLU activation. Additionally, we add pooling and dropout layers between main layers in an alternating fashion such that there are a total of 5 pooling layers and 3 dropout layers, where the dropout percentage is set to 0.5.",
                "year": 2022,
                "authors": []
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5baa40361819980e9d7d307dd36438b198bc5268",
                "externalIds": {
                    "CorpusId": 253968521
                },
                "corpusId": 253968521,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5baa40361819980e9d7d307dd36438b198bc5268",
                "title": "Learning from uncertain concepts via test time interventions",
                "abstract": "With neural networks applied to safety-critical applications, it has become increas-ingly important to understand the defining features of decision-making. Therefore, the need to uncover the black boxes to rational representational space of these neural networks is apparent. Concept bottleneck model (CBM) encourages interpretability by predicting human-understandable concepts. They predict concepts from input images and then labels from concepts. Test time intervention, a salient feature of CBM, allows for human-model interactions. However, these interactions are prone to information leakage and can often be ineffective inappropriate com-munication with humans. We propose a novel uncertainty based strategy, SIUL: Single Interventional Uncertainty Learning to select the interventions. Additionally, we empirically test the robustness of CBM and the effect of SIUL interventions under adversarial attack and distributional shift. Using SIUL, we observe that the interventions suggested lead to meaningful corrections along with mitigation of concept leakage. Extensive experiments on three vision datasets along with a histopathology dataset validate the effectiveness of our interventional learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2086069040",
                        "name": "Ivaxi Sheth"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "A recent and promising direction is to explain model predictions using counterfactuals learned by generative models [Wachter et al., 2017, Singla et al., 2019, 2021, Lang et al., 2021]."
            ],
            "citingPaper": {
                "paperId": "99d67a1c769173b99186229566f7de33a39b643e",
                "externalIds": {
                    "CorpusId": 254119254
                },
                "corpusId": 254119254,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/99d67a1c769173b99186229566f7de33a39b643e",
                "title": "Real world relevance of generative counterfactual explanations",
                "abstract": ",",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2716670",
                        "name": "S. Sankaranarayanan"
                    },
                    {
                        "authorId": "1452598350",
                        "name": "Tom Hartvigsen"
                    },
                    {
                        "authorId": "2159678183",
                        "name": "Lauren Oakden-Rayner"
                    },
                    {
                        "authorId": "2804918",
                        "name": "M. Ghassemi"
                    },
                    {
                        "authorId": "2094770",
                        "name": "Phillip Isola"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "de6ff259714821f8f2fc733b6821d78c8b012067",
                "externalIds": {
                    "CorpusId": 249335393
                },
                "corpusId": 249335393,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/de6ff259714821f8f2fc733b6821d78c8b012067",
                "title": "Defuse: Training More Robust Models through Creation and Correction of Novel Model Errors",
                "abstract": "We typically compute aggregate statistics on held-out test data to assess the generalization of machine learning models. However, test data is only so comprehensive, and in practice, important cases are often missed. Thus, the performance of de-ployed machine learning models can be variable and untrustworthy. Motivated by these concerns, we develop methods to generate and correct novel model errors beyond those available in the data. We propose Defuse: a technique that trains a generative model on a classi\ufb01er\u2019s training dataset and then uses the latent space to generate new samples which are no longer correctly predicted by the classi\ufb01er. For instance, given a classi\ufb01er trained on the MNIST dataset that correctly predicts a test image, Defuse then uses this image to generate new similar images by sampling from the latent space. Defuse then identi\ufb01es the images that differ from the label of the original test input. Defuse enables ef\ufb01cient labeling of these new images, allowing users to re-train a more robust model, thus improving overall model performance. We evaluate the performance of Defuse on classi\ufb01ers trained on real world datasets and \ufb01nd it reveals novel sources of model errors.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "153794305",
                        "name": "Dylan Slack"
                    }
                ]
            }
        }
    ]
}