{
    "offset": 0,
    "data": [
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In Crabb\u00e9 & van der Schaar (2022), the feature importance for unlabelled data is proposed, using XAI methods at the initial phase of data prepossessing to distinguish features that may play a key role in further ML tasks."
            ],
            "citingPaper": {
                "paperId": "a0cb1fc45c4ec22599681858dad08d47ca6e4e36",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-05120",
                    "ArXiv": "2306.05120",
                    "DOI": "10.48550/arXiv.2306.05120",
                    "CorpusId": 259108874
                },
                "corpusId": 259108874,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a0cb1fc45c4ec22599681858dad08d47ca6e4e36",
                "title": "Explainable Predictive Maintenance",
                "abstract": "Explainable Artificial Intelligence (XAI) fills the role of a critical interface fostering interactions between sophisticated intelligent systems and diverse individuals, including data scientists, domain experts, end-users, and more. It aids in deciphering the intricate internal mechanisms of ``black box'' Machine Learning (ML), rendering the reasons behind their decisions more understandable. However, current research in XAI primarily focuses on two aspects; ways to facilitate user trust, or to debug and refine the ML model. The majority of it falls short of recognising the diverse types of explanations needed in broader contexts, as different users and varied application areas necessitate solutions tailored to their specific needs. One such domain is Predictive Maintenance (PdM), an exploding area of research under the Industry 4.0 \\&5.0 umbrella. This position paper highlights the gap between existing XAI methodologies and the specific requirements for explanations within industrial applications, particularly the Predictive Maintenance field. Despite explainability's crucial role, this subject remains a relatively under-explored area, making this paper a pioneering attempt to bring relevant challenges to the research community's attention. We provide an overview of predictive maintenance tasks and accentuate the need and varying purposes for corresponding explanations. We then list and describe XAI techniques commonly employed in the literature, discussing their suitability for PdM tasks. Finally, to make the ideas and claims more concrete, we demonstrate XAI applied in four specific industrial use cases: commercial vehicles, metro trains, steel plants, and wind farms, spotlighting areas requiring further research.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2035603",
                        "name": "Sepideh Pashami"
                    },
                    {
                        "authorId": "3109166",
                        "name": "S\u0142awomir Nowaczyk"
                    },
                    {
                        "authorId": "3011381",
                        "name": "Yuantao Fan"
                    },
                    {
                        "authorId": "144898884",
                        "name": "J. Jakubowski"
                    },
                    {
                        "authorId": "37554023",
                        "name": "Nuno Paiva"
                    },
                    {
                        "authorId": "31019504",
                        "name": "Narjes Davari"
                    },
                    {
                        "authorId": "2019847",
                        "name": "Szymon Bobek"
                    },
                    {
                        "authorId": "33407631",
                        "name": "Samaneh Jamshidi"
                    },
                    {
                        "authorId": "2136831566",
                        "name": "Hamid Sarmadi"
                    },
                    {
                        "authorId": "1572808795",
                        "name": "Abdallah Alabdallah"
                    },
                    {
                        "authorId": "2195386",
                        "name": "Rita P. Ribeiro"
                    },
                    {
                        "authorId": "143887166",
                        "name": "Bruno Veloso"
                    },
                    {
                        "authorId": "1407905896",
                        "name": "M. Sayed-Mouchaweh"
                    },
                    {
                        "authorId": "1776362",
                        "name": "L. Rajaoarisoa"
                    },
                    {
                        "authorId": "2168577012",
                        "name": "Grzegorz J. Nalepa"
                    },
                    {
                        "authorId": "2188367772",
                        "name": "Jo\u00e3o Gama"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "1c7f0b8865307ab58371e0dc630087559542b253",
                "externalIds": {
                    "DOI": "10.1007/s13347-023-00635-6",
                    "CorpusId": 252502439
                },
                "corpusId": 252502439,
                "publicationVenue": {
                    "id": "bd7cf538-b981-449d-9e6c-64fa64047126",
                    "name": "Philosophy & Technology",
                    "type": "journal",
                    "alternate_names": [
                        "Philos  Technol"
                    ],
                    "issn": "2210-5433",
                    "url": "https://www.springer.com/philosophy/epistemology+and+philosophy+of+science/journal/13347",
                    "alternate_urls": [
                        "https://link.springer.com/journal/13347"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1c7f0b8865307ab58371e0dc630087559542b253",
                "title": "On the Philosophy of Unsupervised Learning",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48135453",
                        "name": "D. Watson"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "[48] Jonathan Crabb\u00e9 and Mihaela van der Schaar.",
                "(2)We also restrict to supervised models, since only early works exist to interpret unsupervised models [48, 49]."
            ],
            "citingPaper": {
                "paperId": "3d667922ad907cd88be6de92d13c308130105de4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-06715",
                    "ArXiv": "2304.06715",
                    "DOI": "10.48550/arXiv.2304.06715",
                    "CorpusId": 258108219
                },
                "corpusId": 258108219,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3d667922ad907cd88be6de92d13c308130105de4",
                "title": "Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance",
                "abstract": "Interpretability methods are valuable only if their explanations faithfully describe the explained model. In this work, we consider neural networks whose predictions are invariant under a specific symmetry group. This includes popular architectures, ranging from convolutional to graph neural networks. Any explanation that faithfully explains this type of model needs to be in agreement with this invariance property. We formalize this intuition through the notion of explanation invariance and equivariance by leveraging the formalism from geometric deep learning. Through this rigorous formalism, we derive (1) two metrics to measure the robustness of any interpretability method with respect to the model symmetry group; (2) theoretical robustness guarantees for some popular interpretability methods and (3) a systematic approach to increase the invariance of any interpretability method with respect to a symmetry group. By empirically measuring our metrics for explanations of models associated with various modalities and symmetry groups, we derive a set of 5 guidelines to allow users and developers of interpretability methods to produce robust explanations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2026670465",
                        "name": "Jonathan Crabbe"
                    },
                    {
                        "authorId": "1729969",
                        "name": "M. Schaar"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "b720dc316737256f71b2e2de35bb8836e79cf2e7",
                "externalIds": {
                    "ArXiv": "2303.05506",
                    "DBLP": "conf/iclr/JeffaresLCIS23",
                    "DOI": "10.48550/arXiv.2303.05506",
                    "CorpusId": 257427351
                },
                "corpusId": 257427351,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b720dc316737256f71b2e2de35bb8836e79cf2e7",
                "title": "TANGOS: Regularizing Tabular Neural Networks through Gradient Orthogonalization and Specialization",
                "abstract": "Despite their success with unstructured data, deep neural networks are not yet a panacea for structured tabular data. In the tabular domain, their efficiency crucially relies on various forms of regularization to prevent overfitting and provide strong generalization performance. Existing regularization techniques include broad modelling decisions such as choice of architecture, loss functions, and optimization methods. In this work, we introduce Tabular Neural Gradient Orthogonalization and Specialization (TANGOS), a novel framework for regularization in the tabular setting built on latent unit attributions. The gradient attribution of an activation with respect to a given input feature suggests how the neuron attends to that feature, and is often employed to interpret the predictions of deep networks. In TANGOS, we take a different approach and incorporate neuron attributions directly into training to encourage orthogonalization and specialization of latent attributions in a fully-connected network. Our regularizer encourages neurons to focus on sparse, non-overlapping input features and results in a set of diverse and specialized latent units. In the tabular domain, we demonstrate that our approach can lead to improved out-of-sample generalization performance, outperforming other popular regularization methods. We provide insight into why our regularizer is effective and demonstrate that TANGOS can be applied jointly with existing methods to achieve even greater generalization performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2131018849",
                        "name": "Alan Jeffares"
                    },
                    {
                        "authorId": "2121678977",
                        "name": "Tennison Liu"
                    },
                    {
                        "authorId": "2026670465",
                        "name": "Jonathan Crabbe"
                    },
                    {
                        "authorId": "80471080",
                        "name": "F. Imrie"
                    },
                    {
                        "authorId": "1729969",
                        "name": "M. Schaar"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "a-b, Multiple methods (the proposed likelihood ratio ranking, integrated gradients [26; 27], and Hotspot [28]) were applied to rank the genes in our simulated dataset by how strongly they were captured by multiGroupVI\u2019s group-specific latent spaces.",
                "We compared the performance of our likelihood ratio ranking with two other methods designed for interpreting the latent spaces of unsupervised machine learning models: Hotspot [28], a method that ranks genes by spatial autocorrelation when provided a given metric of cell-cell similarity (e.g. the latent space of an autoencoder), and an adaptation of integrated gradients [26] for unsupervised models proposed in Crabb\u00e9 and van der Schaar [27] applied to multiGroupVI\u2019s group-specific latent spaces.",
                "the latent space of an autoencoder), and an adaptation of integrated gradients [26] for unsupervised models proposed in Crabb\u00e9 and van der Schaar [27] applied to multiGroupVI\u2019s group-specific latent spaces."
            ],
            "citingPaper": {
                "paperId": "3be1dbad10c997d62052a88bf56aff85d78fdd95",
                "externalIds": {
                    "DBLP": "conf/mlcb/WeinbergerLHR22",
                    "DOI": "10.1101/2022.12.13.520349",
                    "CorpusId": 254736221
                },
                "corpusId": 254736221,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/3be1dbad10c997d62052a88bf56aff85d78fdd95",
                "title": "Disentangling shared and group-specific variations in single-cell transcriptomics data with multiGroupVI",
                "abstract": "Single-cell RNA sequencing (scRNA-seq) technologies have enabled a greater understanding of previously unexplored biological diversity. Based on the design of such experiments, individual cells from scRNA-seq datasets can often be attributed to non-overlapping \u201cgroups\u201d. For example, these group labels may denote the cell\u2019s tissue or cell line of origin. In this setting, one important problem consists in discerning patterns in the data that are shared across groups versus those that are group-specific. However, existing methods for this type of analysis are mainly limited to (generalized) linear latent variable models. Here we introduce multiGroupVI, a deep generative model for analyzing grouped scRNA-seq datasets that decomposes the data into shared and group-specific factors of variation. We first validate our approach on a simulated dataset, on which we significantly outperform state-of-the-art methods. We then apply it to explore regional differences in an scRNA-seq dataset sampled from multiple regions of the mouse small intestine. We implemented multiGroupVI using the scvi-tools library [1], and released it as open-source software at https://github.com/Genentech/multiGroupVI.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47232375",
                        "name": "Ethan Weinberger"
                    },
                    {
                        "authorId": "39848341",
                        "name": "Romain Lopez"
                    },
                    {
                        "authorId": "50018036",
                        "name": "Jan-Christian H\u00fctter"
                    },
                    {
                        "authorId": "144416712",
                        "name": "A. Regev"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "1a4798bcf4a16937c97a3d14b9967f63c1ece5f7",
                "externalIds": {
                    "ArXiv": "2210.00107",
                    "DBLP": "journals/corr/abs-2210-00107",
                    "DOI": "10.48550/arXiv.2210.00107",
                    "CorpusId": 252683963
                },
                "corpusId": 252683963,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1a4798bcf4a16937c97a3d14b9967f63c1ece5f7",
                "title": "Contrastive Corpus Attribution for Explaining Representations",
                "abstract": "Despite the widespread use of unsupervised models, very few methods are designed to explain them. Most explanation methods explain a scalar model output. However, unsupervised models output representation vectors, the elements of which are not good candidates to explain because they lack semantic meaning. To bridge this gap, recent works defined a scalar explanation output: a dot product-based similarity in the representation space to the sample being explained (i.e., an explicand). Although this enabled explanations of unsupervised models, the interpretation of this approach can still be opaque because similarity to the explicand's representation may not be meaningful to humans. To address this, we propose contrastive corpus similarity, a novel and semantically meaningful scalar explanation output based on a reference corpus and a contrasting foil set of samples. We demonstrate that contrastive corpus similarity is compatible with many post-hoc feature attribution methods to generate COntrastive COrpus Attributions (COCOA) and quantitatively verify that features important to the corpus are identified. We showcase the utility of COCOA in two ways: (i) we draw insights by explaining augmentations of the same image in a contrastive learning setting (SimCLR); and (ii) we perform zero-shot object localization by explaining the similarity of image representations to jointly learned text representations (CLIP).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7529125",
                        "name": "Christy Lin"
                    },
                    {
                        "authorId": "2616600",
                        "name": "Hugh Chen"
                    },
                    {
                        "authorId": "2125803151",
                        "name": "Chanwoo Kim"
                    },
                    {
                        "authorId": "2180463",
                        "name": "Su-In Lee"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[65] Jonathan Crabb\u00e9 and Mihaela van der Schaar.",
                "We quantitatively compare these various feature importance scores by computing their Pearson correlation r as in [64, 65]."
            ],
            "citingPaper": {
                "paperId": "3aea0c72be64f42f014bc2bbbbd261e524680d19",
                "externalIds": {
                    "ArXiv": "2209.11222",
                    "DBLP": "conf/nips/CrabbeS22",
                    "DOI": "10.48550/arXiv.2209.11222",
                    "CorpusId": 252438631
                },
                "corpusId": 252438631,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3aea0c72be64f42f014bc2bbbbd261e524680d19",
                "title": "Concept Activation Regions: A Generalized Framework For Concept-Based Explanations",
                "abstract": "Concept-based explanations permit to understand the predictions of a deep neural network (DNN) through the lens of concepts specified by users. Existing methods assume that the examples illustrating a concept are mapped in a fixed direction of the DNN's latent space. When this holds true, the concept can be represented by a concept activation vector (CAV) pointing in that direction. In this work, we propose to relax this assumption by allowing concept examples to be scattered across different clusters in the DNN's latent space. Each concept is then represented by a region of the DNN's latent space that includes these clusters and that we call concept activation region (CAR). To formalize this idea, we introduce an extension of the CAV formalism that is based on the kernel trick and support vector classifiers. This CAR formalism yields global concept-based explanations and local concept-based feature importance. We prove that CAR explanations built with radial kernels are invariant under latent space isometries. In this way, CAR assigns the same explanations to latent spaces that have the same geometry. We further demonstrate empirically that CARs offer (1) more accurate descriptions of how concepts are scattered in the DNN's latent space; (2) global explanations that are closer to human concept annotations and (3) concept-based feature importance that meaningfully relate concepts with each other. Finally, we use CARs to show that DNNs can autonomously rediscover known scientific concepts, such as the prostate cancer grading system.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2026670465",
                        "name": "Jonathan Crabbe"
                    },
                    {
                        "authorId": "1729969",
                        "name": "M. Schaar"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We do not compare with methods that are not post-hoc, that require exposure to outliers, or are not actionable (most existing XAI methods) to match model monitoring settings.",
                "To determine trust in model predictions, current works typically use (a) explainability methods (XAI) [11]\u2013[13]; which, though explainable, post-hoc and suitable for providing qualitative insights, require humans to oversee the explanations and are not directly actionable for continuous and automated model monitoring; or (b) uncertainty estimation techniques",
                "To determine trust in model predictions, current works typically use (a) explainability methods (XAI) [11]\u2013[13]; which, though explainable, post-hoc and suitable for providing qualitative insights, require humans to oversee the explanations and are not directly actionable for continuous and automated model monitoring; or (b) uncertainty estimation techniques [6], [14]\u2013[18]; that are difficult to train or are computationally expensive.",
                "XAI methods provide complementary, post-hoc explanations to the predictions of black-box models to induce trust."
            ],
            "citingPaper": {
                "paperId": "fb3f8e1b9c3cfabb9d13e0b518d4955cec31dd5d",
                "externalIds": {
                    "ArXiv": "2207.11290",
                    "DOI": "10.1109/TAI.2023.3272876",
                    "CorpusId": 258489655
                },
                "corpusId": 258489655,
                "publicationVenue": {
                    "id": "3c27e831-750f-45bc-9914-2148a5259eba",
                    "name": "IEEE Transactions on Artificial Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Artif Intell"
                    ],
                    "issn": "2691-4581",
                    "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9078688"
                },
                "url": "https://www.semanticscholar.org/paper/fb3f8e1b9c3cfabb9d13e0b518d4955cec31dd5d",
                "title": "An Explainable and Actionable Mistrust Scoring Framework for Model Monitoring",
                "abstract": "Continuous monitoring of trained ML models to determine when their predictions should and should not be trusted is essential for their safe deployment. Such a framework ought to be high-performing, explainable, post-hoc and actionable. We propose TRUST-LAPSE, a\"mistrust\"scoring framework for continuous model monitoring. We assess the trustworthiness of each input sample's model prediction using a sequence of latent-space embeddings. Specifically, (a) our latent-space mistrust score estimates mistrust using distance metrics (Mahalanobis distance) and similarity metrics (cosine similarity) in the latent-space and (b) our sequential mistrust score determines deviations in correlations over the sequence of past input representations in a non-parametric, sliding-window based algorithm for actionable continuous monitoring. We evaluate TRUST-LAPSE via two downstream tasks: (1) distributionally shifted input detection, and (2) data drift detection. We evaluate across diverse domains - audio and vision using public datasets and further benchmark our approach on challenging, real-world electroencephalograms (EEG) datasets for seizure detection. Our latent-space mistrust scores achieve state-of-the-art results with AUROCs of 84.1 (vision), 73.9 (audio), and 77.1 (clinical EEGs), outperforming baselines by over 10 points. We expose critical failures in popular baselines that remain insensitive to input semantic content, rendering them unfit for real-world model monitoring. We show that our sequential mistrust scores achieve high drift detection rates; over 90% of the streams show<20% error for all domains. Through extensive qualitative and quantitative evaluations, we show that our mistrust scores are more robust and provide explainability for easy adoption into practice.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "12559393",
                        "name": "Nandita Bhaskhar"
                    },
                    {
                        "authorId": "143648587",
                        "name": "D. Rubin"
                    },
                    {
                        "authorId": "1404595471",
                        "name": "C. Lee-Messer"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "0365f88a00f43b81ddbbb5fbe6bbf1fdb97a4de6",
                "externalIds": {
                    "DOI": "10.1038/s41592-023-01955-3",
                    "CorpusId": 245468389,
                    "PubMed": "37550579"
                },
                "corpusId": 245468389,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/0365f88a00f43b81ddbbb5fbe6bbf1fdb97a4de6",
                "title": "Isolating salient variations of interest in single-cell data with contrastiveVI",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47232375",
                        "name": "Ethan Weinberger"
                    },
                    {
                        "authorId": "2143476682",
                        "name": "Chris Lin"
                    },
                    {
                        "authorId": "2180463",
                        "name": "Su-In Lee"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "To determine trust in model predictions, current works typically use (a) explainability methods (XAI) [11]\u2013[13]; which, though explainable, post-hoc and suitable for providing qualitative insights, require humans to oversee the explanations and are not directly actionable for continuous and automated model monitoring; or (b) uncertainty estimation techniques",
                "To determine trust in model predictions, current works typically use (a) explainability methods (XAI) [11]\u2013[13]; which, though explainable, post-hoc and suitable for providing qualitative insights, require humans to oversee the explanations and are not directly actionable for continuous and automated model monitoring; or (b) uncertainty estimation techniques [6], [14]\u2013[18]; that are difficult to train or are computationally expensive.",
                "We don\u2019t compare with methods that are not post-hoc, that require exposure to outliers, or are not actionable (most existing XAI methods) to match model monitoring settings.",
                "XAI methods provide complementary, post-hoc explanations to the predictions of black-box models to induce trust."
            ],
            "citingPaper": {
                "paperId": "c4982e4c761c7fee2e3e65162a6066924bc26f8a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-11290",
                    "DOI": "10.48550/arXiv.2207.11290",
                    "CorpusId": 251040783
                },
                "corpusId": 251040783,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c4982e4c761c7fee2e3e65162a6066924bc26f8a",
                "title": "TRUST-LAPSE: An Explainable & Actionable Mistrust Scoring Framework for Model Monitoring",
                "abstract": "\u2014Continuous monitoring of trained ML models to de- termine when their predictions should and should not be trusted is essential for their safe deployment. Such a framework ought to be high-performing, explainable, post-hoc and actionable. We propose TRUST-LAPSE, a \u201cmistrust\u201d scoring framework for continuous model monitoring. We assess the trustworthiness of each input sample\u2019s model prediction using a sequence of latent- space embeddings. Speci\ufb01cally, (a) our latent-space mistrust score estimates mistrust using distance metrics (Mahalanobis distance) and similarity metrics (cosine similarity) in the latent-space and (b) our sequential mistrust score determines deviations in correlations over the sequence of past input representations in a non-parametric, sliding-window based algorithm for actionable continuous monitoring. We evaluate TRUST-LAPSE via two downstream tasks: (1) distributionally shifted input detection and (2) data drift detection, across diverse domains\u2013 audio & vision using public datasets and further benchmark our approach on challenging, real-world electroencephalograms (EEG) datasets for seizure detection. Our latent-space mistrust scores achieve state-of-the-art results with AUROCs of 84.1 (vision), 73.9 (audio), 77.1 (clinical EEGs), outperforming baselines by over 10 points. We expose critical failures in popular baselines that remain insensitive to input semantic content, rendering them un\ufb01t for real-world model monitoring. We show that our sequential mistrust scores achieve high drift detection rates: over 90% of the streams show < 20% error for all domains. Through extensive qualitative and quantitative evaluations, we show that our mistrust scores are more robust and provide explainability for easy adoption into practice.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "12559393",
                        "name": "Nandita Bhaskhar"
                    },
                    {
                        "authorId": "143648587",
                        "name": "D. Rubin"
                    },
                    {
                        "authorId": "1404595471",
                        "name": "C. Lee-Messer"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "55f3c3b4964555690e30959d3731f970326642a1",
                "externalIds": {
                    "CorpusId": 259327468
                },
                "corpusId": 259327468,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/55f3c3b4964555690e30959d3731f970326642a1",
                "title": "Beyond independent masking in tabular self-supervision",
                "abstract": "This thesis contributes to the development and understanding of masking schemes in tabular self-supervision. We demonstrate that correlated masking is a generally applicable technique that enhances state-of-the-art methods. This claim is validated on a family of synthetic datasets and several benchmarks. We also introduce combination masks, a technique to synthesise multiple masking schemes into a single approach to benefit from the advantages of each. We show combining independent and correlated masking in this way outperforms each individually on a family of seven proteomics datasets for predicting the efficacy of cancer drug treatments. A masking strategy is one of numerous design choices in tabular self-supervision, yet the literature lacks a systematic approach for optimising these hyperparameters without at least some labelled data for cross-validation. To address this, we exploit recent advances in label-free explainable artificial intelligence to reveal why some representations are better than others. This deepens our understanding of why correlated masking is effective, and allows us to conjecture general properties that characterise good representations. We quantify these observations to form metrics that allow us to estimate approximately optimal hyperparameters on two standard benchmark datasets without access to any labelled data or downstream tasks. This is an initial contribution to a broader program of research, which aims to equip the practitioner with a suite of such tools and metrics to guide model development. As an application of this, we design and analyse a novel hierarchical architecture for ensembling encoders that uses collaboration between ensemble members to solve difficult tasks. This design outperforms standard ensembles and our tools explain why; collaboration acts as a form of regularisation and increases consistency amongst the feature importance scores.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144308506",
                        "name": "Andrew Burrell"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": null,
                "externalIds": null,
                "corpusId": "252684601",
                "publicationVenue": null,
                "url": null,
                "title": "ilit y Classifier Encoder Explicand-Centric Feature Attribution for Unsupervised Models image 1 . png Encoder Corpus of Abnormal Cases Foil of Normal Cases Contrastive Corpus Feature Attribution for Unsupervised Models",
                "abstract": null,
                "year": 2022,
                "authors": []
            }
        }
    ]
}