{
    "offset": 0,
    "data": [
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "de2e70c25894f7c2196a043540f2df773cc593cf",
                "externalIds": {
                    "DOI": "10.1109/LRA.2023.3316896",
                    "CorpusId": 262076916
                },
                "corpusId": 262076916,
                "publicationVenue": {
                    "id": "93c335b7-edf4-45f5-8ddc-7c5835154945",
                    "name": "IEEE Robotics and Automation Letters",
                    "alternate_names": [
                        "IEEE Robot Autom Lett"
                    ],
                    "issn": "2377-3766",
                    "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=7083369"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/de2e70c25894f7c2196a043540f2df773cc593cf",
                "title": "Improving Robot Proficiency Self-Assessment via Meta-Assessment",
                "abstract": "Proficiency self-assessment (PSA), which is the ability to estimate how likely one can complete a task, is a beneficial property for autonomous robots. Prior work developed the assumption-alignment tracking (AAT) method for PSA, which estimates the probability that a robot will successfully complete a task. This letter refers to the prediction made by AAT as the first-level assessment (FLA), and further proposes a second-level assessment (SLA) that determines whether the FLA prediction is correct. The probability that the FLA prediction is correct is conditioned on four features: 1) the mean distance from a test sample to its nearest neighbors in the training set; 2) the predicted probability of success made by the FLA; 3) the ratio between the robot's current performance and its performance standard; and 4) the percentage of the task the robot has already completed. The SLA model is trained on the four features using a Random Forest algorithm. It is evaluated by two metrics: discriminability, measured by the area under the ROC curve, and calibration, measured using expected calibration error. On a simulated navigation task and a manipulation task by a Sawyer robot, results demonstrate that the SLA model not only calibrates the FLA model as well as existing calibration methods (Platt calibration and isotonic regression), but also produces very high discriminability even if the FLA model's original discriminability is much lower. Results also indicate the usefulness of each of the four features used by the SLA model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145666540",
                        "name": "X. Cao"
                    },
                    {
                        "authorId": "2243331103",
                        "name": "Jacob W. Crandall"
                    },
                    {
                        "authorId": "2240817717",
                        "name": "Michael A. Goodrich"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2ec74844ce8342ee0ec6e066d35e1511cf84eaf2",
                "externalIds": {
                    "DOI": "10.1016/j.patcog.2023.110023",
                    "CorpusId": 263716636
                },
                "corpusId": 263716636,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2ec74844ce8342ee0ec6e066d35e1511cf84eaf2",
                "title": "Inter-domain mixup for semi-supervised domain adaptation",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "150151364",
                        "name": "Jichang Li"
                    },
                    {
                        "authorId": "2255423082",
                        "name": "Guanbin Li"
                    },
                    {
                        "authorId": "2226950987",
                        "name": "Yizhou Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2022), calibration (Thulasidasan et al., 2019; Zhang et al., 2022a), and adversarial robustness (Pang et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "077a8ceae6e6945596bc36e49eca9328de94dd63",
                "externalIds": {
                    "ArXiv": "2310.00183",
                    "CorpusId": 263334004
                },
                "corpusId": 263334004,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/077a8ceae6e6945596bc36e49eca9328de94dd63",
                "title": "On the Equivalence of Graph Convolution and Mixup",
                "abstract": "This paper investigates the relationship between graph convolution and Mixup techniques. Graph convolution in a graph neural network involves aggregating features from neighboring samples to learn representative features for a specific node or sample. On the other hand, Mixup is a data augmentation technique that generates new examples by averaging features and one-hot labels from multiple samples. One commonality between these techniques is their utilization of information from multiple samples to derive feature representation. This study aims to explore whether a connection exists between these two approaches. Our investigation reveals that, under two mild conditions, graph convolution can be viewed as a specialized form of Mixup that is applied during both the training and testing phases. The two conditions are: 1) \\textit{Homophily Relabel} - assigning the target node's label to all its neighbors, and 2) \\textit{Test-Time Mixup} - Mixup the feature during the test time. We establish this equivalence mathematically by demonstrating that graph convolution networks (GCN) and simplified graph convolution (SGC) can be expressed as a form of Mixup. We also empirically verify the equivalence by training an MLP using the two conditions to achieve comparable performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2249766229",
                        "name": "Xiaotian Han"
                    },
                    {
                        "authorId": "2249759429",
                        "name": "Hanqing Zeng"
                    },
                    {
                        "authorId": "2249829419",
                        "name": "Yu Chen"
                    },
                    {
                        "authorId": "2249759005",
                        "name": "Shaoliang Nie"
                    },
                    {
                        "authorId": "2249853379",
                        "name": "Jingzhou Liu"
                    },
                    {
                        "authorId": "2249757531",
                        "name": "Kanika Narang"
                    },
                    {
                        "authorId": "2249758145",
                        "name": "Zahra Shakeri"
                    },
                    {
                        "authorId": "2178963",
                        "name": "Karthik Abinav Sankararaman"
                    },
                    {
                        "authorId": "2249954878",
                        "name": "Song Jiang"
                    },
                    {
                        "authorId": "2072010",
                        "name": "Madian Khabsa"
                    },
                    {
                        "authorId": "2250669429",
                        "name": "Qifan Wang"
                    },
                    {
                        "authorId": "2249844822",
                        "name": "Xia Hu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8736936fd04cfe3028e146551a5cb08b2d817f34",
                "externalIds": {
                    "ArXiv": "2309.15376",
                    "CorpusId": 263153022
                },
                "corpusId": 263153022,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8736936fd04cfe3028e146551a5cb08b2d817f34",
                "title": "ADGym: Design Choices for Deep Anomaly Detection",
                "abstract": "Deep learning (DL) techniques have recently been applied to anomaly detection (AD), yielding successful outcomes in areas such as finance, medical services, and cloud computing. However, much of the current research evaluates a deep AD algorithm holistically, failing to understand the contributions of individual design choices like loss functions and network architectures. Consequently, the importance of prerequisite steps, such as preprocessing, might be overshadowed by the spotlight on novel loss functions and architectures. In this paper, we address these oversights by posing two questions: (i) Which components (i.e., design choices) of deep AD methods are pivotal in detecting anomalies? (ii) How can we construct tailored AD algorithms for specific datasets by selecting the best design choices automatically, rather than relying on generic, pre-existing solutions? To this end, we introduce ADGym, the first platform designed for comprehensive evaluation and automatic selection of AD design elements in deep methods. Extensive experiments reveal that merely adopting existing leading methods is not ideal. Models crafted using ADGym markedly surpass current state-of-the-art techniques.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2249727206",
                        "name": "Minqi Jiang"
                    },
                    {
                        "authorId": "2204929921",
                        "name": "Chaochuan Hou"
                    },
                    {
                        "authorId": "2204948431",
                        "name": "Ao Zheng"
                    },
                    {
                        "authorId": "2004577591",
                        "name": "Songqiao Han"
                    },
                    {
                        "authorId": "117894122",
                        "name": "Hailiang Huang"
                    },
                    {
                        "authorId": "2247916993",
                        "name": "Qingsong Wen"
                    },
                    {
                        "authorId": "35346885",
                        "name": "Xiyang Hu"
                    },
                    {
                        "authorId": "145454815",
                        "name": "Yue Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The mixup regularization has been verified that optimizing f on mixed-up data results in better transformation capability of the network and improves model calibration [37]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "53487283407d3aec9dac98679896ef6c63adb9ad",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-08838",
                    "ArXiv": "2309.08838",
                    "DOI": "10.48550/arXiv.2309.08838",
                    "CorpusId": 261884429
                },
                "corpusId": 261884429,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/53487283407d3aec9dac98679896ef6c63adb9ad",
                "title": "AOSR-Net: All-in-One Sandstorm Removal Network",
                "abstract": "Most existing sandstorm image enhancement methods are based on traditional theory and prior knowledge, which often restrict their applicability in real-world scenarios. In addition, these approaches often adopt a strategy of color correction followed by dust removal, which makes the algorithm structure too complex. To solve the issue, we introduce a novel image restoration model, named all-in-one sandstorm removal network (AOSR-Net). This model is developed based on a re-formulated sandstorm scattering model, which directly establishes the image mapping relationship by integrating intermediate parameters. Such integration scheme effectively addresses the problems of over-enhancement and weak generalization in the field of sand dust image enhancement. Experimental results on synthetic and real-world sandstorm images demonstrate the superiority of the proposed AOSR-Net over state-of-the-art (SOTA) algorithms.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2134765716",
                        "name": "Yazhong Si"
                    },
                    {
                        "authorId": "49469977",
                        "name": "Xulong Zhang"
                    },
                    {
                        "authorId": "2241742825",
                        "name": "Fan Yang"
                    },
                    {
                        "authorId": "66063851",
                        "name": "Jianzong Wang"
                    },
                    {
                        "authorId": "145292435",
                        "name": "Ning Cheng"
                    },
                    {
                        "authorId": "91353860",
                        "name": "Jing Xiao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cceb11378e7bfb900f68d6d9952b6b6fa1452de8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-07453",
                    "ArXiv": "2309.07453",
                    "DOI": "10.48550/arXiv.2309.07453",
                    "CorpusId": 261822309
                },
                "corpusId": 261822309,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cceb11378e7bfb900f68d6d9952b6b6fa1452de8",
                "title": "SC-MAD: Mixtures of Higher-order Networks for Data Augmentation",
                "abstract": "The myriad complex systems with multiway interactions motivate the extension of graph-based pairwise connections to higher-order relations. In particular, the simplicial complex has inspired generalizations of graph neural networks (GNNs) to simplicial complex-based models. Learning on such systems requires large amounts of data, which can be expensive or impossible to obtain. We propose data augmentation of simplicial complexes through both linear and nonlinear mixup mechanisms that return mixtures of existing labeled samples. In addition to traditional pairwise mixup, we present a convex clustering mixup approach for a data-driven relationship among several simplicial complexes. We theoretically demonstrate that the resultant synthetic simplicial complexes interpolate among existing data with respect to homomorphism densities. Our method is demonstrated on both synthetic and real-world datasets for simplicial complex classification.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1936823928",
                        "name": "Madeline Navarro"
                    },
                    {
                        "authorId": "2239197971",
                        "name": "Santiago Segarra"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fdba5cc6d687df86d0bbed368b9523266b98a04b",
                "externalIds": {
                    "DOI": "10.1016/j.compbiomed.2023.107441",
                    "CorpusId": 261483812,
                    "PubMed": "37683529"
                },
                "corpusId": 261483812,
                "publicationVenue": {
                    "id": "dca681d7-2e9b-4f13-905a-b59dfd692525",
                    "name": "Computers in Biology and Medicine",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Biology Med"
                    ],
                    "issn": "0010-4825",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/351/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/computers-in-biology-and-medicine/",
                        "http://www.sciencedirect.com/science/journal/00104825"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fdba5cc6d687df86d0bbed368b9523266b98a04b",
                "title": "Application of uncertainty quantification to artificial intelligence in healthcare: A review of last decade (2013-2023).",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "119065237",
                        "name": "S. Seoni"
                    },
                    {
                        "authorId": "123747366",
                        "name": "V. Jahmunah"
                    },
                    {
                        "authorId": "36563851",
                        "name": "M. Salvi"
                    },
                    {
                        "authorId": "30691907",
                        "name": "P. Barua"
                    },
                    {
                        "authorId": "2065707618",
                        "name": "F. Molinari"
                    },
                    {
                        "authorId": "1704790",
                        "name": "Usha R. Acharya"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b76c641f86b77081a4dbdccc17004cb2749efc44",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-13132",
                    "ArXiv": "2309.13132",
                    "DOI": "10.1016/j.cmpb.2023.107816",
                    "CorpusId": 262206946,
                    "PubMed": "37778139"
                },
                "corpusId": 262206946,
                "publicationVenue": {
                    "id": "0592418d-f6da-4297-9952-8ca76913db01",
                    "name": "Computer Methods and Programs in Biomedicine",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Method Program Biomed"
                    ],
                    "issn": "0169-2607",
                    "url": "https://www.journals.elsevier.com/computer-methods-and-programs-in-biomedicine",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/01692607"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b76c641f86b77081a4dbdccc17004cb2749efc44",
                "title": "Understanding Calibration of Deep Neural Networks for Medical Image Classification",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "71618319",
                        "name": "A. Sambyal"
                    },
                    {
                        "authorId": "148267816",
                        "name": "Usma Niyaz"
                    },
                    {
                        "authorId": "2503137",
                        "name": "N. C. Krishnan"
                    },
                    {
                        "authorId": "2133448954",
                        "name": "Deepti R. Bathula"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "deec4779098ae23cdf278430f93f4032e1165043",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-11990",
                    "ArXiv": "2308.11990",
                    "DOI": "10.48550/arXiv.2308.11990",
                    "CorpusId": 261076141
                },
                "corpusId": 261076141,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/deec4779098ae23cdf278430f93f4032e1165043",
                "title": "RankMixup: Ranking-Based Mixup Training for Network Calibration",
                "abstract": "Network calibration aims to accurately estimate the level of confidences, which is particularly important for employing deep neural networks in real-world systems. Recent approaches leverage mixup to calibrate the network's predictions during training. However, they do not consider the problem that mixtures of labels in mixup may not accurately represent the actual distribution of augmented samples. In this paper, we present RankMixup, a novel mixup-based framework alleviating the problem of the mixture of labels for network calibration. To this end, we propose to use an ordinal ranking relationship between raw and mixup-augmented samples as an alternative supervisory signal to the label mixtures for network calibration. We hypothesize that the network should estimate a higher level of confidence for the raw samples than the augmented ones (Fig.1). To implement this idea, we introduce a mixup-based ranking loss (MRL) that encourages lower confidences for augmented samples compared to raw ones, maintaining the ranking relationship. We also propose to leverage the ranking relationship among multiple mixup-augmented samples to further improve the calibration capability. Augmented samples with larger mixing coefficients are expected to have higher confidences and vice versa (Fig.1). That is, the order of confidences should be aligned with that of mixing coefficients. To this end, we introduce a novel loss, M-NDCG, in order to reduce the number of misaligned pairs of the coefficients and confidences. Extensive experimental results on standard benchmarks for network calibration demonstrate the effectiveness of RankMixup.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1598361735",
                        "name": "Jongyoun Noh"
                    },
                    {
                        "authorId": "2179023776",
                        "name": "Hyekang Park"
                    },
                    {
                        "authorId": "26973831",
                        "name": "Junghyup Lee"
                    },
                    {
                        "authorId": "38723538",
                        "name": "Bumsub Ham"
                    }
                ]
            }
        },
        {
            "contexts": [
                "MixUp, when benchmarked using calibration metrics, presented CNNs with significant calibration benefits according to various studies [56, 63, 8].",
                "Various studies have shown the advantages of these augmentations for calibration of CNNs using standard calibration-based metrics on general computer vision benchmarks [56, 63, 8, 12, 14]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f890754eabffeefece12c0e2877d35cebb024c73",
                "externalIds": {
                    "ArXiv": "2308.11902",
                    "CorpusId": 261076239
                },
                "corpusId": 261076239,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f890754eabffeefece12c0e2877d35cebb024c73",
                "title": "Studying the Impact of Augmentations on Medical Confidence Calibration",
                "abstract": "The clinical explainability of convolutional neural networks (CNN) heavily relies on the joint interpretation of a model's predicted diagnostic label and associated confidence. A highly certain or uncertain model can significantly impact clinical decision-making. Thus, ensuring that confidence estimates reflect the true correctness likelihood for a prediction is essential. CNNs are often poorly calibrated and prone to overconfidence leading to improper measures of uncertainty. This creates the need for confidence calibration. However, accuracy and performance-based evaluations of CNNs are commonly used as the sole benchmark for medical tasks. Taking into consideration the risks associated with miscalibration is of high importance. In recent years, modern augmentation techniques, which cut, mix, and combine images, have been introduced. Such augmentations have benefited CNNs through regularization, robustness to adversarial samples, and calibration. Standard augmentations based on image scaling, rotating, and zooming, are widely leveraged in the medical domain to combat the scarcity of data. In this paper, we evaluate the effects of three modern augmentation techniques, CutMix, MixUp, and CutOut on the calibration and performance of CNNs for medical tasks. CutMix improved calibration the most while CutOut often lowered the level of calibration.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2087043437",
                        "name": "A. Rao"
                    },
                    {
                        "authorId": "2173628863",
                        "name": "Joonhyung Lee"
                    },
                    {
                        "authorId": "6970989",
                        "name": "O. Aalami"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Previous research [43] suggests that a well-calibrated model should perform well on OoD datasets.",
                "[43] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "442e67d1850cf055007fec1b49474664176a67ec",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-11838",
                    "ArXiv": "2308.11838",
                    "DOI": "10.48550/arXiv.2308.11838",
                    "CorpusId": 261076328
                },
                "corpusId": 261076328,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/442e67d1850cf055007fec1b49474664176a67ec",
                "title": "A Benchmark Study on Calibration",
                "abstract": "Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can robustness be used as a calibration measurement? (iii) How reliable are calibration metrics? (iv) Does a post-hoc calibration method affect all models uniformly? (v) How does calibration interact with accuracy? (vi) What is the impact of bin size on calibration measurement? (vii) Which architectural designs are beneficial for calibration? Additionally, our study bridges an existing gap by exploring calibration within NAS. By providing this dataset, we enable further research into NAS calibration. As far as we are aware, our research represents the first large-scale investigation into calibration properties and the premier study of calibration issues within NAS.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2256641428",
                        "name": "Linwei Tao"
                    },
                    {
                        "authorId": "2233233495",
                        "name": "Younan Zhu"
                    },
                    {
                        "authorId": "2182135302",
                        "name": "Haolan Guo"
                    },
                    {
                        "authorId": "151498428",
                        "name": "Minjing Dong"
                    },
                    {
                        "authorId": "2172076945",
                        "name": "Changming Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The evaluation metrics were accuracy and Overconfidence Error (OE) (Thulasidasan et al. 2019).",
                "Follow-up research (Thulasidasan et al. 2019) demonstrated that this simple technique is helpful for calibrating neural networks, particularly because interpolated soft labels reduce overconfidence.",
                "2018) tried to calibrate the generalization ability of the model by mixing a pair of images and their labels, and subsequent research (Thulasidasan et al. 2019) has shown that the mixup can help alleviate overconfidence."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a21660f05fca1a3073bc8a887b7a2ecf280e8b22",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-10571",
                    "ArXiv": "2308.10571",
                    "DOI": "10.48550/arXiv.2308.10571",
                    "CorpusId": 261048700
                },
                "corpusId": 261048700,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a21660f05fca1a3073bc8a887b7a2ecf280e8b22",
                "title": "Overcoming Overconfidence for Active Learning",
                "abstract": "It is not an exaggeration to say that the recent progress in artificial intelligence technology depends on large-scale and high-quality data. Simultaneously, a prevalent issue exists everywhere: the budget for data labeling is constrained. Active learning is a prominent approach for addressing this issue, where valuable data for labeling is selected through a model and utilized to iteratively adjust the model. However, due to the limited amount of data in each iteration, the model is vulnerable to bias; thus, it is more likely to yield overconfident predictions. In this paper, we present two novel methods to address the problem of overconfidence that arises in the active learning scenario. The first is an augmentation strategy named Cross-Mix-and-Mix (CMaM), which aims to calibrate the model by expanding the limited training distribution. The second is a selection strategy named Ranked Margin Sampling (RankedMS), which prevents choosing data that leads to overly confident predictions. Through various experiments and analyses, we are able to demonstrate that our proposals facilitate efficient data selection by alleviating overconfidence, even though they are readily applicable.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2070346646",
                        "name": "Yujin Hwang"
                    },
                    {
                        "authorId": "2162187222",
                        "name": "Won Jo"
                    },
                    {
                        "authorId": "2110805977",
                        "name": "Juyoung Hong"
                    },
                    {
                        "authorId": "2154073418",
                        "name": "Yukyung Choi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, mixup variants have been shown to be effective on a variety of tasks, including fairness machine learning (Han et al. 2022b, 2023; Mroueh et al. 2021), domain generalization (Zhou et al. 2020; Yao et al. 2022), confidence calibration (Zhang et al. 2022; Thulasidasan et al. 2019).",
                "Due to the simplicity and effectiveness, mixup-based methods have gained popularity in various data types and tasks (Yun et al. 2019; Kim, Choo, and Song 2020; Kim et al. 2023; Sahoo et al. 2021; Wang et al. 2021; Han et al. 2022a; Verma et al. 2019; Han et al. 2022b, 2023; Zhou et al. 2020; Mroueh et al. 2021; Zhang et al. 2022; Thulasidasan et al. 2019).",
                "\u2026have gained popularity in various data types and tasks (Yun et al. 2019; Kim, Choo, and Song 2020; Kim et al. 2023; Sahoo et al. 2021; Wang et al. 2021; Han et al. 2022a; Verma et al. 2019; Han et al. 2022b, 2023; Zhou et al. 2020; Mroueh et al. 2021; Zhang et al. 2022; Thulasidasan et al. 2019)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "11d9abef642cca735eaea2b7846174cc3de3b5d5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-06451",
                    "ArXiv": "2308.06451",
                    "DOI": "10.48550/arXiv.2308.06451",
                    "CorpusId": 260887608
                },
                "corpusId": 260887608,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/11d9abef642cca735eaea2b7846174cc3de3b5d5",
                "title": "Semantic Equivariant Mixup",
                "abstract": "Mixup is a well-established data augmentation technique, which can extend the training distribution and regularize the neural networks by creating ''mixed'' samples based on the label-equivariance assumption, i.e., a proportional mixup of the input data results in the corresponding labels being mixed in the same proportion. However, previous mixup variants may fail to exploit the label-independent information in mixed samples during training, which usually contains richer semantic information. To further release the power of mixup, we first improve the previous label-equivariance assumption by the semantic-equivariance assumption, which states that the proportional mixup of the input data should lead to the corresponding representation being mixed in the same proportion. Then a generic mixup regularization at the representation level is proposed, which can further regularize the model with the semantic information in mixed samples. At a high level, the proposed semantic equivariant mixup (sem) encourages the structure of the input data to be preserved in the representation space, i.e., the change of input will result in the obtained representation information changing in the same way. Different from previous mixup variants, which tend to over-focus on the label-related information, the proposed method aims to preserve richer semantic information in the input with semantic-equivariance assumption, thereby improving the robustness of the model against distribution shifts. We conduct extensive empirical studies and qualitative analyzes to demonstrate the effectiveness of our proposed method. The code of the manuscript is in the supplement.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2008848359",
                        "name": "Zongbo Han"
                    },
                    {
                        "authorId": "2229112999",
                        "name": "Tianchi Xie"
                    },
                    {
                        "authorId": "2152564746",
                        "name": "Bing Wu"
                    },
                    {
                        "authorId": "2150570513",
                        "name": "Qinghua Hu"
                    },
                    {
                        "authorId": "2256775070",
                        "name": "Changqing Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Label smoothing (M\u00fcller, Kornblith, and Hinton 2019) and mixup (Thulasidasan et al. 2019) are popular approaches in this line.",
                "Thulasidasan et al. (Thulasidasan et al. 2019) observed that the combination of label smoothing and mixup training significantly improved calibration.",
                "(21)\nMixup training (Thulasidasan et al. 2019) is another work in this line of exploration.",
                "Data augmentation is an effective way to alleviate this phenomenon and brings implicit calibration effects (Thulasidasan et al. 2019; Mu\u0308ller, Kornblith, and Hinton 2019).",
                "Label smoothing (Mu\u0308ller, Kornblith, and Hinton 2019) and mixup (Thulasidasan et al. 2019) are popular approaches in this line.",
                "Data augmentation is an effective way to alleviate this phenomenon and brings implicit calibration effects (Thulasidasan et al. 2019; M\u00fcller, Kornblith, and Hinton 2019).",
                "Mixup training (Thulasidasan et al. 2019) is another work in this line of exploration.",
                "(Thulasidasan et al. 2019) observed that the combination of label smoothing and mixup training significantly improved calibration."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "e7923b875fb12eea26926955f5e3b836595fa142",
                "externalIds": {
                    "ArXiv": "2308.01222",
                    "DBLP": "journals/corr/abs-2308-01222",
                    "DOI": "10.48550/arXiv.2308.01222",
                    "CorpusId": 260379149
                },
                "corpusId": 260379149,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e7923b875fb12eea26926955f5e3b836595fa142",
                "title": "Calibration in Deep Learning: A Survey of the State-of-the-Art",
                "abstract": "Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent methods proposed to calibrate deep models by using different mechanisms. In this survey, we review the state-of-the-art calibration methods and provide an understanding of their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibration methods that we roughly classified into four categories: post-hoc calibration, regularization methods, uncertainty estimation, and composition methods. We also covered some recent advancements in calibrating large models, particularly large language models (LLMs). Finally, we discuss some open issues, challenges, and potential directions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "98243766",
                        "name": "Cheng Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6dadcac943c6863dfe727f963fb60cfec225082a",
                "externalIds": {
                    "DOI": "10.1016/j.patcog.2023.109909",
                    "CorpusId": 261197969
                },
                "corpusId": 261197969,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6dadcac943c6863dfe727f963fb60cfec225082a",
                "title": "Semantic augmentation by mixing contents for semi-supervised learning",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "41021696",
                        "name": "R\u00e9my Sun"
                    },
                    {
                        "authorId": "37135849",
                        "name": "Cl\u00e9ment Masson"
                    },
                    {
                        "authorId": "2386962",
                        "name": "Gilles H\u00e9naff"
                    },
                    {
                        "authorId": "1728523",
                        "name": "Nicolas Thome"
                    },
                    {
                        "authorId": "51021910",
                        "name": "M. Cord"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mixup [85, 62] explores the neighbourhood of training data through random interpolation of input images and associated labels to improve model calibration degree.",
                "OE = M\u2211 i=1 |Bi| |D| \u00b7 1(Ci > Ai) \u00b7 |Ci \u2212Ai|, (29)\nWe adapt OE to different binning schemes of ECEEW, ECEEM, ECESWEEP to produce OEEW, OEEM, OESWEEP respectively.",
                "2 [62] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.",
                "Evaluation Metrics: We use Equal-Width Expected Calibration Error (ECEEW) [18] and Equal-Width Overconfidence Error (OEEW) [62] with 10 bins (B = 10) to\nevaluate the model calibration degrees.",
                "8 presents model calibration degrees, evaluated in terms of Equal-Width Expected Calibration Error (ECEEW) and Equal-Width Over-confidence Error (OEEW) with 100 bins (B = 100), of various static stochastic label perturbation techniques, in which a unique label perturbation probability \u03b1 is set for all samples throughout the training.",
                "We also compare with model calibration methods include: Temperature Scaling (TS) [18], Brier Loss [4], MMCE [30], Label Smoothing [46], Mixup [62], Focal Loss [45] and AdaFocal [17] implemented on our baseline model.",
                "Evaluation Metrics: We use Equal-Width Expected Calibration Error (ECEEW) [18] and Equal-Width Overconfidence Error (OEEW) [62] with 10 bins (B = 10) to evaluate the model calibration degrees.",
                "We report model calibration results evaluated in terms of Equal-Width Expected Calibration Error (ECEEW) and Equal-Width Over-confidence Error (OEEW) with 10 bins in Tab."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "176a26930af8a44f6ff632c031d8db440af8e2c8",
                "externalIds": {
                    "ArXiv": "2307.13539",
                    "DBLP": "journals/corr/abs-2307-13539",
                    "DOI": "10.48550/arXiv.2307.13539",
                    "CorpusId": 260154764
                },
                "corpusId": 260154764,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/176a26930af8a44f6ff632c031d8db440af8e2c8",
                "title": "Model Calibration in Dense Classification with Adaptive Label Perturbation",
                "abstract": "For safety-related applications, it is crucial to produce trustworthy deep neural networks whose prediction is associated with confidence that can represent the likelihood of correctness for subsequent decision-making. Existing dense binary classification models are prone to being over-confident. To improve model calibration, we propose Adaptive Stochastic Label Perturbation (ASLP) which learns a unique label perturbation level for each training image. ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss, which unifies label perturbation processes including stochastic approaches (like DisturbLabel), and label smoothing, to correct calibration while maintaining classification rates. ASLP follows Maximum Entropy Inference of classic statistical mechanics to maximise prediction entropy with respect to missing information. It performs this while: (1) preserving classification accuracy on known data as a conservative solution, or (2) specifically improves model calibration degree by minimising the gap between the prediction accuracy and expected confidence of the target training label. Extensive results demonstrate that ASLP can significantly improve calibration degrees of dense binary classification models on both in-distribution and out-of-distribution data. The code is available on https://github.com/Carlisle-Liu/ASLP.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108267943",
                        "name": "Jiawei Liu"
                    },
                    {
                        "authorId": "2064449445",
                        "name": "C. Ye"
                    },
                    {
                        "authorId": "2143130631",
                        "name": "Shanpeng Wang"
                    },
                    {
                        "authorId": "2164105032",
                        "name": "Rui-Qing Cui"
                    },
                    {
                        "authorId": "2155698491",
                        "name": "Jing Zhang"
                    },
                    {
                        "authorId": "2166659219",
                        "name": "Kai Zhang"
                    },
                    {
                        "authorId": "1712576",
                        "name": "N. Barnes"
                    }
                ]
            }
        },
        {
            "contexts": [
                "They either modify loss term during network training [21], use soft labels [47, 36], or scale down the logits after training [10].",
                "Mixup is similar to label smoothing, where different data-label pairs are mixed to form new data points [36, 47]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "646c8ebbe6a4c3c07d7f74c455b9ff60ffe2dc4a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-12463",
                    "ArXiv": "2307.12463",
                    "DOI": "10.48550/arXiv.2307.12463",
                    "CorpusId": 260125052
                },
                "corpusId": 260125052,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/646c8ebbe6a4c3c07d7f74c455b9ff60ffe2dc4a",
                "title": "Rethinking Data Distillation: Do Not Overlook Calibration",
                "abstract": "Neural networks trained on distilled data often produce over-confident output and require correction by calibration methods. Existing calibration methods such as temperature scaling and mixup work well for networks trained on original large-scale data. However, we find that these methods fail to calibrate networks trained on data distilled from large source datasets. In this paper, we show that distilled data lead to networks that are not calibratable due to (i) a more concentrated distribution of the maximum logits and (ii) the loss of information that is semantically meaningful but unrelated to classification tasks. To address this problem, we propose Masked Temperature Scaling (MTS) and Masked Distillation Training (MDT) which mitigate the limitations of distilled data and achieve better calibration results while maintaining the efficiency of dataset distillation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2210044747",
                        "name": "Dongyao Zhu"
                    },
                    {
                        "authorId": "2144399315",
                        "name": "Bowen Lei"
                    },
                    {
                        "authorId": "51250527",
                        "name": "J Zhang"
                    },
                    {
                        "authorId": "2112742987",
                        "name": "Yanbo Fang"
                    },
                    {
                        "authorId": "1718601",
                        "name": "Ruqi Zhang"
                    },
                    {
                        "authorId": "2149183481",
                        "name": "Yiqun Xie"
                    },
                    {
                        "authorId": "2116459424",
                        "name": "Dongkuan Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-free methods (M. Xu et al., 2023) modify inputs through transformations (Cubuk et al., 2020; Hendrycks et al., 2020), combining datapoints (Mintun et al., 2021; Thulasidasan et al., 2019), or using image-to-image networks (Hendrycks et al., 2021).",
                "To convert ECE to a calibration score, we normalize it by a maximum error ECEmax and subtract the result from 1.",
                "These techniques have demonstrated performance improvements across various metrics, such as distribution-shift robustness (Hendrycks & Dietterich, 2019; Sagawa et al., 2022; Taori et al., 2020), expected calibration error (ECE) (Thulasidasan et al., 2019), and out-of-distribution detection (Thulasidasan et al., 2019).",
                "We use ECEmax = 0.5 as this corresponds with the pathological case of a binary classifier that is correct 50% of the time but always predicts with 100% confidence.",
                "\u2026demonstrated performance improvements across various metrics, such as distribution-shift robustness (Hendrycks & Dietterich, 2019; Sagawa et al., 2022; Taori et al., 2020), expected calibration error (ECE) (Thulasidasan et al., 2019), and out-of-distribution detection (Thulasidasan et al., 2019).",
                "We therefore choose to compute the calibration score by averaging the calibration of the model across the in-distribution and distribution-shifted\ndatasets, giving\nsCAL = 1\u2212 1\n(N + 1)ECEmax\n[ ECE ID +\nN\u2211 i=1 ECE i\n] , (6)\nwhere ECE ID is the expected calibration error on the in-distribution data and ECE i is measured on the ith distribution-shifted dataset.",
                "In this work, since we focus on classification tasks we use ECE .",
                "For example, Guo et al. (2017) explore metrics such as the expected calibration error (ECE ), which is computed by binning the model outputs and averaging the absoluted difference in the accuracy of each bin to the accuracy predicted by an identity function.",
                "They also discuss the negative log-likelihood, a metric also used by Lakshminarayanan et al. (2017) and Gal and Ghahramani (2016), as it is applicable to regression tasks in addition to classification tasks, though ECE has also been extended to the regression setting (Levi et al., 2022)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6609292eed41493bdf0a7cac98bc5cc158d60da8",
                "externalIds": {
                    "ArXiv": "2307.10586",
                    "DBLP": "journals/corr/abs-2307-10586",
                    "DOI": "10.48550/arXiv.2307.10586",
                    "CorpusId": 259991446
                },
                "corpusId": 259991446,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6609292eed41493bdf0a7cac98bc5cc158d60da8",
                "title": "A Holistic Assessment of the Reliability of Machine Learning Systems",
                "abstract": "As machine learning (ML) systems increasingly permeate high-stakes settings such as healthcare, transportation, military, and national security, concerns regarding their reliability have emerged. Despite notable progress, the performance of these systems can significantly diminish due to adversarial attacks or environmental changes, leading to overconfident predictions, failures to detect input faults, and an inability to generalize in unexpected scenarios. This paper proposes a holistic assessment methodology for the reliability of ML systems. Our framework evaluates five key properties: in-distribution accuracy, distribution-shift robustness, adversarial robustness, calibration, and out-of-distribution detection. A reliability score is also introduced and used to assess the overall system reliability. To provide insights into the performance of different algorithmic approaches, we identify and categorize state-of-the-art techniques, then evaluate a selection on real-world tasks using our proposed reliability metrics and reliability score. Our analysis of over 500 models reveals that designing for one metric does not necessarily constrain others but certain algorithmic techniques can improve reliability across multiple metrics simultaneously. This study contributes to a more comprehensive understanding of ML reliability and provides a roadmap for future research and development.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "39405123",
                        "name": "Anthony Corso"
                    },
                    {
                        "authorId": "2224018219",
                        "name": "David Karamadian"
                    },
                    {
                        "authorId": "134684433",
                        "name": "R. Valentin"
                    },
                    {
                        "authorId": "2224018532",
                        "name": "Mary Cooper"
                    },
                    {
                        "authorId": "79262652",
                        "name": "Mykel J. Kochenderfer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several studies (Thulasidasan et al., 2019; Zhang et al., 2021b; Wen et al., 2021; Zhang et al., 2021a; Li et al., 2021b; Chidambaram et al., 2021; Park et al., 2022) have attempted to understand the principles of MSDA and analyze its effects.",
                "Two works (Thulasidasan et al., 2019; Zhang et al., 2021b) have found that Mixup helps to calibrate convolutional neural networks (CNNs) and makes them less over-confident."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e40c65c50b023512fea88503f1b56bd1b32af14b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-09136",
                    "ArXiv": "2307.09136",
                    "DOI": "10.48550/arXiv.2307.09136",
                    "CorpusId": 259950797
                },
                "corpusId": 259950797,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e40c65c50b023512fea88503f1b56bd1b32af14b",
                "title": "DropMix: Reducing Class Dependency in Mixed Sample Data Augmentation",
                "abstract": "Mixed sample data augmentation (MSDA) is a widely used technique that has been found to improve performance in a variety of tasks. However, in this paper, we show that the effects of MSDA are class-dependent, with some classes seeing an improvement in performance while others experience a decline. To reduce class dependency, we propose the DropMix method, which excludes a specific percentage of data from the MSDA computation. By training on a combination of MSDA and non-MSDA data, the proposed method not only improves the performance of classes that were previously degraded by MSDA, but also increases overall average accuracy, as shown in experiments on two datasets (CIFAR-100 and ImageNet) using three MSDA methods (Mixup, CutMix and PuzzleMix).",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220865249",
                        "name": "Haeil Lee"
                    },
                    {
                        "authorId": "13621262",
                        "name": "Han S. Lee"
                    },
                    {
                        "authorId": "1769295",
                        "name": "Junmo Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Additionally, there are trainingstage calibration methods that employ label smoothing [57, 58] or optimize accuracy-uncertainty differentiably [59]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cc8f96f6ff16867d4a3d24eb505b996edd0b1b04",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-07489",
                    "ArXiv": "2307.07489",
                    "DOI": "10.48550/arXiv.2307.07489",
                    "CorpusId": 259924573
                },
                "corpusId": 259924573,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cc8f96f6ff16867d4a3d24eb505b996edd0b1b04",
                "title": "PseudoCal: A Source-Free Approach to Unsupervised Uncertainty Calibration in Domain Adaptation",
                "abstract": "Unsupervised domain adaptation (UDA) has witnessed remarkable advancements in improving the accuracy of models for unlabeled target domains. However, the calibration of predictive uncertainty in the target domain, a crucial aspect of the safe deployment of UDA models, has received limited attention. The conventional in-domain calibration method, \\textit{temperature scaling} (TempScal), encounters challenges due to domain distribution shifts and the absence of labeled target domain data. Recent approaches have employed importance-weighting techniques to estimate the target-optimal temperature based on re-weighted labeled source data. Nonetheless, these methods require source data and suffer from unreliable density estimates under severe domain shifts, rendering them unsuitable for source-free UDA settings. To overcome these limitations, we propose PseudoCal, a source-free calibration method that exclusively relies on unlabeled target data. Unlike previous approaches that treat UDA calibration as a \\textit{covariate shift} problem, we consider it as an unsupervised calibration problem specific to the target domain. Motivated by the factorization of the negative log-likelihood (NLL) objective in TempScal, we generate a labeled pseudo-target set that captures the structure of the real target. By doing so, we transform the unsupervised calibration problem into a supervised one, enabling us to effectively address it using widely-used in-domain methods like TempScal. Finally, we thoroughly evaluate the calibration performance of PseudoCal by conducting extensive experiments on 10 UDA methods, considering both traditional UDA settings and recent source-free UDA scenarios. The experimental results consistently demonstrate the superior performance of PseudoCal, exhibiting significantly reduced calibration error compared to existing calibration methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "49592281",
                        "name": "D. Hu"
                    },
                    {
                        "authorId": "143932869",
                        "name": "Jian Liang"
                    },
                    {
                        "authorId": "48631088",
                        "name": "Xinchao Wang"
                    },
                    {
                        "authorId": "2121484",
                        "name": "Chuan-Sheng Foo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Most related to our approach are techniques that use data augmentations that blend different inputs together [54] or use ensembles to combine representations [32].",
                "[54] Sunil Thulasidasan, Gopinath Chennupati, Jeff Bilmes, Tanmoy Bhattacharya, and Sarah Michalak."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5330d7f63e54da4808f475303c4d0baf9d20de15",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-02245",
                    "ArXiv": "2307.02245",
                    "DOI": "10.48550/arXiv.2307.02245",
                    "CorpusId": 259341690
                },
                "corpusId": 259341690,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5330d7f63e54da4808f475303c4d0baf9d20de15",
                "title": "Set Learning for Accurate and Calibrated Models",
                "abstract": "Model overconfidence and poor calibration are common in machine learning and difficult to account for when applying standard empirical risk minimization. In this work, we propose a novel method to alleviate these problems that we call odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets rather than for single examples. This naturally allows the model to capture correlations across data examples and achieves both better accuracy and calibration, especially in limited training data and class-imbalanced regimes. Perhaps surprisingly, OKO often yields better calibration even when training with hard labels and dropping any additional calibration parameter tuning, such as temperature scaling. We provide theoretical justification, establishing that OKO naturally yields better calibration, and provide extensive experimental analyses that corroborate our theoretical findings. We emphasize that OKO is a general framework that can be easily adapted to many settings and the trained model can be applied to single examples at inference time, without introducing significant run-time overhead or architecture changes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "72543861",
                        "name": "Lukas Muttenthaler"
                    },
                    {
                        "authorId": "49004415",
                        "name": "Robert A. Vandermeulen"
                    },
                    {
                        "authorId": "47834987",
                        "name": "Qiuyi Zhang"
                    },
                    {
                        "authorId": "2465270",
                        "name": "Thomas Unterthiner"
                    },
                    {
                        "authorId": "116099820",
                        "name": "Klaus-Robert Muller"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "558070a76b80346049d1da3b1bd2e8a4dbf766c0",
                "externalIds": {
                    "ArXiv": "2306.15925",
                    "DBLP": "journals/corr/abs-2306-15925",
                    "DOI": "10.48550/arXiv.2306.15925",
                    "CorpusId": 259275174
                },
                "corpusId": 259275174,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/558070a76b80346049d1da3b1bd2e8a4dbf766c0",
                "title": "Subclass-balancing Contrastive Learning for Long-tailed Recognition",
                "abstract": "Long-tailed recognition with imbalanced class distribution naturally emerges in practical machine learning applications. Existing methods such as data reweighing, resampling, and supervised contrastive learning enforce the class balance with a price of introducing imbalance between instances of head class and tail class, which may ignore the underlying rich semantic substructures of the former and exaggerate the biases in the latter. We overcome these drawbacks by a novel ``subclass-balancing contrastive learning (SBCL)'' approach that clusters each head class into multiple subclasses of similar sizes as the tail classes and enforce representations to capture the two-layer class hierarchy between the original classes and their subclasses. Since the clustering is conducted in the representation space and updated during the course of training, the subclass labels preserve the semantic substructures of head classes. Meanwhile, it does not overemphasize tail class samples, so each individual instance contribute to the representation learning equally. Hence, our method achieves both the instance- and subclass-balance, while the original class labels are also learned through contrastive learning among subclasses from different classes. We evaluate SBCL over a list of long-tailed benchmark datasets and it achieves the state-of-the-art performance. In addition, we present extensive analyses and ablation studies of SBCL to verify its advantages.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220734428",
                        "name": "Chengkai Hou"
                    },
                    {
                        "authorId": "47540245",
                        "name": "Jieyu Zhang"
                    },
                    {
                        "authorId": "49528487",
                        "name": "Hong Wang"
                    },
                    {
                        "authorId": "2213956781",
                        "name": "Tianyi Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the one hand, several approaches proposed to modify the training process in order to obtain strongly calibrated predictions (Lakshminarayanan, Pritzel, and Blundell 2017; Thulasidasan et al. 2019; Mukhoti et al. 2020; Tomani and Buettner 2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "85d4f8c0e580f7cdacae5419eb58d4679787f830",
                "externalIds": {
                    "DBLP": "conf/aaai/HeklerB023",
                    "DOI": "10.1609/aaai.v37i12.26735",
                    "CorpusId": 259716317
                },
                "corpusId": 259716317,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/85d4f8c0e580f7cdacae5419eb58d4679787f830",
                "title": "Test Time Augmentation Meets Post-hoc Calibration: Uncertainty Quantification under Real-World Conditions",
                "abstract": "Communicating the predictive uncertainty of deep neural networks transparently and reliably is important in many safety-critical applications such as medicine. However, modern neural networks tend to be poorly calibrated, resulting in wrong predictions made with a high confidence. While existing post-hoc calibration methods like temperature scaling or isotonic regression yield strongly calibrated predictions in artificial experimental settings, their efficiency can significantly reduce in real-world applications, where scarcity of labeled data or domain drifts are commonly present. In this paper, we first investigate the impact of these characteristics on post-hoc calibration and introduce an easy-to-implement extension of common post-hoc calibration methods based on test time augmentation. In extensive experiments, we demonstrate that our approach results in substantially better calibration on various architectures. We demonstrate the robustness of our proposed approach on a real-world application for skin cancer classification and show that it facilitates safe decision-making under real-world uncertainties.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2743174",
                        "name": "A. Hekler"
                    },
                    {
                        "authorId": "47572067",
                        "name": "T. Brinker"
                    },
                    {
                        "authorId": "2315845",
                        "name": "F. Buettner"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e963f31f4fb5b69517ad3dcd5245d3c799211d38",
                "externalIds": {
                    "DOI": "10.3390/app13127198",
                    "CorpusId": 259435553
                },
                "corpusId": 259435553,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e963f31f4fb5b69517ad3dcd5245d3c799211d38",
                "title": "Self-Training with Entropy-Based Mixup for Low-Resource Chest X-ray Classification",
                "abstract": "Deep learning-based medical image analysis technology has been developed to the extent that it shows an accuracy surpassing the ability of a human radiologist in some tasks. However, data labeling on medical images requires human experts and a great deal of time and expense. Moreover, medical image data usually have an imbalanced distribution for each disease. In particular, in multilabel classification, learning with a small number of labeled data causes overfitting problems. The model easily overfits the limited number of labeled data, while it still underfits the large amount of unlabeled data. In this study, we propose a method that combines entropy-based Mixup and self-training to improve the performance of data-imbalanced chest X-ray classification. The proposed method is to apply the Mixup algorithm to limited labeled data to alleviate the data imbalance problem and perform self-training that effectively utilizes the unlabeled data while iterating this process by replacing the teacher model with the student model. Experimental results in an environment with a limited number of labeled data and a large number of unlabeled data showed that the classification performance was improved by combining entropy-based Mixup and self-training.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "152880325",
                        "name": "Minkyu Park"
                    },
                    {
                        "authorId": "2109174511",
                        "name": "Juntae Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This may include using specialized loss functions tailored for calibration[45], employing ensemble methods[26, 46], or data augmentation[47], etc."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "aecafe836bc2fda034e16ad3fdca7b7e1c8fd019",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-04985",
                    "ArXiv": "2306.04985",
                    "DOI": "10.48550/arXiv.2306.04985",
                    "CorpusId": 259108795
                },
                "corpusId": 259108795,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/aecafe836bc2fda034e16ad3fdca7b7e1c8fd019",
                "title": "Beyond Probability Partitions: Calibrating Neural Networks with Semantic Aware Grouping",
                "abstract": "Research has shown that deep networks tend to be overly optimistic about their predictions, leading to an underestimation of prediction errors. Due to the limited nature of data, existing studies have proposed various methods based on model prediction probabilities to bin the data and evaluate calibration error. We propose a more generalized definition of calibration error called Partitioned Calibration Error (PCE), revealing that the key difference among these calibration error metrics lies in how the data space is partitioned. We put forth an intuitive proposition that an accurate model should be calibrated across any partition, suggesting that the input space partitioning can extend beyond just the partitioning of prediction probabilities, and include partitions directly related to the input. Through semantic-related partitioning functions, we demonstrate that the relationship between model accuracy and calibration lies in the granularity of the partitioning function. This highlights the importance of partitioning criteria for training a calibrated and accurate model. To validate the aforementioned analysis, we propose a method that involves jointly learning a semantic aware grouping function based on deep model features and logits to partition the data space into subsets. Subsequently, a separate calibration function is learned for each subset. Experimental results demonstrate that our approach achieves significant performance improvements across multiple datasets and network architectures, thus highlighting the importance of the partitioning function for calibration.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1510730409",
                        "name": "Jia-Qi Yang"
                    },
                    {
                        "authorId": "1721819",
                        "name": "De-chuan Zhan"
                    },
                    {
                        "authorId": "2147161808",
                        "name": "Le Gan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "MixUp (Thulasidasan et al., 2019b).",
                "MixUp (Thulasidasan et al., 2019b), and Convex (Zhan et al., 2021) use synthesized negative instances to regularize the model.",
                "Zhan et al. (2021) use MixUp technique (Thulasidasan et al., 2019a) to synthesize known data into unknown data."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "91d6ad7498ce073472d8ff03a9a803f573d0fddd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-04950",
                    "ACL": "2023.acl-long.525",
                    "ArXiv": "2306.04950",
                    "DOI": "10.48550/arXiv.2306.04950",
                    "CorpusId": 259108955
                },
                "corpusId": 259108955,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/91d6ad7498ce073472d8ff03a9a803f573d0fddd",
                "title": "Open Set Relation Extraction via Unknown-Aware Training",
                "abstract": "The existing supervised relation extraction methods have achieved impressive performance in a closed-set setting, in which the relations remain the same during both training and testing. In a more realistic open-set setting, unknown relations may appear in the test set. Due to the lack of supervision signals from unknown relations, a well-performing closed-set relation extractor can still confidently misclassify them into known relations. In this paper, we propose an unknown-aware training method, regularizing the model by dynamically synthesizing negative instances that can provide the missing supervision signals. Inspired by text adversarial attack, We adaptively apply small but critical perturbations to original training data,synthesizing difficult enough negative instances that are mistaken by the model as known relations, thus facilitating a compact decision boundary. Experimental results show that our method achieves SOTA unknown relation detection without compromising the classification of known relations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2145804572",
                        "name": "Jun Zhao"
                    },
                    {
                        "authorId": "2145734826",
                        "name": "Xin Zhao"
                    },
                    {
                        "authorId": "2187461272",
                        "name": "Wenyu Zhan"
                    },
                    {
                        "authorId": "47835189",
                        "name": "Qi Zhang"
                    },
                    {
                        "authorId": "2067331064",
                        "name": "Tao Gui"
                    },
                    {
                        "authorId": "2118602528",
                        "name": "Zhongyu Wei"
                    },
                    {
                        "authorId": "2144861665",
                        "name": "Yunwen Chen"
                    },
                    {
                        "authorId": "2149398438",
                        "name": "Xiang Gao"
                    },
                    {
                        "authorId": "1790227",
                        "name": "Xuanjing Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, due to the exponential function employed in the softmax layer, the trained deep neural network often produces high confidence scores even for misclassified samples, as studied extensively in [1, 4, 24]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7fb2a48f07c23233f3f22755d6b926f465695cc5",
                "externalIds": {
                    "DOI": "10.1117/12.2680634",
                    "CorpusId": 259120064
                },
                "corpusId": 259120064,
                "publicationVenue": {
                    "id": "c4e26c05-cc78-4a34-9cc8-567aa54a8877",
                    "name": "International Conference on Machine Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Mach Vis",
                        "ICMV"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7fb2a48f07c23233f3f22755d6b926f465695cc5",
                "title": "Face attribute classification with evidential deep learning",
                "abstract": "We address the problem of uncertainty quantification in the domain of face attribute classification, using Evidential Deep Learning (EDL) framework. The proposed EDL approach leverages the strength of Convolution Neural Networks (CNN), with the objective of representing the uncertainty in the output predictions. Predominantly, the softmax/sigmoid activation functions are applied to map the output logits of the CNN to target class probabilities in multi-class classification problems. By replacing the standard softmax/sigmoid output of a CNN with the parameters of the evidential distribution, EDL learns to represent the uncertainty in its predictions. The proposed approach is evaluated on CelebA and LFWA datasets. The quantitative and qualitative analysis demonstrate the suitability and strength of EDL to estimate the uncertainty in the output predictions without hindering the accuracy of CNN-based models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219778109",
                        "name": "Arin Zeyneloglu"
                    },
                    {
                        "authorId": "2219777986",
                        "name": "Sara Atito Ali Ahmed"
                    },
                    {
                        "authorId": "1757030",
                        "name": "B. Yanikoglu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This method has demonstrated its effectiveness in enhancing the generalization and robustness of deep learning models for image classification tasks [16]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f67035a3798e943ad9ceb5481d8961fb7c56d193",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-01310",
                    "ArXiv": "2306.01310",
                    "DOI": "10.48550/arXiv.2306.01310",
                    "CorpusId": 259063783
                },
                "corpusId": 259063783,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f67035a3798e943ad9ceb5481d8961fb7c56d193",
                "title": "EPIC: Graph Augmentation with Edit Path Interpolation via Learnable Cost",
                "abstract": "Graph-based models have become increasingly important in various domains, but the limited size and diversity of existing graph datasets often limit their performance. To address this issue, we propose EPIC (Edit Path Interpolation via learnable Cost), a novel interpolation-based method for augmenting graph datasets. Our approach leverages graph edit distance to generate new graphs that are similar to the original ones but exhibit some variation in their structures. To achieve this, we learn the graph edit distance through a comparison of labeled graphs and utilize this knowledge to create graph edit paths between pairs of original graphs. With randomly sampled graphs from a graph edit path, we enrich the training set to enhance the generalization capability of classification models. We demonstrate the effectiveness of our approach on several benchmark datasets and show that it outperforms existing augmentation methods in graph classification tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2218725226",
                        "name": "Jaeseung Heo"
                    },
                    {
                        "authorId": "2218943824",
                        "name": "Seungbeom Lee"
                    },
                    {
                        "authorId": "70560338",
                        "name": "Sungsoo Ahn"
                    },
                    {
                        "authorId": "2145138660",
                        "name": "Dongwoo Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As we mentioned in above sections, there are some contradictory results on mixup\u2019s calibration performance in previous studies [16,27].",
                "[27] empirically found that mixup improves calibration across various model architectures and datasets.",
                "Most of these existing studies try to improve calibration performance with the regularization effect induced by mixup [8,16,27,38].",
                "We notice there are some contradictory results on mixup\u2019s calibration performance in previous studies [16, 27]."
            ],
            "intents": [
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ca1c5ff2a7bb55f1960c36915710e36a6528205a",
                "externalIds": {
                    "DBLP": "conf/cvpr/WangLZHZ23",
                    "DOI": "10.1109/CVPR52729.2023.00735",
                    "CorpusId": 260833710
                },
                "corpusId": 260833710,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ca1c5ff2a7bb55f1960c36915710e36a6528205a",
                "title": "On the Pitfall of Mixup for Uncertainty Calibration",
                "abstract": "By simply taking convex combinations between pairs of samples and their labels, mixup training has been shown to easily improve predictive accuracy. It has been recently found that models trained with mixup also perform well on uncertainty calibration. However, in this study, we found that mixup training usually makes models less calibratable than vanilla empirical risk minimization, which means that it would harm uncertainty estimation when post-hoc calibration is considered. By decomposing the mixup process into data transformation and random perturbation, we suggest that the confidence penalty nature of the data transformation is the reason of calibration degradation. To mitigate this problem, we first investigate the mixup inference strategy and found that despite it improves calibration on mixup, this ensemble-like strategy does not necessarily out-perform simple ensemble. Then, we propose a general strategy named mixup inference in training, which adopts a simple decoupling principle for recovering the outputs of raw samples at the end of forward network pass. By embedding the mixup inference, models can be learned from the original one-hot labels and hence avoid the negative impact of confidence penalty. Our experiments show this strategy properly solves mixup's calibration issue without sacrificing the predictive performance, while even improves accuracy than vanilla mixup.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2145350804",
                        "name": "Deng-Bao Wang"
                    },
                    {
                        "authorId": "2117007545",
                        "name": "Lanqing Li"
                    },
                    {
                        "authorId": "144259957",
                        "name": "P. Zhao"
                    },
                    {
                        "authorId": "1714602",
                        "name": "P. Heng"
                    },
                    {
                        "authorId": "3039887",
                        "name": "Min-Ling Zhang"
                    },
                    {
                        "authorId": "82496181",
                        "name": "M. Acc"
                    },
                    {
                        "authorId": "2231441947",
                        "name": "Ece MIp"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Follow-up works (Thulasidasan et al., 2019; Rahaman & Thiery, 2021; Wen et al., 2021) have supported these findings, although the recent work of Minderer et al.",
                "Obtaining such models with good predictive uncertainty is the problem of model calibration, and has seen a flurry of recent work in the context of training deep learning models (Guo et al., 2017; Thulasidasan et al., 2019; Ovadia et al., 2019; Wen et al., 2020; Minderer et al., 2021).",
                ", 2017), data augmentation (Thulasidasan et al., 2019; M\u00fcller et al., 2020), ensembling (Lakshminarayanan et al.",
                "However, several empirical works have shown that temperature scaling alone can be outperformed by training-time modifications such as data augmentation (Thulasidasan et al., 2019; M\u00fcller et al., 2020) and regularized loss functions (Kumar et al."
            ],
            "intents": [
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "3815876dd18c8d338b6ea0eb74915548116d20de",
                "externalIds": {
                    "ArXiv": "2306.00740",
                    "CorpusId": 258999368
                },
                "corpusId": 258999368,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3815876dd18c8d338b6ea0eb74915548116d20de",
                "title": "On the Limitations of Temperature Scaling for Distributions with Overlaps",
                "abstract": "Despite the impressive generalization capabilities of deep neural networks, they have been repeatedly shown to be overconfident when they are wrong. Fixing this issue is known as model calibration, and has consequently received much attention in the form of modified training schemes and post-training calibration procedures such as temperature scaling. While temperature scaling is frequently used because of its simplicity, it is often outperformed by modified training schemes. In this work, we identify a specific bottleneck for the performance of temperature scaling. We show that for empirical risk minimizers for a general set of distributions in which the supports of classes have overlaps, the performance of temperature scaling degrades with the amount of overlap between classes, and asymptotically becomes no better than random when there are a large number of classes. On the other hand, we prove that optimizing a modified form of the empirical risk induced by the Mixup data augmentation technique can in fact lead to reasonably good calibration performance, showing that training-time calibration may be necessary in some situations. We also verify that our theoretical results reflect practice by showing that Mixup significantly outperforms empirical risk minimization (with respect to multiple calibration metrics) on image classification benchmarks with class overlaps introduced in the form of label noise.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2065619709",
                        "name": "Muthuraman Chidambaram"
                    },
                    {
                        "authorId": "144804200",
                        "name": "Rong Ge"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026limited to improved generalization performance in supervised learning; they are known to be helpful in other aspects including model calibration (Thulasidasan et al., 2019), semi-supervised learning (Berthelot et al., 2019; Sohn et al., 2020), contrastive learning (Kalantidis et al., 2020;\u2026",
                "The success of Mixup-style training schemes is not only limited to improved generalization performance in supervised learning; they are known to be helpful in other aspects including model calibration (Thulasidasan et al., 2019), semi-supervised learning (Berthelot et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0b559e2cc541196c2840f3ea8e3d8b0d8d274026",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-00267",
                    "ArXiv": "2306.00267",
                    "DOI": "10.48550/arXiv.2306.00267",
                    "CorpusId": 258999856
                },
                "corpusId": 258999856,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0b559e2cc541196c2840f3ea8e3d8b0d8d274026",
                "title": "Provable Benefit of Mixup for Finding Optimal Decision Boundaries",
                "abstract": "We investigate how pair-wise data augmentation techniques like Mixup affect the sample complexity of finding optimal decision boundaries in a binary linear classification problem. For a family of data distributions with a separability constant $\\kappa$, we analyze how well the optimal classifier in terms of training loss aligns with the optimal one in test accuracy (i.e., Bayes optimal classifier). For vanilla training without augmentation, we uncover an interesting phenomenon named the curse of separability. As we increase $\\kappa$ to make the data distribution more separable, the sample complexity of vanilla training increases exponentially in $\\kappa$; perhaps surprisingly, the task of finding optimal decision boundaries becomes harder for more separable distributions. For Mixup training, we show that Mixup mitigates this problem by significantly reducing the sample complexity. To this end, we develop new concentration results applicable to $n^2$ pair-wise augmented data points constructed from $n$ independent data, by carefully dealing with dependencies between overlapping pairs. Lastly, we study other masking-based Mixup-style techniques and show that they can distort the training loss and make its minimizer converge to a suboptimal classifier in terms of test accuracy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2218729503",
                        "name": "Junsoo Oh"
                    },
                    {
                        "authorId": "2218718711",
                        "name": "Chulee Yun"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f0465919ff430d52c8647f03c41c85733e08665f",
                "externalIds": {
                    "DBLP": "journals/cbm/ZhangLZYLL23",
                    "DOI": "10.1016/j.compbiomed.2023.107079",
                    "CorpusId": 259050940,
                    "PubMed": "37321100"
                },
                "corpusId": 259050940,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f0465919ff430d52c8647f03c41c85733e08665f",
                "title": "A new weakly supervised deep neural network for recognizing Alzheimer's disease",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108065634",
                        "name": "Xiaobo Zhang"
                    },
                    {
                        "authorId": "2116785555",
                        "name": "Zhimin Li"
                    },
                    {
                        "authorId": "2145946075",
                        "name": "Qian Zhang"
                    },
                    {
                        "authorId": "2219482708",
                        "name": "Zegang Yin"
                    },
                    {
                        "authorId": "2110327235",
                        "name": "Zhijie Lu"
                    },
                    {
                        "authorId": "2153494216",
                        "name": "Yang Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The major branch of OOD detection is classification-based methods [41, 42] including confidence enhancement methods [1,31], outlier exposure [2,7,26,44] and post-hoc detection [10\u201312,14,21,23,29,36]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c011db58d6808b05ca948f4a6f70ff554ccb7469",
                "externalIds": {
                    "DBLP": "conf/cvpr/ZhangX23",
                    "DOI": "10.1109/CVPR52729.2023.00330",
                    "CorpusId": 260085493
                },
                "corpusId": 260085493,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c011db58d6808b05ca948f4a6f70ff554ccb7469",
                "title": "Decoupling MaxLogit for Out-of-Distribution Detection",
                "abstract": "In machine learning, it is often observed that standard training outputs anomalously high confidence for both in-distribution (ID) and out-of-distribution (OOD) data. Thus, the ability to detect OOD samples is critical to the model deployment. An essential step for OOD detection is post-hoc scoring. MaxLogit is one of the simplest scoring functions which uses the maximum logits as OOD score. To provide a new viewpoint to study the logit-based scoring function, we reformulate the logit into cosine similarity and logit norm and propose to use MaxCosine and MaxNorm. We empirically find that MaxCosine is a core factor in the effectiveness of MaxLogit. And the performance of MaxLogit is encumbered by MaxNorm. To tackle the problem, we propose the Decoupling MaxLogit (DML) for flexibility to balance MaxCosine and MaxNorm. To further embody the core of our method, we extend DML to DML+ based on the new insights that fewer hard samples and compact feature space are the key components to make logit-based methods effective. We demonstrate the effectiveness of our logit-based OOD detection methods on CIFAR-10, CIFAR-100 and ImageNet and establish state-of-the-art performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2164215120",
                        "name": "Zihan Zhang"
                    },
                    {
                        "authorId": "2166045582",
                        "name": "Xiang Xiang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "41f9a327f5c93720a1b804da5c375c44e2b68df1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-15141",
                    "ArXiv": "2308.15141",
                    "DOI": "10.1016/j.media.2023.102861",
                    "CorpusId": 259039081,
                    "PubMed": "37327613"
                },
                "corpusId": 259039081,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/41f9a327f5c93720a1b804da5c375c44e2b68df1",
                "title": "Uncertainty aware training to improve deep learning model calibration for classification of cardiac MR images",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2127727576",
                        "name": "Tareen Dawood"
                    },
                    {
                        "authorId": "2127379721",
                        "name": "Chen Chen"
                    },
                    {
                        "authorId": "35322898",
                        "name": "B. Sidhu"
                    },
                    {
                        "authorId": "3583803",
                        "name": "B. Ruijsink"
                    },
                    {
                        "authorId": "40150919",
                        "name": "J. Gould"
                    },
                    {
                        "authorId": "12425011",
                        "name": "B. Porter"
                    },
                    {
                        "authorId": "1783059361",
                        "name": "M. Elliott"
                    },
                    {
                        "authorId": "2059882311",
                        "name": "Vishal S. Mehta"
                    },
                    {
                        "authorId": "2550617",
                        "name": "C. Rinaldi"
                    },
                    {
                        "authorId": "1398910685",
                        "name": "E. Puyol-Ant\u00f3n"
                    },
                    {
                        "authorId": "145987167",
                        "name": "R. Razavi"
                    },
                    {
                        "authorId": "144424809",
                        "name": "A. King"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5e44774b2143152b0f5021a700a1c746314bee44",
                "externalIds": {
                    "DBLP": "conf/cvpr/MaLZGZG023",
                    "DOI": "10.1109/CVPR52729.2023.01102",
                    "CorpusId": 260783175
                },
                "corpusId": 260783175,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5e44774b2143152b0f5021a700a1c746314bee44",
                "title": "Annealing-based Label-Transfer Learning for Open World Object Detection",
                "abstract": "Open world object detection (OWOD) has attracted extensive attention due to its practicability in the real world. Previous OWOD works manually designed unknown-discover strategies to select unknown proposals from the background, suffering from uncertainties without appropriate priors. In this paper, we claim the learning of object detection could be seen as an object-level feature-entanglement process, where unknown traits are propagated to the known proposals through convolutional operations and could be distilled to benefit unknown recognition without manual selection. Therefore, we propose a simple yet effective Annealing-based Label-Transfer framework, which sufficiently explores the known proposals to alleviate the uncertainties. Specifically, a Label-Transfer Learning paradigm is introduced to decouple the known and unknown features, while a Sawtooth Annealing Scheduling strategy is further employed to rebuild the decision boundaries of the known and unknown classes, thus promoting both known and unknown recognition. Moreover, previous OWOD works neglected the trade-off of known and unknown performance, and we thus introduce a metric called Equilibrium Index to comprehensively evaluate the effectiveness of the OWOD models. To the best of our knowledge, this is the first OWOD work without manual unknown selection. Extensive experiments conducted on the common-used benchmark validate that our model achieves superior detection performance (200% unknown mAP improvement with the even higher known detection performance) compared to other state-of-the-art methods. Our code is available at https://github.com/DIG-Beihang/ALLOW.git.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47009264",
                        "name": "Yuqing Ma"
                    },
                    {
                        "authorId": "2109024995",
                        "name": "Hainan Li"
                    },
                    {
                        "authorId": "2161262148",
                        "name": "Zhange Zhang"
                    },
                    {
                        "authorId": "6798639",
                        "name": "Jinyang Guo"
                    },
                    {
                        "authorId": "2437353",
                        "name": "Shanghang Zhang"
                    },
                    {
                        "authorId": "152217579",
                        "name": "Ruihao Gong"
                    },
                    {
                        "authorId": "6820648",
                        "name": "Xianglong Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fbd177906d89af4c76700033097b00f36b29047e",
                "externalIds": {
                    "DBLP": "conf/cvpr/PengLCXH23",
                    "ArXiv": "2305.19498",
                    "DOI": "10.1109/CVPR52729.2023.01027",
                    "CorpusId": 258987967
                },
                "corpusId": 258987967,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fbd177906d89af4c76700033097b00f36b29047e",
                "title": "Perception and Semantic Aware Regularization for Sequential Confidence Calibration",
                "abstract": "Deep sequence recognition (DSR) models receive increasing attention due to their superior application to various applications. Most DSR models use merely the target sequences as supervision without considering other related sequences, leading to over-confidence in their predictions. The DSR models trained with label smoothing regularize labels by equally and independently smoothing each token, reallocating a small value to other tokens for mitigating overconfidence. However, they do not consider tokens/sequences correlations that may provide more effective information to regularize training and thus lead to sub-optimal performance. In this work, we find tokens/sequences with high perception and semantic correlations with the target ones contain more correlated and effective information and thus facilitate more effective regularization. To this end, we propose a Perception and Semantic aware Sequence Regularization framework, which explore perceptively and semantically correlated tokens/sequences as regularization. Specifically, we introduce a semantic context-free recognition and a language model to acquire similar sequences with high perceptive similarities and semantic correlation, respectively. Moreover, over-confidence degree varies across samples according to their difficulties. Thus, we further design an adaptive calibration intensity module to compute a difficulty score for each samples to obtain finer-grained regularization. Extensive experiments on canonical sequence recognition tasks, including scene text and speech recognition, demonstrate that our method sets novel state-of-the-art results. Code is available at https://github.com/husterpzh/PSSR.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "13751637",
                        "name": "Zhenghua Peng"
                    },
                    {
                        "authorId": "2187161729",
                        "name": "Yuanmao Luo"
                    },
                    {
                        "authorId": "1765674",
                        "name": "Tianshui Chen"
                    },
                    {
                        "authorId": "2218982267",
                        "name": "Keke Xu"
                    },
                    {
                        "authorId": "1860840",
                        "name": "Shuangping Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Regularization techniques are generally found to be effective for calibrating DNNs. Data augmentation methods, such as Mixup (Thulasidasan et al., 2019) and AugMix (Hendrycks et al., 2019), train DNNs on mixed\nsamples and have been found to reduce the tendency to make over-confident predictions.",
                "Data augmentation methods, such as Mixup (Thulasidasan et al., 2019) and AugMix (Hendrycks et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8a78bb21fd7e7c4577376eebf09a08045219302c",
                "externalIds": {
                    "ArXiv": "2305.13665",
                    "DBLP": "conf/icml/TaoD023",
                    "DOI": "10.48550/arXiv.2305.13665",
                    "CorpusId": 258841119
                },
                "corpusId": 258841119,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8a78bb21fd7e7c4577376eebf09a08045219302c",
                "title": "Dual Focal Loss for Calibration",
                "abstract": "The use of deep neural networks in real-world applications require well-calibrated networks with confidence scores that accurately reflect the actual probability. However, it has been found that these networks often provide over-confident predictions, which leads to poor calibration. Recent efforts have sought to address this issue by focal loss to reduce over-confidence, but this approach can also lead to under-confident predictions. While different variants of focal loss have been explored, it is difficult to find a balance between over-confidence and under-confidence. In our work, we propose a new loss function by focusing on dual logits. Our method not only considers the ground truth logit, but also take into account the highest logit ranked after the ground truth logit. By maximizing the gap between these two logits, our proposed dual focal loss can achieve a better balance between over-confidence and under-confidence. We provide theoretical evidence to support our approach and demonstrate its effectiveness through evaluations on multiple models and datasets, where it achieves state-of-the-art performance. Code is available at https://github.com/Linwei94/DualFocalLoss",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "34697970",
                        "name": "Linwei Tao"
                    },
                    {
                        "authorId": "151498428",
                        "name": "Minjing Dong"
                    },
                    {
                        "authorId": "2172076945",
                        "name": "Changming Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several works have observed that the improved performance of Mixup can be attributed to better network calibration [48] and out-of-manifold regularisation [17]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7aa53e5f3764447b228a0b82fc96df717ec02c10",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-10293",
                    "ArXiv": "2305.10293",
                    "DOI": "10.48550/arXiv.2305.10293",
                    "CorpusId": 258741413
                },
                "corpusId": 258741413,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7aa53e5f3764447b228a0b82fc96df717ec02c10",
                "title": "Infinite Class Mixup",
                "abstract": "Mixup is a widely adopted strategy for training deep networks, where additional samples are augmented by interpolating inputs and labels of training pairs. Mixup has shown to improve classification performance, network calibration, and out-of-distribution generalisation. While effective, a cornerstone of Mixup, namely that networks learn linear behaviour patterns between classes, is only indirectly enforced since the output interpolation is performed at the probability level. This paper seeks to address this limitation by mixing the classifiers directly instead of mixing the labels for each mixed pair. We propose to define the target of each augmented sample as a uniquely new classifier, whose parameters are a linear interpolation of the classifier vectors of the input pair. The space of all possible classifiers is continuous and spans all interpolations between classifier pairs. To make optimisation tractable, we propose a dual-contrastive Infinite Class Mixup loss, where we contrast the classifier of a mixed pair to both the classifiers and the predicted outputs of other mixed pairs in a batch. Infinite Class Mixup is generic in nature and applies to many variants of Mixup. Empirically, we show that it outperforms standard Mixup and variants such as RegMixup and Remix on balanced, long-tailed, and data-constrained benchmarks, highlighting its broad applicability.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1722052",
                        "name": "Thomas Mensink"
                    },
                    {
                        "authorId": "40892875",
                        "name": "P. Mettes"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "209572f14546f5b7a88a22f5e21e17c5e29ae30d",
                "externalIds": {
                    "DBLP": "journals/ipm/ZhanSLH23",
                    "DOI": "10.1016/j.ipm.2022.103258",
                    "CorpusId": 255735482
                },
                "corpusId": 255735482,
                "publicationVenue": {
                    "id": "37f5b9b7-f828-4ae1-a174-45b538cbd4e4",
                    "name": "Information Processing & Management",
                    "type": "journal",
                    "alternate_names": [
                        "Inf Process Manag",
                        "Inf Process  Manag",
                        "Information Processing and Management"
                    ],
                    "issn": "0306-4573",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/244/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/information-processing-and-management/",
                        "http://www.sciencedirect.com/science/journal/03064573",
                        "http://www.journals.elsevier.com/information-processing-and-management/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/209572f14546f5b7a88a22f5e21e17c5e29ae30d",
                "title": "IGCNN-FC: Boosting interpretability and generalization of convolutional neural networks for few chest X-rays analysis",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1500650084",
                        "name": "Mengmeng Zhan"
                    },
                    {
                        "authorId": "2766473",
                        "name": "Xiaoshuang Shi"
                    },
                    {
                        "authorId": "50207815",
                        "name": "Fangqi Liu"
                    },
                    {
                        "authorId": "4478507",
                        "name": "Rongyao Hu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "04c9bfee3564858f39a1af025de23ac59e9ea6e2",
                "externalIds": {
                    "DBLP": "journals/tcsv/ZhangM23",
                    "DOI": "10.1109/TCSVT.2022.3219339",
                    "CorpusId": 253354796
                },
                "corpusId": 253354796,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/04c9bfee3564858f39a1af025de23ac59e9ea6e2",
                "title": "A Good Data Augmentation Policy is not All You Need: A Multi-Task Learning Perspective",
                "abstract": "Data augmentation, which improves the diversity of datasets by applying image transformations, has become one of the most effective techniques in visual representation learning. Usually, the design of augmentation policies faces a diversity-difficulty trade-off. On the one hand, a simple augmentation leads to a low training set diversity, which can not improve model performance significantly. On the other hand, an excessively hard augmentation has an overlarge regularization effect which harms model performance. Recently, automatic augmentation methods have been proposed to address this issue by searching the optimal data augmentation policy from a predefined searching space. However, these methods still suffer from heavy searching overhead or complex optimization objectives. In this paper, instead of searching the optimal augmentation policy, we propose to break the diversity-difficulty trade-off from a multi-task learning perspective. By formulating model learning on the augmented images and the original images as the auxiliary task and the primary task in multi-task learning respectively, the hard augmentation does not directly influence the training of the primary branch and thus its negative influence can be alleviated. Hence, neural networks can learn valuable semantic information even with a totally random augmentation policy. Experimental results on ten datasets for four tasks demonstrate the superiority of our method over the other twelve methods. Codes have been released in https://github.com/ArchipLab-LinfengZhang/data-augmentation-multi-task.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50081570",
                        "name": "Linfeng Zhang"
                    },
                    {
                        "authorId": "2075321204",
                        "name": "Kaisheng Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In standard training (ST), mixup has been widely used to improve the generalization of models [33], [34], [46], [47], [48], [49]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "79b2a45d0bb8727b34fdc78c6442a2ef3bc025f9",
                "externalIds": {
                    "DBLP": "journals/tdsc/ChenZXLCHC23",
                    "DOI": "10.1109/TDSC.2022.3165889",
                    "CorpusId": 248060298
                },
                "corpusId": 248060298,
                "publicationVenue": {
                    "id": "d286fdd0-3b6c-433c-afee-87228d8e9f93",
                    "name": "IEEE Transactions on Dependable and Secure Computing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Dependable Secur Comput"
                    ],
                    "issn": "1545-5971",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=8858"
                },
                "url": "https://www.semanticscholar.org/paper/79b2a45d0bb8727b34fdc78c6442a2ef3bc025f9",
                "title": "Decision Boundary-Aware Data Augmentation for Adversarial Training",
                "abstract": "Adversarial training (AT) is a typical method to learn adversarially robust deep neural networks via training on the adversarial variants generated by their natural examples. However, as training progresses, the training data becomes less attackable, which may undermine the enhancement of model robustness. A straightforward remedy is to incorporate more training data, but it may incur an unaffordable cost. To mitigate this issue, in this paper, we propose a deCisiOn bounDary-aware data Augmentation framework (CODA): in each epoch, the CODA directly employs the meta information of the previous epoch to guide the augmentation process and generate more data that are close to the decision boundary, i.e., attackable data. Compared with the vanilla mixup, our proposed CODA can provide a higher ratio of attackable data, which is beneficial to enhance model robustness; it meanwhile mitigates the model's linear behavior between classes, where the linear behavior is favorable to the standard training for generalization but not to the adversarial training for robustness. As a result, our proposed CODA encourages the model to predict invariantly in the cluster of each class. Experiments demonstrate that our proposed CODA can indeed enhance adversarial robustness across various adversarial training methods and multiple datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1828568",
                        "name": "Chen Chen"
                    },
                    {
                        "authorId": "47539929",
                        "name": "Jingfeng Zhang"
                    },
                    {
                        "authorId": "1507120550",
                        "name": "Xilie Xu"
                    },
                    {
                        "authorId": "3366777",
                        "name": "L. Lyu"
                    },
                    {
                        "authorId": "2145762399",
                        "name": "Chaochao Chen"
                    },
                    {
                        "authorId": "2615734",
                        "name": "Tianlei Hu"
                    },
                    {
                        "authorId": "1887510095",
                        "name": "Gang Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8e82f2b0e7ca92592e7532611280277177712ed2",
                "externalIds": {
                    "DBLP": "journals/sigpro/WangZLZWX23",
                    "DOI": "10.1016/j.sigpro.2023.109116",
                    "CorpusId": 258853073
                },
                "corpusId": 258853073,
                "publicationVenue": {
                    "id": "20298d29-1b7a-4631-8e1d-e0a5d34bcf58",
                    "name": "Signal Processing",
                    "type": "journal",
                    "alternate_names": [
                        "Signal Process"
                    ],
                    "issn": "0165-1684",
                    "url": "https://www.journals.elsevier.com/signal-processing",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/01651684"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8e82f2b0e7ca92592e7532611280277177712ed2",
                "title": "Adversarial MixUp with implicit semantic preservation for semi-supervised hyperspectral image classification",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2250564",
                        "name": "Leiquan Wang"
                    },
                    {
                        "authorId": "2112262402",
                        "name": "Jialiang Zhou"
                    },
                    {
                        "authorId": "1791160",
                        "name": "Zhongwei Li"
                    },
                    {
                        "authorId": "2145734567",
                        "name": "Xin Zhao"
                    },
                    {
                        "authorId": "46740305",
                        "name": "Chunlei Wu"
                    },
                    {
                        "authorId": "2218360378",
                        "name": "M. Xu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "61c9331d2ec181e5e48fe99388b3b4fd289860ec",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-00543",
                    "ArXiv": "2305.00543",
                    "DOI": "10.48550/arXiv.2305.00543",
                    "CorpusId": 258426367
                },
                "corpusId": 258426367,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/61c9331d2ec181e5e48fe99388b3b4fd289860ec",
                "title": "Calibration Error Estimation Using Fuzzy Binning",
                "abstract": "Neural network-based decisions tend to be overconfident, where their raw outcome probabilities do not align with the true decision probabilities. Calibration of neural networks is an essential step towards more reliable deep learning frameworks. Prior metrics of calibration error primarily utilize crisp bin membership-based measures. This exacerbates skew in model probabilities and portrays an incomplete picture of calibration error. In this work, we propose a Fuzzy Calibration Error metric (FCE) that utilizes a fuzzy binning approach to calculate calibration error. This approach alleviates the impact of probability skew and provides a tighter estimate while measuring calibration error. We compare our metric with ECE across different data populations and class memberships. Our results show that FCE offers better calibration error estimation, especially in multi-class settings, alleviating the effects of skew in model confidence scores on calibration error estimation. We make our code and supplementary materials available at: https://github.com/bihani-g/fce",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2037482974",
                        "name": "Geetanjali Bihani"
                    },
                    {
                        "authorId": "10681993",
                        "name": "J. Rayz"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2709ca7273d83f7366ecc38154f6e5f5e1c901e1",
                "externalIds": {
                    "DBLP": "conf/ijcai/YuNJR23",
                    "ArXiv": "2304.14623",
                    "DOI": "10.48550/arXiv.2304.14623",
                    "CorpusId": 258418220
                },
                "corpusId": 258418220,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/2709ca7273d83f7366ecc38154f6e5f5e1c901e1",
                "title": "Quality-agnostic Image Captioning to Safely Assist People with Vision Impairment",
                "abstract": "Automated image captioning has the potential to be a useful tool for people with vision impairments. Images taken by this user group are often noisy, which leads to incorrect and even unsafe model predictions. In this paper, we propose a quality-agnostic framework to improve the performance and robustness of image captioning models for visually impaired people. We address this problem from three angles: data, model, and evaluation. First, we show how data augmentation techniques for generating synthetic noise can address data sparsity in this domain. Second, we enhance the robustness of the model by expanding a state-of-the-art model to a dual network architecture, using the augmented data and leveraging different consistency losses. Our results demonstrate increased performance, e.g. an absolute improvement of 2.15 on CIDEr, compared to state-of-the-art image captioning networks, as well as increased robustness to noise with up to 3 points improvement on CIDEr in more noisy settings. Finally, we evaluate the prediction reliability using confidence calibration on images with different difficulty / noise levels, showing that our models perform more reliably\n\nin safety-critical situations. The improved model is part of an assisted living application, which we develop in partnership with the Royal National Institute of Blind People.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2175073154",
                        "name": "Lu Yu"
                    },
                    {
                        "authorId": "2133187991",
                        "name": "Malvina Nikandrou"
                    },
                    {
                        "authorId": "2115758481",
                        "name": "Jiali Jin"
                    },
                    {
                        "authorId": "1681799",
                        "name": "Verena Rieser"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In this research, we evaluate the calibration using the Expected Calibration Error (ECE) metric which measures the deviation be-\ntween predicted confidence and accuracy.",
                "The third one is Calibration (Guo et al., 2017; Kumar et al., 2019; Thulasidasan et al., 2019; Minderer et al., 2021), which measures the predictive uncertainty of a model, and we use this metric to assess if ChatGPT is overconfidence on its prediction.",
                "The third one is Calibration (Guo et al., 2017; Kumar et al., 2019; Thulasidasan et al., 2019; Minderer et al., 2021), which measures the predictive uncertainty of a model, and we use this metric to assess if Chat-",
                "A properly calibrated classifier should have predictive scores that accurately reflect the probability of correctness (Thulasidasan et al., 2019; Minderer et al., 2021).",
                "Calibration."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "88abef771472c3aa46c53d5d626a0d0c3b66e8cd",
                "externalIds": {
                    "ArXiv": "2304.11633",
                    "DBLP": "journals/corr/abs-2304-11633",
                    "DOI": "10.48550/arXiv.2304.11633",
                    "CorpusId": 258297899
                },
                "corpusId": 258297899,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/88abef771472c3aa46c53d5d626a0d0c3b66e8cd",
                "title": "Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness",
                "abstract": "The capability of Large Language Models (LLMs) like ChatGPT to comprehend user intent and provide reasonable responses has made them extremely popular lately. In this paper, we focus on assessing the overall ability of ChatGPT using 7 fine-grained information extraction (IE) tasks. Specially, we present the systematically analysis by measuring ChatGPT's performance, explainability, calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT or domain experts. Our findings reveal that ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation. In addition, our research indicates that ChatGPT provides high-quality and trustworthy explanations for its decisions. However, there is an issue of ChatGPT being overconfident in its predictions, which resulting in low calibration. Furthermore, ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. We manually annotate and release the test sets of 7 fine-grained IE tasks contains 14 datasets to further promote the research. The datasets and code are available at https://github.com/pkuserc/ChatGPT_for_IE.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2485552",
                        "name": "Bo Li"
                    },
                    {
                        "authorId": "2215216578",
                        "name": "Gexiang Fang"
                    },
                    {
                        "authorId": "2152917615",
                        "name": "Yang Yang"
                    },
                    {
                        "authorId": "117898431",
                        "name": "Quansen Wang"
                    },
                    {
                        "authorId": "145235149",
                        "name": "Wei Ye"
                    },
                    {
                        "authorId": "2118223372",
                        "name": "Wen Zhao"
                    },
                    {
                        "authorId": "1705434",
                        "name": "Shikun Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "learned with the supervised learning manner becomes overconfident [20], it may overlook other correlation information."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "902614f4c22b96db71fdd7b1fb38920a7fc1488b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-08878",
                    "ArXiv": "2304.08878",
                    "DOI": "10.48550/arXiv.2304.08878",
                    "CorpusId": 258187531
                },
                "corpusId": 258187531,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/902614f4c22b96db71fdd7b1fb38920a7fc1488b",
                "title": "Deep Collective Knowledge Distillation",
                "abstract": "Many existing studies on knowledge distillation have focused on methods in which a student model mimics a teacher model well. Simply imitating the teacher's knowledge, however, is not sufficient for the student to surpass that of the teacher. We explore a method to harness the knowledge of other students to complement the knowledge of the teacher. We propose deep collective knowledge distillation for model compression, called DCKD, which is a method for training student models with rich information to acquire knowledge from not only their teacher model but also other student models. The knowledge collected from several student models consists of a wealth of information about the correlation between classes. Our DCKD considers how to increase the correlation knowledge of classes during training. Our novel method enables us to create better performing student models for collecting knowledge. This simple yet powerful method achieves state-of-the-art performances in many experiments. For example, for ImageNet, ResNet18 trained with DCKD achieves 72.27\\%, which outperforms the pretrained ResNet18 by 2.52\\%. For CIFAR-100, the student model of ShuffleNetV1 with DCKD achieves 6.55\\% higher top-1 accuracy than the pretrained ShuffleNetV1.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2214613684",
                        "name": "Jihyeon Seo"
                    },
                    {
                        "authorId": "1560769448",
                        "name": "Kyusam Oh"
                    },
                    {
                        "authorId": "2089765479",
                        "name": "Chanho Min"
                    },
                    {
                        "authorId": "1561604084",
                        "name": "Yongkeun Yun"
                    },
                    {
                        "authorId": "2149157331",
                        "name": "Sungwoo Cho"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, a regularization termwas also introduced into the confidence calibration, such asMixup (Thulasidasan et al 2019) and label smoothing (M\u00fcller et al 2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d70111986387db3a610cbaeb27e67f143390d83e",
                "externalIds": {
                    "DOI": "10.1088/1361-6560/acca5b",
                    "CorpusId": 257953424,
                    "PubMed": "37017082"
                },
                "corpusId": 257953424,
                "publicationVenue": {
                    "id": "d5594aad-095f-4587-802a-b011732c7100",
                    "name": "Physics in Medicine and Biology",
                    "type": "journal",
                    "alternate_names": [
                        "Phys Med Biology"
                    ],
                    "issn": "0031-9155",
                    "url": "http://www.iop.org/EJ/journal/0031-9155",
                    "alternate_urls": [
                        "https://iopscience.iop.org/journal/0031-9155",
                        "http://iopscience.iop.org/0031-9155",
                        "http://iopscience.org/pmb"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d70111986387db3a610cbaeb27e67f143390d83e",
                "title": "ARMO: automated and reliable multi-objective model for lymph node metastasis prediction in head and neck cancer",
                "abstract": "Objective. Accurate diagnosis of lymph node metastasis (LNM) is critical in treatment management for patients with head and neck cancer. Positron emission tomography and computed tomography are routinely used for identifying LNM status. However, for small or less fluorodeoxyglucose (FDG) avid nodes, there are always uncertainties in LNM diagnosis. We are aiming to develop a reliable prediction model is for identifying LNM. Approach. In this study, a new automated and reliable multi-objective learning model (ARMO) is proposed. In ARMO, a multi-objective model is introduced to obtain balanced sensitivity and specificity. Meanwhile, confidence is calibrated by introducing individual reliability, whilst the model uncertainty is estimated by a newly defined overall reliability in ARMO. In the training stage, a Pareto-optimal model set is generated. Then all the Pareto-optimal models are used, and a reliable fusion strategy that introduces individual reliability is developed for calibrating the confidence of each output. The overall reliability is calculated to estimate the model uncertainty for each test sample. Main results. The experimental results demonstrated that ARMO obtained more promising results, which the area under the curve, accuracy, sensitivity and specificity can achieve 0.97, 0.93, 0.88 and 0.94, respectively. Meanwhile, based on calibrated confidence and overall reliability, clinicians could pay particular attention to highly uncertain predictions. Significance. In this study, we developed a unified model that can achieve balanced prediction, confidence calibration and uncertainty estimation simultaneously. The experimental results demonstrated that ARMO can obtain accurate and reliable prediction performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1749019",
                        "name": "Zhiguo Zhou"
                    },
                    {
                        "authorId": "49330493",
                        "name": "Liyuan Chen"
                    },
                    {
                        "authorId": "10751260",
                        "name": "M. Dohopolski"
                    },
                    {
                        "authorId": "1789838",
                        "name": "D. Sher"
                    },
                    {
                        "authorId": "3820886",
                        "name": "Jing Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, deep learning models trained with mixup are less prone to making overconfident predictions on OOD test data in image classification [35].",
                "[35] show that mixup also improves the model\u2019s confidence calibration in image classification.",
                "Another study showed that mixup training [34] can improve the model calibration in image classification [35]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e2979c4b4b84af894085f9b4b79c161ff637b888",
                "externalIds": {
                    "DBLP": "journals/tai/KarimiG23",
                    "DOI": "10.1109/TAI.2022.3159510",
                    "CorpusId": 247485676
                },
                "corpusId": 247485676,
                "publicationVenue": {
                    "id": "3c27e831-750f-45bc-9914-2148a5259eba",
                    "name": "IEEE Transactions on Artificial Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Artif Intell"
                    ],
                    "issn": "2691-4581",
                    "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9078688"
                },
                "url": "https://www.semanticscholar.org/paper/e2979c4b4b84af894085f9b4b79c161ff637b888",
                "title": "Improving Calibration and Out-of-Distribution Detection in Deep Models for Medical Image Segmentation",
                "abstract": "Convolutional neural networks (CNNs) have proved to be powerful medical image segmentation models. In this study, we address some of the main unresolved issues regarding these models. Specifically, training of these models on small medical image datasets is still challenging, with many studies promoting techniques such as transfer learning. Moreover, these models are infamous for producing overconfident predictions and for failing silently when presented with out-of-distribution (OOD) test data. In this article, for improving prediction calibration we advocate for multitask learning, i.e., training a single model on several different datasets, spanning different organs of interest and different imaging modalities. We show that multitask learning can significantly improve model confidence calibration. For OOD detection, we propose a novel method based on spectral analysis of CNN feature maps. We show that different datasets, representing different imaging modalities and/or different organs of interest, have distinct spectral signatures, which can be used to identify whether or not a test image is similar to the images used for training. We show that our proposed method is more accurate than several competing methods, including methods based on prediction uncertainty and image classification.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2130813",
                        "name": "D. Karimi"
                    },
                    {
                        "authorId": "143936525",
                        "name": "A. Gholipour"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A recent work [64] demonstrates that calibration methods [20, 44, 45, 52] are harmful for MisD, and then reveals a surprising and intriguing phenomenon termed as reliable overfitting: the model starts to irreversibly lose confidence reliability after training for a period, even the test accuracy continually increases."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ed85c27f749e4b956d7e5f4ebf502e7bbad3650b",
                "externalIds": {
                    "DBLP": "conf/cvpr/ZhuCZL23",
                    "ArXiv": "2303.17093",
                    "DOI": "10.1109/CVPR52729.2023.01162",
                    "CorpusId": 257834097
                },
                "corpusId": 257834097,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ed85c27f749e4b956d7e5f4ebf502e7bbad3650b",
                "title": "OpenMix: Exploring Outlier Samples for Misclassification Detection",
                "abstract": "Reliable confidence estimation for deep neural classifiers is a challenging yet fundamental requirement in high-stakes applications. Unfortunately, modern deep neural networks are often overconfident for their erroneous predictions. In this work, we exploit the easily available outlier samples, i.e., unlabeled samples coming from non-target classes, for helping detect misclassification errors. Particularly, we find that the well-known Outlier Exposure, which is powerful in detecting out-of-distribution (OOD) samples from unknown classes, does not provide any gain in identifying misclassification errors. Based on these observations, we propose a novel method called OpenMix, which incorporates open-world knowledge by learning to reject uncertain pseudo-samples generated via outlier transformation. OpenMix significantly improves confidence reliability under various scenarios, establishing a strong and unified framework for detecting both misclassified samples from known classes and OOD samples from unknown classes. The code is publicly available at https://github.com/Impression2805/OpenMix.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2075372121",
                        "name": "Fei Zhu"
                    },
                    {
                        "authorId": "2162219075",
                        "name": "Zhen Cheng"
                    },
                    {
                        "authorId": "2870877",
                        "name": "Xu-Yao Zhang"
                    },
                    {
                        "authorId": "1689269",
                        "name": "Cheng-Lin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ca724597371df5ecdf008814a18fb5a37b48e07f",
                "externalIds": {
                    "ArXiv": "2303.15057",
                    "DBLP": "journals/corr/abs-2303-15057",
                    "DOI": "10.48550/arXiv.2303.15057",
                    "CorpusId": 257766938
                },
                "corpusId": 257766938,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ca724597371df5ecdf008814a18fb5a37b48e07f",
                "title": "Meta-Calibration Regularized Neural Networks",
                "abstract": "Miscalibration-the mismatch between predicted probability and the true correctness likelihood-has been frequently identified in modern deep neural networks. Recent work in the field aims to address this problem by training calibrated models directly by optimizing a proxy of the calibration error alongside the conventional objective. Recently, Meta-Calibration (MC) showed the effectiveness of using meta-learning for learning better calibrated models. In this work, we extend MC with two main components: (1) gamma network (gamma-net), a meta network to learn a sample-wise gamma at a continuous space for focal loss for optimizing backbone network; (2) smooth expected calibration error (SECE), a Gaussian-kernel based unbiased and differentiable ECE which aims to smoothly optimizing gamma-net. The proposed method regularizes neural network towards better calibration meanwhile retain predictive performance. Our experiments show that (a) learning sample-wise gamma at continuous space can effectively perform calibration; (b) SECE smoothly optimise gamma-net towards better robustness to binning schemes; (c) the combination of gamma-net and SECE achieve the best calibration performance across various calibration metrics and retain very competitive predictive performance as compared to multiple recently proposed methods on three datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2119128870",
                        "name": "Cheng Wang"
                    },
                    {
                        "authorId": "1411129857",
                        "name": "Jacek Golebiowski"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Bonus Effect of Data Augmentation Calibration [42] studied the overlooked effect of Mixup: their effect on calibration.",
                "1, 2, 3 [42] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "33efde93a619e5201001521f2f0e7b703af9c145",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-14608",
                    "ArXiv": "2303.14608",
                    "DOI": "10.48550/arXiv.2303.14608",
                    "CorpusId": 257766253
                },
                "corpusId": 257766253,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/33efde93a619e5201001521f2f0e7b703af9c145",
                "title": "Analyzing Effects of Mixed Sample Data Augmentation on Model Interpretability",
                "abstract": "Data augmentation strategies are actively used when training deep neural networks (DNNs). Recent studies suggest that they are effective at various tasks. However, the effect of data augmentation on DNNs' interpretability is not yet widely investigated. In this paper, we explore the relationship between interpretability and data augmentation strategy in which models are trained with different data augmentation methods and are evaluated in terms of interpretability. To quantify the interpretability, we devise three evaluation methods based on alignment with humans, faithfulness to the model, and the number of human-recognizable concepts in the model. Comprehensive experiments show that models trained with mixed sample data augmentation show lower interpretability, especially for CutMix and SaliencyMix augmentations. This new finding suggests that it is important to carefully adopt mixed sample data augmentation due to the impact on model interpretability, especially in mission-critical applications.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2106884106",
                        "name": "Soyoun Won"
                    },
                    {
                        "authorId": "40547898",
                        "name": "S. Bae"
                    },
                    {
                        "authorId": "2109708516",
                        "name": "Seong Tae Kim"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "eb332d020cb8877358157b7810e949d8f0256b1e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-13148",
                    "ArXiv": "2303.13148",
                    "DOI": "10.48550/arXiv.2303.13148",
                    "CorpusId": 257687711
                },
                "corpusId": 257687711,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/eb332d020cb8877358157b7810e949d8f0256b1e",
                "title": "Calibrated Out-of-Distribution Detection with a Generic Representation",
                "abstract": "Out-of-distribution detection is a common issue in deploying vision models in practice and solving it is an essential building block in safety critical applications. Most of the existing OOD detection solutions focus on improving the OOD robustness of a classification model trained exclusively on in-distribution (ID) data. In this work, we take a different approach and propose to leverage generic pre-trained representation. We propose a novel OOD method, called GROOD, that formulates the OOD detection as a Neyman-Pearson task with well calibrated scores and which achieves excellent performance, predicated by the use of a good generic representation. Only a trivial training process is required for adapting GROOD to a particular problem. The method is simple, general, efficient, calibrated and with only a few hyper-parameters. The method achieves state-of-the-art performance on a number of OOD benchmarks, reaching near perfect performance on several of them. The source code is available at https://github.com/vojirt/GROOD.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3349120",
                        "name": "Tom\u00e1s Voj\u00edr"
                    },
                    {
                        "authorId": "2833915",
                        "name": "Jan Sochman"
                    },
                    {
                        "authorId": "2855778",
                        "name": "Rahaf Aljundi"
                    },
                    {
                        "authorId": "1691679",
                        "name": "Juan E. Sala Matas"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For instance, accurately predicting traffics at crossroads with high traffic volumes during rush hours is crucial for urban travel but can be challenging due to complex traffic patterns [9].",
                "For instance, ensemble methods [35] can leverage multiple prediction models, and data augmentation techniques [9] involve applying various data augmentation methods to the test input to obtain multiple predictions to form the PI."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "78be3a89583651f0d122921935fa9d0456081abe",
                "externalIds": {
                    "ArXiv": "2303.09273",
                    "DBLP": "journals/corr/abs-2303-09273",
                    "DOI": "10.48550/arXiv.2303.09273",
                    "CorpusId": 257557451
                },
                "corpusId": 257557451,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/78be3a89583651f0d122921935fa9d0456081abe",
                "title": "Adaptive Modeling of Uncertainties for Traffic Forecasting",
                "abstract": "Deep neural networks (DNNs) have emerged as a dominant approach for developing traffic forecasting models. These models are typically trained to minimize error on averaged test cases and produce a single-point prediction, such as a scalar value for traffic speed or travel time. However, single-point predictions fail to account for prediction uncertainty that is critical for many transportation management scenarios, such as determining the best- or worst-case arrival time. We present QuanTraffic, a generic framework to enhance the capability of an arbitrary DNN model for uncertainty modeling. QuanTraffic requires little human involvement and does not change the base DNN architecture during deployment. Instead, it automatically learns a standard quantile function during the DNN model training to produce a prediction interval for the single-point prediction. The prediction interval defines a range where the true value of the traffic prediction is likely to fall. Furthermore, QuanTraffic develops an adaptive scheme that dynamically adjusts the prediction interval based on the location and prediction window of the test input. We evaluated QuanTraffic by applying it to five representative DNN models for traffic forecasting across seven public datasets. We then compared QuanTraffic against five uncertainty quantification methods. Compared to the baseline uncertainty modeling techniques, QuanTraffic with base DNN architectures delivers consistently better and more robust performance than the existing ones on the reported datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2145900634",
                        "name": "Ying Wu"
                    },
                    {
                        "authorId": "89987905",
                        "name": "Yongchao Ye"
                    },
                    {
                        "authorId": "2064285944",
                        "name": "Adnan Zeb"
                    },
                    {
                        "authorId": "2116508870",
                        "name": "James J. Q. Yu"
                    },
                    {
                        "authorId": "11032852",
                        "name": "Z. Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b1dda5c93c2266fde7fcab93ee9797b8b9a3d5db",
                "externalIds": {
                    "DOI": "10.3390/electronics12061421",
                    "CorpusId": 257612451
                },
                "corpusId": 257612451,
                "publicationVenue": {
                    "id": "ccd8e532-73c6-414f-bc91-271bbb2933e2",
                    "name": "Electronics",
                    "type": "journal",
                    "issn": "1450-5843",
                    "alternate_issns": [
                        "2079-9292",
                        "0883-4989"
                    ],
                    "url": "http://www.electronics.etfbl.net/",
                    "alternate_urls": [
                        "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-247562",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-247562",
                        "https://www.mdpi.com/journal/electronics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b1dda5c93c2266fde7fcab93ee9797b8b9a3d5db",
                "title": "Out-of-Distribution (OOD) Detection and Generalization Improved by Augmenting Adversarial Mixup Samples",
                "abstract": "Deep neural network (DNN) models are usually built based on the i.i.d. (independent and identically distributed), also known as in-distribution (ID), assumption on the training samples and test data. However, when models are deployed in a real-world scenario with some distributional shifts, test data can be out-of-distribution (OOD) and both OOD detection and OOD generalization should be simultaneously addressed to ensure the reliability and safety of applied AI systems. Most existing OOD detectors pursue these two goals separately, and therefore, are sensitive to covariate shift rather than semantic shift. To alleviate this problem, this paper proposes a novel adversarial mixup (AM) training method which simply executes OOD data augmentation to synthesize differently distributed data and designs a new AM loss function to learn how to handle OOD data. The proposed AM generates OOD samples being significantly diverged from the support of training data distribution but not completely disjoint to increase the generalization capability of the OOD detector. In addition, the AM is combined with a distributional-distance-aware OOD detector at inference to detect semantic OOD samples more efficiently while being robust to covariate shift due to data tampering. Experimental evaluation validates that the designed AM is effective on both OOD detection and OOD generalization tasks compared to previous OOD detectors and data mixup methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2090159971",
                        "name": "Kyungpil Gwon"
                    },
                    {
                        "authorId": "40382590",
                        "name": "Joonhyuk Yoo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(2015), while the Overconfidence Error (OE)\u2014its counterpart that only considers the miscalibration caused by overconfident predictions\u2014was proposed by (Thulasidasan et\u00a0 al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bc70c796976b1cbf2519503d1fccb1b40d565cad",
                "externalIds": {
                    "DBLP": "journals/ml/TeixeiraJR23",
                    "DOI": "10.1007/s10994-023-06307-y",
                    "CorpusId": 257544188
                },
                "corpusId": 257544188,
                "publicationVenue": {
                    "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
                    "name": "Machine-mediated learning",
                    "type": "journal",
                    "alternate_names": [
                        "Mach learn",
                        "Machine Learning",
                        "Mach Learn"
                    ],
                    "issn": "0732-6718",
                    "alternate_issns": [
                        "0885-6125"
                    ],
                    "url": "http://www.springer.com/computer/artificial/journal/10994",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10994",
                        "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bc70c796976b1cbf2519503d1fccb1b40d565cad",
                "title": "Reducing classifier overconfidence against adversaries through graph algorithms",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2061470428",
                        "name": "Leonardo Teixeira"
                    },
                    {
                        "authorId": "50039334",
                        "name": "Brian Jalaian"
                    },
                    {
                        "authorId": "145617731",
                        "name": "Bruno Ribeiro"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[19] empirically found that mixup can significantly improve",
                "[15,19] showed the favorable calibration effect of LS.",
                "In our experiments, for mixup, we follow the setting in [19] to use \u03b1 = 0."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "07975b0df907e775908cec198cade7ff2a6154c5",
                "externalIds": {
                    "ArXiv": "2303.02970",
                    "DBLP": "conf/eccv/ZhuCZL22",
                    "DOI": "10.1007/978-3-031-19806-9_30",
                    "CorpusId": 253099850
                },
                "corpusId": 253099850,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/07975b0df907e775908cec198cade7ff2a6154c5",
                "title": "Rethinking Confidence Calibration for Failure Prediction",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2075372121",
                        "name": "Fei Zhu"
                    },
                    {
                        "authorId": "2162219075",
                        "name": "Zhen Cheng"
                    },
                    {
                        "authorId": "2870877",
                        "name": "Xu-Yao Zhang"
                    },
                    {
                        "authorId": "1689269",
                        "name": "Cheng-Lin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mixup has empirically shown its effectiveness in improving the generalization and robustness of deep classification models (Zhang et al., 2018; Guo et al., 2019a;b; Thulasidasan et al., 2019; Zhang et al., 2022b).",
                "Thulasidasan et al. (2019) show that Mixup helps to improve the calibration of the trained networks.",
                "1 INTRODUCTION Mixup has empirically shown its effectiveness in improving the generalization and robustness of deep classification models (Zhang et al., 2018; Guo et al., 2019a;b; Thulasidasan et al., 2019; Zhang et al., 2022b).",
                "\u2026models, there has been a recent surge of interest attempting to better understand Mixup\u2019s working mechanism, training characteristics, regularization potential, and possible limitations (see, e.g.,\nThulasidasan et al. (2019), Guo et al. (2019a), Zhang et al. (2021), Zhang et al. (2022b))."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "99486d35169e1937b8ff9e5abd7227b6d1e901b9",
                "externalIds": {
                    "ArXiv": "2303.01475",
                    "DBLP": "conf/iclr/LiuWGM23",
                    "DOI": "10.48550/arXiv.2303.01475",
                    "CorpusId": 254198988
                },
                "corpusId": 254198988,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/99486d35169e1937b8ff9e5abd7227b6d1e901b9",
                "title": "Over-training with Mixup May Hurt Generalization",
                "abstract": "Mixup, which creates synthetic training instances by linearly interpolating random sample pairs, is a simple and yet effective regularization technique to boost the performance of deep models trained with SGD. In this work, we report a previously unobserved phenomenon in Mixup training: on a number of standard datasets, the performance of Mixup-trained models starts to decay after training for a large number of epochs, giving rise to a U-shaped generalization curve. This behavior is further aggravated when the size of original dataset is reduced. To help understand such a behavior of Mixup, we show theoretically that Mixup training may introduce undesired data-dependent label noises to the synthesized data. Via analyzing a least-square regression problem with a random feature model, we explain why noisy labels may cause the U-shaped curve to occur: Mixup improves generalization through fitting the clean patterns at the early training stage, but as training progresses, Mixup becomes over-fitting to the noise in the synthetic data. Extensive experiments are performed on a variety of benchmark datasets, validating this explanation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1645199888",
                        "name": "Zixuan Liu"
                    },
                    {
                        "authorId": "2117428102",
                        "name": "Ziqiao Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Training-Time Calibration Popular training-time approaches consist of reducing the predictive entropy by means of regularization [11], e.g. Label Smoothing [27] or MixUp [30], or loss functions that smooth predictions [25].",
                "For comparison, we include a standard singleloss one-head classifier (SL1H), plus models trained with Label Smoothing (LS [27]), Margin-based Label Smoothing (MbLS [22]), MixUp [30], and using the DCA loss [20].",
                "Label Smoothing [27] or MixUp [30], or loss functions that smooth predictions [25]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "05d77c39e0fe41a8758a1ae562cf61e3a306a721",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-01099",
                    "ArXiv": "2303.01099",
                    "DOI": "10.48550/arXiv.2303.01099",
                    "CorpusId": 257280056
                },
                "corpusId": 257280056,
                "publicationVenue": {
                    "id": "61a709e3-2060-423c-8de5-ffd3885aa31c",
                    "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                    "type": "conference",
                    "alternate_names": [
                        "Medical Image Computing and Computer-Assisted Intervention",
                        "MICCAI",
                        "Med Image Comput Comput Interv",
                        "Int Conf Med Image Comput Comput Interv"
                    ],
                    "url": "http://www.miccai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/05d77c39e0fe41a8758a1ae562cf61e3a306a721",
                "title": "Multi-Head Multi-Loss Model Calibration",
                "abstract": "Delivering meaningful uncertainty estimates is essential for a successful deployment of machine learning models in the clinical practice. A central aspect of uncertainty quantification is the ability of a model to return predictions that are well-aligned with the actual probability of the model being correct, also known as model calibration. Although many methods have been proposed to improve calibration, no technique can match the simple, but expensive approach of training an ensemble of deep neural networks. In this paper we introduce a form of simplified ensembling that bypasses the costly training and inference of deep ensembles, yet it keeps its calibration capabilities. The idea is to replace the common linear classifier at the end of a network by a set of heads that are supervised with different loss functions to enforce diversity on their predictions. Specifically, each head is trained to minimize a weighted Cross-Entropy loss, but the weights are different among the different branches. We show that the resulting averaged predictions can achieve excellent calibration without sacrificing accuracy in two challenging datasets for histopathological and endoscopic image classification. Our experiments indicate that Multi-Head Multi-Loss classifiers are inherently well-calibrated, outperforming other recent calibration techniques and even challenging Deep Ensembles' performance. Code to reproduce our experiments can be found at \\url{https://github.com/agaldran/mhml_calibration} .",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2161526",
                        "name": "A. Galdran"
                    },
                    {
                        "authorId": "3582291",
                        "name": "J. Verjans"
                    },
                    {
                        "authorId": "145575177",
                        "name": "G. Carneiro"
                    },
                    {
                        "authorId": "49463953",
                        "name": "M. Ballester"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The calibrated model can be obtained via data augmentation [22, 57, 62], adversarial training [7, 10, 20], and uncertainty modelling [5, 40]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0b6b235621ce36d07b2e1759b33bf989b05cc99e",
                "externalIds": {
                    "ArXiv": "2303.01577",
                    "DBLP": "journals/corr/abs-2303-01577",
                    "DOI": "10.1145/3544548.3580741",
                    "CorpusId": 257353834
                },
                "corpusId": 257353834,
                "publicationVenue": {
                    "id": "b55b50b1-aae7-47a7-b042-8aecc930073d",
                    "name": "International Conference on Human Factors in Computing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "CHI",
                        "Int Conf Hum Factor Comput Syst",
                        "Human Factors in Computing Systems",
                        "Conference on Human Interface",
                        "Conf Hum Interface",
                        "Hum Factor Comput Syst"
                    ],
                    "url": "http://www.acm.org/sigchi/"
                },
                "url": "https://www.semanticscholar.org/paper/0b6b235621ce36d07b2e1759b33bf989b05cc99e",
                "title": "DeepLens: Interactive Out-of-distribution Data Detection in NLP Models",
                "abstract": "Machine Learning (ML) has been widely used in Natural Language Processing (NLP) applications. A fundamental assumption in ML is that training data and real-world data should follow a similar distribution. However, a deployed ML model may suffer from out-of-distribution (OOD) issues due to distribution shifts in the real-world data. Though many algorithms have been proposed to detect OOD data from text corpora, there is still a lack of interactive tool support for ML developers. In this work, we propose DeepLens, an interactive system that helps users detect and explore OOD issues in massive text corpora. Users can efficiently explore different OOD types in DeepLens with the help of a text clustering method. Users can also dig into a specific text by inspecting salient words highlighted through neuron activation analysis. In a within-subjects user study with 24 participants, participants using DeepLens were able to find nearly twice more types of OOD issues accurately with 22% more confidence compared with a variant of DeepLens that has no interaction or visualization support.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1382534445",
                        "name": "D. Song"
                    },
                    {
                        "authorId": "2108157560",
                        "name": "Zhijie Wang"
                    },
                    {
                        "authorId": "1739753031",
                        "name": "Yuheng Huang"
                    },
                    {
                        "authorId": "2193640276",
                        "name": "Lei Ma"
                    },
                    {
                        "authorId": "2146332311",
                        "name": "Tianyi Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4c997a9f681e81a16bb825e352cc26c17aa2b5d0",
                "externalIds": {
                    "DBLP": "journals/kbs/HuGWG23",
                    "DOI": "10.1016/j.knosys.2023.110520",
                    "CorpusId": 257841238
                },
                "corpusId": 257841238,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4c997a9f681e81a16bb825e352cc26c17aa2b5d0",
                "title": "Mixture of calibrated networks for domain generalization in brain tumor segmentation",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2175543423",
                        "name": "Jingyu Hu"
                    },
                    {
                        "authorId": "3341801",
                        "name": "Xiaojing Gu"
                    },
                    {
                        "authorId": "2155359588",
                        "name": "Zhiqiang Wang"
                    },
                    {
                        "authorId": "2118713522",
                        "name": "Xingsheng Gu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, the point estimation of DNN parameters tends to be overfitting and overconfident [131]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e1ba4318bc924979d933562a3e5d49a2b9f58e4f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-13425",
                    "ArXiv": "2302.13425",
                    "DOI": "10.48550/arXiv.2302.13425",
                    "CorpusId": 257219242
                },
                "corpusId": 257219242,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e1ba4318bc924979d933562a3e5d49a2b9f58e4f",
                "title": "A Survey on Uncertainty Quantification Methods for Deep Neural Networks: An Uncertainty Source Perspective",
                "abstract": "Deep neural networks (DNNs) have achieved tremendous success in making accurate predictions for computer vision, natural language processing, as well as science and engineering domains. However, it is also well-recognized that DNNs sometimes make unexpected, incorrect, but overconfident predictions. This can cause serious consequences in high-stake applications, such as autonomous driving, medical diagnosis, and disaster response. Uncertainty quantification (UQ) aims to estimate the confidence of DNN predictions beyond prediction accuracy. In recent years, many UQ methods have been developed for DNNs. It is of great practical value to systematically categorize these UQ methods and compare their advantages and disadvantages. However, existing surveys mostly focus on categorizing UQ methodologies from a neural network architecture perspective or a Bayesian perspective and ignore the source of uncertainty that each methodology can incorporate, making it difficult to select an appropriate UQ method in practice. To fill the gap, this paper presents a systematic taxonomy of UQ methods for DNNs based on the types of uncertainty sources (data uncertainty versus model uncertainty). We summarize the advantages and disadvantages of methods in each category. We show how our taxonomy of UQ methodologies can potentially help guide the choice of UQ method in different machine learning problems (e.g., active learning, robustness, and reinforcement learning). We also identify current research gaps and propose several future research directions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1569689411",
                        "name": "Wenchong He"
                    },
                    {
                        "authorId": "2112763954",
                        "name": "Zhe Jiang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Despite its simplicity, MixUp has been proven to improve model calibration and better generalization [35]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4e3c4ffe620bde307fced0e3ed2c0879678d67d8",
                "externalIds": {
                    "PubMedCentral": "10006882",
                    "DBLP": "journals/sensors/WongTKO23",
                    "DOI": "10.3390/s23052494",
                    "CorpusId": 257202071,
                    "PubMed": "36904696"
                },
                "corpusId": 257202071,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4e3c4ffe620bde307fced0e3ed2c0879678d67d8",
                "title": "FedDdrl: Federated Double Deep Reinforcement Learning for Heterogeneous IoT with Adaptive Early Client Termination and Local Epoch Adjustment",
                "abstract": "Federated learning (FL) is a technique that allows multiple clients to collaboratively train a global model without sharing their sensitive and bandwidth-hungry data. This paper presents a joint early client termination and local epoch adjustment for FL. We consider the challenges of heterogeneous Internet of Things (IoT) environments including non-independent and identically distributed (non-IID) data as well as diverse computing and communication capabilities. The goal is to strike the best tradeoff among three conflicting objectives, namely global model accuracy, training latency and communication cost. We first leverage the balanced-MixUp technique to mitigate the influence of non-IID data on the FL convergence rate. A weighted sum optimization problem is then formulated and solved via our proposed FL double deep reinforcement learning (FedDdrl) framework, which outputs a dual action. The former indicates whether a participating FL client is dropped, whereas the latter specifies how long each remaining client needs to complete its local training task. Simulation results show that FedDdrl outperforms the existing FL scheme in terms of overall tradeoff. Specifically, FedDdrl achieves higher model accuracy by about 4% while incurring 30% less latency and communication costs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2150070704",
                        "name": "Y. Wong"
                    },
                    {
                        "authorId": "2558480",
                        "name": "Mau-Luen Tham"
                    },
                    {
                        "authorId": "47594946",
                        "name": "Ban-Hoe Kwan"
                    },
                    {
                        "authorId": "32434723",
                        "name": "Y. Owada"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[26] acknowledged the calibration and generalization property of mixup.",
                "Various techniques like vector scaling [7, 20], matrix scaling [7], temperature scaling [6, 12, 20], label smoothing [4, 19], mixup [26, 38], CutMix [34] etc.",
                "Mixup [26] targets the overconfidence of the model, therefore KD + mixup drastically reduces the OE of the model which makes it suitable for high risk domains."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3662f2dfd415823e2cd97d40dd03a18f15c003a9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-11472",
                    "ArXiv": "2302.11472",
                    "DOI": "10.48550/arXiv.2302.11472",
                    "CorpusId": 257079238
                },
                "corpusId": 257079238,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3662f2dfd415823e2cd97d40dd03a18f15c003a9",
                "title": "Distilling Calibrated Student from an Uncalibrated Teacher",
                "abstract": "Knowledge distillation is a common technique for improving the performance of a shallow student network by transferring information from a teacher network, which in general, is comparatively large and deep. These teacher networks are pre-trained and often uncalibrated, as no calibration technique is applied to the teacher model while training. Calibration of a network measures the probability of correctness for any of its predictions, which is critical in high-risk domains. In this paper, we study how to obtain a calibrated student from an uncalibrated teacher. Our approach relies on the fusion of the data-augmentation techniques, including but not limited to cutout, mixup, and CutMix, with knowledge distillation. We extend our approach beyond traditional knowledge distillation and find it suitable for Relational Knowledge Distillation and Contrastive Representation Distillation as well. The novelty of the work is that it provides a framework to distill a calibrated student from an uncalibrated teacher model without compromising the accuracy of the distilled student. We perform extensive experiments to validate our approach on various datasets, including CIFAR-10, CIFAR-100, CINIC-10 and TinyImageNet, and obtained calibrated student models. We also observe robust performance of our approach while evaluating it on corrupted CIFAR-100C data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2192863598",
                        "name": "Ishan Mishra"
                    },
                    {
                        "authorId": "2209211723",
                        "name": "Sethu Vamsi Krishna"
                    },
                    {
                        "authorId": "2082316146",
                        "name": "Deepak Mishra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DNNs trained with mixup are better-calibrated [21]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ba4f5b72edfbec560007735b5bf3a9a82585c968",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-08229",
                    "ArXiv": "2302.08229",
                    "DOI": "10.48550/arXiv.2302.08229",
                    "CorpusId": 256900993
                },
                "corpusId": 256900993,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/ba4f5b72edfbec560007735b5bf3a9a82585c968",
                "title": "Improving Spoken Language Identification with Map-Mix",
                "abstract": "The pre-trained multi-lingual XLSR model generalizes well for language identification after fine-tuning on unseen languages. However, the performance significantly degrades when the languages are not very distinct from each other, for example, in the case of dialects. Low resource dialect classification remains a challenging problem to solve. We present a new data augmentation method that leverages model training dynamics of individual data points to improve sampling for latent mixup. The method works well in low-resource settings where generalization is paramount. Our datamaps-based mixup technique, which we call Map-Mix improves weighted F1 scores by 2% compared to the random mixup baseline and results in a significantly well-calibrated model. The code for our method is open sourced on https://github.com/skit-ai/Map-Mix.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "123326265",
                        "name": "Shangeth Rajaa"
                    },
                    {
                        "authorId": "2206148462",
                        "name": "K. Anandan"
                    },
                    {
                        "authorId": "2198273490",
                        "name": "Swaraj Dalmia"
                    },
                    {
                        "authorId": "2159556631",
                        "name": "Tarun Gupta"
                    },
                    {
                        "authorId": "1742722",
                        "name": "Chng Eng Siong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A well-calibrated model helps improve the model robustness on OoD datasets [Thulasidasan et al., 2019].",
                ", 2019], and data augmentation [Thulasidasan et al., 2019; Hendrycks et al., 2019] are introduced to improve model calibration.",
                "Mixup [Thulasidasan et al., 2019] and AugMix [Hendrycks et al."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9dc32dce3fdd395764f524bf5227aec8d9b356b4",
                "externalIds": {
                    "ArXiv": "2302.06245",
                    "DBLP": "journals/corr/abs-2302-06245",
                    "DOI": "10.48550/arXiv.2302.06245",
                    "CorpusId": 256826903
                },
                "corpusId": 256826903,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/9dc32dce3fdd395764f524bf5227aec8d9b356b4",
                "title": "Calibrating a Deep Neural Network with Its Predecessors",
                "abstract": "Confidence calibration - the process to calibrate the output probability distribution of neural networks - is essential for safety-critical applications of such networks. Recent works verify the link between mis-calibration and overfitting. However, early stopping, as a well-known technique to mitigate overfitting, fails to calibrate networks. In this work, we study the limitions of early stopping and comprehensively analyze the overfitting problem of a network considering each individual block. We then propose a novel regularization method, predecessor combination search (PCS), to improve calibration by searching a combination of best-fitting block predecessors, where block predecessors are the corresponding network blocks with weight parameters from earlier training stages. PCS achieves the state-of-the-art calibration performance on multiple datasets and architectures. In addition, PCS improves model robustness under dataset distribution shift. Supplementary material and code are available at https://github.com/Linwei94/PCS",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "34697970",
                        "name": "Linwei Tao"
                    },
                    {
                        "authorId": "151498428",
                        "name": "Minjing Dong"
                    },
                    {
                        "authorId": "51023221",
                        "name": "Daochang Liu"
                    },
                    {
                        "authorId": "47810178",
                        "name": "Changming Sun"
                    },
                    {
                        "authorId": "9196284",
                        "name": "Chang Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020) or mixup techniques (Thulasidasan et al., 2019; Zhang et al., 2017) as well as other intrinsically calibrated approaches (Sensoy et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "264495ddc425019035b5650cf678a1e9ecf66af1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-05118",
                    "ArXiv": "2302.05118",
                    "DOI": "10.48550/arXiv.2302.05118",
                    "CorpusId": 256808420
                },
                "corpusId": 256808420,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/264495ddc425019035b5650cf678a1e9ecf66af1",
                "title": "Beyond In-Domain Scenarios: Robust Density-Aware Calibration",
                "abstract": "Calibrating deep learning models to yield uncertainty-aware predictions is crucial as deep neural networks get increasingly deployed in safety-critical applications. While existing post-hoc calibration methods achieve impressive results on in-domain test datasets, they are limited by their inability to yield reliable uncertainty estimates in domain-shift and out-of-domain (OOD) scenarios. We aim to bridge this gap by proposing DAC, an accuracy-preserving as well as Density-Aware Calibration method based on k-nearest-neighbors (KNN). In contrast to existing post-hoc methods, we utilize hidden layers of classifiers as a source for uncertainty-related information and study their importance. We show that DAC is a generic method that can readily be combined with state-of-the-art post-hoc methods. DAC boosts the robustness of calibration performance in domain-shift and OOD, while maintaining excellent in-domain predictive uncertainty estimates. We demonstrate that DAC leads to consistently better calibration across a large number of model architectures, datasets, and metrics. Additionally, we show that DAC improves calibration substantially on recent large-scale neural networks pre-trained on vast amounts of data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1583683907",
                        "name": "Christian Tomani"
                    },
                    {
                        "authorId": "1753322930",
                        "name": "Futa Waseda"
                    },
                    {
                        "authorId": "49745955",
                        "name": "Yuesong Shen"
                    },
                    {
                        "authorId": "1695302",
                        "name": "D. Cremers"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[33] has shown that Mixup is efective at calibrating deep models for the tasks of image and text classifcation."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f3fa1ef467c996b30242124a298b5b9d031e9ed5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-05044",
                    "ArXiv": "2302.05044",
                    "DOI": "10.1145/3543507.3583544",
                    "CorpusId": 256808675
                },
                "corpusId": 256808675,
                "publicationVenue": {
                    "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
                    "name": "The Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Web Conf",
                        "WWW"
                    ],
                    "url": "http://www.iw3c2.org/"
                },
                "url": "https://www.semanticscholar.org/paper/f3fa1ef467c996b30242124a298b5b9d031e9ed5",
                "title": "Toward Degree Bias in Embedding-Based Knowledge Graph Completion",
                "abstract": "A fundamental task for knowledge graphs (KGs) is knowledge graph completion (KGC). It aims to predict unseen edges by learning representations for all the entities and relations in a KG. A common concern when learning representations on traditional graphs is degree bias. It can affect graph algorithms by learning poor representations for lower-degree nodes, often leading to low performance on such nodes. However, there has been limited research on whether there exists degree bias for embedding-based KGC and how such bias affects the performance of KGC. In this paper, we validate the existence of degree bias in embedding-based KGC and identify the key factor to degree bias. We then introduce a novel data augmentation method, KG-Mixup, to generate synthetic triples to mitigate such bias. Extensive experiments have demonstrated that our method can improve various embedding-based KGC methods and outperform other methods tackling the bias problem on multiple benchmark datasets. 1",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "93719189",
                        "name": "Harry Shomer"
                    },
                    {
                        "authorId": "144767914",
                        "name": "Wei Jin"
                    },
                    {
                        "authorId": "2108329255",
                        "name": "Wentao Wang"
                    },
                    {
                        "authorId": "1736632",
                        "name": "Jiliang Tang"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2014) and data augmentation methods such as mixup (Thulasidasan et al., 2019) and Augmix (Hendrycks et al.",
                "\u2026smoothing (M\u00fcller et al., 2019), focal loss (Mukhoti et al., 2020), dropout (Srivastava et al., 2014) and data augmentation methods such as mixup (Thulasidasan et al., 2019) and Augmix (Hendrycks et al., 2020).\n\u03bb \u223c Beta(\u03b1, \u03b1)\nMore advanced methods for improving performance are also demonstrated\u2026"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0fd98068168cdd09c7309e5cec95f3cf71c488ab",
                "externalIds": {
                    "ArXiv": "2302.01440",
                    "DBLP": "journals/corr/abs-2302-01440",
                    "DOI": "10.48550/arXiv.2302.01440",
                    "CorpusId": 256598198
                },
                "corpusId": 256598198,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0fd98068168cdd09c7309e5cec95f3cf71c488ab",
                "title": "Generalized Uncertainty of Deep Neural Networks: Taxonomy and Applications",
                "abstract": "Deep neural networks have seen enormous success in various real-world applications. Beyond their predictions as point estimates, increasing attention has been focused on quantifying the uncertainty of their predictions. In this review, we show that the uncertainty of deep neural networks is not only important in a sense of interpretability and transparency, but also crucial in further advancing their performance, particularly in learning systems seeking robustness and efficiency. We will generalize the definition of the uncertainty of deep neural networks to any number or vector that is associated with an input or an input-label pair, and catalog existing methods on ``mining'' such uncertainty from a deep model. We will include those methods from the classic field of uncertainty quantification as well as those methods that are specific to deep neural networks. We then show a wide spectrum of applications of such generalized uncertainty in realistic learning tasks including robust learning such as noisy learning, adversarially robust learning; data-efficient learning such as semi-supervised and weakly-supervised learning; and model-efficient learning such as model compression and knowledge distillation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2113540861",
                        "name": "Chengyu Dong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[28] demonstrate that neural networks trained with mixup are significantly better calibrated under dataset shift, and are less prone to over-confident predictions on out-of-distribution data.",
                "It has been shown by [28], [30] that label smoothing can also effectively improve the quality of a model\u2019s uncertainty estimates."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f94a724c0d4b0174f108e4361130a9094eaed483",
                "externalIds": {
                    "ArXiv": "2302.11188",
                    "DBLP": "journals/corr/abs-2302-11188",
                    "DOI": "10.1109/SaTML54575.2023.00032",
                    "CorpusId": 254152682
                },
                "corpusId": 254152682,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f94a724c0d4b0174f108e4361130a9094eaed483",
                "title": "What Are Effective Labels for Augmented Data? Improving Calibration and Robustness with AutoLabel",
                "abstract": "A wide breadth of research has devised data augmentation approaches that can improve both accuracy and generalization performance for neural networks. However, augmented data can end up being far from the clean training data and what is the appropriate label is less clear. Despite this, most existing work simply uses one-hot labels for augmented data. In this paper, we show re-using one-hot labels for highly distorted data might run the risk of adding noise and degrading accuracy and calibration. To mitigate this, we propose a generic method AutoLabel to automatically learn the confidence in the labels for augmented data, based on the transformation distance between the clean distribution and augmented distribution. AutoLabel is built on label smoothing and is guided by the calibration-performance over a hold-out validation set. We successfully apply AutoLabel to three different data augmentation techniques: the state-of-the-art RandAug, AugMix, and adversarial training. Experiments on CIFAR-10, CIFAR-100 and ImageNet show that AutoLabel significantly improves existing data augmentation techniques over models' calibration and accuracy, especially under distributional shift. Additionally, AutoLabel improves adversarial training by bridging the gap between clean accuracy and adversarial robustness.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145592705",
                        "name": "Yao Qin"
                    },
                    {
                        "authorId": "1524732527",
                        "name": "Xuezhi Wang"
                    },
                    {
                        "authorId": "40627523",
                        "name": "Balaji Lakshminarayanan"
                    },
                    {
                        "authorId": "2226805",
                        "name": "Ed H. Chi"
                    },
                    {
                        "authorId": "2638246",
                        "name": "Alex Beutel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another approach of mixup training [277] generates additional samples during training by convexly combining random pairs of images and their associated labels, which is found to improve not only the classification performance but also the calibration and predictive uncertainty of the model."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fac413d11c7954f85fe81c81d7d57149e0374041",
                "externalIds": {
                    "DBLP": "journals/pieee/ZhangXLML23",
                    "DOI": "10.1109/JPROC.2023.3238024",
                    "CorpusId": 256356570
                },
                "corpusId": 256356570,
                "publicationVenue": {
                    "id": "6faaccca-1cc4-45a9-aeb6-96a4901d2606",
                    "name": "Proceedings of the IEEE",
                    "type": "journal",
                    "alternate_names": [
                        "Proc IEEE"
                    ],
                    "issn": "0018-9219",
                    "alternate_issns": [
                        "1558-2256"
                    ],
                    "url": "http://www.ieee.org/portal/pages/pubs/proceedings/",
                    "alternate_urls": [
                        "http://www.ieee.org/products/onlinepubs/pub/about_conference.html",
                        "https://ieeexplore.ieee.org/servlet/opac?punumber=5",
                        "http://proceedingsoftheieee.ieee.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fac413d11c7954f85fe81c81d7d57149e0374041",
                "title": "A Survey on Learning to Reject",
                "abstract": "Learning to reject is a special kind of self-awareness (the ability to know what you do not know), which is an essential factor for humans to become smarter. Although machine intelligence has become very accurate nowadays, it lacks such kind of self-awareness and usually acts as omniscient, resulting in overconfident errors. This article presents a comprehensive overview of this topic from three perspectives: confidence, calibration, and discrimination. Confidence is an important measurement for the reliability of model predictions. Rejection can be realized by setting thresholds on confidence. However, most models, especially modern deep neural networks, are usually overconfident. Therefore, calibration is a process to ensure confidence matching the actual likelihood of correctness, including two approaches: post-calibration and self-calibration. Calibration reflects the global characteristic of confidence, and the local distinguishing property of confidence is also important. In light of this, discrimination focuses on the performance of accepting positive samples while rejecting negative samples. As a binary classification problem, the challenge of discrimination comes from the missing and nonrepresentativeness of the negative data. Three discrimination tasks are comprehensively analyzed and discussed: failure rejection, unknown rejection, and fake rejection. By rejecting failures, the risk could be controlled especially for mission-critical applications. By rejecting unknowns, the awareness of the knowledge blind zone would be enhanced. By rejecting fakes, security and privacy could be protected. We provide a general taxonomy, organization, and discussion of the methods for solving these problems, which are studied separately in the literature. The connections between different approaches and future directions that are worth further investigation are also presented. With a discriminative and calibrated confidence, learning to reject will let the decision-making process be more practical, reliable, and secure.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2870877",
                        "name": "Xu-Yao Zhang"
                    },
                    {
                        "authorId": "3057896",
                        "name": "Guosen Xie"
                    },
                    {
                        "authorId": "2116524872",
                        "name": "Xiu-Chuan Li"
                    },
                    {
                        "authorId": "2070183439",
                        "name": "T. Mei"
                    },
                    {
                        "authorId": "1689269",
                        "name": "Cheng-Lin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similar to the results of [5], with the mixup augmentation, the improvement in accuracy was not significant, but a model calibration effect could be seen in some cases.",
                "However, it did not achieve much in improving the generalization performance of the model in preparation for the correction effect [5, 6].",
                "Recently, several studies have introduced that data augmentation is effective for model generalization as well as calibration [5, 6], but the results are not significant in terms of generalization performance."
            ],
            "intents": [
                "result",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "34c0c5ea32571c1620082c7fa6329d22e1307b2d",
                "externalIds": {
                    "ArXiv": "2301.13444",
                    "DBLP": "journals/corr/abs-2301-13444",
                    "DOI": "10.48550/arXiv.2301.13444",
                    "CorpusId": 256415884
                },
                "corpusId": 256415884,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/34c0c5ea32571c1620082c7fa6329d22e1307b2d",
                "title": "Rethinking Soft Label in Label Distribution Learning Perspective",
                "abstract": "The primary goal of training in early convolutional neural networks (CNN) is the higher generalization performance of the model. However, as the expected calibration error (ECE), which quanti\ufb01es the explanatory power of model inference, was recently introduced, research on training models that can be explained is in progress. We hypothesized that a gap in supervision criteria during training and inference leads to overcon\ufb01dence, and investigated that performing label distribution learning (LDL) would enhance the model calibration in CNN training. To verify this assumption, we used a simple LDL setting with recent data augmentation techniques. Based on a series of experiments, the following results are obtained: 1) State-of-the-art KD methods signi\ufb01cantly impede model calibration. 2) Training using LDL with recent data augmentation can have excellent effects on model calibration and even in generalization performance. 3) Online LDL brings additional improvements in model calibration and accuracy with long training, especially in large-size models. Using the proposed approach, we simultaneously achieved a lower ECE and higher generalization performance for the image classi\ufb01cation datasets CIFAR10, 100, STL10, and ImageNet. We performed several visualiza-tions and analyses and witnessed several interesting behaviors in CNN training with the LDL.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2154320929",
                        "name": "Seungbum Hong"
                    },
                    {
                        "authorId": "1435732544",
                        "name": "Jihun Yoon"
                    },
                    {
                        "authorId": "2111127820",
                        "name": "Bogyu Park"
                    },
                    {
                        "authorId": "2501453",
                        "name": "Min-Kook Choi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, predictions from a single model can be misleading, such as they are prone to overconfidence (Thulasidasan et al. 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8e9931c561dbba7227baba58f7b487a5e2b5c676",
                "externalIds": {
                    "ArXiv": "2301.12378",
                    "DBLP": "conf/aaai/LiRYJY023",
                    "DOI": "10.48550/arXiv.2301.12378",
                    "CorpusId": 256390550
                },
                "corpusId": 256390550,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/8e9931c561dbba7227baba58f7b487a5e2b5c676",
                "title": "Towards Inference Efficient Deep Ensemble Learning",
                "abstract": "Ensemble methods can deliver surprising performance gains but also bring significantly higher computational costs, e.g., can be up to 2048X in large-scale ensemble tasks. However, we found that the majority of computations in ensemble methods are redundant. For instance, over 77% of samples in CIFAR-100 dataset can be correctly classified with only a single ResNet-18 model, which indicates that only around 23% of the samples need an ensemble of extra models. To this end, we propose an inference efficient ensemble learning method, to simultaneously optimize for effectiveness and efficiency in ensemble learning. More specifically, we regard ensemble of models as a sequential inference process and learn the optimal halting event for inference on a specific sample. At each timestep of the inference process, a common selector judges if the current ensemble has reached ensemble effectiveness and halt further inference, otherwise filters this challenging sample for the subsequent models to conduct more powerful ensemble. Both the base models and common selector are jointly optimized to dynamically adjust ensemble inference for different samples with various hardness, through the novel optimization goals including sequential ensemble boosting and computation saving. The experiments with different backbones on real-world datasets illustrate our method can bring up to 56% inference cost reduction while maintaining comparable performance to full ensemble, achieving significantly better ensemble utility than other baselines. Code and supplemental materials are available at https://seqml.github.io/irene.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2051629352",
                        "name": "Ziyue Li"
                    },
                    {
                        "authorId": "144931569",
                        "name": "Kan Ren"
                    },
                    {
                        "authorId": "2135622500",
                        "name": "Yifan Yang"
                    },
                    {
                        "authorId": "2046022",
                        "name": "Xinyang Jiang"
                    },
                    {
                        "authorId": "2108623481",
                        "name": "Yuqing Yang"
                    },
                    {
                        "authorId": "2181524288",
                        "name": "Dongsheng Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Unfortunately, the severity of these mistakes is compounded by the fact that the predictions computed by neural networks are often overconfident (Guo et al., 2017), partly due to overfitting (Thulasidasan et al., 2019; Ovadia et al., 2019).",
                ", 2017), partly due to overfitting (Thulasidasan et al., 2019; Ovadia et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "346f6655aaed055075635f6ad770abb7c5020e2f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-11556",
                    "ArXiv": "2301.11556",
                    "DOI": "10.48550/arXiv.2301.11556",
                    "CorpusId": 256358336
                },
                "corpusId": 256358336,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/346f6655aaed055075635f6ad770abb7c5020e2f",
                "title": "Conformal inference is (almost) free for neural networks trained with early stopping",
                "abstract": "Early stopping based on hold-out data is a popular regularization technique designed to mitigate overfitting and increase the predictive accuracy of neural networks. Models trained with early stopping often provide relatively accurate predictions, but they generally still lack precise statistical guarantees unless they are further calibrated using independent hold-out data. This paper addresses the above limitation with conformalized early stopping: a novel method that combines early stopping with conformal calibration while efficiently recycling the same hold-out data. This leads to models that are both accurate and able to provide exact predictive inferences without multiple data splits nor overly conservative adjustments. Practical implementations are developed for different learning tasks -- outlier detection, multi-class classification, regression -- and their competitive performance is demonstrated on real data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2148335724",
                        "name": "Zi-Chen Liang"
                    },
                    {
                        "authorId": "2150921497",
                        "name": "Yan Zhou"
                    },
                    {
                        "authorId": "15178977",
                        "name": "Matteo Sesia"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026a transformation that maps the raw outputs of classifiers to their expected probabilities Kull et al. (2019); Guo et al. (2017a); Gupta and Ramdas (2021), and ad-hoc methods that adapt the training process to produce better calibrated models Thulasidasan et al. (2019); Hendrycks et al. (2019a).",
                "Important techniques in this category include mixup training Thulasidasan et al. (2019), pre-training Hendrycks et al. (2019a), label-smoothing Mu\u0308ller et al. (2019), data augmentation Ashukha et al. (2020), self-supervised learning Hendrycks et al. (2019b), Bayesian approximation (MC-dropout) Gal\u2026"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6d0dcd69c3eeccc9946d4967fc0f50d2391f5aa3",
                "externalIds": {
                    "ArXiv": "2301.04452",
                    "DBLP": "journals/corr/abs-2301-04452",
                    "DOI": "10.48550/arXiv.2301.04452",
                    "CorpusId": 255595506
                },
                "corpusId": 255595506,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6d0dcd69c3eeccc9946d4967fc0f50d2391f5aa3",
                "title": "Uncertainty Estimation based on Geometric Separation",
                "abstract": "In machine learning, accurately predicting the probability that a speci\ufb01c input is correct is crucial for risk management. This process, known as uncertainty (or con\ufb01dence) estimation, is particularly important in mission-critical applications such as autonomous driving. In this work, we put for-ward a novel geometric-based approach for improving uncertainty estimations in machine learning models. Our approach involves using the geometric distance of the current input from existing training inputs as a signal for estimating uncertainty, and then calibrating this signal using standard post-hoc techniques. We demonstrate that our method leads to more accurate uncertainty estimations than recently proposed approaches through extensive evaluation on a variety of datasets and models. Additionally, we optimize our approach so that it can be implemented on large datasets in near real-time applications, making it suitable for time-sensitive scenarios.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2172503795",
                        "name": "Gabriella Chouraqui"
                    },
                    {
                        "authorId": "2432326",
                        "name": "L. Cohen"
                    },
                    {
                        "authorId": "2651037",
                        "name": "Gil Einziger"
                    },
                    {
                        "authorId": "2172484861",
                        "name": "Liel Leman"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "10fc2262e94c33df16f4ea62a8b700686017208e",
                "externalIds": {
                    "DBLP": "journals/nn/LinHHTCTH23",
                    "DOI": "10.1016/j.neunet.2023.01.018",
                    "CorpusId": 256292370,
                    "PubMed": "36736002"
                },
                "corpusId": 256292370,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/10fc2262e94c33df16f4ea62a8b700686017208e",
                "title": "DEFAEK: Domain Effective Fast Adaptive Network for Face Anti-Spoofing",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2125097609",
                        "name": "Jiun-Da Lin"
                    },
                    {
                        "authorId": "2186730645",
                        "name": "Yue Han"
                    },
                    {
                        "authorId": "2119406938",
                        "name": "Po-Han Huang"
                    },
                    {
                        "authorId": "1382569223",
                        "name": "Julianne Tan"
                    },
                    {
                        "authorId": "36407236",
                        "name": "Jun-Cheng Chen"
                    },
                    {
                        "authorId": "47829536",
                        "name": "M. Tanveer"
                    },
                    {
                        "authorId": "145525478",
                        "name": "K. Hua"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mathematically, x\u03bb = \u03bbx1 + (1 \u2212 \u03bb)x2 (4) y\u03bb \u223c Py\u03bb = \u03bbPy1 + (1 \u2212 \u03bb)Py2 (5)"
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "91d791c8d31a369457715313cbe29cd812b79049",
                "externalIds": {
                    "DOI": "10.1117/12.2661854",
                    "CorpusId": 255264468
                },
                "corpusId": 255264468,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/91d791c8d31a369457715313cbe29cd812b79049",
                "title": "Interpolation-aware models for train-test consistency in mixup",
                "abstract": "Mixup is a learning principle that trains a neural network on convex combinations of pairs of examples and their labels. Despite of its good performance, there is an inherent inconsistency between training and testing in mixup, which makes theoretical understanding difficult and hurts the performance in some cases. In this work, we propose \u03bb-mixup to alleviate this inconsistency. Specifically, \u03bb-mixup reformulates the model to take the interpolation coefficient (\ud835\udf06) as input as well, so that a class of models indexed by \ud835\udf06 is learned and we can select one specific coefficient or multiple coefficients for ensembles depending on the testing distribution. We theoretically demonstrate that, with enough data and model capacity, \u03bb-mixup can recover the original conditional distribution. Moreover, we conduct image classification tasks on multiple datasets, including CIFAR-10, CIFAR-100 and Tiny-Imagenet, showing that comparing with mixup, \u03bb-mixup exhibits better generalization, calibration and robustness to adversarial attacks and out-of-distribution transformations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2180500386",
                        "name": "Yinhan Hu"
                    },
                    {
                        "authorId": "1881723",
                        "name": "Congying Han"
                    },
                    {
                        "authorId": "7390729",
                        "name": "Tiande Guo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In this direction, diverse methods have been proposed to calibrate the model (during training) using the regularization-like techniques, thereby constraining the prediction\u2019s overconfidence (Thulasidasan et al., 2019; Mukhoti et al., 2020; Pereyra et al., 2017; Mu\u0308ller et al., 2019).",
                "Label smoothing and Mixup tend to regularize the DNN to prevent overconfidence (M\u00fcller et al., 2019; Thulasidasan et al., 2019).",
                "In this direction, diverse methods have been proposed to calibrate the model (during training) using the regularization-like techniques, thereby constraining the prediction\u2019s overconfidence (Thulasidasan et al., 2019; Mukhoti et al., 2020; Pereyra et al., 2017; M\u00fcller et al., 2019).",
                "Label smoothing and Mixup tend to regularize the DNN to prevent overconfidence (Mu\u0308ller et al., 2019; Thulasidasan et al., 2019)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "cf0d478b04b8bb91bbe4a383a052ab16456fa64b",
                "externalIds": {
                    "ArXiv": "2212.13621",
                    "DBLP": "journals/corr/abs-2212-13621",
                    "DOI": "10.48550/arXiv.2212.13621",
                    "CorpusId": 255186386
                },
                "corpusId": 255186386,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cf0d478b04b8bb91bbe4a383a052ab16456fa64b",
                "title": "Annealing Double-Head: An Architecture for Online Calibration of Deep Neural Networks",
                "abstract": "Model calibration, which is concerned with how frequently the model predicts correctly, not only plays a vital part in statistical model design, but also has substantial practical applications, such as optimal decision-making in the real world. However, it has been discovered that modern deep neural networks are generally poorly calibrated due to the overestimation (or underestimation) of predictive confidence, which is closely related to overfitting. In this paper, we propose Annealing Double-Head, a simple-to-implement but highly effective architecture for calibrating the DNN during training. To be precise, we construct an additional calibration head-a shallow neural network that typically has one latent layer-on top of the last latent layer in the normal model to map the logits to the aligned confidence. Furthermore, a simple Annealing technique that dynamically scales the logits by calibration head in training procedure is developed to improve its performance. Under both the in-distribution and distributional shift circumstances, we exhaustively evaluate our Annealing Double-Head architecture on multiple pairs of contemporary DNN architectures and vision and speech datasets. We demonstrate that our method achieves state-of-the-art model calibration performance without post-processing while simultaneously providing comparable predictive accuracy in comparison to other recently proposed calibration methods on a range of learning tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2065601125",
                        "name": "Erdong Guo"
                    },
                    {
                        "authorId": "144177076",
                        "name": "D. Draper"
                    },
                    {
                        "authorId": "39747992",
                        "name": "M. Iorio"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although data augmentation enlarges the size of training data and therefore helps meet the requirement for large training data, it can also result in poorly calibrated ensembles, especially when using modern data augmentation techniques such as mixup [61].",
                "Moreover, modern data augmentation techniques such as mixup [61] or exposure to out-of-domain examples [16, 44] have been proposed."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e3a009314bb307b803eaf963584eae5cac9e8b22",
                "externalIds": {
                    "DOI": "10.1145/3579654.3579661",
                    "CorpusId": 257508714
                },
                "corpusId": 257508714,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e3a009314bb307b803eaf963584eae5cac9e8b22",
                "title": "Regularization Strength Impact on Neural Network Ensembles",
                "abstract": "In the last decade, several approaches have been proposed for regularizing deeper and wider neural networks (NNs), which is of importance in areas like image classification. It is now common practice to incorporate several regularization approaches in the training procedure of NNs. However, the impact of regularization strength on the properties of an ensemble of NNs remains unclear. For this reason, the study empirically compared the impact of NNs built based on two different regularization strengths (weak regularization (WR) and strong regularization (SR)) on the properties of an ensemble, such as the magnitude of logits, classification accuracy, calibration error, and ability to separate true predictions (TPs) and false predictions (FPs). The comparison was based on results from different experiments conducted on three different models, datasets, and architectures. Experimental results show that the increase in regularization strength 1) reduces the magnitude of logits; 2) can increase or decrease the classification accuracy depending on the dataset and/or architecture; 3) increases the calibration error; and 4) can improve or harm the separability between TPs and FPs depending on the dataset, architecture, model type and/or FP type.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2211496826",
                        "name": "Cedrique Rovile Njieutcheu Tassi"
                    },
                    {
                        "authorId": "34804492",
                        "name": "A. B\u00f6rner"
                    },
                    {
                        "authorId": "1453548521",
                        "name": "Rudolph Triebel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Regularized training focuses on calibrating DNN over training, such as [29, 30, 40, 45].",
                "To calibrate DNNs\u2019 confidence in prediction, researchers have developed a rich line of works on image classification using regularized training [29,30,40,45], post-hoc processing [11,19,24,35], and Bayesian modeling [15, 18, 21, 46], to name a few."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fa2238778e9f591aa057a82ce64103073b7d3ad4",
                "externalIds": {
                    "DBLP": "conf/cvpr/WangGW23",
                    "ArXiv": "2212.12053",
                    "DOI": "10.1109/CVPR52729.2023.02265",
                    "CorpusId": 255096562
                },
                "corpusId": 255096562,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fa2238778e9f591aa057a82ce64103073b7d3ad4",
                "title": "On Calibrating Semantic Segmentation Models: Analyses and An Algorithm",
                "abstract": "We study the problem of semantic segmentation calibration. Lots of solutions have been proposed to approach model miscalibration of confidence in image classification. However, to date, confidence calibration research on se-mantic segmentation is still limited. We provide a system-atic study on the calibration of semantic segmentation models and propose a simple yet effective approach. First, we find that model capacity, crop size, multi-scale testing, and prediction correctness have impact on calibration. Among them, prediction correctness, especially misprediction, is more important to miscalibration due to over-confidence. Next, we propose a simple, unifying, and effective approach, namely selective scaling, by separating correct/incorrect prediction for scaling and more focusing on misprediction logit smoothing. Then, we study popular existing cali-bration methods and compare them with selective scaling on semantic segmentation calibration. We conduct exten-sive experiments with a variety of benchmarks on both in-domain and domain-shift calibration and show that selective scaling consistently outperforms other methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49370795",
                        "name": "Dongdong Wang"
                    },
                    {
                        "authorId": "40206014",
                        "name": "Boqing Gong"
                    },
                    {
                        "authorId": "49681507",
                        "name": "Liqiang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "TREC [138] Sentence Classification [135], [136], [137], [139], [140], [141], [142] Uncertainty & Calibration [106]",
                "ImageNet [101] Image Classification [14], [15], [26], [32], [35], [37], [38], [39], [40], [41], [43], [44], [46] [47], [48], [49], [50], [51], [52], [53], [54], [55], [57], [58], [59], [62] [63], [64], [65], [66], [67], [68], [70], [71], [72], [74], [75], [77], [79] [81], [82], [85], [102], [103], [104], [105] Robustness [14], [46], [49], [52], [57], [71], [74], [89], [91], [105], [106] Uncertainty & Calibration [52], [74], [86], [105], [106] Object Localization [15], [49], [52], [95] Pascal VOC [107] Object Detection [15], [46], [57], [58], [64], [66], [68], [70], [72], [102], [105], [108] [109], [110], [111], [112] Cityscapes [113] Object Detection [108], [112] MS-COCO [114] Image Captioning [15], [50], [54], [57], [66], [70], [102], [103], [105] ADE [115] Semantic Segmentation [50], [103], [104]",
                "STL [123] Image Classification [66] Semi-Supervised [93], [97] Uncertainty & Calibration [106] Robustness [106]",
                "Experiments conducted on several image classification benchmarks and models [106] demonstrate that mix-trained deep neural networks can significantly improve calibration.",
                "MR [134] Sentence Classification [135], [136], [137] Uncertainty & Calibration [106]"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "0d110d199d74a6fae14c9edb3492bd02bf0a517f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-10888",
                    "ArXiv": "2212.10888",
                    "DOI": "10.48550/arXiv.2212.10888",
                    "CorpusId": 254926498
                },
                "corpusId": 254926498,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0d110d199d74a6fae14c9edb3492bd02bf0a517f",
                "title": "A Survey of Mix-based Data Augmentation: Taxonomy, Methods, Applications, and Explainability",
                "abstract": "Data augmentation (DA) is indispensable in modern machine learning and deep neural networks. The basic idea of DA is to construct new training data to improve the model's generalization by adding slightly disturbed versions of existing data or synthesizing new data. In this work, we review a small but essential subset of DA -- Mix-based Data Augmentation (MixDA) that generates novel samples by mixing multiple examples. Unlike conventional DA approaches based on a single-sample operation or requiring domain knowledge, MixDA is more general in creating a broad spectrum of new data and has received increasing attention in the community. We begin with proposing a new taxonomy classifying MixDA into, Mixup-based, Cutmix-based, and hybrid approaches according to a hierarchical view of the data mix. Various MixDA techniques are then comprehensively reviewed in a more fine-grained way. Owing to its generalization, MixDA has penetrated a variety of applications which are also completely reviewed in this work. We also examine why MixDA works from different aspects of improving model performance, generalization, and calibration while explaining the model behavior based on the properties of MixDA. Finally, we recapitulate the critical findings and fundamental challenges of current MixDA studies, and outline the potential directions for future works. Different from previous related works that summarize the DA approaches in a specific domain (e.g., images or natural language processing) or only review a part of MixDA studies, we are the first to provide a systematical survey of MixDA in terms of its taxonomy, methodology, applications, and explainability. This work can serve as a roadmap to MixDA techniques and application reviews while providing promising directions for researchers interested in this exciting area.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "121135550",
                        "name": "Chengtai Cao"
                    },
                    {
                        "authorId": null,
                        "name": "Fan Zhou"
                    },
                    {
                        "authorId": "2143178343",
                        "name": "Yurou Dai"
                    },
                    {
                        "authorId": "2110324733",
                        "name": "Jianping Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Besides classification with rejection accuracy, many other metrics have been proposed, such as expected calibration error [3], thresholded adaptive calibration error [25], overconfidence error [26], and calibrated log-likelihood [24]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ee125e8e60f665bc3e8184b345ffdb5aef766456",
                "externalIds": {
                    "DOI": "10.1109/ISRITI56927.2022.10052966",
                    "CorpusId": 257408078
                },
                "corpusId": 257408078,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ee125e8e60f665bc3e8184b345ffdb5aef766456",
                "title": "Towards Robust Diabetic Retinopathy Classifier Using Natural Gradient Langevin Dynamics",
                "abstract": "Although many studies demonstrate high accuracy in medical image classification, the proposed methods lack the ability to express uncertainty in their prediction. Quantifying uncertainty is especially critical in medical applications, often involving shifted input data from training distributions due to different medical equipment and other factors. Bayesian deep learning (BDL) provides a principled mechanism to represent uncertainty. Models with well-calibrated uncertainty estimates have the ability to indicate when their output should or should not be trusted. Natural Gradient Langevin Dynamics (NGLD) is a class of scalable BDL methods based on Stochastic Gradient Langevin Dynamics (SGLD) with preconditioning. Recent BDL benchmark studies show that SGLD has a competitive predictive uncertainty quality compared to other BDL methods in image classification using general datasets such as MNIST and CIFAR. This work evaluates the performance and calibration of various approximate NGLD methods on DR classification tasks based on retinal fundus image data. The evaluation was conducted on an in-domain dataset and shifted dataset. Specifically, we use the DDR dataset for training and in-domain evaluation and the APTOS2019 dataset for shifted dataset evaluation. The results indicate that NGLD is more robust to dataset shift with 88.43 % accuracy compared to SGD with 80.74% accuracy (both with confidence threshold \u03c4 == 0.9).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2211002287",
                        "name": "Hanif Amal Robbani"
                    },
                    {
                        "authorId": "1760853",
                        "name": "A. Bustamam"
                    },
                    {
                        "authorId": "41033213",
                        "name": "Risman Adnan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Calibration methods with training procedure use data augmentation [44, 54] or modify training loss [22, 26, 33]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7d826dfb184be983018590c64cfb4a79349472a4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-02295",
                    "ArXiv": "2212.02295",
                    "DOI": "10.1109/CVPR52729.2023.01507",
                    "CorpusId": 254247081
                },
                "corpusId": 254247081,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7d826dfb184be983018590c64cfb4a79349472a4",
                "title": "Block Selection Method for Using Feature Norm in Out-of-Distribution Detection",
                "abstract": "Detecting out-of-distribution (OOD) inputs during the inference stage is crucial for deploying neural networks in the real world. Previous methods typically relied on the highly activated feature map outputted by the network. In this study, we revealed that the norm of the feature map obtained from a block other than the last block can serve as a better indicator for OOD detection. To leverage this insight, we propose a simple framework that comprises two metrics: FeatureNorm, which computes the norm of the feature map, and NormRatio, which calculates the ratio of FeatureNorm for ID and OOD samples to evaluate the OOD detection performance of each block. To identify the block that provides the largest difference between FeatureNorm of ID and FeatureNorm of OOD, we create jigsaw puzzles as pseudo OOD from ID training samples and compute NormRatio, selecting the block with the highest value. After identifying the suitable block, OOD detection using FeatureNorm outperforms other methods by reducing FPR95 by up to 52.77% on CIFAR10 benchmark and up to 48.53% on ImageNet benchmark. We demonstrate that our framework can generalize to various architectures and highlight the significance of block selection, which can also improve previous OOD detection methods. Our code is available at https://github.com/gistailab/block-selection-for-OOD-detection.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "83360169",
                        "name": "Yeonguk Yu"
                    },
                    {
                        "authorId": "1859825",
                        "name": "Sungho Shin"
                    },
                    {
                        "authorId": "121945666",
                        "name": "Seongju Lee"
                    },
                    {
                        "authorId": "2087929655",
                        "name": "C. Jun"
                    },
                    {
                        "authorId": "11093130",
                        "name": "Kyoobin Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "and overconfidence error which gives high weight to confident but wrong predictions, a situation that is of particular concern in medical applications [25],",
                "They include postprocessing methods such as isotonic regression [23], conformal prediction [24] and Platt scaling [17], as well as methods that modify the training process such as mixup [25, 26], modelling probability distributions of class probabilities with Dirichlet distributions [27] and the use of dropout during training and prediction [28].",
                "Dropout may also be combined with mixup training [25] and post-processing schemes such as Platt scaling [17] in the future."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "051d39966701af82e0a587e0dba24d3459b461be",
                "externalIds": {
                    "DOI": "10.1109/SPMB55497.2022.10014868",
                    "CorpusId": 256034454
                },
                "corpusId": 256034454,
                "publicationVenue": {
                    "id": "3ef53ada-b7b9-4a1d-bda8-c637d448722a",
                    "name": "IEEE Signal Processing in Medicine and Biology Symposium",
                    "type": "conference",
                    "alternate_names": [
                        "SPMB",
                        "IEEE Signal Process Med Biology Symp"
                    ],
                    "issn": "2372-7241",
                    "alternate_issns": [
                        "2473-716X"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1800605"
                },
                "url": "https://www.semanticscholar.org/paper/051d39966701af82e0a587e0dba24d3459b461be",
                "title": "Calibration of Automatic Seizure Detection Algorithms",
                "abstract": "An EEG seizure detection algorithm employed in a clinical setting is likely to encounter many EEG segments that are difficult to classify due to the complexity of EEG signals and small data sets frequently used to train seizure detectors. The detectors should therefore be able to notify the clinician when they are uncertain in their predictions and they should also be accurate for confident predictions. This would enable the clinician to focus mainly on the parts of the recording where confidence in predictions is low. Here we analyse the calibration of neonatal and adult seizure detection algorithms based on a convolutional neural network in terms of how well the output seizure/non-seizure probabilities estimate the corresponding empirical frequencies. We found that the detectors turned out to be overconfident, in particular when incorrectly predicting seizure segments as non-seizure segments. The calibration of both detectors, measured in terms of expected calibration error and overconfidence error, was improved noticeably with the use of Monte Carlo dropout. We find that a straightforward application of dropout during training and classification leads to a noticeable improvement in the calibration of EEG seizure detectors based on a convolutional neural network.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2042650877",
                        "name": "A. Borovac"
                    },
                    {
                        "authorId": "1690247",
                        "name": "T. Runarsson"
                    },
                    {
                        "authorId": "1825639122",
                        "name": "G. Thorvardsson"
                    },
                    {
                        "authorId": "2101245005",
                        "name": "S. Gudmundsson"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fa39aeda97d2ee18988bc02ca8a8cb74d1ec6aa3",
                "externalIds": {
                    "DBLP": "journals/ijon/WangZZL23",
                    "DOI": "10.1016/j.neucom.2022.12.011",
                    "CorpusId": 254391804
                },
                "corpusId": 254391804,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fa39aeda97d2ee18988bc02ca8a8cb74d1ec6aa3",
                "title": "Training with scaled logits to alleviate class-level over-fitting in few-shot learning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109050605",
                        "name": "Rui-Qi Wang"
                    },
                    {
                        "authorId": "2075372121",
                        "name": "Fei Zhu"
                    },
                    {
                        "authorId": "2870877",
                        "name": "Xu-Yao Zhang"
                    },
                    {
                        "authorId": "1689269",
                        "name": "Cheng-Lin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Thulasidasan et al. found that mixup training can improve the calibration of deep neural networks, which means that the predicted probabilities of the model are more closely aligned with the true underlying probabilities of the data (Thulasidasan et al., 2019).",
                "found that mixup training can improve the calibration of deep neural networks, which means that the predicted probabilities of the model are more closely aligned with the true underlying probabilities of the data (Thulasidasan et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "61cdaa147921c25fcec0705d067ceca0da749e74",
                "externalIds": {
                    "ArXiv": "2212.00214",
                    "CorpusId": 256231530
                },
                "corpusId": 256231530,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/61cdaa147921c25fcec0705d067ceca0da749e74",
                "title": "Test-Time Mixup Augmentation for Data and Class-Dependent Uncertainty Estimation in Deep Learning Image Classification",
                "abstract": "Uncertainty estimation of the trained deep learning networks is valuable for opti-mizing learning ef\ufb01ciency and evaluating the reliability of network predictions. In this paper, we propose a method for estimating uncertainty in deep learning image classi\ufb01cation using test-time mixup augmentation (TTMA). To improve the ability to distinguish correct and incorrect predictions in existing aleatoric uncertainty, we introduce the TTMA data uncertainty (TTMA-DU) by applying mixup augmentation to test data and measuring the entropy of the predicted label histogram. In addition to TTMA-DU, we propose the TTMA class-dependent uncertainty (TTMA-CDU), which captures aleatoric uncertainty speci\ufb01c to individual classes and provides insight into class confusion and class similarity within the trained network. We validate our proposed methods on the ISIC-18 skin lesion diagnosis dataset and the CIFAR-100 real-world image classi\ufb01cation dataset. Our experiments show that (1) TTMA-DU more effectively differentiates correct and incorrect predictions compared to existing uncertainty measures due to mixup perturbation, and (2) TTMA-CDU provides information on class confusion and class similarity for both datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "13621262",
                        "name": "Han S. Lee"
                    },
                    {
                        "authorId": "2220865249",
                        "name": "Haeil Lee"
                    },
                    {
                        "authorId": "2076106",
                        "name": "H. Hong"
                    },
                    {
                        "authorId": "1769295",
                        "name": "Junmo Kim"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "07a512e2bf81c186899f586de8bd0f315879c7e1",
                "externalIds": {
                    "DOI": "10.1101/2022.11.24.22282667",
                    "CorpusId": 254065657
                },
                "corpusId": 254065657,
                "publicationVenue": {
                    "id": "d5e5b5e7-54b1-4f53-82fc-4853f3e71c58",
                    "name": "medRxiv",
                    "type": "journal",
                    "url": "https://www.medrxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/07a512e2bf81c186899f586de8bd0f315879c7e1",
                "title": "Interpretable Detection of Epiretinal Membrane from Optical Coherence Tomography with Deep Neural Networks",
                "abstract": "Purpose: To automatically detect epiretinal membranes (ERM) in various OCT scans of the central and paracentral macula region and classify them by size using deep neural networks (DNNs). Methods: 11,061 OCT-images of 624 volume OCT scans (624 eyes of 461 patients) were included and graded according to the presence of an ERM and its size (small 100-1000m, large >1000m). The data set was divided into training, validation and test sets (comprising of 75%, 10%, 15% of the data, respectively). An ensemble of DNNs was trained and saliency maps were generated using Guided Backprob. OCT-scans were also transformed into a one-dimensional-value using t-SNE analysis. Results: The DNNs' receiver-operating-characteristics on the test set showed a high performance for no ERM, small ERM and large ERM cases (AUC: 0.99, 0.92, 0.99, respectively; 3-way accuracy: 89% ), with small ERMs being the most difficult ones to detect. t-SNE analysis sorted cases by size and, in particular, revealed increased classification uncertainty at the transitions between groups. Saliency maps reliably highlighted ERM, regardless of the presence of other OCT features (i.e. retinal thickening, intraretinal pseudo- cysts, epiretinal proliferation) and entities such as ERM-retinoschisis, macular pseudohole and lamellar macular hole. Conclusion: DNNs can reliably detect and grade ERMs according to their size not only in the fovea but also in the paracentral region. This is also achieved in cases of hard-to-detect, small ERMs. In addition, the generated saliency maps can be used effectively to highlight small ERMs that might otherwise be missed. The proposed model could be used for screening programs or decision support systems in the future.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145366222",
                        "name": "M. Ayhan"
                    },
                    {
                        "authorId": "2154046017",
                        "name": "J. Neubauer"
                    },
                    {
                        "authorId": "47488428",
                        "name": "M. Uzel"
                    },
                    {
                        "authorId": "4959578",
                        "name": "F. Gelisken"
                    },
                    {
                        "authorId": "1689077",
                        "name": "Philipp Berens"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, [42] and [31] found that Mixup can be used to improve the loss calibration and the robustness of the network.",
                "The introduction of \u03bbs requires the network prediction to be reliable, however, some recent papers [31, 41] found that deep networks tend to be under-confident when the \u03bb is sampled from a Beta distribution Beta(\u03b1, \u03b1) with \u03b1 > 0."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "62ce349a6dbc58f64ae02d7203c2f9a06cf6f6d4",
                "externalIds": {
                    "ArXiv": "2211.15846",
                    "DBLP": "journals/corr/abs-2211-15846",
                    "DOI": "10.48550/arXiv.2211.15846",
                    "CorpusId": 254069733
                },
                "corpusId": 254069733,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/62ce349a6dbc58f64ae02d7203c2f9a06cf6f6d4",
                "title": "LUMix: Improving Mixup by Better Modelling Label Uncertainty",
                "abstract": "Modern deep networks can be better generalized when trained with noisy samples and regularization techniques. Mixup and CutMix have been proven to be effective for data augmentation to help avoid overfitting. Previous Mixup-based methods linearly combine images and labels to generate additional training data. However, this is problematic if the object does not occupy the whole image as we demonstrate in Figure 1. Correctly assigning the label weights is hard even for human beings and there is no clear criterion to measure it. To tackle this problem, in this paper, we propose LUMix, which models such uncertainty by adding label perturbation during training. LUMix is simple as it can be implemented in just a few lines of code and can be universally applied to any deep networks \\eg CNNs and Vision Transformers, with minimal computational cost. Extensive experiments show that our LUMix can consistently boost the performance for networks with a wide range of diversity and capacity on ImageNet, \\eg $+0.7\\%$ for a small model DeiT-S and $+0.6\\%$ for a large variant XCiT-L. We also demonstrate that LUMix can lead to better robustness when evaluated on ImageNet-O and ImageNet-A. The source code can be found \\href{https://github.com/kevin-ssy/LUMix}{here}",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115306116",
                        "name": "Shuyang Sun"
                    },
                    {
                        "authorId": "90972805",
                        "name": "Jieneng Chen"
                    },
                    {
                        "authorId": "2053865518",
                        "name": "Ruifei He"
                    },
                    {
                        "authorId": "145081362",
                        "name": "A. Yuille"
                    },
                    {
                        "authorId": "143635540",
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "authorId": "2052830285",
                        "name": "Song Bai"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We believe this behavior to be generally desirable, especially for classifiers that provide well-calibrated predictions during training (Thulasidasan et al. 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8880f6e2e3d295a1aeed39ee24fdd8b05f261483",
                "externalIds": {
                    "ArXiv": "2211.15890",
                    "DBLP": "journals/corr/abs-2211-15890",
                    "DOI": "10.48550/arXiv.2211.15890",
                    "CorpusId": 254069986
                },
                "corpusId": 254069986,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8880f6e2e3d295a1aeed39ee24fdd8b05f261483",
                "title": "On Robust Learning from Noisy Labels: A Permutation Layer Approach",
                "abstract": "The existence of label noise imposes significant challenges (e.g., poor generalization) on the training process of deep neural networks (DNN). As a remedy, this paper introduces a permutation layer learning approach termed PermLL to dynamically calibrate the training process of the DNN subject to instance-dependent and instance-independent label noise. The proposed method augments the architecture of a conventional DNN by an instance-dependent permutation layer. This layer is essentially a convex combination of permutation matrices that is dynamically calibrated for each sample. The primary objective of the permutation layer is to correct the loss of noisy samples mitigating the effect of label noise. We provide two variants of PermLL in this paper: one applies the permutation layer to the model's prediction, while the other applies it directly to the given noisy label. In addition, we provide a theoretical comparison between the two variants and show that previous methods can be seen as one of the variants. Finally, we validate PermLL experimentally and show that it achieves state-of-the-art performance on both real and synthetic datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "151199415",
                        "name": "Salman Alsubaihi"
                    },
                    {
                        "authorId": "2192705888",
                        "name": "Mohammed Alkhrashi"
                    },
                    {
                        "authorId": "2134875",
                        "name": "Raied Aljadaany"
                    },
                    {
                        "authorId": "8403201",
                        "name": "Fahad Albalawi"
                    },
                    {
                        "authorId": "2931652",
                        "name": "Bernard Ghanem"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Confidence reliability is a very important issue for DNNs which can be used for various tasks like: misclassification detection [6], [23], confidence calibration [7], [24] and out-of-distribution detection [8], [30].",
                "1, the reliability of confidence can be evaluated by three tasks: misclassification detection [6], [23], confidence calibration [7], [24] and outof-distribution detection [8], [25], which have attracted growing attention in the machine learning community recently.",
                "[24] found that the predicted scores of DNNs trained with mixup [11] are better indicators of the actual likelihood of a prediction.",
                "One class of approaches [24], [38] aims to learn well-calibratedmodels during training.",
                "To demonstrate the superiority of classAug over dataAug, we have made comparison between classAug and state-of-the-art data augmentation techniques, such as mixup [24], Cutmix [12], and RandAug [74]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "bce31b5db69e6caea6b70455b499d1db188341ab",
                "externalIds": {
                    "DBLP": "journals/pami/ZhuZWL23",
                    "DOI": "10.1109/TPAMI.2022.3225117",
                    "CorpusId": 254068790,
                    "PubMed": "36441890"
                },
                "corpusId": 254068790,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bce31b5db69e6caea6b70455b499d1db188341ab",
                "title": "Learning by Seeing More Classes",
                "abstract": "Traditional pattern recognition models usually assume a fixed and identical number of classes during both training and inference stages. In this paper, we study an interesting but ignored question: can increasing the number of classes during training improve the generalization and reliability performance? For a <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"zhang-ieq1-3225117.gif\"/></alternatives></inline-formula>-class problem, instead of training with only these <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"zhang-ieq2-3225117.gif\"/></alternatives></inline-formula> classes, we propose to learn with <inline-formula><tex-math notation=\"LaTeX\">$k+m$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq3-3225117.gif\"/></alternatives></inline-formula> classes, where the additional <inline-formula><tex-math notation=\"LaTeX\">$m$</tex-math><alternatives><mml:math><mml:mi>m</mml:mi></mml:math><inline-graphic xlink:href=\"zhang-ieq4-3225117.gif\"/></alternatives></inline-formula> classes can be either real classes from other datasets or synthesized from known classes. Specifically, we propose two strategies for constructing new classes from known classes. By making the model see more classes during training, we can obtain several advantages. First, the added <inline-formula><tex-math notation=\"LaTeX\">$m$</tex-math><alternatives><mml:math><mml:mi>m</mml:mi></mml:math><inline-graphic xlink:href=\"zhang-ieq5-3225117.gif\"/></alternatives></inline-formula> classes serve as a regularization which is helpful to improve the generalization accuracy on the original <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"zhang-ieq6-3225117.gif\"/></alternatives></inline-formula> classes. Second, this will alleviate the overconfident phenomenon and produce more reliable confidence estimation for different tasks like misclassification detection, confidence calibration, and out-of-distribution detection. Lastly, the additional classes can also improve the learned feature representation, which is beneficial for new classes generalization in few-shot learning and class-incremental learning. Compared with the widely proved concept of data augmentation (dataAug), our method is driven from another dimension of augmentation based on additional classes (classAug). Comprehensive experiments demonstrated the superiority of our classAug under various open-environment metrics on benchmark datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2075372121",
                        "name": "Fei Zhu"
                    },
                    {
                        "authorId": "2870877",
                        "name": "Xu-Yao Zhang"
                    },
                    {
                        "authorId": "2109050605",
                        "name": "Rui-Qi Wang"
                    },
                    {
                        "authorId": "1689269",
                        "name": "Cheng-Lin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "tion (Zhong et al. 2021b; Thulasidasan et al. 2019), it has shown a limitation in balancing distributions thereby losing overall accuracy.",
                "Although Mixup achieves performance gains in terms of AP by its positive impacts on confidence calibra-\ntion (Zhong et al. 2021b; Thulasidasan et al. 2019), it has shown a limitation in balancing distributions thereby losing overall accuracy."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5020cc02130c7625836f28969aa632eb87d83f28",
                "externalIds": {
                    "DBLP": "conf/aaai/MoonSH23",
                    "ArXiv": "2211.13471",
                    "DOI": "10.48550/arXiv.2211.13471",
                    "CorpusId": 254018016
                },
                "corpusId": 254018016,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/5020cc02130c7625836f28969aa632eb87d83f28",
                "title": "Minority-Oriented Vicinity Expansion with Attentive Aggregation for Video Long-Tailed Recognition",
                "abstract": "A dramatic increase in real-world video volume with extremely diverse and emerging topics naturally forms a long-tailed video distribution in terms of their categories, and it spotlights the need for Video Long-Tailed Recognition (VLTR).\nIn this work, we summarize the challenges in VLTR and explore how to overcome them.\nThe challenges are: (1) it is impractical to re-train the whole model for high-quality features, (2) acquiring frame-wise labels requires extensive cost, and (3) long-tailed data triggers biased training.\nYet, most existing works for VLTR unavoidably utilize image-level features extracted from pretrained models which are task-irrelevant, and learn by video-level labels.\nTherefore, to deal with such (1) task-irrelevant features and (2) video-level labels, we introduce two complementary learnable feature aggregators.\nLearnable layers in each aggregator are to produce task-relevant representations, and each aggregator is to assemble the snippet-wise knowledge into a video representative.\nThen, we propose Minority-Oriented Vicinity Expansion (MOVE) that explicitly leverages the class frequency into approximating the vicinity distributions to alleviate (3) biased training.\nBy combining these solutions, our approach achieves state-of-the-art results on large-scale VideoLT and synthetically induced Imbalanced-MiniKinetics200. \nWith VideoLT features from ResNet-50, it attains 18% and 58% relative improvements on head and tail classes over the previous state-of-the-art method, respectively. \nCode and dataset are available at https://github.com/wjun0830/MOVE.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2067245729",
                        "name": "WonJun Moon"
                    },
                    {
                        "authorId": "77363082",
                        "name": "Hyun Seok Seong"
                    },
                    {
                        "authorId": "7212202",
                        "name": "Jae-Pil Heo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To remedy this problem, various approaches have been proposed including Monte Carlo dropout [15], latent Gaussian processes [51], deep ensembles [32], training multiple independent subnetworks [20], and augmentation [49].",
                "Unfortunately, several lines of work observe that modern neural networks lack such calibration [18, 28, 30, 42, 49], particularly under distribution shift [42]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e3efdaa40ad4c9a604f98252fbc5231f923fedfd",
                "externalIds": {
                    "ArXiv": "2211.10193",
                    "DBLP": "journals/corr/abs-2211-10193",
                    "DOI": "10.48550/arXiv.2211.10193",
                    "CorpusId": 253707877
                },
                "corpusId": 253707877,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e3efdaa40ad4c9a604f98252fbc5231f923fedfd",
                "title": "Layer-Stack Temperature Scaling",
                "abstract": "Recent works demonstrate that early layers in a neural network contain useful information for prediction. Inspired by this, we show that extending temperature scaling across all layers improves both calibration and accuracy. We call this procedure\"layer-stack temperature scaling\"(LATES). Informally, LATES grants each layer a weighted vote during inference. We evaluate it on five popular convolutional neural network architectures both in- and out-of-distribution and observe a consistent improvement over temperature scaling in terms of accuracy, calibration, and AUC. All conclusions are supported by comprehensive statistical analyses. Since LATES neither retrains the architecture nor introduces many more parameters, its advantages can be reaped without requiring additional data beyond what is used in temperature scaling. Finally, we show that combining LATES with Monte Carlo Dropout matches state-of-the-art results on CIFAR10/100.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2141435213",
                        "name": "Amr Khalifa"
                    },
                    {
                        "authorId": "144473519",
                        "name": "M. Mozer"
                    },
                    {
                        "authorId": "2812848",
                        "name": "Hanie Sedghi"
                    },
                    {
                        "authorId": "3007442",
                        "name": "Behnam Neyshabur"
                    },
                    {
                        "authorId": "2922782",
                        "name": "Ibrahim M. Alabdulmohsin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "362487e012fdb97b4572371038a18e4b943fe50c",
                "externalIds": {
                    "ArXiv": "2211.07692",
                    "DBLP": "journals/corr/abs-2211-07692",
                    "DOI": "10.48550/arXiv.2211.07692",
                    "CorpusId": 253523533
                },
                "corpusId": 253523533,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/362487e012fdb97b4572371038a18e4b943fe50c",
                "title": "Self-training of Machine Learning Models for Liver Histopathology: Generalization under Clinical Shifts",
                "abstract": "Histopathology images are gigapixel-sized and include features and information at di\ufb00erent resolutions. Collecting annotations in histopathology requires highly specialized pathologists, mak-ing it expensive and time-consuming. Self-training can alleviate annotation constraints by learning from both labeled and unlabeled data, reducing the amount of annotations required from pathologists. We study the design of teacher-student self-training systems for Non-alcoholic Steatohepatitis (NASH) using clinical histopathology datasets with limited annotations. We evaluate the models on in-distribution and out-of-distribution test data under clinical data shifts. We demonstrate that through self-training, the best student model statistically outperforms the teacher with a 3% absolute di\ufb00erence on the macro F1 score. The best student model also approaches the performance of a fully supervised model trained with twice as many annotations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2133302463",
                        "name": "Jin Li"
                    },
                    {
                        "authorId": "145882781",
                        "name": "Deepta Rajan"
                    },
                    {
                        "authorId": "2190328218",
                        "name": "Chintan Shah"
                    },
                    {
                        "authorId": "2122349954",
                        "name": "Dinkar Juyal"
                    },
                    {
                        "authorId": "2075404504",
                        "name": "S. Chakraborty"
                    },
                    {
                        "authorId": "1752757585",
                        "name": "Chandan Akiti"
                    },
                    {
                        "authorId": "38099788",
                        "name": "F. Kos"
                    },
                    {
                        "authorId": "50037358",
                        "name": "Janani Iyer"
                    },
                    {
                        "authorId": "115939921",
                        "name": "A. Sampat"
                    },
                    {
                        "authorId": "46888811",
                        "name": "A. Behrooz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This method was originally proposed as data agnostic approach which also shows good results if applied to image data [4,2,16]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "62783fc3ec90c765af00dbc407d05752baf71d31",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-05862",
                    "ArXiv": "2211.05862",
                    "DOI": "10.48550/arXiv.2211.05862",
                    "CorpusId": 253499234
                },
                "corpusId": 253499234,
                "publicationVenue": {
                    "id": "61a709e3-2060-423c-8de5-ffd3885aa31c",
                    "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                    "type": "conference",
                    "alternate_names": [
                        "Medical Image Computing and Computer-Assisted Intervention",
                        "MICCAI",
                        "Med Image Comput Comput Interv",
                        "Int Conf Med Image Comput Comput Interv"
                    ],
                    "url": "http://www.miccai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/62783fc3ec90c765af00dbc407d05752baf71d31",
                "title": "MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis",
                "abstract": "Multiple instance learning exhibits a powerful approach for whole slide image-based diagnosis in the absence of pixel- or patch-level annotations. In spite of the huge size of hole slide images, the number of individual slides is often rather small, leading to a small number of labeled samples. To improve training, we propose and investigate different data augmentation strategies for multiple instance learning based on the idea of linear interpolations of feature vectors (known as MixUp). Based on state-of-the-art multiple instance learning architectures and two thyroid cancer data sets, an exhaustive study is conducted considering a range of common data augmentation strategies. Whereas a strategy based on to the original MixUp approach showed decreases in accuracy, the use of a novel intra-slide interpolation method led to consistent increases in accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1833662",
                        "name": "M. Gadermayr"
                    },
                    {
                        "authorId": "2150728148",
                        "name": "Lukas Koller"
                    },
                    {
                        "authorId": "151140447",
                        "name": "M. Tschuchnig"
                    },
                    {
                        "authorId": "1503825580",
                        "name": "L. Stangassinger"
                    },
                    {
                        "authorId": "39705772",
                        "name": "Christina Kreutzer"
                    },
                    {
                        "authorId": "1382340422",
                        "name": "S. Couillard-Despr\u00e9s"
                    },
                    {
                        "authorId": "4298846",
                        "name": "G. Oostingh"
                    },
                    {
                        "authorId": "49131923",
                        "name": "A. Hittmair"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For Mixup and Cutmix, we perform experiments using all combinations of \u03b1 values in the set {0.1, 0.4, 0.8, 1.0, 1.2}.",
                "\u2026models, including post-hoc softmax temperature scaling (Guo et al., 2017), ensemble-based techniques (Lakshminarayanan et al., 2017), Mixup (Zhang et al., 2017; Thulasidasan et al., 2019), Monte-Carlo dropout, and uncertainty estimates in Bayesian networks (Neal, 2012; Gal & Ghahramani, 2016).",
                "Such simplicity makes CSSL\n5Mixup and Cutmix are not performed simultaneously.",
                "For AutoAugment, we utilize the ImageNet augmentation policy, and \u03b1 values of 0.1 and 0.8 are again adopted for Mixup and CutMix, respectively.",
                "Numerous methodologies have been proposed for producing calibrated models, including post-hoc softmax temperature scaling (Guo et al., 2017), ensemble-based techniques (Lakshminarayanan et al., 2017), Mixup (Zhang et al., 2017; Thulasidasan et al., 2019), Monte-Carlo dropout, and uncertainty estimates in Bayesian networks (Neal, 2012; Gal & Ghahramani, 2016).",
                "REMIND also leverages random resized crops and manifold Mixup (Verma et al., 2019) on the feature representations stored within the replay buffer throughout streaming.",
                "The results of these experiments, reported in terms of Top-1 \u2126all, are provided in Table 10, where it can be seen that the best results are achieved by combining Mixup, Cutmix, and AutoAugment techniques into a single augmentation policy.",
                ", 2017), Mixup (Zhang et al., 2017; Thulasidasan et al., 2019), Monte-Carlo dropout, and uncertainty estimates in Bayesian networks (Neal, 2012; Gal & Ghahramani, 2016).",
                "Interpolation methods (e.g., Mixup (Zhang et al., 2017; Wolfe & Lundgaard, 2019; Inoue, 2018) and CutMix (Yun et al., 2019)) take stochasticallyweighted combinations of images and label pairs during training, which provides regularization benefits.",
                "In particular, we perform experiments with Mixup, CutMix, and SamplePairing interpolation strategies13 for class-incremental learning on the CIFAR100 dataset, adopting the same experimental setting as described in Section 6.1.",
                "In particular, CSSL combines random crops and flips, Mixup, Cutmix, and\nAutoaugment into a single, sequential augmentation policy5; see Appendix B.1 for further details.",
                "We test all combinations of the Mixup, Cutmix, AutoAugment, and RandAugment data augmentation strategies to arrive at the final, optimal augmentation policy used within the proposed methodology for CSSL.",
                "13The \u03b1 values used for Mixup and Cutmix are tuned by searching over the set {0.1, 0.4, 0.8, 1.0, 1.2} using a hold-out validation set, and we present the results of the best-performing \u03b1 for each method.",
                "Previous work indicates that using Mixup encourage good calibration properties (Thulasidasan et al., 2019), indicating that models obtained from CSSL and REMIND\u2014both of which use some form of Mixup\u2014should be highly-calibrated.",
                "Mixup and Cutmix use \u03b1 values of 0.1 and 0.8, respectively, and AutoAugment adopts the Imagenet learned augmentation policy.",
                "Our policy randomly chooses between Mixup or Cutmix for each data example with equal probability.",
                "For Mixup and Cutmix, we utilize \u03b1 values of 0.1 and 0.8, respectively."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ca78e62e12dc60e49c5b88176b5a1e96223d44d2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-04624",
                    "ArXiv": "2211.04624",
                    "DOI": "10.48550/arXiv.2211.04624",
                    "CorpusId": 253420726
                },
                "corpusId": 253420726,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ca78e62e12dc60e49c5b88176b5a1e96223d44d2",
                "title": "Cold Start Streaming Learning for Deep Networks",
                "abstract": "The ability to dynamically adapt neural networks to newly-available data without performance deterioration would revolutionize deep learning applications. Streaming learning (i.e., learning from one data example at a time) has the potential to enable such real-time adaptation, but current approaches i) freeze a majority of network parameters during streaming and ii) are dependent upon offline, base initialization procedures over large subsets of data, which damages performance and limits applicability. To mitigate these shortcomings, we propose Cold Start Streaming Learning (CSSL), a simple, end-to-end approach for streaming learning with deep networks that uses a combination of replay and data augmentation to avoid catastrophic forgetting. Because CSSL updates all model parameters during streaming, the algorithm is capable of beginning streaming from a random initialization, making base initialization optional. Going further, the algorithm\u2019s simplicity allows theoretical convergence guarantees to be derived using analysis of the Neural Tangent Random Feature (NTRF). In experiments, we find that CSSL outperforms existing baselines for streaming learning in experiments on CIFAR100, ImageNet, and Core50 datasets. Additionally, we propose a novel multi-task streaming learning setting and show that CSSL performs favorably in this domain. Put simply, CSSL performs well and demonstrates that the complicated, multi-step training pipelines adopted by most streaming methodologies can be replaced with a simple, end-toend learning approach without sacrificing performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "34210029",
                        "name": "Cameron R. Wolfe"
                    },
                    {
                        "authorId": "2126894228",
                        "name": "Anastasios Kyrillidis"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "abd34ce2e04c03bfc8f5370a4741ef2feaf6686f",
                "externalIds": {
                    "DBLP": "conf/uai/CollinsBLPSLW23",
                    "ArXiv": "2211.01202",
                    "CorpusId": 257405493
                },
                "corpusId": 257405493,
                "publicationVenue": {
                    "id": "f9af8000-42f8-410d-a622-e8811e41660a",
                    "name": "Conference on Uncertainty in Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Uncertainty in Artificial Intelligence",
                        "UAI",
                        "Conf Uncertain Artif Intell",
                        "Uncertain Artif Intell"
                    ],
                    "url": "http://www.auai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/abd34ce2e04c03bfc8f5370a4741ef2feaf6686f",
                "title": "Human-in-the-Loop Mixup",
                "abstract": "Aligning model representations to humans has been found to improve robustness and generalization. However, such methods often focus on standard observational data. Synthetic data is proliferating and powering many advances in machine learning; yet, it is not always clear whether synthetic labels are perceptually aligned to humans -- rendering it likely model representations are not human aligned. We focus on the synthetic data used in mixup: a powerful regularizer shown to improve model robustness, generalization, and calibration. We design a comprehensive series of elicitation interfaces, which we release as HILL MixE Suite, and recruit 159 participants to provide perceptual judgments along with their uncertainties, over mixup examples. We find that human perceptions do not consistently align with the labels traditionally used for synthetic points, and begin to demonstrate the applicability of these findings to potentially increase the reliability of downstream models, particularly when incorporating human uncertainty. We release all elicited judgments in a new data hub we call H-Mix.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2055306799",
                        "name": "Katherine M. Collins"
                    },
                    {
                        "authorId": "32326200",
                        "name": "Umang Bhatt"
                    },
                    {
                        "authorId": "36326884",
                        "name": "Weiyang Liu"
                    },
                    {
                        "authorId": "2748067",
                        "name": "Vihari Piratla"
                    },
                    {
                        "authorId": "66747016",
                        "name": "Ilia Sucholutsky"
                    },
                    {
                        "authorId": "10129234",
                        "name": "B. Love"
                    },
                    {
                        "authorId": "145689461",
                        "name": "Adrian Weller"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Additionally, mixup training has been shown to improve network calibration for both in- and out-of-distribution data [14].",
                "[14] Sunil Thulasidasan, Gopinath Chennupati, Jeff Bilmes, Tanmoy Bhattacharya, and Sarah Michalak."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "125fbbde9d7bbeead27715fc48de37fd805861d4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-16413",
                    "ArXiv": "2210.16413",
                    "DOI": "10.48550/arXiv.2210.16413",
                    "CorpusId": 253237243
                },
                "corpusId": 253237243,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/125fbbde9d7bbeead27715fc48de37fd805861d4",
                "title": "When does mixup promote local linearity in learned representations?",
                "abstract": "mixup is a regularization technique that arti\ufb01cially produces new samples using convex combinations of original training points. This simple technique has shown strong empirical performance, and has been heavily used as part of semi-supervised learning techniques such as mixmatch [1] and interpolation consistent training (ICT) [17]. In this paper, we look at mixup through a representation learning lens in a semi-supervised learning setup. In particular, we study the role of mixup in promoting linearity in the learned network representations. Towards this, we study two questions: (1) how does the mixup loss that enforces linearity in the last network layer propagate the linearity to the earlier layers?; and (2) how does the enforcement of stronger mixup loss on more than two data points affect the convergence of training? We empirically investigate these properties of mixup on vision datasets such as CIFAR-10, CIFAR-100 and SVHN. Our results show that supervised mixup training does not make all the network layers linear; in fact the intermediate layers become more non-linear during mixup training compared to a network that is trained without mixup . However, when mixup is used as an unsupervised loss, we observe that all the network layers become more linear resulting in faster training convergence.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "22235380",
                        "name": "Arslan Chaudhry"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2799898",
                        "name": "Andreas Veit"
                    },
                    {
                        "authorId": "3078751",
                        "name": "Sadeep Jayasumana"
                    },
                    {
                        "authorId": "145686644",
                        "name": "S. Ramalingam"
                    },
                    {
                        "authorId": "49596260",
                        "name": "Surinder Kumar"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "41e68a78f5bd266b1ae54d521ebd0be0e9314cd8",
                "externalIds": {
                    "ArXiv": "2210.15198",
                    "DBLP": "journals/corr/abs-2210-15198",
                    "DOI": "10.48550/arXiv.2210.15198",
                    "CorpusId": 253157655
                },
                "corpusId": 253157655,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/41e68a78f5bd266b1ae54d521ebd0be0e9314cd8",
                "title": "Watermarking for Out-of-distribution Detection",
                "abstract": "Out-of-distribution (OOD) detection aims to identify OOD data based on representations extracted from well-trained deep models. However, existing methods largely ignore the reprogramming property of deep models and thus may not fully unleash their intrinsic strength: without modifying parameters of a well-trained deep model, we can reprogram this model for a new purpose via data-level manipulation (e.g., adding a specific feature perturbation to the data). This property motivates us to reprogram a classification model to excel at OOD detection (a new task), and thus we propose a general methodology named watermarking in this paper. Specifically, we learn a unified pattern that is superimposed onto features of original data, and the model's detection capability is largely boosted after watermarking. Extensive experiments verify the effectiveness of watermarking, demonstrating the significance of the reprogramming property of deep models in OOD detection.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "123451812",
                        "name": "Qizhou Wang"
                    },
                    {
                        "authorId": "2152943340",
                        "name": "Feng Liu"
                    },
                    {
                        "authorId": "2109116068",
                        "name": "Yonggang Zhang"
                    },
                    {
                        "authorId": "2155703272",
                        "name": "Jing Zhang"
                    },
                    {
                        "authorId": "2171109070",
                        "name": "Chen Gong"
                    },
                    {
                        "authorId": "121698214",
                        "name": "Tongliang Liu"
                    },
                    {
                        "authorId": "2153287285",
                        "name": "Bo Han"
                    }
                ]
            }
        },
        {
            "contexts": [
                "compared to standard training over different model architectures, tasks, and domains (Liang et al., 2018; He et al., 2019; Thulasidasan et al., 2019; Lamb et al., 2019; Arazo et al., 2019; Guo, 2020; Verma et al., 2021b; Wang et al., 2021)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0aa7dcf0e94308ed1a5ad1e9064f9f965d257a90",
                "externalIds": {
                    "DBLP": "conf/icml/Chidambaram00023",
                    "ArXiv": "2210.13512",
                    "DOI": "10.48550/arXiv.2210.13512",
                    "CorpusId": 253107555
                },
                "corpusId": 253107555,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0aa7dcf0e94308ed1a5ad1e9064f9f965d257a90",
                "title": "Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup",
                "abstract": "Mixup is a data augmentation technique that relies on training using random convex combinations of data points and their labels. In recent years, Mixup has become a standard primitive used in the training of state-of-the-art image classification models due to its demonstrated benefits over empirical risk minimization with regards to generalization and robustness. In this work, we try to explain some of this success from a feature learning perspective. We focus our attention on classification problems in which each class may have multiple associated features (or views) that can be used to predict the class correctly. Our main theoretical results demonstrate that, for a non-trivial class of data distributions with two features per class, training a 2-layer convolutional network using empirical risk minimization can lead to learning only one feature for almost all classes while training with a specific instantiation of Mixup succeeds in learning both features for every class. We also show empirically that these theoretical insights extend to the practical settings of image benchmarks modified to have multiple features.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2065619709",
                        "name": "Muthuraman Chidambaram"
                    },
                    {
                        "authorId": "2144799011",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "2277966",
                        "name": "Chenwei Wu"
                    },
                    {
                        "authorId": "144804200",
                        "name": "Rong Ge"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Also, regularization during training such as label smoothing [298] and mixup [138] have been shown to improve calibration [214, 228, 306]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1eaca22499c3b31cb01246bc9852f47e724c9762",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-11318",
                    "ArXiv": "2210.11318",
                    "DOI": "10.1145/3626186",
                    "CorpusId": 253018670
                },
                "corpusId": 253018670,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1eaca22499c3b31cb01246bc9852f47e724c9762",
                "title": "A Survey of Computer Vision Technologies In Urban and Controlled-environment Agriculture",
                "abstract": "In the evolution of agriculture to its next stage, Agriculture 5.0, artificial intelligence will play a central role. Controlled-environment agriculture, or CEA, is a special form of urban and suburban agricultural practice that offers numerous economic, environmental, and social benefits, including shorter transportation routes to population centers, reduced environmental impact, and increased productivity. Due to its ability to control environmental factors, CEA couples well with computer vision (CV) in the adoption of real-time monitoring of the plant conditions and autonomous cultivation and harvesting. The objective of this paper is to familiarize CV researchers with agricultural applications and agricultural practitioners with the solutions offered by CV. We identify five major CV applications in CEA, analyze their requirements and motivation, and survey the state of the art as reflected in 68 technical papers using deep learning methods. In addition, we discuss five key subareas of computer vision and how they related to these CEA problems, as well as fourteen vision-based CEA datasets. We hope the survey will help researchers quickly gain a bird-eye view of the striving research area and will spark inspiration for new research and development.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152365014",
                        "name": "Jiayun Luo"
                    },
                    {
                        "authorId": "1728712",
                        "name": "Boyang Albert Li"
                    },
                    {
                        "authorId": "2188346915",
                        "name": "Cyril Leung"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026been several studies testing this property in modern neural networks (Guo et al., 2017; Nixon et al., 2019; Minderer et al., 2021; Wang et al., 2021b) and proposing ways to improve it (Thulasidasan et al., 2019; Mukhoti et al., 2020; Karandikar et al., 2021; Zhao et al., 2021a; Tian et al., 2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "670a8db8e47cfe234558ed913242427a1b8b8348",
                "externalIds": {
                    "DBLP": "conf/emnlp/UlmerFH22",
                    "ArXiv": "2210.15452",
                    "DOI": "10.48550/arXiv.2210.15452",
                    "CorpusId": 253157331
                },
                "corpusId": 253157331,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/670a8db8e47cfe234558ed913242427a1b8b8348",
                "title": "Exploring Predictive Uncertainty and Calibration in NLP: A Study on the Impact of Method & Data Scarcity",
                "abstract": "We investigate the problem of determining the predictive confidence (or, conversely, uncertainty) of a neural classifier through the lens of low-resource languages. By training models on sub-sampled datasets in three different languages, we assess the quality of estimates from a wide array of approaches and their dependence on the amount of available data. We find that while approaches based on pre-trained models and ensembles achieve the best results overall, the quality of uncertainty estimates can surprisingly suffer with more data. We also perform a qualitative analysis of uncertainties on sequences, discovering that a model's total uncertainty seems to be influenced to a large degree by its data uncertainty, not model uncertainty. All model implementations are open-sourced in a software package.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "133864309",
                        "name": "Dennis Ulmer"
                    },
                    {
                        "authorId": "3010230",
                        "name": "J. Frellsen"
                    },
                    {
                        "authorId": "2579449",
                        "name": "Christian Hardmeier"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Therefore, careful tuning of the noise parameters is necessary to strike a balance between robustness and performance [18]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "834f895daedcb5f8f79749b862da1b81689483f0",
                "externalIds": {
                    "ArXiv": "2210.08415",
                    "DBLP": "journals/corr/abs-2210-08415",
                    "DOI": "10.48550/arXiv.2210.08415",
                    "CorpusId": 252917646
                },
                "corpusId": 252917646,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/834f895daedcb5f8f79749b862da1b81689483f0",
                "title": "Stability of Accuracy for the Training of DNNs Via the Uniform Doubling Condition",
                "abstract": "We study the stability of accuracy during the training of deep neural networks (DNNs). In this context, the training of a DNN is performed via the minimization of a cross-entropy loss function, and the performance metric is accuracy (the proportion of objects that are classified correctly). While training results in a decrease of loss, the accuracy does not necessarily increase during the process and may sometimes even decrease. The goal of achieving stability of accuracy is to ensure that if accuracy is high at some initial time, it remains high throughout training. A recent result by Berlyand, Jabin, and Safsten introduces a doubling condition on the training data, which ensures the stability of accuracy during training for DNNs using the absolute value activation function. For training data in $\\mathbb{R}^n$, this doubling condition is formulated using slabs in $\\mathbb{R}^n$ and depends on the choice of the slabs. The goal of this paper is twofold. First, to make the doubling condition uniform, that is, independent of the choice of slabs. This leads to sufficient conditions for stability in terms of training data only. In other words, for a training set $T$ that satisfies the uniform doubling condition, there exists a family of DNNs such that a DNN from this family with high accuracy on the training set at some training time $t_0$ will have high accuracy for all time $t>t_0$. Moreover, establishing uniformity is necessary for the numerical implementation of the doubling condition. The second goal is to extend the original stability results from the absolute value activation function to a broader class of piecewise linear activation functions with finitely many critical points, such as the popular Leaky ReLU.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "102394181",
                        "name": "Yitzchak Shmalo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similarly, other simple and effective methods to enhance model uncertainty estimation such as Ensemble [79] and Mixup [74] also demonstrate excellent performance.",
                "Representative methods include Mixup [74], CutMix [75], and PixMix [76]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4e9e30f4702f64af5aacbb5791172c5b37510dc3",
                "externalIds": {
                    "DBLP": "conf/nips/YangWZZDPWCLSDZ22",
                    "ArXiv": "2210.07242",
                    "DOI": "10.48550/arXiv.2210.07242",
                    "CorpusId": 252873458
                },
                "corpusId": 252873458,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4e9e30f4702f64af5aacbb5791172c5b37510dc3",
                "title": "OpenOOD: Benchmarking Generalized Out-of-Distribution Detection",
                "abstract": "Out-of-distribution (OOD) detection is vital to safety-critical machine learning applications and has thus been extensively studied, with a plethora of methods developed in the literature. However, the field currently lacks a unified, strictly formulated, and comprehensive benchmark, which often results in unfair comparisons and inconclusive results. From the problem setting perspective, OOD detection is closely related to neighboring fields including anomaly detection (AD), open set recognition (OSR), and model uncertainty, since methods developed for one domain are often applicable to each other. To help the community to improve the evaluation and advance, we build a unified, well-structured codebase called OpenOOD, which implements over 30 methods developed in relevant fields and provides a comprehensive benchmark under the recently proposed generalized OOD detection framework. With a comprehensive comparison of these methods, we are gratified that the field has progressed significantly over the past few years, where both preprocessing methods and the orthogonal post-hoc methods show strong potential.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2295601",
                        "name": "Jingkang Yang"
                    },
                    {
                        "authorId": "10073021",
                        "name": "Peng Wang"
                    },
                    {
                        "authorId": "9083667",
                        "name": "Dejian Zou"
                    },
                    {
                        "authorId": "2218011359",
                        "name": "Zitang Zhou"
                    },
                    {
                        "authorId": "2053138915",
                        "name": "Kun Ding"
                    },
                    {
                        "authorId": "1749122",
                        "name": "Wen-Hsiao Peng"
                    },
                    {
                        "authorId": "2109293355",
                        "name": "Haoqi Wang"
                    },
                    {
                        "authorId": "1745485",
                        "name": "Guangyao Chen"
                    },
                    {
                        "authorId": "143771569",
                        "name": "Bo Li"
                    },
                    {
                        "authorId": "31454397",
                        "name": "Yiyou Sun"
                    },
                    {
                        "authorId": "151480429",
                        "name": "Xuefeng Du"
                    },
                    {
                        "authorId": "9368124",
                        "name": "Kaiyang Zhou"
                    },
                    {
                        "authorId": "1726357",
                        "name": "Wayne Zhang"
                    },
                    {
                        "authorId": "3422872",
                        "name": "Dan Hendrycks"
                    },
                    {
                        "authorId": "1527103472",
                        "name": "Yixuan Li"
                    },
                    {
                        "authorId": "2145254462",
                        "name": "Ziwei Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Fortunately, training good classifiers is easy with modern AutoML (Erickson et al., 2020) and techniques for calibration, data augmentation, and transfer learning (Thulasidasan et al., 2019).",
                ", 2020) and techniques for calibration, data augmentation, and transfer learning (Thulasidasan et al., 2019)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b87f32e6513b24fe601752c95564c2c3aa59392f",
                "externalIds": {
                    "ArXiv": "2210.06812",
                    "CorpusId": 256358902
                },
                "corpusId": 256358902,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b87f32e6513b24fe601752c95564c2c3aa59392f",
                "title": "CROWDLAB: Supervised learning to infer consensus labels and quality scores for data with multiple annotators",
                "abstract": "Real-world data for classification is often labeled by multiple annotators. For analyzing such data, we introduce CROWDLAB, a straightforward approach to utilize any trained classifier to estimate: (1) A consensus label for each example that aggregates the available annotations; (2) A confidence score for how likely each consensus label is correct; (3) A rating for each annotator quantifying the overall correctness of their labels. Existing algorithms to estimate related quantities in crowdsourcing often rely on sophisticated generative models with iterative inference. CROWDLAB instead uses a straightforward weighted ensemble. Existing algorithms often rely solely on annotator statistics, ignoring the features of the examples from which the annotations derive. CROWDLAB utilizes any classifier model trained on these features, and can thus better generalize between examples with similar features. On real-world multi-annotator image data, our proposed method provides superior estimates for (1)-(3) than existing algorithms like Dawid-Skene/GLAD.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2174448740",
                        "name": "Hui Wen Goh"
                    },
                    {
                        "authorId": "2099410561",
                        "name": "Ulyana Tkachenko"
                    },
                    {
                        "authorId": "153430733",
                        "name": "Jonas W. Mueller"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Hence, confidence calibration is a vivid field of research and proposed methods are based on additional loss functions [32, 35, 45, 48, 52], on adaptions of the training input by label smoothing [54, 60, 63, 75] or on data augmentation [20, 45, 76, 88]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "061263925e2ad7504fd4b57f1c12cd5e7fcae582",
                "externalIds": {
                    "ArXiv": "2210.05938",
                    "DBLP": "journals/corr/abs-2210-05938",
                    "DOI": "10.48550/arXiv.2210.05938",
                    "CorpusId": 252846375
                },
                "corpusId": 252846375,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/061263925e2ad7504fd4b57f1c12cd5e7fcae582",
                "title": "Robust Models are less Over-Confident",
                "abstract": "Despite the success of convolutional neural networks (CNNs) in many academic benchmarks for computer vision tasks, their application in the real-world is still facing fundamental challenges. One of these open problems is the inherent lack of robustness, unveiled by the striking effectiveness of adversarial attacks. Current attack methods are able to manipulate the network's prediction by adding specific but small amounts of noise to the input. In turn, adversarial training (AT) aims to achieve robustness against such attacks and ideally a better model generalization ability by including adversarial samples in the trainingset. However, an in-depth analysis of the resulting robust models beyond adversarial robustness is still pending. In this paper, we empirically analyze a variety of adversarially trained models that achieve high robust accuracies when facing state-of-the-art attacks and we show that AT has an interesting side-effect: it leads to models that are significantly less overconfident with their decisions, even on clean data than non-robust models. Further, our analysis of robust models shows that not only AT but also the model's building blocks (like activation functions and pooling) have a strong influence on the models' prediction confidences. Data&Project website: https://github.com/GeJulia/robustness_confidences_evaluation",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2161241926",
                        "name": "Julia Grabinski"
                    },
                    {
                        "authorId": "65947743",
                        "name": "Paul Gavrikov"
                    },
                    {
                        "authorId": "3299100",
                        "name": "J. Keuper"
                    },
                    {
                        "authorId": "3316866",
                        "name": "M. Keuper"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mixup is a data augmentation method [14] which is shown to output well-calibrated predictive scores [13], and is again performed during training.",
                "In addition, Mixup and Focal Loss are known to outperform Label Smoothing as shown in [13, 8]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c65a118527965ec4e1614220e7840e96147c6162",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-06592",
                    "ArXiv": "2210.06592",
                    "DOI": "10.48550/arXiv.2210.06592",
                    "CorpusId": 252873008
                },
                "corpusId": 252873008,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c65a118527965ec4e1614220e7840e96147c6162",
                "title": "Can Calibration Improve Sample Prioritization?",
                "abstract": "Calibration can reduce overconfident predictions of deep neural networks, but can calibration also accelerate training? In this paper, we show that it can when used to prioritize some examples for performing subset selection. We study the effect of popular calibration techniques in selecting better subsets of samples during training (also called sample prioritization) and observe that calibration can improve the quality of subsets, reduce the number of examples per epoch (by at least 70%), and can thereby speed up the overall training process. We further study the effect of using calibrated pre-trained models coupled with calibration during training to guide sample prioritization, which again seems to improve the quality of samples selected.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151199484",
                        "name": "Ganesh Tata"
                    },
                    {
                        "authorId": "51914845",
                        "name": "Gautham Krishna Gudur"
                    },
                    {
                        "authorId": "2530185",
                        "name": "Gopinath Chennupati"
                    },
                    {
                        "authorId": "145901278",
                        "name": "M. E. Khan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[6] conclude that Transformers are calibrated better than CNNs yielding overconfident predictions [21, 22, 23]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dc6a210c59befd2ed79298cc58552d20fc03057b",
                "externalIds": {
                    "ArXiv": "2210.05742",
                    "DBLP": "journals/corr/abs-2210-05742",
                    "DOI": "10.48550/arXiv.2210.05742",
                    "CorpusId": 252846759
                },
                "corpusId": 252846759,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dc6a210c59befd2ed79298cc58552d20fc03057b",
                "title": "Curved Representation Space of Vision Transformers",
                "abstract": "Neural networks with self-attention (a.k.a. Transformers) like ViT and Swin have emerged as a better alternative to traditional convolutional neural networks (CNNs) for computer vision tasks. However, our understanding of how the new architecture works is still limited. In this paper, we focus on the phenomenon that Transformers show higher robustness against corruptions than CNNs, while not being overcon\ufb01dent (in fact, we \ufb01nd Transformers are actually undercon\ufb01dent). This is contrary to the intuition that robustness increases with con\ufb01dence. We resolve this contradiction by investigating how the output of the penultimate layer moves in the representation space as the input data moves within a small area. In particular, we show the following. (1) While CNNs exhibit fairly linear relationship between the input and output movements, Transformers show nonlinear relationship for some data. For those data, the output of Transformers moves in a curved trajectory as the input moves linearly. (2) When a data is located in a curved region, it is hard to move it out of the decision region since the output moves along a curved trajectory instead of a straight line to the decision boundary, resulting in high robustness of Transformers. (3) If a data is slightly modi\ufb01ed to jump out of the curved region, the movements afterwards become linear and the output goes to the decision boundary directly. Thus, Transformers can be attacked easily after a small random jump and the perturbation in the \ufb01nal attacked data remains imperceptible. In other words, there does exist a decision boundary near the data, which is hard to \ufb01nd only because of the curved representation space. This also explains the undercon\ufb01dent prediction of Transformers. (4) The curved regions in the representation space start to form at an early training stage and grow throughout the training course. Some data are trapped in the regions, obstructing Transformers from reducing the training loss.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "102361085",
                        "name": "Juyeop Kim"
                    },
                    {
                        "authorId": "2118871785",
                        "name": "Junha Park"
                    },
                    {
                        "authorId": "2917665",
                        "name": "Songkuk Kim"
                    },
                    {
                        "authorId": "2144729280",
                        "name": "Jongseok Lee"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ac7225c5d5aa71d686cd28d84983fa6bb9896d02",
                "externalIds": {
                    "ArXiv": "2210.04783",
                    "DBLP": "journals/corr/abs-2210-04783",
                    "DOI": "10.48550/arXiv.2210.04783",
                    "CorpusId": 252780568
                },
                "corpusId": 252780568,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ac7225c5d5aa71d686cd28d84983fa6bb9896d02",
                "title": "On the Importance of Calibration in Semi-supervised Learning",
                "abstract": "State-of-the-art (SOTA) semi-supervised learning (SSL) methods have been highly successful in leveraging a mix of labeled and unlabeled data by combining techniques of consistency regularization and pseudo-labeling. During pseudo-labeling, the model\u2019s predictions on unlabeled data are used for training and thus, model calibration is important in mitigating con\ufb01rmation bias. Yet, many SOTA methods are optimized for model performance, with little focus directed to improve model calibration. In this work, we empirically demonstrate that model calibration is strongly correlated with model performance and propose to improve calibration via approximate Bayesian techniques. We introduce a family of new SSL models that optimizes for calibration and demonstrate their effectiveness across standard vision benchmarks of CIFAR-10, CIFAR-100 and ImageNet, giving up to 15.9% improvement in test accuracy. Furthermore, we also demonstrate their effectiveness in additional realistic and challenging problems, such as class-imbalanced datasets and in photonics science.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40917646",
                        "name": "Charlotte Loh"
                    },
                    {
                        "authorId": "26916003",
                        "name": "R. Dangovski"
                    },
                    {
                        "authorId": "2114819100",
                        "name": "Shivchander Sudalairaj"
                    },
                    {
                        "authorId": "2197109",
                        "name": "Seung-Jun Han"
                    },
                    {
                        "authorId": "3471102",
                        "name": "Ligong Han"
                    },
                    {
                        "authorId": "2142741421",
                        "name": "Leonid Karlinsky"
                    },
                    {
                        "authorId": "1973666",
                        "name": "M. Solja\u010di\u0107"
                    },
                    {
                        "authorId": "2018087",
                        "name": "Akash Srivastava"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[9] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.",
                "Proper data augmentation also contributes to OOD uncertainty estimation [16, 17, 9]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "df8176027e3b9857e6bc6f45b3fc183351571fbd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-04200",
                    "ArXiv": "2210.04200",
                    "DOI": "10.48550/arXiv.2210.04200",
                    "CorpusId": 252780290
                },
                "corpusId": 252780290,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/df8176027e3b9857e6bc6f45b3fc183351571fbd",
                "title": "Boosting Out-of-distribution Detection with Typical Features",
                "abstract": "Out-of-distribution (OOD) detection is a critical task for ensuring the reliability and safety of deep neural networks in real-world scenarios. Different from most previous OOD detection methods that focus on designing OOD scores or introducing diverse outlier examples to retrain the model, we delve into the obstacle factors in OOD detection from the perspective of typicality and regard the feature's high-probability region of the deep model as the feature's typical set. We propose to rectify the feature into its typical set and calculate the OOD score with the typical features to achieve reliable uncertainty estimation. The feature rectification can be conducted as a {plug-and-play} module with various OOD scores. We evaluate the superiority of our method on both the commonly used benchmark (CIFAR) and the more challenging high-resolution benchmark with large label space (ImageNet). Notably, our approach outperforms state-of-the-art methods by up to 5.11$\\%$ in the average FPR95 on the ImageNet benchmark.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2153095684",
                        "name": "Yao Zhu"
                    },
                    {
                        "authorId": "47557806",
                        "name": "YueFeng Chen"
                    },
                    {
                        "authorId": "92617269",
                        "name": "Chuanlong Xie"
                    },
                    {
                        "authorId": "2118899321",
                        "name": "Xiaodan Li"
                    },
                    {
                        "authorId": "2119062118",
                        "name": "Rong Zhang"
                    },
                    {
                        "authorId": "1645209767",
                        "name": "Hui Xue"
                    },
                    {
                        "authorId": "2512006",
                        "name": "Xiang Tian"
                    },
                    {
                        "authorId": "152869379",
                        "name": "Bolun Zheng"
                    },
                    {
                        "authorId": "2573672",
                        "name": "Yao-wu Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Additionally, it has been noted that calibration behaves di erently in deep ensembles [Lakshminarayanan et al., 2017, Wen et al., 2021], and when using data augmentations [Thulasidasan et al., 2019, Wen et al., 2021], or label smoothing [Szegedy et al., 2016], for example."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0aa2660c9d18add4d345961840b03dece5f21409",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01964",
                    "ArXiv": "2210.01964",
                    "DOI": "10.48550/arXiv.2210.01964",
                    "CorpusId": 252715522
                },
                "corpusId": 252715522,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0aa2660c9d18add4d345961840b03dece5f21409",
                "title": "The Calibration Generalization Gap",
                "abstract": "Calibration is a fundamental property of a good predictive model: it requires that the model predicts correctly in proportion to its confidence. Modern neural networks, however, provide no strong guarantees on their calibration -- and can be either poorly calibrated or well-calibrated depending on the setting. It is currently unclear which factors contribute to good calibration (architecture, data augmentation, overparameterization, etc), though various claims exist in the literature. We propose a systematic way to study the calibration error: by decomposing it into (1) calibration error on the train set, and (2) the calibration generalization gap. This mirrors the fundamental decomposition of generalization. We then investigate each of these terms, and give empirical evidence that (1) DNNs are typically always calibrated on their train set, and (2) the calibration generalization gap is upper-bounded by the standard generalization gap. Taken together, this implies that models with small generalization gap (|Test Error - Train Error|) are well-calibrated. This perspective unifies many results in the literature, and suggests that interventions which reduce the generalization gap (such as adding data, using heavy augmentation, or smaller model size) also improve calibration. We thus hope our initial study lays the groundwork for a more systematic and comprehensive understanding of the relation between calibration, generalization, and optimization.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32103697",
                        "name": "Annabelle Carrell"
                    },
                    {
                        "authorId": "35416244",
                        "name": "Neil Rohit Mallinar"
                    },
                    {
                        "authorId": "145202377",
                        "name": "James Lucas"
                    },
                    {
                        "authorId": "2181918",
                        "name": "Preetum Nakkiran"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It thus reduces the effect of outliers and, consequently, lessens the likelihood of overfitting (Thulasidasan et al., 2019; Zhang et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e22a53c39ab034e80a85e248386c4e040da71055",
                "externalIds": {
                    "PubMedCentral": "9668611",
                    "DOI": "10.1016/j.nicl.2022.103214",
                    "CorpusId": 252587774,
                    "PubMed": "36183611"
                },
                "corpusId": 252587774,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e22a53c39ab034e80a85e248386c4e040da71055",
                "title": "Data augmentation with Mixup: Enhancing performance of a functional neuroimaging-based prognostic deep learning classifier in recent onset psychosis",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2663442",
                        "name": "J. Smucny"
                    },
                    {
                        "authorId": "2167331582",
                        "name": "Ge Shi"
                    },
                    {
                        "authorId": "2409894",
                        "name": "T. Lesh"
                    },
                    {
                        "authorId": "144836484",
                        "name": "C. Carter"
                    },
                    {
                        "authorId": "143763341",
                        "name": "I. Davidson"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "db3026c40231ede91a904baabfd8061d05fb949d",
                "externalIds": {
                    "DOI": "10.3390/app12189007",
                    "CorpusId": 252170336
                },
                "corpusId": 252170336,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/db3026c40231ede91a904baabfd8061d05fb949d",
                "title": "A Study on the Calibrated Confidence of Text Classification Using a Variational Bayes",
                "abstract": "Recently, predictions based on big data have become more successful. In fact, research using images or text can make a long-imagined future come true. However, the data often contain a lot of noise, or the model does not account for the data, which increases uncertainty. Moreover, the gap between accuracy and likelihood is widening in modern predictive models. This gap may increase the uncertainty of predictions. In particular, applications such as self-driving cars and healthcare have problems that can be directly threatened by these uncertainties. Previous studies have proposed methods for reducing uncertainty in applications using images or signals. However, although studies that use natural language processing are being actively conducted, there remains insufficient discussion about uncertainty in text classification. Therefore, we propose a method that uses Variational Bayes to reduce the difference between accuracy and likelihood in text classification. This paper conducts an experiment using patent data in the field of technology management to confirm the proposed method\u2019s practical applicability. As a result of the experiment, the calibrated confidence in the model was very small, from a minimum of 0.02 to a maximum of 0.04. Furthermore, through statistical tests, we proved that the proposed method within the significance level of 0.05 was more effective at calibrating the confidence than before.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144703237",
                        "name": "Juhyun Lee"
                    },
                    {
                        "authorId": "1730690",
                        "name": "Sangsung Park"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ccd2a1161675740a13e24cc286134bd0cf84d63b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-05379",
                    "ArXiv": "2208.05379",
                    "DOI": "10.1162/tacl_a_00515",
                    "CorpusId": 251468137
                },
                "corpusId": 251468137,
                "publicationVenue": {
                    "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
                    "name": "Transactions of the Association for Computational Linguistics",
                    "type": "journal",
                    "alternate_names": [
                        "Trans Assoc Comput Linguistics",
                        "TACL"
                    ],
                    "issn": "2307-387X",
                    "url": "https://www.mitpressjournals.org/loi/tacl",
                    "alternate_urls": [
                        "http://www.transacl.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ccd2a1161675740a13e24cc286134bd0cf84d63b",
                "title": "Multi-task Active Learning for Pre-trained Transformer-based Models",
                "abstract": "Abstract Multi-task learning, in which several tasks are jointly learned by a single model, allows NLP models to share information from multiple annotations and may facilitate better predictions when the tasks are inter-related. This technique, however, requires annotating the same text with multiple annotation schemes, which may be costly and laborious. Active learning (AL) has been demonstrated to optimize annotation processes by iteratively selecting unlabeled examples whose annotation is most valuable for the NLP model. Yet, multi-task active learning (MT-AL) has not been applied to state-of-the-art pre-trained Transformer-based NLP models. This paper aims to close this gap. We explore various multi-task selection criteria in three realistic multi-task scenarios, reflecting different relations between the participating tasks, and demonstrate the effectiveness of multi-task compared to single-task selection. Our results suggest that MT-AL can be effectively used in order to minimize annotation efforts for multi-task NLP models.1",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "117095364",
                        "name": "Guy Rotman"
                    },
                    {
                        "authorId": "1762757",
                        "name": "Roi Reichart"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0b505dac462d6233dacbcc1eda5e873c8c06f2af",
                "externalIds": {
                    "PubMedCentral": "9359318",
                    "DOI": "10.3389/fmedt.2022.919046",
                    "CorpusId": 250959353,
                    "PubMed": "35958121"
                },
                "corpusId": 250959353,
                "publicationVenue": {
                    "id": "bc962af1-0f08-4924-8ef7-6fd883d48fdd",
                    "name": "Frontiers in Medical Technology",
                    "alternate_names": [
                        "Front Med Technol"
                    ],
                    "issn": "2673-3129",
                    "url": "https://www.frontiersin.org/journals/medical-technology/sections/medtech-data-analytics#articles"
                },
                "url": "https://www.semanticscholar.org/paper/0b505dac462d6233dacbcc1eda5e873c8c06f2af",
                "title": "Failure Detection in Deep Neural Networks for Medical Imaging",
                "abstract": "Deep neural networks (DNNs) have started to find their role in the modern healthcare system. DNNs are being developed for diagnosis, prognosis, treatment planning, and outcome prediction for various diseases. With the increasing number of applications of DNNs in modern healthcare, their trustworthiness and reliability are becoming increasingly important. An essential aspect of trustworthiness is detecting the performance degradation and failure of deployed DNNs in medical settings. The softmax output values produced by DNNs are not a calibrated measure of model confidence. Softmax probability numbers are generally higher than the actual model confidence. The model confidence-accuracy gap further increases for wrong predictions and noisy inputs. We employ recently proposed Bayesian deep neural networks (BDNNs) to learn uncertainty in the model parameters. These models simultaneously output the predictions and a measure of confidence in the predictions. By testing these models under various noisy conditions, we show that the (learned) predictive confidence is well calibrated. We use these reliable confidence values for monitoring performance degradation and failure detection in DNNs. We propose two different failure detection methods. In the first method, we define a fixed threshold value based on the behavior of the predictive confidence with changing signal-to-noise ratio (SNR) of the test dataset. The second method learns the threshold value with a neural network. The proposed failure detection mechanisms seamlessly abstain from making decisions when the confidence of the BDNN is below the defined threshold and hold the decision for manual review. Resultantly, the accuracy of the models improves on the unseen test samples. We tested our proposed approach on three medical imaging datasets: PathMNIST, DermaMNIST, and OrganAMNIST, under different levels and types of noise. An increase in the noise of the test images increases the number of abstained samples. BDNNs are inherently robust and show more than 10% accuracy improvement with the proposed failure detection methods. The increased number of abstained samples or an abrupt increase in the predictive variance indicates model performance degradation or possible failure. Our work has the potential to improve the trustworthiness of DNNs and enhance user confidence in the model predictions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3698852",
                        "name": "Sabeen Ahmed"
                    },
                    {
                        "authorId": "2867021",
                        "name": "Dimah Dera"
                    },
                    {
                        "authorId": "2178684332",
                        "name": "Saud Ul Hassan"
                    },
                    {
                        "authorId": "1724294",
                        "name": "N. Bouaynaya"
                    },
                    {
                        "authorId": "145129130",
                        "name": "G. Rasool"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Data augmentation strategies based on mixing [24,25] have been shown to improve calibration [26,27] along with crafted loss functions [28]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "128551f4ebb146791eb47045bf691a966bc14408",
                "externalIds": {
                    "ArXiv": "2207.11211",
                    "DBLP": "journals/corr/abs-2207-11211",
                    "DOI": "10.48550/arXiv.2207.11211",
                    "CorpusId": 251018194
                },
                "corpusId": 251018194,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/128551f4ebb146791eb47045bf691a966bc14408",
                "title": "Improving Predictive Performance and Calibration by Weight Fusion in Semantic Segmentation",
                "abstract": "Averaging predictions of a deep ensemble of networks is apopular and effective method to improve predictive performance andcalibration in various benchmarks and Kaggle competitions. However, theruntime and training cost of deep ensembles grow linearly with the size ofthe ensemble, making them unsuitable for many applications. Averagingensemble weights instead of predictions circumvents this disadvantageduring inference and is typically applied to intermediate checkpoints ofa model to reduce training cost. Albeit effective, only few works haveimproved the understanding and the performance of weight averaging.Here, we revisit this approach and show that a simple weight fusion (WF)strategy can lead to a significantly improved predictive performance andcalibration. We describe what prerequisites the weights must meet interms of weight space, functional space and loss. Furthermore, we presenta new test method (called oracle test) to measure the functional spacebetween weights. We demonstrate the versatility of our WF strategy acrossstate of the art segmentation CNNs and Transformers as well as real worlddatasets such as BDD100K and Cityscapes. We compare WF with similarapproaches and show our superiority for in- and out-of-distribution datain terms of predictive performance and calibration.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "52090133",
                        "name": "Timo S\u00e4mann"
                    },
                    {
                        "authorId": "48213047",
                        "name": "A. Hammam"
                    },
                    {
                        "authorId": "3056236",
                        "name": "Andrei Bursuc"
                    },
                    {
                        "authorId": "1760556",
                        "name": "C. Stiller"
                    },
                    {
                        "authorId": "145695951",
                        "name": "H. Gro\u00df"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "346e3d1871d17ce2b64a8f691e93c7792b96974c",
                "externalIds": {
                    "ArXiv": "2207.08210",
                    "DBLP": "journals/corr/abs-2207-08210",
                    "DOI": "10.48550/arXiv.2207.08210",
                    "CorpusId": 250627292
                },
                "corpusId": 250627292,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/346e3d1871d17ce2b64a8f691e93c7792b96974c",
                "title": "A Simple Test-Time Method for Out-of-Distribution Detection",
                "abstract": "Neural networks are known to produce over-con\ufb01dent predictions on input images, even when these images are out-of-distribution (OOD) samples. This limits the applications of neural network models in real-world scenarios, where OOD samples exist. Many existing approaches identify the OOD instances via exploiting various cues, such as \ufb01nding irregular patterns in the feature space, logits space, gradient space or the raw space of images. In contrast, this paper proposes a simple Test-time Linear Training (ETLT) method for OOD detection. Empirically, we \ufb01nd that the probabilities of input images being out-of-distribution are surprisingly linearly correlated to the features extracted by neural networks. To be speci\ufb01c, many state-of-the-art OOD algorithms, although designed to measure reliability in different ways, actually lead to OOD scores mostly linearly related to their image features. Thus, by simply learning a linear regression model trained from the paired image features and inferred OOD scores at test-time, we can make a more precise OOD prediction for the test instances. We further propose an online variant of the proposed method, which achieves promising performance and is more practical in real-world applications. Remarkably, we improve FPR95 from 51 . 37% to 12 . 30% on CIFAR-10 datasets with maximum softmax probability as the base OOD detector. Extensive experiments on several benchmark datasets show the ef\ufb01cacy of ETLT for OOD detection task.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2114992204",
                        "name": "Ke Fan"
                    },
                    {
                        "authorId": "2108853258",
                        "name": "Yikai Wang"
                    },
                    {
                        "authorId": "2087411164",
                        "name": "Qian Yu"
                    },
                    {
                        "authorId": "2108339378",
                        "name": "Da Li"
                    },
                    {
                        "authorId": "35782003",
                        "name": "Yanwei Fu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4312d1d5a5ac0c97c8b9681711a430b5c3de31b3",
                "externalIds": {
                    "PubMedCentral": "9469924",
                    "DOI": "10.1093/jamiaopen/ooac075",
                    "CorpusId": 252229682,
                    "PubMed": "36110150"
                },
                "corpusId": 252229682,
                "publicationVenue": {
                    "id": "811a7a2a-70ce-4bde-9f59-dd8bb126e30f",
                    "name": "JAMIA Open",
                    "issn": "2574-2531"
                },
                "url": "https://www.semanticscholar.org/paper/4312d1d5a5ac0c97c8b9681711a430b5c3de31b3",
                "title": "Using ensembles and distillation to optimize the deployment of deep learning models for the classification of electronic cancer pathology reports",
                "abstract": "Abstract Objective We aim to reduce overfitting and model overconfidence by distilling the knowledge of an ensemble of deep learning models into a single model for the classification of cancer pathology reports. Materials and Methods We consider the text classification problem that involves 5 individual tasks. The baseline model consists of a multitask convolutional neural network (MtCNN), and the implemented ensemble (teacher) consists of 1000 MtCNNs. We performed knowledge transfer by training a single model (student) with soft labels derived through the aggregation of ensemble predictions. We evaluate performance based on accuracy and abstention rates by using softmax thresholding. Results The student model outperforms the baseline MtCNN in terms of abstention rates and accuracy, thereby allowing the model to be used with a larger volume of documents when deployed. The highest boost was observed for subsite and histology, for which the student model classified an additional 1.81% reports for subsite and 3.33% reports for histology. Discussion Ensemble predictions provide a useful strategy for quantifying the uncertainty inherent in labeled data and thereby enable the construction of soft labels with estimated probabilities for multiple classes for a given document. Training models with the derived soft labels reduce model confidence in difficult-to-classify documents, thereby leading to a reduction in the number of highly confident wrong predictions. Conclusions Ensemble model distillation is a simple tool to reduce model overconfidence in problems with extreme class imbalance and noisy datasets. These methods can facilitate the deployment of deep learning models in high-risk domains with low computational resources where minimizing inference time is required.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2124861538",
                        "name": "Kevin De Angeli"
                    },
                    {
                        "authorId": "1406062858",
                        "name": "Shang Gao"
                    },
                    {
                        "authorId": "2151429233",
                        "name": "Andrew E. Blanchard"
                    },
                    {
                        "authorId": "3742715",
                        "name": "E. Durbin"
                    },
                    {
                        "authorId": "2154602619",
                        "name": "Xiao-Cheng Wu"
                    },
                    {
                        "authorId": "4886706",
                        "name": "A. Stroup"
                    },
                    {
                        "authorId": "2151383170",
                        "name": "J. Doherty"
                    },
                    {
                        "authorId": "2142503769",
                        "name": "Stephen M. Schwartz"
                    },
                    {
                        "authorId": "2141749693",
                        "name": "Charles Wiggins"
                    },
                    {
                        "authorId": "9587918",
                        "name": "Linda Coyle"
                    },
                    {
                        "authorId": "144203902",
                        "name": "Lynne Penberthy"
                    },
                    {
                        "authorId": "1783513",
                        "name": "G. Tourassi"
                    },
                    {
                        "authorId": "97960483",
                        "name": "Hong-Jun Yoon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Among these we can mention attempts to link: robustness and calibration [34,32,43] showing that models which are robust to adversarial attacks are more interpretable; data augmentation, calibration and interpretation showing that MixUp data augmentation greatly impacts the calibration of learnt models [42] and existing saliency methods being used to improve the MixUp procedure itself [15]; or calibration and fairness [29], showing the incompatibility between most of fairness definitions and calibration."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0a91968e33af59f679533366a69c08d73fac7e85",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-03324",
                    "ArXiv": "2207.03324",
                    "DOI": "10.48550/arXiv.2207.03324",
                    "CorpusId": 250334352
                },
                "corpusId": 250334352,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0a91968e33af59f679533366a69c08d73fac7e85",
                "title": "Calibrate to Interpret",
                "abstract": "Trustworthy machine learning is driving a large number of ML community works in order to improve ML acceptance and adoption. The main aspect of trustworthy machine learning are the followings: fairness, uncertainty, robustness, explainability and formal guaranties. Each of these individual domains gains the ML community interest, visible by the number of related publications. However few works tackle the interconnection between these fields. In this paper we show a first link between uncertainty and explainability, by studying the relation between calibration and interpretation. As the calibration of a given model changes the way it scores samples, and interpretation approaches often rely on these scores, it seems safe to assume that the confidence-calibration of a model interacts with our ability to interpret such model. In this paper, we show, in the context of networks trained on image classification tasks, to what extent interpretations are sensitive to confidence-calibration. It leads us to suggest a simple practice to improve the interpretation outcomes: Calibrate to Interpret.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2175251459",
                        "name": "Gregory Scafarto"
                    },
                    {
                        "authorId": "2117000633",
                        "name": "N. Posocco"
                    },
                    {
                        "authorId": "35662436",
                        "name": "Antoine Bonnefoy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[Thulasidasan et al., 2019] have empirically shown that the network trained with Mixup provides better-calibrated results.",
                "For the quantitative analysis of the confidence calibration, we used two popular metrics, the expected calibration error (ECE) [Naeini et al., 2015] and the overconfidence error (OE) [Thulasidasan et al., 2019].",
                "De-\nspite its simplicity, Mixup training achieves robustness toward adversarial examples [Zhang et al., 2018] and improves calibration [Thulasidasan et al., 2019].",
                "One possible reason is under-fitting since its complicated training prevents sufficient convergence for the conventional learning procedure [Thulasidasan et al., 2019].",
                "In particular, using a large \u03b1 value degrades the accuracy largely in Mixup [Thulasidasan et al., 2019; Zhang et al., 2018].",
                "Furthermore, recent work [Thulasidasan et al., 2019] reports that Mixup training encourages that the output of DNNs, i.e., the estimated label distribution, can serve as a better indicator of the actual likelihood of a correct prediction.",
                "Furthermore, recent work [Thulasidasan et al., 2019] reports that Mixup training encourages that the output of DNNs, i.",
                "Lately, [Thulasidasan et al., 2019] show that the label smoothing effect is a crucial factor for achieving accurate predictive uncertainty.",
                ", 2015] and the overconfidence error (OE) [Thulasidasan et al., 2019].",
                "[Thulasidasan et al., 2019] also show that data augmentation alone without mixed labels can improve the prediction accuracy."
            ],
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "59f77f925f58f1d03a242fd23cbad4a021d6267d",
                "externalIds": {
                    "DBLP": "conf/ijcai/BangBKJKKLS22",
                    "DOI": "10.24963/ijcai.2022/390",
                    "CorpusId": 250637916
                },
                "corpusId": 250637916,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/59f77f925f58f1d03a242fd23cbad4a021d6267d",
                "title": "Logit Mixing Training for More Reliable and Accurate Prediction",
                "abstract": "When a person solves the multi-choice problem, she considers not only what is the answer but also what is not the answer. Knowing what choice is not the answer and utilizing the relationships between choices, she can improve the prediction accuracy. Inspired by this human reasoning process, we propose a new training strategy to fully utilize inter-class relationships, namely LogitMix. Our strategy is combined with recent data augmentation techniques, e.g., Mixup, Manifold Mixup, CutMix, and PuzzleMix. Then, we suggest using a mixed logit, i.e., a mixture of two logits, as an auxiliary training objective. Since the logit can preserve both positive and negative inter-class relationships, it can impose a network to learn the probability of wrong answers correctly. Our extensive experimental results on the image- and language-based tasks demonstrate that LogitMix achieves state-of-the-art performance among recent data augmentation techniques regarding calibration error and prediction accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3421575",
                        "name": "Duhyeon Bang"
                    },
                    {
                        "authorId": "51122294",
                        "name": "Kyungjune Baek"
                    },
                    {
                        "authorId": "2109183932",
                        "name": "Jiwoo Kim"
                    },
                    {
                        "authorId": "49571622",
                        "name": "Yunho Jeon"
                    },
                    {
                        "authorId": "2116515957",
                        "name": "Jinhong Kim"
                    },
                    {
                        "authorId": "3968500",
                        "name": "Jiwon Kim"
                    },
                    {
                        "authorId": "1865093",
                        "name": "Jongwuk Lee"
                    },
                    {
                        "authorId": "2081318",
                        "name": "Hyunjung Shim"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "20b1afe12366ace3b6b5a53d4e7c203d4cf2688c",
                "externalIds": {
                    "DBLP": "journals/npjdm/BiffiSDHSAABCDF22",
                    "PubMedCentral": "9247164",
                    "DOI": "10.1038/s41746-022-00633-6",
                    "CorpusId": 250120011,
                    "PubMed": "35773468"
                },
                "corpusId": 250120011,
                "publicationVenue": {
                    "id": "ef485645-f75f-4344-8b9d-3c260e69503b",
                    "name": "npj Digital Medicine",
                    "alternate_names": [
                        "npj Digit Med"
                    ],
                    "issn": "2398-6352",
                    "url": "http://www.nature.com/npjdigitalmed/"
                },
                "url": "https://www.semanticscholar.org/paper/20b1afe12366ace3b6b5a53d4e7c203d4cf2688c",
                "title": "A novel AI device for real-time optical characterization of colorectal polyps",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "30646129",
                        "name": "C. Biffi"
                    },
                    {
                        "authorId": "1797385",
                        "name": "Pietro Salvagnini"
                    },
                    {
                        "authorId": "11568968",
                        "name": "Nhan Ngo Dinh"
                    },
                    {
                        "authorId": "2098316464",
                        "name": "C. Hassan"
                    },
                    {
                        "authorId": "2112284078",
                        "name": "Prateek Sharma"
                    },
                    {
                        "authorId": null,
                        "name": "Giulio Halim Sebastian Sabela M\u00e1rio Agn\u00e8s Gl\u00f2ria Fern\u00e1nde Antonelli Awadie Bernhofer Carballal Dinis-Ribeiro"
                    },
                    {
                        "authorId": "115447643",
                        "name": "G. Antonelli"
                    },
                    {
                        "authorId": "12596814",
                        "name": "H. Awadie"
                    },
                    {
                        "authorId": "2144037166",
                        "name": "Sebastian Bernhofer"
                    },
                    {
                        "authorId": "39552876",
                        "name": "S. Carballal"
                    },
                    {
                        "authorId": "1388888559",
                        "name": "M. Dinis-Ribeiro"
                    },
                    {
                        "authorId": "1423017848",
                        "name": "A. Fern\u00e1ndez-Clotet"
                    },
                    {
                        "authorId": "15642741",
                        "name": "G. F. Esparrach"
                    },
                    {
                        "authorId": "5225530",
                        "name": "I. Gralnek"
                    },
                    {
                        "authorId": "2174072122",
                        "name": "Yuta Higasa"
                    },
                    {
                        "authorId": "2068689817",
                        "name": "Takuro Hirabayashi"
                    },
                    {
                        "authorId": "2174075433",
                        "name": "Tatsuki Hirai"
                    },
                    {
                        "authorId": "5110132",
                        "name": "M. Iwatate"
                    },
                    {
                        "authorId": "2057328130",
                        "name": "Miki Kawano"
                    },
                    {
                        "authorId": "2174075697",
                        "name": "Markus Mader"
                    },
                    {
                        "authorId": "4957173",
                        "name": "A. Maieron"
                    },
                    {
                        "authorId": "2174074964",
                        "name": "Sebastian Mattes"
                    },
                    {
                        "authorId": "2174075751",
                        "name": "Tastuya Nakai"
                    },
                    {
                        "authorId": "9873133",
                        "name": "I. Ord\u00e1s"
                    },
                    {
                        "authorId": "1581834887",
                        "name": "R. Ortig\u00e3o"
                    },
                    {
                        "authorId": "2174076054",
                        "name": "Oswaldo Ortiz Z\u00fa\u00f1iga"
                    },
                    {
                        "authorId": "3711851",
                        "name": "M. Pellis\u00e9"
                    },
                    {
                        "authorId": "145803904",
                        "name": "C. Pinto"
                    },
                    {
                        "authorId": "48981780",
                        "name": "F. Riedl"
                    },
                    {
                        "authorId": "1585199302",
                        "name": "Ariadna S\u00e1nchez"
                    },
                    {
                        "authorId": "47746166",
                        "name": "E. Steiner"
                    },
                    {
                        "authorId": "2112766605",
                        "name": "Yukari Tanaka"
                    },
                    {
                        "authorId": "2169094581",
                        "name": "Andrea Cherubini"
                    }
                ]
            }
        },
        {
            "contexts": [
                "has been shown to increase robustness towards adversarial samples, a better estimate of uncertainty [27], and better generalization of the trained model [33]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3ccd600bab618a919c0e6e69d1724c9afd453a5a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-15186",
                    "ArXiv": "2206.15186",
                    "DOI": "10.48550/arXiv.2206.15186",
                    "CorpusId": 250144312
                },
                "corpusId": 250144312,
                "publicationVenue": {
                    "id": "61a709e3-2060-423c-8de5-ffd3885aa31c",
                    "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                    "type": "conference",
                    "alternate_names": [
                        "Medical Image Computing and Computer-Assisted Intervention",
                        "MICCAI",
                        "Med Image Comput Comput Interv",
                        "Int Conf Med Image Comput Comput Interv"
                    ],
                    "url": "http://www.miccai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/3ccd600bab618a919c0e6e69d1724c9afd453a5a",
                "title": "Out-of-Distribution Detection for Long-tailed and Fine-grained Skin Lesion Images",
                "abstract": "Recent years have witnessed a rapid development of automated methods for skin lesion diagnosis and classification. Due to an increasing deployment of such systems in clinics, it has become important to develop a more robust system towards various Out-of-Distribution(OOD) samples (unknown skin lesions and conditions). However, the current deep learning models trained for skin lesion classification tend to classify these OOD samples incorrectly into one of their learned skin lesion categories. To address this issue, we propose a simple yet strategic approach that improves the OOD detection performance while maintaining the multi-class classification accuracy for the known categories of skin lesion. To specify, this approach is built upon a realistic scenario of a long-tailed and fine-grained OOD detection task for skin lesion images. Through this approach, 1) First, we target the mixup amongst middle and tail classes to address the long-tail problem. 2) Later, we combine the above mixup strategy with prototype learning to address the fine-grained nature of the dataset. The unique contribution of this paper is two-fold, justified by extensive experiments. First, we present a realistic problem setting of OOD task for skin lesion. Second, we propose an approach to target the long-tailed and fine-grained aspects of the problem setting simultaneously to increase the OOD performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1444768042",
                        "name": "Deval Mehta"
                    },
                    {
                        "authorId": "1869980",
                        "name": "Y. Gal"
                    },
                    {
                        "authorId": "40415931",
                        "name": "Adrian Bowling"
                    },
                    {
                        "authorId": "94085349",
                        "name": "Paul Bonnington"
                    },
                    {
                        "authorId": "1808390",
                        "name": "ZongYuan Ge"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We would like to highlight that our observation is in contrast to the prior work [Thulasidasan et al., 2019] which suggests that Mixup provides reliable uncertainty estimates for OOD data as well.",
                "It has been observed that temperature scaling [Guo et al., 2017] or replacing the typical cross-entropy loss [Chung et al., 2021, Thulasidasan et al., 2019, Mukhoti et al., 2020] can be highly effective to reduce this mismatch."
            ],
            "intents": [
                "result",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f0f6aa56d74bce44cbf511cd692038e9145e8462",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-14502",
                    "ArXiv": "2206.14502",
                    "DOI": "10.48550/arXiv.2206.14502",
                    "CorpusId": 250113456
                },
                "corpusId": 250113456,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f0f6aa56d74bce44cbf511cd692038e9145e8462",
                "title": "RegMixup: Mixup as a Regularizer Can Surprisingly Improve Accuracy and Out Distribution Robustness",
                "abstract": "We show that the effectiveness of the well celebrated Mixup [Zhang et al., 2018] can be further improved if instead of using it as the sole learning objective, it is utilized as an additional regularizer to the standard cross-entropy loss. This simple change not only provides much improved accuracy but also significantly improves the quality of the predictive uncertainty estimation of Mixup in most cases under various forms of covariate shifts and out-of-distribution detection experiments. In fact, we observe that Mixup yields much degraded performance on detecting out-of-distribution samples possibly, as we show empirically, because of its tendency to learn models that exhibit high-entropy throughout; making it difficult to differentiate in-distribution samples from out-distribution ones. To show the efficacy of our approach (RegMixup), we provide thorough analyses and experiments on vision datasets (ImageNet&CIFAR-10/100) and compare it with a suite of recent approaches for reliable uncertainty estimation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2088918572",
                        "name": "Francesco Pinto"
                    },
                    {
                        "authorId": "2110162580",
                        "name": "Harry Yang"
                    },
                    {
                        "authorId": "153317808",
                        "name": "S. Lim"
                    },
                    {
                        "authorId": "143635540",
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "authorId": "144679302",
                        "name": "P. Dokania"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Despite their extraordinary performance, DNNs are oftencriticized as being poorly calibrated and prone to be overconfident, thus leading to unsatisfied uncertainty estimation [10, 11, 12]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f7f3e3b56bbaf2a382a229844e0cc6d1bcb5924b",
                "externalIds": {
                    "DBLP": "conf/interspeech/YeSWCX22",
                    "ArXiv": "2206.13071",
                    "DOI": "10.48550/arXiv.2206.13071",
                    "CorpusId": 250072161
                },
                "corpusId": 250072161,
                "publicationVenue": {
                    "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
                    "name": "Interspeech",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Int Speech Commun Assoc",
                        "INTERSPEECH",
                        "Conference of the International Speech Communication Association"
                    ],
                    "issn": "2308-457X",
                    "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
                    "alternate_urls": [
                        "http://www.isca-speech.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f7f3e3b56bbaf2a382a229844e0cc6d1bcb5924b",
                "title": "Uncertainty Calibration for Deep Audio Classifiers",
                "abstract": "Although deep Neural Networks (DNNs) have achieved tremendous success in audio classification tasks, their uncertainty calibration are still under-explored. A well-calibrated model should be accurate when it is certain about its prediction and indicate high uncertainty when it is likely to be inaccurate. In this work, we investigate the uncertainty calibration for deep audio classifiers. In particular, we empirically study the performance of popular calibration methods: (i) Monte Carlo Dropout, (ii) ensemble, (iii) focal loss, and (iv) spectral-normalized Gaussian process (SNGP), on audio classification datasets. To this end, we evaluate (i-iv) for the tasks of environment sound and music genre classification. Results indicate that uncalibrated deep audio classifiers may be over-confident, and SNGP performs the best and is very efficient on the two datasets of this paper.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155647387",
                        "name": "Tong Ye"
                    },
                    {
                        "authorId": "81065454",
                        "name": "Shijing Si"
                    },
                    {
                        "authorId": "66063851",
                        "name": "Jianzong Wang"
                    },
                    {
                        "authorId": "145292435",
                        "name": "Ning Cheng"
                    },
                    {
                        "authorId": "91353860",
                        "name": "Jing Xiao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Important techniques in this category\ninclude mixup training [Thulasidasan et al., 2019], pretraining [Hendrycks et al., 2019a], label-smoothing [M\u00fcller et al., 2019], data augmentation [Ashukha et al., 2020], selfsupervised learning [Hendrycks et al., 2019b], Bayesian approximation (MC-dropout)\u2026",
                "\u2026a transformation that maps from classifiers raw outputs to their expected probabilities [Kull et al., 2019, Guo et al., 2017a, Gupta and Ramdas, 2021b], and ad-hoc methods that adapt the training process to generate better calibrated design [Thulasidasan et al., 2019, Hendrycks et al., 2019a]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8fc34146b7ae9515450985f8f8a8315e4684b1c4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-11562",
                    "ArXiv": "2206.11562",
                    "DOI": "10.48550/arXiv.2206.11562",
                    "CorpusId": 249953832
                },
                "corpusId": 249953832,
                "publicationVenue": {
                    "id": "f9af8000-42f8-410d-a622-e8811e41660a",
                    "name": "Conference on Uncertainty in Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Uncertainty in Artificial Intelligence",
                        "UAI",
                        "Conf Uncertain Artif Intell",
                        "Uncertain Artif Intell"
                    ],
                    "url": "http://www.auai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/8fc34146b7ae9515450985f8f8a8315e4684b1c4",
                "title": "A Geometric Method for Improved Uncertainty Estimation in Real-time",
                "abstract": "Machine learning classifiers are probabilistic in nature, and thus inevitably involve uncertainty. Predicting the probability of a specific input to be correct is called uncertainty (or confidence) estimation and is crucial for risk management. Post-hoc model calibrations can improve models' uncertainty estimations without the need for retraining, and without changing the model. Our work puts forward a geometric-based approach for uncertainty estimation. Roughly speaking, we use the geometric distance of the current input from the existing training inputs as a signal for estimating uncertainty and then calibrate that signal (instead of the model's estimation) using standard post-hoc calibration techniques. We show that our method yields better uncertainty estimations than recently proposed approaches by extensively evaluating multiple datasets and models. In addition, we also demonstrate the possibility of performing our approach in near real-time applications. Our code is available at our Github https://github.com/NoSleepDeveloper/Geometric-Calibrator.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2172503795",
                        "name": "Gabriella Chouraqui"
                    },
                    {
                        "authorId": "2432326",
                        "name": "L. Cohen"
                    },
                    {
                        "authorId": "2651037",
                        "name": "Gil Einziger"
                    },
                    {
                        "authorId": "2172484861",
                        "name": "Liel Leman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "One is to design OOD detection oriented neural networks and objective [13] [15] [1] [11] [4] [23], which can achieve high performance OOD detection but they need overhead computation cost to retrain the models."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5442e45e36f18401ad2e671dc6f3036b14f4eaf4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-05055",
                    "ArXiv": "2207.05055",
                    "DOI": "10.48550/arXiv.2207.05055",
                    "CorpusId": 250450944
                },
                "corpusId": 250450944,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5442e45e36f18401ad2e671dc6f3036b14f4eaf4",
                "title": "WeShort: Out-of-distribution Detection With Weak Shortcut structure",
                "abstract": "Neural networks have achieved impressive performance for data in the distribution which is the same as the training set but can produce an overcon\ufb01dent incorrect result for the data these networks have never seen. Therefore, detecting whether input comes from an out-of-distribution(OOD) is key to ensuring the safety of neural networks in the real world. In this paper, we propose a simple and effective post-hoc technique, WeShort, to reduce the overcon\ufb01dence of neural networks in OOD data. Our method is inspired by the observation of the internal residual structure, which shows the separation of the OOD and in-distribution (ID) data in the Shortcut. Our method is compatible with different OOD detection methods and can generalize well to different architectures of networks. We demonstrate our method in various OOD datasets to show its competitive performance and to provide reasonable hypotheses and dis-cussions to explain why our method works. On the ImageNet benchmark, Weshort achieves state-of-the-art performance on the false positive rate (FPR95) and the area under the receiver operating characteristic (AUROC) on the family of post-hoc methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144384491",
                        "name": "Jin-Siang Lin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Thus, we reasonably guess that VQAMix may contribute to the model\u2019s interpretability by taking the soft label as the supervision during the training process according to [56]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e9480d62e216f77d5556b7eda769daa4c92d004d",
                "externalIds": {
                    "DBLP": "journals/tmi/GongCMLL22",
                    "DOI": "10.1109/TMI.2022.3185008",
                    "CorpusId": 249886240,
                    "PubMed": "35727773"
                },
                "corpusId": 249886240,
                "publicationVenue": {
                    "id": "e0cda45d-3074-4ac0-80b8-e5250df00b89",
                    "name": "IEEE Transactions on Medical Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Med Imaging"
                    ],
                    "issn": "0278-0062",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=42",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e9480d62e216f77d5556b7eda769daa4c92d004d",
                "title": "VQAMix: Conditional Triplet Mixup for Medical Visual Question Answering",
                "abstract": "Medical visual question answering (VQA) aims to correctly answer a clinical question related to a given medical image. Nevertheless, owing to the expensive manual annotations of medical data, the lack of labeled data limits the development of medical VQA. In this paper, we propose a simple yet effective data augmentation method, VQAMix, to mitigate the data limitation problem. Specifically, VQAMix generates more labeled training samples by linearly combining a pair of VQA samples, which can be easily embedded into any visual-language model to boost performance. However, mixing two VQA samples would construct new connections between images and questions from different samples, which will cause the answers for those new fabricated image-question pairs to be missing or meaningless. To solve the missing answer problem, we first develop the Learning with Missing Labels (LML) strategy, which roughly excludes the missing answers. To alleviate the meaningless answer issue, we design the Learning with Conditional-mixed Labels (LCL) strategy, which further utilizes language-type prior to forcing the mixed pairs to have reasonable answers that belong to the same category. Experimental results on the VQA-RAD and PathVQA benchmarks show that our proposed method significantly improves the performance of the baseline by about 7% and 5% on the averaging result of two backbones, respectively. More importantly, VQAMix could improve confidence calibration and model interpretability, which is significant for medical VQA models in practical applications. All code and models are available at https://github.com/haifangong/VQAMix.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "67273856",
                        "name": "Haifan Gong"
                    },
                    {
                        "authorId": "2149511037",
                        "name": "Guanqi Chen"
                    },
                    {
                        "authorId": "39699474",
                        "name": "Mingzhi Mao"
                    },
                    {
                        "authorId": "1700892",
                        "name": "Z. Li"
                    },
                    {
                        "authorId": "2108853215",
                        "name": "Guanbin Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "in NeurIPS 2019 (top-tier, H5 Index: 192) [42], and theoretically explained in ICLR 2021 (top-tier, H5",
                "Researchers [42, 20] showed that training with mixup can significantly improve the model calibration.",
                "\u2022 In agreement with previous work [13, 47, 32, 42], temperature scaling maintained the discrimination and improved calibration in all scenarios, including balanced- vs."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bf0a1dfcb246f6687d9a67411f8178ae61758e0a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-08833",
                    "ArXiv": "2206.08833",
                    "DOI": "10.48550/arXiv.2206.08833",
                    "CorpusId": 249847834
                },
                "corpusId": 249847834,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bf0a1dfcb246f6687d9a67411f8178ae61758e0a",
                "title": "A Comparative Study of Confidence Calibration in Deep Learning: From Computer Vision to Medical Imaging",
                "abstract": "Although deep learning prediction models have been successful in the discrimination of di erent classes, they can often su er from poor calibration across challenging domains including healthcare. Moreover, the long-tail distribution poses great challenges in deep learning classification problems including clinical disease prediction. There are approaches proposed recently to calibrate deep prediction in computer vision, but there are no studies found to demonstrate how the representative models work in di erent challenging contexts. In this paper, we bridge the confidence calibration from computer vision to medical imaging with a comparative study of four high-impact calibration models. Our studies are conducted in di erent contexts (natural image classification and lung cancer risk estimation) including in balanced vs. imbalanced training sets and in computer vision vs. medical imaging. Our results support key findings: (1) We achieve new conclusions which are not studied under di erent learning contexts, e.g., combining two calibration models that both mitigate the overconfident prediction can lead to under-confident prediction, and simpler calibration models from computer vision domain tend to more generalizable to medical imaging. (2) We highlight the gap between general computer vision tasks and medical imaging prediction, e.g., calibration methods ideal for general computer vision tasks may in fact damage the calibration of medical imaging prediction. (3) We also reinforce previous conclusions in natural image classification settings. We believe that this study has merits to guide readers to choose calibration models and understand gaps between general computer vision and medical imaging domains. 1",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2183412",
                        "name": "Riqiang Gao"
                    },
                    {
                        "authorId": "2218368942",
                        "name": "Thomas Z. Li"
                    },
                    {
                        "authorId": "46556781",
                        "name": "Yucheng Tang"
                    },
                    {
                        "authorId": "2510540",
                        "name": "Zhoubing Xu"
                    },
                    {
                        "authorId": "46477206",
                        "name": "M. Kammer"
                    },
                    {
                        "authorId": "33683509",
                        "name": "S. Antic"
                    },
                    {
                        "authorId": "31835741",
                        "name": "K. Sandler"
                    },
                    {
                        "authorId": "144732149",
                        "name": "Fabien Maldonado"
                    },
                    {
                        "authorId": "2257806",
                        "name": "T. Lasko"
                    },
                    {
                        "authorId": "2133475009",
                        "name": "Bennett A. Landman"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "82566ceb10f9ab9d73ff9f1793f297cbdd54b0e0",
                "externalIds": {
                    "PubMedCentral": "10103736",
                    "DOI": "10.1101/2022.06.13.22276315",
                    "CorpusId": 249706911,
                    "PubMed": "37052912"
                },
                "corpusId": 249706911,
                "publicationVenue": {
                    "id": "d5e5b5e7-54b1-4f53-82fc-4853f3e71c58",
                    "name": "medRxiv",
                    "type": "journal",
                    "url": "https://www.medrxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/82566ceb10f9ab9d73ff9f1793f297cbdd54b0e0",
                "title": "Multitask Learning for Activity Detection in Neovascular Age-Related Macular Degeneration",
                "abstract": "Objective: Modeling of the ophthalmologist's decision-making process for activity detection in neovascular age-related macular degeneration with a multi-task convolutional deep neuronal network which takes intra- and subretinal fluid into account. Design: A cohort study to evaluate the multi-task deep learning model for activity detection. Participants: n = 70 patients (46 female, 24 male) attended the University Eye Hospital Tuebingen between 21.2.2018 and 27.6.2018. 3762 optical coherence tomography B-scans (right eye: 2011, left eye: 1751) were acquired from them with Heidelberg Spectralis, Heidelberg, Germany. Methods: B-scans were graded by a retina specialist and an ophthalmology resident, and then used to develop a multi-task deep learning model to concurrently predict disease activity in neovascular age-related macular degeneration along with the presence of sub- and intraretinal fluid. Main outcome measures: Performance metrics compared to single-task networks, visualization of the representation driving the DNN-based decisions using t-distributed stochastic neighbor embedding and analysis of the model's decisions via clinically validated saliency mapping techniques. Results: The multi-task model surpassed single-task networks in accuracy for activity detection. Visualizations via t-distributed stochastic neighbor embedding and saliency maps highlighted that the network's decisions for activity of neovascular age-related macular degeneration were based on the presence of sub- and intraretinal fluids, the optical coherence tomography characteristics used for treatment decision in clinical routine. Conclusion: Multi-task learning increases the performance of neuronal networks for predicting disease activity, while providing clinicians with an easily accessible decision control, which resembles human reasoning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145366222",
                        "name": "M. Ayhan"
                    },
                    {
                        "authorId": "1641091270",
                        "name": "H. Faber"
                    },
                    {
                        "authorId": "5517703",
                        "name": "L. Kuehlewein"
                    },
                    {
                        "authorId": "3751490",
                        "name": "W. Inhoffen"
                    },
                    {
                        "authorId": "1395781066",
                        "name": "G. Aliyeva"
                    },
                    {
                        "authorId": "6367559",
                        "name": "F. Ziemssen"
                    },
                    {
                        "authorId": "1689077",
                        "name": "Philipp Berens"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, naive pseudo labeling can result in accumulation of errors (Guo et al., 2017; Wei et al., 2022; Meinke & Hein, 2020; Thulasidasan et al., 2019; Kristiadi et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2ab8d3a547d6a806d75332bae0915d4f37a41d1e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-07240",
                    "ArXiv": "2206.07240",
                    "DOI": "10.48550/arXiv.2206.07240",
                    "CorpusId": 249674320
                },
                "corpusId": 249674320,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2ab8d3a547d6a806d75332bae0915d4f37a41d1e",
                "title": "Test-Time Adaptation for Visual Document Understanding",
                "abstract": "For visual document understanding (VDU), self-supervised pretraining has been shown to successfully generate transferable representations, yet, effective adaptation of such representations to distribution shifts at test-time remains to be an unexplored area. We propose DocTTA, a novel test-time adaptation method for documents, that does source-free domain adaptation using unlabeled target document data. DocTTA leverages cross-modality self-supervised learning via masked visual language modeling, as well as pseudo labeling to adapt models learned on a \\textit{source} domain to an unlabeled \\textit{target} domain at test time. We introduce new benchmarks using existing public datasets for various VDU tasks, including entity recognition, key-value extraction, and document visual question answering. DocTTA shows significant improvements on these compared to the source model performance, up to 1.89\\% in (F1 score), 3.43\\% (F1 score), and 17.68\\% (ANLS score), respectively. Our benchmark datasets are available at \\url{https://saynaebrahimi.github.io/DocTTA.html}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "27556211",
                        "name": "Sayna Ebrahimi"
                    },
                    {
                        "authorId": "2676352",
                        "name": "Sercan \u00d6. Arik"
                    },
                    {
                        "authorId": "1945962",
                        "name": "Tomas Pfister"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Previous works have demonstrated that mixup-like data augmentation techniques can greatly improve the uncertainty estimation on unseen data Thulasidasan et al. (2019); Hendrycks et al. (2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9ec32d50f2c24ff827f2d7c41bbac4df56e77900",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-06661",
                    "ArXiv": "2206.06661",
                    "DOI": "10.48550/arXiv.2206.06661",
                    "CorpusId": 249642077
                },
                "corpusId": 249642077,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9ec32d50f2c24ff827f2d7c41bbac4df56e77900",
                "title": "SoTeacher: A Student-oriented Teacher Network Training Framework for Knowledge Distillation",
                "abstract": "How to train an ideal teacher for knowledge distillation is still an open problem. It has been widely observed that a teacher minimizing the empirical risk not necessarily yields the best performing student, suggesting a fundamental discrepancy between the common practice in teacher network training and the distillation objective. To fill this gap, we propose a novel student-oriented teacher network training framework SoTeacher, inspired by recent findings that student performance hinges on teacher's capability to approximate the true label distribution of training samples. We theoretically established that (1) the empirical risk minimizer with proper scoring rules as loss function can provably approximate the true label distribution of training data if the hypothesis function is locally Lipschitz continuous around training samples; and (2) when data augmentation is employed for training, an additional constraint is required that the minimizer has to produce consistent predictions across augmented views of the same training input. In light of our theory, SoTeacher renovates the empirical risk minimization by incorporating Lipschitz regularization and consistency regularization. It is worth mentioning that SoTeacher is applicable to almost all teacher-student architecture pairs, requires no prior knowledge of the student upon teacher's training, and induces almost no computation overhead. Experiments on two benchmark datasets confirm that SoTeacher can improve student performance significantly and consistently across various knowledge distillation algorithms and teacher-student pairs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2113540861",
                        "name": "Chengyu Dong"
                    },
                    {
                        "authorId": "2109392217",
                        "name": "Liyuan Liu"
                    },
                    {
                        "authorId": "2884976",
                        "name": "Jingbo Shang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mixup [69] and its extensions [60,68] have been effective at improving confidence calibration [58], and to some extent, generalization [68, 69], in the balanced setting.",
                "Among calibration methods in the literature, we explore mixup [69] for two main reasons: (1) mixup has shown to improve calibration in the balanced setting [58] and to some extent in the longtailed setting [71]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "814c5859ba569cc8d1d7977bc1a424621ae7de24",
                "externalIds": {
                    "DBLP": "conf/cvpr/AimarJFK23",
                    "ArXiv": "2206.05260",
                    "DOI": "10.1109/CVPR52729.2023.01912",
                    "CorpusId": 254018198
                },
                "corpusId": 254018198,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/814c5859ba569cc8d1d7977bc1a424621ae7de24",
                "title": "Balanced Product of Calibrated Experts for Long-Tailed Recognition",
                "abstract": "Many real-world recognition problems are characterized by long-tailed label distributions. These distributions make representation learning highly challenging due to limited generalization over the tail classes. If the test distribution differs from the training distribution, e.g. uniform versus long-tailed, the problem of the distribution shift needs to be addressed. A recent line of work proposes learning multiple diverse experts to tackle this issue. Ensemble diversity is encouraged by various techniques, e.g. by specializing different experts in the head and the tail classes. In this work, we take an analytical approach and extend the notion of logit adjustment to ensembles to form a Balanced Product of Experts (BalPoE). BalPoE combines a family of experts with different test-time target distributions, generalizing several previous approaches. We show how to properly define these distributions and combine the experts in order to achieve unbiased predictions, by proving that the ensemble is Fisher-consistent for minimizing the balanced error. Our theoretical analysis shows that our balanced ensemble requires calibrated experts, which we achieve in practice using mixup. We conduct extensive experiments and our method obtains new state-of-the-art results on three long-tailed datasets: CIFAR-100-LT, ImageNet-LT, and iNaturalist-2018. Our code is available at https://github.com/emasa/BalPoE-CalibratedLT.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2064245444",
                        "name": "Emanuel Sanchez Aimar"
                    },
                    {
                        "authorId": "71308961",
                        "name": "Arvi Jonnarth"
                    },
                    {
                        "authorId": "2228323",
                        "name": "M. Felsberg"
                    },
                    {
                        "authorId": "40462390",
                        "name": "Marco Kuhlmann"
                    }
                ]
            }
        },
        {
            "contexts": [
                "More broadly, there has been much recent work develop methods to improve calibration for deep learning models, including augmentation-based training [Thulasidasan et al., 2019, Hendrycks et al., 2019b], self-supervised learning [Hendrycks et al., 2019a], ensembling [Lakshminarayanan et al., 2017],\u2026"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "46ff4e969a958d177204ed5c9cf78a59d9ce927d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-02757",
                    "ArXiv": "2206.02757",
                    "DOI": "10.48550/arXiv.2206.02757",
                    "CorpusId": 249394635
                },
                "corpusId": 249394635,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/46ff4e969a958d177204ed5c9cf78a59d9ce927d",
                "title": "Robust Calibration with Multi-domain Temperature Scaling",
                "abstract": "Uncertainty quantification is essential for the reliable deployment of machine learning models to high-stakes application domains. Uncertainty quantification is all the more challenging when training distribution and test distribution are different, even the distribution shifts are mild. Despite the ubiquity of distribution shifts in real-world applications, existing uncertainty quantification approaches mainly study the in-distribution setting where the train and test distributions are the same. In this paper, we develop a systematic calibration model to handle distribution shifts by leveraging data from multiple domains. Our proposed method -- multi-domain temperature scaling -- uses the heterogeneity in the domains to improve calibration robustness under distribution shift. Through experiments on three benchmark data sets, we find our proposed method outperforms existing methods as measured on both in-distribution and out-of-distribution test sets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2142173500",
                        "name": "Yaodong Yu"
                    },
                    {
                        "authorId": "153079946",
                        "name": "Stephen Bates"
                    },
                    {
                        "authorId": "2146275249",
                        "name": "Yi-An Ma"
                    },
                    {
                        "authorId": "1453240859",
                        "name": "Michael I. Jordan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6fa274b4606da1b594780d940e5fe3cb4267e7a8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-01570",
                    "ArXiv": "2206.01570",
                    "DOI": "10.1109/IJCNN55064.2022.9892866",
                    "CorpusId": 249375463
                },
                "corpusId": 249375463,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/6fa274b4606da1b594780d940e5fe3cb4267e7a8",
                "title": "On Calibration of Graph Neural Networks for Node Classification",
                "abstract": "Graphs can model real-world, complex systems by representing entities and their interactions in terms of nodes and edges. To better exploit the graph structure, graph neural networks have been developed, which learn entity and edge embeddings for tasks such as node classification and link prediction. These models achieve good performance with respect to accuracy, but the confidence scores associated with the predictions might not be calibrated. That means that the scores might not reflect the ground-truth probabilities of the predicted events, which would be especially important for safety-critical applications. Even though graph neural networks are used for a wide range of tasks, the calibration thereof has not been sufficiently explored yet. We investigate the calibration of graph neural networks for node classification, study the effect of existing post-processing calibration methods, and analyze the influence of model capacity, graph density, and a new loss function on calibration. Further, we propose a topology-aware calibration method that takes the neighboring nodes into account and yields improved calibration compared to baseline methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111151090",
                        "name": "Tong Liu"
                    },
                    {
                        "authorId": "2144405786",
                        "name": "Yushan Liu"
                    },
                    {
                        "authorId": "7506584",
                        "name": "Marcel Hildebrandt"
                    },
                    {
                        "authorId": "2020545",
                        "name": "Mitchell Joblin"
                    },
                    {
                        "authorId": "49404233",
                        "name": "Hang Li"
                    },
                    {
                        "authorId": "1742501819",
                        "name": "Volker Tresp"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "15adf2f3da2986a3f3fc5c153a3c5f73097f8b56",
                "externalIds": {
                    "PubMedCentral": "9200984",
                    "DOI": "10.3389/fpsyt.2022.912600",
                    "CorpusId": 249244013,
                    "PubMed": "35722548"
                },
                "corpusId": 249244013,
                "publicationVenue": {
                    "id": "07931092-9002-4e9f-a62d-00af30be540f",
                    "name": "Frontiers in Psychiatry",
                    "type": "journal",
                    "alternate_names": [
                        "Front Psychiatry"
                    ],
                    "issn": "1664-0640",
                    "url": "https://www.frontiersin.org/journals/psychiatry",
                    "alternate_urls": [
                        "http://www.frontiersin.org/psychiatry",
                        "http://www.frontiersin.org/psychiatry/about"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/15adf2f3da2986a3f3fc5c153a3c5f73097f8b56",
                "title": "Deep Learning in Neuroimaging: Overcoming Challenges With Emerging Approaches",
                "abstract": "Deep learning (DL) is of great interest in psychiatry due its potential yet largely untapped ability to utilize multidimensional datasets (such as fMRI data) to predict clinical outcomes. Typical DL methods, however, have strong assumptions, such as large datasets and underlying model opaqueness, that are suitable for natural image prediction problems but not medical imaging. Here we describe three relatively novel DL approaches that may help accelerate its incorporation into mainstream psychiatry research and ultimately bring it into the clinic as a prognostic tool. We first introduce two methods that can reduce the amount of training data required to develop accurate models. These may prove invaluable for fMRI-based DL given the time and monetary expense required to acquire neuroimaging data. These methods are (1) transfer learning \u2212 the ability of deep learners to incorporate knowledge learned from one data source (e.g., fMRI data from one site) and apply it toward learning from a second data source (e.g., data from another site), and (2) data augmentation (via Mixup) \u2212 a self-supervised learning technique in which \u201cvirtual\u201d instances are created. We then discuss explainable artificial intelligence (XAI), i.e., tools that reveal what features (and in what combinations) deep learners use to make decisions. XAI can be used to solve the \u201cblack box\u201d criticism common in DL and reveal mechanisms that ultimately produce clinical outcomes. We expect these techniques to greatly enhance the applicability of DL in psychiatric research and help reveal novel mechanisms and potential pathways for therapeutic intervention in mental illness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2663442",
                        "name": "J. Smucny"
                    },
                    {
                        "authorId": "2167331582",
                        "name": "Ge Shi"
                    },
                    {
                        "authorId": "143763341",
                        "name": "I. Davidson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The strong performance of MixUp on CIFAR-100 is probably due to it being particularly well-suited to the hierarchical nature of the classes [23, 28]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b3ed63e72419897971fe81bd1b6a6361419d970c",
                "externalIds": {
                    "DBLP": "conf/nips/OyenKHS22",
                    "ArXiv": "2206.01106",
                    "DOI": "10.48550/arXiv.2206.01106",
                    "CorpusId": 249282343
                },
                "corpusId": 249282343,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b3ed63e72419897971fe81bd1b6a6361419d970c",
                "title": "Robustness to Label Noise Depends on the Shape of the Noise Distribution in Feature Space",
                "abstract": "Machine learning classifiers have been demonstrated, both empirically and theoretically, to be robust to label noise under certain conditions -- notably the typical assumption is that label noise is independent of the features given the class label. We provide a theoretical framework that generalizes beyond this typical assumption by modeling label noise as a distribution over feature space. We show that both the scale and the shape of the noise distribution influence the posterior likelihood; and the shape of the noise distribution has a stronger impact on classification performance if the noise is concentrated in feature space where the decision boundary can be moved. For the special case of uniform label noise (independent of features and the class label), we show that the Bayes optimal classifier for $c$ classes is robust to label noise until the ratio of noisy samples goes above $\\frac{c-1}{c}$ (e.g. 90% for 10 classes), which we call the tipping point. However, for the special case of class-dependent label noise (independent of features given the class label), the tipping point can be as low as 50%. Most importantly, we show that when the noise distribution targets decision boundaries (label noise is directly dependent on feature space), classification robustness can drop off even at a small scale of noise. Even when evaluating recent label-noise mitigation methods we see reduced accuracy when label noise is dependent on features. These findings explain why machine learning often handles label noise well if the noise distribution is uniform in feature-space; yet it also points to the difficulty of overcoming label noise when it is concentrated in a region of feature space where a decision boundary can move.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2864621",
                        "name": "D. Oyen"
                    },
                    {
                        "authorId": "41052688",
                        "name": "Michal Kucer"
                    },
                    {
                        "authorId": "47874805",
                        "name": "N. Hengartner"
                    },
                    {
                        "authorId": "2142379551",
                        "name": "H. Singh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Two examples are label smoothing [28] and mixup [47] which are originally proposed to improve generalization [37, 46] and adversarial robustness [55], respectively.",
                "probability estimates in the literature, including but not limited to post-processing [6, 38], Bayesian approximation [2, 5], regularization [28, 47], and deep ensemble [22].",
                "CPC was compared to several popular single-model calibration baselines: vanilla DNN, temperature scaling [6], MC dropout [44], label smoothing [28, 46], and mixup [47, 55]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7a2a548f12e1be2300a34dc467e97b8abe4d0cbd",
                "externalIds": {
                    "DBLP": "conf/cvpr/ChengV22",
                    "DOI": "10.1109/CVPR52688.2022.01334",
                    "CorpusId": 249948182
                },
                "corpusId": 249948182,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7a2a548f12e1be2300a34dc467e97b8abe4d0cbd",
                "title": "Calibrating Deep Neural Networks by Pairwise Constraints",
                "abstract": "It is well known that deep neural networks (DNNs) pro-duce poorly calibrated estimates of class-posterior prob-abilities. We hypothesize that this is due to the limited calibration supervision provided by the cross-entropy loss, which places all emphasis on the probability of the true class and mostly ignores the remaining. We consider how each example can supervise all classes and show that the calibration of a C-way classification problem is equivalent to the calibration of C(C - 1) /2 pairwise binary classifi-cation problems that can be derived from it. This suggests the hypothesis that DNN calibration can be improved by providing calibration supervision to all such binary prob-lems. An implementation of this calibration by pairwise constraints (CPC) is then proposed, based on two types of binary calibration constraints. This is finally shown to be implementable with a very minimal increase in the complex-ity of cross-entropy training. Empirical evaluations of the proposed CPC method across multiple datasets and DNN architectures demonstrate state-of-the-art calibration per-formance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "26329505",
                        "name": "Jiacheng Cheng"
                    },
                    {
                        "authorId": "1699559",
                        "name": "N. Vasconcelos"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These approaches encompass Monte-Carlo Dropout (MCDropout) [10], Maximum Class Probability (MCP) [17], Trust Score [20]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fc1e700c687f06e886f9e096f8b909635451d329",
                "externalIds": {
                    "DBLP": "journals/jbd/Al-Dmour22",
                    "DOI": "10.1186/s40537-022-00624-0",
                    "CorpusId": 249263423
                },
                "corpusId": 249263423,
                "publicationVenue": {
                    "id": "d60da343-ab92-4310-b3d7-2c0860287a9d",
                    "name": "Journal of Big Data",
                    "type": "journal",
                    "alternate_names": [
                        "J Big Data",
                        "Journal on Big Data"
                    ],
                    "issn": "2196-1115",
                    "alternate_issns": [
                        "2579-0048"
                    ],
                    "url": "http://www.journalofbigdata.com/",
                    "alternate_urls": [
                        "http://www.springer.com/computer/database+management+&+information+retrieval/journal/40537",
                        "http://techscience.com/JBD/index.html",
                        "https://journalofbigdata.springeropen.com",
                        "https://journalofbigdata.springeropen.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fc1e700c687f06e886f9e096f8b909635451d329",
                "title": "Ramifications of incorrect image segmentations; emphasizing on the potential effects on deep learning methods failure",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1401645844",
                        "name": "Hayat Al-Dmour"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some recent research indicates that soft-label augmentation techniques, such as label smoothing [209] and mixup [210,211], can effectively mitigate the over-confidence problem, thus helping network calibration."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2726fb04936e6c668dff18b2117e92e34c826698",
                "externalIds": {
                    "DBLP": "journals/sensors/ZhouLZLYY22",
                    "PubMedCentral": "9185239",
                    "DOI": "10.3390/s22114208",
                    "CorpusId": 249325453,
                    "PubMed": "35684831"
                },
                "corpusId": 249325453,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2726fb04936e6c668dff18b2117e92e34c826698",
                "title": "Towards Deep Radar Perception for Autonomous Driving: Datasets, Methods, and Challenges",
                "abstract": "With recent developments, the performance of automotive radar has improved significantly. The next generation of 4D radar can achieve imaging capability in the form of high-resolution point clouds. In this context, we believe that the era of deep learning for radar perception has arrived. However, studies on radar deep learning are spread across different tasks, and a holistic overview is lacking. This review paper attempts to provide a big picture of the deep radar perception stack, including signal processing, datasets, labelling, data augmentation, and downstream tasks such as depth and velocity estimation, object detection, and sensor fusion. For these tasks, we focus on explaining how the network structure is adapted to radar domain knowledge. In particular, we summarise three overlooked challenges in deep radar perception, including multi-path effects, uncertainty problems, and adverse weather effects, and present some attempts to solve them.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118764762",
                        "name": "Yi Zhou"
                    },
                    {
                        "authorId": "49480144",
                        "name": "Lulu Liu"
                    },
                    {
                        "authorId": "2112672456",
                        "name": "Hao Zhao"
                    },
                    {
                        "authorId": "1397286739",
                        "name": "M. L\u00f3pez-Ben\u00edtez"
                    },
                    {
                        "authorId": "49296984",
                        "name": "Limin Yu"
                    },
                    {
                        "authorId": "24015303",
                        "name": "Yutao Yue"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, we apply mixup [41, 33], an effective data augmentation method.",
                "Nevertheless, this issue is distinct from the field of out-of-distribution (OOD) samples, where a rich body of research exists already [33, 36, 20, 35, 42, 34].",
                "MixUp [41, 33] is a method, which trains a neural network on convex combinations of pairs of examples and their labels; thus, favoring simple linear behavior in-between training examples."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a670e4e63a4bfae8eec0c30ac7afbad5573d8081",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-15094",
                    "ArXiv": "2205.15094",
                    "DOI": "10.48550/arXiv.2205.15094",
                    "CorpusId": 249191612
                },
                "corpusId": 249191612,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a670e4e63a4bfae8eec0c30ac7afbad5573d8081",
                "title": "CHALLENGER: Training with Attribution Maps",
                "abstract": "We show that utilizing attribution maps for training neural networks can improve regularization of models and thus increase performance. Regularization is key in deep learning, especially when training complex models on relatively small datasets. In order to understand inner workings of neural networks, attribution methods such as Layer-wise Relevance Propagation (LRP) have been extensively studied, particularly for interpreting the relevance of input features. We introduce Challenger, a module that leverages the explainable power of attribution maps in order to manipulate particularly relevant input patterns. Therefore, exposing and subsequently resolving regions of ambiguity towards separating classes on the ground-truth data manifold, an issue that arises particularly when training models on rather small datasets. Our Challenger module increases model performance through building more diverse filters within the network and can be applied to any input data domain. We demonstrate that our approach results in substantially better classification as well as calibration performance on datasets with only a few samples up to datasets with thousands of samples. In particular, we show that our generic domain-independent approach yields state-of-the-art results in vision, natural language processing and on time series tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1583683907",
                        "name": "Christian Tomani"
                    },
                    {
                        "authorId": "1695302",
                        "name": "D. Cremers"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d489d27a815138b3f4c5fb510a6401d2c1f2c849",
                "externalIds": {
                    "DOI": "10.1109/icaibd55127.2022.9820117",
                    "CorpusId": 250516800
                },
                "corpusId": 250516800,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d489d27a815138b3f4c5fb510a6401d2c1f2c849",
                "title": "Underwater Acoustic Target Classification with Joint Learning Framework and Data Augmentation",
                "abstract": "As speech recognition technologies have garnered increasing attention recently, the possibility of integrating related technologies into the field of underwater acoustic target classification (ATC) has piqued researchers\u2019 curiosity. However, the classification of underwater acoustic targets confronts several obstacles due to the time-varying and complicated nature of the underwater environment,, such as the low signal-to-noise ratio and background interference. To address these concerns, this paper proposes a combined feature extraction framework and a deep convolutional neural network learning framework. This combination features include MFCC, CQT, Gammatone, Log-mel which are extracted from the underwater acoustic signal and fed into the joint learning framework. The classifier has a deep learning architecture, which is a deep neural network system that can learn differentiating features from physical feature representations to abstract concepts in a hierarchical pattern. Multiple datasets under diverse settings have been used to verify the performance of our proposed architecture. Using the acoustic data gathered in several underwater experiments conducted in a shallow sea setting, the proposed framework achieves a recognition rate of 89.9% and a competitive classification accuracy in the underwater scenario.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Liang Chen"
                    },
                    {
                        "authorId": "2152943280",
                        "name": "Feng Liu"
                    },
                    {
                        "authorId": "1452340962",
                        "name": "Daihui Li"
                    },
                    {
                        "authorId": "47287365",
                        "name": "Tongsheng Shen"
                    },
                    {
                        "authorId": "2110555742",
                        "name": "Dexin Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "615671736f303f05a9f2a026a9e677788eef96b3",
                "externalIds": {
                    "ArXiv": "2205.12507",
                    "DBLP": "conf/emnlp/Si0MB22",
                    "DOI": "10.18653/v1/2022.findings-emnlp.204",
                    "CorpusId": 253098276
                },
                "corpusId": 253098276,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/615671736f303f05a9f2a026a9e677788eef96b3",
                "title": "Re-Examining Calibration: The Case of Question Answering",
                "abstract": "For users to trust model predictions, they need to understand model outputs, particularly their confidence - calibration aims to adjust (calibrate) models' confidence to match expected accuracy. We argue that the traditional calibration evaluation does not promote effective calibrations: for example, it can encourage always assigning a mediocre confidence score to all predictions, which does not help users distinguish correct predictions from wrong ones. Building on those observations, we propose a new calibration metric, MacroCE, that better captures whether the model assigns low confidence to wrong predictions and high confidence to correct predictions. Focusing on the practical application of open-domain question answering, we examine conventional calibration methods applied on the widely-used retriever-reader pipeline, all of which do not bring significant gains under our new MacroCE metric. Toward better calibration, we propose a new calibration method (ConsCal) that uses not just final model predictions but whether multiple model checkpoints make consistent predictions. Altogether, we provide an alternative view of calibration along with a new metric, re-evaluation of existing calibration methods on our metric, and proposal of a more effective calibration method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152358188",
                        "name": "Chenglei Si"
                    },
                    {
                        "authorId": "145756130",
                        "name": "Chen Zhao"
                    },
                    {
                        "authorId": "2149886296",
                        "name": "Sewon Min"
                    },
                    {
                        "authorId": "1389036863",
                        "name": "Jordan L. Boyd-Graber"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Various methods have been proposed to improve DNN\u2019s calibration in training [33, 54, 55, 56] or by a post-processing module after training [31, 32, 57]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "812ff246291fe4a618f174facf09ac8854b10ae0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-12134",
                    "ArXiv": "2205.12134",
                    "DOI": "10.48550/arXiv.2205.12134",
                    "CorpusId": 249017491
                },
                "corpusId": 249017491,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/812ff246291fe4a618f174facf09ac8854b10ae0",
                "title": "Adversarial Attack on Attackers: Post-Process to Mitigate Black-Box Score-Based Query Attacks",
                "abstract": "The score-based query attacks (SQAs) pose practical threats to deep neural networks by crafting adversarial perturbations within dozens of queries, only using the model's output scores. Nonetheless, we note that if the loss trend of the outputs is slightly perturbed, SQAs could be easily misled and thereby become much less effective. Following this idea, we propose a novel defense, namely Adversarial Attack on Attackers (AAA), to confound SQAs towards incorrect attack directions by slightly modifying the output logits. In this way, (1) SQAs are prevented regardless of the model's worst-case robustness; (2) the original model predictions are hardly changed, i.e., no degradation on clean accuracy; (3) the calibration of confidence scores can be improved simultaneously. Extensive experiments are provided to verify the above advantages. For example, by setting $\\ell_\\infty=8/255$ on CIFAR-10, our proposed AAA helps WideResNet-28 secure 80.59% accuracy under Square attack (2500 queries), while the best prior defense (i.e., adversarial training) only attains 67.44%. Since AAA attacks SQA's general greedy strategy, such advantages of AAA over 8 defenses can be consistently observed on 8 CIFAR-10/ImageNet models under 6 SQAs, using different attack targets, bounds, norms, losses, and strategies. Moreover, AAA calibrates better without hurting the accuracy. Our code is available at https://github.com/Sizhe-Chen/AAA.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "150293336",
                        "name": "Sizhe Chen"
                    },
                    {
                        "authorId": "2151325553",
                        "name": "Zhehao Huang"
                    },
                    {
                        "authorId": "2035647651",
                        "name": "Qinghua Tao"
                    },
                    {
                        "authorId": "2145900047",
                        "name": "Yingwen Wu"
                    },
                    {
                        "authorId": "3011497",
                        "name": "Cihang Xie"
                    },
                    {
                        "authorId": "144335593",
                        "name": "X. Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As such, the authors used a form of ECE in their evaluation that measures the miscalibration of the most probable class output by the classifier, a choice mirrored in a number of subsequent works (Thulasidasan et al., 2019; Mu\u0308ller et al., 2019; Mukhoti et al., 2020; Alexandari et al., 2020).",
                "As such, the authors used a form of ECE in their evaluation that measures the miscalibration of the most probable class output by the classifier, a choice mirrored in a number of subsequent works (Thulasidasan et al., 2019; M\u00fcller et al., 2019; Mukhoti et al., 2020; Alexandari et al., 2020).",
                "Training-time interventions in our experiments include: Label smoothing (LS) (Szegedy et al., 2016; Mu\u0308ller et al., 2019), Mix-up (MU) (Zhang et al., 2018; Thulasidasan et al., 2019), and Focal loss (FL) (Lin et al., 2017; Mukhoti et al., 2020).",
                ", 2019), Mix-up (MU) (Zhang et al., 2018; Thulasidasan et al., 2019), and Focal loss (FL) (Lin et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6dde380d60fef16572df39218856a2aa6ce7d7dc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-11454",
                    "ArXiv": "2205.11454",
                    "DOI": "10.48550/arXiv.2205.11454",
                    "CorpusId": 248986320
                },
                "corpusId": 248986320,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6dde380d60fef16572df39218856a2aa6ce7d7dc",
                "title": "What is Your Metric Telling You? Evaluating Classifier Calibration under Context-Specific Definitions of Reliability",
                "abstract": "Classifier calibration has received recent attention from the machine learning community due both to its practical utility in facilitating decision making, as well as the observation that modern neural network classifiers are poorly calibrated. Much of this focus has been towards the goal of learning classifiers such that their output with largest magnitude (the\"predicted class\") is calibrated. However, this narrow interpretation of classifier outputs does not adequately capture the variety of practical use cases in which classifiers can aid in decision making. In this work, we argue that more expressive metrics must be developed that accurately measure calibration error for the specific context in which a classifier will be deployed. To this end, we derive a number of different metrics using a generalization of Expected Calibration Error (ECE) that measure calibration error under different definitions of reliability. We then provide an extensive empirical evaluation of commonly used neural network architectures and calibration techniques with respect to these metrics. We find that: 1) definitions of ECE that focus solely on the predicted class fail to accurately measure calibration error under a selection of practically useful definitions of reliability and 2) many common calibration techniques fail to improve calibration performance uniformly across ECE metrics derived from these diverse definitions of reliability.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2166053502",
                        "name": "John Kirchenbauer"
                    },
                    {
                        "authorId": "2166053945",
                        "name": "Jacob Oaks"
                    },
                    {
                        "authorId": "39110759",
                        "name": "Eric Heim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "model regularization [11], [12], this process strongly perturbs the semantic information from reliably labeled samples."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "37badc878ac4d86411f7d00de238d256e4a26c3c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-10158",
                    "ArXiv": "2205.10158",
                    "DOI": "10.1109/ICPR56361.2022.9956602",
                    "CorpusId": 248965372
                },
                "corpusId": 248965372,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/37badc878ac4d86411f7d00de238d256e4a26c3c",
                "title": "Swapping Semantic Contents for Mixing Images",
                "abstract": "Deep architecture have proven capable of solving many tasks provided a sufficient amount of labeled data. In fact, the amount of available labeled data has become the principal bottleneck in low label settings such as Semi-Supervised Learning. Mixing Data Augmentations do not typically yield new labeled samples, as indiscriminately mixing contents creates between-class samples. In this work, we introduce the SciMix framework that can learn to replace the global semantic content from one sample. By teaching a StyleGan generator to embed a semantic style code into image backgrounds, we obtain new mixing scheme for data augmentation. We then demonstrate that SciMix yields novel mixed samples that inherit many characteristics from their non-semantic parents. Afterwards, we verify those samples can be used to improve the performance semi-supervised frameworks like Mean Teacher or Fixmatch, and even fully supervised learning on a small labeled dataset.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "41021696",
                        "name": "R\u00e9my Sun"
                    },
                    {
                        "authorId": "37135849",
                        "name": "Cl\u00e9ment Masson"
                    },
                    {
                        "authorId": "2112211438",
                        "name": "Gilles H'enaff"
                    },
                    {
                        "authorId": "1728523",
                        "name": "Nicolas Thome"
                    },
                    {
                        "authorId": "51021910",
                        "name": "M. Cord"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[8] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9b378643779397165f779027167894956ea67d9b",
                "externalIds": {
                    "DBLP": "conf/nips/EinbinderRSZ22",
                    "ArXiv": "2205.05878",
                    "DOI": "10.48550/arXiv.2205.05878",
                    "CorpusId": 248722103
                },
                "corpusId": 248722103,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9b378643779397165f779027167894956ea67d9b",
                "title": "Training Uncertainty-Aware Classifiers with Conformalized Deep Learning",
                "abstract": "Deep neural networks are powerful tools to detect hidden patterns in data and leverage them to make predictions, but they are not designed to understand uncertainty and estimate reliable probabilities. In particular, they tend to be overconfident. We begin to address this problem in the context of multi-class classification by developing a novel training algorithm producing models with more dependable uncertainty estimates, without sacrificing predictive power. The idea is to mitigate overconfidence by minimizing a loss function, inspired by advances in conformal inference, that quantifies model uncertainty by carefully leveraging hold-out data. Experiments with synthetic and real data demonstrate this method can lead to smaller conformal prediction sets with higher conditional coverage, after exact calibration with hold-out data, compared to state-of-the-art alternatives.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2164989431",
                        "name": "Bat-Sheva Einbinder"
                    },
                    {
                        "authorId": "3295351",
                        "name": "Yaniv Romano"
                    },
                    {
                        "authorId": "15178977",
                        "name": "Matteo Sesia"
                    },
                    {
                        "authorId": "2165080630",
                        "name": "Yanfei Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Despite this, MixUp has been explored for NLP tasks with substantial success using hidden state representations (Verma et al., 2019).",
                "Manifold MixUp (M-MixUp) (Verma et al., 2019) generates additional samples by interpolating random training samples in the feature space (obtained from the task-specific layer on top of the BERT pre-trained language model)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "53f61f53acc5589505ad18e166997afeac5fe06b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-03403",
                    "ArXiv": "2205.03403",
                    "ACL": "2022.naacl-main.314",
                    "DOI": "10.48550/arXiv.2205.03403",
                    "CorpusId": 248562839
                },
                "corpusId": 248562839,
                "publicationVenue": {
                    "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
                    "name": "North American Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "North Am Chapter Assoc Comput Linguistics",
                        "NAACL"
                    ],
                    "url": "https://www.aclweb.org/portal/naacl"
                },
                "url": "https://www.semanticscholar.org/paper/53f61f53acc5589505ad18e166997afeac5fe06b",
                "title": "A Data Cartography based MixUp for Pre-trained Language Models",
                "abstract": "MixUp is a data augmentation strategy where additional samples are generated during training by combining random pairs of training samples and their labels. However, selecting random pairs is not potentially an optimal choice. In this work, we propose TDMixUp, a novel MixUp strategy that leverages Training Dynamics and allows more informative samples to be combined for generating new data samples. Our proposed TDMixUp first measures confidence, variability, (Swayamdipta et al., 2020), and Area Under the Margin (AUM) (Pleiss et al., 2020) to identify the characteristics of training samples (e.g., as easy-to-learn or ambiguous samples), and then interpolates these characterized samples. We empirically validate that our method not only achieves competitive performance using a smaller subset of the training data compared with strong baselines, but also yields lower expected calibration error on the pre-trained language model, BERT, on both in-domain and out-of-domain settings in a wide range of NLP tasks. We publicly release our code.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118885320",
                        "name": "Seohong Park"
                    },
                    {
                        "authorId": "2140493460",
                        "name": "Cornelia Caragea"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another interesting approach is the use of mixup [28,32] to improve OoD detection performance.",
                "Authors of [28] find that mixup trained networks are significantly better calibrated and are less prone to over-confident predictions on out-ofdistribution and random-noise data."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8a050cba5ba458ac14de0144d2be93200a07357e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-03493",
                    "ArXiv": "2205.03493",
                    "DOI": "10.48550/arXiv.2205.03493",
                    "CorpusId": 248572424
                },
                "corpusId": 248572424,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8a050cba5ba458ac14de0144d2be93200a07357e",
                "title": "Norm-Scaling for Out-of-Distribution Detection",
                "abstract": "Out-of-Distribution (OoD) inputs are examples that do not belong to the true underlying distribution of the dataset. Research has shown that deep neural nets make confident mispredictions on OoD inputs. Therefore, it is critical to identify OoD inputs for safe and reliable deployment of deep neural nets. Often a threshold is applied on a similarity score to detect OoD inputs. One such similarity is angular similarity which is the dot product of latent representation with the mean class representation. Angular similarity encodes uncertainty, for example, if the angular similarity is less, it is less certain that the input belongs to that class. However, we observe that, different classes have different distributions of angular similarity. Therefore, applying a single threshold for all classes is not ideal since the same similarity score represents different uncertainties for different classes. In this paper, we propose norm-scaling which normalizes the logits separately for each class. This ensures that a single value consistently represents similar uncertainty for various classes. We show that norm-scaling, when used with maximum softmax probability detector, achieves 9.78% improvement in AUROC, 5.99% improvement in AUPR and 33.19% reduction in FPR95 metrics over previous state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2064867801",
                        "name": "Deepak Ravikumar"
                    },
                    {
                        "authorId": "2091913080",
                        "name": "Kaushik Roy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Data augmentation (Thulasidasan et al., 2019; Hendrycks* et al., 2020; Cubuk et al., 2020; Chen et al., 2020a,b) represents a second class of approach that improves the representation learning of neural networks by explicitly injecting expert knowledge about the types of surface-form perturbations\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "60f2ae3b448b035e957603e84d1a073ad708879b",
                "externalIds": {
                    "DBLP": "journals/jmlr/LiuP0LWJNSTL23",
                    "ArXiv": "2205.00403",
                    "DOI": "10.48550/arXiv.2205.00403",
                    "CorpusId": 248495970
                },
                "corpusId": 248495970,
                "publicationVenue": {
                    "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                    "name": "Journal of machine learning research",
                    "type": "journal",
                    "alternate_names": [
                        "Journal of Machine Learning Research",
                        "J mach learn res",
                        "J Mach Learn Res"
                    ],
                    "issn": "1532-4435",
                    "alternate_issns": [
                        "1533-7928"
                    ],
                    "url": "http://www.ai.mit.edu/projects/jmlr/",
                    "alternate_urls": [
                        "http://jmlr.csail.mit.edu/",
                        "http://www.jmlr.org/",
                        "http://portal.acm.org/affiliated/jmlr"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/60f2ae3b448b035e957603e84d1a073ad708879b",
                "title": "A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness",
                "abstract": "Accurate uncertainty quantification is a major challenge in deep learning, as neural networks can make overconfident errors and assign high confidence predictions to out-of-distribution (OOD) inputs. The most popular approaches to estimate predictive uncertainty in deep learning are methods that combine predictions from multiple neural networks, such as Bayesian neural networks (BNNs) and deep ensembles. However their practicality in real-time, industrial-scale applications are limited due to the high memory and computational cost. Furthermore, ensembles and BNNs do not necessarily fix all the issues with the underlying member networks. In this work, we study principled approaches to improve uncertainty property of a single network, based on a single, deterministic representation. By formalizing the uncertainty quantification as a minimax learning problem, we first identify distance awareness, i.e., the model's ability to quantify the distance of a testing example from the training data, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation. We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs with two simple changes: (1) applying spectral normalization to hidden weights to enforce bi-Lipschitz smoothness in representations and (2) replacing the last output layer with a Gaussian process layer. On a suite of vision and language understanding benchmarks, SNGP outperforms other single-model approaches in prediction, calibration and out-of-domain detection. Furthermore, SNGP provides complementary benefits to popular techniques such as deep ensembles and data augmentation, making it a simple and scalable building block for probabilistic deep learning. Code is open-sourced at https://github.com/google/uncertainty-baselines",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108345570",
                        "name": "J. Liu"
                    },
                    {
                        "authorId": "151216770",
                        "name": "Shreyas Padhy"
                    },
                    {
                        "authorId": "92095972",
                        "name": "Jie Jessie Ren"
                    },
                    {
                        "authorId": "2112304965",
                        "name": "Zi Lin"
                    },
                    {
                        "authorId": "38356166",
                        "name": "Yeming Wen"
                    },
                    {
                        "authorId": "3451901",
                        "name": "Ghassen Jerfel"
                    },
                    {
                        "authorId": "81408931",
                        "name": "Zachary Nado"
                    },
                    {
                        "authorId": "144108062",
                        "name": "Jasper Snoek"
                    },
                    {
                        "authorId": "47497262",
                        "name": "Dustin Tran"
                    },
                    {
                        "authorId": "40627523",
                        "name": "Balaji Lakshminarayanan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[78] presents work on calibrating a model by training with Mixup, a data augmentation technique where new samples are generated by combining images and their associated labels."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ef17ffdd75a1b0347add2b91f90d937277138e90",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-13631",
                    "ArXiv": "2204.13631",
                    "DOI": "10.48550/arXiv.2204.13631",
                    "CorpusId": 248426953
                },
                "corpusId": 248426953,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/ef17ffdd75a1b0347add2b91f90d937277138e90",
                "title": "Reliable Visual Question Answering: Abstain Rather Than Answer Incorrectly",
                "abstract": "Machine learning has advanced dramatically, narrowing the accuracy gap to humans in multimodal tasks like visual question answering (VQA). However, while humans can say\"I don't know\"when they are uncertain (i.e., abstain from answering a question), such ability has been largely neglected in multimodal research, despite the importance of this problem to the usage of VQA in real settings. In this work, we promote a problem formulation for reliable VQA, where we prefer abstention over providing an incorrect answer. We first enable abstention capabilities for several VQA models, and analyze both their coverage, the portion of questions answered, and risk, the error on that portion. For that, we explore several abstention approaches. We find that although the best performing models achieve over 70% accuracy on the VQA v2 dataset, introducing the option to abstain by directly using a model's softmax scores limits them to answering less than 7.5% of the questions to achieve a low risk of error (i.e., 1%). This motivates us to utilize a multimodal selection function to directly estimate the correctness of the predicted answers, which we show can increase the coverage by, for example, 2.3x from 6.8% to 15.6% at 1% risk. While it is important to analyze both coverage and risk, these metrics have a trade-off which makes comparing VQA models challenging. To address this, we also propose an Effective Reliability metric for VQA that places a larger cost on incorrect answers compared to abstentions. This new problem formulation, metric, and analysis for VQA provide the groundwork for building effective and reliable VQA models that have the self-awareness to abstain if and only if they don't know the answer.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "153188991",
                        "name": "Spencer Whitehead"
                    },
                    {
                        "authorId": "52013156",
                        "name": "Suzanne Petryk"
                    },
                    {
                        "authorId": "2163739291",
                        "name": "Vedaad Shakib"
                    },
                    {
                        "authorId": "49988044",
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "authorId": "1753210",
                        "name": "Trevor Darrell"
                    },
                    {
                        "authorId": "34721166",
                        "name": "Anna Rohrbach"
                    },
                    {
                        "authorId": "34849128",
                        "name": "Marcus Rohrbach"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Reference [5] focuses on Mixup\u2019s effects of improving calibration and predictive uncertainty.",
                "calibration [5], robustness [6], [7] and generalization [6],"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6f9d9fdd8e0fc20d6d581bef2255e681dbbd9a12",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-10695",
                    "ArXiv": "2204.10695",
                    "DOI": "10.1109/TIP.2023.3290514",
                    "CorpusId": 248366254,
                    "PubMed": "37405884"
                },
                "corpusId": 248366254,
                "publicationVenue": {
                    "id": "e117fa7f-05b7-4dd6-b64b-0a0a7c0393d8",
                    "name": "IEEE Transactions on Image Processing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Image Process"
                    ],
                    "issn": "1057-7149",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=83",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6f9d9fdd8e0fc20d6d581bef2255e681dbbd9a12",
                "title": "Universum-Inspired Supervised Contrastive Learning",
                "abstract": "As an effective data augmentation method, Mixup synthesizes an extra amount of samples through linear interpolations. Despite its theoretical dependency on data properties, Mixup reportedly performs well as a regularizer and calibrator contributing reliable robustness and generalization to deep model training. In this paper, inspired by Universum Learning which uses out-of-class samples to assist the target tasks, we investigate Mixup from a largely under-explored perspective - the potential to generate in-domain samples that belong to none of the target classes, that is, universum. We find that in the framework of supervised contrastive learning, Mixup-induced universum can serve as surprisingly high-quality hard negatives, greatly relieving the need for large batch sizes in contrastive learning. With these findings, we propose Universum-inspired supervised Contrastive learning (UniCon), which incorporates Mixup strategy to generate Mixup-induced universum as universum negatives and pushes them apart from anchor samples of the target classes. We extend our method to the unsupervised setting, proposing Unsupervised Universum-inspired contrastive model (Un-Uni). Our approach not only improves Mixup with hard labels, but also innovates a novel measure to generate universum data. With a linear classifier on the learned representations, UniCon shows state-of-the-art performance on various datasets. Specially, UniCon achieves 81.7% top-1 accuracy on CIFAR-100, surpassing the state of art by a significant margin of 5.2% with a much smaller batch size, typically, 256 in UniCon vs. 1024 in SupCon (Khosla et al., 2020) using ResNet-50. Un-Uni also outperforms SOTA methods on CIFAR-100. The code of this paper is released on https://github.com/hannaiiyanggit/UniCon.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2163391035",
                        "name": "Aiyang Han"
                    },
                    {
                        "authorId": "51021226",
                        "name": "Chuanxing Geng"
                    },
                    {
                        "authorId": "48848338",
                        "name": "Songcan Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "lasidasan provides several methods for this, including one which modifies the training of a neural network for this robustness [8, 9]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "93722176866886218bfcbed8d183e2738741a90c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-07609",
                    "ArXiv": "2205.07609",
                    "DOI": "10.1016/j.nima.2022.167174",
                    "CorpusId": 248811634
                },
                "corpusId": 248811634,
                "publicationVenue": {
                    "id": "490dcb7a-2200-49f2-8f12-1ee688f1d150",
                    "name": "Nuclear Instruments and Methods in Physics Research Section A : Accelerators, Spectrometers, Detectors and Associated Equipment",
                    "type": "journal",
                    "alternate_names": [
                        "Nuclear Instruments and Methods in Physics Research",
                        "Nucl Instrum Method Phys Res Sect  Accel M Detect Assoc Equip",
                        "Nucl Instrum  Method Phys Res Sect A-accelerators M Detect Assoc Equip",
                        "Nuclear Instruments & Methods in Physics Research Section A-accelerators Spectrometers Detectors and Associated Equipment",
                        "Nucl Instrum Method Phys Res"
                    ],
                    "issn": "0168-9002",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/505701/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/01675087",
                        "http://www.sciencedirect.com/science/journal/01689002"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/93722176866886218bfcbed8d183e2738741a90c",
                "title": "Reduction of detection limit and quantification uncertainty due to interferent by neural classification with abstention",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40304670",
                        "name": "A. Hagen"
                    },
                    {
                        "authorId": "144700906",
                        "name": "K. Jarman"
                    },
                    {
                        "authorId": "144944645",
                        "name": "Jesse D. Ward"
                    },
                    {
                        "authorId": "31686532",
                        "name": "G. Eiden"
                    },
                    {
                        "authorId": "7569391",
                        "name": "C. Barinaga"
                    },
                    {
                        "authorId": "32129188",
                        "name": "E. Mace"
                    },
                    {
                        "authorId": "3654993",
                        "name": "C. Aalseth"
                    },
                    {
                        "authorId": "13896957",
                        "name": "Anthony J. Carado"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Most modern DNNs, when trained for classification in a supervised learning setting, are trained using one-hot encoding that have all the probability mass centered in one class; the training labels are thus zero-entropy signals that admit no uncertainty about the input [51]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cdf2397e3b90dab5e60ef28405187b79a68b48f1",
                "externalIds": {
                    "DBLP": "conf/cvpr/HebbalaguppePM022",
                    "ArXiv": "2203.13834",
                    "DOI": "10.1109/CVPR52688.2022.01561",
                    "CorpusId": 247762984
                },
                "corpusId": 247762984,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cdf2397e3b90dab5e60ef28405187b79a68b48f1",
                "title": "A Stitch in Time Saves Nine: A Train-Time Regularizing Loss for Improved Neural Network Calibration",
                "abstract": "Deep Neural Networks (dnns) are known to make over-confident mistakes, which makes their use problematic in safety-critical applications. State-of-the-art (sota) calibration techniques improve on the confidence of predicted labels alone, and leave the confidence of non-max classes (e.g. top-2, top-5) uncalibrated. Such calibration is not suitable for label refinement using post-processing. Further, most sota techniques learn a few hyper-parameters post-hoc, leaving out the scope for image, or pixel specific calibration. This makes them unsuitable for calibration under domain shift, or for dense prediction tasks like semantic segmentation. In this paper, we argue for intervening at the train time itself, so as to directly produce calibrated dnn models. We propose a novel auxiliary loss function: Multi-class Difference in Confidence and Accuracy (mdca), to achieve the same. mdca can be used in conjunction with other application/task specific loss functions. We show that training with mdca leads to better calibrated models in terms of Expected Calibration Error (ece), and Static Calibration Error (sce) on image classification, and segmentation tasks. We report ece (sce) score of 0.72 (1.60) on the cifar 100 dataset, in comparison to 1.90 (1.71) by the sota. Under domain shift, a ResNet-18 model trained on pacs dataset using mdca gives a average ece (sce) score of 19.7 (9.7) across all domains, compared to 24.2 (11.8) by the sota. For segmentation task, we report a 2\u00d7 reduction in calibration error on pascal-voc dataset in comparison to Focal Loss [32]. Finally, mdca training improves calibration even on imbalanced data, and for natural language classification tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3177394",
                        "name": "R. Hebbalaguppe"
                    },
                    {
                        "authorId": "2105537941",
                        "name": "Jatin Prakash"
                    },
                    {
                        "authorId": "2160538620",
                        "name": "Neelabh Madan"
                    },
                    {
                        "authorId": "145676235",
                        "name": "Chetan Arora"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5d36844ef1b1f8c5e957436b00a903b7b4d2bad3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-13168",
                    "ArXiv": "2203.13168",
                    "DOI": "10.1109/ICRA48891.2023.10161460",
                    "CorpusId": 247628259
                },
                "corpusId": 247628259,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5d36844ef1b1f8c5e957436b00a903b7b4d2bad3",
                "title": "Model-Agnostic Multi-Agent Perception Framework",
                "abstract": "Existing multi-agent perception systems assume that every agent utilizes the same model with identical parameters and architecture. The performance can be degraded with different perception models due to the mismatch in their confidence scores. In this work, we propose a model-agnostic multi-agent perception framework to reduce the negative effect caused by the model discrepancies without sharing the model information. Specifically, we propose a confidence calibrator that can eliminate the prediction confidence score bias. Each agent performs such calibration independently on a standard public database to protect intellectual property. We also propose a corresponding bounding box aggregation algorithm that considers the confidence scores and the spatial agreement of neighboring boxes. Our experiments shed light on the necessity of model calibration across different agents, and the results show that the proposed framework improves the baseline 3D object detection performance of heterogeneous agents. The code can be found at this url.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2139128397",
                        "name": "Weizhe (Wesley) Chen"
                    },
                    {
                        "authorId": "47462785",
                        "name": "Runsheng Xu"
                    },
                    {
                        "authorId": "2119233630",
                        "name": "Hao Xiang"
                    },
                    {
                        "authorId": "1777505",
                        "name": "Lantao Liu"
                    },
                    {
                        "authorId": "2146393082",
                        "name": "Jiaqi Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "feature layout indicates that we can learn a more compact and disjoint decision boundary, which has been evaluated to be critical in machine learning applications such mixup [66,49] and uncertainty estimation in deep learning [15]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c96c551ece333d6e7f95f77176cedef07b3b1b18",
                "externalIds": {
                    "ArXiv": "2203.11183",
                    "DBLP": "journals/corr/abs-2203-11183",
                    "DOI": "10.48550/arXiv.2203.11183",
                    "CorpusId": 247594448
                },
                "corpusId": 247594448,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/c96c551ece333d6e7f95f77176cedef07b3b1b18",
                "title": "Masked Discrimination for Self-Supervised Learning on Point Clouds",
                "abstract": "Masked autoencoding has achieved great success for self-supervised learning in the image and language domains. However, mask based pretraining has yet to show benefits for point cloud understanding, likely due to standard backbones like PointNet being unable to properly handle the training versus testing distribution mismatch introduced by masking during training. In this paper, we bridge this gap by proposing a discriminative mask pretraining Transformer framework, MaskPoint}, for point clouds. Our key idea is to represent the point cloud as discrete occupancy values (1 if part of the point cloud; 0 if not), and perform simple binary classification between masked object points and sampled noise points as the proxy task. In this way, our approach is robust to the point sampling variance in point clouds, and facilitates learning rich representations. We evaluate our pretrained models across several downstream tasks, including 3D shape classification, segmentation, and real-word object detection, and demonstrate state-of-the-art results while achieving a significant pretraining speedup (e.g., 4.1x on ScanNet) compared to the prior state-of-the-art Transformer baseline. Code is available at https://github.com/haotian-liu/MaskPoint.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2143856368",
                        "name": "Haotian Liu"
                    },
                    {
                        "authorId": "2053144019",
                        "name": "Mu Cai"
                    },
                    {
                        "authorId": "144756076",
                        "name": "Yong Jae Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DNNs trained with this technique are typically more generalizable and calibrated [39], whose predictions tend to be less overconfident."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "643514435ded72536e54f2ba39bd5b914cbd078b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-10761",
                    "ArXiv": "2203.10761",
                    "DOI": "10.48550/arXiv.2203.10761",
                    "CorpusId": 247594829
                },
                "corpusId": 247594829,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/643514435ded72536e54f2ba39bd5b914cbd078b",
                "title": "Decoupled Mixup for Data-efficient Learning",
                "abstract": "Mixup is an efficient data augmentation approach that improves the generalization of neural networks by smoothing the decision boundary with mixed data. Recently, dynamic mixup methods have improved previous static policies effectively (e.g., linear interpolation) by maximizing salient regions or maintaining the target in mixed samples. The discrepancy is that the generated mixed samples from dynamic policies are more instance discriminative than the static ones, e.g., the foreground objects are decoupled from the background. However, optimizing mixup policies with dynamic methods in input space is an expensive computation compared to static ones. Hence, we are trying to transfer the decoupling mechanism of dynamic methods from the data level to the objective function level and propose the general decoupled mixup (DM) loss. The primary effect is that DM can adaptively focus on discriminative features without losing the original smoothness of the mixup while avoiding heavy computational overhead. As a result, DM enables static mixup methods to achieve comparable or even exceed the performance of dynamic methods. This also leads to an interesting objective design problem for mixup training that we need to focus on both smoothing the decision boundaries and identifying discriminative features. Extensive experiments on supervised and semi-supervised learning benchmarks across seven classification datasets validate the effectiveness of DM by equipping it with various mixup methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145253136",
                        "name": "Zicheng Liu"
                    },
                    {
                        "authorId": "2118155623",
                        "name": "Siyuan Li"
                    },
                    {
                        "authorId": "2155370038",
                        "name": "Ge Wang"
                    },
                    {
                        "authorId": "2111728470",
                        "name": "Cheng Tan"
                    },
                    {
                        "authorId": "47767812",
                        "name": "Lirong Wu"
                    },
                    {
                        "authorId": "1390908654",
                        "name": "Stan Z. Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2012), stochastic inputs in Mixup training (Zhang et al., 2018; Thulasidasan et al., 2019), stochastic computational processes in variational inference algorithms (Kingma and Welling, 2014; Salimans et al.",
                "\u2026include the probabilistic computational unit built in fuzzy logic (Nov\u00e1k et al., 2012), stochastic inputs in Mixup training (Zhang et al., 2018; Thulasidasan et al., 2019), stochastic computational processes in variational inference algorithms (Kingma and Welling, 2014; Salimans et al., 2015),\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "67859f9a07e36016d9d503214d0f8dcf1cd7ad46",
                "externalIds": {
                    "ArXiv": "2203.11123",
                    "DOI": "10.1101/2022.03.21.485207",
                    "CorpusId": 247594283
                },
                "corpusId": 247594283,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/67859f9a07e36016d9d503214d0f8dcf1cd7ad46",
                "title": "Gene expression noise accelerates the evolution of a biological oscillator",
                "abstract": "Gene expression is a biochemical process, where stochastic binding and unbinding events naturally generate fluctuations and cell-to-cell variability in gene dynamics. These fluctuations typically have destructive consequences for proper biological dynamics and function (e.g., loss of timing and synchrony in biological oscillators). Here, we show that gene expression noise counter-intuitively accelerates the evolution of a biological oscillator and, thus, can impart a benefit to living organisms. We used computer simulations to evolve two mechanistic models of a biological oscillator at different levels of gene expression noise. We first show that gene expression noise induces oscillatory-like dynamics in regions of parameter space that cannot oscillate in the absence of noise. We then demonstrate that these noise-induced oscillations generate a fitness landscape whose gradient robustly and quickly guides evolution by mutation towards robust and self-sustaining oscillation. These results suggest that noise can help dynamical systems evolve or learn new behavior by revealing cryptic dynamic phenotypes outside the bifurcation point. Graphical Abstract",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108918177",
                        "name": "Y. Lin"
                    },
                    {
                        "authorId": "1696201",
                        "name": "Nicolas E. Buchler"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, Thulasidasan et al. (2019) investigated the impact of mixup for model calibration of NLU but only explored in-domain settings with simple deep learning architecture such as CNNs. Kong et al. (2020) explored BERT calibration using mixup as a regularization component on in-domain and\u2026",
                "While simple to implement, mixup has been shown to improve both predictive performance and model calibration, particularly on image classification tasks due to its regularization effect through data augmentation (Thulasidasan et al., 2019).",
                "\u2022 Mixup (Zhang et al., 2018; Thulasidasan et al., 2019): Mixup augments training data by linearly interpolating randomly selected training samples in the input space."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "05f6628948f79d0cce8664cc8146fd459d53e9d5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-07559",
                    "ACL": "2022.acl-long.368",
                    "ArXiv": "2203.07559",
                    "DOI": "10.48550/arXiv.2203.07559",
                    "CorpusId": 247450599
                },
                "corpusId": 247450599,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/05f6628948f79d0cce8664cc8146fd459d53e9d5",
                "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
                "abstract": "A well-calibrated neural model produces confidence (probability outputs) closely approximated by the expected accuracy. While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks, little is known about using mixup for model calibration on natural language understanding (NLU) tasks. In this paper, we explore mixup for model calibration on several NLU tasks and propose a novel mixup strategy for pre-trained language models that improves model calibration further. Our proposed mixup is guided by both the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) and the saliency map of each sample (Simonyan et al., 2013). Moreover, we combine our mixup strategy with model miscalibration correction techniques (i.e., label smoothing and temperature scaling) and provide detailed analyses of their impact on our proposed mixup. We focus on systematically designing experiments on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning. Our method achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118885320",
                        "name": "Seohong Park"
                    },
                    {
                        "authorId": "2140493460",
                        "name": "Cornelia Caragea"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Empirically, Mixup is helpful for robust representation learning, and it alleviates the overconfident problems and the failure of distribution shift settings as well as the in-distribution accuracy [20]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "314ec6dedd14a09606297e5d4e57022e7927f5ea",
                "externalIds": {
                    "ArXiv": "2203.03897",
                    "CorpusId": 252992822
                },
                "corpusId": 252992822,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/314ec6dedd14a09606297e5d4e57022e7927f5ea",
                "title": "Geodesic Multi-Modal Mixup for Robust Fine-Tuning",
                "abstract": "Pre-trained large-scale models provide a transferable embedding, and they show promising performance on diverse downstream tasks. However, the analysis of learned embedding has not been explored well, and the transferability for cross-modal tasks can be improved. This paper provides a perspective to understand multi-modal embedding in terms of uniformity and alignment. We newly \ufb01nd that the representation learned by multi-modal learning models such as CLIP has two separated embedding spaces for each heterogeneous dataset with less alignment. Besides, there are unexplored large intermediate areas between the two modalities with less uniformity. As a result, lack of alignment and uniformity might restrict the robustness and transferability of the representation for the downstream task. To this end, we provide a new end-to-end \ufb01ne-tuning method for robust representation that encourages better uniformity and alignment score. First, we propose a Geodesic Multi-Modal Mixup that mixes the representation of image and text to generate the hard negative samples on the hyperspherical embedding space. Second, we \ufb01ne-tune the multi-modal model on hard negative samples as well as normal negatives and positive samples with contrastive loss. Through extensive experiments on retrieval, classi\ufb01cation, and structure-awareness task, we demonstrate that our geodesic multi-modal Mixup learns a robust representation and provides improved performance on various downstream tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2127769429",
                        "name": "Junhyuk So"
                    },
                    {
                        "authorId": "2152046120",
                        "name": "Changdae Oh"
                    },
                    {
                        "authorId": "2188272447",
                        "name": "Yongtaek Lim"
                    },
                    {
                        "authorId": "2184781278",
                        "name": "Hoyoon Byun"
                    },
                    {
                        "authorId": "2067943570",
                        "name": "Minchul Shin"
                    },
                    {
                        "authorId": "2490092",
                        "name": "Kyungwoo Song"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c6bed5b58029081f70a321b6ae331752315810fd",
                "externalIds": {
                    "ArXiv": "2203.01850",
                    "DBLP": "journals/corr/abs-2203-01850",
                    "DOI": "10.48550/arXiv.2203.01850",
                    "CorpusId": 247222794
                },
                "corpusId": 247222794,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c6bed5b58029081f70a321b6ae331752315810fd",
                "title": "T-Cal: An optimal test for the calibration of predictive models",
                "abstract": "The prediction accuracy of machine learning methods is steadily increasing, but the calibration of their uncertainty predictions poses a significant challenge. Numerous works focus on obtaining well-calibrated predictive models, but less is known about reliably assessing model calibration. This limits our ability to know when algorithms for improving calibration have a real effect, and when their improvements are merely artifacts due to random noise in finite datasets. In this work, we consider detecting mis-calibration of predictive models using a finite validation dataset as a hypothesis testing problem. The null hypothesis is that the predictive model is calibrated, while the alternative hypothesis is that the deviation from calibration is sufficiently large. We find that detecting mis-calibration is only possible when the conditional probabilities of the classes are sufficiently smooth functions of the predictions. When the conditional class probabilities are H\\\"older continuous, we propose T-Cal, a minimax optimal test for calibration based on a debiased plug-in estimator of the $\\ell_2$-Expected Calibration Error (ECE). We further propose Adaptive T-Cal, a version that is adaptive to unknown smoothness. We verify our theoretical findings with a broad range of experiments, including with several popular deep neural net architectures and several standard post-hoc calibration methods. T-Cal is a practical general-purpose tool, which -- combined with classical tests for discrete-valued predictors -- can be used to test the calibration of virtually any probabilistic classification method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40950489",
                        "name": "Donghwan Lee"
                    },
                    {
                        "authorId": "1441111040",
                        "name": "Xinmeng Huang"
                    },
                    {
                        "authorId": "34112189",
                        "name": "Hamed Hassani"
                    },
                    {
                        "authorId": "2694895",
                        "name": "Edgar Dobriban"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d316a77d1724de9b5e008e699bd0cf3bfedc59e8",
                "externalIds": {
                    "DBLP": "journals/tii/AriefTW23",
                    "ArXiv": "2202.11283",
                    "DOI": "10.1109/TII.2022.3154783",
                    "CorpusId": 247058303
                },
                "corpusId": 247058303,
                "publicationVenue": {
                    "id": "2135230a-3b24-4b71-9583-60624389377a",
                    "name": "IEEE Transactions on Industrial Informatics",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Ind Informatics"
                    ],
                    "issn": "1551-3203",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=9424",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9424"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d316a77d1724de9b5e008e699bd0cf3bfedc59e8",
                "title": "Better Modeling Out-of-Distribution Regression on Distributed Acoustic Sensor Data Using Anchored Hidden State Mixup",
                "abstract": "Generalizing the application of machine learning models to situations where the statistical distribution of training and test data are different has been a complex problem. Our contributions in this article are threefold: 1) we introduce an anchored-based out-of-distribution (OOD) Regression Mixup algorithm, leveraging manifold hidden state mixup and observation similarities to form a novel regularization penalty; 2) we provide a first of its kind high-resolution distributed acoustic sensor dataset that is suitable for testing OOD regression modeling, allowing other researchers to benchmark progress in this area; and 3) we demonstrate with an extensive evaluation the generalization performance of the proposed method against existing approaches and then show that our method achieves state-of-the-art performance. We also demonstrate a wider applicability of the proposed method by exhibiting improved generalization performances on other types of regression datasets, including Udacity and Rotation-MNIST datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "107783401",
                        "name": "Hasan Asy\u2019ari Arief"
                    },
                    {
                        "authorId": "2155887238",
                        "name": "P. J. Thomas"
                    },
                    {
                        "authorId": "2448790",
                        "name": "T. Wiktorski"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, [15] shows that neural networks trained with mixup are significantly better calibrated and less prone to over-confident predictions on random noise data.",
                "A previous study [15] shows that relatively small values of \u03b1 \u2208 [0."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e3ba3e7416495c8b9640df967bbe6a10d3c673e6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-10672",
                    "ArXiv": "2202.10672",
                    "DOI": "10.1109/ICASSP43922.2022.9746411",
                    "CorpusId": 247025744
                },
                "corpusId": 247025744,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/e3ba3e7416495c8b9640df967bbe6a10d3c673e6",
                "title": "Contrastive-mixup Learning for Improved Speaker Verification",
                "abstract": "This paper proposes a novel formulation of prototypical loss with mixup for speaker verification. Mixup is a simple yet efficient data augmentation technique that fabricates a weighted combination of random data point and label pairs for deep neural network training. Mixup has attracted increasing attention due to its ability to improve robustness and generalization of deep neural networks. Although mixup has shown success in diverse domains, most applications have centered around closed-set classification tasks. In this work, we propose contrastive-mixup, a novel augmentation strategy that learns distinguishing representations based on a distance metric. During training, mixup operations generate convex interpolations of both inputs and virtual labels. Moreover, we have reformulated the prototypical loss function such that mixup is enabled on metric learning objectives. To demonstrate its generalization given limited training data, we conduct experiments by varying the number of available utterances from each speaker in the VoxCeleb database. Experimental results show that applying contrastive-mixup outperforms the existing baseline, reducing error rate by 16% relatively, especially when the number of training utterances per speaker is limited.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Xin Zhang"
                    },
                    {
                        "authorId": "35035988",
                        "name": "Minho Jin"
                    },
                    {
                        "authorId": "2073535939",
                        "name": "R. Cheng"
                    },
                    {
                        "authorId": "50391855",
                        "name": "Ruirui Li"
                    },
                    {
                        "authorId": "2056284949",
                        "name": "Eunjung Han"
                    },
                    {
                        "authorId": "1762744",
                        "name": "A. Stolcke"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is because that deep networks are likely to memorize some specific training statistics, resulting in overconfident prediction and poor generalization ability [4]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bb3222cc5e50ecdac486361bf7b9d1b917581993",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-08647",
                    "ArXiv": "2202.08647",
                    "DOI": "10.1109/icassp43922.2022.9747083",
                    "CorpusId": 246904868
                },
                "corpusId": 246904868,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/bb3222cc5e50ecdac486361bf7b9d1b917581993",
                "title": "Semantically Proportional Patchmix for Few-Shot Learning",
                "abstract": "Few-shot learning aims to classify unseen classes with only a limited number of labeled data. Recent works have demonstrated that training models with a simple transfer learning strategy can achieve competitive results in few-shot classification. Although excelling at distinguishing training data, these models are not well generalized to unseen data, probably due to insufficient feature representations on evaluation. To tackle this issue, we propose Semantically Proportional Patchmix (SePPMix), in which patches are cut and pasted among training images and the ground truth labels are mixed proportionally to the semantic information of the patches. In this way, we can improve the generalization ability of the model by regional dropout effect without introducing severe label noise. To learn more robust representations of data, we further take rotate transformation on the mixed images and predict rotations as a rule-based regularizer. Extensive experiments on prevalent few-shot benchmarks have shown the effectiveness of our proposed method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2157406958",
                        "name": "Jingquan Wang"
                    },
                    {
                        "authorId": null,
                        "name": "Jing Xu"
                    },
                    {
                        "authorId": "143749869",
                        "name": "Y. Pan"
                    },
                    {
                        "authorId": "1683510",
                        "name": "Zenglin Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many works have pointed out Softmax layer overconfidence as an open issue in the field of deep learning [20]\u2013[23]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a7b6368c5070efc2a2a3f77f069a50f6bdfcff7c",
                "externalIds": {
                    "DBLP": "journals/access/MelottiPBFG22",
                    "ArXiv": "2202.07825",
                    "DOI": "10.1109/ACCESS.2022.3175195",
                    "CorpusId": 246866987
                },
                "corpusId": 246866987,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a7b6368c5070efc2a2a3f77f069a50f6bdfcff7c",
                "title": "Reducing Overconfidence Predictions in Autonomous Driving Perception",
                "abstract": "In state-of-the-art deep learning for object recognition, Softmax and Sigmoid layers are most commonly employed as the predictor outputs. Such layers often produce overconfidence predictions rather than proper probabilistic scores, which can thus harm the decision-making of \u2018critical\u2019 perception systems applied in autonomous driving and robotics. Given this, we propose a probabilistic approach based on distributions calculated out of the Logit layer scores of pre-trained networks which are then used to constitute new decision layers based on Maximum Likelihood (ML) and Maximum a-Posteriori (MAP) inference. We demonstrate that the hereafter called ML and MAP layers are more suitable for probabilistic interpretations than Softmax and Sigmoid-based predictions for object recognition.We explore distinct sensor modalities via RGB images and LiDARs (RV: range-view) data from the KITTI and Lyft Level-5 datasets, where our approach shows promising performance compared to the usual Softmax and Sigmoid layers, with the benefit of enabling interpretable probabilistic predictions. Another advantage of the approach introduced in this paper is that the so-called ML and MAP layers can be implemented in existing trained networks, that is, the approach benefits from the output of the Logit layer of pre-trained networks. Thus, there is no need to carry out a new training phase since the ML and MAP layers are used in the test/prediction phase. The Classification results are presented using reliability diagrams, while detection results are illustrated using precision-recall curves.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51918162",
                        "name": "Gledson Melotti"
                    },
                    {
                        "authorId": "2097466",
                        "name": "C. Premebida"
                    },
                    {
                        "authorId": "89293047",
                        "name": "Jordan J. Bird"
                    },
                    {
                        "authorId": "144708349",
                        "name": "D. Faria"
                    },
                    {
                        "authorId": "2237423915",
                        "name": "Nuno Gonccalves"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other approaches aim to calibrate models during training, using well-chosen metrics [39, 29] or through data augmentation [48]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c8d42b2311e2fd7faee1d7a7c5280840de0cad56",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-03295",
                    "ArXiv": "2202.03295",
                    "DOI": "10.1088/2632-2153/acd749",
                    "CorpusId": 246634119
                },
                "corpusId": 246634119,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c8d42b2311e2fd7faee1d7a7c5280840de0cad56",
                "title": "Theoretical characterization of uncertainty in high-dimensional linear classification",
                "abstract": "Being able to reliably assess not only the accuracy but also the uncertainty of models\u2019 predictions is an important endeavor in modern machine learning. Even if the model generating the data and labels is known, computing the intrinsic uncertainty after learning the model from a limited number of samples amounts to sampling the corresponding posterior probability measure. Such sampling is computationally challenging in high-dimensional problems and theoretical results on heuristic uncertainty estimators in high-dimensions are thus scarce. In this manuscript, we characterize uncertainty for learning from a limited number of samples of high-dimensional Gaussian input data and labels generated by the probit model. In this setting, the Bayesian uncertainty (i.e. the posterior marginals) can be asymptotically obtained by the approximate message passing algorithm, bypassing the canonical but costly Monte Carlo sampling of the posterior. We then provide a closed-form formula for the joint statistics between the logistic classifier, the uncertainty of the statistically optimal Bayesian classifier and the ground-truth probit uncertainty. The formula allows us to investigate the calibration of the logistic classifier learning from a limited amount of samples. We discuss how over-confidence can be mitigated by appropriately regularizing.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2153463900",
                        "name": "Lucas Clart'e"
                    },
                    {
                        "authorId": "143616600",
                        "name": "Bruno Loureiro"
                    },
                    {
                        "authorId": "2909402",
                        "name": "F. Krzakala"
                    },
                    {
                        "authorId": "2107086291",
                        "name": "Lenka Zdeborov'a"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f471af0ba7d366233da32a1f7151fde42816790f",
                "externalIds": {
                    "DBLP": "journals/ml/FilhoSPSKF23",
                    "ArXiv": "2112.10327",
                    "DOI": "10.1007/s10994-023-06336-7",
                    "CorpusId": 256901249
                },
                "corpusId": 256901249,
                "publicationVenue": {
                    "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
                    "name": "Machine-mediated learning",
                    "type": "journal",
                    "alternate_names": [
                        "Mach learn",
                        "Machine Learning",
                        "Mach Learn"
                    ],
                    "issn": "0732-6718",
                    "alternate_issns": [
                        "0885-6125"
                    ],
                    "url": "http://www.springer.com/computer/artificial/journal/10994",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10994",
                        "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f471af0ba7d366233da32a1f7151fde42816790f",
                "title": "Classifier calibration: a survey on how to assess and improve predicted class probabilities",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3024963",
                        "name": "T. S. Filho"
                    },
                    {
                        "authorId": "2112980202",
                        "name": "Hao Song"
                    },
                    {
                        "authorId": "1399591467",
                        "name": "Miquel Perello-Nieto"
                    },
                    {
                        "authorId": "2126710957",
                        "name": "Ra\u00fal Santos-Rodr\u00edguez"
                    },
                    {
                        "authorId": "66368615",
                        "name": "Meelis Kull"
                    },
                    {
                        "authorId": "47840704",
                        "name": "Peter A. Flach"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The DNNs trained with such samples are significantly better calibrated [38], i."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "967bccffaeef1dbc3331dfa87bad4ea672085164",
                "externalIds": {
                    "DBLP": "conf/bigdataconf/LuH21",
                    "DOI": "10.1109/BigData52589.2021.9671816",
                    "CorpusId": 245934179
                },
                "corpusId": 245934179,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/967bccffaeef1dbc3331dfa87bad4ea672085164",
                "title": "MixNN: Combating Noisy Labels in Deep Learning by Mixing with Nearest Neighbors",
                "abstract": "Noisy labels are ubiquitous in real-world datasets, especially in the ones from web sources. Training deep neural networks on noisy datasets is a challenging task, as the networks have been shown to overfit the noisy labels in training, resulting in performance degradation. When trained on noisy datasets, deep neural networks have been observed to fit t he clean samples during an \"early learning\" phase, before eventually memorizing the mislabeled samples. We further explore the representation distributions in the early learning stage and find that the representations of similar samples from the same classes congregate regardless of their noisy labels. Inspired by these findings, we propose MixNN, a novel framework to mitigate the influence of noisy labels. In contrast with existing methods, which identify and eliminate the mislabeled samples, we modify the mislabeled samples by mixing them with their nearest neighbors through a weighted sum approach. The weights are calculated with a mixture model learning from the sample loss distribution. To enhance the performance in the presence of extreme label noise, we propose a strategy to estimate the soft targets by gradually correcting the noisy labels. We demonstrate that the estimated targets yield a more accurate approximation to ground truth labels and a better quality of the learned representations with more separated and clearly bounded clusters. Extensive experiments in two benchmarks and two challenging real-world datasets demonstrate that our approach outperforms the existing state-of-the-art methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "19244094",
                        "name": "Yangdi Lu"
                    },
                    {
                        "authorId": "2087100588",
                        "name": "Wenbo He"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It was shown that the mixup was beneficial to avoid overconfident predictions in several tasks such as image classification [19], [20], object detection [21], text classification [20] and semantic segmentation [22]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ae4fafe5416a068e0e61adee366cd71947ae7de7",
                "externalIds": {
                    "DBLP": "conf/icpr/Yazici0Y22",
                    "ArXiv": "2112.05485",
                    "DOI": "10.1109/ICPR56361.2022.9956585",
                    "CorpusId": 245117748
                },
                "corpusId": 245117748,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ae4fafe5416a068e0e61adee366cd71947ae7de7",
                "title": "Visual Transformers with Primal Object Queries for Multi-Label Image Classification",
                "abstract": "Multi-label image classification is about predicting a set of class labels that can be considered as orderless sequential data. Transformers process the sequential data as a whole, therefore they are inherently good at set prediction. The first vision-based transformer model, which was proposed for the object detection task introduced the concept of object queries. Object queries are learnable positional encodings that are used by attention modules in decoder layers to decode the object classes or bounding boxes using the region of interests in an image. However, inputting the same set of object queries to different decoder layers hinders the training: it results in lower performance and delays convergence. In this paper, we propose the usage of primal object queries that are only provided at the start of the transformer decoder stack. In addition, we improve the mixup technique proposed for multi-label classification. The proposed transformer model with primal object queries improves the state-of-the-art class wise F1 metric by 2.1% and 1.8%; and speeds up the convergence by 79.0% and 38.6% on MS-COCO and NUS-WIDE datasets respectively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "15035398",
                        "name": "V. O. Yazici"
                    },
                    {
                        "authorId": "2820687",
                        "name": "Joost van de Weijer"
                    },
                    {
                        "authorId": "9072783",
                        "name": "Longlong Yu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "daeb8a9063c788c3a190484cd79d7bba78c9a47c",
                "externalIds": {
                    "ArXiv": "2112.02902",
                    "DBLP": "conf/eccv/RymarczykSGLTZ22",
                    "DOI": "10.1007/978-3-031-19775-8_21",
                    "CorpusId": 244909006
                },
                "corpusId": 244909006,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/daeb8a9063c788c3a190484cd79d7bba78c9a47c",
                "title": "Interpretable Image Classification with Differentiable Prototypes Assignment",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "146543800",
                        "name": "Dawid Rymarczyk"
                    },
                    {
                        "authorId": "2096834",
                        "name": "Lukasz Struski"
                    },
                    {
                        "authorId": "2113967232",
                        "name": "Michal G'orszczak"
                    },
                    {
                        "authorId": "80045483",
                        "name": "K. Lewandowska"
                    },
                    {
                        "authorId": "145541197",
                        "name": "J. Tabor"
                    },
                    {
                        "authorId": "2064445681",
                        "name": "Bartosz Zieli'nski"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "38f8c2f6f0ade222240df87cda46033daed8e10a",
                "externalIds": {
                    "DBLP": "conf/eccv/DaiHG22",
                    "ArXiv": "2112.02450",
                    "DOI": "10.1007/978-3-031-19784-0_15",
                    "CorpusId": 250526103
                },
                "corpusId": 250526103,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/38f8c2f6f0ade222240df87cda46033daed8e10a",
                "title": "Adaptive Feature Interpolation for Low-Shot Image Generation",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2143823659",
                        "name": "M. Dai"
                    },
                    {
                        "authorId": "103011435",
                        "name": "Haibin Hang"
                    },
                    {
                        "authorId": "49932298",
                        "name": "Xiaoyang Guo"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "04cdc2bfd2a096e4b5346828b362257b8076063f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-01360",
                    "ArXiv": "2112.01360",
                    "DOI": "10.1109/TITS.2023.3268578",
                    "CorpusId": 244798740
                },
                "corpusId": 244798740,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/04cdc2bfd2a096e4b5346828b362257b8076063f",
                "title": "Probabilistic Approach for Road-Users Detection",
                "abstract": "Object detection in autonomous driving applications implies the detection and tracking of semantic objects that are commonly native to urban driving environments, as pedestrians and vehicles. One of the major challenges in state-of-the-art deep-learning based object detection are false positives which occur with overconfident scores. This is highly undesirable in autonomous driving and other critical robotic-perception domains because of safety concerns. This paper proposes an approach to alleviate the problem of overconfident predictions by introducing a novel probabilistic layer to deep object detection networks in testing. The suggested approach avoids the traditional Sigmoid or Softmax prediction layer which often produces overconfident predictions. It is demonstrated that the proposed technique reduces overconfidence in the false positives without degrading the performance on the true positives. The approach is validated on the 2D-KITTI objection detection through the YOLOV4 and SECOND (Lidar-based detector). The proposed approach enables interpretable probabilistic predictions without the requirement of re-training the network and therefore is very practical.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51918162",
                        "name": "Gledson Melotti"
                    },
                    {
                        "authorId": "2237109641",
                        "name": "Weihao Lu"
                    },
                    {
                        "authorId": "2206265023",
                        "name": "Pedro Conde"
                    },
                    {
                        "authorId": "2210035",
                        "name": "Dezong Zhao"
                    },
                    {
                        "authorId": "2389486",
                        "name": "A. Asvadi"
                    },
                    {
                        "authorId": "2105917986",
                        "name": "Nuno Gon\u00e7alves"
                    },
                    {
                        "authorId": "2097466",
                        "name": "C. Premebida"
                    }
                ]
            }
        },
        {
            "contexts": [
                "MixUp was shown to have two main effects: (1) it diversifies the training images and thus enlarges the training distribution on the vicinity of each training sample [6] and (2) it improves the network calibration [25, 57], reducing the overconfidence in recent classes."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b10c6201fec56772fa97bbcaf37b4ead61b6270a",
                "externalIds": {
                    "ArXiv": "2111.11326",
                    "DBLP": "conf/cvpr/DouillardRCC22",
                    "DOI": "10.1109/CVPR52688.2022.00907",
                    "CorpusId": 244478589
                },
                "corpusId": 244478589,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b10c6201fec56772fa97bbcaf37b4ead61b6270a",
                "title": "DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion",
                "abstract": "Deep network architectures struggle to continually learn new tasks without forgetting the previous tasks. A recent trend indicates that dynamic architectures based on an ex-pansion of the parameters can reduce catastrophic forget-ting efficiently in continual learning. However, existing approaches often require a task identifier at test-time, need complex tuning to balance the growing number of parameters, and barely share any information across tasks. As a result, they struggle to scale to a large number of tasks without significant overhead. In this paper, we propose a transformer architecture based on a dedicated encoder/decoder framework. Critically, the encoder and decoder are shared among all tasks. Through a dynamic expansion of special tokens, we specialize each forward of our decoder network on a task distribution. Our strategy scales to a large number of tasks while having neg-ligible memory and time overheads due to strict control of the expansion of the parameters. Moreover, this efficient strategy doesn't need any hyperparameter tuning to control the network's expansion. Our model reaches excellent results on CIFAR100 and state-of-the-art performances on the large-scale ImageNet100 and ImageNet100 while having fewer parameters than concurrent dynamic frameworks.11Code is released at https://github.com/arthurdouillard/dytox.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1660848177",
                        "name": "Arthur Douillard"
                    },
                    {
                        "authorId": "2141578751",
                        "name": "Alexandre Ram'e"
                    },
                    {
                        "authorId": "1637414390",
                        "name": "Guillaume Couairon"
                    },
                    {
                        "authorId": "51021910",
                        "name": "M. Cord"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This quantification has been observed to be inaccurate, and several methods have been developed to improve it (Platt, 1999; Guo et al., 2017; Szegedy et al., 2016; Zhang et al., 2020; Thulasidasan et al., 2020; Mukhoti et al., 2020; Thagaard et al., 2020), including Bayesian neural networks (Gal & Ghahramani, 2016; Wang et al.",
                "The first group smooths the target 0/1 labels in order to prevent output estimates from collapsing to 0/1 (Mukhoti et al., 2020; Szegedy et al., 2016; Zhang et al., 2018; Thulasidasan et al., 2020).",
                "\u2026to be inaccurate, and several methods have been developed to improve it (Platt, 1999; Guo et al., 2017; Szegedy et al., 2016; Zhang et al., 2020; Thulasidasan et al., 2020; Mukhoti et al., 2020; Thagaard et al., 2020), including Bayesian neural networks (Gal & Ghahramani, 2016; Wang et al.,\u2026"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0fe0c49acd73c59a41de2603e48ecd341fd59e51",
                "externalIds": {
                    "ArXiv": "2111.10734",
                    "DBLP": "conf/icml/LiuKZLMYHZRNF22",
                    "CorpusId": 244478199
                },
                "corpusId": 244478199,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0fe0c49acd73c59a41de2603e48ecd341fd59e51",
                "title": "Deep Probability Estimation",
                "abstract": "Reliable probability estimation is of crucial importance in many real-world applications where there is inherent (aleatoric) uncertainty. Probability-estimation models are trained on observed outcomes (e.g. whether it has rained or not, or whether a patient has died or not), because the ground-truth probabilities of the events of interest are typically unknown. The problem is therefore analogous to binary classification, with the difference that the objective is to estimate probabilities rather than predicting the specific outcome. This work investigates probability estimation from high-dimensional data using deep neural networks. There exist several methods to improve the probabilities generated by these models but they mostly focus on model (epistemic) uncertainty. For problems with inherent uncertainty, it is challenging to evaluate performance without access to ground-truth probabilities. To address this, we build a synthetic dataset to study and compare different computable metrics. We evaluate existing methods on the synthetic data as well as on three real-world probability estimation tasks, all of which involve inherent uncertainty: precipitation forecasting from radar images, predicting cancer patient survival from histopathology images, and predicting car crashes from dashcam videos. We also give a theoretical analysis of a model for high-dimensional probability estimation which reproduces several of the phenomena evinced in our experiments. Finally, we propose a new method for probability estimation using neural networks, which modifies the training process to promote output probabilities that are consistent with empirical probabilities computed from the data. The method outperforms existing approaches on most metrics on the simulated as well as real-world data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "94035244",
                        "name": "Sheng Liu"
                    },
                    {
                        "authorId": "1406369273",
                        "name": "Aakash Kaku"
                    },
                    {
                        "authorId": "48506952",
                        "name": "Weicheng Zhu"
                    },
                    {
                        "authorId": "4143589",
                        "name": "M. Leibovich"
                    },
                    {
                        "authorId": "9247167",
                        "name": "S. Mohan"
                    },
                    {
                        "authorId": "2110706638",
                        "name": "Boyang Yu"
                    },
                    {
                        "authorId": "103206782",
                        "name": "L. Zanna"
                    },
                    {
                        "authorId": "2678645",
                        "name": "N. Razavian"
                    },
                    {
                        "authorId": "1400418223",
                        "name": "C. Fernandez-Granda"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The calibration of a model can be measured by the expected calibration error (ECE) [17] and the overconfidence error (OE) [29]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "38d0121775dd360cdbe15010b93f3a42708c1319",
                "externalIds": {
                    "DBLP": "conf/ijcai/HintersdorfSK22",
                    "ArXiv": "2111.09076",
                    "DOI": "10.24963/ijcai.2022/422",
                    "CorpusId": 248476483
                },
                "corpusId": 248476483,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/38d0121775dd360cdbe15010b93f3a42708c1319",
                "title": "To Trust or Not To Trust Prediction Scores for Membership Inference Attacks",
                "abstract": "Membership inference attacks (MIAs) aim to determine whether a specific sample was used to train a predictive model. Knowing this may indeed lead to a privacy breach. Most MIAs, however, make use of the model's prediction scores - the probability of each output given some input - following the intuition that the trained model tends to behave differently on its training data. We argue that this is a fallacy for many modern deep network architectures. Consequently, MIAs will miserably fail since overconfidence leads to high false-positive rates not only on known domains but also on out-of-distribution data and implicitly acts as a defense against MIAs. Specifically, using generative adversarial networks, we are able to produce a potentially infinite number of samples falsely classified as part of the training data. In other words, the threat of MIAs is overestimated, and less information is leaked than previously assumed. Moreover, there is actually a trade-off between the overconfidence of models and their susceptibility to MIAs: the more classifiers know when they do not know, making low confidence predictions, the more they reveal the training data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2140396795",
                        "name": "Dominik Hintersdorf"
                    },
                    {
                        "authorId": "2140415179",
                        "name": "Lukas Struppek"
                    },
                    {
                        "authorId": "2066493115",
                        "name": "K. Kersting"
                    }
                ]
            }
        },
        {
            "contexts": [
                "to be overconfident when trained with hard labels [52], [53]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "eb25b162071c906d1ec190d7587ec3b917debdfb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-06971",
                    "ArXiv": "2111.06971",
                    "DOI": "10.1109/ACCESS.2023.3261884",
                    "CorpusId": 244117483
                },
                "corpusId": 244117483,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/eb25b162071c906d1ec190d7587ec3b917debdfb",
                "title": "Exploiting All Samples in Low-Resource Sentence Classification: Early Stopping and Initialization Parameters",
                "abstract": "To improve deep-learning performance in low-resource settings, many researchers have redesigned model architectures or applied additional data (e.g., external resources, unlabeled samples). However, there have been relatively few discussions on how to make good use of small amounts of labeled samples, although it is potentially beneficial and should be done before applying additional data or redesigning models. In this study, we assume a low-resource setting in which only a few labeled samples (i.e., 30\u2013100 per class) are available, and we discuss how to exploit them without additional data or model redesigns. We explore possible approaches in the following three aspects: training-validation splitting, early stopping, and weight initialization. Extensive experiments are conducted on six public sentence classification datasets. Performance on various evaluation metrics (e.g., accuracy, loss, and calibration error) significantly varied depending on the approaches that were combined in the three aspects. Based on the results, we propose an integrated method, which is to initialize the model with a weight averaging method and use a non-validation stop method to train all samples. This simple integrated method consistently outperforms the competitive methods; e.g., the average accuracy of six datasets of this method was 1.8% higher than those of conventional validation-based methods. In addition, the integrated method further improves the performance when adapted to several state-of-the-art models that use additional data or redesign the network architecture (e.g., self-training and enhanced structural models). Our results highlight the importance of the training strategy and suggest that the integrated method can be the first step in the low-resource setting. This study provides empirical knowledge that will be helpful when dealing with low-resource data in future efforts. Our code is publicly available at https://github.com/DMCB-GIST/exploit_all_samples.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2111527643",
                        "name": "Hongseok Choi"
                    },
                    {
                        "authorId": "49923640",
                        "name": "Hyunju Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "According to some recent machine learning studies, the uncertainty estimations of many widely used machine learning classifiers are not reliable [20; 12; 32; 36]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a822d93a8cd98cf47b69d9242f07acb1e14385c7",
                "externalIds": {
                    "ArXiv": "2111.04104",
                    "DBLP": "journals/corr/abs-2111-04104",
                    "CorpusId": 243847765
                },
                "corpusId": 243847765,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a822d93a8cd98cf47b69d9242f07acb1e14385c7",
                "title": "Uncertainty Calibration for Ensemble-Based Debiasing Methods",
                "abstract": "Ensemble-based debiasing methods have been shown effective in mitigating the reliance of classifiers on specific dataset bias, by exploiting the output of a bias-only model to adjust the learning target. In this paper, we focus on the bias-only model in these ensemble-based methods, which plays an important role but has not gained much attention in the existing literature. Theoretically, we prove that the debiasing performance can be damaged by inaccurate uncertainty estimations of the bias-only model. Empirically, we show that existing bias-only models fall short in producing accurate uncertainty estimations. Motivated by these findings, we propose to conduct calibration on the bias-only model, thus achieving a three-stage ensemble-based debiasing framework, including bias modeling, model calibrating, and debiasing. Experimental results on NLI and fact verification tasks show that our proposed three-stage debiasing framework consistently outperforms the traditional two-stage one in out-of-distribution accuracy.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51130380",
                        "name": "Ruibin Xiong"
                    },
                    {
                        "authorId": "98716335",
                        "name": "Yimeng Chen"
                    },
                    {
                        "authorId": "48537499",
                        "name": "Liang Pang"
                    },
                    {
                        "authorId": "2145447992",
                        "name": "Xueqi Chen"
                    },
                    {
                        "authorId": "37510256",
                        "name": "Yanyan Lan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "mixup is proven to be effective on balanced dataset due to its improvement of calibration [58, 18], but it is unsatisfactory in LT scenarios (see in Tab.",
                "Classification calibration [18, 58] represents the predicted winning Softmax scores indicate the actual likelihood of a correct prediction.",
                "[58] point out that the effectiveness of mixup in balanced datasets originates from superior calibration modification.",
                "[58] Sunil Thulasidasan, Gopinath Chennupati, Jeff A.",
                "However, the authors in [58] illustrate that such interpolation without labels is negative for classification calibration.",
                ", the predicted confidence indicates actual accuracy likelihood [18, 58].",
                "The confidence data is obtained by the average Softmax winning score in a test mini-batch [58].",
                "mixup [64] and its extensions [59, 63, 12] are effective feature improvement methods and contribute to a well-calibrated model in balanced datasets [58, 65], i."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "9d37ccb9fe16cc076be62313e85b754d37aa34c3",
                "externalIds": {
                    "DBLP": "conf/nips/XuCY21",
                    "ArXiv": "2111.03874",
                    "CorpusId": 243848094
                },
                "corpusId": 243848094,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9d37ccb9fe16cc076be62313e85b754d37aa34c3",
                "title": "Towards Calibrated Model for Long-Tailed Visual Recognition from Prior Perspective",
                "abstract": "Real-world data universally confronts a severe class-imbalance problem and exhibits a long-tailed distribution, i.e., most labels are associated with limited instances. The na\\\"ive models supervised by such datasets would prefer dominant labels, encounter a serious generalization challenge and become poorly calibrated. We propose two novel methods from the prior perspective to alleviate this dilemma. First, we deduce a balance-oriented data augmentation named Uniform Mixup (UniMix) to promote mixup in long-tailed scenarios, which adopts advanced mixing factor and sampler in favor of the minority. Second, motivated by the Bayesian theory, we figure out the Bayes Bias (Bayias), an inherent bias caused by the inconsistency of prior, and compensate it as a modification on standard cross-entropy loss. We further prove that both the proposed methods ensure the classification calibration theoretically and empirically. Extensive experiments verify that our strategies contribute to a better-calibrated model, and their combination achieves state-of-the-art performance on CIFAR-LT, ImageNet-LT, and iNaturalist 2018.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1390847590",
                        "name": "Zhengzhuo Xu"
                    },
                    {
                        "authorId": "150355978",
                        "name": "Zenghao Chai"
                    },
                    {
                        "authorId": "2117728946",
                        "name": "Chun Yuan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "10fb39c01176fe4758169ac01a6476101697d080",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-03135",
                    "ArXiv": "2111.03135",
                    "CorpusId": 243832624
                },
                "corpusId": 243832624,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/10fb39c01176fe4758169ac01a6476101697d080",
                "title": "Scaffolding Sets",
                "abstract": "Predictors map individual instances in a population to the interval $[0,1]$. For a collection $\\mathcal C$ of subsets of a population, a predictor is multi-calibrated with respect to $\\mathcal C$ if it is simultaneously calibrated on each set in $\\mathcal C$. We initiate the study of the construction of scaffolding sets, a small collection $\\mathcal S$ of sets with the property that multi-calibration with respect to $\\mathcal S$ ensures correctness, and not just calibration, of the predictor. Our approach is inspired by the folk wisdom that the intermediate layers of a neural net learn a highly structured and useful data representation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "22224047",
                        "name": "M. Burhanpurkar"
                    },
                    {
                        "authorId": "10394991",
                        "name": "Zhun Deng"
                    },
                    {
                        "authorId": "1781565",
                        "name": "C. Dwork"
                    },
                    {
                        "authorId": "10537441",
                        "name": "Linjun Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "771374d3368116fc0f8acba0955769401381d831",
                "externalIds": {
                    "ArXiv": "2110.15231",
                    "DBLP": "journals/corr/abs-2110-15231",
                    "CorpusId": 240070965
                },
                "corpusId": 240070965,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/771374d3368116fc0f8acba0955769401381d831",
                "title": "Exploring Covariate and Concept Shift for Detection and Calibration of Out-of-Distribution Data",
                "abstract": "Moving beyond testing on in-distribution data works on Out-of-Distribution (OOD) detection have recently increased in popularity. A recent attempt to categorize OOD data introduces the concept of near and far OOD detection. Specifically, prior works define characteristics of OOD data in terms of detection difficulty. We propose to characterize the spectrum of OOD data using two types of distribution shifts: covariate shift and concept shift, where covariate shift corresponds to change in style, e.g., noise, and concept shift indicates a change in semantics. This characterization reveals that sensitivity to each type of shift is important to the detection and confidence calibration of OOD data. Consequently, we investigate score functions that capture sensitivity to each type of dataset shift and methods that improve them. To this end, we theoretically derive two score functions for OOD detection, the covariate shift score and concept shift score, based on the decomposition of KL-divergence for both scores, and propose a geometrically-inspired method (Geometric ODIN) to improve OOD detection under both shifts with only in-distribution data. Additionally, the proposed method naturally leads to an expressive post-hoc calibration function which yields state-of-the-art calibration performance on both in-distribution and out-of-distribution data. We are the first to propose a method that works well across both OOD detection and calibration and under different types of shifts. View project page at https://sites.google.com/view/geometric-decomposition.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "11007025",
                        "name": "Junjiao Tian"
                    },
                    {
                        "authorId": "2147217995",
                        "name": "Yen-Change Hsu"
                    },
                    {
                        "authorId": "1785381533",
                        "name": "Yilin Shen"
                    },
                    {
                        "authorId": "1705713",
                        "name": "Hongxia Jin"
                    },
                    {
                        "authorId": "145276578",
                        "name": "Z. Kira"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Enhancement [58], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209]",
                "data augmentation [195], ensembling with leaving-out strategy [196], adversarial training [197], [198], [199], [200], [214], stronger data augmentation [201], [202], [203], [204],"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7b2180d7fa0d65e8756401cb077bf3dea3f9b575",
                "externalIds": {
                    "ArXiv": "2110.11334",
                    "DBLP": "journals/corr/abs-2110-11334",
                    "CorpusId": 239049401
                },
                "corpusId": 239049401,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7b2180d7fa0d65e8756401cb077bf3dea3f9b575",
                "title": "Generalized Out-of-Distribution Detection: A Survey",
                "abstract": "Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety of machine learning systems. For instance, in autonomous driving, we would like the driving system to issue an alert and hand over the control to humans when it detects unusual scenes or objects that it has never seen during training time and cannot make a safe decision. The term, OOD detection, first emerged in 2017 and since then has received increasing attention from the research community, leading to a plethora of methods developed, ranging from classification-based to density-based to distance-based ones. Meanwhile, several other problems, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD), are closely related to OOD detection in terms of motivation and methodology. Despite common goals, these topics develop in isolation, and their subtle differences in definition and problem setting often confuse readers and practitioners. In this survey, we first present a unified framework called generalized OOD detection, which encompasses the five aforementioned problems, i.e., AD, ND, OSR, OOD detection, and OD. Under our framework, these five problems can be seen as special cases or sub-tasks, and are easier to distinguish. We then review each of these five areas by summarizing their recent technical developments, with a special focus on OOD detection methodologies. We conclude this survey with open challenges and potential research directions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2295601",
                        "name": "Jingkang Yang"
                    },
                    {
                        "authorId": "9368124",
                        "name": "Kaiyang Zhou"
                    },
                    {
                        "authorId": "1527103472",
                        "name": "Yixuan Li"
                    },
                    {
                        "authorId": "2117940996",
                        "name": "Ziwei Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026technique has been proved to work very well for augmenting images and texts, where Mixup has been successfully deployed to mix two unrelated images (Zhang et al. 2018a; Thulasidasan et al. 2019) or two semantically different words in sentences (Guo, Mao, and Zhang 2019a; Guo 2020).",
                "Nevertheless, such \u201ccounter-intuition\u201d technique has been proved to work very well for augmenting images and texts, where Mixup has been successfully deployed to mix two unrelated images (Zhang et al. 2018a; Thulasidasan et al. 2019) or two semantically different words in sentences (Guo, Mao, and Zhang 2019a; Guo 2020)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "def2c67f039009752915913b3bfddf586dad11cc",
                "externalIds": {
                    "ArXiv": "2110.09344",
                    "DBLP": "conf/aaai/GuoM23",
                    "DOI": "10.1609/aaai.v37i6.25941",
                    "CorpusId": 254017928
                },
                "corpusId": 254017928,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/def2c67f039009752915913b3bfddf586dad11cc",
                "title": "Interpolating Graph Pair to Regularize Graph Classification",
                "abstract": "We present a simple and yet effective interpolation-based regularization technique, aiming to improve the generalization of Graph Neural Networks (GNNs) on supervised graph classification. We leverage Mixup, an effective regularizer for vision, where random sample pairs and their labels are interpolated to create synthetic images for training. Unlike images with grid-like coordinates, graphs have arbitrary structure and topology, which can be very sensitive to any modification that alters the graph's semantic meanings. This posts two unanswered questions for Mixup-like regularization schemes: Can we directly mix up a pair of graph inputs? If so, how well does such mixing strategy regularize the learning of GNNs? To answer these two questions, we propose ifMixup, which first adds dummy nodes to make two graphs have the same input size and then simultaneously performs linear interpolation between the aligned node feature vectors and the aligned edge representations of the two graphs. We empirically show that such simple mixing schema can effectively regularize the classification learning, resulting in superior predictive accuracy to popular graph augmentation and GNN methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1694050",
                        "name": "Hongyu Guo"
                    },
                    {
                        "authorId": "2047889",
                        "name": "Yongyi Mao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Training with these linear interpolations and the soft labels yields a smooth decision boundary in the embedding space which has shown to improve the calibration of the uncertainty estimates obtained from the outputs [33]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "48e09f69cfc379f66f7030b0c142fb9eb8284e1e",
                "externalIds": {
                    "DBLP": "conf/icmi/FoltynD21",
                    "DOI": "10.1145/3461615.3486570",
                    "CorpusId": 245265562
                },
                "corpusId": 245265562,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/48e09f69cfc379f66f7030b0c142fb9eb8284e1e",
                "title": "Towards Reliable Multimodal Stress Detection under Distribution Shift",
                "abstract": "The recognition of stress is an important issue from a health care perspective as well as in the human-computer interaction context. With the help of multimodal sensors, stress can be detected relatively well under laboratory conditions. However, when models are used in the real world, shifts in the data distribution can occur, often leading to performance degradation. It is therefore desirable that models in these scenarios are at least able to accurately capture this uncertainty and thus know what they do not know. This work aims to investigate how synthetic shifts in the data distribution can affect the reliability of a multimodal stress detection model in terms of calibration and uncertainty quantification. We compare a baseline with three known approaches that aim to improve reliability of uncertainty estimates. Our results show that all methods we tested improve the calibration. However, calibration generally deteriorates and spreads with stronger shifts for all approaches. They perform especially poorly for shifts in highly relevant modalities. Overall, we conclude that in the conducted experiments the investigated methods are not sufficiently reliable under distribution shifts.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2105729816",
                        "name": "Andreas Foltyn"
                    },
                    {
                        "authorId": "2028192005",
                        "name": "J. Deuschel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Indeed, they generally tend to be over-confident, particularly when employing softmax activation functions [Thulasidasan et al., 2019]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "24de93faaf0b7c0bc2563d53bbfe026015429a55",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-08265",
                    "ArXiv": "2110.08265",
                    "DOI": "10.1007/978-3-031-43412-9_3",
                    "CorpusId": 239015955
                },
                "corpusId": 239015955,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/24de93faaf0b7c0bc2563d53bbfe026015429a55",
                "title": "Knowledge-driven Active Learning",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "79277428",
                        "name": "Gabriele Ciravegna"
                    },
                    {
                        "authorId": "1699175",
                        "name": "F. Precioso"
                    },
                    {
                        "authorId": "145467467",
                        "name": "M. Gori"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The empirical advantages of Mixup training have been affirmed by several follow-up works (He et al., 2019; Thulasidasan et al., 2019; Lamb et al., 2019; Arazo et al., 2019; Guo, 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e19e9cf2aa468267f1f1f1075840d6b77f594c14",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-07647",
                    "ArXiv": "2110.07647",
                    "CorpusId": 239009938
                },
                "corpusId": 239009938,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e19e9cf2aa468267f1f1f1075840d6b77f594c14",
                "title": "Towards Understanding the Data Dependency of Mixup-style Training",
                "abstract": "In the Mixup training paradigm, a model is trained using convex combinations of data points and their associated labels. Despite seeing very few true data points during training, models trained using Mixup seem to still minimize the original empirical risk and exhibit better generalization and robustness on various tasks when compared to standard training. In this paper, we investigate how these benefits of Mixup training rely on properties of the data in the context of classification. For minimizing the original empirical risk, we compute a closed form for the Mixup-optimal classification, which allows us to construct a simple dataset on which minimizing the Mixup loss can provably lead to learning a classifier that does not minimize the empirical loss on the data. On the other hand, we also give sufficient conditions for Mixup training to also minimize the original empirical risk. For generalization, we characterize the margin of a Mixup classifier, and use this to understand why the decision boundary of a Mixup classifier can adapt better to the full structure of the training data when compared to standard training. In contrast, we also show that, for a large class of linear models and linearly separable datasets, Mixup training leads to learning the same classifier as standard training.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2065619709",
                        "name": "Muthuraman Chidambaram"
                    },
                    {
                        "authorId": "2144796430",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "2134348585",
                        "name": "Yuzheng Hu"
                    },
                    {
                        "authorId": "2277966",
                        "name": "Chenwei Wu"
                    },
                    {
                        "authorId": "144804200",
                        "name": "Rong Ge"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6834d852277c792da48e742e78fcd30082d3d9e7",
                "externalIds": {
                    "DBLP": "journals/fcomp/SomKLDAT21",
                    "DOI": "10.3389/fcomp.2021.728801",
                    "CorpusId": 238748880
                },
                "corpusId": 238748880,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6834d852277c792da48e742e78fcd30082d3d9e7",
                "title": "Automated Student Group Collaboration Assessment and Recommendation System Using Individual Role and Behavioral Cues",
                "abstract": "Early development of specific skills can help students succeed in fields like Science, Technology, Engineering and Mathematics. Different education standards consider \u201cCollaboration\u201d as a required and necessary skill that can help students excel in these fields. Instruction-based methods is the most common approach, adopted by teachers to instill collaborative skills. However, it is difficult for a single teacher to observe multiple student groups and provide constructive feedback to each student. With growing student population and limited teaching staff, this problem seems unlikely to go away. Development of machine-learning-based automated systems for student group collaboration assessment and feedback can help address this problem. Building upon our previous work, in this paper, we propose simple CNN deep-learning models that take in spatio-temporal representations of individual student roles and behavior annotations as input for group collaboration assessment. The trained classification models are further used to develop an automated recommendation system to provide individual-level or group-level feedback. The recommendation system suggests different roles each student in the group could have assumed that would facilitate better overall group collaboration. To the best of our knowledge, we are the first to develop such a feedback system. We also list the different challenges faced when working with the annotation data and describe the approaches we used to address those challenges.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1396461326",
                        "name": "Anirudh Som"
                    },
                    {
                        "authorId": "52162164",
                        "name": "Sujeong Kim"
                    },
                    {
                        "authorId": "1422583512",
                        "name": "Bladimir Lopez-Prado"
                    },
                    {
                        "authorId": "5896767",
                        "name": "Svati Dhamija"
                    },
                    {
                        "authorId": "8773292",
                        "name": "Nonye Alozie"
                    },
                    {
                        "authorId": "1860011",
                        "name": "Amir Tamrakar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mixup [1], which generates new examples by combining random data pairs and their labels, has shown promising performance on model generalization [3] and calibration [4]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ba12047fd669f349fab9f698add743852a8138d2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-04964",
                    "ArXiv": "2110.04964",
                    "DOI": "10.1109/icassp43922.2022.9746299",
                    "CorpusId": 238583636
                },
                "corpusId": 238583636,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/ba12047fd669f349fab9f698add743852a8138d2",
                "title": "Label-Occurrence-Balanced Mixup for Long-Tailed Recognition",
                "abstract": "Mixup is a popular data augmentation method, with many variants subsequently proposed. These methods mainly create new examples via convex combination of random data pairs and their corresponding one-hot labels. However, most of them adhere to a random sampling and mixing strategy, without considering the frequency of label occurrence in the mixing process. When applying mixup to long-tailed data, a label suppression issue arises, where the frequency of label occurrence for each class is imbalanced and most of the new examples will be completely or partially assigned with head labels. The suppression effect may further aggravate the problem of data imbalance and lead to a poor performance on tail classes. To address this problem, we propose Label-Occurrence-Balanced Mixup to augment data while keeping the label occurrence for each class statistically balanced. In a word, we employ two independent class-balanced samplers to select data pairs and mix them to generate new data. We test our method on several long-tailed vision and sound recognition benchmarks. Experimental results show that our method significantly promotes the adaptability of mixup method to imbalanced data and achieves superior performance compared with state-of-the-art long-tailed learning methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116580033",
                        "name": "Shaoyu Zhang"
                    },
                    {
                        "authorId": "2127380530",
                        "name": "Chen Chen"
                    },
                    {
                        "authorId": "2141813642",
                        "name": "Xiujuan Zhang"
                    },
                    {
                        "authorId": "1711600",
                        "name": "Silong Peng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The method has been successfully used in many supervised learning and semi-supervised learning tasks [6, 10, 14, 16, 18, 26, 62, 69]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "940472ee689867e63dca0e623aa18a4a2a3f3bd7",
                "externalIds": {
                    "DBLP": "conf/uist/WangHZMBHJ21",
                    "DOI": "10.1145/3472749.3474743",
                    "CorpusId": 238638811
                },
                "corpusId": 238638811,
                "publicationVenue": {
                    "id": "c62b1316-0733-4b4c-8017-c07e18afa954",
                    "name": "ACM Symposium on User Interface Software and Technology",
                    "type": "conference",
                    "alternate_names": [
                        "User Interface Software and Technology",
                        "ACM Symp User Interface Softw Technol",
                        "User Interface Softw Technol",
                        "UIST"
                    ],
                    "url": "http://www.acm.org/uist/"
                },
                "url": "https://www.semanticscholar.org/paper/940472ee689867e63dca0e623aa18a4a2a3f3bd7",
                "title": "Taming fNIRS-based BCI Input for Better Calibration and Broader Use",
                "abstract": "Brain-computer interfaces (BCI) are an emerging technology with many potential applications. Functional near-infrared spectroscopy (fNIRS) can provide a convenient and unobtrusive real time input for BCI. fNIRS is especially promising as a signal that could be used to automatically classify a user\u2019s current cognitive workload. However, the data needed to train such a classifier is currently not widely available, difficult to collect, and difficult to interpret due to noise and cross-subject variation. A further challenge is the need for significant user-specific calibration. To address these issues, we introduce a new dataset gathered from 15 subjects and a new multi-stage supervised machine learning pipeline. Our approach learns from both observed data and augmented data derived from multiple subjects in its early stages, and then fine-tunes predictions to an individual subject in its last stage. We show promising gains in accuracy in a standard \u201cn-back\u201d cognitive workload classification task compared to baselines that use only subject-specific data or only group-level data, even when our approach is given much less subject-specific data. Even though these experiments analyzed the data retrospectively, we carefully removed anything from our process that could not have been done in real time, because our process is targeted at future real-time operation. This paper contributes a new dataset, a new multi-stage training pipeline, results showing significant improvement compared to alternative pipelines, and discussion of the implications for user interface design. Our complete dataset and software are publicly available at https://tufts-hci-lab.github.io/code_and_datasets/. We hope these results make fNIRS-based interactive brain input easier for a wide range of future researchers and designers to explore.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2144697927",
                        "name": "Liang Wang"
                    },
                    {
                        "authorId": "2151325913",
                        "name": "Zhe Huang"
                    },
                    {
                        "authorId": "2135944192",
                        "name": "Ziyu Zhou"
                    },
                    {
                        "authorId": "2132094287",
                        "name": "Devon McKeon"
                    },
                    {
                        "authorId": "51904482",
                        "name": "Giles Blaney"
                    },
                    {
                        "authorId": "2067787391",
                        "name": "M. C. Hughes"
                    },
                    {
                        "authorId": "1723792",
                        "name": "R. Jacob"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "22ae58456f30cfdd608d7e55c1d66de258700d17",
                "externalIds": {
                    "DBLP": "journals/jaiscr/IsakssonSRGBMPP22",
                    "DOI": "10.2478/jaiscr-2022-0003",
                    "CorpusId": 238530335
                },
                "corpusId": 238530335,
                "publicationVenue": {
                    "id": "a320fa1c-fe85-4d55-8e1e-10b44181cb8c",
                    "name": "Journal of Artificial Intelligence and Soft Computing Research",
                    "type": "journal",
                    "alternate_names": [
                        "J Artif Intell Soft Comput Res"
                    ],
                    "issn": "2083-2567",
                    "url": "http://www.degruyter.com/view/j/jaiscr",
                    "alternate_urls": [
                        "http://jaiscr.eu/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/22ae58456f30cfdd608d7e55c1d66de258700d17",
                "title": "Mixup (Sample Pairing) Can Improve the Performance of Deep Segmentation Networks",
                "abstract": "Abstract Researchers address the generalization problem of deep image processing networks mainly through extensive use of data augmentation techniques such as random flips, rotations, and deformations. A data augmentation technique called mixup, which constructs virtual training samples from convex combinations of inputs, was recently proposed for deep classification networks. The algorithm contributed to increased performance on classification in a variety of datasets, but so far has not been evaluated for image segmentation tasks. In this paper, we tested whether the mixup algorithm can improve the generalization performance of deep segmentation networks for medical image data. We trained a standard U-net architecture to segment the prostate in 100 T2-weighted 3D magnetic resonance images from prostate cancer patients, and compared the results with and without mixup in terms of Dice similarity coefficient and mean surface distance from a reference segmentation made by an experienced radiologist. Our results suggest that mixup offers a statistically significant boost in performance compared to non-mixup training, leading to up to 1.9% increase in Dice and a 10.9% decrease in surface distance. The mixup algorithm may thus offer an important aid for medical image segmentation applications, which are typically limited by severe data scarcity.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1500682272",
                        "name": "L. Isaksson"
                    },
                    {
                        "authorId": "2689154",
                        "name": "P. Summers"
                    },
                    {
                        "authorId": "49160901",
                        "name": "S. Raimondi"
                    },
                    {
                        "authorId": "145463217",
                        "name": "S. Gandini"
                    },
                    {
                        "authorId": "145901092",
                        "name": "A. Bhalerao"
                    },
                    {
                        "authorId": "3604404",
                        "name": "G. Marvaso"
                    },
                    {
                        "authorId": "1855293",
                        "name": "G. Petralia"
                    },
                    {
                        "authorId": "1500677692",
                        "name": "M. Pepa"
                    },
                    {
                        "authorId": "1397677067",
                        "name": "B. Jereczek-Fossa"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0e7a4b08979122cc23040d35c887f6f201c9de2f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-10077",
                    "ArXiv": "2110.10077",
                    "DOI": "10.1016/j.advwatres.2022.104272",
                    "CorpusId": 239024817
                },
                "corpusId": 239024817,
                "publicationVenue": {
                    "id": "69521336-de5e-4067-9be1-4f404a665d11",
                    "name": "Advances in Water Resources",
                    "type": "journal",
                    "alternate_names": [
                        "Adv Water Resour"
                    ],
                    "issn": "0309-1708",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/422913/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/advances-in-water-resources",
                        "http://www.sciencedirect.com/science/journal/03091708"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0e7a4b08979122cc23040d35c887f6f201c9de2f",
                "title": "Deep Learning to Estimate Permeability using Geophysical Data",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2991880",
                        "name": "M. Mudunuru"
                    },
                    {
                        "authorId": "35205045",
                        "name": "E. Cromwell"
                    },
                    {
                        "authorId": "1390775273",
                        "name": "H. Wang"
                    },
                    {
                        "authorId": "1478367868",
                        "name": "X. Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several works [6], [11], [16], [17], [18], [19], [20], [21] consider either algorithmic improvements or application specific challenges associated to uncertainty quantification."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "064082b263edd0f8470687a672a24a1afca5931b",
                "externalIds": {
                    "ArXiv": "2110.02459",
                    "DBLP": "journals/corr/abs-2110-02459",
                    "CorpusId": 238408286
                },
                "corpusId": 238408286,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/064082b263edd0f8470687a672a24a1afca5931b",
                "title": "Post-hoc Models for Performance Estimation of Machine Learning Inference",
                "abstract": "Estimating how well a machine learning model performs during inference is critical in a variety of scenarios (for example, to quantify uncertainty, or to choose from a library of available models). However, the standard accuracy estimate of softmax confidence is not versatile and cannot reliably predict different performance metrics (e.g., F1-score, recall) or the performance in different application scenarios or input domains. In this work, we systematically generalize performance estimation to a diverse set of metrics and scenarios and discuss generalized notions of uncertainty calibration. We propose the use of post-hoc models to accomplish this goal and investigate design parameters, including the model type, feature engineering, and performance metric, to achieve the best estimation quality. Emphasis is given to object detection problems and, unlike prior work, our approach enables the estimation of per-image metrics such as recall and F1-score. Through extensive experiments with computer vision models and datasets in three use cases -- mobile edge offloading, model selection, and dataset shift -- we find that proposed post-hoc models consistently outperform the standard calibrated confidence baselines. To the best of our knowledge, this is the first work to develop a unified framework to address different performance estimation problems for machine learning inference.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108029216",
                        "name": "Xuechen Zhang"
                    },
                    {
                        "authorId": "3103394",
                        "name": "Samet Oymak"
                    },
                    {
                        "authorId": "1391202254",
                        "name": "Jiasi Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Further, it will be interesting to study whether NFM may also lead to better model calibration by extending the analysis of [66, 83].",
                "[66] Sunil Thulasidasan, Gopinath Chennupati, Jeff Bilmes, Tanmoy Bhattacharya, and Sarah Michalak."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a1653c7f5a3e136b382475d9ff82a80ef24262cf",
                "externalIds": {
                    "ArXiv": "2110.02180",
                    "DBLP": "journals/corr/abs-2110-02180",
                    "CorpusId": 238354231
                },
                "corpusId": 238354231,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a1653c7f5a3e136b382475d9ff82a80ef24262cf",
                "title": "Noisy Feature Mixup",
                "abstract": "We introduce Noisy Feature Mixup (NFM), an inexpensive yet effective method for data augmentation that combines the best of interpolation based training and noise injection schemes. Rather than training with convex combinations of pairs of examples and their labels, we use noise-perturbed convex combinations of pairs of data points in both input and feature space. This method includes mixup and manifold mixup as special cases, but it has additional advantages, including better smoothing of decision boundaries and enabling improved model robustness. We provide theory to understand this as well as the implicit regularization effects of NFM. Our theory is supported by empirical results, demonstrating the advantage of NFM, as compared to mixup and manifold mixup. We show that residual networks and vision transformers trained with NFM have favorable trade-offs between predictive accuracy on clean data and robustness with respect to various types of data perturbation across a range of computer vision benchmark datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "98291940",
                        "name": "S. H. Lim"
                    },
                    {
                        "authorId": "2371914",
                        "name": "N. Benjamin Erichson"
                    },
                    {
                        "authorId": "2061027899",
                        "name": "Francisco Utrera"
                    },
                    {
                        "authorId": "2110536989",
                        "name": "Winnie Xu"
                    },
                    {
                        "authorId": "1717098",
                        "name": "Michael W. Mahoney"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "25770a3bc24ac979c318fe0d489afd52fbb9b50a",
                "externalIds": {
                    "MAG": "3205540135",
                    "PubMedCentral": "8755769",
                    "DOI": "10.1038/s41598-021-04529-5",
                    "CorpusId": 244634087,
                    "PubMed": "35022467"
                },
                "corpusId": 244634087,
                "publicationVenue": {
                    "id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                    "name": "Scientific Reports",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Rep"
                    ],
                    "issn": "2045-2322",
                    "url": "http://www.nature.com/srep/",
                    "alternate_urls": [
                        "http://www.nature.com/srep/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/25770a3bc24ac979c318fe0d489afd52fbb9b50a",
                "title": "Training calibration-based counterfactual explainers for deep learning models in medical image analysis",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1744175",
                        "name": "Jayaraman J. Thiagarajan"
                    },
                    {
                        "authorId": "2123725394",
                        "name": "Kowshik Thopalli"
                    },
                    {
                        "authorId": "145882781",
                        "name": "Deepta Rajan"
                    },
                    {
                        "authorId": "143655174",
                        "name": "P. Turaga"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Since the pioneering work of [8] and [5], introduced to improve the generation accuracy, recently another family of methods include using soft labels during training for improving the uncertainty estimation of neural networks [6, 9, 27].",
                "tion [6, 9] which are employed during training.",
                "In this article, we identify that the over-confidence in deep radar classifiers, which emanates from using hard labels, can be fixed using soft labels [6, 7, 8, 9] and propose two novel heuristics to compute sample-specific smoothing factors to refine the hard labels."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "44b690cabd8c2ea5218500ea152a7faeed1991bf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-12851",
                    "ArXiv": "2109.12851",
                    "DOI": "10.1109/RadarConf2248738.2022.9764233",
                    "CorpusId": 237940739
                },
                "corpusId": 237940739,
                "publicationVenue": {
                    "id": "4e08736c-dbd0-4931-b4f0-84c9f7f66837",
                    "name": "International Radar Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Int Radar Conf",
                        "IEEE Radar Conference",
                        "RadarCon",
                        "IEEE Radar Conf"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/44b690cabd8c2ea5218500ea152a7faeed1991bf",
                "title": "Improving Uncertainty of Deep Learning-based Object Classification on Radar Spectra using Label Smoothing",
                "abstract": "Object type classification for automotive radar has greatly improved with recent deep learning (DL) solutions, however these developments have mostly focused on the clas-sification accuracy. Before employing DL solutions in safety-critical applications, such as automated driving, an indispensable prerequisite is the accurate quantification of the classifiers' reliability. Unfortunately, DL classifiers are characterized as black-box systems which output severely over-confident pre-dictions, leading downstream decision-making systems to false conclusions with possibly catastrophic consequences. We find that deep radar classifiers maintain high-confidences for ambiguous, difficult samples, e.g. small objects measured at large distances, under domain shift and signal corruptions, regardless of the correctness of the predictions. The focus of this article is to learn deep radar spectra classifiers which offer robust real-time uncertainty estimates using label smoothing during training. Label smoothing is a technique of refining, or softening, the hard labels typically available in classification datasets. In this article, we exploit radar-specific know-how to define soft labels which encourage the classifiers to learn to output high-quality calibrated uncertainty estimates, thereby partially resolving the problem of over-confidence. Our investigations show how simple radar knowledge can easily be combined with complex data-driven learning algorithms to yield safe automotive radar perception.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51290549",
                        "name": "Kanil Patel"
                    },
                    {
                        "authorId": "52020792",
                        "name": "William H. Beluch"
                    },
                    {
                        "authorId": "2451538",
                        "name": "K. Rambach"
                    },
                    {
                        "authorId": "144578436",
                        "name": "Michael Pfeiffer"
                    },
                    {
                        "authorId": "37606919",
                        "name": "B. Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Despite its simplicity, it has been shown that optimizing f\u03b8 on mixed-up data leads to better generalization and improves model calibration [26]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8d44510312f6dbee5d2062ec64233a9adc80c346",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-09850",
                    "ArXiv": "2109.09850",
                    "DOI": "10.1007/978-3-030-87240-3_31",
                    "CorpusId": 237581109
                },
                "corpusId": 237581109,
                "publicationVenue": {
                    "id": "61a709e3-2060-423c-8de5-ffd3885aa31c",
                    "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                    "type": "conference",
                    "alternate_names": [
                        "Medical Image Computing and Computer-Assisted Intervention",
                        "MICCAI",
                        "Med Image Comput Comput Interv",
                        "Int Conf Med Image Comput Comput Interv"
                    ],
                    "url": "http://www.miccai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/8d44510312f6dbee5d2062ec64233a9adc80c346",
                "title": "Balanced-MixUp for Highly Imbalanced Medical Image Classification",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2161526",
                        "name": "A. Galdran"
                    },
                    {
                        "authorId": "145575177",
                        "name": "G. Carneiro"
                    },
                    {
                        "authorId": "49463953",
                        "name": "M. Ballester"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9adf2e712895d853194eeb0bd382edb59a9806e5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-03216",
                    "ArXiv": "2109.03216",
                    "DOI": "10.1109/ICCV48922.2021.00076",
                    "CorpusId": 237431056
                },
                "corpusId": 237431056,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/9adf2e712895d853194eeb0bd382edb59a9806e5",
                "title": "Learning Fast Sample Re-weighting Without Reward Data",
                "abstract": "Training sample re-weighting is an effective approach for tackling data biases such as imbalanced and corrupted labels. Recent methods develop learning-based algorithms to learn sample re-weighting strategies jointly with model training based on the frameworks of reinforcement learning and meta learning. However, depending on additional unbiased reward data is limiting their general applicability. Furthermore, existing learning-based sample re-weighting methods require nested optimizations of models and weighting parameters, which requires expensive second-order computation. This paper addresses these two problems and presents a novel learning-based fast sample re-weighting (FSR) method that does not require additional reward data. The method is based on two key ideas: learning from history to build proxy reward data and feature sharing to reduce the optimization cost. Our experiments show the proposed method achieves competitive results compared to state of the arts on label noise robustness and long-tailed recognition, and does so while achieving significantly improved training efficiency. The source code is publicly available at https://github.com/google-research/google-research/tree/master/ieg.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2128158461",
                        "name": "Zizhao Zhang"
                    },
                    {
                        "authorId": "1945962",
                        "name": "Tomas Pfister"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Unlike OOD generalization tasks, which primarily focus on performance under distribution shifts, the OOD detection community [211, 152, 98, 244] concentrates more on detecting samples from unseen distributions."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e5b2e2a284db5ba7c2c011daba9769d2c56b6586",
                "externalIds": {
                    "ArXiv": "2108.13624",
                    "DBLP": "journals/corr/abs-2108-13624",
                    "CorpusId": 237364121
                },
                "corpusId": 237364121,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e5b2e2a284db5ba7c2c011daba9769d2c56b6586",
                "title": "Towards Out-Of-Distribution Generalization: A Survey",
                "abstract": "Traditional machine learning paradigms are based on the assumption that both training and test data follow the same statistical pattern, which is mathematically referred to as Independent and Identically Distributed ($i.i.d.$). However, in real-world applications, this $i.i.d.$ assumption often fails to hold due to unforeseen distributional shifts, leading to considerable degradation in model performance upon deployment. This observed discrepancy indicates the significance of investigating the Out-of-Distribution (OOD) generalization problem. OOD generalization is an emerging topic of machine learning research that focuses on complex scenarios wherein the distributions of the test data differ from those of the training data. This paper represents the first comprehensive, systematic review of OOD generalization, encompassing a spectrum of aspects from problem definition, methodological development, and evaluation procedures, to the implications and future directions of the field. Our discussion begins with a precise, formal characterization of the OOD generalization problem. Following that, we categorize existing methodologies into three segments: unsupervised representation learning, supervised model learning, and optimization, according to their positions within the overarching learning process. We provide an in-depth discussion on representative methodologies for each category, further elucidating the theoretical links between them. Subsequently, we outline the prevailing benchmark datasets employed in OOD generalization studies. To conclude, we overview the existing body of work in this domain and suggest potential avenues for future research on OOD generalization. A summary of the OOD generalization methodologies surveyed in this paper can be accessed at http://out-of-distribution-generalization.com.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "24069072",
                        "name": "Zheyan Shen"
                    },
                    {
                        "authorId": "2120846500",
                        "name": "Jiashuo Liu"
                    },
                    {
                        "authorId": "2145031333",
                        "name": "Yue He"
                    },
                    {
                        "authorId": "51258901",
                        "name": "Xingxuan Zhang"
                    },
                    {
                        "authorId": "150287491",
                        "name": "Renzhe Xu"
                    },
                    {
                        "authorId": "2187083103",
                        "name": "Han Yu"
                    },
                    {
                        "authorId": "143738684",
                        "name": "Peng Cui"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As label mixing is a form of label smoothing [17], label mixup is beneficial to classification tasks and has achieved both better generalization and increased model calibration in classification applications across various data domains such as images, audio, text, and tabular data [16, 18].",
                "Similar to label mixup, the success of loss mixup is highly sensitive to the mixing parameter \u03bb [19, 18, 20]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "58b0962b0e632b8e74089018de7585565aab9cd4",
                "externalIds": {
                    "MAG": "3197269694",
                    "DBLP": "conf/interspeech/ChangTK21",
                    "DOI": "10.21437/interspeech.2021-859",
                    "CorpusId": 239624951
                },
                "corpusId": 239624951,
                "publicationVenue": {
                    "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
                    "name": "Interspeech",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Int Speech Commun Assoc",
                        "INTERSPEECH",
                        "Conference of the International Speech Communication Association"
                    ],
                    "issn": "2308-457X",
                    "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
                    "alternate_urls": [
                        "http://www.isca-speech.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/58b0962b0e632b8e74089018de7585565aab9cd4",
                "title": "Single-Channel Speech Enhancement Using Learnable Loss Mixup",
                "abstract": "Generalization remains a major problem in supervised learning of single-channel speech enhancement. In this work, we propose learnable loss mixup (LLM) , a simple and effort-less training diagram, to improve the generalization of deep learning-based speech enhancement models. Loss mixup , of which learnable loss mixup is a special variant, optimizes a mixture of the loss functions of random sample pairs to train a model on virtual training data constructed from these pairs of samples. In learnable loss mixup , by conditioning on the mixed data, the loss functions are mixed using a non-linear mixing function automatically learned via neural parameterization. Our experimental results on the VCTK benchmark show that learnable loss mixup achieves 3.26 PESQ, outperforming the state-of-the-art.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2134532944",
                        "name": "Oscar Chang"
                    },
                    {
                        "authorId": "3275309",
                        "name": "D. Tran"
                    },
                    {
                        "authorId": "145733034",
                        "name": "K. Koishida"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[32] proposed to use mix-up augmentation to improve the model calibration."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "412cd438f8a0371763774341814e3b344c9b82e7",
                "externalIds": {
                    "DBLP": "journals/cbm/GarifullinLU21",
                    "DOI": "10.1016/j.compbiomed.2021.104725",
                    "CorpusId": 237147782,
                    "PubMed": "34399196"
                },
                "corpusId": 237147782,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/412cd438f8a0371763774341814e3b344c9b82e7",
                "title": "Deep Bayesian baseline for segmenting diabetic retinopathy lesions: Advances and challenges",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144134078",
                        "name": "A. Garifullin"
                    },
                    {
                        "authorId": "1688468",
                        "name": "L. Lensu"
                    },
                    {
                        "authorId": "2088892131",
                        "name": "Hannu Uusitalo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In particular, label smoothing modifies the ground-truth labels by fusing them with a uniform distribution, essentially forcing neural networks to produce \u2018more flattened\u2019 probabilities; whereas Mixup is a data augmentation method that randomly mixes two instances at both the image and label space, with a byproduct effect of improving calibration.",
                "Regularization methods, such as label smoothing [36] and Mixup [37], have also been demonstrated effective in improving calibration.",
                "Baselines We compare our approach with nine baseline methods: MC-Dropout [7], Temperature Scaling [14], Mixup [37], Label Smoothing [36], TrustScore [20], JEM [12], DBLE [40], and OvA DM [34]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "87f93aa1228758ae1a71d35ba634577ff5bf89a7",
                "externalIds": {
                    "DBLP": "conf/iccv/Wang0CZ0L21",
                    "ArXiv": "2107.12628",
                    "DOI": "10.1109/ICCV48922.2021.00917",
                    "CorpusId": 236447335
                },
                "corpusId": 236447335,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/87f93aa1228758ae1a71d35ba634577ff5bf89a7",
                "title": "Energy-Based Open-World Uncertainty Modeling for Confidence Calibration",
                "abstract": "Confidence calibration is of great importance to the reliability of decisions made by machine learning systems. However, discriminative classifiers based on deep neural networks are often criticized for producing overconfident predictions that fail to reflect the true correctness likelihood of classification accuracy. We argue that such an inability to model uncertainty is mainly caused by the closed-world nature in softmax: a model trained by the cross-entropy loss will be forced to classify input into one of K pre-defined categories with high probability. To address this problem, we for the first time propose a novel K+1-way softmax formulation, which incorporates the modeling of open-world uncertainty as the extra dimension. To unify the learning of the original K-way classification task and the extra dimension that models uncertainty, we 1) propose a novel energy-based objective function, and moreover, 2) theoretically prove that optimizing such an objective essentially forces the extra dimension to capture the marginal data distribution. Extensive experiments show that our approach, Energy-based Open-World Softmax (EOW-Softmax), is superior to existing state-of-the-art methods in improving confidence calibration.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115738764",
                        "name": "Yezhen Wang"
                    },
                    {
                        "authorId": "71788673",
                        "name": "Bo Li"
                    },
                    {
                        "authorId": "47828117",
                        "name": "Tong Che"
                    },
                    {
                        "authorId": "9368124",
                        "name": "Kaiyang Zhou"
                    },
                    {
                        "authorId": "2117940996",
                        "name": "Ziwei Liu"
                    },
                    {
                        "authorId": "2119081394",
                        "name": "Dongsheng Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These include: consistency regularization of self-supervised learning [19] and the MixUp data augmentation algorithm [20], both implemented in the MixMatch approach."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fb6adc2c77afa15e0fbeac4ae495fee44928eb1d",
                "externalIds": {
                    "DBLP": "conf/ijcnn/RamirezMRCYMELM21",
                    "DOI": "10.1109/IJCNN52387.2021.9533719",
                    "CorpusId": 237599114
                },
                "corpusId": 237599114,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/fb6adc2c77afa15e0fbeac4ae495fee44928eb1d",
                "title": "Improving Uncertainty Estimations for Mammogram Classification using Semi-Supervised Learning",
                "abstract": "Computer aided diagnosis for mammogram images have seen positive results through the usage of deep learning architectures. However, limited sample sizes for the target datasets might prevent the usage of a deep learning model under real world scenarios. The usage of unlabeled data to improve the accuracy of the model can be an approach to tackle the lack of target data. Moreover, important model attributes for the medical domain as model uncertainty might be improved through the usage of unlabeled data. Therefore, in this work we explore the impact of using unlabeled data through the implementation of a recent approach known as MixMatch, for mammogram images. We evaluate the improvement on accuracy and uncertainty of the model using popular and simple approaches to estimate uncertainty. For this aim, we propose the usage of the uncertainty balanced accuracy metric.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1387523092",
                        "name": "Saul Calderon-Ramirez"
                    },
                    {
                        "authorId": "2121211586",
                        "name": "Diego Murillo-Hernandez"
                    },
                    {
                        "authorId": "2121185587",
                        "name": "Kevin Rojas-Salazar"
                    },
                    {
                        "authorId": "2127762224",
                        "name": "Luis-Alexander Calvo-Valverd"
                    },
                    {
                        "authorId": "2144996381",
                        "name": "Shengxiang Yang"
                    },
                    {
                        "authorId": "2511274",
                        "name": "Armaghan Moemeni"
                    },
                    {
                        "authorId": "2064680231",
                        "name": "D. Elizondo"
                    },
                    {
                        "authorId": "123170618",
                        "name": "Ezequiel L\u00f3pez-Rubio"
                    },
                    {
                        "authorId": "1398215310",
                        "name": "Miguel A. Molina-Cabello"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "37caf49db601f8017a910a1b5215d103316f2f42",
                "externalIds": {
                    "DBLP": "conf/kdd/HuK21",
                    "ArXiv": "2107.07114",
                    "DOI": "10.1145/3447548.3467382",
                    "CorpusId": 235899427
                },
                "corpusId": 235899427,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/37caf49db601f8017a910a1b5215d103316f2f42",
                "title": "Uncertainty-Aware Reliable Text Classification",
                "abstract": "Deep neural networks have significantly contributed to the success in predictive accuracy for classification tasks. However, they tend to make over-confident predictions in real-world settings, where domain shifting and out-of-distribution (OOD) examples exist. Most research on uncertainty estimation focuses on computer vision because it provides visual validation on uncertainty quality. However, few have been presented in the natural language process domain. Unlike Bayesian methods that indirectly infer uncertainty through weight uncertainties, current evidential uncertainty-based methods explicitly model the uncertainty of class probabilities through subjective opinions. They further consider inherent uncertainty in data with different root causes, vacuity (i.e., uncertainty due to a lack of evidence) and dissonance (i.e., uncertainty due to conflicting evidence). In our paper, we firstly apply evidential uncertainty in OOD detection for text classification tasks. We propose an inexpensive framework that adopts both auxiliary outliers and pseudo off-manifold samples to train the model with prior knowledge of a certain class, which has high vacuity for OOD samples. Extensive empirical experiments demonstrate that our model based on evidential uncertainty outperforms other counterparts for detecting OOD examples. Our approach can be easily deployed to traditional recurrent neural networks and fine-tuned pre-trained transformers.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49995036",
                        "name": "Yibo Hu"
                    },
                    {
                        "authorId": "145155297",
                        "name": "L. Khan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[278] regularized it by using a data-agnostic data augmentation technique named",
                "According to [278], the label smoothing resulting from mixup training can be viewed as a form of entropy-based regularization resulting in inherent calibration of networks trained",
                "binning based calibration measure [283], [15], [274], [275], [278], [47].",
                "1 [48] 2 [47] 3 [276] 4[15], [68], [274] 5 [31], [275] 6 [272], [209], [273], [274], [16] 8 [277] 9 [268], [270] 10 [278], [279], [234], [280] 11 [269], [266], [11], [271], [279]"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "fc70db46738fff97d9ee3d66c6f9c57794d7b4fa",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-03342",
                    "ArXiv": "2107.03342",
                    "DOI": "10.1007/s10462-023-10562-9",
                    "CorpusId": 235755082
                },
                "corpusId": 235755082,
                "publicationVenue": {
                    "id": "ea8553fe-2467-4367-afee-c4deb3754820",
                    "name": "Artificial Intelligence Review",
                    "type": "journal",
                    "alternate_names": [
                        "Artif Intell Rev"
                    ],
                    "issn": "0269-2821",
                    "url": "https://link.springer.com/journal/10462"
                },
                "url": "https://www.semanticscholar.org/paper/fc70db46738fff97d9ee3d66c6f9c57794d7b4fa",
                "title": "A Survey of Uncertainty in Deep Neural Networks",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2051292610",
                        "name": "J. Gawlikowski"
                    },
                    {
                        "authorId": "1486494981",
                        "name": "Cedrique Rovile Njieutcheu Tassi"
                    },
                    {
                        "authorId": "2051285705",
                        "name": "Mohsin Ali"
                    },
                    {
                        "authorId": "2329051",
                        "name": "Jongseo Lee"
                    },
                    {
                        "authorId": "1753619041",
                        "name": "Matthias Humt"
                    },
                    {
                        "authorId": "2118032380",
                        "name": "Jianxiang Feng"
                    },
                    {
                        "authorId": "2124880868",
                        "name": "Anna M. Kruspe"
                    },
                    {
                        "authorId": "1453548521",
                        "name": "Rudolph Triebel"
                    },
                    {
                        "authorId": "2054660189",
                        "name": "P. Jung"
                    },
                    {
                        "authorId": "46525320",
                        "name": "R. Roscher"
                    },
                    {
                        "authorId": "1380493607",
                        "name": "M. Shahzad"
                    },
                    {
                        "authorId": "2121300248",
                        "name": "Wen Yang"
                    },
                    {
                        "authorId": "1740759",
                        "name": "R. Bamler"
                    },
                    {
                        "authorId": "46875441",
                        "name": "Xiaoxiang Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018), calibration, and predictive certainty (Thulasidasan et al., 2019).",
                "It is also reported in Zhang et al. (2018) that Mixup helps with stability, adversarial robustness (Zhang et al., 2018), calibration, and predictive certainty (Thulasidasan et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e1e17e17abc51b99a95c9ae8d67a6909924f3986",
                "externalIds": {
                    "DBLP": "conf/iclr/YoonSHY21",
                    "ArXiv": "2107.00233",
                    "CorpusId": 235614375
                },
                "corpusId": 235614375,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e1e17e17abc51b99a95c9ae8d67a6909924f3986",
                "title": "FedMix: Approximation of Mixup under Mean Augmented Federated Learning",
                "abstract": "Federated learning (FL) allows edge devices to collectively learn a model without directly sharing data within each device, thus preserving privacy and eliminating the need to store data globally. While there are promising results under the assumption of independent and identically distributed (iid) local data, current state-of-the-art algorithms suffer from performance degradation as the heterogeneity of local data across clients increases. To resolve this issue, we propose a simple framework, Mean Augmented Federated Learning (MAFL), where clients send and receive averaged local data, subject to the privacy requirements of target applications. Under our framework, we propose a new augmentation algorithm, named FedMix, which is inspired by a phenomenal yet simple data augmentation method, Mixup, but does not require local raw data to be directly shared among devices. Our method shows greatly improved performance in the standard benchmark datasets of FL, under highly non-iid federated settings, compared to conventional algorithms.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "80830463",
                        "name": "Tehrim Yoon"
                    },
                    {
                        "authorId": "2110848093",
                        "name": "Sumin Shin"
                    },
                    {
                        "authorId": "2110796623",
                        "name": "S. Hwang"
                    },
                    {
                        "authorId": "1720494",
                        "name": "Eunho Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, representation learning has known major breakthroughs due to the advance in the field of contrastive learning [4, 5, 6, 12, 14, 23].",
                "Previous self-supervised contrastive learning methods [4, 6, 12, 14] indicate that the use of a batch normalization layer after the first fully connected layer has shown to generate more powerful representations."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4b07292f9ce297256118f8758ada85f83285bf08",
                "externalIds": {
                    "ArXiv": "2106.16093",
                    "DBLP": "conf/bmvc/ScalbertCV21",
                    "CorpusId": 235683170
                },
                "corpusId": 235683170,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/4b07292f9ce297256118f8758ada85f83285bf08",
                "title": "Multi-Source domain adaptation via supervised contrastive learning and confident consistency regularization",
                "abstract": "Multi-Source Unsupervised Domain Adaptation (multi-source UDA) aims to learn a model from several labeled source domains while performing well on a different target domain where only unlabeled data are available at training time. To align source and target features distributions, several recent works use source and target explicit statistics matching such as features moments or class centroids. Yet, these approaches do not guarantee class conditional distributions alignment across domains. In this work, we propose a new framework called Contrastive Multi-Source Domain Adaptation (CMSDA) for multi-source UDA that addresses this limitation. Discriminative features are learned from interpolated source examples via cross entropy minimization and from target examples via consistency regularization and hard pseudo-labeling. Simultaneously, interpolated source examples are leveraged to align source class conditional distributions through an interpolated version of the supervised contrastive loss. This alignment leads to more general and transferable features which further improves the generalization on the target domain. Extensive experiments have been carried out on three standard multi-source UDA datasets where our method reports state-of-the-art results.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1415690946",
                        "name": "Marin Scalbert"
                    },
                    {
                        "authorId": "1893915",
                        "name": "M. Vakalopoulou"
                    },
                    {
                        "authorId": "2117036502",
                        "name": "Florent Couzini'e-Devy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Techniques in this category include mixup training [Thulasidasan et al., 2019], pre-training [Hendrycks et al.",
                "To avoid possible misleading, maxl zl is referred to as winning score v (i.e., v = maxl zl) [Thulasidasan et al., 2019] hereinafter.",
                "Techniques in this category include mixup training [Thulasidasan et al., 2019], pre-training [Hendrycks et al., 2019a], label-smoothing [M\u00fcller et al., 2019], data augmentation [Ashukha et al., 2020], self-supervised learning [Hendrycks et al., 2019b], Bayesian approximation [Gal and Ghahramani,\u2026"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8ba1b3b4a87776553d7ff82a7500d9c464ea2cc9",
                "externalIds": {
                    "DBLP": "conf/uai/MaHXG021",
                    "ArXiv": "2106.14662",
                    "CorpusId": 235658315
                },
                "corpusId": 235658315,
                "publicationVenue": {
                    "id": "f9af8000-42f8-410d-a622-e8811e41660a",
                    "name": "Conference on Uncertainty in Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Uncertainty in Artificial Intelligence",
                        "UAI",
                        "Conf Uncertain Artif Intell",
                        "Uncertain Artif Intell"
                    ],
                    "url": "http://www.auai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/8ba1b3b4a87776553d7ff82a7500d9c464ea2cc9",
                "title": "Improving Uncertainty Calibration of Deep Neural Networks via Truth Discovery and Geometric Optimization",
                "abstract": "Deep Neural Networks (DNNs), despite their tremendous success in recent years, could still cast doubts on their predictions due to the intrinsic uncertainty associated with their learning process. Ensemble techniques and post-hoc calibrations are two types of approaches that have individually shown promise in improving the uncertainty calibration of DNNs. However, the synergistic effect of the two types of methods has not been well explored. In this paper, we propose a truth discovery framework to integrate ensemble-based and post-hoc calibration methods. Using the geometric variance of the ensemble candidates as a good indicator for sample uncertainty, we design an accuracy-preserving truth estimator with provably no accuracy drop. Furthermore, we show that post-hoc calibration can also be enhanced by truth discovery-regularized optimization. On large-scale datasets including CIFAR and ImageNet, our method shows consistent improvement against state-of-the-art calibration approaches on both histogram-based and kernel density-based evaluation metrics. Our codes are available at https://github.com/horsepurve/truly-uncertain.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9542165",
                        "name": "Chunwei Ma"
                    },
                    {
                        "authorId": "2153448",
                        "name": "Ziyun Huang"
                    },
                    {
                        "authorId": "35891727",
                        "name": "Jiayi Xian"
                    },
                    {
                        "authorId": "50987693",
                        "name": "Mingchen Gao"
                    },
                    {
                        "authorId": "7162036",
                        "name": "Jinhui Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Enforcing this strict linearity is expected to guide a model to behave linearly in between the data instances and to be robust to adversarial examples [29, 30]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "137ad8c05a1d37326a5a9d3e9f951395ec96b407",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-11528",
                    "ArXiv": "2106.11528",
                    "CorpusId": 235593093
                },
                "corpusId": 235593093,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/137ad8c05a1d37326a5a9d3e9f951395ec96b407",
                "title": "Recent Deep Semi-supervised Learning Approaches and Related Works",
                "abstract": "The author of this work proposes an overview of the recent semi-supervised learning approaches and related works. Despite the remarkable success of neural networks in various applications, there exist few formidable constraints including the need for a large amount of labeled data. Therefore, semi-supervised learning, which is a learning scheme in which the scarce labels and a larger amount of unlabeled data are utilized to train models (e.g., deep neural networks) is getting more important. Based on the key assumptions of semi-supervised learning, which are the manifold assumption, cluster assumption, and continuity assumption, the work reviews the recent semi-supervised learning approaches. In particular, the methods in regard to using deep neural networks in a semi-supervised learning setting are primarily discussed. In addition, the existing works are first classified based on the underlying idea and explained, and then the holistic approaches that unify the aforementioned ideas are detailed.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2135792230",
                        "name": "Gyeongho Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "5SYNTHetic collection of Imagery and Annotations\nMeta learning in the graceful degradation problem is most applicable to the quick adaptation of models upon encountering data outside of their training distribution, this might be fast video segmentation of new object as in Thulasidasan et al. (2019)."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "95870b1cd7bbc23f364e4bf98d5c6081e27060be",
                "externalIds": {
                    "ArXiv": "2106.11119",
                    "DBLP": "journals/corr/abs-2106-11119",
                    "CorpusId": 235490640
                },
                "corpusId": 235490640,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/95870b1cd7bbc23f364e4bf98d5c6081e27060be",
                "title": "Graceful Degradation and Related Fields",
                "abstract": "When machine learning models encounter data which is out of the distribution on which they were trained they have a tendency to behave poorly, most prominently over-confidence in erroneous predictions. Such behaviours will have disastrous effects on real-world machine learning systems. In this field graceful degradation refers to the optimisation of model performance as it encounters this out-of-distribution data. This work presents a definition and discussion of graceful degradation and where it can be applied in deployed visual systems. Following this a survey of relevant areas is undertaken, novelly splitting the graceful degradation problem into active and passive approaches. In passive approaches, graceful degradation is handled and achieved by the model in a self-contained manner, in active approaches the model is updated upon encountering epistemic uncertainties. This work communicates the importance of the problem and aims to prompt the development of machine learning strategies that are aware of graceful degradation. 1 ar X iv :2 10 6. 11 11 9v 1 [ cs .L G ] 2 1 Ju n 20 21 Feasibility Study: Progressive Intelligence and Graceful Degradation",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "121796740",
                        "name": "J. Dymond"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f3f7836da8bb5121292523692b2187a2e0d6eea7",
                "externalIds": {
                    "ArXiv": "2106.09385",
                    "CorpusId": 238253494
                },
                "corpusId": 238253494,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f3f7836da8bb5121292523692b2187a2e0d6eea7",
                "title": "On Deep Neural Network Calibration by Regularization and its Impact on Refinement",
                "abstract": "Deep neural networks have been shown to be highly miscalibrated. often they tend to be overconfident in their predictions. It poses a significant challenge for safetycritical systems to utilise deep neural networks (DNNs), reliably. Many recently proposed approaches to mitigate this have demonstrated substantial progress in improving DNN calibration. However, they hardly touch upon refinement, which historically has been an essential aspect of calibration. Refinement indicates separability of a network\u2019s correct and incorrect predictions. This paper presents a theoretically and empirically supported exposition reviewing refinement of a calibrated model. Firstly, we show the breakdown of expected calibration error (ECE), into predicted confidence and refinement under the assumption of over-confident predictions. Secondly, linking with this result, we highlight that regularisation based calibration only focuses on naively reducing a model\u2019s confidence. This logically has a severe downside to a model\u2019s refinement as correct and incorrect predictions become tightly coupled. Lastly, connecting refinement with ECE also provides support to existing refinement based approaches which improve calibration but do not explain the reasoning behind it. We support our claims through rigorous empirical evaluations of many state of the art calibration approaches on widely used datasets and neural networks. We find that many calibration approaches with the likes of label smoothing, mixup etc. lower the usefulness of a DNN by degrading its refinement. Even under natural data shift, this calibrationrefinement trade-off holds for the majority of calibration methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109434876",
                        "name": "Aditya Singh"
                    },
                    {
                        "authorId": "39720629",
                        "name": "Alessandro Bay"
                    },
                    {
                        "authorId": "39599054",
                        "name": "B. Sengupta"
                    },
                    {
                        "authorId": "2063974309",
                        "name": "Andrea Mirabile"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, their method is designed for data augmentation to enhance in-distribution performance and requires corresponding combinations in the label space (Thulasidasan et al., 2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "68c506d3d7e830df15226c020638af2e5fdd7a98",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-08616",
                    "ACL": "2021.acl-long.273",
                    "ArXiv": "2106.08616",
                    "DOI": "10.18653/v1/2021.acl-long.273",
                    "CorpusId": 235446895
                },
                "corpusId": 235446895,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/68c506d3d7e830df15226c020638af2e5fdd7a98",
                "title": "Out-of-Scope Intent Detection with Self-Supervision and Discriminative Training",
                "abstract": "Out-of-scope intent detection is of practical importance in task-oriented dialogue systems. Since the distribution of outlier utterances is arbitrary and unknown in the training stage, existing methods commonly rely on strong assumptions on data distribution such as mixture of Gaussians to make inference, resulting in either complex multi-step training procedures or hand-crafted rules such as confidence threshold selection for outlier detection. In this paper, we propose a simple yet effective method to train an out-of-scope intent classifier in a fully end-to-end manner by simulating the test scenario in training, which requires no assumption on data distribution and no additional post-processing or threshold setting. Specifically, we construct a set of pseudo outliers in the training stage, by generating synthetic outliers using inliner features via self-supervision and sampling out-of-scope sentences from easily available open-domain datasets. The pseudo outliers are used to train a discriminative classifier that can be directly applied to and generalize well on the test task. We evaluate our method extensively on four benchmark dialogue datasets and observe significant improvements over state-of-the-art approaches. Our code has been released at https://github.com/liam0949/DCLOOS.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2061239456",
                        "name": "Li-Ming Zhan"
                    },
                    {
                        "authorId": "2152874181",
                        "name": "Haowen Liang"
                    },
                    {
                        "authorId": "73548014",
                        "name": "Bo Liu"
                    },
                    {
                        "authorId": "2147259441",
                        "name": "Lu Fan"
                    },
                    {
                        "authorId": "19195265",
                        "name": "Xiao-Ming Wu"
                    },
                    {
                        "authorId": "1902169",
                        "name": "Albert Y. S. Lam"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026modern neural networks to be poorly calibrated, as suggested previously (Guo et al., 2017; Lakshminarayanan et al., 2017; Malinin & Gales, 2018; Thulasidasan et al., 2019; Hendrycks et al., 2020b; Ovadia et al., 2019; Wenzel et al., 2020; Havasi et al., 2021; Rahaman & Thiery, 2020; Leathart &\u2026",
                "This suggests that there may be no continuing trend for highly accurate modern neural networks to be poorly calibrated, as suggested previously (Guo et al., 2017; Lakshminarayanan et al., 2017; Malinin & Gales, 2018; Thulasidasan et al., 2019; Hendrycks et al., 2020b; Ovadia et al., 2019; Wenzel et al., 2020; Havasi et al., 2021; Rahaman & Thiery, 2020; Leathart & Polaczuk, 2020).",
                "Many strategies have been proposed to improve model calibration such as post-hoc rescaling of predictions (Guo et al., 2017), averaging multiple predictions (Lakshminarayanan et al., 2017; Wen et al., 2020), and data augmentation (Thulasidasan et al., 2019; Wen et al., 2021).",
                "\u2026networks can be surprisingly poor, despite the advances in accuracy (e.g. Guo et al. 2017; Lakshminarayanan et al. 2017; Malinin & Gales 2018; Thulasidasan et al. 2019; Hendrycks et al. 2020b; Ovadia et al. 2019; Wenzel et al. 2020; Havasi et al. 2021; Rahaman & Thiery 2020; Leathart &\u2026",
                "Other works have corroborated some of these findings (e.g., Thulasidasan et al. 2019; Wen et al. 2021).",
                "In fact, over the last few years, there have been many reports that calibration of modern neural networks can be surprisingly poor, despite the advances in accuracy (e.g. Guo et al. 2017; Lakshminarayanan et al. 2017; Malinin & Gales 2018; Thulasidasan et al. 2019; Hendrycks et al. 2020b; Ovadia et al. 2019; Wenzel et al. 2020; Havasi et al. 2021; Rahaman & Thiery 2020; Leathart & Polaczuk 2020)."
            ],
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "e757488d2e8684e3da7b14fbb000b7e4a0bab001",
                "externalIds": {
                    "ArXiv": "2106.07998",
                    "DBLP": "journals/corr/abs-2106-07998",
                    "CorpusId": 235435823
                },
                "corpusId": 235435823,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e757488d2e8684e3da7b14fbb000b7e4a0bab001",
                "title": "Revisiting the Calibration of Modern Neural Networks",
                "abstract": "Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classification models. We systematically relate model calibration and accuracy, and find that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46352821",
                        "name": "Matthias Minderer"
                    },
                    {
                        "authorId": "2941141",
                        "name": "Josip Djolonga"
                    },
                    {
                        "authorId": "3451951",
                        "name": "Rob Romijnders"
                    },
                    {
                        "authorId": "73774594",
                        "name": "F. Hubis"
                    },
                    {
                        "authorId": "2743563",
                        "name": "Xiaohua Zhai"
                    },
                    {
                        "authorId": "2815290",
                        "name": "N. Houlsby"
                    },
                    {
                        "authorId": "47497262",
                        "name": "Dustin Tran"
                    },
                    {
                        "authorId": "34302129",
                        "name": "Mario Lucic"
                    }
                ]
            }
        },
        {
            "contexts": [
                "TREC is a commonly used dataset to evaluate mixup methods in sentence classification (Guo et al., 2019; Thulasidasan et al., 2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "47ad38b92b843ef2521db9a650038d7d00e4e067",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-08062",
                    "ArXiv": "2106.08062",
                    "ACL": "2021.findings-acl.285",
                    "DOI": "10.18653/v1/2021.findings-acl.285",
                    "CorpusId": 235436032
                },
                "corpusId": 235436032,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/47ad38b92b843ef2521db9a650038d7d00e4e067",
                "title": "SSMix: Saliency-Based Span Mixup for Text Classification",
                "abstract": "Data augmentation with mixup has shown to be effective on various computer vision tasks. Despite its great success, there has been a hurdle to apply mixup to NLP tasks since text consists of discrete tokens with variable length. In this work, we propose SSMix, a novel mixup method where the operation is performed on input text rather than on hidden vectors like previous approaches. SSMix synthesizes a sentence while preserving the locality of two original texts by span-based mixing and keeping more tokens related to the prediction relying on saliency information. With extensive experiments, we empirically validate that our method outperforms hidden-level mixup methods on a wide range of text classification benchmarks, including textual entailment, sentiment classification, and question-type classification. Our code is available at https://github.com/clovaai/ssmix.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47948265",
                        "name": "Soyoung Yoon"
                    },
                    {
                        "authorId": "11798271",
                        "name": "Gyuwan Kim"
                    },
                    {
                        "authorId": "2152042873",
                        "name": "Kyumin Park"
                    }
                ]
            }
        },
        {
            "contexts": [
                "has shown to improve classifier\u2019s calibration and reduced prediction uncertainity in [73].",
                "This proves [73, 46] the better generalization and"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3459e17209dd8d64a3ee5a097b1a040b0a283b11",
                "externalIds": {
                    "ArXiv": "2106.07085",
                    "DBLP": "journals/corr/abs-2106-07085",
                    "CorpusId": 235421963
                },
                "corpusId": 235421963,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3459e17209dd8d64a3ee5a097b1a040b0a283b11",
                "title": "Survey: Image Mixing and Deleting for Data Augmentation",
                "abstract": "Neural networks are prone to overfitting and memorizing data patterns. To avoid over-fitting and enhance their generalization and performance, various methods have been suggested in the literature, including dropout, regularization, label smoothing, etc. One such method is augmentation which introduces different types of corruption in the data to prevent the model from overfitting and to memorize patterns present in the data. A sub-area of data augmentation is image mixing and deleting. This specific type of augmentation either deletes image regions or mixes two images to hide or make particular characteristics of images confusing for the network, forcing it to emphasize the overall structure of the object in an image. Models trained with this approach have proven to perform and generalize well compared to those trained without image mixing or deleting. An added benefit that comes with this method of training is robustness against image corruption. Due to its low computational cost and recent success, researchers have proposed many image mixing and deleting techniques. We furnish an in-depth survey of image mixing and deleting techniques and provide categorization via their most distinguishing features. We initiate our discussion with some fundamental relevant concepts. Next, we present essentials, such as each category's strengths and limitations, describing their working mechanism, basic formulations, and applications. We also discuss the general challenges and recommend possible future research directions for image mixing and deleting data augmentation techniques. Datasets and codes for evaluation are publicly available here.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "32749940",
                        "name": "Humza Naveed"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Training with MixUp data augmentation [281] is also found to beneit model calibration [225]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f096baa9f77a9ff112aea94757b5601005038d36",
                "externalIds": {
                    "ArXiv": "2106.04823",
                    "DBLP": "journals/csur/MohseniWXYWY23",
                    "DOI": "10.1145/3551385",
                    "CorpusId": 247315425
                },
                "corpusId": 247315425,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f096baa9f77a9ff112aea94757b5601005038d36",
                "title": "Taxonomy of Machine Learning Safety: A Survey and Primer",
                "abstract": "The open-world deployment of Machine Learning (ML) algorithms in safety-critical applications such as autonomous vehicles needs to address a variety of ML vulnerabilities such as interpretability, verifiability, and performance limitations. Research explores different approaches to improve ML dependability by proposing new models and training techniques to reduce generalization error, achieve domain adaptation, and detect outlier examples and adversarial attacks. However, there is a missing connection between ongoing ML research and well-established safety principles. In this article, we present a structured and comprehensive review of ML techniques to improve the dependability of ML algorithms in uncontrolled open-world settings. From this review, we propose the Taxonomy of ML Safety that maps state-of-the-art ML techniques to key engineering safety strategies. Our taxonomy of ML safety presents a safety-oriented categorization of ML techniques to provide guidance for improving dependability of the ML design and development. The proposed taxonomy can serve as a safety checklist to aid designers in improving coverage and diversity of safety strategies employed in any given ML system.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3228458",
                        "name": "Sina Mohseni"
                    },
                    {
                        "authorId": "113727681",
                        "name": "Haotao Wang"
                    },
                    {
                        "authorId": "2723309",
                        "name": "Chaowei Xiao"
                    },
                    {
                        "authorId": "1751019",
                        "name": "Zhiding Yu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "2107713092",
                        "name": "J. Yadawa"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f431b6698574cec48b1a4a938fe14b9b1865cac2",
                "externalIds": {
                    "ArXiv": "2106.04527",
                    "DOI": "10.1109/TNNLS.2022.3203315",
                    "CorpusId": 252438224,
                    "PubMed": "36136921"
                },
                "corpusId": 252438224,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f431b6698574cec48b1a4a938fe14b9b1865cac2",
                "title": "LaplaceNet: A Hybrid Graph-Energy Neural Network for Deep Semisupervised Classification.",
                "abstract": "Semisupervised learning (SSL) has received a lot of recent attention as it alleviates the need for large amounts of labeled data which can often be expensive, requires expert knowledge, and be time consuming to collect. Recent developments in deep semisupervised classification have reached unprecedented performance and the gap between supervised and SSL is ever-decreasing. This improvement in performance has been based on the inclusion of numerous technical tricks, strong augmentation techniques, and costly optimization schemes with multiterm loss functions. We propose a new framework, LaplaceNet, for deep semisupervised classification that has a greatly reduced model complexity. We utilize a hybrid approach where pseudolabels are produced by minimizing the Laplacian energy on a graph. These pseudolabels are then used to iteratively train a neural-network backbone. Our model outperforms state-of-the-art methods for deep semisupervised classification, over several benchmark datasets. Furthermore, we consider the application of strong augmentations to neural networks theoretically and justify the use of a multisampling approach for SSL. We demonstrate, through rigorous experimentation, that a multisampling augmentation approach improves generalization and reduces the sensitivity of the network to augmentation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1410092918",
                        "name": "P. Sellars"
                    },
                    {
                        "authorId": "1388720262",
                        "name": "Angelica I. Avil\u00e9s-Rivero"
                    },
                    {
                        "authorId": "29398330",
                        "name": "C. Schonlieb"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The work of [40] and [4] both directly evaluated Mixup\u2019s effect on OOD detection."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7bdc1a737a8864b80c7abd5cca71c6514de25345",
                "externalIds": {
                    "ArXiv": "2106.03917",
                    "DBLP": "conf/wacv/ZhangILCL23",
                    "DOI": "10.1109/WACV56688.2023.00549",
                    "CorpusId": 247319058
                },
                "corpusId": 247319058,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/7bdc1a737a8864b80c7abd5cca71c6514de25345",
                "title": "Mixture Outlier Exposure: Towards Out-of-Distribution Detection in Fine-grained Environments",
                "abstract": "Many real-world scenarios in which DNN-based recognition systems are deployed have inherently fine-grained attributes (e.g., bird-species recognition, medical image classification). In addition to achieving reliable accuracy, a critical subtask for these models is to detect Out-of-distribution (OOD) inputs. Given the nature of the deployment environment, one may expect such OOD inputs to also be fine-grained w.r.t. the known classes (e.g., a novel bird species), which are thus extremely difficult to identify. Unfortunately, OOD detection in fine-grained scenarios remains largely underexplored. In this work, we aim to fill this gap by first carefully constructing four large-scale fine-grained test environments, in which existing methods are shown to have difficulties. Particularly, we find that even explicitly incorporating a diverse set of auxiliary outlier data during training does not provide sufficient coverage over the broad region where fine-grained OOD samples locate. We then propose Mixture Outlier Exposure (MixOE), which mixes ID data and training outliers to expand the coverage of different OOD granularities, and trains the model such that the prediction confidence linearly decays as the input transitions from ID to OOD. Extensive experiments and analyses demonstrate the effectiveness of MixOE for building up OOD detector in finegrained environments. The code is available at https://github.com/zjysteven/MixOE.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108123095",
                        "name": "Jingyang Zhang"
                    },
                    {
                        "authorId": "52121635",
                        "name": "Nathan Inkawhich"
                    },
                    {
                        "authorId": "2141218451",
                        "name": "Randolph Linderman"
                    },
                    {
                        "authorId": "5442167",
                        "name": "Yiran Chen"
                    },
                    {
                        "authorId": "51208601",
                        "name": "H. Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1945446005adf9f10f44d2b8fc3d94ecd12548e7",
                "externalIds": {
                    "DOI": "10.1002/mrm.28828",
                    "CorpusId": 235322686,
                    "PubMed": "34080720"
                },
                "corpusId": 235322686,
                "publicationVenue": {
                    "id": "c1269fa2-b51c-4648-bf0c-95797c4a74cd",
                    "name": "Magnetic Resonance in Medicine",
                    "type": "journal",
                    "alternate_names": [
                        "Magn Reson Med"
                    ],
                    "issn": "0740-3194",
                    "url": "http://www3.interscience.wiley.com/cgi-bin/jhome/10005196",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/15222594",
                        "http://onlinelibrary.wiley.com/journal/10.1002/(ISSN)1522-2594",
                        "http://www.interscience.wiley.com/jpages/0740-3194/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1945446005adf9f10f44d2b8fc3d94ecd12548e7",
                "title": "Local perturbation responses and checkerboard tests: Characterization tools for nonlinear MRI methods",
                "abstract": "Modern methods for MR image reconstruction, denoising, and parameter mapping are becoming increasingly nonlinear, black\u2010box, and at risk of \u201challucination.\u201d These trends mean that traditional tools for judging confidence in an image (visual quality assessment, point\u2010spread functions (PSFs), g\u2010factor maps, etc.) are less helpful than before. This paper describes and evaluates an approach that can help with assessing confidence in images produced by arbitrary nonlinear methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7833464",
                        "name": "Chin-Cheng Chan"
                    },
                    {
                        "authorId": "143682127",
                        "name": "J. Haldar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other uncertainty estimation methods such as [18, 34, 10] can also be used to estimate the uncertainty in conjunction with our proposed method."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a43f7d6a751a6ad8667272f1176d2f15dbd8feb6",
                "externalIds": {
                    "DBLP": "conf/cvpr/ZaeemzadehBSCRS21",
                    "DOI": "10.1109/CVPR46437.2021.00933",
                    "CorpusId": 233203164
                },
                "corpusId": 233203164,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a43f7d6a751a6ad8667272f1176d2f15dbd8feb6",
                "title": "Out-of-Distribution Detection Using Union of 1-Dimensional Subspaces",
                "abstract": "The goal of out-of-distribution (OOD) detection is to handle the situations where the test samples are drawn from a different distribution than the training data. In this paper, we argue that OOD samples can be detected more easily if the training data is embedded into a low-dimensional space, such that the embedded training samples lie on a union of 1-dimensional subspaces. We show that such embedding of the in-distribution (ID) samples provides us with two main advantages. First, due to compact representation in the feature space, OOD samples are less likely to occupy the same region as the known classes. Second, the first singular vector of ID samples belonging to a 1-dimensional subspace can be used as their robust representative. Motivated by these observations, we train a deep neural network such that the ID samples are embedded onto a union of 1-dimensional subspaces. At the test time, employing sampling techniques used for approximate Bayesian inference in deep learning, input samples are detected as OOD if they occupy the region corresponding to the ID samples with probability 0. Spectral components of the ID samples are used as robust representative of this region. Our method does not have any hyperparameter to be tuned using extra information and it can be applied on different modalities with minimal change. The effectiveness of the proposed method is demonstrated on different benchmark datasets, both in the image and video classification domains.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2621521",
                        "name": "Alireza Zaeemzadeh"
                    },
                    {
                        "authorId": "51452438",
                        "name": "N. Bisagno"
                    },
                    {
                        "authorId": "2125030557",
                        "name": "Zeno Sambugaro"
                    },
                    {
                        "authorId": "3058987",
                        "name": "N. Conci"
                    },
                    {
                        "authorId": "1789219",
                        "name": "N. Rahnavard"
                    },
                    {
                        "authorId": "145103012",
                        "name": "M. Shah"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mixup has also been widely used in other learning tasks such as semi-supervised learning [16], neural network calibration [17], and adversarial defense [18]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a43447736407fdd058cea6a15ee55801f13f91d5",
                "externalIds": {
                    "PubMedCentral": "8843050",
                    "DOI": "10.1109/TBME.2021.3085576",
                    "CorpusId": 248775544
                },
                "corpusId": 248775544,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a43447736407fdd058cea6a15ee55801f13f91d5",
                "title": "Mix Contrast for COVID-19 Mild-to-Critical Prediction",
                "abstract": "Objective: In a few patients with mild COVID-19, there is a possibility of the infection becoming severe or critical in the future. This work aims to identify high-risk patients who have a high probability of changing from mild to critical COVID-19 (only account for 5% of cases). Methods: Using traditional convolutional neural networks for classification may not be suitable to identify this 5% of high risk patients from an entire dataset due to the highly imbalanced label distribution. To address this problem, we propose a Mix Contrast model, which matches original features with mixed features for contrastive learning. Three modules are proposed for training the model: 1) a cumulative learning strategy for synthesizing the mixed feature; 2) a commutative feature combination module for learning the commutative law of feature concatenation; 3) a united pairwise loss assigning adaptive weights for sample pairs with different class anchors based on their current optimization status. Results: We collect a multi-center computed tomography dataset including 918 confirmed COVID-19 patients from four hospitals and evaluate the proposed method on both the COVID-19 mild-to-critical prediction and COVID-19 diagnosis tasks. For mild-to-critical prediction, the experimental results show a recall of 0.80 and a specificity of 0.815. For diagnosis, the model shows comparable results with deep neural networks using a large dataset. Our method demonstrates improvements when the amount of training data is small or imbalanced. Significance: Identifying mild-to-critical COVID-19 patients is important for early prevention and personalized treatment planning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2165199957",
                        "name": "Yongbei Yongbei Zhu Zhu"
                    },
                    {
                        "authorId": null,
                        "name": "Shuo Shuo Wang Wang"
                    },
                    {
                        "authorId": null,
                        "name": "Siwen Siwen Wang Wang"
                    },
                    {
                        "authorId": "2165204163",
                        "name": "Qingxia Qingxia Wu Wu"
                    },
                    {
                        "authorId": "2165202924",
                        "name": "Liusu Liusu Wang Wang"
                    },
                    {
                        "authorId": "2165204359",
                        "name": "Hongjun Hongjun Li Li"
                    },
                    {
                        "authorId": "2165204559",
                        "name": "Meiyun Meiyun Wang Wang"
                    },
                    {
                        "authorId": "2165204557",
                        "name": "Meng Meng Niu Niu"
                    },
                    {
                        "authorId": "2165204756",
                        "name": "Yunfei Yunfei Zha Zha"
                    },
                    {
                        "authorId": "1634520603",
                        "name": "Jie Jie Tian Tian"
                    }
                ]
            }
        },
        {
            "contexts": [
                "through data augmentation, better calibrated DNNs (Mixup [23]) or alleviate the over-confidence of the model (CutMix [24])."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3547c1c11b672c5ccbd262c7ebd8f42fc1e2dcdb",
                "externalIds": {
                    "ArXiv": "2105.11828",
                    "DBLP": "journals/corr/abs-2105-11828",
                    "CorpusId": 235186903
                },
                "corpusId": 235186903,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3547c1c11b672c5ccbd262c7ebd8f42fc1e2dcdb",
                "title": "Bridging the Gap Between Explainable AI and Uncertainty Quantification to Enhance Trustability",
                "abstract": "After the tremendous advances of deep learning and other AI methods, more attention is flowing into other properties of modern approaches, such as interpretability, fairness, etc. combined in frameworks like Responsible AI. Two research directions, namely Explainable AI and Uncertainty Quantification are becoming more and more important, but have been so far never combined and jointly explored. In this paper, I show how both research areas provide potential for combination, why more research should be done in this direction and how this would lead to an increase in trustability in AI systems.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1804577",
                        "name": "Dominik Seuss"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2022 Mixup As shown in Thulasidasan et al. [2019], Mixup can be an effective OoD detector, so we also use this as one of our baselines."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9dfc77ebffc7efb0958033554c401c7b795c2133",
                "externalIds": {
                    "DBLP": "conf/icmla/ThulasidasanTDC21",
                    "ArXiv": "2105.07107",
                    "DOI": "10.1109/ICMLA52953.2021.00050",
                    "CorpusId": 234741750
                },
                "corpusId": 234741750,
                "publicationVenue": {
                    "id": "f6752838-f268-4a1b-87e7-c5f30a36713c",
                    "name": "International Conference on Machine Learning and Applications",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Mach Learn Appl",
                        "ICMLA"
                    ],
                    "url": "http://www.icmla-conference.org/"
                },
                "url": "https://www.semanticscholar.org/paper/9dfc77ebffc7efb0958033554c401c7b795c2133",
                "title": "An Effective Baseline for Robustness to Distributional Shift",
                "abstract": "Refraining from confidently predicting when faced with categories of inputs different from those seen during training is an important requirement for the safe deployment of deep learning systems. While simple to state, this has been a particularly challenging problem in deep learning, where models often end up making overconfident predictions in such situations. In this work, we present a simple, but highly effective approach to deal with out-of-distribution detection that uses the principle of abstention: when encountering a sample from an unseen class, the desired behavior is to abstain from predicting. Our approach uses a network with an extra abstention class and is trained on a dataset that is augmented with an uncurated set that consists of a large number of out-of-distribution (OoD) samples that are assigned the label of the abstention class; the model is then trained to learn an effective discriminator between in and out-of-distribution samples. We compare this relatively simple approach against a wide variety of more complex methods that have been proposed both for out-of-distribution detection as well as uncertainty modeling in deep learning, and empirically demonstrate its effectiveness on a wide variety of of benchmarks and deep architectures for image recognition and text classification, often outperforming existing approaches by significant margins. Given the simplicity and effectiveness of this method, we propose that this approach be used as a new additional baseline for future work in this domain.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1780360",
                        "name": "S. Thulasidasan"
                    },
                    {
                        "authorId": "2093329661",
                        "name": "Sushil Thapa"
                    },
                    {
                        "authorId": "1753019031",
                        "name": "S. Dhaubhadel"
                    },
                    {
                        "authorId": "2530185",
                        "name": "Gopinath Chennupati"
                    },
                    {
                        "authorId": "144296799",
                        "name": "Tanmoy Bhattacharya"
                    },
                    {
                        "authorId": "1748118",
                        "name": "J. Bilmes"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[300] discovered that mix-up training [296]"
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7f7add3cb04e47e7b77070dd09b51765394d5ac6",
                "externalIds": {
                    "DBLP": "journals/pami/FanZXCS23",
                    "ArXiv": "2105.03053",
                    "DOI": "10.1109/TPAMI.2022.3166451",
                    "CorpusId": 234097488,
                    "PubMed": "35404809"
                },
                "corpusId": 234097488,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7f7add3cb04e47e7b77070dd09b51765394d5ac6",
                "title": "Salient Objects in Clutter",
                "abstract": "In this paper, we identify and address a serious design bias of existing salient object detection (SOD) datasets, which unrealistically assume that each image should contain at least one clear and uncluttered salient object. This design bias has led to a saturation in performance for state-of-the-art SOD models when evaluated on existing datasets. However, these models are still far from satisfactory when applied to real-world scenes. Based on our analyses, we propose a new high-quality dataset and update the previous saliency benchmark. Specifically, our dataset, called Salient Objects in Clutter (SOC), includes images with both salient and non-salient objects from several common object categories. In addition to object category annotations, each salient image is accompanied by attributes that reflect common challenges in common scenes, which can help provide deeper insight into the SOD problem. Further, with a given saliency encoder, e.g., the backbone network, existing saliency models are designed to achieve mapping from the training image set to the training ground-truth set. We therefore argue that improving the dataset can yield higher performance gains than focusing only on the decoder design. With this in mind, we investigate several dataset-enhancement strategies, including label smoothing to implicitly emphasize salient boundaries, random image augmentation to adapt saliency models to various scenarios, and self-supervised learning as a regularization strategy to learn from small datasets. Our extensive results demonstrate the effectiveness of these tricks. We also provide a comprehensive benchmark for SOD, which can be found in our repository: https://github.com/DengPingFan/SODBenchmark.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "23999143",
                        "name": "Deng-Ping Fan"
                    },
                    {
                        "authorId": "2155705097",
                        "name": "Jing Zhang"
                    },
                    {
                        "authorId": "2110691137",
                        "name": "Gang Xu"
                    },
                    {
                        "authorId": "1557350184",
                        "name": "Mingg-Ming Cheng"
                    },
                    {
                        "authorId": "144082425",
                        "name": "Ling Shao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Sampling-free uncertainty estimation [21] and data augmentation [22], [23] are candidates."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c50327062419ebe5fb1eebe0ed49fba08210cb3e",
                "externalIds": {
                    "ArXiv": "2106.05870",
                    "DBLP": "journals/corr/abs-2106-05870",
                    "DOI": "10.1109/RadarConf2147009.2021.9455269",
                    "CorpusId": 235390396
                },
                "corpusId": 235390396,
                "publicationVenue": {
                    "id": "4e08736c-dbd0-4931-b4f0-84c9f7f66837",
                    "name": "International Radar Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Int Radar Conf",
                        "IEEE Radar Conference",
                        "RadarCon",
                        "IEEE Radar Conf"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c50327062419ebe5fb1eebe0ed49fba08210cb3e",
                "title": "Investigation of Uncertainty of Deep Learning-based Object Classification on Radar Spectra",
                "abstract": "Deep learning (DL) has recently attracted increasing interest to improve object type classification for automotive radar. In addition to high accuracy, it is crucial for decision making in autonomous vehicles to evaluate the reliability of the predictions; however, decisions of DL networks are non-transparent. Current DL research has investigated how uncertainties of predictions can be quantified, and in this article, we evaluate the potential of these methods for safe, automotive radar perception. In particular we evaluate how uncertainty quantification can support radar perception under (1) domain shift, (2) corruptions of input signals, and (3) in the presence of unknown objects. We find that in agreement with phenomena observed in the literature, deep radar classifiers are overly confident, even in their wrong predictions. This raises concerns about the use of the confidence values for decision making under uncertainty, as the model fails to notify when it cannot handle an unknown situation. Accurate confidence values would allow optimal integration of multiple information sources, e.g. via sensor fusion. We show that by applying state-of-the-art post-hoc uncertainty calibration, the quality of confidence measures can be significantly improved, thereby partially resolving the over-confidence problem. Our investigation shows that further research into training and calibrating DL networks is necessary and offers great potential for safe automotive object classification with radar sensors.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51290549",
                        "name": "Kanil Patel"
                    },
                    {
                        "authorId": "52020792",
                        "name": "William H. Beluch"
                    },
                    {
                        "authorId": "2451538",
                        "name": "K. Rambach"
                    },
                    {
                        "authorId": "122540552",
                        "name": "Adriana-Eliza Cozma"
                    },
                    {
                        "authorId": "144578436",
                        "name": "Michael Pfeiffer"
                    },
                    {
                        "authorId": "49188298",
                        "name": "Bin Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Neural network prediction scores are well known to not be well calibrated, a subject of recent investigation by Thulasidasan et al. (2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b8c3d5062a6f0fb3b09f6a5cd5c0c106f52eb824",
                "externalIds": {
                    "DOI": "10.1121/10.0004992",
                    "CorpusId": 235778324,
                    "PubMed": "34241092"
                },
                "corpusId": 235778324,
                "publicationVenue": {
                    "id": "cc124a8d-f3a0-44cc-85b4-6a1a6ffdd673",
                    "name": "Journal of the Acoustical Society of America",
                    "type": "journal",
                    "alternate_names": [
                        "J Acoust Soc Am"
                    ],
                    "issn": "0001-4966",
                    "url": "http://scitation.aip.org/content/asa/journal/jasa",
                    "alternate_urls": [
                        "https://asa.scitation.org/journal/jas",
                        "https://asa.scitation.org/toc/jas/current"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b8c3d5062a6f0fb3b09f6a5cd5c0c106f52eb824",
                "title": "Using context to train time-domain echolocation click detectors.",
                "abstract": "This work demonstrates the effectiveness of using humans in the loop processes for constructing large training sets for machine learning tasks. A corpus of over 57\u2009000 toothed whale echolocation clicks was developed by using a permissive energy-based echolocation detector followed by a machine-assisted quality control process that exploits contextual cues. Subsets of these data were used to train feed forward neural networks that detected over 850\u2009000 echolocation clicks that were validated using the same quality control process. It is shown that this network architecture performs well in a variety of contexts and is evaluated against a withheld data set that was collected nearly five years apart from the development data at a location over 600\u2009km distant. The system was capable of finding echolocation bouts that were missed by human analysts, and the patterns of error in the classifier consist primarily of anthropogenic sources that were not included as counter-training examples. In the absence of such events, typical false positive rates are under ten events per hour even at low thresholds.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35154087",
                        "name": "M. Roch"
                    },
                    {
                        "authorId": "70402245",
                        "name": "Scott Lindeneau"
                    },
                    {
                        "authorId": "2118603664",
                        "name": "Gurisht Singh Aurora"
                    },
                    {
                        "authorId": "4914460",
                        "name": "K. Frasier"
                    },
                    {
                        "authorId": "2665688",
                        "name": "J. Hildebrand"
                    },
                    {
                        "authorId": "1742496",
                        "name": "H. Glotin"
                    },
                    {
                        "authorId": "1398275017",
                        "name": "S. Baumann\u2010Pickering"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c1a9bdb19a5ec2d23b10335e97bc1410b0cad671",
                "externalIds": {
                    "ArXiv": "2104.14805",
                    "DBLP": "conf/eccv/FanTT22",
                    "DOI": "10.1007/978-3-031-20044-1_5",
                    "CorpusId": 233476265
                },
                "corpusId": 233476265,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/c1a9bdb19a5ec2d23b10335e97bc1410b0cad671",
                "title": "Few-Shot Video Object Detection",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2105557652",
                        "name": "Qi Fan"
                    },
                    {
                        "authorId": "2088295",
                        "name": "Chi-Keung Tang"
                    },
                    {
                        "authorId": "5068280",
                        "name": "Yu-Wing Tai"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "267a16c6b5f3c50ca9f8842e743436811b74b4a4",
                "externalIds": {
                    "DBLP": "journals/widm/KurianSKMR21",
                    "MAG": "3157970579",
                    "DOI": "10.1002/widm.1410",
                    "CorpusId": 235382228
                },
                "corpusId": 235382228,
                "publicationVenue": {
                    "id": "1e853ea4-0f70-4d45-aa7e-7612a227cbcf",
                    "name": "Wiley Interdisciplinary Reviews Data Mining and Knowledge Discovery",
                    "type": "journal",
                    "alternate_names": [
                        "Wiley Interdiscip Rev Min Knowl Discov",
                        "Wiley Interdisciplinary Reviews-Data Mining and Knowledge Discovery",
                        "Wiley Interdiscip Rev Data Min Knowl Discov"
                    ],
                    "issn": "1942-4795",
                    "url": "http://onlinelibrary.wiley.com/journal/10.1002/(ISSN)1942-4795",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/19424795"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/267a16c6b5f3c50ca9f8842e743436811b74b4a4",
                "title": "A 2021 update on cancer image analytics with deep learning",
                "abstract": "Deep learning (DL)\u2010based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high\u2010level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "8759953",
                        "name": "N. Kurian"
                    },
                    {
                        "authorId": "2049437",
                        "name": "A. Sethi"
                    },
                    {
                        "authorId": "50818605",
                        "name": "Anil Reddy Konduru"
                    },
                    {
                        "authorId": "4883630",
                        "name": "A. Mahajan"
                    },
                    {
                        "authorId": "40239871",
                        "name": "S. Rane"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several studies have been conducted on confidence calibration (Guo et al. 2017; Zhang, Dalca, and Sabuncu 2019; Thulasidasan et al. 2019; Wan et al. 2018; Kull et al. 2019).",
                "Starting with this paper, several confidence calibration methods are proposed (Zhang, Dalca, and Sabuncu 2019; Thulasidasan et al. 2019; Wan et al. 2018; Kull et al. 2019)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4b001c535aa5fdac765005641542e8710300bd7d",
                "externalIds": {
                    "DBLP": "conf/aaai/EnomotoE21",
                    "ArXiv": "2104.09286",
                    "DOI": "10.1609/aaai.v35i8.16900",
                    "CorpusId": 233296030
                },
                "corpusId": 233296030,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4b001c535aa5fdac765005641542e8710300bd7d",
                "title": "Learning to Cascade: Confidence Calibration for Improving the Accuracy and Computational Cost of Cascade Inference Systems",
                "abstract": "Recently, deep neural networks have become to be used in a variety of applications.\nWhile the accuracy of deep neural networks is increasing, the confidence score, which indicates the reliability of the prediction results, is becoming more important.\nDeep neural networks are seen as highly accurate but known to be overconfident, making it important to calibrate the confidence score.\nMany studies have been conducted on confidence calibration.\nThey calibrate the confidence score of the model to match its accuracy, but it is not clear whether these confidence scores can improve the performance of systems that use confidence scores.\nThis paper focuses on cascade inference systems, one kind of systems using confidence scores, and discusses the desired confidence score to improve system performance in terms of inference accuracy and computational cost.\nBased on the discussion, we propose a new confidence calibration method, Learning to Cascade.\nLearning to Cascade is a simple but novel method that optimizes the loss term for confidence calibration simultaneously with the original loss term.\nExperiments are conducted using two datasets, CIFAR-100 and ImageNet, in two system settings, and show that naive application of existing calibration methods to cascade inference systems sometimes performs worse.\nHowever, Learning to Cascade always achieves a better trade-off between inference accuracy and computational cost.\nThe simplicity of Learning to Cascade allows it to be easily applied to improve the performance of existing systems.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1381159698",
                        "name": "Shohei Enomoto"
                    },
                    {
                        "authorId": "3307195",
                        "name": "Takeharu Eda"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Uncertainty calibration performance is measured using the root-mean-square calibration error (RMSE), Expected Calibration Error (ECE) [43], [44], and Overconfidence Error (OE) [44]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "75a247a22888b1e4386e56897851a72aaac99f45",
                "externalIds": {
                    "DBLP": "conf/ijcnn/TsiligkaridisT23",
                    "ArXiv": "2104.01231",
                    "DOI": "10.1109/IJCNN54540.2023.10191763",
                    "CorpusId": 246241164
                },
                "corpusId": 246241164,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/75a247a22888b1e4386e56897851a72aaac99f45",
                "title": "Diverse Gaussian Noise Consistency Regularization for Robustness and Uncertainty Calibration",
                "abstract": "Deep neural networks achieve high prediction accuracy when the train and test distributions coincide. In practice though, various types of corruptions occur which deviate from this setup and cause severe performance degradations. Few methods have been proposed to address generalization in the presence of unforeseen domain shifts. In particular, digital noise corruptions arise commonly in practice during the image acquisition stage and present a significant challenge for current methods. In this paper, we propose a diverse Gaussian noise consistency regularization method for improving robustness of image classifiers under a variety of corruptions while still maintaining high clean accuracy. We derive bounds to motivate and understand the behavior of our Gaussian noise consistency regularization using a local loss landscape analysis. Our approach improves robustness against unforeseen noise corruptions by 4.2-18.4 % over adversarial training and other strong diverse data augmentation baselines across several benchmarks. Furthermore, it improves robustness and uncertainty calibration by 3.7% and 5.5 %, respectively, against all common corruptions (weather, digital, blur, noise) when combined with state-of-the-art diverse data augmentations. Code is available at https://github.com/TheoT1/DiGN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1902176",
                        "name": "Theodoros Tsiligkaridis"
                    },
                    {
                        "authorId": "3451485",
                        "name": "Athanasios Tsiligkaridis"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "78c51917366158513433f857cdaa5bce69798f61",
                "externalIds": {
                    "DBLP": "conf/cvpr/VenkataramananK22",
                    "ArXiv": "2103.15375",
                    "DOI": "10.1109/CVPR52688.2022.01858",
                    "CorpusId": 232404064
                },
                "corpusId": 232404064,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/78c51917366158513433f857cdaa5bce69798f61",
                "title": "AlignMixup: Improving Representations By Interpolating Aligned Features",
                "abstract": "Mixup is a powerful data augmentation method that in-terpolates between two or more examples in the input or feature space and between the corresponding target labels. However, how to best interpolate images is not well defined. Recent mixup methods overlay or cut-and-paste two or more objects into one image, which needs care in selecting regions. Mixup has also been connected to autoencoders, because often autoencoders generate an image that continuously deforms into another. However, such images are typically of low quality. In this work, we revisit mixup from the deformation perspective and introduce AligtiMixup, where we geometrically align two images in the feature space. The correspondences allow us to interpolate between two sets of features, while keeping the locations of one set. Interestingly, this retains mostly the geometry or pose of one image and the appearance or texture of the other. We also show that an autoencoder can still improve representation learning under mixup, without the classifier ever seeing decoded images. AlignMixup outperforms state-of-the-art mixup methods on five different benchmarks. Code available at https://github.com/shashankvkt/AlignMixup_CVPR22.git",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "39863130",
                        "name": "Shashanka Venkataramanan"
                    },
                    {
                        "authorId": "1744904",
                        "name": "Yannis Avrithis"
                    },
                    {
                        "authorId": "1801242",
                        "name": "Ewa Kijak"
                    },
                    {
                        "authorId": "1778357",
                        "name": "L. Amsaleg"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DNNs tend to predict over-confidently in classification tasks [47], mixup methods can significantly alleviate this problem."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "08695fdaa5544e22742a8aea1ddeb9808ab20ab2",
                "externalIds": {
                    "ArXiv": "2103.13027",
                    "DBLP": "conf/eccv/LiuLWLCWL22",
                    "DOI": "10.1007/978-3-031-20053-3_26",
                    "CorpusId": 238531789
                },
                "corpusId": 238531789,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/08695fdaa5544e22742a8aea1ddeb9808ab20ab2",
                "title": "AutoMix: Unveiling the Power of Mixup for Stronger Classifiers",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145253136",
                        "name": "Zicheng Liu"
                    },
                    {
                        "authorId": "2118155623",
                        "name": "Siyuan Li"
                    },
                    {
                        "authorId": "2118289574",
                        "name": "Di Wu"
                    },
                    {
                        "authorId": "48354556",
                        "name": "Zhiyuan Chen"
                    },
                    {
                        "authorId": "47767812",
                        "name": "Lirong Wu"
                    },
                    {
                        "authorId": "2148902244",
                        "name": "Jianzhu Guo"
                    },
                    {
                        "authorId": "1390908654",
                        "name": "Stan Z. Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We used object recognition datasets (CIFAR-10 and CIFAR-100 [8]) and a fashion-product recognition dataset (Fashion MNIST [21]) as in the previous study [17].",
                "As reported in [17], Mixup substantially reduces the ECE compared with the baseline method, but its ECE still increases to some extent when the training dataset becomes small-scale.",
                "MixConf basically follows the scheme of Mixup, which is known to contribute to model\u2019s calibration [17], but is more carefully designed for confidence calibration."
            ],
            "intents": [
                "methodology",
                "result"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7278da5dca8d6221d80dc63587c5d45f5c712a0b",
                "externalIds": {
                    "ArXiv": "2103.08193",
                    "DBLP": "journals/corr/abs-2103-08193",
                    "CorpusId": 232233402
                },
                "corpusId": 232233402,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7278da5dca8d6221d80dc63587c5d45f5c712a0b",
                "title": "Semi-supervised learning by selective training with pseudo labels via confidence estimation",
                "abstract": "We propose a novel semi-supervised learning (SSL) method that adopts selective training with pseudo labels. In our method, we generate hard pseudo-labels and also estimate their confidence, which represents how likely each pseudo-label is to be correct. Then, we explicitly select which pseudo-labeled data should be used to update the model. Specifically, assuming that loss on incorrectly pseudo-labeled data sensitively increase against data augmentation, we select the data corresponding to relatively small loss after applying data augmentation. The confidence is used not only for screening candidates of pseudo-labeled data to be selected but also for automatically deciding how many pseudo-labeled data should be selected within a mini-batch. Since accurate estimation of the confidence is crucial in our method, we also propose a new data augmentation method, called MixConf, that enables us to obtain confidence-calibrated models even when the number of training data is small. Experimental results with several benchmark datasets validate the advantage of our SSL method as well as MixConf.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1380363846",
                        "name": "Masato Ishii"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Subsequent work by (Thulasidasan et al., 2019) showed that mixup training and the soft decision boundary has the effect of improving network calibration."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f69cdac2aeb8868bf7f5e4f930edf8bd4a7b9fb9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-10803",
                    "ArXiv": "2102.10803",
                    "CorpusId": 231986255
                },
                "corpusId": 231986255,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f69cdac2aeb8868bf7f5e4f930edf8bd4a7b9fb9",
                "title": "Improving Uncertainty Calibration via Prior Augmented Data",
                "abstract": "Neural networks have proven successful at learning from complex data distributions by acting as universal function approximators. However, they are often overconfident in their predictions, which leads to inaccurate and miscalibrated probabilistic predictions. The problem of overconfidence becomes especially apparent in cases where the test-time data distribution differs from that which was seen during training. We propose a solution to this problem by seeking out regions of feature space where the model is unjustifiably overconfident, and conditionally raising the entropy of those predictions towards that of the prior distribution of the labels. Our method results in a better calibrated network and is agnostic to the underlying model structure, so it can be applied to any neural network which produces a probability density as an output. We demonstrate the effectiveness of our method and validate its performance on both classification and regression problems, applying it to recent probabilistic neural network models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2051330801",
                        "name": "Jeffrey Willette"
                    },
                    {
                        "authorId": "2108550899",
                        "name": "Juho Lee"
                    },
                    {
                        "authorId": "35788904",
                        "name": "Sung Ju Hwang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Thulasidasan et al. (2019) show that MixUp can improve calibration for CNN based models."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6a6ad8dd1b1ac7bb0544912825e85ff733c580d8",
                "externalIds": {
                    "ArXiv": "2102.11402",
                    "DBLP": "journals/corr/abs-2102-11402",
                    "CorpusId": 232013527
                },
                "corpusId": 232013527,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6a6ad8dd1b1ac7bb0544912825e85ff733c580d8",
                "title": "MixUp Training Leads to Reduced Overfitting and Improved Calibration for the Transformer Architecture",
                "abstract": "MixUp is a computer vision data augmentation technique that uses convex interpolations of input data and their labels to enhance model generalization during training. However, the application of MixUp to the natural language understanding (NLU) domain has been limited, due to the difficulty of interpolating text directly in the input space. In this study, we propose MixUp methods at the Input, Manifold, and sentence embedding levels for the transformer architecture, and apply them to finetune the BERT model for a diverse set of NLU tasks. We find that MixUp can improve model performance, as well as reduce test loss and model calibration error by up to 50%.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108277713",
                        "name": "Wancong Zhang"
                    },
                    {
                        "authorId": "84487571",
                        "name": "I. Vaidya"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mixup has successfully been used as a form of data augmentation in image classification, improving generalization and calibration [32, 26]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "125169330bf7c0dac2d5b653a32a04f71518ed02",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-09517",
                    "ArXiv": "2102.09517",
                    "DOI": "10.1109/CVPRW53098.2021.00390",
                    "CorpusId": 231951462
                },
                "corpusId": 231951462,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/125169330bf7c0dac2d5b653a32a04f71518ed02",
                "title": "Essentials for Class Incremental Learning",
                "abstract": "Contemporary neural networks are limited in their ability to learn from evolving streams of training data. When trained sequentially on new or evolving tasks, their accuracy drops sharply, making them unsuitable for many real-world applications. In this work, we shed light on the causes of this well known yet unsolved phenomenon - often referred to as catastrophic forgetting - in a class-incremental setup. We show that a combination of simple components and a loss that balances intra-task and intertask learning can already resolve forgetting to the same extent as more complex measures proposed in literature. Moreover, we identify poor quality of the learned representation as another reason for catastrophic forgetting in class-IL. We show that performance is correlated with secondary class information (dark knowledge) learned by the model and it can be improved by an appropriate regularizer. With these lessons learned, class-incremental learning results on CIFAR-100 and ImageNet improve over the state-of-the-art by a large margin, while keeping the approach simple.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9452482",
                        "name": "Sudhanshu Mittal"
                    },
                    {
                        "authorId": "36011789",
                        "name": "Silvio Galesso"
                    },
                    {
                        "authorId": "1710872",
                        "name": "T. Brox"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In standard training (ST), mixup has been widely used to improve the generalization (Zhang et al., 2018; Thulasidasan et al., 2019; Berthelot et al., 2019; 2020; Kim et al., 2021; Zhang et al., 2021b)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5dbc41ce67d979a1e3b7099577a4827c99119d3b",
                "externalIds": {
                    "ArXiv": "2102.07327",
                    "DBLP": "journals/corr/abs-2102-07327",
                    "CorpusId": 231925169
                },
                "corpusId": 231925169,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5dbc41ce67d979a1e3b7099577a4827c99119d3b",
                "title": "Guided Interpolation for Adversarial Training",
                "abstract": "To enhance adversarial robustness, adversarial training learns deep neural networks on the adversarial variants generated by their natural data. However, as the training progresses, the training data becomes less and less attackable, undermining the robustness enhancement. A straightforward remedy is to incorporate more training data, but sometimes incurring an unaffordable cost. In this paper, to mitigate this issue, we propose the guided interpolation framework (GIF): in each epoch, the GIF employs the previous epoch's meta information to guide the data's interpolation. Compared with the vanilla mixup, the GIF can provide a higher ratio of attackable data, which is beneficial to the robustness enhancement; it meanwhile mitigates the model's linear behavior between classes, where the linear behavior is favorable to generalization but not to the robustness. As a result, the GIF encourages the model to predict invariantly in the cluster of each class. Experiments demonstrate that the GIF can indeed enhance adversarial robustness on various adversarial training methods and various datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1828568",
                        "name": "Chen Chen"
                    },
                    {
                        "authorId": "47539929",
                        "name": "Jingfeng Zhang"
                    },
                    {
                        "authorId": "1507120550",
                        "name": "Xilie Xu"
                    },
                    {
                        "authorId": "2615734",
                        "name": "Tianlei Hu"
                    },
                    {
                        "authorId": "47537639",
                        "name": "Gang Niu"
                    },
                    {
                        "authorId": "1887510095",
                        "name": "Gang Chen"
                    },
                    {
                        "authorId": "67154907",
                        "name": "Masashi Sugiyama"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "62cf516ff55454a0d3ed2f380fcf74837209ba08",
                "externalIds": {
                    "DBLP": "conf/icml/BaiMWX21",
                    "ArXiv": "2102.07856",
                    "CorpusId": 231933881
                },
                "corpusId": 231933881,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/62cf516ff55454a0d3ed2f380fcf74837209ba08",
                "title": "Don't Just Blame Over-parametrization for Over-confidence: Theoretical Analysis of Calibration in Binary Classification",
                "abstract": "Modern machine learning models with high accuracy are often miscalibrated -- the predicted top probability does not reflect the actual accuracy, and tends to be over-confident. It is commonly believed that such over-confidence is mainly due to over-parametrization, in particular when the model is large enough to memorize the training data and maximize the confidence. In this paper, we show theoretically that over-parametrization is not the only reason for over-confidence. We prove that logistic regression is inherently over-confident, in the realizable, under-parametrized setting where the data is generated from the logistic model, and the sample size is much larger than the number of parameters. Further, this over-confidence happens for general well-specified binary classification problems as long as the activation is symmetric and concave on the positive part. Perhaps surprisingly, we also show that over-confidence is not always the case -- there exists another activation function (and a suitable loss function) under which the learned classifier is under-confident at some probability values. Overall, our theory provides a precise characterization of calibration in realizable binary classification, which we verify on simulations and real data experiments.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1491626939",
                        "name": "Yu Bai"
                    },
                    {
                        "authorId": "2068869988",
                        "name": "Song Mei"
                    },
                    {
                        "authorId": "46507194",
                        "name": "Haiquan Wang"
                    },
                    {
                        "authorId": "2228109",
                        "name": "Caiming Xiong"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3faf8099f6ada8ef1ff19a6fc1942ec606167aad",
                "externalIds": {
                    "DOI": "10.1117/12.2582290",
                    "CorpusId": 231883843
                },
                "corpusId": 231883843,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3faf8099f6ada8ef1ff19a6fc1942ec606167aad",
                "title": "Self-training with improved regularization for sample-efficient chest x-ray classification",
                "abstract": "Automated diagnostic assistants in healthcare necessitate accurate AI models that can be trained with limited labeled data, can cope with severe class imbalances and can support simultaneous prediction of multiple disease conditions. To this end, we present a deep learning framework that utilizes a number of key components to enable robust modeling in such challenging scenarios. Using an important use-case in chest X-ray classification, we provide several key insights on the effective use of data augmentation, self-training via distillation and confidence tempering for small data learning in medical imaging. Our results show that using 85% lesser labeled data, we can build predictive models that match the performance of classifiers trained in a large-scale data setting.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145882781",
                        "name": "Deepta Rajan"
                    },
                    {
                        "authorId": "1744175",
                        "name": "Jayaraman J. Thiagarajan"
                    },
                    {
                        "authorId": "2308391",
                        "name": "A. Karargyris"
                    },
                    {
                        "authorId": "33201965",
                        "name": "Satyananda Kashyap"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018), and its predictions tend to be overconfident (Malkin and Bilmes, 2009; Thulasidasan et al., 2019).",
                "Large-scale pretrained models are not well calibrated (Jiang et al., 2018), and its predictions tend to be overconfident (Malkin and Bilmes, 2009; Thulasidasan et al., 2019).",
                "When trained naively with cross entropy loss on unambiguously annotated data (examples with a single label), models generate a over-confident distribution (Thulasidasan et al., 2019) putting a strong weight on a single label.",
                "When\ntrained naively with cross entropy loss on unambiguously annotated data (examples with a single label), models generate a over-confident distribution (Thulasidasan et al., 2019) putting a strong weight on a single label."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a9b338b0817e0e554052ed5565d9cfc743aa85bb",
                "externalIds": {
                    "ArXiv": "2102.06859",
                    "DBLP": "journals/corr/abs-2102-06859",
                    "CorpusId": 231925365
                },
                "corpusId": 231925365,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a9b338b0817e0e554052ed5565d9cfc743aa85bb",
                "title": "Capturing Label Distribution: A Case Study in NLI",
                "abstract": "We study estimating inherent human disagreement (annotation label distribution) in natural language inference task. Post-hoc smoothing of the predicted label distribution to match the expected label entropy is very effective. Such simple manipulation can reduce KL divergence by almost half, yet will not improve majority label prediction accuracy or learn label distributions. To this end, we introduce a small amount of examples with multiple references into training. We depart from the standard practice of collecting a single reference per each training example, and find that collecting multiple references can achieve better accuracy under the fixed annotation budget. Lastly, we provide rich analyses comparing these two methods for improving label distribution estimation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2107944048",
                        "name": "Shujian Zhang"
                    },
                    {
                        "authorId": "29777869",
                        "name": "Chengyue Gong"
                    },
                    {
                        "authorId": "2890423",
                        "name": "Eunsol Choi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a32a2be00294e255b93ae37e9bb1f9995f488277",
                "externalIds": {
                    "DBLP": "conf/icml/ZhangDK022",
                    "ArXiv": "2102.06289",
                    "CorpusId": 231918640
                },
                "corpusId": 231918640,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a32a2be00294e255b93ae37e9bb1f9995f488277",
                "title": "When and How Mixup Improves Calibration",
                "abstract": "In many machine learning applications, it is important for the model to provide confidence scores that accurately capture its prediction uncertainty. Although modern learning methods have achieved great success in predictive accuracy, generating calibrated confidence scores remains a major challenge. Mixup, a popular yet simple data augmentation technique based on taking convex combinations of pairs of training examples, has been empirically found to significantly improve confidence calibration across diverse applications. However, when and how Mixup helps calibration is still a mystery. In this paper, we theoretically prove that Mixup improves calibration in \\textit{high-dimensional} settings by investigating natural statistical models. Interestingly, the calibration benefit of Mixup increases as the model capacity increases. We support our theories with experiments on common architectures and datasets. In addition, we study how Mixup improves calibration in semi-supervised learning. While incorporating unlabeled data can sometimes make the model less calibrated, adding Mixup training mitigates this issue and provably improves calibration. Our analysis provides new insights and a framework to understand Mixup and calibration.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "10537441",
                        "name": "Linjun Zhang"
                    },
                    {
                        "authorId": "10394991",
                        "name": "Zhun Deng"
                    },
                    {
                        "authorId": "1392876047",
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "authorId": "145085305",
                        "name": "James Y. Zou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similar approaches where the proxies or auxiliary data is created using tools such as generative adversarial networks are reported in [22, 14, 27, 31]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1d439a001f985fd846e903124f8a4b42a40e434a",
                "externalIds": {
                    "ArXiv": "2102.01336",
                    "CorpusId": 236428670
                },
                "corpusId": 236428670,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1d439a001f985fd846e903124f8a4b42a40e434a",
                "title": "Probabilistic Trust Intervals for Out of Distribution Detection",
                "abstract": "Building neural network classifiers with an ability to distinguish between in and out-of distribution inputs is an important step towards faithful deep learning systems. Some of the successful approaches for this, resort to architectural novelties, such as ensembles, with increased complexities in terms of the number of parameters and training procedures. Whereas some other approaches make use of surrogate samples, which are easy to create and work as proxies for actual out-of-distribution (OOD) samples, to train the networks for OOD detection. In this paper, we propose a very simple approach for enhancing the ability of a pretrained network to detect OOD inputs without even altering the original parameter values. We define a probabilistic trust interval for each weight parameter of the network and optimize its size according to the in-distribution (ID) inputs. It allows the network to sample additional weight values along with the original values at the time of inference and use the observed disagreement among the corresponding outputs for OOD detection. In order to capture the disagreement effectively, we also propose a measure and establish its suitability using empirical evidence. Our approach outperforms the existing state-of-the-art methods on various OOD datasets by considerable margins without using any real or surrogate OOD samples. We also analyze the performance of our approach on adversarial and corrupted inputs such as CIFAR-10-C and demonstrate its ability to clearly distinguish such inputs as well. By using fundamental theorem of calculus on neural networks, we explain why our technique doesn't need to observe OOD samples during training to achieve results better than the previous works.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "122085719",
                        "name": "Gagandeep Singh"
                    },
                    {
                        "authorId": "2082316146",
                        "name": "Deepak Mishra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Since the output prediction with mixup augmentation is better calibrated (Thulasidasan et al., 2019) we use relaxed thresholds for \u03c4p (0.50) and \u03bap (0.10)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a21792db1c8d80c1d1f8525dab4959cc60b8e0ea",
                "externalIds": {
                    "ArXiv": "2101.06329",
                    "DBLP": "conf/iclr/RizveDRS21",
                    "CorpusId": 231632854
                },
                "corpusId": 231632854,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a21792db1c8d80c1d1f8525dab4959cc60b8e0ea",
                "title": "In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning",
                "abstract": "The recent research in semi-supervised learning (SSL) is mostly dominated by consistency regularization based methods which achieve strong performance. However, they heavily rely on domain-specific data augmentations, which are not easy to generate for all data modalities. Pseudo-labeling (PL) is a general SSL approach that does not have this constraint but performs relatively poorly in its original formulation. We argue that PL underperforms due to the erroneous high confidence predictions from poorly calibrated models; these predictions generate many incorrect pseudo-labels, leading to noisy training. We propose an uncertainty-aware pseudo-label selection (UPS) framework which improves pseudo labeling accuracy by drastically reducing the amount of noise encountered in the training process. Furthermore, UPS generalizes the pseudo-labeling process, allowing for the creation of negative pseudo-labels; these negative pseudo-labels can be used for multi-label classification as well as negative learning to improve the single-label classification. We achieve strong performance when compared to recent SSL methods on the CIFAR-10 and CIFAR-100 datasets. Also, we demonstrate the versatility of our method on the video dataset UCF-101 and the multi-label dataset Pascal VOC.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9247631",
                        "name": "Mamshad Nayeem Rizve"
                    },
                    {
                        "authorId": "2064921899",
                        "name": "Kevin Duarte"
                    },
                    {
                        "authorId": "2116440",
                        "name": "Y. Rawat"
                    },
                    {
                        "authorId": "145103012",
                        "name": "M. Shah"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The literature on general confidence scoring is rich and continually evolving; the most interesting research avenues involve Bayesian averaging [26], generative models [27, 28], input perturbations [29, 30], exploiting inner activations [31, 32]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d6d98430ecaa9f994ce02726567ebca473230a7f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-05525",
                    "ArXiv": "2101.05525",
                    "DOI": "10.1109/SLT48900.2021.9383570",
                    "CorpusId": 231603281
                },
                "corpusId": 231603281,
                "publicationVenue": {
                    "id": "d8dfb5ba-9312-410c-a361-8ad05f945939",
                    "name": "Spoken Language Technology Workshop",
                    "type": "conference",
                    "alternate_names": [
                        "SLT",
                        "Spok Lang Technol Workshop"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d6d98430ecaa9f994ce02726567ebca473230a7f",
                "title": "An Evaluation of Word-Level Confidence Estimation for End-to-End Automatic Speech Recognition",
                "abstract": "Quantifying the confidence (or conversely the uncertainty) of a prediction is a highly desirable trait of an automatic system, as it improves the robustness and usefulness in downstream tasks. In this paper we investigate confidence estimation for end-to-end automatic speech recognition (ASR). Previous work has addressed confidence measures for lattice-based ASR, while current machine learning research mostly focuses on confidence measures for unstructured deep learning. However, as the ASR systems are increasingly being built upon deep end-to-end methods, there is little work that tries to develop confidence measures in this context. We fill this gap by providing an extensive benchmark of popular confidence methods on four well-known speech datasets. There are two challenges we overcome in adapting existing methods: working on structured data (sequences) and obtaining confidences at a coarser level than the predictions (words instead of tokens). Our results suggest that a strong baseline can be obtained by scaling the logits by a learnt temperature, followed by estimating the confidence as the negative entropy of the predictive distribution and, finally, sum pooling to aggregate at word level.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3095774",
                        "name": "Dan Onea\u0163\u0103"
                    },
                    {
                        "authorId": "2017546",
                        "name": "Alexandru Caranica"
                    },
                    {
                        "authorId": "2222977",
                        "name": "Adriana Stan"
                    },
                    {
                        "authorId": "1700021",
                        "name": "H. Cucu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mixup can be regarded as a kind of data augmentation method and it often enhances the generalization performance of CNNs. Mixup can also ease the over-confident prediction problem for deep neural networks (Thulasidasan et al. 2019).",
                "Therefore, mixup involves the representation space\nunseen during normal training, and raises the generalization of deep neural networks significantly, especially on small datasets (Thulasidasan et al. 2019).",
                "However, \u03b1 greater than 1 tends to perform better in some cases (Thulasidasan et al. 2019).",
                "Mixup can also ease the over-confident prediction problem for deep neural networks (Thulasidasan et al. 2019).",
                "Therefore, mixup involves the representation space unseen during normal training, and raises the generalization of deep neural networks significantly, especially on small datasets (Thulasidasan et al. 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "3cd6f6a1f2aaa9663f05b259fb928151473b2d85",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-04342",
                    "ArXiv": "2101.04342",
                    "DOI": "10.1007/978-3-030-87358-5_12",
                    "CorpusId": 231582982
                },
                "corpusId": 231582982,
                "publicationVenue": {
                    "id": "51de135d-ed73-4b1f-912e-624ffc3d7cc9",
                    "name": "International Conference on Image and Graphics",
                    "type": "conference",
                    "alternate_names": [
                        "ICIG",
                        "Int Conf Image Graph"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1387"
                },
                "url": "https://www.semanticscholar.org/paper/3cd6f6a1f2aaa9663f05b259fb928151473b2d85",
                "title": "Mixup Without Hesitation",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110750027",
                        "name": "Hao Yu"
                    },
                    {
                        "authorId": "2046522381",
                        "name": "Huanyu Wang"
                    },
                    {
                        "authorId": "1808816",
                        "name": "Jianxin Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Research in computer vision has shown that improving model calibration improves adversarial robustness as well as out-of-distribution detection (Hendrycks and Gimpel 2017; Thulasidasan et al. 2019; Hendrycks, Lee, and Mazeika 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7defc117a11c16fb70bea6cbc0b58e48244992c8",
                "externalIds": {
                    "ArXiv": "2101.03453",
                    "DBLP": "conf/aaai/GuptaKS21",
                    "DOI": "10.1609/aaai.v35i14.17531",
                    "CorpusId": 231573267
                },
                "corpusId": 231573267,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7defc117a11c16fb70bea6cbc0b58e48244992c8",
                "title": "BERT & Family Eat Word Salad: Experiments with Text Understanding",
                "abstract": "In this paper, we study the response of large models from the BERT family to incoherent inputs that should confuse any model that claims to understand natural language. We define simple heuristics to construct such examples. Our experiments show that state-of-the-art models consistently fail to recognize them as ill-formed, and instead produce high confidence predictions on them. As a consequence of this phenomenon, models trained on sentences with randomly permuted word order perform close to state-of-the-art models. To alleviate these issues, we show that if models are explicitly trained to recognize invalid inputs, they can be robust to such attacks without a drop in performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "15634433",
                        "name": "Ashim Gupta"
                    },
                    {
                        "authorId": "48232381",
                        "name": "Giorgi Kvernadze"
                    },
                    {
                        "authorId": "3052879",
                        "name": "Vivek Srikumar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In [30], for instance, has been measured the robustness and calibration of Mixup training [31] showing improved results over a baseline model."
            ],
            "intents": [
                "result"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2b07149d0d891138bc4064b7386eb23aca18b7a6",
                "externalIds": {
                    "DBLP": "conf/icpr/PollastriMBLPMG20",
                    "DOI": "10.1109/ICPR48806.2021.9412685",
                    "CorpusId": 232286164
                },
                "corpusId": 232286164,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2b07149d0d891138bc4064b7386eb23aca18b7a6",
                "title": "Confidence Calibration for Deep Renal Biopsy Immunofluorescence Image Classification",
                "abstract": "With this work we tackle immunofluorescence classification in renal biopsy, employing state-of-the-art Convolutional Neural Networks. In this setting, the aim of the probabilistic model is to assist an expert practitioner towards identifying the location pattern of antibody deposits within a glomerulus. Since modern neural networks often provide overconfident outputs, we stress the importance of having a reliable prediction, demonstrating that Temperature Scaling (TS), a recently introduced re-calibration technique, can be successfully applied to immunofluorescence classification in renal biopsy. Experimental results demonstrate that the designed model yields good accuracy on the specific task, and that TS is able to provide reliable probabilities, which are highly valuable for such a task given the low inter-rater agreement.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51133784",
                        "name": "F. Pollastri"
                    },
                    {
                        "authorId": "66769721",
                        "name": "Juan Maro\u00f1as"
                    },
                    {
                        "authorId": "3490384",
                        "name": "Federico Bolelli"
                    },
                    {
                        "authorId": "40607759",
                        "name": "G. Ligabue"
                    },
                    {
                        "authorId": "145111869",
                        "name": "Roberto Paredes Palacios"
                    },
                    {
                        "authorId": "2051893",
                        "name": "R. Magistroni"
                    },
                    {
                        "authorId": "1705203",
                        "name": "C. Grana"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1dce7b802b187052b10f6f0ae0f6de71be5c4b24",
                "externalIds": {
                    "DBLP": "conf/bigcomp/HanLCL21",
                    "DOI": "10.1109/BigComp51126.2021.00073",
                    "CorpusId": 232236974
                },
                "corpusId": 232236974,
                "publicationVenue": {
                    "id": "88cc76ce-f486-46c4-bfa1-a31809d402d7",
                    "name": "International Conference on Big Data and Smart Computing",
                    "type": "conference",
                    "alternate_names": [
                        "BIGCOMP",
                        "BigComp",
                        "Int Conf Big Data Smart Comput"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1dce7b802b187052b10f6f0ae0f6de71be5c4b24",
                "title": "An Empirical Study for Class Imbalance in Extreme Multi-label Text Classification",
                "abstract": "Extreme multi-label text classification (XMTC) is the problem of finding the most relevant multi-labels from a text corpus with millions of labels. One of the key challenges in XMTC is that most labels appear only a few times, i.e., the class imbalance issue. To overcome the class imbalance problem, existing studies suggested various methods using different loss functions (i.e., focal loss function) and data augmentation (i.e., mix-up). In this paper, we investigate the effectiveness of two main approaches over the RNN-based and transformer-based deep XMTC models. In experimental results, we found that some improvement can be achieved when focal loss and mix-up are applied for deep XMTC models on various datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47595490",
                        "name": "Sangwoo Han"
                    },
                    {
                        "authorId": "144245091",
                        "name": "Chanmann Lim"
                    },
                    {
                        "authorId": "2053865237",
                        "name": "Bonggeon Cha"
                    },
                    {
                        "authorId": "1865093",
                        "name": "Jongwuk Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", mixup[9, 14], label smoothing [5], multiscale [2], various data augmentations [6], can bring accuracy improvements."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8f7dd966a7d7b125a83745b02b13dc3b0fc56269",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-14672",
                    "ArXiv": "2012.14672",
                    "CorpusId": 229923639
                },
                "corpusId": 229923639,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8f7dd966a7d7b125a83745b02b13dc3b0fc56269",
                "title": "Tips and Tricks for Webly-Supervised Fine-Grained Recognition: Learning from the WebFG 2020 Challenge",
                "abstract": "WebFG 2020 is an international challenge hosted by Nanjing University of Science and Technology, University of Edinburgh, Nanjing University, The University of Adelaide, Waseda University, etc. This challenge mainly pays attention to the webly-supervised fine-grained recognition problem. In the literature, existing deep learning methods highly rely on large-scale and high-quality labeled training data, which poses a limitation to their practicability and scalability in real world applications. In particular, for fine-grained recognition, a visual task that requires professional knowledge for labeling, the cost of acquiring labeled training data is quite high. It causes extreme difficulties to obtain a large amount of high-quality training data. Therefore, utilizing free web data to train fine-grained recognition models has attracted increasing attentions from researchers in the fine-grained community. This challenge expects participants to develop webly-supervised fine-grained recognition methods, which leverages web images in training fine-grained recognition models to ease the extreme dependence of deep learning methods on large-scale manually labeled datasets and to enhance their practicability and scalability. In this technical report, we have pulled together the top WebFG 2020 solutions of total 54 competing teams, and discuss what methods worked best across the set of winning teams, and what surprisingly did not help.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2126047",
                        "name": "Xiu-Shen Wei"
                    },
                    {
                        "authorId": "1751674298",
                        "name": "Yu-Yan Xu"
                    },
                    {
                        "authorId": "2436931",
                        "name": "Yazhou Yao"
                    },
                    {
                        "authorId": "2141318817",
                        "name": "Jia Wei"
                    },
                    {
                        "authorId": "51307517",
                        "name": "Si Xi"
                    },
                    {
                        "authorId": "2152923713",
                        "name": "Wenyuan Xu"
                    },
                    {
                        "authorId": "2108134214",
                        "name": "Weidong Zhang"
                    },
                    {
                        "authorId": "2112410679",
                        "name": "Xiaoxin Lv"
                    },
                    {
                        "authorId": "32718221",
                        "name": "Dengpan Fu"
                    },
                    {
                        "authorId": "2117896825",
                        "name": "Qing Li"
                    },
                    {
                        "authorId": "2108425565",
                        "name": "Baoying Chen"
                    },
                    {
                        "authorId": "2036403384",
                        "name": "Haojie Guo"
                    },
                    {
                        "authorId": "1589400026",
                        "name": "Taolue Xue"
                    },
                    {
                        "authorId": "50632930",
                        "name": "Haipeng Jing"
                    },
                    {
                        "authorId": "2108346871",
                        "name": "Zhiheng Wang"
                    },
                    {
                        "authorId": "1390715741",
                        "name": "Tianming Zhang"
                    },
                    {
                        "authorId": "48984973",
                        "name": "Mingwen Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mixup (Zhang et al. 2018) and its variants (Verma et al. 2018; Thulasidasan et al. 2019) are a recently proposed approach to improve model robustness and generalization by training a model on convex combinations of data sample pairs and their labels.",
                "2018) and its variants (Verma et al. 2018; Thulasidasan et al. 2019) are a recently proposed approach to improve model robustness and generalization by training a model on convex combinations of data sample pairs and their labels."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "73aa94064c4c2e592ddad579c51960c5838828b8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-11769",
                    "ArXiv": "2012.11769",
                    "DOI": "10.1609/aaai.v35i8.16874",
                    "CorpusId": 229349128
                },
                "corpusId": 229349128,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/73aa94064c4c2e592ddad579c51960c5838828b8",
                "title": "Self-Progressing Robust Training",
                "abstract": "Enhancing model robustness under new and even adversarial environments is a crucial milestone toward building trustworthy machine learning systems. Current robust training methods such as adversarial training explicitly uses an ``attack'' (e.g., l_infty-norm bounded perturbation) to generate adversarial examples during model training for improving adversarial robustness. In this paper, we take a different perspective and propose a new framework SPROUT, self-progressing robust training. During model training, SPROUT progressively adjusts training label distribution via our proposed parametrized label smoothing technique, making training free of attack generation and more scalable. We also motivate SPROUT using a general formulation based on vicinity risk minimization, which includes many robust training methods as special cases.\nCompared with state-of-the-art adversarial training methods (PGD-l_infty and TRADES) under l_infty-norm bounded attacks and various invariance tests, SPROUT consistently attains superior performance and is more scalable to large neural networks. Our results shed new light on scalable, effective and attack-independent robust training methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2424698",
                        "name": "Minhao Cheng"
                    },
                    {
                        "authorId": "153191489",
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "authorId": "143743061",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "3307026",
                        "name": "Shiyu Chang"
                    },
                    {
                        "authorId": "1793529",
                        "name": "Cho-Jui Hsieh"
                    },
                    {
                        "authorId": "1730372",
                        "name": "Payel Das"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Data augmentation methods include Mixup [19] and AugMix [21]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "eccfae5f09387461a3b9aae3838cc3d326b68816",
                "externalIds": {
                    "ArXiv": "2012.07923",
                    "DBLP": "conf/nips/KrishnanT20",
                    "MAG": "3104448368",
                    "CorpusId": 227275177
                },
                "corpusId": 227275177,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/eccfae5f09387461a3b9aae3838cc3d326b68816",
                "title": "Improving model calibration with accuracy versus uncertainty optimization",
                "abstract": "Obtaining reliable and accurate quantification of uncertainty estimates from deep neural networks is important in safety-critical applications. A well-calibrated model should be accurate when it is certain about its prediction and indicate high uncertainty when it is likely to be inaccurate. Uncertainty calibration is a challenging problem as there is no ground truth available for uncertainty estimates. We propose an optimization method that leverages the relationship between accuracy and uncertainty as an anchor for uncertainty calibration. We introduce a differentiable accuracy versus uncertainty calibration (AvUC) loss function that allows a model to learn to provide well-calibrated uncertainties, in addition to improved accuracy. We also demonstrate the same methodology can be extended to post-hoc uncertainty calibration on pretrained models. We illustrate our approach with mean-field stochastic variational inference and compare with state-of-the-art methods. Extensive experiments demonstrate our approach yields better model calibration than existing methods on large-scale image classification tasks under distributional shift.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "13026224",
                        "name": "R. Krishnan"
                    },
                    {
                        "authorId": "1798616",
                        "name": "Omesh Tickoo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[39] discovered that mixuptraining [52] with label smoothing can significantly improve model calibration."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d63c0cdfc661bb91a1ab8bb716e55b685d07a00c",
                "externalIds": {
                    "ArXiv": "2012.06020",
                    "DBLP": "journals/corr/abs-2012-06020",
                    "MAG": "3111016417",
                    "CorpusId": 228372611
                },
                "corpusId": 228372611,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d63c0cdfc661bb91a1ab8bb716e55b685d07a00c",
                "title": "Uncertainty-Aware Deep Calibrated Salient Object Detection",
                "abstract": "Existing deep neural network based salient object detection (SOD) methods mainly focus on pursuing high network accuracy. However, those methods overlook the gap between network accuracy and prediction confidence, known as the confidence uncalibration problem. Thus, state-of-the-art SOD networks are prone to be overconfident. In other words, the predicted confidence of the networks does not reflect the real probability of correctness of salient object detection, which significantly hinder their real-world applicability. In this paper, we introduce an uncertaintyaware deep SOD network, and propose two strategies from different perspectives to prevent deep SOD networks from being overconfident. The first strategy, namely Boundary Distribution Smoothing (BDS), generates continuous labels by smoothing the original binary ground-truth with respect to pixel-wise uncertainty. The second strategy, namely Uncertainty-Aware Temperature Scaling (UATS), exploits a relaxed Sigmoid function during both training and testing with spatially-variant temperature scaling to produce softened output. Both strategies can be incorporated into existing deep SOD networks with minimal efforts. Moreover, we propose a new saliency evaluation metric, namely dense calibration measure C, to measure how the model is calibrated on a given dataset. Extensive experimental results on seven benchmark datasets demonstrate that our solutions can not only better calibrate SOD models, but also improve the network accuracy.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2155698491",
                        "name": "Jing Zhang"
                    },
                    {
                        "authorId": "1681554",
                        "name": "Yuchao Dai"
                    },
                    {
                        "authorId": "1490933487",
                        "name": "Xin Yu"
                    },
                    {
                        "authorId": "1686714",
                        "name": "M. Harandi"
                    },
                    {
                        "authorId": "1712576",
                        "name": "N. Barnes"
                    },
                    {
                        "authorId": "143750012",
                        "name": "R. Hartley"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is because the commonly used cross-entropy loss is known to be highly overconfident [27, 40].",
                "As the commonly used cross-entropy loss is known to be highly overconfident [27, 40], LossNet tends to produce polarized results, and high weights could be assigned to some noisy data."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a298fd2bbad144a84adec7c2d4644764ad5f20a3",
                "externalIds": {
                    "ArXiv": "2012.05273",
                    "DBLP": "journals/corr/abs-2012-05273",
                    "MAG": "3112642831",
                    "CorpusId": 228083649
                },
                "corpusId": 228083649,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a298fd2bbad144a84adec7c2d4644764ad5f20a3",
                "title": "MetaInfoNet: Learning Task-Guided Information for Sample Reweighting",
                "abstract": "Deep neural networks have been shown to easily overfit to biased training data with label noise or class imbalance. Meta-learning algorithms are commonly designed to alleviate this issue in the form of sample reweighting, by learning a meta weighting network that takes training losses as inputs to generate sample weights. In this paper, we advocate that choosing proper inputs for the meta weighting network is crucial for desired sample weights in a specific task, while training loss is not always the correct answer. In view of this, we propose a novel meta-learning algorithm, MetaInfoNet, which automatically learns effective representations as inputs for the meta weighting network by emphasizing task-related information with an information bottleneck strategy. Extensive experimental results on benchmark datasets with label noise or class imbalance validate that MetaInfoNet is superior to many state-of-the-art methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2115312735",
                        "name": "Hongxin Wei"
                    },
                    {
                        "authorId": "2117930471",
                        "name": "Lei Feng"
                    },
                    {
                        "authorId": "103856029",
                        "name": "R. Wang"
                    },
                    {
                        "authorId": "143706343",
                        "name": "Bo An"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is increasingly being recognized that decision-making systems must be both accurate and calibrated (Wallace and Dahabreh 2012; Guo et al. 2017; Thulasidasan et al. 2019; Huang et al. 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5a88e260ecb841c9f102039b09b6051fcf61589f",
                "externalIds": {
                    "ArXiv": "2012.02312",
                    "CorpusId": 227305783
                },
                "corpusId": 227305783,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5a88e260ecb841c9f102039b09b6051fcf61589f",
                "title": "ReMix: Calibrated Resampling for Class Imbalance in Deep learning",
                "abstract": "Class imbalance is a problem of significant importance in applied deep learning where trained models are exploited for decision support and automated decisions in critical areas such as health and medicine, transportation, and finance. The challenge of learning deep models from imbalanced training data remains high, and the state-of-the-art solutions are typically data dependent and primarily focused on image data. Real-world imbalanced classification problems, however, are much more diverse thus necessitating a general solution that can be applied to tabular, image and text data. In this paper, we propose ReMix, a training technique that leverages batch resampling, instance mixing and soft-labels to enable the induction of robust deep models for imbalanced learning. Our results show that dense nets and CNNs trained with ReMix generally outperform the alternatives according to the g-mean and are better calibrated according to the balanced Brier score.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1771538",
                        "name": "C. Bellinger"
                    },
                    {
                        "authorId": "2294473",
                        "name": "Roberto Corizzo"
                    },
                    {
                        "authorId": "1743642",
                        "name": "N. Japkowicz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is increasingly being recognized that decision-making systems must be both accurate and calibrated (Wallace and Dahabreh 2012; Guo et al. 2017; Thulasidasan et al. 2019; Huang et al. 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c2305add700ca49be728cb255d4b7eaf40d1bb66",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-02312",
                    "MAG": "3111952214",
                    "CorpusId": 260507773
                },
                "corpusId": 260507773,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c2305add700ca49be728cb255d4b7eaf40d1bb66",
                "title": "ReMix: Calibrated Resampling for Class Imbalance in Deep learning",
                "abstract": "Class imbalance is a problem of significant importance in applied deep learning where trained models are exploited for decision support and automated decisions in critical areas such as health and medicine, transportation, and finance. The challenge of learning deep models from imbalanced training data remains high, and the state-of-the-art solutions are typically data dependent and primarily focused on image data. Real-world imbalanced classification problems, however, are much more diverse thus necessitating a general solution that can be applied to tabular, image and text data. In this paper, we propose ReMix, a training technique that leverages batch resampling, instance mixing and soft-labels to enable the induction of robust deep models for imbalanced learning. Our results show that dense nets and CNNs trained with ReMix generally outperform the alternatives according to the g-mean and are better calibrated according to the balanced Brier score.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1771538",
                        "name": "C. Bellinger"
                    },
                    {
                        "authorId": "2294473",
                        "name": "Roberto Corizzo"
                    },
                    {
                        "authorId": "1743642",
                        "name": "N. Japkowicz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, [39] notices that label smoothing during mixup training has a calibration effect which regularizes over-confident predictions."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "927ed8655721357f81137ba117b56a41e3587a9e",
                "externalIds": {
                    "DBLP": "conf/bmvc/ChuZ022",
                    "ArXiv": "2011.13356",
                    "CorpusId": 252780465
                },
                "corpusId": 252780465,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/927ed8655721357f81137ba117b56a41e3587a9e",
                "title": "A Unified Mixture-View Framework for Unsupervised Representation Learning",
                "abstract": "Recent unsupervised contrastive representation learning follows a Single Instance Multi-view (SIM) paradigm where positive pairs are usually constructed with intra-image data augmentation. In this paper, we propose an effective approach called Beyond Single Instance Multi-view (BSIM). Specifically, we impose more accurate instance discrimination capability by measuring the joint similarity between two randomly sampled instances and their mixture, namely spurious-positive pairs. We believe that learning joint similarity helps to improve the performance when encoded features are distributed more evenly in the latent space. We apply it as an orthogonal improvement for unsupervised contrastive representation learning, including current outstanding methods SimCLR, MoCo, and BYOL. We evaluate our learned representations on many downstream benchmarks like linear classification on ImageNet-1k and PASCAL VOC 2007, object detection on MS COCO 2017 and VOC, etc. We obtain substantial gains with a large margin almost on all these tasks compared with prior arts.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "27628828",
                        "name": "Xiangxiang Chu"
                    },
                    {
                        "authorId": "31818765",
                        "name": "Xiaohang Zhan"
                    },
                    {
                        "authorId": "49846372",
                        "name": "Bo Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, [38] notices that label smoothing during mixup training has a calibration effect which regularizes over-confident predictions."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6f0a1757a022a439ab16ef88d04f2bfcfefdd7d7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-13356",
                    "MAG": "3108632511",
                    "CorpusId": 227208978
                },
                "corpusId": 227208978,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6f0a1757a022a439ab16ef88d04f2bfcfefdd7d7",
                "title": "Beyond Single Instance Multi-view Unsupervised Representation Learning",
                "abstract": "Recent unsupervised contrastive representation learning follows a Single Instance Multi-view (SIM) paradigm where positive pairs are usually constructed with intra-image data augmentation. In this paper, we propose an effective approach called Beyond Single Instance Multi-view (BSIM). Specifically, we impose more accurate instance discrimination capability by measuring the joint similarity between two randomly sampled instances and their mixture, namely spurious-positive pairs. We believe that learning joint similarity helps to improve the performance when encoded features are distributed more evenly in the latent space. We apply it as an orthogonal improvement for unsupervised contrastive representation learning, including current outstanding methods SimCLR, MoCo, and BYOL. We evaluate our learned representations on many downstream benchmarks like linear classification on ImageNet-1k and PASCAL VOC 2007, object detection on MS COCO 2017 and VOC, etc. We obtain substantial gains with a large margin almost on all these tasks compared with prior arts.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "27628828",
                        "name": "Xiangxiang Chu"
                    },
                    {
                        "authorId": "31818765",
                        "name": "Xiaohang Zhan"
                    },
                    {
                        "authorId": "49141839",
                        "name": "Xiaolin Wei"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[236] investigated the predictive uncertainty and calibration of models trained with"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "172b266f190d89ec6e2164560eba3707e8936e6e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-06225",
                    "ArXiv": "2011.06225",
                    "MAG": "3102100346",
                    "DOI": "10.1016/j.inffus.2021.05.008",
                    "CorpusId": 226307260
                },
                "corpusId": 226307260,
                "publicationVenue": {
                    "id": "06afdd0b-0d85-413f-af8a-c3045c12c561",
                    "name": "Information Fusion",
                    "type": "journal",
                    "alternate_names": [
                        "Inf Fusion"
                    ],
                    "issn": "1566-2535",
                    "url": "https://www.journals.elsevier.com/information-fusion",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/15662535"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/172b266f190d89ec6e2164560eba3707e8936e6e",
                "title": "A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "7412048",
                        "name": "Moloud Abdar"
                    },
                    {
                        "authorId": "1866603",
                        "name": "Farhad Pourpanah"
                    },
                    {
                        "authorId": "1833049320",
                        "name": "Sadiq Hussain"
                    },
                    {
                        "authorId": "1404229235",
                        "name": "Dana Rezazadegan"
                    },
                    {
                        "authorId": "2150977916",
                        "name": "Li Liu"
                    },
                    {
                        "authorId": "1678622",
                        "name": "M. Ghavamzadeh"
                    },
                    {
                        "authorId": "1731709",
                        "name": "P. Fieguth"
                    },
                    {
                        "authorId": "1719250",
                        "name": "Xiaochun Cao"
                    },
                    {
                        "authorId": "145434108",
                        "name": "A. Khosravi"
                    },
                    {
                        "authorId": "2066217301",
                        "name": "U. Acharya"
                    },
                    {
                        "authorId": "144531494",
                        "name": "V. Makarenkov"
                    },
                    {
                        "authorId": "1743136",
                        "name": "S. Nahavandi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Different from other approaches that aim to retrain a model on the augmented training set (Patel et al., 2019; Thulasidasan et al., 2019), our proposed algorithm does not change the predictions and thus retains the original prediction accuracy while adjusting\nthe confidence of the predictions.",
                "Several manifold-based confidence calibration have been proposed (Bahat and Shakhnarovich, 2020; Thulasidasan et al., 2019; Patel et al., 2019; Lee et al., 2017; Verma et al., 2019).",
                "Bahat and Shakhnarovich (2020) augments test data using transformations to calibrate confidence, while Thulasidasan et al. (2019) and Patel et al. (2019) augment data by interpolating existing data and using an auto-encoder based model, respectively.",
                "Different from other approaches that aim to retrain a model on the augmented training set (Patel et al., 2019; Thulasidasan et al., 2019), our proposed algorithm does not change the predictions and thus retains the original prediction accuracy while adjusting the confidence of the predictions.",
                "To address this issue, techniques that utilize redundancy in the example space have been proposed (Bahat and Shakhnarovich, 2020; Thulasidasan et al., 2019; Patel et al., 2019).",
                "Most of these techniques augment the training dataset with examples on the same manifold and re-train a model on the augmented dataset (Thulasidasan et al., 2019; Patel et al., 2019)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2c4a569830bbf74af447842359882a14f840e803",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-04182",
                    "ArXiv": "2011.04182",
                    "MAG": "3105676370",
                    "CorpusId": 226281543
                },
                "corpusId": 226281543,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2c4a569830bbf74af447842359882a14f840e803",
                "title": "Improving Classifier Confidence using Lossy Label-Invariant Transformations",
                "abstract": "Providing reliable model uncertainty estimates is imperative to enabling robust decision making by autonomous agents and humans alike. While recently there have been significant advances in confidence calibration for trained models, examples with poor calibration persist in most calibrated models. Consequently, multiple techniques have been proposed that leverage label-invariant transformations of the input (i.e., an input manifold) to improve worst-case confidence calibration. However, manifold-based confidence calibration techniques generally do not scale and/or require expensive retraining when applied to models with large input spaces (e.g., ImageNet). In this paper, we present the recursive lossy label-invariant calibration (ReCal) technique that leverages label-invariant transformations of the input that induce a loss of discriminatory information to recursively group (and calibrate) inputs - without requiring model retraining. We show that ReCal outperforms other calibration methods on multiple datasets, especially, on large-scale datasets such as ImageNet.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3363949",
                        "name": "Sooyong Jang"
                    },
                    {
                        "authorId": "144637634",
                        "name": "Insup Lee"
                    },
                    {
                        "authorId": "4726675",
                        "name": "James Weimer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026overconfidence is caused by the training samples with same winning scores due to the one-hot labels, and adding noise perturbation in the training process is a way to mitigate aleatoric uncertainty, we apply mix-up (Zhang et al., 2017; Thulasidasan et al., 2019) to jointly address the two issues.",
                ", 2019), the existing metrics directly or indirectly depend on winning score, which is the maximum probability in a semantic vector (softmax vector from the last layer of a DNN model) (Thulasidasan et al., 2019).",
                "\u2026et al., 2018; Wang et al., 2019; Shen et al., 2019; Xiao and Wang, 2019; Kumar et al., 2019), the existing metrics directly or indirectly depend on winning score, which is the maximum probability in a semantic vector (softmax vector from the last layer of a DNN model) (Thulasidasan et al., 2019).",
                "Since the overconfidence is caused by the training samples with same winning scores due to the one-hot labels, and adding noise perturbation in the training process is a way to mitigate aleatoric uncertainty, we apply mix-up (Zhang et al., 2017; Thulasidasan et al., 2019) to jointly address the two issues.",
                "Besides, Overconfidence Error is proposed by applying winning score as confidence and penalizing samples with confidence values greater than accuracy values (Thulasidasan et al., 2019)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ec90077fe97b28241fe440a901e6c6fbac41abe8",
                "externalIds": {
                    "ACL": "2020.emnlp-main.671",
                    "DBLP": "conf/emnlp/HeZLCCAXL20",
                    "MAG": "3101988982",
                    "DOI": "10.18653/v1/2020.emnlp-main.671",
                    "CorpusId": 226262391
                },
                "corpusId": 226262391,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/ec90077fe97b28241fe440a901e6c6fbac41abe8",
                "title": "Towards More Accurate Uncertainty Estimation in Text Classification",
                "abstract": "The uncertainty measurement of classified results is especially important in areas requiring limited human resources for higher accuracy. For instance, data-driven algorithms diagnosing diseases need accurate uncertainty score to decide whether additional but limited quantity of experts are needed for rectification. However, few uncertainty models focus on improving the performance of text classification where human resources are involved. To achieve this, we aim at generating accurate uncertainty score by improving the confidence of winning scores. Thus, a model called MSD, which includes three independent components as \u201cmix-up\u201d, \u201cself-ensembling\u201d, \u201cdistinctiveness score\u201d, is proposed to improve the accuracy of uncertainty score by reducing the effect of overconfidence of winning score and considering the impact of different categories of uncertainty simultaneously. MSD can be applied with different Deep Neural Networks. Extensive experiments with ablation setting are conducted on four real-world datasets, on which, competitive results are obtained.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "47752577",
                        "name": "Jianfeng He"
                    },
                    {
                        "authorId": "2048981220",
                        "name": "Xuchao Zhang"
                    },
                    {
                        "authorId": "3433489",
                        "name": "Shuo Lei"
                    },
                    {
                        "authorId": "2211341",
                        "name": "Zhiqian Chen"
                    },
                    {
                        "authorId": "49102991",
                        "name": "Fanglan Chen"
                    },
                    {
                        "authorId": "1471374166",
                        "name": "Abdulaziz Alhamadani"
                    },
                    {
                        "authorId": "2054421433",
                        "name": "Bei Xiao"
                    },
                    {
                        "authorId": "1752590",
                        "name": "Chang-Tien Lu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "03644c0003fac30074bda9143610f901770a5154",
                "externalIds": {
                    "ACL": "2020.findings-emnlp.189",
                    "ArXiv": "2011.02207",
                    "DBLP": "journals/corr/abs-2011-02207",
                    "MAG": "3100560429",
                    "DOI": "10.18653/v1/2020.findings-emnlp.189",
                    "CorpusId": 226245980
                },
                "corpusId": 226245980,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/03644c0003fac30074bda9143610f901770a5154",
                "title": "Extracting Chemical\u2013Protein Interactions via Calibrated Deep Neural Network and Self-training",
                "abstract": "The extraction of interactions between chemicals and proteins from several biomedical articles is important in many fields of biomedical research such as drug development and prediction of drug side effects. Several natural language processing methods, including deep neural network (DNN) models, have been applied to address this problem. However, these methods were trained with hard-labeled data, which tend to become over-confident, leading to degradation of the model reliability. To estimate the data uncertainty and improve the reliability, \u201ccalibration\u201d techniques have been applied to deep learning models. In this study, to extract chemical\u2013protein interactions, we propose a DNN-based approach incorporating uncertainty information and calibration techniques. Our model first encodes the input sequence using a pre-trained language-understanding model, following which it is trained using two calibration methods: mixup training and addition of a confidence penalty loss. Finally, the model is re-trained with augmented data that are extracted using the estimated uncertainties. Our approach has achieved state-of-the-art performance with regard to the Biocreative VI ChemProt task, while preserving higher calibration abilities than those of previous approaches. Furthermore, our approach also presents the possibilities of using uncertainty estimation for performance improvement.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2007564294",
                        "name": "Dongha Choi"
                    },
                    {
                        "authorId": "49923640",
                        "name": "Hyunju Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[46] showed that mixed up training can improve calibration and predictive uncertainty of models."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c83a3448491e14e4f22ef895700ec34331437e2c",
                "externalIds": {
                    "ArXiv": "2010.12721",
                    "DBLP": "conf/nips/MehrtashAGKWW20",
                    "MAG": "3094515489",
                    "CorpusId": 225066839,
                    "PubMed": "36415583"
                },
                "corpusId": 225066839,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c83a3448491e14e4f22ef895700ec34331437e2c",
                "title": "PEP: Parameter Ensembling by Perturbation",
                "abstract": "Ensembling is now recognized as an effective approach for increasing the predictive performance and calibration of deep networks. We introduce a new approach, Parameter Ensembling by Perturbation (PEP), that constructs an ensemble of parameter values as random perturbations of the optimal parameter set from training by a Gaussian with a single variance parameter. The variance is chosen to maximize the log-likelihood of the ensemble average ( L ) on the validation data set. Empirically, and perhaps surprisingly, L has a well-defined maximum as the variance grows from zero (which corresponds to the baseline model). Conveniently, calibration level of predictions also tends to grow favorably until the peak of L is reached. In most experiments, PEP provides a small improvement in performance, and, in some cases, a substantial improvement in empirical calibration. We show that this \"PEP effect\" (the gain in log-likelihood) is related to the mean curvature of the likelihood function and the empirical Fisher information. Experiments on ImageNet pre-trained networks including ResNet, DenseNet, and Inception showed improved calibration and likelihood. We further observed a mild improvement in classification accuracy on these networks. Experiments on classification benchmarks such as MNIST and CIFAR-10 showed improved calibration and likelihood, as well as the relationship between the PEP effect and overfitting; this demonstrates that PEP can be used to probe the level of overfitting that occurred during training. In general, no special training procedure or network architecture is needed, and in the case of pre-trained networks, no additional training is needed.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1975413",
                        "name": "Alireza Mehrtash"
                    },
                    {
                        "authorId": "2427371",
                        "name": "P. Abolmaesumi"
                    },
                    {
                        "authorId": "1729630",
                        "name": "P. Golland"
                    },
                    {
                        "authorId": "2676616",
                        "name": "T. Kapur"
                    },
                    {
                        "authorId": "35466637",
                        "name": "D. Wassermann"
                    },
                    {
                        "authorId": "1785317",
                        "name": "W. Wells"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u20261, 2.5, 5}; for VAT, we search the perturbation size in {10\u22123, 10\u22124, 10\u22125} as in (Jiang et al., 2020); for Mixup, we search the interpolation parameter from {0.1, 0.2, 0.3, 0.4} as suggested in (Zhang et al., 2018; Thulasidasan et al., 2019); for Manifold-mixup, we search from {0.2, 0.4, 1, 2, 4}.",
                "\u2022Mixup (Zhang et al., 2018; Thulasidasan et al., 2019) augments training data by linearly interpolating training samples in the input space.",
                "\u2026Adversarial Training (VAT) (Miyato et al., 2018) introduces a smoothness-inducing adversarial regularizer to encourage the local Lipschitz continuity of DNNs. \u2022Mixup (Zhang et al., 2018; Thulasidasan et al., 2019) augments training data by linearly interpolating training samples in the input space."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "05ef3566499e24888a8c500944336616f8f418a4",
                "externalIds": {
                    "ArXiv": "2010.11506",
                    "DBLP": "journals/corr/abs-2010-11506",
                    "ACL": "2020.emnlp-main.102",
                    "MAG": "3094508337",
                    "DOI": "10.18653/v1/2020.emnlp-main.102",
                    "CorpusId": 222327644
                },
                "corpusId": 222327644,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/05ef3566499e24888a8c500944336616f8f418a4",
                "title": "Calibrated Fine-Tuning for Pre-trained Language Models via Manifold Smoothing",
                "abstract": "Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization. To mitigate this issue, we propose a regularized fine-tuning method. Our method introduces two types of regularization for better calibration: (1) On-manifold regularization, which generates pseudo on-manifold samples through interpolation within the data manifold. Augmented training with these pseudo samples imposes a smoothness regularization to improve in-distribution calibration. (2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data. Our experiments demonstrate that the proposed method outperforms existing calibration methods for text classification in terms of expectation calibration error, misclassification detection, and OOD detection on six datasets. Our code can be found at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2865034",
                        "name": "Lingkai Kong"
                    },
                    {
                        "authorId": "5795999",
                        "name": "Haoming Jiang"
                    },
                    {
                        "authorId": "8103389",
                        "name": "Yuchen Zhuang"
                    },
                    {
                        "authorId": "2053220976",
                        "name": "Jie Lyu"
                    },
                    {
                        "authorId": "36345161",
                        "name": "T. Zhao"
                    },
                    {
                        "authorId": "2152735278",
                        "name": "Chao Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[46] demonstrated the robustness of deep models with the use of mixup."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ed6f5f1061fc39897b024167fb863e57a5be5037",
                "externalIds": {
                    "MAG": "3106461424",
                    "DBLP": "journals/corr/abs-2010-11422",
                    "ArXiv": "2010.11422",
                    "CorpusId": 225039917
                },
                "corpusId": 225039917,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ed6f5f1061fc39897b024167fb863e57a5be5037",
                "title": "Learning Loss for Test-Time Augmentation",
                "abstract": "Data augmentation has been actively studied for robust neural networks. Most of the recent data augmentation methods focus on augmenting datasets during the training phase. At the testing phase, simple transformations are still widely used for test-time augmentation. This paper proposes a novel instance-level test-time augmentation that efficiently selects suitable transformations for a test input. Our proposed method involves an auxiliary module to predict the loss of each possible transformation given the input. Then, the transformations having lower predicted losses are applied to the input. The network obtains the results by averaging the prediction results of augmented inputs. Experimental results on several image classification benchmarks show that the proposed instance-aware test-time augmentation improves the model's robustness against various corruptions.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "14972026",
                        "name": "Ildoo Kim"
                    },
                    {
                        "authorId": "2145732998",
                        "name": "Younghoon Kim"
                    },
                    {
                        "authorId": "2155640909",
                        "name": "Sungwoong Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, the same idea can be used on metadata that we\u2019d like to balance uncertainty estimates, e.g., gender and age groups.\net al. (2020) report 1.7% ECE with Rank-1 Bayesian neural nets and 3.0% with Deep Ensembles; Thulasidasan et al. (2019a) report 3.2% for ResNet-50 with Mixup, 2.9% for ResNet-50 with an entropy-regularized loss, and 1.8% for ResNet-50 with label smoothing.",
                "It has been a key factor driving state-of-the-art: for example, Mixup (Zhang et al., 2018; Thulasidasan et al., 2019a), AugMix (Hendrycks et al., 2020), and test-time data augmentation (Ashukha et al., 2020).",
                "This is counterintuitive as we would expect Mixup, which improves calibration of individual models (Thulasidasan et al., 2019a), to also improve the calibration of their ensemble.",
                "\u2026uncertainty estimates, e.g., gender and age groups.\net al. (2020) report 1.7% ECE with Rank-1 Bayesian neural nets and 3.0% with Deep Ensembles; Thulasidasan et al. (2019a) report 3.2% for ResNet-50 with Mixup, 2.9% for ResNet-50 with an entropy-regularized loss, and 1.8% for ResNet-50 with\u2026",
                "Ensembles are the among the most known and simple approaches to improving calibration (Ovadia et al., 2019; Lakshminarayanan et al., 2017), and Thulasidasan et al. (2019b) showed that Mixup improves calibration in a single network.",
                "Mixup was shown to be effective for generalization and calibration of deep neural networks (Zhang et al., 2018; Thulasidasan et al., 2019b)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "8bc63c8bd96b40d8d0f5329e06e0a96eaf214c7f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-09875",
                    "MAG": "3093783084",
                    "ArXiv": "2010.09875",
                    "CorpusId": 224803680
                },
                "corpusId": 224803680,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8bc63c8bd96b40d8d0f5329e06e0a96eaf214c7f",
                "title": "Combining Ensembles and Data Augmentation can Harm your Calibration",
                "abstract": "Ensemble methods which average over multiple neural network predictions are a simple approach to improve a model's calibration and robustness. Similarly, data augmentation techniques, which encode prior information in the form of invariant feature transformations, are effective for improving calibration and robustness. In this paper, we show a surprising pathology: combining ensembles and data augmentation can harm model calibration. This leads to a trade-off in practice, whereby improved accuracy by combining the two techniques comes at the expense of calibration. On the other hand, selecting only one of the techniques ensures good uncertainty estimates at the expense of accuracy. We investigate this pathology and identify a compounding under-confidence among methods which marginalize over sets of weights and data augmentation techniques which soften labels. Finally, we propose a simple correction, achieving the best of both worlds with significant accuracy and calibration gains over using only ensembles or data augmentation individually. Applying the correction produces new state-of-the art in uncertainty calibration across CIFAR-10, CIFAR-100, and ImageNet.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "38356166",
                        "name": "Yeming Wen"
                    },
                    {
                        "authorId": "3451901",
                        "name": "Ghassen Jerfel"
                    },
                    {
                        "authorId": "2066044990",
                        "name": "Rafael Muller"
                    },
                    {
                        "authorId": "144477225",
                        "name": "Michael W. Dusenberry"
                    },
                    {
                        "authorId": "144108062",
                        "name": "Jasper Snoek"
                    },
                    {
                        "authorId": "40627523",
                        "name": "Balaji Lakshminarayanan"
                    },
                    {
                        "authorId": "47497262",
                        "name": "Dustin Tran"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2019) and confidence calibration (Thulasidasan et al., 2019).",
                "Not only improving the generalization on the supervised task, it also improves adversarial robustness (Zhang et al., 2018; Pang et al., 2019) and confidence calibration (Thulasidasan et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "175b27969cf0c24595151fb07c84437fd0344a5a",
                "externalIds": {
                    "ArXiv": "2010.08887",
                    "MAG": "3093157486",
                    "DBLP": "journals/corr/abs-2010-08887",
                    "CorpusId": 224705377
                },
                "corpusId": 224705377,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/175b27969cf0c24595151fb07c84437fd0344a5a",
                "title": "i-Mix: A Strategy for Regularizing Contrastive Representation Learning",
                "abstract": "Contrastive representation learning has shown to be an effective way of learning representations from unlabeled data. However, much progress has been made in vision domains relying on data augmentations carefully designed using domain knowledge. In this work, we propose i-Mix, a simple yet effective regularization strategy for improving contrastive representation learning in both vision and non-vision domains. We cast contrastive learning as training a non-parametric classifier by assigning a unique virtual class to each data in a batch. Then, data instances are mixed in both the input and virtual label spaces, providing more augmented data during training. In experiments, we demonstrate that i-Mix consistently improves the quality of self-supervised representations across domains, resulting in significant performance gains on downstream tasks. Furthermore, we confirm its regularization effect via extensive ablation studies across model and dataset sizes.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2208511",
                        "name": "Kibok Lee"
                    },
                    {
                        "authorId": "2117870479",
                        "name": "Yian Zhu"
                    },
                    {
                        "authorId": "1729571",
                        "name": "Kihyuk Sohn"
                    },
                    {
                        "authorId": "2116729195",
                        "name": "Chun-Liang Li"
                    },
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    },
                    {
                        "authorId": "1697141",
                        "name": "Honglak Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In this paper, inspired by the success of mix-up training [34] in recent studies [29, 30, 32], we propose Mix-up Contrast (MixCo) as an extension of contrastive learning approach.",
                "Mix-up training consistently improved the performance in various studies [30, 32]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3021152ab7540da7fd85baf2560568d8ef4a9b23",
                "externalIds": {
                    "MAG": "3093423309",
                    "DBLP": "journals/corr/abs-2010-06300",
                    "ArXiv": "2010.06300",
                    "CorpusId": 222310572
                },
                "corpusId": 222310572,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3021152ab7540da7fd85baf2560568d8ef4a9b23",
                "title": "MixCo: Mix-up Contrastive Learning for Visual Representation",
                "abstract": "Contrastive learning has shown remarkable results in recent self-supervised approaches for visual representation. By learning to contrast positive pairs' representation from the corresponding negatives pairs, one can train good visual representations without human annotations. This paper proposes Mix-up Contrast (MixCo), which extends the contrastive learning concept to semi-positives encoded from the mix-up of positive and negative images. MixCo aims to learn the relative similarity of representations, reflecting how much the mixed images have the original positives. We validate the efficacy of MixCo when applied to the recent self-supervised learning algorithms under the standard linear evaluation protocol on TinyImageNet, CIFAR10, and CIFAR100. In the experiments, MixCo consistently improves test accuracy. Remarkably, the improvement is more significant when the learning capacity (e.g., model size) is limited, suggesting that MixCo might be more useful in real-world scenarios. The code is available at: https://github.com/Lee-Gihun/MixCo-Mixup-Contrast.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2109662037",
                        "name": "Sungnyun Kim"
                    },
                    {
                        "authorId": "2028896390",
                        "name": "Gihun Lee"
                    },
                    {
                        "authorId": "2104224550",
                        "name": "Sangmin Bae"
                    },
                    {
                        "authorId": "145317736",
                        "name": "Seyoung Yun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It has been empirically shown to substantially improve test performance and robustness to adversarial noise of state-of-the-art neural network architectures (Zhang et al., 2018; Lamb et al., 2019; Thulasidasan et al., 2019; Zhang et al., 2018; Arazo et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "92ed2e34501903d922d74f28a012d6e337418fa4",
                "externalIds": {
                    "DBLP": "conf/iclr/ZhangDKG021",
                    "ArXiv": "2010.04819",
                    "MAG": "3091886042",
                    "CorpusId": 222290992
                },
                "corpusId": 222290992,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/92ed2e34501903d922d74f28a012d6e337418fa4",
                "title": "How Does Mixup Help With Robustness and Generalization?",
                "abstract": "Mixup is a popular data augmentation technique based on taking convex combinations of pairs of examples and their labels. This simple technique has been shown to substantially improve both the robustness and the generalization of the trained model. However, it is not well-understood why such improvement occurs. In this paper, we provide theoretical analysis to demonstrate how using Mixup in training helps model robustness and generalization. For robustness, we show that minimizing the Mixup loss corresponds to approximately minimizing an upper bound of the adversarial loss. This explains why models obtained by Mixup training exhibits robustness to several kinds of adversarial attacks such as Fast Gradient Sign Method (FGSM). For generalization, we prove that Mixup augmentation corresponds to a specific type of data-adaptive regularization which reduces overfitting. Our analysis provides new insights and a framework to understand Mixup.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "10537441",
                        "name": "Linjun Zhang"
                    },
                    {
                        "authorId": "10394991",
                        "name": "Zhun Deng"
                    },
                    {
                        "authorId": "1392876047",
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "authorId": "27316199",
                        "name": "Amirata Ghorbani"
                    },
                    {
                        "authorId": "145085305",
                        "name": "James Y. Zou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, the interpolation technique has also been used in semi-supervised learning (Berthelot et al. 2019) and also used to improve the robustness (Li, Socher, and Hoi 2020), uncertainty (Hendrycks et al. 2019), and calibration (Thulasidasan et al. 2019) of DNN classifiers.",
                "2019), and calibration (Thulasidasan et al. 2019) of DNN classifiers."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a3a022bf41423650d6b749ea37afcd52171d400e",
                "externalIds": {
                    "MAG": "3092748482",
                    "CorpusId": 222290659
                },
                "corpusId": 222290659,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a3a022bf41423650d6b749ea37afcd52171d400e",
                "title": "Local Region Knowledge Distillation",
                "abstract": "Knowledge distillation (KD) is an effective technique to transfer knowledge from one neural network (teacher) to another (student), thus improving the performance of the student. The existing work trains the student to mimic the outputs of the teacher on training data. We argue that transferring knowledge at sparse training data points cannot enable the student to well capture the local shape of the teacher function. To address this issue, we propose locally linear region knowledge distillation (LRKD) which transfers the knowledge in local, liner regions from a teacher to a student. LRKD enforces the student to mimic the local shape of the teacher function in linear regions. Extensive experiments with various network architectures demonstrate that LRKD outperforms the state-ofthe-art approaches by a large margin and is more data-efficient. Moreover, LRKD is compatible with the existing distillation methods and further improves their performances significantly. Introduction Deep neural networks (DNNs) have achieved state-of-the-art performances on a wide range of applications in artificial intelligence including computer vision (Krizhevsky, Sutskever, and Hinton 2012; Girshick 2015; He et al. 2016) and natural language processing (Andreas et al. 2016; Serban et al. 2016). The performance gain comes at a high cost of computation and memory in inference. This makes them unsuitable for deployment with limited resources or a strict latency requirement. One solution to this problem is knowledge distillation which transfers the knowledge from a large network (teacher) to a small and fast network (student). To the end, the student obtains a significant performance boost, even possibly matching or surpassing the performance of the teacher. Hinton et al. (Hinton, Vinyals, and Dean 2015) proposed the original Knowledge Distillation (KD) that utilizes the soft labels generated by a teacher as the targets to train a student. From the perspective of function curve fitting, the student function is to learn the local surface shape of the teacher function at each training data point. It is widely believed that the generalization ability of a DNN is highly related to the stability which can be described as the local function shape of a DNN. When we have a large teacher network which typically has a much better generalization performance Copyright \u00a9 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. than that of the student, we can suppose that the teacher function has a more stable (better) shape within the data distribution than that of the student. Intuitively, if the student better captures the local shape of the teacher, it can have a better generalization ability. However, the existing work only transfers the knowledge at each individual training data point from a teacher to a student as the real data distribution is typically unknown. Sparse individual data point fitting may not necessarily enable the student to well capture the local shape of the teacher function. As shown in Figure 1, even if the student fS perfectly fits the teacher fT at each training data point, i.e., x1, x2, and x3, their local shapes near these data points can be highly different. To alleviate this problem, the typically used strategy is to train on different but similar data points to the training data. This is known as data augmentation (Simard et al. 1998) formalized by the Vicinal Risk Minimization (VRM) (Chapelle et al. 2001) principle. In VRM, human knowledge is necessary to define a vicinity or neighborhood around each training data point. Then, additional new data points can be drawn from the vicinity distribution of the training data to enlarge the support of the training data distribution. For example, in image classification, it is common to define the vicinity of an image as the set of its random flippings and crops after a mildly padding. Nevertheless, data augmentation has its own limitation that a new generated data point is very close to the original training data point, since they contain almost the identical objective only with different backgrounds caused by padding or cropping. Due to this limitation, as shown in Figure 2, even if the student fS fits the teacher fT at all the training data points (i.e., x1, x2, and x3) and the augmented data points (i.e., x 1 , x aug 2 , and x aug 3 ), their local shapes can still differ substantially. To address the above issue, we propose a regularizer for KD to enforce the student to mimic the shape of the teacher in local, linear regions, thus achieving locally linear region knowledge distillation (LRKD). As shown in Figure 3, instead of only fitting the data points near the training samples, LRKD also fits the data points in the linear region between two augmented samples, e.g., x\u03021 and x\u03022 in the linear region between x 1 and x aug 2 , and x\u03023 in the linear region between x 2 and x aug 3 . The data points in a linear region can be represented by a linear combination of two augmented examples, i.e., \u03b3 \u2217 xi+(1 \u2212 \u03b3) \u2217 xj where xi and xj are two samples ar X iv :2 01 0. 04 81 2v 1 [ cs .L G ] 9 O ct 2 02 0 Figure 1: KD without data augmentation Figure 2: KD with data augmentation Figure 3: LRKD drawn from the vicinity distribution of the training data and \u03b3 is a random variable within range [0, 1]. As seen from Figure 3, distilling knowledge in local, linear regions can make the student better capture the local shape of the teacher. We summarize the main contributions as follows: \u2022 We observe that only fitting the data points in the vicinity around training data cannot enable a student to well capture the local shape of a teacher. To address this issue, we propose LRKD which transfers the knowledge in local, linear regions from a teacher to a student. \u2022 We compare LRKD with more than 10 state-of-the-art approaches. Despite its simplicity, LRKD outperforms these methods substantially and is more data-efficient. \u2022 LRKD is compatible with the existing distillation methods to further improve their performances significantly.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2150478789",
                        "name": "Xiang Deng"
                    },
                    {
                        "authorId": "104383701",
                        "name": "Zhongfei"
                    },
                    {
                        "authorId": "2146910347",
                        "name": "Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "hnique has also been used in semi-supervised learning (Berthelot et al. 2019) and also used to improve the robustness (Li, Socher, and Hoi 2020), uncertainty (Hendrycks et al. 2019), and calibration (Thulasidasan et al. 2019) of DNN classi\ufb01ers. Our Framework From KD to L2RKD Hinton et al. (Hinton, Vinyals, and Dean 2015) propose KD which minimizes the output probability differences between a student and a teacher over dat"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "30c9b4458d5da0e4f9e4b7a3dbe7f45e549d45be",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-04812",
                    "ArXiv": "2010.04812",
                    "MAG": "3092468609",
                    "CorpusId": 224801420
                },
                "corpusId": 224801420,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/30c9b4458d5da0e4f9e4b7a3dbe7f45e549d45be",
                "title": "Locally Linear Region Knowledge Distillation",
                "abstract": "Knowledge distillation (KD) is an effective technique to transfer knowledge from one neural network (teacher) to another (student), thus improving the performance of the student. To make the student better mimic the behavior of the teacher, the existing work focuses on designing different criteria to align their logits or representations. Different from these efforts, we address knowledge distillation from a novel data perspective. We argue that transferring knowledge at sparse training data points cannot enable the student to well capture the local shape of the teacher function. To address this issue, we propose locally linear region knowledge distillation ($\\rm L^2$RKD) which transfers the knowledge in local, linear regions from a teacher to a student. This is achieved by enforcing the student to mimic the outputs of the teacher function in local, linear regions. To the end, the student is able to better capture the local shape of the teacher function and thus achieves a better performance. Despite its simplicity, extensive experiments demonstrate that $\\rm L^2$RKD is superior to the original KD in many aspects as it outperforms KD and the other state-of-the-art approaches by a large margin, shows robustness and superiority under few-shot settings, and is more compatible with the existing distillation approaches to further improve their performances significantly.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2150478789",
                        "name": "Xiang Deng"
                    },
                    {
                        "authorId": "2118748124",
                        "name": "Zhongfei Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b43ce5d3c913b6a85ea0921286ec761404dc3967",
                "externalIds": {
                    "MAG": "3092460572",
                    "DBLP": "conf/miccai/ThiagarajanVRS20",
                    "DOI": "10.1007/978-3-030-60365-6_8",
                    "CorpusId": 222180299
                },
                "corpusId": 222180299,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b43ce5d3c913b6a85ea0921286ec761404dc3967",
                "title": "Improving Reliability of Clinical Models Using Prediction Calibration",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1744175",
                        "name": "Jayaraman J. Thiagarajan"
                    },
                    {
                        "authorId": "153441560",
                        "name": "Bindya Venkatesh"
                    },
                    {
                        "authorId": "145882781",
                        "name": "Deepta Rajan"
                    },
                    {
                        "authorId": "1706272",
                        "name": "P. Sattigeri"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[30] show that networks trained with mixup are better calibrated.",
                "[30] found that CNNs trained with mixup are better calibrated."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "09ce0a5073dde62e6cff2d5dfed1944e024e951e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-00466",
                    "MAG": "3109758441",
                    "ArXiv": "2104.00466",
                    "DOI": "10.1109/CVPR46437.2021.01622",
                    "CorpusId": 229534691
                },
                "corpusId": 229534691,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/09ce0a5073dde62e6cff2d5dfed1944e024e951e",
                "title": "Improving Calibration for Long-Tailed Recognition",
                "abstract": "Deep neural networks may perform poorly when training datasets are heavily class-imbalanced. Recently, two-stage methods decouple representation learning and classifier learning to improve performance. But there is still the vital issue of miscalibration. To address it, we design two methods to improve calibration and performance in such scenarios. Motivated by the fact that predicted probability distributions of classes are highly related to the numbers of class instances, we propose label-aware smoothing to deal with different degrees of over-confidence for classes and improve classifier learning. For dataset bias between these two stages due to different samplers, we further propose shifted batch normalization in the decoupling framework. Our proposed methods set new records on multiple popular long-tailed recognition benchmark datasets, including CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, Places-LT, and iNaturalist 2018.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "49164901",
                        "name": "Zhisheng Zhong"
                    },
                    {
                        "authorId": "1474280620",
                        "name": "Jiequan Cui"
                    },
                    {
                        "authorId": "25059098",
                        "name": "Shu Liu"
                    },
                    {
                        "authorId": "1729056",
                        "name": "Jiaya Jia"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Also, since the ground-truth labels are represented as one-hot-coded vectors, the cross-entropy loss has the tendency to make the model overconfident since it only focuses on the predicted probability corresponding to the ground-truth label (Thulasidasan et al. 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e81b4d4407c3f93380fbb4d14f00a8f2e11f140b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2009-10762",
                    "ArXiv": "2009.10762",
                    "MAG": "3089191880",
                    "CorpusId": 221856389
                },
                "corpusId": 221856389,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e81b4d4407c3f93380fbb4d14f00a8f2e11f140b",
                "title": "Role of Orthogonality Constraints in Improving Properties of Deep Networks for Image Classification",
                "abstract": "Standard deep learning models that employ the categorical cross-entropy loss are known to perform well at image classification tasks. However, many standard models thus obtained often exhibit issues like feature redundancy, low interpretability, and poor calibration. A body of recent work has emerged that has tried addressing some of these challenges by proposing the use of new regularization functions in addition to the cross-entropy loss. In this paper, we present some surprising findings that emerge from exploring the role of simple orthogonality constraints as a means of imposing physics-motivated constraints common in imaging. We propose an Orthogonal Sphere (OS) regularizer that emerges from physics-based latent-representations under simplifying assumptions. Under further simplifying assumptions, the OS constraint can be written in closed-form as a simple orthonormality term and be used along with the cross-entropy loss function. The findings indicate that orthonormality loss function results in a) rich and diverse feature representations, b) robustness to feature sub-selection, c) better semantic localization in the class activation maps, and d) reduction in model calibration error. We demonstrate the effectiveness of the proposed OS regularization by providing quantitative and qualitative results on four benchmark datasets - CIFAR10, CIFAR100, SVHN and tiny ImageNet.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3452524",
                        "name": "Hongjun Choi"
                    },
                    {
                        "authorId": "1396461326",
                        "name": "Anirudh Som"
                    },
                    {
                        "authorId": "143655174",
                        "name": "P. Turaga"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following Guo et al. (2017) and Thulasidasan et al. (2019), softmax predictions are grouped into M interval bins of equal size.",
                "Following the calibration metrics in Guo et al. (2017) and Thulasidasan et al. (2019), we evaluate the calibration of the model in Figure 4."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a51547aabbb94e7347fdad77ad8ff3c76182995a",
                "externalIds": {
                    "ArXiv": "2009.09364",
                    "ACL": "2020.emnlp-main.17",
                    "DBLP": "conf/emnlp/AnLWLHTZHC20",
                    "MAG": "3087365230",
                    "DOI": "10.18653/v1/2020.emnlp-main.17",
                    "CorpusId": 221818948
                },
                "corpusId": 221818948,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/a51547aabbb94e7347fdad77ad8ff3c76182995a",
                "title": "Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference",
                "abstract": "The neural attention mechanism plays an important role in many natural language processing applications. In particular, the use of multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives. Without explicit constraining, however, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the model's representation power. In this paper, for the first time, we provide a novel understanding of multi-head attention from a Bayesian perspective. Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens model's expressiveness. Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions: why and how one uses multi-head attention. Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on various tasks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "49640821",
                        "name": "Bang An"
                    },
                    {
                        "authorId": "2053220976",
                        "name": "Jie Lyu"
                    },
                    {
                        "authorId": "2920297",
                        "name": "Zhenyi Wang"
                    },
                    {
                        "authorId": null,
                        "name": "Chunyuan Li"
                    },
                    {
                        "authorId": "46622514",
                        "name": "Changwei Hu"
                    },
                    {
                        "authorId": "1491233580",
                        "name": "Fei Tan"
                    },
                    {
                        "authorId": "1390533012",
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "authorId": "50819900",
                        "name": "Yifan Hu"
                    },
                    {
                        "authorId": "1752041",
                        "name": "Changyou Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mixup can improve model accuracy [29], model calibration [26], and model robustness to certain types of image corruptions [5].",
                "The effect of this training is a substantial improvement in model calibration and accuracy on large-scale image classification tasks [30,26,5]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "47bd96a858c93b2d8dd476903885dc8caf81d55d",
                "externalIds": {
                    "ArXiv": "2009.04659",
                    "DBLP": "journals/corr/abs-2009-04659",
                    "MAG": "3084368726",
                    "DOI": "10.1007/978-3-030-66415-2_12",
                    "CorpusId": 221586409
                },
                "corpusId": 221586409,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/47bd96a858c93b2d8dd476903885dc8caf81d55d",
                "title": "Improved Robustness to Open Set Inputs via Tempered Mixup",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1397305162",
                        "name": "Ryne Roady"
                    },
                    {
                        "authorId": "31449728",
                        "name": "Tyler L. Hayes"
                    },
                    {
                        "authorId": "3290098",
                        "name": "Christopher Kanan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While this process is intuitively obvious to humans, neural networks behave abnormally in many cases, making overconfident mistakes when encountering confusing or unknown inputs [3], [5]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f55d59eb3b5273f221b3f553df788302b032b8ad",
                "externalIds": {
                    "MAG": "3087268877",
                    "ArXiv": "2009.05094",
                    "DBLP": "journals/corr/abs-2009-05094",
                    "CorpusId": 221865825
                },
                "corpusId": 221865825,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f55d59eb3b5273f221b3f553df788302b032b8ad",
                "title": "Why I'm not Answering: Understanding Determinants of Classification of an Abstaining Classifier for Cancer Pathology Reports",
                "abstract": "Safe deployment of deep learning systems in critical real world applications requires models to make few mistakes, and only under predictable circumstances. Development of such a model is not yet possible, in general. In this work, we address this problem with an abstaining classifier tuned to have $>$95% accuracy, and identify the determinants of abstention with LIME (the Local Interpretable Model-agnostic Explanations method). Essentially, we are training our model to learn the attributes of pathology reports that are likely to lead to incorrect classifications, albeit at the cost of reduced sensitivity. We demonstrate our method in a multitask setting to classify cancer pathology reports from the NCI SEER cancer registries on six tasks of greatest importance. For these tasks, we reduce the classification error rate by factors of 2-5 by abstaining on 25-45% of the reports. For the specific case of cancer site, we are able to identify metastasis and reports involving lymph nodes as responsible for many of the classification mistakes, and that the extent and types of mistakes vary systematically with cancer site (eg. breast, lung, and prostate). When combining across three of the tasks, our model classifies 50% of the reports with an accuracy greater than 95% for three of the six tasks and greater than 85% for all six tasks on the retained samples. By using this information, we expect to define work flows that incorporate machine learning only in the areas where it is sufficiently robust and accurate, saving human attention to areas where it is required.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1753019031",
                        "name": "S. Dhaubhadel"
                    },
                    {
                        "authorId": "1401382238",
                        "name": "J. Mohd-Yusof"
                    },
                    {
                        "authorId": "2071722228",
                        "name": "K. Ganguly"
                    },
                    {
                        "authorId": "2530185",
                        "name": "Gopinath Chennupati"
                    },
                    {
                        "authorId": "1780360",
                        "name": "S. Thulasidasan"
                    },
                    {
                        "authorId": "1785446",
                        "name": "N. Hengartner"
                    },
                    {
                        "authorId": "40868145",
                        "name": "B. Mumphrey"
                    },
                    {
                        "authorId": "3742715",
                        "name": "E. Durbin"
                    },
                    {
                        "authorId": "5996926",
                        "name": "J. Doherty"
                    },
                    {
                        "authorId": "1941384596",
                        "name": "Mireille Lemieux"
                    },
                    {
                        "authorId": "103259720",
                        "name": "Noah Schaefferkoetter"
                    },
                    {
                        "authorId": "1783513",
                        "name": "G. Tourassi"
                    },
                    {
                        "authorId": "9587918",
                        "name": "Linda Coyle"
                    },
                    {
                        "authorId": "144203902",
                        "name": "Lynne Penberthy"
                    },
                    {
                        "authorId": "48288971",
                        "name": "Benjamin H. McMahon"
                    },
                    {
                        "authorId": "2140695232",
                        "name": "T. Bhattacharya"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[23] demonstrate that Mixup is also useful for neural network calibration.",
                "The trainable methods are not compared in this work because the literature shows that they have a worse or similar calibration performance with temperature scaling [15, 21, 23].",
                "In this section, we introduce some existing calibration methods, including temperature scaling [19, 16], entropy regularization [14], MMCE regularization [15], label smoothing [20, 21], and Mixup training [22, 23]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4ec633eae5ea8613b951e42f2feaf2ceddede1de",
                "externalIds": {
                    "MAG": "3084285228",
                    "ArXiv": "2009.04057",
                    "DBLP": "journals/corr/abs-2009-04057",
                    "CorpusId": 221554270
                },
                "corpusId": 221554270,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4ec633eae5ea8613b951e42f2feaf2ceddede1de",
                "title": "Improved Trainable Calibration Method for Neural Networks on Medical Imaging Classification",
                "abstract": "Recent works have shown that deep neural networks can achieve super-human performance in a wide range of image classification tasks in the medical imaging domain. However, these works have primarily focused on classification accuracy, ignoring the important role of uncertainty quantification. Empirically, neural networks are often miscalibrated and overconfident in their predictions. This miscalibration could be problematic in any automatic decision-making system, but we focus on the medical field in which neural network miscalibration has the potential to lead to significant treatment errors. We propose a novel calibration approach that maintains the overall classification accuracy while significantly improving model calibration. The proposed approach is based on expected calibration error, which is a common metric for quantifying miscalibration. Our approach can be easily integrated into any classification task as an auxiliary loss term, thus not requiring an explicit training round for calibration. We show that our approach reduces calibration error significantly across various architectures and datasets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1958036",
                        "name": "G. Liang"
                    },
                    {
                        "authorId": "2153634936",
                        "name": "Yu Zhang"
                    },
                    {
                        "authorId": "2108157664",
                        "name": "Xiaoqin Wang"
                    },
                    {
                        "authorId": "145801672",
                        "name": "Nathan Jacobs"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some other works explored within-training strategies that can provide high-quality model uncertainty, such as label smoothing [21], dropout [6], mixup [33], Bayesian models [16], etc.",
                "Following the uncertainty calibration approaches [7,33], we also investigate the relationship between statistical metrics (e.",
                "[33] proves its strong uncertainty calibration capability beyond its label smoothing effects.",
                "As the model is trained on noisy web labels, we employ mixup [39], which is known as an effective regularization to make DNNs less prone to over-confident predictions and predicted scores of DNNs better calibrated to the actual confidence of a correct prediction [33]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "49dc81509472c4184bf76973577cdae14f6c9956",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2008-11894",
                    "MAG": "3080166884",
                    "ArXiv": "2008.11894",
                    "DOI": "10.1007/978-3-030-58598-3_46",
                    "CorpusId": 221340864
                },
                "corpusId": 221340864,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/49dc81509472c4184bf76973577cdae14f6c9956",
                "title": "Webly Supervised Image Classification with Self-Contained Confidence",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2295601",
                        "name": "Jingkang Yang"
                    },
                    {
                        "authorId": "2205018430",
                        "name": "Litong Feng"
                    },
                    {
                        "authorId": "2109629048",
                        "name": "Weirong Chen"
                    },
                    {
                        "authorId": "2117851634",
                        "name": "Xiaopeng Yan"
                    },
                    {
                        "authorId": "1993661696",
                        "name": "Huabin Zheng"
                    },
                    {
                        "authorId": "144389940",
                        "name": "P. Luo"
                    },
                    {
                        "authorId": "1726357",
                        "name": "Wayne Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Since the original classification network could easily obtain high confidence, in which the generated CAM only attends to small discriminative object parts, we utilize mixup data augmentation to calibrate the uncertainty in prediction [38].",
                "First, inspired by the mixup data augmentation in [49], we observe that including mixup could effectively calibrate the model uncertainty on overconfident predictions [38] and in return enables the model to attend to more object regions.",
                "In this paper, we propose to integrate the idea of mixup data augmentation [49], thereby calibrating the uncertainty in prediction [38] as well as allowing the model to attend to other regions of the image.",
                "Numerous Mixup variants [3, 14, 37, 38, 40, 47] have been proposed to extend mixup for better prediction of uncertainty and calibration of the DNNs."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "059d7ec33ed074120ae084e4d72d46b262c4626c",
                "externalIds": {
                    "ArXiv": "2008.01201",
                    "DBLP": "journals/corr/abs-2008-01201",
                    "MAG": "3046927112",
                    "CorpusId": 220961689
                },
                "corpusId": 220961689,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/059d7ec33ed074120ae084e4d72d46b262c4626c",
                "title": "Mixup-CAM: Weakly-supervised Semantic Segmentation via Uncertainty Regularization",
                "abstract": "Obtaining object response maps is one important step to achieve weakly-supervised semantic segmentation using image-level labels. However, existing methods rely on the classification task, which could result in a response map only attending on discriminative object regions as the network does not need to see the entire object for optimizing the classification loss. To tackle this issue, we propose a principled and end-to-end train-able framework to allow the network to pay attention to other parts of the object, while producing a more complete and uniform response map. Specifically, we introduce the mixup data augmentation scheme into the classification network and design two uncertainty regularization terms to better interact with the mixup strategy. In experiments, we conduct extensive analysis to demonstrate the proposed method and show favorable performance against state-of-the-art approaches.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "49644946",
                        "name": "Yu-Ting Chang"
                    },
                    {
                        "authorId": "2116722207",
                        "name": "Qiaosong Wang"
                    },
                    {
                        "authorId": "1761842",
                        "name": "Wei-Chih Hung"
                    },
                    {
                        "authorId": "3221010",
                        "name": "Robinson Piramuthu"
                    },
                    {
                        "authorId": "2580349",
                        "name": "Yi-Hsuan Tsai"
                    },
                    {
                        "authorId": "37144787",
                        "name": "Ming-Hsuan Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Additionally, mixup has the property of curbing confirmation bias by enforcing label smoothness by combining yi and yj as noted by [29]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2c3c2ad3d318d6a1e79c247a763ba6e435df0122",
                "externalIds": {
                    "MAG": "3049306822",
                    "DBLP": "conf/das/DasJ20",
                    "DOI": "10.1007/978-3-030-57058-3_3",
                    "CorpusId": 220054171
                },
                "corpusId": 220054171,
                "publicationVenue": {
                    "id": "02d53b80-30d7-493c-9453-ed7406056b31",
                    "name": "International Workshop on Document Analysis Systems",
                    "type": "conference",
                    "alternate_names": [
                        "DAS",
                        "Document Analysis Systems",
                        "Int Workshop Doc Anal Syst",
                        "Doc Anal Syst"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=647"
                },
                "url": "https://www.semanticscholar.org/paper/2c3c2ad3d318d6a1e79c247a763ba6e435df0122",
                "title": "Adapting OCR with Limited Supervision",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "121490865",
                        "name": "Deepayan Das"
                    },
                    {
                        "authorId": "1694502",
                        "name": "C. V. Jawahar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It has been shown that models trained with Mixup is robust toward out-of-distribution data [10] and is beneficial for the uncertainty calibration of a network [28]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3b4fd630260685b500c50e40fd801b6689dca570",
                "externalIds": {
                    "ArXiv": "2007.08505",
                    "DBLP": "journals/corr/abs-2007-08505",
                    "MAG": "3042473535",
                    "DOI": "10.1007/978-3-030-58523-5_28",
                    "CorpusId": 220546366
                },
                "corpusId": 220546366,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/3b4fd630260685b500c50e40fd801b6689dca570",
                "title": "FeatMatch: Feature-Based Augmentation for Semi-Supervised Learning",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "47387175",
                        "name": "Chia-Wen Kuo"
                    },
                    {
                        "authorId": "7437104",
                        "name": "Chih-Yao Ma"
                    },
                    {
                        "authorId": "2238908925",
                        "name": "Jia-Bin Huang"
                    },
                    {
                        "authorId": "145276578",
                        "name": "Z. Kira"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026have been carried out for improving uncertainty calibration on modern neural networks on tasks such as image classification (Guo et al., 2017; Thulasidasan et al., 2019) and anomaly detection (Snoek et al., 2019), the relationship between uncertainty calibration and efficiency of active\u2026",
                "Uncertainty calibration in AL Even though a large number of studies have been carried out for improving uncertainty calibration on modern neural networks on tasks such as image classification (Guo et al., 2017; Thulasidasan et al., 2019) and anomaly detection (Snoek et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3e47adc77d314ac3a33db24f0e65357912b90565",
                "externalIds": {
                    "MAG": "3041627739",
                    "ArXiv": "2007.06364",
                    "DBLP": "journals/corr/abs-2007-06364",
                    "CorpusId": 220496336
                },
                "corpusId": 220496336,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3e47adc77d314ac3a33db24f0e65357912b90565",
                "title": "On uncertainty estimation in active learning for image segmentation",
                "abstract": "Uncertainty estimation is important for interpreting the trustworthiness of machine learning models in many applications. This is especially critical in the data-driven active learning setting where the goal is to achieve a certain accuracy with minimum labeling effort. In such settings, the model learns to select the most informative unlabeled samples for annotation based on its estimated uncertainty. The highly uncertain predictions are assumed to be more informative for improving model performance. In this paper, we explore uncertainty calibration within an active learning framework for medical image segmentation, an area where labels often are scarce. Various uncertainty estimation methods and acquisition strategies (regions and full images) are investigated. We observe that selecting regions to annotate instead of full images leads to more well-calibrated models. Additionally, we experimentally show that annotating regions can cut 50% of pixels that need to be labeled by humans compared to annotating full images.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2165247017",
                        "name": "Bo Li"
                    },
                    {
                        "authorId": "9308738",
                        "name": "T. S. Alstr\u00f8m"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Based on the suggestions and findings in other papers [23,22], for our experiments we set \u03b1 = 0.",
                "Apart from improving the classification performance on various image classification benchmarks [23], Mixup also leads to better calibrated deep-learning models [22]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d2d6c2b6ed2e50ec11f35345442274c1e432bbc9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-06667",
                    "MAG": "3043518320",
                    "ArXiv": "2007.06667",
                    "DOI": "10.1007/978-3-030-65414-6_8",
                    "CorpusId": 220514462
                },
                "corpusId": 220514462,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d2d6c2b6ed2e50ec11f35345442274c1e432bbc9",
                "title": "A Machine Learning Approach to Assess Student Group Collaboration Using Individual Level Behavioral Cues",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1396461326",
                        "name": "Anirudh Som"
                    },
                    {
                        "authorId": "52162164",
                        "name": "Sujeong Kim"
                    },
                    {
                        "authorId": "1813662479",
                        "name": "Bladmir Lopez-Prado"
                    },
                    {
                        "authorId": "5896767",
                        "name": "Svati Dhamija"
                    },
                    {
                        "authorId": "8773292",
                        "name": "Nonye Alozie"
                    },
                    {
                        "authorId": "1860011",
                        "name": "Amir Tamrakar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mixup also shed lights upon other learning tasks such as semi-supervised learning [33,1], adversarial defense [31] and neural network calibration [30]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2064826f2c6f4082f9667bd581d1fae47247ecb8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-03943",
                    "MAG": "3041497966",
                    "ArXiv": "2007.03943",
                    "DOI": "10.1007/978-3-030-65414-6_9",
                    "CorpusId": 220404325
                },
                "corpusId": 220404325,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2064826f2c6f4082f9667bd581d1fae47247ecb8",
                "title": "Remix: Rebalanced Mixup",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2065332024",
                        "name": "Hsin-Ping Chou"
                    },
                    {
                        "authorId": "48435516",
                        "name": "Shih-Chieh Chang"
                    },
                    {
                        "authorId": "7588888",
                        "name": "Jia-Yu Pan"
                    },
                    {
                        "authorId": "2149192010",
                        "name": "Wei Wei"
                    },
                    {
                        "authorId": "50270386",
                        "name": "Da-Cheng Juan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2022 Number of epochs : 30 \u2022 Loss function : Binary Cross Entropy \u2022 Mixup beta distribution [17] : \u03b1 = \u03b2 = 0.",
                "In addition, the mixup method [17] was used."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7161afddfe3b7e8ba5b26152c07b2c6a9bdc2157",
                "externalIds": {
                    "MAG": "3040972704",
                    "DBLP": "conf/ijcnn/BestFPPMSSG20",
                    "DOI": "10.1109/IJCNN48605.2020.9207567",
                    "CorpusId": 221653068
                },
                "corpusId": 221653068,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/7161afddfe3b7e8ba5b26152c07b2c6a9bdc2157",
                "title": "Deep Learning and Domain Transfer for Orca Vocalization Detection",
                "abstract": "In this paper, we study the difficulties of domain transfer when training deep learning models, on a specific task that is orca vocalization detection. Deep learning appears to be an answer to many sound recognition tasks in human speech analysis as well as in bioacoustics. This method allows to learn from large amounts of data, and find the best scoring way to discriminate between classes (e.g. orca vocalization and other sounds). However, to learn the perfect data representation and discrimination boundaries, all possible data configurations need to be processed. This causes problems when those configurations are ever changing (e.g. in our experiment, a change in the recording system happened to considerably disturb our previously well performing model). We thus explore approaches to compensate on the difficulties faced with domain transfer, with two convolutionnal neural networks (CNN) architectures, one that works in the time-frequency domain, and one that works directly on the time domain.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144772954",
                        "name": "Paul Best"
                    },
                    {
                        "authorId": "1381117218",
                        "name": "Maxence Ferrari"
                    },
                    {
                        "authorId": "144364587",
                        "name": "Marion Poupard"
                    },
                    {
                        "authorId": "9205995",
                        "name": "S\u00e9bastien Paris"
                    },
                    {
                        "authorId": "2802885",
                        "name": "R. Marxer"
                    },
                    {
                        "authorId": "48396432",
                        "name": "H. Symonds"
                    },
                    {
                        "authorId": "3335746",
                        "name": "P. Spong"
                    },
                    {
                        "authorId": "1742496",
                        "name": "H. Glotin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A Mixup data augmentation [23] using an alpha of 0."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3c4af04b7f856a607de7956c1c26a78c2a1eff39",
                "externalIds": {
                    "MAG": "3040712167",
                    "DBLP": "conf/ijcnn/FerrariGMA20",
                    "DOI": "10.1109/IJCNN48605.2020.9207085",
                    "CorpusId": 221665050
                },
                "corpusId": 221665050,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/3c4af04b7f856a607de7956c1c26a78c2a1eff39",
                "title": "DOCC10: Open access dataset of marine mammal transient studies and end-to-end CNN classification",
                "abstract": "Classification of transients is a difficult task. In bioacoustics, almost all studies are still done with human labeling. In passive acoustic monitoring (PAM), the data to label are made up from months of continuous recordings with multiple recording stations and the time required to label everything with human labeling is longer than the next recording session will take to produce new data, even with multiple experts. To help lay a foundation for the emergence of automatic labeling of marine mammal transients, we built a dataset using weak labels from a 3TB dataset of marine mammal transients of DCLDE 2018. The DCLDE dataset was made for a click classification challenge. The new dataset has strong labels and opened a new challenge, DOCC10, whose baseline is also described in this paper. The accuracy of 71% of the baseline is already good enough to curate the large dataset, leaving only some regions of interest still to be expertised. But this is far from perfect, and there remains space for future improvement, or challenging alternative techniques. A smaller version of DOCC10 named DOCC7 is also presented.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1381117218",
                        "name": "Maxence Ferrari"
                    },
                    {
                        "authorId": "1742496",
                        "name": "H. Glotin"
                    },
                    {
                        "authorId": "2802885",
                        "name": "R. Marxer"
                    },
                    {
                        "authorId": "2264694",
                        "name": "M. Asch"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018) and recently found to be able to improve calibration in (Thulasidasan et al., 2019), (6) Confidence-calibrated adversarial training (CCAT) (Stutz et al.",
                "\u2026mixup, which is a data augmentation technique originally proposed in (Zhang et al., 2018) and recently found to be able to improve calibration in (Thulasidasan et al., 2019), (6) Confidence-calibrated adversarial training (CCAT) (Stutz et al., 2020), a method builds on adversarial training by\u2026",
                "Label smoothing Label smoothing is originally proposed in Szegedy et al. (2016) and is shown to be effective in improving the quality of uncertainty estimates in M\u00fcller et al. (2019); Thulasidasan et al. (2019).",
                "Recently, mixup training (Zhang et al., 2018) has been shown to improve both models\u2019 generalization and calibration (Thulasidasan et al., 2019), by preventing the model from being over-confident in its predictions.",
                "To find the best hyperparameter for label smoothing, previous methods (Szegedy et al., 2016; Thulasidasan et al., 2019) sweep in a range and choose the one that has the best validation",
                "Note that AR-AdaLS is only trained on the clean training data without any data augmentation compared to mixup (Thulasidasan et al., 2019) and CCAT (Stutz et al., 2020).",
                "Note that AR-AdaLS is only trained on the clean training data without any data augmentation compared to mixup (Thulasidasan et al., 2019) and CCAT (Stutz et al.",
                "To find the best hyperparameter for label smoothing, previous methods (Szegedy et al., 2016; Thulasidasan et al., 2019) sweep in a range and choose the one that has the best validation\n1Note, predicted confidence is not a good indicator for splitting the training dataset as the model can easily\u2026",
                "For example, models are often miscalibrated where the predicted confidence is not indicative of the true likelihood of the model being correct (Guo et al., 2017; Thulasidasan et al., 2019; Lakshminarayanan et al., 2017; Wen et al., 2020; Kull et al., 2019).",
                ", 2018) has been shown to improve both models\u2019 generalization and calibration (Thulasidasan et al., 2019), by preventing the model from being over-confident in its predictions."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "dc2cc26ef149f7de7bed9ba7c2a4798ba4d4bf91",
                "externalIds": {
                    "DBLP": "conf/nips/QinWBC21",
                    "ArXiv": "2006.16375",
                    "CorpusId": 245010935
                },
                "corpusId": 245010935,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/dc2cc26ef149f7de7bed9ba7c2a4798ba4d4bf91",
                "title": "Improving Calibration through the Relationship with Adversarial Robustness",
                "abstract": "Neural networks lack adversarial robustness, i.e., they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined when models give miscalibrated predictions, i.e., the predicted probability is not a good indicator of how much we should trust our model. In this paper, we study the connection between adversarial robustness and calibration and find that the inputs for which the model is sensitive to small perturbations (are easily attacked) are more likely to have poorly calibrated predictions. Based on this insight, we examine if calibration can be improved by addressing those adversarially unrobust inputs. To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations of adversarial robustness and calibration into training by adaptively softening labels for an example based on how easily it can be attacked by an adversary. We find that our method, taking the adversarial robustness of the in-distribution data into consideration, leads to better calibration over the model even under distributional shifts. In addition, AR-AdaLS can also be applied to an ensemble model to further improve model calibration.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145592705",
                        "name": "Yao Qin"
                    },
                    {
                        "authorId": "1524732527",
                        "name": "Xuezhi Wang"
                    },
                    {
                        "authorId": "2638246",
                        "name": "Alex Beutel"
                    },
                    {
                        "authorId": "2226805",
                        "name": "Ed H. Chi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, mixup training [37] has been shown to improve both models\u2019 generalization and calibration [32], by preventing the model from being over-confident in its predictions.",
                "To find the best hyperparameter for label smoothing, previous methods [30, 32] sweep in a range and choose the one that has the best validation performance.",
                "Label Smoothing Label smoothing is originally proposed in [30] and is shown to be effective in improving the quality of uncertainty estimates in [20, 32].",
                "For example, models are often miscalibrated where the predicted confidence is not indicative of the true likelihood of the model being correct [8, 32, 16, 33, 15]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a309f0ba642a46cec903784818ddff5c9e702129",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-16375",
                    "MAG": "3038897811",
                    "CorpusId": 220265810
                },
                "corpusId": 220265810,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a309f0ba642a46cec903784818ddff5c9e702129",
                "title": "Improving Uncertainty Estimates through the Relationship with Adversarial Robustness",
                "abstract": "Robustness issues arise in a variety of forms and are studied through multiple lenses in the machine learning literature. Neural networks lack adversarial robustness -- they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined when models give miscalibrated or unstable uncertainty estimates, i.e. the predicted probability is not a good indicator of how much we should trust our model and could vary greatly over multiple independent runs. In this paper, we study the connection between adversarial robustness, predictive uncertainty (calibration) and model uncertainty (stability) on multiple classification networks and datasets. We find that the inputs for which the model is sensitive to small perturbations (are easily attacked) are more likely to have poorly calibrated and unstable predictions. Based on this insight, we examine if calibration and stability can be improved by addressing those adversarially unrobust inputs. To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations of adversarial robustness and uncertainty into training by adaptively softening labels conditioned on how easily it can be attacked by adversarial examples. We find that our method, taking the adversarial robustness of the in-distribution data into consideration, leads to better calibration and stability over the model even under distributional shifts. In addition, AR-AdaLS can also be applied to an ensemble model to achieve the best calibration performance.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145592705",
                        "name": "Yao Qin"
                    },
                    {
                        "authorId": "1524732527",
                        "name": "Xuezhi Wang"
                    },
                    {
                        "authorId": "2638246",
                        "name": "Alex Beutel"
                    },
                    {
                        "authorId": "2226805",
                        "name": "Ed H. Chi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DNNs are not only overconfident on the data they are trained on but also on unseen out-of-distribution data [9,25].",
                "We compare the four variants of our proposed method with L1 distance, L2 distance, Autoencoder distance (AE) and Word Embedding distance (WE) to the vanilla training using one-hot labels, as well as various techniques that improve confidence calibration: temperature scaling (TS) [7], uniform label smoothing [24,18], mixup training [25], Dirichlet calibration with off-diagonal regularization (Dir-ODIR) [10], and ensemble temperature scaling (ETS) [31].",
                "Non-post-hoc methods are mostly based on adapting the training procedure, including modifying the training loss [17,27], label smoothing [24,18], and data augmentation [25,29].",
                "Since the discovery of this challenging problem, several methods [31,18,27,25] have been explored and empirically shown to improve confidence calibration performance on the predictions, which we refer to as prediction calibration, for which only the model\u2019s prediction (the winning class) and its associated confidence (the maximum softmax score) are considered."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "36477043bce715ff1da0d8efcdbd68d8de083468",
                "externalIds": {
                    "ArXiv": "2006.14028",
                    "DBLP": "conf/icann/LiuJ21",
                    "DOI": "10.1007/978-3-030-86380-7_16",
                    "CorpusId": 237505656
                },
                "corpusId": 237505656,
                "publicationVenue": {
                    "id": "3e64b1c1-745f-4edf-bd92-b8ef122bb49c",
                    "name": "International Conference on Artificial Neural Networks",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Artif Neural Netw",
                        "ICANN"
                    ],
                    "url": "http://www.e-nns.org/"
                },
                "url": "https://www.semanticscholar.org/paper/36477043bce715ff1da0d8efcdbd68d8de083468",
                "title": "Class-Similarity Based Label Smoothing for Confidence Calibration",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2144634231",
                        "name": "Chihuang Liu"
                    },
                    {
                        "authorId": "1714905",
                        "name": "J. J\u00e1J\u00e1"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mixup [29] trains a classifier not only on the training data but also on linear interpolations of random pairs of samples and their labels.",
                "We compare our proposed method to the vanilla training using one-hot labels, as well as three other techniques that improve confidence calibration: temperature scaling [10], uniform label smoothing [28, 20], and mixup training [29].",
                "Studies have shown that DNNs are not only overconfident on the data they are trained on but also on unseen out-of distribution data [12, 29]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5549af03361d893d1bf62d438ba19617d38d04a6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-14028",
                    "MAG": "3037142499",
                    "CorpusId": 220055910
                },
                "corpusId": 220055910,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5549af03361d893d1bf62d438ba19617d38d04a6",
                "title": "Class-Similarity Based Label Smoothing for Generalized Confidence Calibration",
                "abstract": "Since modern neural networks are known to be overconfident, several techniques have been recently introduced to address this problem and improve calibration. However, the current notion of calibration is overly simple since only single prediction confidence is considered while the information regarding the rest of the classes is ignored. The output of a neural network is a probability distribution where the scores are estimated confidences of the input belonging to the corresponding classes, and hence they represent a complete estimate of the output likelihood that should be calibrated. In this paper, we first introduce a generalized definition of confidence calibration, which motivates the development of a novel form of label smoothing where the value of each class label is based on its similarity with the reference class. We adopt different similarity measurements, including those that capture semantic similarity, and demonstrate through extensive experiments the advantage of our method over both uniform label smoothing and other techniques.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "40813771",
                        "name": "Chihuang Liu"
                    },
                    {
                        "authorId": "1714905",
                        "name": "J. J\u00e1J\u00e1"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026et al., 2018), scalable Gaussian processes (Milios et al., 2018), sampling-free uncertainty estimation (Postels et al., 2019), data augmentation (Patel et al., 2019; Thulasidasan et al., 2019; Yun et al., 2019; Hendrycks et al., 2020) and ensemble distribution distillation (Malinin et al., 2020).",
                ", 2018) which is a data augmentation shown to improve calibration (Thulasidasan et al., 2019).",
                ", 2017); and 3) using Mixup data augmentation (Zhang et al., 2018; Thulasidasan et al., 2019).",
                "A WRN CIFAR100 classifier is trained in three modes: 1) no during-training calibration; 2) using entropy regularization (Pereyra et al., 2017); and 3) using Mixup data augmentation (Zhang et al., 2018; Thulasidasan et al., 2019).",
                "Additionally, we adopt Mixup (Zhang et al., 2018) which is a data augmentation shown to improve calibration (Thulasidasan et al., 2019).",
                ", 2019), data augmentation (Patel et al., 2019; Thulasidasan et al., 2019; Yun et al., 2019; Hendrycks et al., 2020) and ensemble distribution distillation (Malinin et al."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ecafa739f42dea74c23d7a3bb4ab613aeb86d1aa",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-13092",
                    "ArXiv": "2006.13092",
                    "MAG": "3036157921",
                    "CorpusId": 219981345
                },
                "corpusId": 219981345,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ecafa739f42dea74c23d7a3bb4ab613aeb86d1aa",
                "title": "Multi-Class Uncertainty Calibration via Mutual Information Maximization-based Binning",
                "abstract": "Post-hoc calibration is a common approach for providing high-quality confidence estimates of deep neural network predictions. Recent work has shown that widely used scaling methods underestimate their calibration error, while alternative Histogram Binning (HB) methods with verifiable calibration performance often fail to preserve classification accuracy. In the case of multi-class calibration with a large number of classes K, HB also faces the issue of severe sample-inefficiency due to a large class imbalance resulting from the conversion into K one-vs-rest class-wise calibration problems. The goal of this paper is to resolve the identified issues of HB in order to provide verified and calibrated confidence estimates using only a small holdout calibration dataset for bin optimization while preserving multi-class ranking accuracy. From an information-theoretic perspective, we derive the I-Max concept for binning, which maximizes the mutual information between labels and binned (quantized) logits. This concept mitigates potential loss in ranking performance due to lossy quantization, and by disentangling the optimization of bin edges and representatives allows simultaneous improvement of ranking and calibration performance. In addition, we propose a shared class-wise (sCW) binning strategy that fits a single calibrator on the merged training sets of all K class-wise problems, yielding reliable estimates from a small calibration set. The combination of sCW and I-Max binning outperforms the state of the art calibration methods on various evaluation metrics across different benchmark datasets and models, even when using only a small set of calibration data, e.g. 1k samples for ImageNet.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51290549",
                        "name": "Kanil Patel"
                    },
                    {
                        "authorId": "52020792",
                        "name": "William H. Beluch"
                    },
                    {
                        "authorId": "49188662",
                        "name": "Binh Yang"
                    },
                    {
                        "authorId": "144578436",
                        "name": "Michael Pfeiffer"
                    },
                    {
                        "authorId": "2109979241",
                        "name": "Dan Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Among the former, the main idea is to increase the entropy of the classifier to avoid overconfident predictions, which is accomplished via modifying the training loss [12, 15, 23], label smoothing [16, 21], and data augmentation techniques [25, 28, 32]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "af2549305c2839e85367a07db10366670161afba",
                "externalIds": {
                    "ArXiv": "2006.12800",
                    "DBLP": "journals/corr/abs-2006-12800",
                    "MAG": "3036314645",
                    "CorpusId": 219981518
                },
                "corpusId": 219981518,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/af2549305c2839e85367a07db10366670161afba",
                "title": "Calibration of Neural Networks using Splines",
                "abstract": "Calibrating neural networks is of utmost importance when employing them in safety-critical applications where the downstream decision making depends on the predicted probabilities. Measuring calibration error amounts to comparing two empirical distributions. In this work, we introduce a binning-free calibration measure inspired by the classical Kolmogorov-Smirnov (KS) statistical test in which the main idea is to compare the respective cumulative probability distributions. From this, by approximating the empirical cumulative distribution using a differentiable function via splines, we obtain a recalibration function, which maps the network outputs to actual (calibrated) class assignment probabilities. The spine-fitting is performed using a held-out calibration set and the obtained recalibration function is evaluated on an unseen test set. We tested our method against existing calibration approaches on various image classification datasets and our spline-based recalibration approach consistently outperforms existing methods on KS error as well as other commonly used calibration measures.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1491124544",
                        "name": "Kartik Gupta"
                    },
                    {
                        "authorId": "104610936",
                        "name": "Amir M. Rahimi"
                    },
                    {
                        "authorId": "144722114",
                        "name": "Thalaiyasingam Ajanthan"
                    },
                    {
                        "authorId": "1722052",
                        "name": "Thomas Mensink"
                    },
                    {
                        "authorId": "1781120",
                        "name": "C. Sminchisescu"
                    },
                    {
                        "authorId": "143750012",
                        "name": "R. Hartley"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mixup is a data augmentation procedure proposed by [23] that is known to improve calibration ([19]) when applied to standard models."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2de4ad9efa61121809cbfd6afa0b0887afdaade8",
                "externalIds": {
                    "ArXiv": "2006.07737",
                    "DBLP": "journals/corr/abs-2006-07737",
                    "MAG": "3035745410",
                    "CorpusId": 219687415
                },
                "corpusId": 219687415,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2de4ad9efa61121809cbfd6afa0b0887afdaade8",
                "title": "Generalization by Recognizing Confusion",
                "abstract": "A recently-proposed technique called self-adaptive training augments modern neural networks by allowing them to adjust training labels on the fly, to avoid overfitting to samples that may be mislabeled or otherwise non-representative. By combining the self-adaptive objective with mixup, we further improve the accuracy of self-adaptive models for image recognition; the resulting classifier obtains state-of-the-art accuracies on datasets corrupted with label noise. Robustness to label noise implies a lower generalization gap; thus, our approach also leads to improved generalizability. We find evidence that the Rademacher complexity of these algorithms is low, suggesting a new path towards provable generalization for this type of deep learning model. Last, we highlight a novel connection between difficulties accounting for rare classes and robustness under noise, as rare classes are in a sense indistinguishable from label noise. Our code can be found at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2058851444",
                        "name": "Daniel Chiu"
                    },
                    {
                        "authorId": "150106155",
                        "name": "Franklyn Wang"
                    },
                    {
                        "authorId": "1794750",
                        "name": "S. Kominers"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To this end, the works in computer vision tasks with well-established benchmark studies(Thulasidasan et al., 2019; Snoek et al., 2019) has shown that Bayesian learning is beneficial for better generalization to OOD and corrupted samples."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0b1b4744b03309cdf2feaee54a5a9fdcadc452f4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-07021",
                    "MAG": "3035367659",
                    "ArXiv": "2006.07021",
                    "CorpusId": 219636373
                },
                "corpusId": 219636373,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0b1b4744b03309cdf2feaee54a5a9fdcadc452f4",
                "title": "A benchmark study on reliable molecular supervised learning via Bayesian learning",
                "abstract": "Virtual screening aims to find desirable compounds from chemical library by using computational methods. For this purpose with machine learning, model outputs that can be interpreted as predictive probability will be beneficial, in that a high prediction score corresponds to high probability of correctness. In this work, we present a study on the prediction performance and reliability of graph neural networks trained with the recently proposed Bayesian learning algorithms. Our work shows that Bayesian learning algorithms allow well-calibrated predictions for various GNN architectures and classification tasks. Also, we show the implications of reliable predictions on virtual screening, where Bayesian learning may lead to higher success in finding hit compounds.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2057221024",
                        "name": "Doyeong Hwang"
                    },
                    {
                        "authorId": "2110959613",
                        "name": "Grace Lee"
                    },
                    {
                        "authorId": "1748963603",
                        "name": "Hanseok Jo"
                    },
                    {
                        "authorId": "1748903557",
                        "name": "Seyoul Yoon"
                    },
                    {
                        "authorId": "145185994",
                        "name": "Seongok Ryu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent works [3, 18, 19] discover the hidden gems of label smoothing [20], mixup [21], and adversarial training [22] on improving the calibration performance and the uncertainty representation ability."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f7ae18fa9ce1128d6b135505cd25bfea7746dc9b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-06399",
                    "ArXiv": "2006.06399",
                    "MAG": "3034891383",
                    "CorpusId": 219573223
                },
                "corpusId": 219573223,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f7ae18fa9ce1128d6b135505cd25bfea7746dc9b",
                "title": "Deep Learning Requires Explicit Regularization for Reliable Predictive Probability",
                "abstract": "From the statistical learning perspective, complexity control via explicit regularization is a necessity for improving the generalization of over-parameterized models, which deters the memorization of intricate patterns existing only in the training data. However, the impressive generalization performance of over-parameterized neural networks with only implicit regularization challenges this traditional role of explicit regularization. Furthermore, explicit regularization does not prevent neural networks from memorizing unnatural patterns, such as random labels, that cannot be generalized. In this work, we revisit the role and importance of explicit regularization methods for generalizing the predictive probability, not just the generalization of the 0-1 loss. Specifically, we present extensive empirical evidence showing the versatility of explicit regularization techniques on improving the reliability of the predictive probability, which enables better uncertainty representation and prevents the overconfidence problem. Our findings present a new direction to improve the predictive probability quality of deterministic neural networks, unlike the mainstream of approaches concentrates on building stochastic representation with Bayesian neural networks, ensemble methods, and hybrid models.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "71526308",
                        "name": "Taejong Joo"
                    },
                    {
                        "authorId": "1498640710",
                        "name": "U. Chung"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This interaction between label smoothing (due to output Mixup) and Jacobian regularization (due to input Mixup) may explain why Mixup on inputs only performs poorly compared to Mixup on both inputs and outputs [31].",
                "These include improved calibration [31], robustness to input adversarial noise [36], and robustness to label corruption [36]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "264972e27882e53a4671f95b2a4c789e15d12cd7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-06049",
                    "ArXiv": "2006.06049",
                    "MAG": "3035364336",
                    "CorpusId": 219573706
                },
                "corpusId": 219573706,
                "publicationVenue": {
                    "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                    "name": "Journal of machine learning research",
                    "type": "journal",
                    "alternate_names": [
                        "Journal of Machine Learning Research",
                        "J mach learn res",
                        "J Mach Learn Res"
                    ],
                    "issn": "1532-4435",
                    "alternate_issns": [
                        "1533-7928"
                    ],
                    "url": "http://www.ai.mit.edu/projects/jmlr/",
                    "alternate_urls": [
                        "http://jmlr.csail.mit.edu/",
                        "http://www.jmlr.org/",
                        "http://portal.acm.org/affiliated/jmlr"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/264972e27882e53a4671f95b2a4c789e15d12cd7",
                "title": "On Mixup Regularization",
                "abstract": "Mixup is a data augmentation technique that creates new examples as convex combinations of training points and labels. This simple technique has empirically shown to improve the accuracy of many state-of-the-art models in different settings and applications, but the reasons behind this empirical success remain poorly understood. In this paper we take a substantial step in explaining the theoretical foundations of Mixup, by clarifying its regularization effects. We show that Mixup can be interpreted as standard empirical risk minimization estimator subject to a combination of data transformation and random perturbation of the transformed data. We further show that these transformations and perturbations induce multiple known regularization schemes, including label smoothing and reduction of the Lipschitz constant of the estimator, and that these schemes interact synergistically with each other, resulting in a self calibrated and effective regularization effect that prevents overfitting and overconfident predictions. We illustrate our theoretical analysis by experiments that empirically support our conclusions.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "31661413",
                        "name": "Luigi Carratino"
                    },
                    {
                        "authorId": "2057320980",
                        "name": "Moustapha Ciss'e"
                    },
                    {
                        "authorId": "2068720",
                        "name": "Rodolphe Jenatton"
                    },
                    {
                        "authorId": "152303545",
                        "name": "Jean-Philippe Vert"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another promising approach could be using recent data augmentation techniques [66], [67] or strategies based on pretrained models [42]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "37ce819fc0bf456c4a19c6f28a6e92282347c6b7",
                "externalIds": {
                    "DBLP": "journals/tnn/MacedoRZOL22",
                    "ArXiv": "2006.04005",
                    "DOI": "10.1109/TNNLS.2021.3112897",
                    "CorpusId": 235239803,
                    "PubMed": "34596562"
                },
                "corpusId": 235239803,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/37ce819fc0bf456c4a19c6f28a6e92282347c6b7",
                "title": "Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples",
                "abstract": "In this article, we argue that the unsatisfactory out-of-distribution (OOD) detection performance of neural networks is mainly due to the SoftMax loss anisotropy and propensity to produce low entropy probability distributions in disagreement with the principle of maximum entropy. On the one hand, current OOD detection approaches usually do not directly fix the SoftMax loss drawbacks, but rather build techniques to circumvent it. Unfortunately, those methods usually produce undesired side effects (e.g., classification accuracy drop, additional hyperparameters, slower inferences, and collecting extra data). On the other hand, we propose replacing SoftMax loss with a novel loss function that does not suffer from the mentioned weaknesses. The proposed IsoMax loss is isotropic (exclusively distance-based) and provides high entropy posterior probability distributions. Replacing the SoftMax loss by IsoMax loss requires no model or training changes. Additionally, the models trained with IsoMax loss produce as fast and energy-efficient inferences as those trained using SoftMax loss. Moreover, no classification accuracy drop is observed. The proposed method does not rely on outlier/background data, hyperparameter tuning, temperature calibration, feature extraction, metric learning, adversarial training, ensemble procedures, or generative models. Our experiments showed that IsoMax loss works as a seamless SoftMax loss drop-in replacement that significantly improves neural networks\u2019 OOD detection performance. Hence, it may be used as a baseline OOD detection approach to be combined with current or future OOD detection techniques to achieve even higher results.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51473026",
                        "name": "David Mac\u00eado"
                    },
                    {
                        "authorId": "1769575",
                        "name": "T. I. Ren"
                    },
                    {
                        "authorId": "2948325",
                        "name": "C. Zanchettin"
                    },
                    {
                        "authorId": "40057895",
                        "name": "Adriano Oliveira"
                    },
                    {
                        "authorId": "1746612",
                        "name": "Teresa B Ludermir"
                    }
                ]
            }
        },
        {
            "contexts": [
                "By using an implicit bias that linear interpolations of data should lead to predictions that are linearly interpolated in the target space, Mix Up enables generation of well-calibrated models whose generalization performance is slightly better (Thulasidasan et al., 2019).",
                "\u2026to Data Augmentation techniques, there have been attempts to explain the latent effect of data augmentation using mathematical formulations, such as in (Chen et al., 2019; Thulasidasan et al., 2019; He et al., 2019), and for Mixed Sample Data Augmentation Techniques in (Harris et al., 2020).",
                "Specific to Data Augmentation techniques, there have been attempts to explain the latent effect of data augmentation using mathematical formulations, such as in (Chen et al., 2019; Thulasidasan et al., 2019; He et al., 2019), and for Mixed Sample Data Augmentation Techniques in (Harris et al."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "04be154a14b9b3ffdd96b03b19d5f937512ce437",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-03810",
                    "MAG": "3033159813",
                    "ArXiv": "2006.03810",
                    "CorpusId": 219531762
                },
                "corpusId": 219531762,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/04be154a14b9b3ffdd96b03b19d5f937512ce437",
                "title": "An Empirical Analysis of the Impact of Data Augmentation on Knowledge Distillation",
                "abstract": "Generalization Performance of Deep Learning models trained using Empirical Risk Minimization can be improved significantly by using Data Augmentation strategies such as simple transformations, or using Mixed Samples. We attempt to empirically analyze the impact of such strategies on the transfer of generalization between teacher and student models in a distillation setup. We observe that if a teacher is trained using any of the mixed sample augmentation strategies, such as MixUp or CutMix, the student model distilled from it is impaired in its generalization capabilities. We hypothesize that such strategies limit a model's capability to learn example-specific features, leading to a loss in quality of the supervision signal during distillation. We present a novel Class-Discrimination metric to quantitatively measure this dichotomy in performance and link it to the discriminative capacity induced by the different strategies on a network's latent space.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "121490889",
                        "name": "Deepan Das"
                    },
                    {
                        "authorId": "1436024527",
                        "name": "Haley Massa"
                    },
                    {
                        "authorId": "1738774570",
                        "name": "Abhimanyu Kulkarni"
                    },
                    {
                        "authorId": "145071799",
                        "name": "Theodoros Rekatsinas"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also investigated the calibration of our scNym models by comparing the prediction confidence scores to prediction accuracy (Thulasidasan et al., 2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a8b10471725ab0dfa01410f4127b0c3079cec61f",
                "externalIds": {
                    "MAG": "3033243930",
                    "PubMedCentral": "8494222",
                    "DOI": "10.1101/gr.268581.120",
                    "CorpusId": 219603805,
                    "PubMed": "33627475"
                },
                "corpusId": 219603805,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a8b10471725ab0dfa01410f4127b0c3079cec61f",
                "title": "Semisupervised adversarial neural networks for single-cell classification",
                "abstract": "Annotating cell identities is a common bottleneck in the analysis of single cell genomics experiments. Here, we present scNym, a semi-supervised, adversarial neural network that learns to transfer cell identity annotations from one experiment to another. scNym takes advantage of information in both labeled datasets and new, unlabeled datasets to learn rich representations of cell identity that enable effective annotation transfer. We show that scNym effectively transfers annotations across experiments despite biological and technical differences, achieving performance superior to existing methods. We also show that scNym models can synthesize information from multiple training and target datasets to improve performance. In addition to high performance, we show that scNym models are well-calibrated and interpretable with saliency methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "37515963",
                        "name": "Jacob C. Kimmel"
                    },
                    {
                        "authorId": "1714478",
                        "name": "David R. Kelley"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c8451b2107e302da1d87c3b17946db3f4ca5e49d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2005-02231",
                    "ArXiv": "2005.02231",
                    "MAG": "3021725297",
                    "CorpusId": 218502244
                },
                "corpusId": 218502244,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c8451b2107e302da1d87c3b17946db3f4ca5e49d",
                "title": "Self-Training with Improved Regularization for Few-Shot Chest X-Ray Classification",
                "abstract": "Automated diagnostic assistants in healthcare necessitate accurate AI models that can be trained with limited labeled data, can cope with severe class imbalances and can support simultaneous prediction of multiple disease conditions. To this end, we present a novel few-shot learning approach that utilizes a number of key components to enable robust modeling in such challenging scenarios. Using an important use-case in chest X-ray classification, we provide several key insights on the effective use of data augmentation, self-training via distillation and confidence tempering for few-shot learning in medical imaging. Our results show that using only ~10% of the labeled data, we can build predictive models that match the performance of classifiers trained in a large-scale data setting.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145882781",
                        "name": "Deepta Rajan"
                    },
                    {
                        "authorId": "1744175",
                        "name": "Jayaraman J. Thiagarajan"
                    },
                    {
                        "authorId": "2308391",
                        "name": "A. Karargyris"
                    },
                    {
                        "authorId": "33201965",
                        "name": "Satyananda Kashyap"
                    }
                ]
            }
        },
        {
            "contexts": [
                "data augmentation techniques [25] and regularization strategies [21] to more sophisticated methods that quantify the epistemic (or model) uncertainties and aleatoric (or data) uncertainties for calibrating model confidences [7,8,12,22]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b23bf288a39768a3be5eef5e5e86fcc6a1b71c3d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2004-14480",
                    "MAG": "3023124993",
                    "ArXiv": "2004.14480",
                    "CorpusId": 216914314
                },
                "corpusId": 216914314,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b23bf288a39768a3be5eef5e5e86fcc6a1b71c3d",
                "title": "Calibrating Healthcare AI: Towards Reliable and Interpretable Deep Predictive Models",
                "abstract": "The wide-spread adoption of representation learning technologies in clinical decision making strongly emphasizes the need for characterizing model reliability and enabling rigorous introspection of model behavior. While the former need is often addressed by incorporating uncertainty quantification strategies, the latter challenge is addressed using a broad class of interpretability techniques. In this paper, we argue that these two objectives are not necessarily disparate and propose to utilize prediction calibration to meet both objectives. More specifically, our approach is comprised of a calibration-driven learning method, which is also used to design an interpretability technique based on counterfactual reasoning. Furthermore, we introduce \\textit{reliability plots}, a holistic evaluation mechanism for model reliability. Using a lesion classification problem with dermoscopy images, we demonstrate the effectiveness of our approach and infer interesting insights about the model behavior.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1744175",
                        "name": "Jayaraman J. Thiagarajan"
                    },
                    {
                        "authorId": "1706272",
                        "name": "P. Sattigeri"
                    },
                    {
                        "authorId": "145882781",
                        "name": "Deepta Rajan"
                    },
                    {
                        "authorId": "153441560",
                        "name": "Bindya Venkatesh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "But fine-grained testing does not work well when comparing mixup methods and non-mixup methods, since mixup is better class-calibrated [41]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0bbbbdf1d6dbcc53c41a0cea7589e42cfb806e82",
                "externalIds": {
                    "DBLP": "journals/pr/FengZGTCLSM22",
                    "ArXiv": "2004.08514",
                    "DOI": "10.1016/j.patcog.2022.108777",
                    "CorpusId": 248693092
                },
                "corpusId": 248693092,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0bbbbdf1d6dbcc53c41a0cea7589e42cfb806e82",
                "title": "DMT: Dynamic mutual training for semi-supervised learning",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "101592953",
                        "name": "Zhengyang Feng"
                    },
                    {
                        "authorId": "67190665",
                        "name": "Qianyu Zhou"
                    },
                    {
                        "authorId": "66918045",
                        "name": "Qiqi Gu"
                    },
                    {
                        "authorId": "144184998",
                        "name": "Xin Tan"
                    },
                    {
                        "authorId": "48502143",
                        "name": "Guangliang Cheng"
                    },
                    {
                        "authorId": "145820358",
                        "name": "Xuequan Lu"
                    },
                    {
                        "authorId": "1788070",
                        "name": "Jianping Shi"
                    },
                    {
                        "authorId": "2149343311",
                        "name": "Lizhuang Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This overconfidence can occur even with randomly labeled training data as deep networks are likely to just memorize the training statistics [36].",
                "Many efforts have been made to understand the generalization performance of deep learning [43, 10, 23, 2, 38, 21, 36, 45].",
                "Why Auxiliary Classifiers? Several works have found that deep networks are prone to over-confident predictions, and this hinders a network from learning generalization [23, 45, 36].",
                "classification predictions about an input, thus causing loss in the generalization performance [23, 45, 20, 36]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ac2d8263981eca1026a7eb8c7c8806f669097f9d",
                "externalIds": {
                    "MAG": "3015113763",
                    "DBLP": "journals/corr/abs-2004-00251",
                    "ArXiv": "2004.00251",
                    "DOI": "10.1016/j.neunet.2021.02.007",
                    "CorpusId": 214743539,
                    "PubMed": "33652370"
                },
                "corpusId": 214743539,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ac2d8263981eca1026a7eb8c7c8806f669097f9d",
                "title": "Self-Augmentation: Generalizing Deep Networks to Unseen Classes for Few-Shot Learning",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "115026943",
                        "name": "Jinhwan Seo"
                    },
                    {
                        "authorId": "2109313062",
                        "name": "Hong G Jung"
                    },
                    {
                        "authorId": "50112753",
                        "name": "Seong-Whan Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "56]. The basic idea is to generate new training data-label pairs by convex combinations of training samples. Several studies demonstrated it\u2019s benet for various tasks such as calibrating uncertainty [42] and domain adaptation for images [23,51,54]. 3 Approach In this section, we present the main building blocks of our approach. We rst describe our general pipeline and training procedure, and then exp"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "436300ef1ab6c9e576b5ed20172d55253b59ff8e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2003-12641",
                    "ArXiv": "2003.12641",
                    "MAG": "3013245634",
                    "DOI": "10.1109/WACV48630.2021.00017",
                    "CorpusId": 214714283
                },
                "corpusId": 214714283,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/436300ef1ab6c9e576b5ed20172d55253b59ff8e",
                "title": "Self-Supervised Learning for Domain Adaptation on Point Clouds",
                "abstract": "Self-supervised learning (SSL) is a technique for learning useful representations from unlabeled data. It has been applied effectively to domain adaptation (DA) on images and videos. It is still unknown if and how it can be leveraged for domain adaptation in 3D perception problems. Here we describe the first study of SSL for DA on point clouds. We introduce a new family of pretext tasks, Deformation Reconstruction, inspired by the deformations encountered in sim-to-real transformations. In addition, we propose a novel training procedure for labeled point cloud data motivated by the MixUp method called Point cloud Mixup (PCM). Evaluations on domain adaptations datasets for classification and segmentation, demonstrate a large improvement over existing and baseline methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1452343136",
                        "name": "Idan Achituve"
                    },
                    {
                        "authorId": "3416939",
                        "name": "Haggai Maron"
                    },
                    {
                        "authorId": "1732280",
                        "name": "Gal Chechik"
                    }
                ]
            }
        },
        {
            "contexts": [
                "62 average ECE) than in [30], while they report a 2.",
                "Motivated by the fundamentals and good performance of Mixup, a very recent work [30] has studied how Mixup affects the uncertainty quantification and the calibration performance on DNN.",
                "In the experimental section, we show that some models trained with Mixup do not necessarily improve the calibration, as recently noted in [30].",
                "Finally, on the side of DA strategies, [30] measure the robustness and calibration of Mixup training and [24] propose On-Manifold Adversarial Data Augmentation, which attempts to generate challenging examples by following an on-manifold adversarial attack path in the latent space of a generative model.",
                "By comparing with the results reported in [30], we can conclude that Mixup behaves particularly well in CIFAR100, probably because the intersection between classes can be explained through a linear relation.",
                "In general, our results contrast with those reported in [30] where they provide general improvement in calibration performance due to Mixup."
            ],
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "e11952d036c3802aa85ed6303a50a288a27d20cf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2003-09946",
                    "MAG": "3012855981",
                    "CorpusId": 214612496
                },
                "corpusId": 214612496,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e11952d036c3802aa85ed6303a50a288a27d20cf",
                "title": "Improving Calibration in Mixup-trained Deep Neural Networks through Confidence-Based Loss Functions",
                "abstract": "Deep Neural Networks (DNN) represent the state of the art in many tasks. However, due to their overparameterization, their generalization capabilities are in doubt and are still under study. Consequently, DNN can overfit and assign overconfident predictions, as they tend to learn highly oscillating decision thresholds. This has been shown to affect the calibration of the confidences assigned to unseen data. Data Augmentation (DA) strategies have been proposed to overcome some of these limitations. One of the most popular is Mixup, which has shown a great ability to improve the accuracy of these models. Recent work has provided evidence that Mixup also improves the uncertainty quantification and calibration of DNN. In this work, we argue and provide empirical evidence that, due to its fundamentals, Mixup does not necessarily improve calibration. Based on our observations we propose a new loss function that improves the calibration, and also sometimes the accuracy. Our loss is inspired by Bayes decision theory and introduces a new training framework for designing losses for probabilistic modelling. We provide state-of-the-art accuracy with consistent improvements in calibration performance.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "66769721",
                        "name": "Juan Maro\u00f1as"
                    },
                    {
                        "authorId": "1403102803",
                        "name": "D. Ramos-Castro"
                    },
                    {
                        "authorId": "145111869",
                        "name": "Roberto Paredes Palacios"
                    }
                ]
            }
        },
        {
            "contexts": [
                "62 average ECE) than in [30], while they report a 2.",
                "Motivated by the fundamentals and good performance of Mixup, a very recent work [30] has studied how Mixup affects the uncertainty quantification and the calibration performance on DNN.",
                "In the experimental section, we show that some models trained with Mixup do not necessarily improve the calibration, as recently noted in [30].",
                "Finally, on the side of DA strategies, [30] measure the robustness and calibration of Mixup training and [24] propose On-Manifold Adversarial Data Augmentation, which attempts to generate challenging examples by following an on-manifold adversarial attack path in the latent space of a generative model.",
                "By comparing with the results reported in [30], we can conclude that Mixup behaves particularly well in CIFAR100, probably because the intersection between classes can be explained through a linear relation.",
                "In general, our results contrast with those reported in [30] where they provide general improvement in calibration performance due to Mixup."
            ],
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "966e0f484814ea9a3695cb2d85440b81c4d4f48c",
                "externalIds": {
                    "DBLP": "conf/sspr/MaronasRP20",
                    "ArXiv": "2003.09946",
                    "MAG": "3097920418",
                    "DOI": "10.1007/978-3-030-73973-7_7",
                    "CorpusId": 226191510
                },
                "corpusId": 226191510,
                "publicationVenue": {
                    "id": "1af76ce1-d89d-4026-8b81-9bc22d1fe31c",
                    "name": "International Workshop on Structural and Syntactic Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "S+SSPR",
                        "Int Workshop Struct Syntactic Pattern Recognit"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/966e0f484814ea9a3695cb2d85440b81c4d4f48c",
                "title": "On Calibration of Mixup Training for Deep Neural Networks",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "66769721",
                        "name": "Juan Maro\u00f1as"
                    },
                    {
                        "authorId": "1403102803",
                        "name": "D. Ramos-Castro"
                    },
                    {
                        "authorId": "145111869",
                        "name": "Roberto Paredes Palacios"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0c6a3cd1c97f50729a6aca060addb8cbb03c1965",
                "externalIds": {
                    "MAG": "3011250331",
                    "ArXiv": "2003.07611",
                    "DBLP": "journals/corr/abs-2003-07611",
                    "CorpusId": 212737201
                },
                "corpusId": 212737201,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0c6a3cd1c97f50729a6aca060addb8cbb03c1965",
                "title": "A comprehensive study on the prediction reliability of graph neural networks for virtual screening",
                "abstract": "Prediction models based on deep neural networks are increasingly gaining attention for fast and accurate virtual screening systems. For decision makings in virtual screening, researchers find it useful to interpret an output of classification system as probability, since such interpretation allows them to filter out more desirable compounds. However, probabilistic interpretation cannot be correct for models that hold over-parameterization problems or inappropriate regularizations, leading to unreliable prediction and decision making. In this regard, we concern the reliability of neural prediction models on molecular properties, especially when models are trained with sparse data points and imbalanced distributions. This work aims to propose guidelines for training reliable models, we thus provide methodological details and ablation studies on the following train principles. We investigate the effects of model architectures, regularization methods, and loss functions on the prediction performance and reliability of classification results. Moreover, we evaluate prediction reliability of models on virtual screening scenario. Our result highlights that correct choice of regularization and inference methods is evidently important to achieve high success rate, especially in data imbalanced situation. All experiments were performed under a single unified model implementation to alleviate external randomness in model training and to enable precise comparison of results.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2109001200",
                        "name": "Soojung Yang"
                    },
                    {
                        "authorId": "2115397426",
                        "name": "K. Lee"
                    },
                    {
                        "authorId": "145185994",
                        "name": "Seongok Ryu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Data augmentation methods [30, 34] overcome overfitting by enriching the training data with new artificially generated pseudo data points and labels.",
                "Here we follow the calibration literature [30, 7, 14] and use the negative log likelihood (NLL) loss, i."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0e22713b648a0fd51755bac3d694e3c55ff196e4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2003-06820",
                    "MAG": "3098898971",
                    "ArXiv": "2003.06820",
                    "CorpusId": 212725550
                },
                "corpusId": 212725550,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0e22713b648a0fd51755bac3d694e3c55ff196e4",
                "title": "Intra Order-preserving Functions for Calibration of Multi-Class Neural Networks",
                "abstract": "Predicting calibrated confidence scores for multi-class deep networks is important for avoiding rare but costly mistakes. A common approach is to learn a post-hoc calibration function that transforms the output of the original network into calibrated confidence scores while maintaining the network's accuracy. However, previous post-hoc calibration techniques work only with simple calibration functions, potentially lacking sufficient representation to calibrate the complex function landscape of deep networks. In this work, we aim to learn general post-hoc calibration functions that can preserve the top-k predictions of any deep network. We call this family of functions intra order-preserving functions. We propose a new neural network architecture that represents a class of intra order-preserving functions by combining common neural network components. Additionally, we introduce order-invariant and diagonal sub-families, which can act as regularization for better generalization when the training data size is small. We show the effectiveness of the proposed method across a wide range of datasets and classifiers. Our method outperforms state-of-the-art post-hoc calibration methods, namely temperature scaling and Dirichlet calibration, in multiple settings.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "104610936",
                        "name": "Amir M. Rahimi"
                    },
                    {
                        "authorId": "2966051",
                        "name": "Amirreza Shaban"
                    },
                    {
                        "authorId": "1978613",
                        "name": "Ching-An Cheng"
                    },
                    {
                        "authorId": "3288815",
                        "name": "Byron Boots"
                    },
                    {
                        "authorId": "143750012",
                        "name": "R. Hartley"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Calibration: A recent study (Thulasidasan et al., 2019) showed that DNNs trained with Mixup are significantly better calibrated than DNNs trained in a regular fashion.",
                "We measure the Expected Calibration Error (ECE) (Thulasidasan et al., 2019; Guo et al., 2017) of the proposed method, following (Thulasidasan et al.",
                "Although still in its early phase, the above efforts (Zhang et al., 2017b; Verma et al., 2019; Pang* et al., 2020; Thulasidasan et al., 2019) also indicate a trend to view Mixup from perspectives of robustness and calibration.",
                ", 2017) of the proposed method, following (Thulasidasan et al., 2019): predictions (total N predictions) are grouped into M interval bins (Bm) of equal size.",
                "We measure the Expected Calibration Error (ECE) [26, 8] of the proposed method, following [26].",
                ", 2020) and (Thulasidasan et al., 2019), in fact, have inferences that motivate the need to consider a latent Mixup space to address a model\u2019s robustness and predictive uncertainty.",
                "Other efforts related to Mixup (Thulasidasan et al., 2019) have shown that Mixup-trained networks are better calibrated i."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2c0f44fae5cd6d35c04c532ce8a4b7070d39b224",
                "externalIds": {
                    "ArXiv": "2003.06566",
                    "DBLP": "journals/prl/ManglaSHB21",
                    "DOI": "10.1016/j.patrec.2021.10.016",
                    "CorpusId": 235367617
                },
                "corpusId": 235367617,
                "publicationVenue": {
                    "id": "f35e3e87-9df4-497b-aa0d-bb8584197290",
                    "name": "Pattern Recognition Letters",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit Lett"
                    ],
                    "issn": "0167-8655",
                    "url": "https://www.journals.elsevier.com/pattern-recognition-letters/",
                    "alternate_urls": [
                        "http://www.journals.elsevier.com/pattern-recognition-letters/",
                        "http://www.sciencedirect.com/science/journal/01678655"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2c0f44fae5cd6d35c04c532ce8a4b7070d39b224",
                "title": "On the benefits of defining vicinal distributions in latent space",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "69453609",
                        "name": "Puneet Mangla"
                    },
                    {
                        "authorId": "152902853",
                        "name": "Vedant Singh"
                    },
                    {
                        "authorId": null,
                        "name": "Shreyas Jayant Havaldar"
                    },
                    {
                        "authorId": "1699429",
                        "name": "V. Balasubramanian"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Calibration error: A recent study [42] showed that DNNs trained with Mixup are significantly better calibrated than DNNs trained in a regular fashion.",
                "We measure the Expected Calibration Error(ECE) [42,16] of our trained networks, following [42]: predictions (total N predictions) are grouped into M interval bins (Bm) of equal size.",
                "Other efforts on Mixup [42] have shown that Mixup-trained networks are significantly better calibrated than ones trained in the regular fashion."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "665d28683b29fbaab32ef37833f0c6d885e5d104",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2003-06566",
                    "MAG": "3011529506",
                    "CorpusId": 212725078
                },
                "corpusId": 212725078,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/665d28683b29fbaab32ef37833f0c6d885e5d104",
                "title": "VarMixup: Exploiting the Latent Space for Robust Training and Inference",
                "abstract": "The vulnerability of Deep Neural Networks (DNNs) to adversarial attacks has led to the development of many defense approaches. Among them, Adversarial Training (AT) is a popular and widely used approach for training adversarially robust models. Mixup Training (MT), a recent popular training algorithm, improves the generalization performance of models by introducing globally linear behavior in between training examples. Although still in its early phase, we observe a shift in trend of exploiting Mixup from perspectives of generalisation to that of adversarial robustness. It has been shown that the Mixup trained models improves the robustness of models but only passively. A recent approach, Mixup Inference (MI), proposes an inference principle for Mixup trained models to counter adversarial examples at inference time by mixing the input with other random clean samples. In this work, we propose a new approach - \\textit{VarMixup (Variational Mixup)} - to better sample mixup images by using the latent manifold underlying the data. Our experiments on CIFAR-10, CIFAR-100, SVHN and Tiny-Imagenet demonstrate that \\textit{VarMixup} beats state-of-the-art AT techniques without training the model adversarially. Additionally, we also conduct ablations that show that models trained on \\textit{VarMixup} samples are also robust to various input corruptions/perturbations, have low calibration error and are transferable.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "69453609",
                        "name": "Puneet Mangla"
                    },
                    {
                        "authorId": "152902853",
                        "name": "Vedant Singh"
                    },
                    {
                        "authorId": null,
                        "name": "Shreyas Jayant Havaldar"
                    },
                    {
                        "authorId": "1699429",
                        "name": "V. Balasubramanian"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[14, 15] and Label Smoothing [16, 17] that were part of high performance deep networks for classification were later shown empirically to achieve calibration."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ecbe4eaf35f6b3086cec03a25b94a11c205782cb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2002-12860",
                    "ArXiv": "2002.12860",
                    "MAG": "3007072242",
                    "CorpusId": 211572856
                },
                "corpusId": 211572856,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ecbe4eaf35f6b3086cec03a25b94a11c205782cb",
                "title": "Quantile Regularization: Towards Implicit Calibration of Regression Models",
                "abstract": "Recent works have shown that most deep learning models are often poorly calibrated, i.e., they may produce overconfident predictions that are wrong. It is therefore desirable to have models that produce predictive uncertainty estimates that are reliable. Several approaches have been proposed recently to calibrate classification models. However, there is relatively little work on calibrating regression models. We present a method for calibrating regression models based on a novel quantile regularizer defined as the cumulative KL divergence between two CDFs. Unlike most of the existing approaches for calibrating regression models, which are based on post-hoc processing of the model's output and require an additional dataset, our method is trainable in an end-to-end fashion without requiring an additional dataset. The proposed regularizer can be used with any training objective for regression. We also show that post-hoc calibration methods like Isotonic Calibration sometimes compound miscalibration whereas our method provides consistently better calibrations. We provide empirical results demonstrating that the proposed quantile regularizer significantly improves calibration for regression models trained using approaches, such as Dropout VI and Deep Ensembles.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1512255229",
                        "name": "Saiteja Utpala"
                    },
                    {
                        "authorId": "145593549",
                        "name": "Piyush Rai"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several works [10, 19, 22, 6, 29, 17, 13, 25, 18] aim to address this calibration challenge."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2c5d7cc351441a2fd041e6c16349977332af8bed",
                "externalIds": {
                    "MAG": "3007458866",
                    "DBLP": "journals/corr/abs-2002-09831",
                    "ArXiv": "2002.09831",
                    "CorpusId": 211259293
                },
                "corpusId": 211259293,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2c5d7cc351441a2fd041e6c16349977332af8bed",
                "title": "On the Role of Dataset Quality and Heterogeneity in Model Confidence",
                "abstract": "Safety-critical applications require machine learning models that output accurate and calibrated probabilities. While uncalibrated deep networks are known to make over-confident predictions, it is unclear how model confidence is impacted by the variations in the data, such as label noise or class size. In this paper, we investigate the role of the dataset quality by studying the impact of dataset size and the label noise on the model confidence. We theoretically explain and experimentally demonstrate that, surprisingly, label noise in the training data leads to under-confident networks, while reduced dataset size leads to over-confident models. We then study the impact of dataset heterogeneity, where data quality varies across classes, on model confidence. We demonstrate that this leads to heterogenous confidence/accuracy behavior in the test data and is poorly handled by the standard calibration algorithms. To overcome this, we propose an intuitive heterogenous calibration technique and show that the proposed approach leads to improved calibration metrics (both average and worst-case errors) on the CIFAR datasets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2143753767",
                        "name": "Yuan Zhao"
                    },
                    {
                        "authorId": "1391202254",
                        "name": "Jiasi Chen"
                    },
                    {
                        "authorId": "3103394",
                        "name": "Samet Oymak"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To do this, we use ResNet-110 and Wide-ResNet-26-10 trained on CIFAR-10 and consider the SVHN [23] test set and CIFAR-10-C [9] with Gaussian noise corruption at severity 5 as OoD data.",
                "Furthermore, we empirically observe that models trained using focal loss are not only better calibrated under i.i.d. assumptions, but can also be better at detecting OoD samples which we show by taking CIFAR-10 as the in-distribution dataset and SVHN and CIFAR-10-C as out-of-distribution datasets, something which temperature scaling fails to achieve.",
                "More advantages of focal loss: Behaviour on Out-of-Distribution (OoD) data: A perfectly calibrated model should have low confidence whenever it misclassifies, including when it encounters data which is OoD [34].",
                "Finally, we also make the interesting observation that whilst temperature scaling may not work for detecting out-ofdistribution (OoD) samples, our approach can.",
                "Since focal loss has implicit regularisation effects on the network (see \u00a74), we investigate if it helps to learn representations that are more robust to OoD data."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "07d440f44f5f955afef3c32f2610c7a716c36f97",
                "externalIds": {
                    "ArXiv": "2002.09437",
                    "DBLP": "conf/nips/MukhotiKSGTD20",
                    "MAG": "3104668038",
                    "CorpusId": 211252346
                },
                "corpusId": 211252346,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/07d440f44f5f955afef3c32f2610c7a716c36f97",
                "title": "Calibrating Deep Neural Networks using Focal Loss",
                "abstract": "Miscalibration - a mismatch between a model's confidence and its correctness - of Deep Neural Networks (DNNs) makes their predictions hard to rely on. Ideally, we want networks to be accurate, calibrated and confident. We show that, as opposed to the standard cross-entropy loss, focal loss [Lin et. al., 2017] allows us to learn models that are already very well calibrated. When combined with temperature scaling, whilst preserving accuracy, it yields state-of-the-art calibrated models. We provide a thorough analysis of the factors causing miscalibration, and use the insights we glean from this to justify the empirically excellent performance of focal loss. To facilitate the use of focal loss in practice, we also provide a principled approach to automatically select the hyperparameter involved in the loss function. We perform extensive experiments on a variety of computer vision and NLP datasets, and with a wide variety of network architectures, and show that our approach achieves state-of-the-art calibration without compromising on accuracy in almost all cases. Code is available at this https URL",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "7438962",
                        "name": "Jishnu Mukhoti"
                    },
                    {
                        "authorId": "3468926",
                        "name": "Viveka Kulharia"
                    },
                    {
                        "authorId": "3494481",
                        "name": "Amartya Sanyal"
                    },
                    {
                        "authorId": "143777501",
                        "name": "S. Golodetz"
                    },
                    {
                        "authorId": "143635540",
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "authorId": "144679302",
                        "name": "P. Dokania"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "994e398a62747ce81fa361ff400fa5016ee064dd",
                "externalIds": {
                    "MAG": "3034672614",
                    "ArXiv": "2002.08709",
                    "DBLP": "journals/corr/abs-2002-08709",
                    "CorpusId": 211205200
                },
                "corpusId": 211205200,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/994e398a62747ce81fa361ff400fa5016ee064dd",
                "title": "Do We Need Zero Training Loss After Achieving Zero Training Error?",
                "abstract": "Overparameterized deep networks have the capacity to memorize training data with zero training error. Even after memorization, the training loss continues to approach zero, making the model overconfident and the test performance degraded. Since existing regularizers do not directly aim to avoid zero training loss, they often fail to maintain a moderate level of training loss, ending up with a too small or too large loss. We propose a direct solution called flooding that intentionally prevents further reduction of the training loss when it reaches a reasonably small value, which we call the flooding level. Our approach makes the loss float around the flooding level by doing mini-batched gradient descent as usual but gradient ascent if the training loss is below the flooding level. This can be implemented with one line of code, and is compatible with any stochastic optimizer and other regularizers. With flooding, the model will continue to \"random walk\" with the same non-zero training loss, and we expect it to drift into an area with a flat loss landscape that leads to better generalization. We experimentally show that flooding improves performance and as a byproduct, induces a double descent curve of the test loss.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2069409929",
                        "name": "Takashi Ishida"
                    },
                    {
                        "authorId": "2210255",
                        "name": "I. Yamane"
                    },
                    {
                        "authorId": "151357825",
                        "name": "Tomoya Sakai"
                    },
                    {
                        "authorId": "47537639",
                        "name": "Gang Niu"
                    },
                    {
                        "authorId": "67154907",
                        "name": "Masashi Sugiyama"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While many approaches aim to address this network overconfidence problem (Blundell et al. 2015; Gal and Ghahramani 2016; Lee et al. 2018; Thulasidasan et al. 2019), BAYES-TREX is complementary to these efforts."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "131b8dd9398232a55cab6a7a1b44ebe24161cbe3",
                "externalIds": {
                    "DBLP": "conf/aaai/BoothZ0S21",
                    "MAG": "3113260814",
                    "DOI": "10.1609/aaai.v35i13.17361",
                    "CorpusId": 229280739
                },
                "corpusId": 229280739,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/131b8dd9398232a55cab6a7a1b44ebe24161cbe3",
                "title": "Bayes-TrEx: a Bayesian Sampling Approach to Model Transparency by Example",
                "abstract": "Post-hoc explanation methods are gaining popularity for interpreting, understanding, and debugging neural networks. Most analyses using such methods explain decisions in response to inputs drawn from the test set. However, the test set may have few \nexamples that trigger some model behaviors, such as high-confidence failures or ambiguous classifications. To address these challenges, we introduce a flexible model inspection framework: Bayes-TrEx. Given a data distribution, Bayes-TrEx finds in-distribution examples which trigger a specified prediction confidence. We demonstrate several use cases of Bayes-TrEx, including revealing highly confident (mis)classifications, visualizing class boundaries via ambiguous examples, understanding novel-class extrapolation behavior, and exposing neural network overconfidence. We use Bayes-TrEx to study classifiers trained on CLEVR, MNIST, and Fashion-MNIST, and we show that this framework enables more flexible holistic model analysis than just inspecting the test set. Code and supplemental material are available at https://github.com/serenabooth/Bayes-TrEx.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "7594244",
                        "name": "S. Booth"
                    },
                    {
                        "authorId": "2110339246",
                        "name": "Yilun Zhou"
                    },
                    {
                        "authorId": "47287735",
                        "name": "Ankit J. Shah"
                    },
                    {
                        "authorId": "143873972",
                        "name": "J. Shah"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While many approaches aim to address this neural network overconfidence problem (e.g. Thulasidasan et al., 2019; Lee et al., 2017; Gal & Ghahramani, 2016; Blundell et al., 2015), our work is complementary to these efforts."
            ],
            "intents": [
                "result"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2ce0b114ee2f682f19a44a06cfd99731aa5122e2",
                "externalIds": {
                    "ArXiv": "2002.10248",
                    "DBLP": "journals/corr/abs-2002-10248",
                    "MAG": "3008950730",
                    "CorpusId": 211259039
                },
                "corpusId": 211259039,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2ce0b114ee2f682f19a44a06cfd99731aa5122e2",
                "title": "Bayes-Probe: Distribution-Guided Sampling for Prediction Level Sets",
                "abstract": "Building machine learning models requires a suite of tools for interpretation, understanding, and debugging. Many existing methods have been proposed, but it can still be difficult to probe for examples which communicate model behaviour. We introduce Bayes-Probe, a model inspection method for analyzing neural networks by generating distribution-conforming examples of known prediction confidence. By selecting appropriate distributions and confidence prediction values, Bayes-Probe can be used to synthesize ambivalent predictions, uncover in-distribution adversarial examples, and understand novel-class extrapolation and domain adaptation behaviours. Bayes-Probe is model agnostic, requiring only a data generator and classifier prediction. We use Bayes-Probe to analyze models trained on both procedurally-generated data (CLEVR) and organic data (MNIST and Fashion-MNIST). Code is available at https://github.com/serenabooth/Bayes-Probe.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "7594244",
                        "name": "S. Booth"
                    },
                    {
                        "authorId": "2110339246",
                        "name": "Yilun Zhou"
                    },
                    {
                        "authorId": "47287735",
                        "name": "Ankit J. Shah"
                    },
                    {
                        "authorId": "143873972",
                        "name": "J. Shah"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ba7296e44a30d8f4e86052f743b395c4462a5596",
                "externalIds": {
                    "MAG": "3037387613",
                    "CorpusId": 220250779
                },
                "corpusId": 220250779,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ba7296e44a30d8f4e86052f743b395c4462a5596",
                "title": "Bayes-TrEx: Model Transparency by Example",
                "abstract": "Post-hoc explanation methods are gaining popularity as tools for interpreting, understanding, and debugging neural networks. Most post-hoc methods explain decisions in response to individual inputs. These individual inputs are typically drawn from the test set; however, the test set may be biased or may only sparsely invoke some model behaviours. To address these challenges, we introduce Bayes-TrEx, a model-agnostic method for generating distribution-conforming examples of known prediction confidence. Using a classifier prediction and a data generator, Bayes-TrEx can be used to visualize class boundaries; to find in-distribution adversarial examples; to understand novel-class extrapolation; and to expose neural network overconfidence. We demonstrate Bayes-TrEx with rendered data (CLEVR) and organic data (MNIST, Fashion-MNIST). Code: this http URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "7594244",
                        "name": "S. Booth"
                    },
                    {
                        "authorId": "2110339246",
                        "name": "Yilun Zhou"
                    },
                    {
                        "authorId": "47287735",
                        "name": "Ankit J. Shah"
                    },
                    {
                        "authorId": "143873972",
                        "name": "J. Shah"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018) and its variants (Thulasidasan et al., 2019; Verma et al., 2018), and Label smoothing (Shafahi et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8de0b5b58f62f8fbc6c5b88692bcabcd93eadb30",
                "externalIds": {
                    "MAG": "3005576178",
                    "DBLP": "conf/ijcai/ChengLCDH22",
                    "ArXiv": "2002.06789",
                    "DOI": "10.24963/ijcai.2022/95",
                    "CorpusId": 211132607
                },
                "corpusId": 211132607,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/8de0b5b58f62f8fbc6c5b88692bcabcd93eadb30",
                "title": "CAT: Customized Adversarial Training for Improved Robustness",
                "abstract": "Adversarial training has become one of the most effective methods for improving robustness of neural networks. However, it often suffers from poor generalization on both clean and perturbed data. Current robust training method always use a uniformed perturbation strength for every samples to generate adversarial examples during model training for improving adversarial robustness. However, we show it would lead worse training and generalizaiton error and forcing the prediction to match one-hot label.\n\nIn this paper, therefore, we propose a new algorithm, named Customized Adversarial Training (CAT), which adaptively customizes the perturbation level and the corresponding label for each training sample in adversarial training. We first show theoretically the CAT scheme improves the generalization. Also, through extensive experiments, we show that the proposed algorithm achieves better clean and robust accuracy than previous adversarial training methods. The full version of this paper is available at https://arxiv.org/abs/2002.06789.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2424698",
                        "name": "Minhao Cheng"
                    },
                    {
                        "authorId": "144438755",
                        "name": "Qi Lei"
                    },
                    {
                        "authorId": "153191489",
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "authorId": "1783667",
                        "name": "I. Dhillon"
                    },
                    {
                        "authorId": "1793529",
                        "name": "Cho-Jui Hsieh"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2017) or by intelligently varying the inputs (Thulasidasan et al., 2019).",
                "However, deep networks become overconfident from overfitting, which can be partially addressed by the usage of normalization and weight decay (Guo et al., 2017) or by intelligently varying the inputs (Thulasidasan et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "83373f1bf2d5366caa4b44107ea5d6df3dc3a471",
                "externalIds": {
                    "ArXiv": "2002.05212",
                    "DBLP": "journals/corr/abs-2002-05212",
                    "MAG": "3005789139",
                    "PubMedCentral": "9231643",
                    "CorpusId": 211096611,
                    "PubMed": "35754923"
                },
                "corpusId": 211096611,
                "publicationVenue": {
                    "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                    "name": "Journal of machine learning research",
                    "type": "journal",
                    "alternate_names": [
                        "Journal of Machine Learning Research",
                        "J mach learn res",
                        "J Mach Learn Res"
                    ],
                    "issn": "1532-4435",
                    "alternate_issns": [
                        "1533-7928"
                    ],
                    "url": "http://www.ai.mit.edu/projects/jmlr/",
                    "alternate_urls": [
                        "http://jmlr.csail.mit.edu/",
                        "http://www.jmlr.org/",
                        "http://portal.acm.org/affiliated/jmlr"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/83373f1bf2d5366caa4b44107ea5d6df3dc3a471",
                "title": "Estimating Uncertainty Intervals from Collaborating Networks",
                "abstract": "Effective decision making requires understanding the uncertainty inherent in a prediction. In regression, this uncertainty can be estimated by a variety of methods; however, many of these methods are laborious to tune, generate overconfident uncertainty intervals, or lack sharpness (give imprecise intervals). We address these challenges by proposing a novel method to capture predictive distributions in regression by defining two neural networks with two distinct loss functions. Specifically, one network approximates the cumulative distribution function, and the second network approximates its inverse. We refer to this method as Collaborating Networks (CN). Theoretical analysis demonstrates that a fixed point of the optimization is at the idealized solution, and that the method is asymptotically consistent to the ground truth distribution. Empirically, learning is straightforward and robust. We benchmark CN against several common approaches on two synthetic and six real-world datasets, including forecasting A1c values in diabetic patients from electronic health records, where uncertainty is critical. In the synthetic data, the proposed approach essentially matches ground truth. In the real-world datasets, CN improves results on many performance metrics, including log-likelihood estimates, mean absolute errors, coverage estimates, and prediction interval widths.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50018367",
                        "name": "Tianhui Zhou"
                    },
                    {
                        "authorId": "50024168",
                        "name": "Yitong Li"
                    },
                    {
                        "authorId": "2145065676",
                        "name": "Yuan Wu"
                    },
                    {
                        "authorId": "2064615977",
                        "name": "David Carlson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is a surprising result since it has been empirically showed in existing works [22] that augmentation strategies such as mixup often produce highly",
                "Recently, in [22], it was found that mixup regularization led to improved calibration in the resulting model.",
                "Surprisingly, state-of-the-methods such as mixup, which produce highly calibrated models [22] (in 2(a), mixup achieves the lowest ECE at 0% compression), do not always produce tickets that are inherently well-calibrated."
            ],
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a41183d756e2078b61903b70cd606f4c0b13e2fe",
                "externalIds": {
                    "MAG": "3005249894",
                    "ArXiv": "2002.03875",
                    "DBLP": "journals/corr/abs-2002-03875",
                    "CorpusId": 211069074
                },
                "corpusId": 211069074,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a41183d756e2078b61903b70cd606f4c0b13e2fe",
                "title": "Calibrate and Prune: Improving Reliability of Lottery Tickets Through Prediction Calibration",
                "abstract": "The hypothesis that sub-network initializations (lottery) exist within the initializations of over-parameterized networks, which when trained in isolation produce highly generalizable models, has led to crucial insights into network initialization and has enabled efficient inferencing. Supervised models with uncalibrated confidences tend to be overconfident even when making wrong prediction. In this paper, for the first time, we study how explicit confidence calibration in the over-parameterized network impacts the quality of the resulting lottery tickets. More specifically, we incorporate a suite of calibration strategies, ranging from mixup regularization, variance-weighted confidence calibration to the newly proposed likelihood-based calibration and normalized bin assignment strategies. Furthermore, we explore different combinations of architectures and datasets, and make a number of key findings about the role of confidence calibration. Our empirical studies reveal that including calibration mechanisms consistently lead to more effective lottery tickets, in terms of accuracy as well as empirical calibration metrics, even when retrained using data with challenging distribution shifts with respect to the source dataset.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "153441560",
                        "name": "Bindya Venkatesh"
                    },
                    {
                        "authorId": "1744175",
                        "name": "Jayaraman J. Thiagarajan"
                    },
                    {
                        "authorId": "51149615",
                        "name": "Kowshik Thopalli"
                    },
                    {
                        "authorId": "1706272",
                        "name": "P. Sattigeri"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Figure 1 displays this phenomenon which has been replicated following the details provided by [7].",
                "In this work we aim to replicate the results reported by [7] on their analysis of the effect of Mixup [5] on a network\u2019s calibration."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6a51bfe7da3c72154af5d731f340d935d230d036",
                "externalIds": {
                    "MAG": "3088218098",
                    "CorpusId": 226808232
                },
                "corpusId": 226808232,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6a51bfe7da3c72154af5d731f340d935d230d036",
                "title": "On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks NeurIPS Reproducibility Challenge 2019",
                "abstract": "Miscalibration of a model is defined as the mismatch between predicting probability estimates and the true correctness likelihood. In this work we aim to replicate the results reported by [7] on their analysis of the effect of Mixup [5] on a network\u2019s calibration. Mixup is an effective yet simple approach of data augmentation which generates a convex combination of a pair of training images and their corresponding labels as the input and target for training a network. We replicate the results reported by the authors for CIFAR-100[6], Fashion-MNIST[10], STL-10[2], out-of-distribution and random noise data.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2109434876",
                        "name": "Aditya Singh"
                    },
                    {
                        "authorId": "39720629",
                        "name": "Alessandro Bay"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Overfitting to training data with one-hot encoded or hard labels [18], and over-confidence of ReLU networks for out-of-data inputs [5] have been identified as potential root causes for this behavior.",
                "In [18] it was shown that Mixup not only improves generalization, but also yields well-"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "94ac3cfdda587680bc29329721599caee9145c2d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1912-07458",
                    "ArXiv": "1912.07458",
                    "MAG": "2995851819",
                    "DOI": "10.1109/ICPR48806.2021.9413010",
                    "CorpusId": 209376583
                },
                "corpusId": 209376583,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/94ac3cfdda587680bc29329721599caee9145c2d",
                "title": "On-manifold Adversarial Data Augmentation Improves Uncertainty Calibration",
                "abstract": "Uncertainty estimates help to identify ambiguous, novel, or anomalous inputs, but the reliable quantification of uncertainty has proven to be challenging for modern deep networks. To improve uncertainty estimation, we propose On-Manifold Adversarial Data Augmentation or OMADA, which specifically attempts to generate challenging examples by following an on-manifold adversarial attack path in the latent space of an autoencoder that closely approximates the decision boundaries between classes. On a variety of datasets and for multiple network architectures, OMADA consistently yields more accurate and better calibrated classifiers than baseline models, and outperforms competing approaches such as Mixup, as well as achieving similar performance to (at times better than) postprocessing calibration methods such as temperature scaling. Variants of OMADA can employ different sampling schemes for ambiguous on-manifold examples based on the entropy of their estimated soft labels, which exhibit specific strengths for generalization, calibration of predicted uncertainty, or detection of out-of-distribution inputs.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "51290549",
                        "name": "Kanil Patel"
                    },
                    {
                        "authorId": "52020792",
                        "name": "William H. Beluch"
                    },
                    {
                        "authorId": "2109979241",
                        "name": "Dan Zhang"
                    },
                    {
                        "authorId": "144578436",
                        "name": "Michael Pfeiffer"
                    },
                    {
                        "authorId": "2118581303",
                        "name": "Bin Yang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "096033bd0d4bc867c7be1b8220d9afdc22c03cdc",
                "externalIds": {
                    "ArXiv": "1912.01730",
                    "DBLP": "conf/iclr/XingAZP20",
                    "MAG": "2996317113",
                    "CorpusId": 208617304
                },
                "corpusId": 208617304,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/096033bd0d4bc867c7be1b8220d9afdc22c03cdc",
                "title": "Distance-Based Learning from Errors for Confidence Calibration",
                "abstract": "Deep neural networks (DNNs) are poorly-calibrated when trained in conventional ways. To improve confidence calibration of DNNs, we propose a novel training method, distance-based learning from errors (DBLE). DBLE bases its confidence estimation on distances in the representation space. We first adapt prototypical learning for training of a classification model for DBLE. It yields a representation space where a test sample's distance to its ground-truth class center can calibrate the model's performance. At inference, however, these distances are not available due to the lack of ground-truth label. To circumvent this by approximately inferring the distance for every test sample, we propose to train a confidence model jointly with the classification model, by merely learning from mis-classified training samples, which we show to be highly-beneficial for effective learning. On multiple data sets and DNN architectures, we demonstrate that DBLE outperforms alternative single-modal confidence calibration approaches. DBLE also achieves comparable performance with computationally-expensive ensemble approaches with lower computational cost and lower number of parameters.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "50461046",
                        "name": "Chen Xing"
                    },
                    {
                        "authorId": "2676352",
                        "name": "Sercan \u00d6. Arik"
                    },
                    {
                        "authorId": "2476328",
                        "name": "Zizhao Zhang"
                    },
                    {
                        "authorId": "1945962",
                        "name": "Tomas Pfister"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mixup has been shown to yield several benefits, such as reducing overfitting and better calibrating the confidence of deep learning models [11, 19]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8ff3f7f3b27415672bc8363b69ce79a6f1dc5015",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1912-00772",
                    "ArXiv": "1912.00772",
                    "MAG": "2991478099",
                    "CorpusId": 208526909
                },
                "corpusId": 208526909,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8ff3f7f3b27415672bc8363b69ce79a6f1dc5015",
                "title": "Data Augmentation for Deep Transfer Learning",
                "abstract": "Current approaches to deep learning are beginning to rely heavily on transfer learning as an effective method for reducing overfitting, improving model performance, and quickly learning new tasks. Similarly, such pre-trained models are often used to create embedding representations for various types of data, such as text and images, which can then be fed as input into separate, downstream models. However, in cases where such transfer learning models perform poorly (i.e., for data outside of the training distribution), one must resort to fine-tuning such models, or even retraining them completely. Currently, no form of data augmentation has been proposed that can be applied directly to embedding inputs to improve downstream model performance. In this work, we introduce four new types of data augmentation that are generally applicable to embedding inputs, thus making them useful in both Natural Language Processing (NLP) and Computer Vision (CV) applications. For models trained on downstream tasks with such embedding inputs, these augmentation methods are shown to improve the AUC score of the models from a score of 0.9582 to 0.9812 and significantly increase the model's ability to identify classes of data that are not seen during training.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "34210029",
                        "name": "Cameron R. Wolfe"
                    },
                    {
                        "authorId": "11196604",
                        "name": "Keld T. Lundgaard"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2017), which performs augmentation by taking a weighted average of two input images, was proposed and shown to improve model performance (Liang et al., 2018; Thulasidasan et al., 2019).",
                "More recently, Mixup (Zhang et al., 2017), which performs augmentation by taking a weighted average of two input images, was proposed and shown to improve model performance (Liang et al., 2018; Thulasidasan et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b4101e777b0de179c4e9a05dbf81e07009150f04",
                "externalIds": {
                    "MAG": "3091885843",
                    "CorpusId": 222175855
                },
                "corpusId": 222175855,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b4101e777b0de179c4e9a05dbf81e07009150f04",
                "title": "E-Stitchup: Data Augmentation for Pre-Trained Embeddings",
                "abstract": "In this work, we propose data augmentation methods for embeddings from pre-trained deep learning models that take a weighted combination of a pair of input embeddings, as inspired by Mixup, and combine such augmentation with extra label softening. These methods are shown to significantly increase classification accuracy, reduce training time, and improve confidence calibration of a downstream model that is trained with them. As a result of such improved confidence calibration, the model output can be more intuitively interpreted and used to accurately identify out-of-distribution data by applying an appropriate confidence threshold to model predictions. The identified out-of-distribution data can then be prioritized for labeling, thus focusing labeling effort on data that is more likely to boost model performance. These findings, we believe, lay a solid foundation for improving the classification performance and calibration of models that use pre-trained embeddings as input and provide several benefits that prove extremely useful in a production-level deep learning system.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "34210029",
                        "name": "Cameron R. Wolfe"
                    },
                    {
                        "authorId": "11196604",
                        "name": "Keld T. Lundgaard"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Finally, intrinsically uncertainty-aware neural networks is a very active research field and independent and concurrent studies include new takes on Bayesian neural networks (Joo, Chung, and Seo 2020; Chan et al. 2020) and an extension of MixUp (Zhang, Kailkhura, and Han 2020).",
                "We also compare the additional baselines verified uncertainty calibration (VUC) (Kumar, Liang, and Ma 2019), and MixUp (Thulasidasan et al. 2019); to illustrate that the different modeling assumptions of OOD detection methods do not translate into calibrated predicted uncertainty under domain drift, we also jointly trained a classifier and a GAN (Lee et al. 2017).",
                "Considering perturbations at maximum strength (epsilon 90), FALCON is the only model yielding uncertainty-aware confidence scores at a median of less than 0.5 (for VUC and MixUp see Appendix).",
                "Our findings for baseline methods confirm their results, in particular for deep ensembles and SVI (they did not consider EDL, MNF, VUC and MixUp).",
                "More recently, Thulasidasan et al. (2019) have shown that using MixUp training, where label- and input smoothing is performed, yields good results for in-domain calibration.",
                "We also compare the additional baselines verified uncertainty calibration (VUC) (Kumar, Liang, and Ma 2019), and MixUp (Thulasidasan et al. 2019); to illustrate that the different modeling assumptions of OOD detection methods do not translate into calibrated predicted uncertainty under domain\u2026",
                "We also compare the additional baselines verified uncertainty calibration (VUC) (Kumar, Liang, and Ma 2019), and MixUp (Thulasidasan et al. 2019); to illustrate that the different modeling assumptions of OOD detection methods do not translate into calibrated predicted uncertainty under domain drift, we also jointly trained a classifier and a GAN (Lee et al."
            ],
            "intents": [
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "d3b1cfec88ff61e495a5aeec3e8126651c2472cb",
                "externalIds": {
                    "MAG": "3004554494",
                    "DBLP": "conf/aaai/TomaniB21",
                    "ArXiv": "2012.10923",
                    "DOI": "10.1609/aaai.v35i11.17188",
                    "CorpusId": 209475154
                },
                "corpusId": 209475154,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d3b1cfec88ff61e495a5aeec3e8126651c2472cb",
                "title": "Towards Trustworthy Predictions from Deep Neural Networks with Fast Adversarial Calibration",
                "abstract": "To facilitate a wide-spread acceptance of AI systems guiding decision making in real-world applications, trustworthiness of deployed models is key. That is, it is crucial for predictive models to be uncertainty-aware and yield well-calibrated (and thus trustworthy) predictions for both in-domain samples as well as under domain shift. Recent efforts to account for predictive uncertainty include post-processing steps for trained neural networks, Bayesian neural networks as well as alternative non-Bayesian approaches such as ensemble approaches and evidential deep learning. Here, we propose an efficient yet general modelling approach for obtaining well-calibrated, trustworthy probabilities for samples obtained after a domain shift. We introduce a new training strategy combining an entropy-encouraging loss term with an adversarial calibration loss term and demonstrate that this results in well-calibrated and technically trustworthy predictions for a wide range of domain drifts. We comprehensively evaluate previously proposed approaches on different data modalities, a large range of data sets including sequence data, network architectures and perturbation strategies. We observe that our modelling approach substantially outperforms existing state-of-the-art approaches, yielding well-calibrated predictions under domain drift.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1583683907",
                        "name": "Christian Tomani"
                    },
                    {
                        "authorId": "2315845",
                        "name": "F. Buettner"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018) and its variants (Verma et al., 2018; Thulasidasan et al., 2019) are a recently proposed approach to improve model robustness and generalization by training a model on convex combinations of data sample pairs and their labels.",
                "Mixup (Zhang et al., 2018) and its variants (Verma et al., 2018; Thulasidasan et al., 2019) are a recently proposed approach to improve model robustness and generalization by training a model on convex combinations of data sample pairs and their labels."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4846d33d2019b7ab361556526c96b9dac0acbd60",
                "externalIds": {
                    "MAG": "3006298810",
                    "CorpusId": 209479244
                },
                "corpusId": 209479244,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4846d33d2019b7ab361556526c96b9dac0acbd60",
                "title": "SPROUT: Self-Progressing Robust Training",
                "abstract": "Enhancing model robustness under new and even adversarial environments is a crucial milestone toward building trustworthy and reliable machine learning systems. Current robust training methods such as adversarial training explicitly specify an \u201cattack\u201d (e.g., `\u221e-norm bounded perturbation) to generate adversarial examples during model training in order to improve adversarial robustness. In this paper, we take a different perspective and propose a new framework SPROUT, self-progressing robust training. During model training, SPROUT progressively adjusts training label distribution via our proposed parametrized label smoothing technique, making training free of attack generation and more scalable. We also motivate SPROUT using a general formulation based on vicinity risk minimization, which includes many robust training methods as special cases. Compared with stateof-the-art adversarial training methods (PGD-`\u221e and TRADES) under `\u221e-norm bounded attacks and various invariance tests, SPROUT consistently attains superior performance and is more scalable to large neural networks. Our results shed new light on scalable, effective and attack-independent robust training methods.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2424698",
                        "name": "Minhao Cheng"
                    },
                    {
                        "authorId": "153191489",
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "authorId": "143743061",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "3307026",
                        "name": "Shiyu Chang"
                    },
                    {
                        "authorId": "1793529",
                        "name": "Cho-Jui Hsieh"
                    },
                    {
                        "authorId": "1730372",
                        "name": "Payel Das"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another promising approach could be using recent data augmentation techniques [59], [60] or strategies based on pretrained models [37]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0d22431900ba2eb3f014f92787ce46053b4deb70",
                "externalIds": {
                    "DBLP": "conf/ijcnn/MacedoRZOL21",
                    "MAG": "3106636111",
                    "ArXiv": "1908.05569",
                    "DOI": "10.1109/IJCNN52387.2021.9533899",
                    "CorpusId": 227225527
                },
                "corpusId": 227225527,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/0d22431900ba2eb3f014f92787ce46053b4deb70",
                "title": "Entropic Out-of-Distribution Detection",
                "abstract": "Out-of-distribution (OOD) detection approaches usually present special requirements (e.g., hyperparameter validation, collection of outlier data) and produce side effects (e.g., classification accuracy drop, slower energy-inefficient inferences). We argue that these issues are a consequence of the SoftMax loss anisotropy and disagreement with the maximum entropy principle. Thus, we propose the IsoMax loss and the entropic score. The seamless drop-in replacement of the SoftMax loss by IsoMax loss requires neither additional data collection nor hyperparameter validation. The trained models do not exhibit classification accuracy drop and produce fast energy-efficient inferences. Moreover, our experiments show that training neural networks with IsoMax loss significantly improves their OOD detection performance. The IsoMax loss exhibits state-of-the-art performance under the mentioned conditions (fast energy-efficient inference, no classification accuracy drop, no collection of outlier data, and no hyperparameter validation), which we call the seamless OOD detection task. In future work, current OOD detection methods may replace the SoftMax loss with the IsoMax loss to improve their performance on the commonly studied non-seamless OOD detection problem.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "51473026",
                        "name": "David Mac\u00eado"
                    },
                    {
                        "authorId": "1769575",
                        "name": "T. I. Ren"
                    },
                    {
                        "authorId": "2948325",
                        "name": "C. Zanchettin"
                    },
                    {
                        "authorId": "40057895",
                        "name": "Adriano Oliveira"
                    },
                    {
                        "authorId": "1746612",
                        "name": "Teresa B Ludermir"
                    }
                ]
            }
        },
        {
            "contexts": [
                "uts to decide works even better when many classes are presented. We speculate that recent advances in data augmentation techniques may help to improve IsoMax+ES OOD detection performance even further [52], [53]. 4.3 Robustness Analysis Fig. 3 presents OOD detection performance of SoftMax and IsoMax losses in many models (DenseNet and ResNet), metrics (AUROC and TNR@TPR95), and datasets (SVHN, CIFAR10,"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "fbbe267abce26ebf61ed7db9842ebf2cda290876",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1908-05569",
                    "MAG": "2967144045",
                    "CorpusId": 199668851
                },
                "corpusId": 199668851,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fbbe267abce26ebf61ed7db9842ebf2cda290876",
                "title": "Distinction Maximization Loss: Fast, Scalable, Turnkey, and Native Neural Networks Out-of-Distribution Detection simply by Replacing the SoftMax Loss",
                "abstract": "Recently, many methods to reduce neural networks uncertainty have been proposed. However, most of the techniques used in these solutions usually present severe drawbacks. In this paper, we argue that neural networks low out-of-distribution detection performance is mainly due to the SoftMax loss anisotropy. Therefore, we built an isotropic loss to reduce neural networks uncertainty in a fast, scalable, turnkey, and native approach. Our experiments show that replacing SoftMax with the proposed loss does not affect classification accuracy. Moreover, our proposal overcomes ODIN typically by a large margin while producing usually competitive results against a state-of-the-art Mahalanobis method despite avoiding their limitations. Hence, neural networks uncertainty may be significantly reduced by a simple loss change without relying on special procedures such as data augmentation, adversarial training/validation, ensembles, or additional classification/regression models.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2062702722",
                        "name": "David Mac\u00eado"
                    },
                    {
                        "authorId": "7757139",
                        "name": "Ing Ren Tsang"
                    },
                    {
                        "authorId": "2948325",
                        "name": "C. Zanchettin"
                    },
                    {
                        "authorId": "40057895",
                        "name": "Adriano Oliveira"
                    },
                    {
                        "authorId": "1909393",
                        "name": "A. Tapp"
                    },
                    {
                        "authorId": "1746612",
                        "name": "Teresa B Ludermir"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, we speculate this could also be achieved in a better way using isotropic regularization or special data augmentation techniques (Thulasidasan et al., 2019; Yun et al., 2019) to avoid the need for out-of-distribution or adversarial samples."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a3f12ce796340936242500c4392d8278d58a544d",
                "externalIds": {
                    "MAG": "3005153142",
                    "CorpusId": 211032260
                },
                "corpusId": 211032260,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a3f12ce796340936242500c4392d8278d58a544d",
                "title": "Isotropic Maximization Loss and Entropic Score: Fast, Accurate, Scalable, Unexposed, Turnkey, and Native Neural Networks Out-of-Distribution Detection.",
                "abstract": "Current out-of-distribution detection (ODD) approaches require cumbersome procedures that add undesired side-effects to the solution. In this paper, we argue that the uncertainty in neural networks is mainly due to SoftMax loss anisotropy. Consequently, we propose an isotropic loss (IsoMax) and a decision score (Entropic Score) to significantly improve the ODD performance while keeping the overall solution fast, accurate, scalable, unexposed, turnkey, and native. Our experiments indeed showed that uncertainty is extremely reduced simply by replacing the SoftMax loss without relying on techniques such as adversarial training/validation, special-purpose data augmentation, outlier exposure, ensembles methods, Bayesian mechanisms, generative approaches, metric learning, or additional classifiers/regressions. The results also showed that our straightforward proposal overcomes ODIN, ACET, and is competitive against the Mahalanobis approach besides avoiding their undesired requirements and weaknesses. Since IsoMax loss works as a direct and transparent SoftMax loss drop-in replacement, these techniques may be used combined with our loss to increase the overall performance even more if their associated drawbacks are not a concern in a particular use case.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "51473026",
                        "name": "David Mac\u00eado"
                    },
                    {
                        "authorId": "1769575",
                        "name": "T. I. Ren"
                    },
                    {
                        "authorId": "2948325",
                        "name": "C. Zanchettin"
                    },
                    {
                        "authorId": "40057895",
                        "name": "Adriano Oliveira"
                    },
                    {
                        "authorId": "1909393",
                        "name": "A. Tapp"
                    },
                    {
                        "authorId": "1746612",
                        "name": "Teresa B Ludermir"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We speculate that recent advances in data augmentation techniques may help to improve IsoMax+ES OOD detection performance even further [51], [52]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "77d77317649151ae812044768f99c96986b6cd5a",
                "externalIds": {
                    "MAG": "3033922452",
                    "CorpusId": 220714225
                },
                "corpusId": 220714225,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/77d77317649151ae812044768f99c96986b6cd5a",
                "title": "Isotropy Maximization Loss and Entropic Score: Accurate, Fast, Efficient, Scalable, and Turnkey Neural Networks Out-of-Distribution Detection Based on The Principle of Maximum Entropy.",
                "abstract": "Current out-of-distribution (OOD) detection approaches require cumbersome procedures that add undesired side effects to the solution. In this paper, we argue that the low OOD detection performance of neural networks is due to cross-entropy SoftMax loss anisotropy and extreme propensity to produce low entropy (high confidence) posterior probability distributions in frontal disagreement with the Principle of Maximum Entropy. Consequently, we propose IsoMax, a loss that is isotropic (distance-based) and produces high entropy (low confidence) posterior probability distributions despite still relying on cross-entropy minimization. Additionally, we propose a speedy Entropic Score for OOD detection. IsoMax loss works as a seamless SoftMax loss drop-in replacement that keeps the overall solution accurate, fast, efficient, scalable, and turnkey. Our experiments indeed confirmed that neural networks OOD detection performance may be extremely improved without relying on techniques such as adversarial training or validation, data augmentation, ensembles methods, generative approaches, model architectural changes, metric learning, or additional classifiers or regressions. The results also showed that our straightforward approach is competitive against state-of-the-art solutions besides avoiding previous methods undesired drawbacks.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "51473026",
                        "name": "David Mac\u00eado"
                    },
                    {
                        "authorId": "1769575",
                        "name": "T. I. Ren"
                    },
                    {
                        "authorId": "2948325",
                        "name": "C. Zanchettin"
                    },
                    {
                        "authorId": "40057895",
                        "name": "Adriano Oliveira"
                    },
                    {
                        "authorId": "1746612",
                        "name": "Teresa B Ludermir"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(7) As shown in [26], overconfidence in deep neural networks is a consequence of training on hard labels and it is the label smoothing effect from randomly combining yp and yq during mixup training that reduces prediction confidence and improves",
                "To deal with this issue, we propose to use mixup augmentation [25] as an effective regularization that helps calibrate deep neural networks [26] and, therefore, alleviates confirmation bias."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "35e8312d8bdcffb8e0c956d20d5a581cad1c1b8a",
                "externalIds": {
                    "DBLP": "conf/ijcnn/ArazoOAOM20",
                    "MAG": "2964677000",
                    "ArXiv": "1908.02983",
                    "DOI": "10.1109/IJCNN48605.2020.9207304",
                    "CorpusId": 199501839
                },
                "corpusId": 199501839,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/35e8312d8bdcffb8e0c956d20d5a581cad1c1b8a",
                "title": "Pseudo-Labeling and Confirmation Bias in Deep Semi-Supervised Learning",
                "abstract": "Semi-supervised learning, i.e. jointly learning from labeled and unlabeled samples, is an active research topic due to its key role on relaxing human supervision. In the context of image classification, recent advances to learn from unlabeled samples are mainly focused on consistency regularization methods that encourage invariant predictions for different perturbations of unlabeled samples. We, conversely, propose to learn from unlabeled data by generating soft pseudo-labels using the network predictions. We show that a naive pseudo-labeling overfits to incorrect pseudo-labels due to the so-called confirmation bias and demonstrate that mixup augmentation and setting a minimum number of labeled samples per mini-batch are effective regularization techniques for reducing it. The proposed approach achieves state-of-the-art results in CIFAR-10/100, SVHN, and Mini-ImageNet despite being much simpler than other methods. These results demonstrate that pseudo-labeling alone can outperform consistency regularization methods, while the opposite was supposed in previous work. Source code is available at https://git.io/fjQsC.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "107621684",
                        "name": "Eric Arazo"
                    },
                    {
                        "authorId": "2531432",
                        "name": "Diego Ortego"
                    },
                    {
                        "authorId": "83107779",
                        "name": "Paul Albert"
                    },
                    {
                        "authorId": "98536322",
                        "name": "N. O\u2019Connor"
                    },
                    {
                        "authorId": "145470864",
                        "name": "Kevin McGuinness"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3b656bba99dbcf6c6d8fd764d53ea64cd38c7050",
                "externalIds": {
                    "MAG": "2933254221",
                    "ArXiv": "1904.01685",
                    "DBLP": "conf/cvpr/NixonDZJT19",
                    "CorpusId": 102486060
                },
                "corpusId": 102486060,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3b656bba99dbcf6c6d8fd764d53ea64cd38c7050",
                "title": "Measuring Calibration in Deep Learning",
                "abstract": "Overconfidence and underconfidence in machine learning classifiers is measured by calibration: the degree to which the probabilities predicted for each class match the accuracy of the classifier on that prediction. \nHow one measures calibration remains a challenge: expected calibration error, the most popular metric, has numerous flaws which we outline, and there is no clear empirical understanding of how its choices affect conclusions in practice, and what recommendations there are to counteract its flaws. \nIn this paper, we perform a comprehensive empirical study of choices in calibration measures including measuring all probabilities rather than just the maximum prediction, thresholding probability values, class conditionality, number of bins, bins that are adaptive to the datapoint density, and the norm used to compare accuracies to confidences. To analyze the sensitivity of calibration measures, we study the impact of optimizing directly for each variant with recalibration techniques. Across MNIST, Fashion MNIST, CIFAR-10/100, and ImageNet, we find that conclusions on the rank ordering of recalibration methods is drastically impacted by the choice of calibration measure. We find that conditioning on the class leads to more effective calibration evaluations, and that using the L2 norm rather than the L1 norm improves both optimization for calibration metrics and the rank correlation measuring metric consistency. Adaptive binning schemes lead to more stablity of metric rank ordering when the number of bins vary, and is also recommended. We open source a library for the use of our calibration measures.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "10115554",
                        "name": "Jeremy Nixon"
                    },
                    {
                        "authorId": "144477225",
                        "name": "Michael W. Dusenberry"
                    },
                    {
                        "authorId": "2144125812",
                        "name": "Linchuan Zhang"
                    },
                    {
                        "authorId": "3451901",
                        "name": "Ghassen Jerfel"
                    },
                    {
                        "authorId": "47497262",
                        "name": "Dustin Tran"
                    }
                ]
            }
        },
        {
            "contexts": [
                "undesirable behaviors [21], [24] and has been shown to improve calibration and OoD performance [24]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dd38c824bf70c3ea4788c59386d2931d63a305c2",
                "externalIds": {
                    "DBLP": "journals/access/RavikumarKGR23",
                    "DOI": "10.1109/ACCESS.2023.3254920",
                    "CorpusId": 257470083
                },
                "corpusId": 257470083,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/dd38c824bf70c3ea4788c59386d2931d63a305c2",
                "title": "Intra-Class Mixup for Out-of-Distribution Detection",
                "abstract": "Deep neural networks (DNNs) have found widespread adoption in solving image recognition and natural language processing tasks. However, they make confident mispredictions when presented with data that does not belong to the training distribution, i.e. out-of-distribution (OoD) samples. Research has shown that angular representations can be useful to address the curse of dimensionality and improve OoD detection performance. However, when evaluating the angular separability using Fisher\u2019s criterion we find that empirical risk minimization and inter-class mixup trained DNNs have low angular separability between in-distribution data and OoD data. To improve angular separability, we propose intra-class mixup. We provide mathematical reasoning that shows that intra-class mixup results in reduced angular spread because of reduced variance at the input during training. Further, to take full advantage of improved angular separability from intra-class mixup we propose supplementing the separation metric with the cosine of angular margin to improve OoD detection. Angular margin is the angle between the final layer weight vector and the sample representation. The proposed intra-class mixup when applied to various existing OoD detection techniques shows an improvement of 4.21% and 6.21% in AUROC performance over empirical risk minimization and inter-class mixup, respectively. Further, intra-class mixup aided with the cosine of angular margin improves AUROC performance by 6.71% and 8.75% over empirical risk minimization and inter-class mixup, respectively.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2064867801",
                        "name": "Deepak Ravikumar"
                    },
                    {
                        "authorId": "20708889",
                        "name": "Sangamesh Kodge"
                    },
                    {
                        "authorId": "150134055",
                        "name": "Isha Garg"
                    },
                    {
                        "authorId": "2061563319",
                        "name": "K. Roy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026M\u00fcller et al., 2019), Entropy Regularized Loss (ERL; Pereyra et al., 2017), Virtual Adversarial Training (VAT; Miyato et al., 2018), and (5) Data-augmentation: Mixup (Zhang et al., 2018), Manifold-Mixup (M-Mixup; Verma et al., 2019), and Manifold-regularization (Mregularization; Kong et al., 2020)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "969f54b6e4ad943e4976e9b661fa7272a59633ae",
                "externalIds": {
                    "ACL": "2023.eacl-main.21",
                    "DBLP": "conf/eacl/XuZ23",
                    "DOI": "10.18653/v1/2023.eacl-main.21",
                    "CorpusId": 258378137
                },
                "corpusId": 258378137,
                "publicationVenue": {
                    "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
                    "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Eur Chapter Assoc Comput Linguistics",
                        "EACL"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/eacl/"
                },
                "url": "https://www.semanticscholar.org/paper/969f54b6e4ad943e4976e9b661fa7272a59633ae",
                "title": "Triple-Hybrid Energy-based Model Makes Better Calibrated Natural Language Understanding Models",
                "abstract": "Though pre-trained language models achieve notable success in many applications, it\u2019s usually controversial for over-confident predictions. Specifically, the in-distribution (ID) miscalibration and out-of-distribution (OOD) detection are main concerns. Recently, some works based on energy-based models~(EBM) have shown great improvements on both ID calibration and OOD detection for images. However, it\u2019s rarely explored in natural language understanding tasks due to the non-differentiability of text data which makes it more difficult for EBM training. In this paper, we first propose a triple-hybrid EBM which combines the benefits of classifier, conditional generative model and marginal generative model altogether. Furthermore, we leverage contrastive learning to approximately train the proposed model, which circumvents the non-differentiability issue of text data. Extensive experiments have been done on GLUE and six other multiclass datasets in various domains. Our model outperforms previous methods in terms of ID calibration and OOD detection by a large margin while maintaining competitive accuracy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2141375220",
                        "name": "Haotian Xu"
                    },
                    {
                        "authorId": null,
                        "name": "Yingying Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8a7d7bded4478916180ae018c99d1f06d57228ef",
                "externalIds": {
                    "DBLP": "journals/access/RobbaniBAA23",
                    "DOI": "10.1109/ACCESS.2023.3279124",
                    "CorpusId": 258881260
                },
                "corpusId": 258881260,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8a7d7bded4478916180ae018c99d1f06d57228ef",
                "title": "On Performance and Calibration of Natural Gradient Langevin Dynamics",
                "abstract": "Producing deep neural network (DNN) models with calibrated confidence is essential for applications in many fields, such as medical image analysis, natural language processing, and robotics. Modern neural networks have been reported to be poorly calibrated compared with those from a decade ago. The stochastic gradient Langevin dynamics (SGLD) algorithm offers a tractable approximate Bayesian inference applicable to DNN, providing a principled method for learning the uncertainty. A recent benchmark study showed that SGLD could produce a more robust model to covariate shifts than other competing methods. However, vanilla SGLD is also known to be slow, and preconditioning can improve SGLD efficacy. This paper proposes eigenvalue-corrected Kronecker factorization (EKFAC) preconditioned SGLD (EKSGLD), in which a novel second-order gradient approximation is employed as a preconditioner for the SGLD algorithm. This approach is expected to bring together the advantages of both second-order optimization and the approximate Bayesian method. Experiments were conducted to compare the performance of EKSGLD with existing preconditioning methods and showed that it could achieve higher predictive accuracy and better calibration on the validation set. EKSGLD improved the best accuracy by 3.06% on CIFAR-10 and 4.15% on MNIST, improved the best negative log-likelihood by 16.2% on CIFAR-10 and 11.4% on MNIST, and improved the best thresholded adaptive calibration error by 4.05% on CIFAR-10.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2211002287",
                        "name": "Hanif Amal Robbani"
                    },
                    {
                        "authorId": "1760853",
                        "name": "A. Bustamam"
                    },
                    {
                        "authorId": "41033213",
                        "name": "Risman Adnan"
                    },
                    {
                        "authorId": "2109858698",
                        "name": "Shandar Ahmad"
                    }
                ]
            }
        },
        {
            "contexts": [
                "They also combine this approach with mix-up (Thulasidasan et al., 2019) and a distinctiveness score based on the MD."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f841a80e5de0bc5e0b4ffb4eff91d443672e33a9",
                "externalIds": {
                    "ACL": "2023.acl-long.652",
                    "DBLP": "conf/acl/VazhentsevKTPPB23",
                    "DOI": "10.18653/v1/2023.acl-long.652",
                    "CorpusId": 259370752
                },
                "corpusId": 259370752,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/f841a80e5de0bc5e0b4ffb4eff91d443672e33a9",
                "title": "Hybrid Uncertainty Quantification for Selective Text Classification in Ambiguous Tasks",
                "abstract": "Many text classification tasks are inherently ambiguous, which results in automatic systems having a high risk of making mistakes, in spite of using advanced machine learning models. For example, toxicity detection in user-generated content is a subjective task, and notions of toxicity can be annotated according to a variety of definitions that can be in conflict with one another. Instead of relying solely on automatic solutions, moderation of the most difficult and ambiguous cases can be delegated to human workers. Potential mistakes in automated classification can be identified by using uncertainty estimation (UE) techniques. Although UE is a rapidly growing field within natural language processing, we find that state-of-the-art UE methods estimate only epistemic uncertainty and show poor performance, or under-perform trivial methods for ambiguous tasks such as toxicity detection. We argue that in order to create robust uncertainty estimation methods for ambiguous tasks it is necessary to account also for aleatoric uncertainty. In this paper, we propose a new uncertainty estimation method that combines epistemic and aleatoric UE methods. We show that by using our hybrid method, we can outperform state-of-the-art UE methods for toxicity detection and other ambiguous text classification tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2165225340",
                        "name": "Artem Vazhentsev"
                    },
                    {
                        "authorId": "46902583",
                        "name": "Gleb Kuzmin"
                    },
                    {
                        "authorId": "2164381839",
                        "name": "Akim Tsvigun"
                    },
                    {
                        "authorId": "2027664756",
                        "name": "A. Panchenko"
                    },
                    {
                        "authorId": "144180694",
                        "name": "Maxim Panov"
                    },
                    {
                        "authorId": "3359236",
                        "name": "M. Burtsev"
                    },
                    {
                        "authorId": "1967424",
                        "name": "Artem Shelmanov"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7a8280a7cc11463dd7f98683018f328b64041a99",
                "externalIds": {
                    "CorpusId": 263164334
                },
                "corpusId": 263164334,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7a8280a7cc11463dd7f98683018f328b64041a99",
                "title": "Mitigating Confirmation Bias in Semi-supervised Learning via Efficient Bayesian Model Averaging",
                "abstract": "State-of-the-art (SOTA) semi-supervised learning (SSL) methods have been highly successful in leveraging a mix of labeled and unlabeled data, often via self-training or pseudo-labeling. During pseudo-labeling, the model\u2019s predictions on unlabeled data are used for training and may result in confirmation bias where the model reinforces its own mistakes. In this work, we show that SOTA SSL methods often suffer from confirmation bias and demonstrate that this is often a result of using a poorly calibrated classifier for pseudo labeling. We introduce BaM-SSL, an efficient Bayesian Model averaging technique that improves uncertainty quantification in SSL methods with limited computational or memory overhead. We demonstrate that BaM-SSL mitigates confirmation bias in SOTA SSL methods across standard vision benchmarks of CIFAR-10, CIFAR-100 and ImageNet, giving up to 16% improvement in test accuracy on the CIFAR-100 with 400 labels benchmark. Furthermore, we also demonstrate their effectiveness in additional realistic and challenging problems, such as class-imbalanced datasets and in photonics science.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40917646",
                        "name": "Charlotte Loh"
                    },
                    {
                        "authorId": "26916003",
                        "name": "R. Dangovski"
                    },
                    {
                        "authorId": "2114819100",
                        "name": "Shivchander Sudalairaj"
                    },
                    {
                        "authorId": "2197109",
                        "name": "Seung-Jun Han"
                    },
                    {
                        "authorId": "2249472971",
                        "name": "Ligong Han"
                    },
                    {
                        "authorId": "2248417277",
                        "name": "Leonid Karlinsky"
                    },
                    {
                        "authorId": "2149992752",
                        "name": "Marin Soljacic"
                    },
                    {
                        "authorId": "2243025154",
                        "name": "Akash Srivastava"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b01f35c51426aadd092156c91a01d154cb4a1404",
                "externalIds": {
                    "CorpusId": 263629082
                },
                "corpusId": 263629082,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b01f35c51426aadd092156c91a01d154cb4a1404",
                "title": "Human-in-the-loop mixup",
                "abstract": "Synthetic data is proliferating and powering many advances in machine learning. However, it is not always clear if synthetic labels are perceptually sensible to humans. The web provides us with a platform to take a step towards addressing this question from a human-centric perspective, through online elicitation. We design a series of elicitation interfaces, which we release as HILL MixE Suite , and recruit 159 participants, to provide perceptual judgments over the kinds of synthetic data constructed during mixup training: a powerful regularizer shown to improve model robustness, generalization, and calibration. We find that human perception does not consistently align with the labels traditionally used for synthetic points and begin to demonstrate the applicability of these findings to potentially increase the reliability of down-stream models. We release all elicited judgments in a new data hub we call H-Mix .",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2055306799",
                        "name": "Katherine M. Collins"
                    },
                    {
                        "authorId": "32326200",
                        "name": "Umang Bhatt"
                    },
                    {
                        "authorId": "2243412679",
                        "name": "Weiyang Liu"
                    },
                    {
                        "authorId": "2748067",
                        "name": "Vihari Piratla"
                    },
                    {
                        "authorId": "2253780346",
                        "name": "Bradley Love"
                    },
                    {
                        "authorId": "2253576893",
                        "name": "Adrian Weller"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026is a strong yet computationally intensive baseline (Ashukha et al., 2020), and results of another recently proposed computationally intensive method called MSD (He et al., 2020) that leverage \u201cmix-up\u201d (Thulasidasan et al., 2019), \u201cself-ensembling\u201d, MD,\nand the MC dropout (all layers are activated).",
                "They also use mix-up (Thulasidasan et al., 2019) to generate additional training instance representations that help to capture aleatoric uncertainty, self-ensembling, MC dropout, and a distinctiveness score to measure the",
                "They also use mix-up (Thulasidasan et al., 2019) to generate additional training instance representations that help to capture aleatoric uncertainty, self-ensembling, MC dropout, and a distinctiveness score to measure the epistemic uncertainty.",
                ", 2020) that leverage \u201cmix-up\u201d (Thulasidasan et al., 2019), \u201cself-ensembling\u201d, MD,"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "8ae920111435a7db8da360c654c771c53f57c69a",
                "externalIds": {
                    "ACL": "2022.acl-long.566",
                    "DBLP": "conf/acl/VazhentsevKSTTF22",
                    "DOI": "10.18653/v1/2022.acl-long.566",
                    "CorpusId": 248780161
                },
                "corpusId": 248780161,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/8ae920111435a7db8da360c654c771c53f57c69a",
                "title": "Uncertainty Estimation of Transformer Predictions for Misclassification Detection",
                "abstract": "Uncertainty estimation (UE) of model predictions is a crucial step for a variety of tasks such as active learning, misclassification detection, adversarial attack detection, out-of-distribution detection, etc. Most of the works on modeling the uncertainty of deep neural networks evaluate these methods on image classification tasks. Little attention has been paid to UE in natural language processing. To fill this gap, we perform a vast empirical investigation of state-of-the-art UE methods for Transformer models on misclassification detection in named entity recognition and text classification tasks and propose two computationally efficient modifications, one of which approaches or even outperforms computationally intensive methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2165225340",
                        "name": "Artem Vazhentsev"
                    },
                    {
                        "authorId": "46902583",
                        "name": "Gleb Kuzmin"
                    },
                    {
                        "authorId": "1967424",
                        "name": "Artem Shelmanov"
                    },
                    {
                        "authorId": "2164381839",
                        "name": "Akim Tsvigun"
                    },
                    {
                        "authorId": "3465002",
                        "name": "Evgenii Tsymbalov"
                    },
                    {
                        "authorId": "1490644954",
                        "name": "Kirill Fedyanin"
                    },
                    {
                        "authorId": "144180694",
                        "name": "Maxim Panov"
                    },
                    {
                        "authorId": "2027664756",
                        "name": "A. Panchenko"
                    },
                    {
                        "authorId": "145004420",
                        "name": "Gleb Gusev"
                    },
                    {
                        "authorId": "3359236",
                        "name": "M. Burtsev"
                    },
                    {
                        "authorId": "51132893",
                        "name": "Manvel Avetisian"
                    },
                    {
                        "authorId": "2065646046",
                        "name": "L. Zhukov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mixup (Zhang et al., 2018) and its extensions (Verma et al., 2019; Yun et al., 2019) have been shown very effective for improving model calibration (Thulasidasan et al., 2019) and generalization in the presence of balanced datasets.",
                "Although Mixup (Zhang et al., 2018) was introduced to alleviate memorization for over-parameterized models (Arpit et al., 2017), it was later shown to improve calibration in the balanced setting (Thulasidasan et al., 2019).",
                ", 2017), it was later shown to improve calibration in the balanced setting (Thulasidasan et al., 2019).",
                ", 2019) have been shown very effective for improving model calibration (Thulasidasan et al., 2019) and generalization in the presence of balanced datasets."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6dd6b9117803021403cfc8ead2ae5d4e4103d4c7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-05260",
                    "DOI": "10.48550/arXiv.2206.05260",
                    "CorpusId": 249605539
                },
                "corpusId": 249605539,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6dd6b9117803021403cfc8ead2ae5d4e4103d4c7",
                "title": "Balanced Product of Experts for Long-Tailed Recognition",
                "abstract": "Many real-world recognition problems suffer from an imbalanced or long-tailed label distribution. Those distributions make representation learning more chal-lenging due to limited generalization over the tail classes. If the test distribution differs from the training distribution, e.g. uniform versus long-tailed, the problem of the distribution shift needs to be addressed. To this aim, recent works have extended softmax cross-entropy using margin modi\ufb01cations, inspired by Bayes\u2019 theorem. In this paper, we generalize several approaches with a Balanced Product of Experts (BalPoE), which combines a family of models with different test-time target distributions to tackle the imbalance in the data. The proposed experts are trained in a single stage, either jointly or independently, and fused seamlessly into a BalPoE. We show that BalPoE is Fisher consistent for minimizing the balanced error and perform extensive experiments to validate the effectiveness of our approach. Finally, we investigate the effect of Mixup in this setting, discovering that regularization is a key ingredient for learning calibrated experts. Our experiments show that a regularized BalPoE can perform remarkably well in test accuracy and calibration metrics, leading to state-of-the-art results on CIFAR-100-LT, ImageNet-LT, and iNaturalist-2018 datasets. The code will be made publicly available upon paper acceptance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2064245444",
                        "name": "Emanuel Sanchez Aimar"
                    },
                    {
                        "authorId": "71308961",
                        "name": "Arvi Jonnarth"
                    },
                    {
                        "authorId": "2228323",
                        "name": "M. Felsberg"
                    },
                    {
                        "authorId": "40462390",
                        "name": "Marco Kuhlmann"
                    }
                ]
            }
        },
        {
            "contexts": [
                "diction emanating from the desirable regularization effects it induces (Carratino et al., 2020; Zhang et al., 2018; Thulasidasan et al., 2019).",
                "a broad range of tasks ranging from computer vision (Zhang et al., 2018; Thulasidasan et al., 2019; Carratino et al., 2020; Wang et al., 2020a) to natural language processing (Guo et al.",
                "It has been empirically shown that mixup can hone the accuracy and calibration of the pre-\n9266\ndiction emanating from the desirable regularization effects it induces (Carratino et al., 2020; Zhang et al., 2018; Thulasidasan et al., 2019).",
                "While mixup is making significant inroads in a broad range of tasks ranging from computer vision (Zhang et al., 2018; Thulasidasan et al., 2019; Carratino et al., 2020; Wang et al., 2020a) to natural language processing (Guo et al., 2019; Guo, 2020; Chen et al., 2020; Yin et al., 2021; Kong et al.,\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a31e2fdd8c8e2e6d053d512625bbdf32eb9ee5d2",
                "externalIds": {
                    "ACL": "2022.emnlp-main.629",
                    "DBLP": "conf/emnlp/HosseiniC22",
                    "DOI": "10.18653/v1/2022.emnlp-main.629",
                    "CorpusId": 256460933
                },
                "corpusId": 256460933,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/a31e2fdd8c8e2e6d053d512625bbdf32eb9ee5d2",
                "title": "Calibrating Student Models for Emotion-related Tasks",
                "abstract": "Knowledge Distillation (KD) is an effective method to transfer knowledge from one network (a.k.a. teacher) to another (a.k.a. student). In this paper, we study KD on the emotion-related tasks from a new perspective: calibration. We further explore the impact of the mixup data augmentation technique on the distillation objective and propose to use a simple yet effective mixup method informed by training dynamics for calibrating the student models. Underpinned by the regularization impact of the mixup process by providing better training signals to the student models using training dynamics, our proposed mixup strategy gradually enhances the student model\u2019s calibration while effectively improving its performance. We evaluate the calibration of pre-trained language models through knowledge distillation over three tasks of emotion detection, sentiment analysis, and empathy detection. By conducting extensive experiments on different datasets, with both in-domain and out-of-domain test sets, we demonstrate that student models distilled from teacher models trained using our proposed mixup method obtained the lowest Expected Calibration Errors (ECEs) and best performance on both in-domain and out-of-domain test sets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39217993",
                        "name": "Mahshid Hosseini"
                    },
                    {
                        "authorId": "2140493460",
                        "name": "Cornelia Caragea"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To improve the diversity and variation of the perturbed samples (thus increasing the learnable information from a limited-size buffer), we investigate the role of MixUp [58, 67, 68], a data augmentation technique applied together with RAR \u2014 we find that it brings substantial improvements when there are strict buffer size constraints.",
                "Moreover, we conduct an ablation study showing that the key components such as replay samples selection strategy, sample pairing & adversarial perturbation, MixUp, etc, each bring appreciable improvements.",
                "The proposed MixUp strategy is different from [21, 40] as we apply mixup among the replay samples and then generate RAR perturbed samples anchored around them.",
                "Moreover, we study the role of MixUp in increasing the variation of replay augmentations, which significantly improves CL in the small buffer regime.",
                "The proposed use of MixUp among the replay samples before RAR is able to increase the variation of RAR perturbed samples, which is essential to CL with a small buffer."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "675e3644c687e7cd1cc49c2898495fab5fbd4726",
                "externalIds": {
                    "DBLP": "conf/nips/KumariW0B22",
                    "CorpusId": 258509557
                },
                "corpusId": 258509557,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/675e3644c687e7cd1cc49c2898495fab5fbd4726",
                "title": "Retrospective Adversarial Replay for Continual Learning",
                "abstract": "Continual learning is an emerging research challenge in machine learning that addresses the problem where models quickly fit the most recently trained-on data but suffer from catastrophic forgetting of previous data due to distribution shifts \u2014 it does this by maintaining a small historical replay buffer in replay-based methods. To avoid these problems, this paper proposes a method, \u201cRetrospective Adversarial Replay (RAR)\u201d, that synthesizes adversarial samples near the forgetting boundary. RAR perturbs a buffered sample towards its nearest neighbor drawn from the current task in a latent representation space. By replaying such samples, we are able to refine the boundary between previous and current tasks, hence combating forgetting and reducing bias towards the current task. To mitigate the severity of a small replay buffer, we develop a novel MixUp-based strategy to increase replay variation by replaying mixed augmentations. Combined with RAR, this achieves a holistic framework that helps to alleviate catastrophic forgetting. We show that this excels on broadly-used benchmarks and outperforms other continual learning baselines especially when only a small buffer is available. We conduct a thorough ablation study over each key component as well as a hyperparameter sensitivity analysis to demonstrate the effectiveness and robustness of RAR.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49561117",
                        "name": "Lilly Kumari"
                    },
                    {
                        "authorId": "4519720",
                        "name": "Shengjie Wang"
                    },
                    {
                        "authorId": "2144115714",
                        "name": "Tianyi Zhou"
                    },
                    {
                        "authorId": "1748118",
                        "name": "J. Bilmes"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Empirically, Mixup is helpful for robust representation learning, and it alleviates the overconfident problems, and the failure of distribution shift settings as well as the in-distribution accuracy [34]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6ad591be67b2a7ed8ac9d29df71c2b60fd4c9128",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-03897",
                    "DOI": "10.48550/arXiv.2203.03897",
                    "CorpusId": 247315609
                },
                "corpusId": 247315609,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6ad591be67b2a7ed8ac9d29df71c2b60fd4c9128",
                "title": "Multi-Modal Mixup for Robust Fine-tuning",
                "abstract": "Pre-trained large-scale models provide a transferable embedding, and they show comparable performance on the diverse downstream task. However, the transferability of multi-modal learning is restricted, and the analysis of learned embedding has not been explored well. This paper provides a perspective to understand the multi-modal embedding in terms of uniformity and alignment. We newly find that the representation learned by multi-modal learning models such as CLIP has a two separated representation space for each heterogeneous dataset with less alignment. Besides, there are unexplored large intermediate areas between two modalities with less uniformity. Less robust embedding might restrict the transferability of the representation for the downstream task. This paper provides a new end-to-end fine-tuning method for robust representation that encourages better uniformity and alignment score. First, we propose a multi-modal Mixup, m-Mix that mixes the representation of image and text to generate the hard negative samples. Second, we finetune the multi-modal model on a hard negative sample as well as normal negative and positive samples with contrastive learning. Our multi-modal Mixup provides a robust representation, and we validate our methods on classification, retrieval, and structure-awareness task.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2127769429",
                        "name": "Junhyuk So"
                    },
                    {
                        "authorId": "2152046120",
                        "name": "Changdae Oh"
                    },
                    {
                        "authorId": "2067943570",
                        "name": "Minchul Shin"
                    },
                    {
                        "authorId": "2490092",
                        "name": "Kyungwoo Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other works along the track also show improvements on various discriminative learning tasks [20,38,40,34]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f7310c427678e12520afeaf284a5d68afb494a93",
                "externalIds": {
                    "CorpusId": 247446892
                },
                "corpusId": 247446892,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f7310c427678e12520afeaf284a5d68afb494a93",
                "title": "Implicit Data Augmentation Using Feature Interpolation for Low-Shot Image Generation",
                "abstract": "Training of generative models especially Generative Adversarial Networks can easily diverge in low-data setting. To mitigate this issue, we propose a novel implicit data augmentation approach which facilitates stable training and synthesize high-quality samples without need of label information. Specifically, we view the discriminator as a metric embedding of the real data manifold, which offers proper distances between real data points. We then utilize information in the feature space to develop a data-driven augmentation method. Experiments on few-shot generation tasks show the proposed method significantly improve results from strong baselines, and allows generating high-quality images with around 100 training samples.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2143823659",
                        "name": "M. Dai"
                    },
                    {
                        "authorId": "103011435",
                        "name": "Haibin Hang"
                    },
                    {
                        "authorId": "49932298",
                        "name": "Xiaoyang Guo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Table 3 lists the over-confidence error (OE); the equation for OE is presented in Thulasidasan et al. (2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8f929a063c9d8b6cea677b4417cb65de36f474c8",
                "externalIds": {
                    "ACL": "2022.acl-long.52",
                    "DBLP": "conf/acl/ChoiCL22",
                    "DOI": "10.18653/v1/2022.acl-long.52",
                    "CorpusId": 248780286
                },
                "corpusId": 248780286,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/8f929a063c9d8b6cea677b4417cb65de36f474c8",
                "title": "Early Stopping Based on Unlabeled Samples in Text Classification",
                "abstract": "Early stopping, which is widely used to prevent overfitting, is generally based on a separate validation set. However, in low resource settings, validation-based stopping can be risky because a small validation set may not be sufficiently representative, and the reduction in the number of samples by validation split may result in insufficient samples for training. In this study, we propose an early stopping method that uses unlabeled samples. The proposed method is based on confidence and class distribution similarities. To further improve the performance, we present a calibration method to better estimate the class distribution of the unlabeled samples. The proposed method is advantageous because it does not require a separate validation set and provides a better stopping point by using a large unlabeled set. Extensive experiments are conducted on five text classification datasets and several stop-methods are compared. Our results show that the proposed model even performs better than using an additional validation set as well as the existing stop-methods, in both balanced and imbalanced data settings. Our code is available at https://github.com/DMCB-GIST/BUS-stop.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111527643",
                        "name": "Hongseok Choi"
                    },
                    {
                        "authorId": "2007564294",
                        "name": "Dongha Choi"
                    },
                    {
                        "authorId": "49923640",
                        "name": "Hyunju Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Thulasidasan et al. (2019) find that for image classification, using mixup training improves calibration evaluated by the ECE metric."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3b9abc7c30cdf2d29514587c3b1b596d584e3d9b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-12507",
                    "DOI": "10.48550/arXiv.2205.12507",
                    "CorpusId": 249062754
                },
                "corpusId": 249062754,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3b9abc7c30cdf2d29514587c3b1b596d584e3d9b",
                "title": "Revisiting Calibration for Question Answering",
                "abstract": "Model calibration aims to adjust (calibrate) models\u2019 confidence so that they match expected accuracy. We argue that the tradi-tional evaluation of calibration (expected calibration error; ECE) does not reflect useful-ness of the model confidence. For example, after conventional temperature scaling, confidence scores become similar for all predictions, which makes it hard for users to dis-tinguish correct predictions from wrong ones, even though it achieves low ECE. Building on those observations, we propose a new calibration metric, MacroCE, that better captures whether the model assigns low confidence to wrong predictions and high confidence to correct predictions. We examine various conventional calibration methods including temperature scaling, feature-based classifier, neural answer reranking, and label smoothing, all of which do not bring significant gains under our new MacroCE metric. Towards more effective calibration, we propose a new calibration method based on the model\u2019s prediction consistency along the training trajectory. This new method, which we name as consistency calibration, shows promise for better calibration.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152358188",
                        "name": "Chenglei Si"
                    },
                    {
                        "authorId": "145756130",
                        "name": "Chen Zhao"
                    },
                    {
                        "authorId": "48872685",
                        "name": "Sewon Min"
                    },
                    {
                        "authorId": "1389036863",
                        "name": "Jordan L. Boyd-Graber"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Fortunately, training good classifiers is easy nowadays with AutoML [Erickson et al., 2020] and versatile techniques for calibration, data augmentation, and transfer learning [Thulasidasan et al., 2019]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "780cb1cf1f972c2aedab692c2e2a8c8b4e29c721",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-06812",
                    "DOI": "10.48550/arXiv.2210.06812",
                    "CorpusId": 252873101
                },
                "corpusId": 252873101,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/780cb1cf1f972c2aedab692c2e2a8c8b4e29c721",
                "title": "Utilizing supervised models to infer consensus labels and their quality from data with multiple annotators",
                "abstract": "Real-world data for classi\ufb01cation is often labeled by multiple annotators. For analyzing such data, we introduce CROWDLAB, a straightforward approach to estimate: (1) A consensus label for each example that aggregates the individual annotations (more accurately than aggregation via majority-vote or other algorithms used in crowdsourcing); (2) A con\ufb01dence score for how likely each consensus label is correct (via well-calibrated estimates that account for the number of annotations for each example and their agreement, prediction-con\ufb01dence from a trained classi\ufb01er, and trustworthiness of each annotator vs. the classi\ufb01er); (3) A rating for each annotator quanti-fying the overall correctness of their labels. While many algorithms have been proposed to estimate related quantities in crowdsourcing, these often rely on sophisticated generative models with iterative inference schemes, whereas CROWDLAB is based on simple weighted ensembling. Many algorithms also rely solely on annotator statistics, ignoring the features of the examples from which the annotations derive. CROWDLAB in contrast utilizes any classi\ufb01er model trained on these features, which can generalize between examples with similar features. In evaluations on real-world multi-annotator image data, our proposed method provides superior estimates for (1)-(3) than many alternative algorithms.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2174448740",
                        "name": "Hui Wen Goh"
                    },
                    {
                        "authorId": "2099410561",
                        "name": "Ulyana Tkachenko"
                    },
                    {
                        "authorId": "153430733",
                        "name": "Jonas W. Mueller"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, we evaluated the ability to separate TPs and FPs by evaluating the area under the receiver operator characteristic (AU-ROC) applied in [37, 5].",
                "Model averaging can yield well-calibrated confidence [4, 5] and is one of the state-of-the-art methods for detecting FPs caused by out-of-distribution examples [4, 3].",
                "Specifically, we evaluated the calibration error using measures, such as the negative log likelihood (NLL) applied in [4, 5, 31], expected calibration error (ECE) applied in [13, 8, 12], and Brier score (BS) applied in [4]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9b312469d8e4e1bd6510a019ca560da2f0bc9cb2",
                "externalIds": {
                    "DBLP": "conf/ijcai/TassiGFT22",
                    "CorpusId": 252819806
                },
                "corpusId": 252819806,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9b312469d8e4e1bd6510a019ca560da2f0bc9cb2",
                "title": "The impact of averaging logits over probabilities on ensembles of neural networks",
                "abstract": "Model averaging has become a standard for improving neural networks in terms of accuracy, calibration, and the ability to detect false predictions (FPs). However, recent findings show that model averaging does not necessarily lead to calibrated confidences, especially for underconfident networks. While existing methods for improving the calibration of combined networks focus on recalibrating, building, or sampling calibrated models, we focus on the combination process. Specifically, we evaluate the impact of averaging logits instead of probabilities on the quality of confidence (QoC). We compare combined logits instead of probabilities of members (networks) for models such as ensembles, Monte Carlo Dropout (MCD), and Mixture of Monte Carlo Dropout (MMCD). Comparison is done using experimental results on three datasets using three different architectures. We show that averaging logits instead of probabilities increase the confidence thereby improving the confidence calibration for underconfident models. For example, for MCD evaluated on CIFAR10, averaging logits instead of probabilities reduces the expected calibration error (ECE) from 12.03% to 5.44%. However, the increase in confidence can bring harm to confidence calibration for overconfident models and the separability between true predictions (TPs) and FPs. For example, for MMCD evaluated on MNIST, the average confidence on FPs due to the noisy data increases from 51.31% to 94.58% when averaging logits instead of probabilities. While averaging logits can be applied with underconfident models to improve the calibration on test data, we suggest to average probabilities for safety- and mission-critical applications where the separability of TPs and FPs is of paramount importance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1486494981",
                        "name": "Cedrique Rovile Njieutcheu Tassi"
                    },
                    {
                        "authorId": "2051292610",
                        "name": "J. Gawlikowski"
                    },
                    {
                        "authorId": "72969221",
                        "name": "A. Fitri"
                    },
                    {
                        "authorId": "1453548521",
                        "name": "Rudolph Triebel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Second, despite this simplicity, mixup is a powerful and popular training-time method that has been leveraged to address model fairness [9], improve model calibration [60, 73], and increase model robustness via regularizing the form of category boundaries learned implicitly [72]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0fe2649f04cda8df2ce93721981e50ca42bfde8a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-01202",
                    "DOI": "10.48550/arXiv.2211.01202",
                    "CorpusId": 253255407
                },
                "corpusId": 253255407,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0fe2649f04cda8df2ce93721981e50ca42bfde8a",
                "title": "Web-based Elicitation of Human Perception on mixup Data",
                "abstract": "Synthetic data is proliferating on the web and powering many advances in machine learning. However, it is not always clear if synthetic labels are perceptually sensible to humans. The web provides us with a platform to take a step towards addressing this question through online elicitation. We design a series of elicitation interfaces, which we release as HILL MixE Suite , and recruit 159 participants, to provide perceptual judgments over the kinds of synthetic data constructed during mixup training: a powerful regularizer shown to improve model robustness, generalization, and calibration. We \ufb01nd that human perception does not consistently align with the labels traditionally used for synthetic points and begin to demonstrate the applicability of these \ufb01ndings to potentially increase the reliability of downstream models. We release all elicited judgments in a new data hub we call H-Mix .",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2055306799",
                        "name": "Katherine M. Collins"
                    },
                    {
                        "authorId": "32326200",
                        "name": "Umang Bhatt"
                    },
                    {
                        "authorId": "36326884",
                        "name": "Weiyang Liu"
                    },
                    {
                        "authorId": "2748067",
                        "name": "Vihari Piratla"
                    },
                    {
                        "authorId": "10129234",
                        "name": "B. Love"
                    },
                    {
                        "authorId": "145689461",
                        "name": "Adrian Weller"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is noteworthy that the condition in above lemma is easy to satisfy since the model prone to over-confident (Thulasidasan et al., 2019), thus, f(y = k|\u03b1(xi ); \u03b8)/f(y = k|\u03b1(xi ); \u03b8) is relatively small in real tasks.",
                "It is noteworthy that the condition in above lemma is easy to satisfy since the model prone to over-confident (Thulasidasan et al., 2019), thus, f(y = k\u2032|\u03b1(xui ); \u03b8)/f(y = k|\u03b1(xui ); \u03b8) is relatively small in real tasks."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a653808f6f529b13193902f63865e7a8cb61bf0d",
                "externalIds": {
                    "DBLP": "conf/icml/GuoL22",
                    "CorpusId": 250340758
                },
                "corpusId": 250340758,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a653808f6f529b13193902f63865e7a8cb61bf0d",
                "title": "Class-Imbalanced Semi-Supervised Learning with Adaptive Thresholding",
                "abstract": "Semi-supervised learning (SSL) has proven to be successful in overcoming labeling dif\ufb01culties by leveraging unlabeled data. Previous SSL algorithms typically assume a balanced class distribution. However, real-world datasets are usually class-imbalanced, causing the performance of existing SSL algorithms to be seriously decreased. One essential reason is that pseudo-labels for unlabeled data are selected based on a \ufb01xed con\ufb01dence threshold, resulting in low performance on minority classes. In this paper, we develop a simple yet effective framework, which only involves adaptive thresholding for different classes in SSL algorithms, and achieves remarkable performance improvement on more than twenty imbalance ratios. Speci\ufb01cally, we explicitly optimize the number of pseudo-labels for each class in the SSL objective, so as to simultaneously obtain adaptive thresholds and minimize empirical risk. Moreover, the determination of the adaptive threshold can be ef\ufb01ciently obtained by a closed-form solution. Extensive experimental results demonstrate the effectiveness of our proposed algorithms.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "30414295",
                        "name": "Lan-Zhe Guo"
                    },
                    {
                        "authorId": "2110463675",
                        "name": "Yu-Feng Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "796847348e51960d697f511a56dc9a191ff1f275",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-12053",
                    "DOI": "10.48550/arXiv.2212.12053",
                    "CorpusId": 261092306
                },
                "corpusId": 261092306,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/796847348e51960d697f511a56dc9a191ff1f275",
                "title": "On Calibrating Semantic Segmentation Models: Analysis and An Algorithm",
                "abstract": "We study the problem of semantic segmentation calibration. For image classi\ufb01cation, lots of existing solutions are proposed to alleviate model miscalibration of con\ufb01dence. However, to date, con\ufb01dence calibration research on semantic segmentation is still limited. We provide a systematic study on the calibration of semantic segmentation models and propose a simple yet effective approach. First, we \ufb01nd that model capacity, crop size, multi-scale testing, and prediction correctness have impact on calibration. Among them, prediction correctness, especially misprediction, is more important to miscalibration due to over-con\ufb01dence. Next, we propose a simple, unifying, and effective approach, namely selective scaling, by separating correct/incorrect prediction for scaling and more focusing on misprediction logit smoothing. Then, we study popular existing calibration methods and compare them with selective scaling on semantic segmentation calibration. We conduct extensive experiments with a variety of benchmarks on both in-domain and domain-shift calibration, and show that selective scaling consistently outperforms other methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111219938",
                        "name": "Dongdong Wang"
                    },
                    {
                        "authorId": "40206014",
                        "name": "Boqing Gong"
                    },
                    {
                        "authorId": "49681507",
                        "name": "Liqiang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For MX, we use \u03b1 = 0.2 based on the results provided by (Thulasidasan et al. 2019; Singh and Bay 2020).",
                "\u2022 Mixup (MX): Thulasidasan et al. (2019) showed that the data augmentation approach of Mixup also helps in calibrating a model.",
                "For ERL, we use the strength to be 0.1 based on the experiments of Thulasidasan et al. (2019).",
                "Mixup (Zhang et al. 2018; Thulasidasan et al. 2019) and AugMix (Hendrycks et al.",
                "The strength of the entropy regularizer in ERL is set to 0.1 based on the experiments of Thulasidasan et al. (2019).",
                "Mixup (Zhang et al. 2018; Thulasidasan et al. 2019) and AugMix (Hendrycks et al. 2020) combine data augmentation and regularization.",
                "A surprising observation to note is the poor performance of MX. MX as shown by Thulasidasan et al. (2019) performs well on out-of-distribution detection.",
                "This is partly true in practice as for all deep neural networks the problem of calibration entails over-confident predictions(Thulasidasan et al. 2019).",
                "For LS, we use = 0.1 as utilized by M\u00fcller et al. (2019) and Thulasidasan et al. (2019).",
                "Thulasidasan et al. (2019) has demonstrated Mixup\u2019s ability to distinguish ood samples however, we believe that natural shift is a weaker notion of data shift than ood evaluation and MX fails to provide any benefit in this regard."
            ],
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "7794867c2ebc9efcc8af1b784dd3439fe890cba1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-09385",
                    "CorpusId": 235458445
                },
                "corpusId": 235458445,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7794867c2ebc9efcc8af1b784dd3439fe890cba1",
                "title": "On the Dark Side of Calibration for Modern Neural Networks",
                "abstract": "Modern neural networks are highly uncalibrated. It poses a significant challenge for safety-critical systems to utilise deep neural networks (DNNs), reliably. Many recently proposed approaches have demonstrated substantial progress in improving DNN calibration. However, they hardly touch upon refinement, which historically has been an essential aspect of calibration. Refinement indicates separability of a network\u2019s correct and incorrect predictions. This paper presents a theoretically and empirically supported exposition for reviewing a model\u2019s calibration and refinement. Firstly, we show the breakdown of expected calibration error (ECE), into predicted confidence and refinement. Connecting with this result, we highlight that regularisation based calibration only focuses on naively reducing a model\u2019s confidence. This logically has a severe downside to a model\u2019s refinement. We support our claims through rigorous empirical evaluations of many state of the art calibration approaches on standard datasets. We find that many calibration approaches with the likes of label smoothing, mixup etc. lower the utility of a DNN by degrading its refinement. Even under natural data shift, this calibrationrefinement trade-off holds for the majority of calibration methods. These findings call for an urgent retrospective into some popular pathways taken for modern DNN calibration.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109434876",
                        "name": "Aditya Singh"
                    },
                    {
                        "authorId": "39720629",
                        "name": "Alessandro Bay"
                    },
                    {
                        "authorId": "39599054",
                        "name": "B. Sengupta"
                    },
                    {
                        "authorId": "2063974309",
                        "name": "Andrea Mirabile"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\ud574\ub2f9 \ubc29\ubc95\uc740 \uac04\ub2e8\ud558\uc9c0\ub9cc, \ud6a8\uacfc\uc801\uc778 \ub370\uc774\ud130 \uc99d\uac15 \uae30\ubc95\uc73c\ub85c \uc54c\ub824\uc838 \uc788\uc73c\uba70 Mixup \ubc29\ubc95\uc73c\ub85c \uc0dd\uc131\ub41c \ub370\uc774\ud130\ub97c \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ud558\uba74 \uc2ec\uce35\uc2e0\uacbd\ub9dd\uc758 \uc815\ud655\ub3c4\uc640 \uc2e0\ub8b0 \uc810\uc218\ub97c \uc77c\uce58 \ud558\uac8c \ud558\ub294 \uad50\uc815 (Calibration)\uc5d0 \ud6a8\uacfc\uac00 \uc788\uc74c\uc774 \uc785\uc99d\ub418\uc5c8\ub2e4 [14]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b0ebd688eaf1030fa72719f0576a4aa03a1d1d8c",
                "externalIds": {
                    "CorpusId": 233454333
                },
                "corpusId": 233454333,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b0ebd688eaf1030fa72719f0576a4aa03a1d1d8c",
                "title": "Adversarial-Mixup : Increasing Robustness to Out-of-Distribution Data and Reliability of Inference )",
                "abstract": "Detecting Out-of-Distribution (OOD) data is fundamentally required when Deep Neural Network (DNN) is applied to real-world AI such as autonomous driving. However, modern DNNs are quite vulnerable to the over-confidence problem even if the test data are far away from the trained data distribution. To solve the problem, this paper proposes a novel Adversarial-Mixup training method to let the DNN model be more robust by detecting OOD data effectively. Experimental results show that the proposed Adversarial-Mixup method improves the overall performance of OOD detection by 78% comparing with the State-of-the-Art methods. Furthermore, we show that the proposed method can alleviate the over-confidence problem by reducing the confidence score of OOD data than the previous methods, resulting in more reliable and robust DNNs.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40382590",
                        "name": "Joonhyuk Yoo"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2019) and confidence calibration (Thulasidasan et al., 2019).",
                "Not only improving the generalization on the supervised task, it also improves adversarial robustness (Pang et al., 2019) and confidence calibration (Thulasidasan et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c709a3ac669aa334d6a0e9544f9191c5516da8a2",
                "externalIds": {
                    "DBLP": "conf/iclr/LeeZSLSL21",
                    "CorpusId": 234685335
                },
                "corpusId": 234685335,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c709a3ac669aa334d6a0e9544f9191c5516da8a2",
                "title": "i-Mix: A Domain-Agnostic Strategy for Contrastive Representation Learning",
                "abstract": "Contrastive representation learning has shown to be effective to learn representations from unlabeled data. However, much progress has been made in vision domains relying on data augmentations carefully designed using domain knowledge. In this work, we propose i-Mix, a simple yet effective domain-agnostic regularization strategy for improving contrastive representation learning. We cast contrastive learning as training a non-parametric classifier by assigning a unique virtual class to each data in a batch. Then, data instances are mixed in both the input and virtual label spaces, providing more augmented data during training. In experiments, we demonstrate that i-Mix consistently improves the quality of learned representations across domains, including image, speech, and tabular data. Furthermore, we confirm its regularization effect via extensive ablation studies across model and dataset sizes. The code is available at https://github.com/kibok90/imix.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2208511",
                        "name": "Kibok Lee"
                    },
                    {
                        "authorId": "2117870479",
                        "name": "Yian Zhu"
                    },
                    {
                        "authorId": "1729571",
                        "name": "Kihyuk Sohn"
                    },
                    {
                        "authorId": "2116729195",
                        "name": "Chun-Liang Li"
                    },
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    },
                    {
                        "authorId": "2118338545",
                        "name": "Honglak Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Additionally, mixup has the property of curbing confirmation bias by enforcing label smoothness by combining yi and yj as noted by [73]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a4253ac950778e6854bbe2453330a7e051dc3ce9",
                "externalIds": {
                    "CorpusId": 235078936
                },
                "corpusId": 235078936,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a4253ac950778e6854bbe2453330a7e051dc3ce9",
                "title": "Enhancing OCR Performance with Low Supervision",
                "abstract": "Over the last decade, a tremendous emphasis has been laid on collection and digitization of a vast number of books leading to the creation of so-called \u2018Digital Libraries\u2019. Projects like Google Book and Project Gutenberg have made significant progress in digitizing over millions of books and making it available to the public. Efforts have also been made from the perspective of Indic languages where the task to identify and recognize books from several Indian languages has been undertaken by the National Digital Library of India. Advantages of digital libraries can be manifold. Digitization of ancient manuscripts ensures the preservation of knowledge and promotes research. Books in digital libraries are indexed which facilitates easy search and retrieval. They are easy to store and do not take as much effort in maintenance as their physical counterparts. One of the most important steps in the digitization effort is the recognition and conversion of physical pages into editable text using an OCR. There are commercial OCRs available like Tesseract and Abby fine reader, however, the ability of an OCR to recognize text without committing too many errors depends very much on the print quality of the pages as well as font style of the type-written text. A pre-trained OCR will invariably make errors across pages whose distribution is different in terms of fonts and print quality from the pages on which it was trained. If the domain gap is too large then the number of error words will be too high which will result in investing significant effort in the correction. Since the books need to be indexed, one cannot afford to have too many word errors in the OCR recognized pages. Thus, a major effort must be spent on correcting the error words, misclassified by the OCR. Manually correcting each isolated error word will incur a huge cost and is infeasible. In this thesis, we look at methods to improve OCR accuracy with minimum human involvement. To this effect, we propose two approaches. In the first approach, we strive to improve the OCR performance via an efficient postprocessing technique where we aim to group similar erroneous words and correct them simultaneously. We argue that since a book has a common underlying theme, it will contain many word repetitions. These word co-occurrences can be taken advantage of by grouping similar error words and correcting them in batches. We propose a novel clustering scheme which combines features from both images as well as its text transcription to group error word predictions. The grouped error predictions can then be corrected either automatically or with the help of a human annotator. We show via experimental verification that automatic correction of error word batches might not be the most efficient way to correct the error words and employing a human annotator to verify the error word clusters will be a more systematic way to address the issue.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "121490865",
                        "name": "Deepayan Das"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bf3bf8dab688f75619215ac03cf7a711befbe6d6",
                "externalIds": {
                    "DBLP": "journals/iacr/XiaoD21b",
                    "CorpusId": 235343310
                },
                "corpusId": 235343310,
                "publicationVenue": {
                    "id": "166fd2b5-a928-4a98-a449-3b90935cc101",
                    "name": "IACR Cryptology ePrint Archive",
                    "type": "journal",
                    "alternate_names": [
                        "IACR Cryptol eprint Arch"
                    ],
                    "url": "http://eprint.iacr.org/"
                },
                "url": "https://www.semanticscholar.org/paper/bf3bf8dab688f75619215ac03cf7a711befbe6d6",
                "title": "Towards Understanding Practical Randomness Beyond Noise: Differential Privacy and Mixup",
                "abstract": "Information-theoretical privacy relies on randomness. Representatively, Differential Privacy (DP) has emerged as the gold standard to quantify the individual privacy preservation provided by given randomness. However, almost all randomness in existing differentially private optimization and learning algorithms is restricted to noise perturbation. In this paper, we set out to provide a privacy analysis framework to understand the privacy guarantee produced by other randomness commonly used in optimization and learning algorithms (e.g., parameter randomness). We take mixup: a random linear aggregation of inputs, as a concrete example. Our contributions are twofold. First, we develop a rigorous analysis on the privacy amplification provided by mixup either on samples or updates, where we find the hybrid structure of mixup and the Laplace Mechanism produces a new type of DP guarantee lying between Pure DP and Approximate DP. Such an average-case privacy amplification can produce tighter composition bounds. Second, both empirically and theoretically, we show that proper mixup comes almost free of utility compromise.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34959471",
                        "name": "Hanshen Xiao"
                    },
                    {
                        "authorId": "1695217",
                        "name": "S. Devadas"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2d9db50b78b09b7ad4b23c129547e8591036a0ec",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-04527",
                    "CorpusId": 235367649
                },
                "corpusId": 235367649,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2d9db50b78b09b7ad4b23c129547e8591036a0ec",
                "title": "LaplaceNet: A Hybrid Energy-Neural Model for Deep Semi-Supervised Classification",
                "abstract": "Semi-supervised learning has received a lot of recent attention as it alleviates the need for large amounts of labelled data which can often be expensive, requires expert knowledge and be time consuming to collect. Recent developments in deep semi-supervised classification have reached unprecedented performance and the gap between supervised and semi-supervised learning is ever-decreasing. This improvement in performance has been based on the inclusion of numerous technical tricks, strong augmentation techniques and costly optimisation schemes with multi-term loss functions. We propose a new framework, LaplaceNet, for deep semi-supervised classification that has a greatly reduced model complexity. We utilise a hybrid energyneural network where graph based pseudo-labels, generated by minimising the graphical Laplacian, are used to iteratively improve a neural-network backbone. Our model outperforms state-of-the-art methods for deep semi-supervised classification, over several benchmark datasets. Furthermore, we consider the application of strong-augmentations to neural networks theoretically and justify the use of a multi-sampling approach for semi-supervised learning. We demonstrate, through rigorous experimentation, that a multi-sampling augmentation approach improves generalisation and reduces the sensitivity of the network to augmentation. Code coming soon!",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1410092918",
                        "name": "P. Sellars"
                    },
                    {
                        "authorId": "1388720262",
                        "name": "Angelica I. Avil\u00e9s-Rivero"
                    },
                    {
                        "authorId": "1711104",
                        "name": "C. Sch\u00f6nlieb"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, in [36], the impact of MixUp data augmentation on the model uncertainty estimation (also known as model calibration) is assessed."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "81e1c4c0d77938b10f97d03722dc8c8d2420ff56",
                "externalIds": {
                    "DBLP": "journals/access/RamirezYMCEORJL21",
                    "DOI": "10.1109/ACCESS.2021.3085418",
                    "CorpusId": 235476528,
                    "PubMed": "34812397"
                },
                "corpusId": 235476528,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/81e1c4c0d77938b10f97d03722dc8c8d2420ff56",
                "title": "Improving Uncertainty Estimation With Semi-Supervised Deep Learning for COVID-19 Detection Using Chest X-Ray Images",
                "abstract": "In this work we implement a COVID-19 infection detection system based on chest X-ray images with uncertainty estimation. Uncertainty estimation is vital for safe usage of computer aided diagnosis tools in medical applications. Model estimations with high uncertainty should be carefully analyzed by a trained radiologist. We aim to improve uncertainty estimations using unlabelled data through the MixMatch semi-supervised framework. We test popular uncertainty estimation approaches, comprising Softmax scores, Monte-Carlo dropout and deterministic uncertainty quantification. To compare the reliability of the uncertainty estimates, we propose the usage of the Jensen-Shannon distance between the uncertainty distributions of correct and incorrect estimations. This metric is statistically relevant, unlike most previously used metrics, which often ignore the distribution of the uncertainty estimations. Our test results show a significant improvement in uncertainty estimates when using unlabelled data. The best results are obtained with the use of the Monte Carlo dropout method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1387523092",
                        "name": "Saul Calderon-Ramirez"
                    },
                    {
                        "authorId": "2144996381",
                        "name": "Shengxiang Yang"
                    },
                    {
                        "authorId": "2511274",
                        "name": "Armaghan Moemeni"
                    },
                    {
                        "authorId": "1419514160",
                        "name": "Simon Colreavy-Donnelly"
                    },
                    {
                        "authorId": "143845181",
                        "name": "D. Elizondo"
                    },
                    {
                        "authorId": "1594025351",
                        "name": "Luis Oala"
                    },
                    {
                        "authorId": "1401180789",
                        "name": "J. Rodr\u00edguez-Capit\u00e1n"
                    },
                    {
                        "authorId": "1397604841",
                        "name": "M. Jim\u00e9nez-Navarro"
                    },
                    {
                        "authorId": "123170618",
                        "name": "Ezequiel L\u00f3pez-Rubio"
                    },
                    {
                        "authorId": "1398215310",
                        "name": "Miguel A. Molina-Cabello"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d9dd87a39f5836b9e63728280360532fdefa671c",
                "externalIds": {
                    "DBLP": "journals/iacr/XiaoD21",
                    "CorpusId": 232094767
                },
                "corpusId": 232094767,
                "publicationVenue": {
                    "id": "166fd2b5-a928-4a98-a449-3b90935cc101",
                    "name": "IACR Cryptology ePrint Archive",
                    "type": "journal",
                    "alternate_names": [
                        "IACR Cryptol eprint Arch"
                    ],
                    "url": "http://eprint.iacr.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d9dd87a39f5836b9e63728280360532fdefa671c",
                "title": "DAUnTLeSS: Data Augmentation and Uniform Transformation for Learning with Scalability and Security",
                "abstract": "We revisit private optimization and learning from an information processing view. The main contribution of this paper is twofold. First, different from the classic cryptographic framework of operation-by-operation obfuscation, a novel private learning and inference framework via either random or data-dependent transformation on the sample domain is proposed. Second, we propose a novel security analysis framework, termed probably approximately correct (PAC) inference resistance, which bridges the information loss in data processing and prior knowledge. Using the entropy of private data, we develop an information theoretical security amplifier with a foundation of PAC security. We study the applications of such a framework from generalized linear regression models to modern learning techniques, such as deep learning. On the information theoretical privacy side, we compare three privacy interpretations: ambiguity, statistical indistinguishability (Differential Privacy) and PAC inference resistance, and precisely describe the information leakage of our framework. We show the advantages of this new random transform approach with respect to underlying privacy guarantees, computational efficiency and utility for fully-connected neural networks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34959471",
                        "name": "Hanshen Xiao"
                    },
                    {
                        "authorId": "1695217",
                        "name": "S. Devadas"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[53] evaluate mixup training, which convexly combines different images (and their labels) in the training In dataset to create new images (and labels)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cd5ea066e9cf7858f547c329054602e812613927",
                "externalIds": {
                    "CorpusId": 235679118
                },
                "corpusId": 235679118,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cd5ea066e9cf7858f547c329054602e812613927",
                "title": "Language Guided Out-of-Distribution Detection",
                "abstract": "Language-Guided Out-of-Distribution Detection",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1985966420",
                        "name": "William Gan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f59a9350b235bf7b6b778821e94c39eb6a3ea3ee",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-09546",
                    "CorpusId": 236134349
                },
                "corpusId": 236134349,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f59a9350b235bf7b6b778821e94c39eb6a3ea3ee",
                "title": "Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Technical Challenges and Solutions",
                "abstract": "Machine learning is expected to fuel significant improvements in medical care. To ensure that fundamental principles such as beneficence, respect for human autonomy, prevention of harm, justice, privacy, and transparency are respected, medical machine learning applications must be developed responsibly. A large number of high-level declarations of ethical principles have been put forth for this purpose, but there is a severe lack of technical guidelines explicating the practical consequences for medical machine learning. Similarly, there is currently considerable uncertainty regarding the exact regulatory requirements placed upon medical machine learning systems. In this paper, we survey the technical challenges involved in creating medical machine learning systems responsibly and in conformity with existing regulations, as well as possible solutions to address these challenges. We begin by providing a brief overview of existing regulations affecting medical machine learning, showing that properties such as safety, robustness, reliability, privacy, security, transparency, explainability, and nondiscrimination are all demanded already by existing law and regulations \u2014 albeit, in many cases, to an uncertain degree. Next, we discuss the key technical obstacles to achieving these desirable properties, and important techniques to overcome those barriers in the medical context. Since most of the technical challenges are very young and new problems frequently emerge, the scientific discourse is rapidly evolving and has not yet converged on clear best-practice solutions. Nevertheless, we aim to illuminate the underlying technical challenges, possible ways for addressing them, and their respective merits and drawbacks. In particular, we notice that distribution shift, spurious correlations, model underspecification, and data scarcity represent severe challenges in the medical context (and others) that are very difficult to solve with classical black-box deep neural networks. Important measures that may help to address these challenges include the use of large and representative datasets and federated learning as a means to that end, the careful exploitation of domain knowledge wherever feasible, the use of inherently transparent models, comprehensive model testing and verification, as well as stakeholder inclusion. ar X iv :2 10 7. 09 54 6v 1 [ cs .L G ] 2 0 Ju l 2 02 1",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "38482905",
                        "name": "Eike Petersen"
                    },
                    {
                        "authorId": "1388012403",
                        "name": "Yannik Potdevin"
                    },
                    {
                        "authorId": "2071358",
                        "name": "Esfandiar Mohammadi"
                    },
                    {
                        "authorId": "2633705",
                        "name": "S. Zidowitz"
                    },
                    {
                        "authorId": "2120158440",
                        "name": "Sabrina Breyer"
                    },
                    {
                        "authorId": "2831101",
                        "name": "Dirk Nowotka"
                    },
                    {
                        "authorId": "2120161211",
                        "name": "Sandra Henn"
                    },
                    {
                        "authorId": "2120168333",
                        "name": "Ludwig Pechmann"
                    },
                    {
                        "authorId": "1713104",
                        "name": "M. Leucker"
                    },
                    {
                        "authorId": "48973267",
                        "name": "P. Rostalski"
                    },
                    {
                        "authorId": "65943986",
                        "name": "C. Herzog"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The work of [39] and [4] both empirically evaluated Mixup\u2019s effect on OOD detection."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4ee5bc42d1388f4270e0f0c60fecd38aaca99be6",
                "externalIds": {
                    "CorpusId": 244347958
                },
                "corpusId": 244347958,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4ee5bc42d1388f4270e0f0c60fecd38aaca99be6",
                "title": "Mixture Outlier Exposure for Out-of-Distribution Detection in Fine-Grained Settings",
                "abstract": "Enabling out-of-distribution (OOD) detection for DNNs is critical for their safe and reliable operation in the open world. Despite recent progress, current works often consider a coarse level of granularity in the OOD problem, which fail to approximate many real-world fine-grained tasks where high granularity may be expected between the in-distribution (ID) data and the OOD data (e.g., identifying novel bird species for a bird classification system in the wild). In this work, we start by carefully constructing four large-scale fine-grained test environments in which existing methods are shown to have difficulties. We find that current methods, including ones that include a large/diverse set of outliers during DNN training, have poor coverage over the broad region where fine-grained OOD samples locate. We then propose Mixture Outlier Exposure (MixOE), which effectively expands the covered OOD region by mixing ID data and training outliers, and regularizes the model behaviour by linearly decaying the prediction confidence as the input transitions from ID to OOD. Extensive experiments and analyses demonstrate the effectiveness of MixOE for improving OOD detection in fine-grained settings.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2144192969",
                        "name": "Jingyang Zhang"
                    },
                    {
                        "authorId": "52121635",
                        "name": "Nathan Inkawhich"
                    },
                    {
                        "authorId": "2141218451",
                        "name": "Randolph Linderman"
                    },
                    {
                        "authorId": "5442167",
                        "name": "Yiran Chen"
                    },
                    {
                        "authorId": "51208601",
                        "name": "H. Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Relative Uncertainty Learning Inspired by the relativity of the uncertainty concept and the mixup method [53, 42, 46], we mix two different facial features according to their uncertainty values which enables the FER model to learn uncertainty through the relativity of different samples."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f67989d9c39af8a591ab4394d7a96231b0ecb50f",
                "externalIds": {
                    "DBLP": "conf/nips/ZhangWD21",
                    "CorpusId": 244895367
                },
                "corpusId": 244895367,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f67989d9c39af8a591ab4394d7a96231b0ecb50f",
                "title": "Relative Uncertainty Learning for Facial Expression Recognition",
                "abstract": "In facial expression recognition (FER), the uncertainties introduced by inherent noises like ambiguous facial expressions and inconsistent labels raise con-cerns about the credibility of recognition results. To quantify these uncertainties and achieve good performance under noisy data, we regard uncertainty as a relative concept and propose an innovative uncertainty learning method called Relative Uncertainty Learning (RUL). Rather than assuming Gaussian uncertainty distributions for all datasets, RUL builds an extra branch to learn uncertainty from the relative difficulty of samples by feature mixup. Specifically, we use uncertainties as weights to mix facial features and design an add-up loss to encourage uncertainty learning. It is easy to implement and adds little or no extra computation overhead. Extensive experiments show that RUL outperforms state-of-the-art FER uncertainty learning methods in both real-world and synthetic noisy FER datasets. Besides, RUL also works well on other datasets such as CIFAR and Tiny ImageNet. The code is available at https://github.com/zyh-uaiaaaa/Relative-Uncertainty-Learning .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108545448",
                        "name": "Yuhang Zhang"
                    },
                    {
                        "authorId": "2109141458",
                        "name": "Chengrui Wang"
                    },
                    {
                        "authorId": "1774956",
                        "name": "Weihong Deng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other works along the track also show improvements on various discriminative learning tasks [24, 36, 40, 43]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "237c06299366d93d8971f3322f691b756b6a064e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-02450",
                    "CorpusId": 244908181
                },
                "corpusId": 244908181,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/237c06299366d93d8971f3322f691b756b6a064e",
                "title": "Implicit Data Augmentation Using Feature Interpolation for Diversified Low-Shot Image Generation",
                "abstract": "Training of generative models especially Generative Adversarial Networks can easily diverge in low-data setting. To mitigate this issue, we propose a novel implicit data augmentation approach which facilitates stable training and synthesize diverse samples. Speci\ufb01cally, we view the discriminator as a metric embedding of the real data manifold, which offers proper distances between real data points. We then utilize information in the feature space to develop a data-driven augmentation method. We further bring up a simple metric to evaluate the diversity of synthesized samples. Experiments on few-shot generation tasks show our method improves FID and diversity of results compared to current methods, and allows generating high-quality and diverse images with less than 100 training samples.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Mengyu Dai"
                    },
                    {
                        "authorId": "103011435",
                        "name": "Haibin Hang"
                    },
                    {
                        "authorId": "49932298",
                        "name": "Xiaoyang Guo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Different from post-hoc calibration methods, another line of research aims to learn calibrated networks during training by modifying the training process [29, 12, 8].",
                "[29] found that DNNs trained with mixup are significantly better calibrated than DNNs trained in the regular fashion."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2c6ee2692351ae09ad9e933b0ba1eee0fd04c4cc",
                "externalIds": {
                    "DBLP": "conf/nips/WangFZ21",
                    "CorpusId": 245011563
                },
                "corpusId": 245011563,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2c6ee2692351ae09ad9e933b0ba1eee0fd04c4cc",
                "title": "Rethinking Calibration of Deep Neural Networks: Do Not Be Afraid of Overconfidence",
                "abstract": "Capturing accurate uncertainty quanti\ufb01cation of the predictions from deep neural networks is important in many real-world decision-making applications. A reliable predictor is expected to be accurate when it is con\ufb01dent about its predictions and indicate high uncertainty when it is likely to be inaccurate. However, modern neural networks have been found to be poorly calibrated, primarily in the direction of overcon\ufb01dence. In recent years, there is a surge of research on model calibration by leveraging implicit or explicit regularization techniques during training, which achieve well calibration performance by avoiding overcon\ufb01dent outputs. In our study, we empirically found that despite the predictions obtained from these regularized models are better calibrated , they suffer from not being as calibratable , namely, it is harder to further calibrate these predictions with post-hoc calibration methods like temperature scaling and histogram binning. We conduct a series of empirical studies showing that overcon\ufb01dence may not hurt \ufb01nal calibration performance if post-hoc calibration is allowed, rather, the penalty of con\ufb01dent outputs will compress the room of potential improvement in post-hoc calibration phase. Our experimental \ufb01ndings point out a new direction to improve calibration of DNNs by considering main training and post-hoc calibration as a uni\ufb01ed framework.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145350804",
                        "name": "Deng-Bao Wang"
                    },
                    {
                        "authorId": "47010134",
                        "name": "Lei Feng"
                    },
                    {
                        "authorId": "3039887",
                        "name": "Min-Ling Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Also the influence of mixup [18] is investigated, which can improve the calibration [17] and performs a data-level change compared to the ensembles."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6318d9ba3d15da9f3a4ff0a9bb95c43068b501ef",
                "externalIds": {
                    "CorpusId": 245022658
                },
                "corpusId": 245022658,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6318d9ba3d15da9f3a4ff0a9bb95c43068b501ef",
                "title": "Benchmarking Robustness to Natural Distribution Shifts for Facial Analysis",
                "abstract": "During the deployment of machine learning models, performance degradation can occur compared to the training and validation data. This generalization gap can appear for a variety of reasons and be particularly critical in applications where certain groups of people are disadvantaged by the outcome, e.g. facial analysis. Literature provides a vast amount of methods to either perform robust classification under distribution shifts or at least to express the uncertainty caused by the shifts. However, there is still a need for data that exhibit different natural distribution shifts considering specific subgroups to test these methods. We use a balanced dataset for facial analysis and introduce subpopulation shifts, spurious correlations, and subpopulation-specific label noise. This forms our basis to investigate to what extent known approaches for calibrating neural networks remain reliable under these specified shifts. Each of the modifications leads to performance degradation, but the combination of ensembles and temperature scaling is particularly useful to stabilize the calibration over the shifts.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2028192005",
                        "name": "J. Deuschel"
                    },
                    {
                        "authorId": "2105729816",
                        "name": "Andreas Foltyn"
                    },
                    {
                        "authorId": "2144555969",
                        "name": "Leonie Anna Adams"
                    },
                    {
                        "authorId": "2130317560",
                        "name": "J. M. Vieregge"
                    },
                    {
                        "authorId": "2097048717",
                        "name": "Ute"
                    },
                    {
                        "authorId": "2144553437",
                        "name": "Schmid"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2c4320d07db4e2ec46d71f838055239d0b9946b7",
                "externalIds": {
                    "DBLP": "journals/access/ChoCL21",
                    "DOI": "10.1109/ACCESS.2021.3108445",
                    "CorpusId": 237446372
                },
                "corpusId": 237446372,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2c4320d07db4e2ec46d71f838055239d0b9946b7",
                "title": "Re-Ranking System with BERT for Biomedical Concept Normalization",
                "abstract": "In recent years, various neural network architectures have been successfully applied to natural language processing (NLP) tasks such as named entity normalization. Named entity normalization is a fundamental task for extracting information in free text, which aims to map entity mentions in a text to gold standard entities in a given domain-specific ontology; however, the normalization task in the biomedical domain is still challenging because of multiple synonyms, various acronyms, and numerous lexical variations. In this study, we regard the task of biomedical entity normalization as a ranking problem and propose an approach to rank normalized concepts. We additionally employ two factors that can notably affect the performance of normalization, such as task-specific pre-training (Task-PT) and calibration approach. Among five different biomedical benchmark corpora, our experimental results show that our proposed model achieved significant improvements over the previous methods and advanced the state-of-the-art performance for biomedical entity normalization, with up to 0.5% increase in accuracy and 1.2% increase in F-score.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "4485082",
                        "name": "Hyejin Cho"
                    },
                    {
                        "authorId": "2007564294",
                        "name": "Dongha Choi"
                    },
                    {
                        "authorId": "49923640",
                        "name": "Hyunju Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Calibration Deep neural networks tend to predict overconfidently (Thulasidasan et al. 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5a69ec9a521e8562f1691f08722ef8fe38b5a426",
                "externalIds": {
                    "CorpusId": 260436690
                },
                "corpusId": 260436690,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5a69ec9a521e8562f1691f08722ef8fe38b5a426",
                "title": "Unveiling the Power of Mixup for Stronger Classifiers",
                "abstract": "Mixup-based data augmentations have achieved great success as regularizers for deep neural networks. However, existing methods rely on deliberately handcrafted mixup policies, which ignore or oversell the semantic matching between mixed samples and labels. Driven by their prior assumptions, early methods attempt to smooth decision boundaries by random linear interpolation while others focus on maximizing classrelated information via offline saliency optimization. As a result, the issue of label mismatch has not been well addressed. Additionally, the optimization stability of mixup training is constantly troubled by the label mismatch. To address these challenges, we first reformulate mixup for supervised classification as two sub-tasks, mixup sample generation and classification, then propose Automatic Mixup (AutoMix), a revolutionary mixup framework. Specifically, a learnable lightweight Mix Block (MB) with a cross-attention mechanism is proposed to generate a mixed sample by modeling a fair relationship between the pair of samples under direct supervision of the corresponding mixed label. Moreover, the proposed Momentum Pipeline (MP) enhances training stability and accelerates convergence on top of making the Mix Block fully trained end-to-end. Extensive experiments on five popular classification benchmarks show that the proposed approach consistently outperforms leading methods by a large margin.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2200082418",
                        "name": "Zicheng Liu"
                    },
                    {
                        "authorId": "2118155623",
                        "name": "Siyuan Li"
                    },
                    {
                        "authorId": "2118289574",
                        "name": "Di Wu"
                    },
                    {
                        "authorId": "2174997552",
                        "name": "Zhiyuan Chen"
                    },
                    {
                        "authorId": "47767812",
                        "name": "Lirong Wu"
                    },
                    {
                        "authorId": "2148902244",
                        "name": "Jianzhu Guo"
                    },
                    {
                        "authorId": "1390908654",
                        "name": "Stan Z. Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Therefore, Bayesian techniques have known a rising interest in medical imaging for classification [12], segmentation [15,22] and registration [18].",
                "But unlike Nketia and colleagues [22] who studied cellular segmentation, the confidence scores we use are derived from a deep BNNs rather than handcrafted.",
                "[22] introduces an uncertainty metric based on distribution similarity of the two most probable classes.",
                "Predicting such uncertainty measures helps interpreting the output of machine learning programs [6], which is especially useful in the medical imaging domain [14,22,23] where explainability is crucial [8].",
                "[22] considered the Bhattacharya coefficient, since it is interpretable (0: certain, 1: uncertain), Eq.",
                "The problem of uncertainty quantification has also been addressed using variational Bayesian methods [22]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c879f498d3baf174861a9935d3c0c28604c4e86d",
                "externalIds": {
                    "DBLP": "conf/miccai/2020unsure",
                    "DOI": "10.1007/978-3-030-60365-6",
                    "CorpusId": 222125715
                },
                "corpusId": 222125715,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c879f498d3baf174861a9935d3c0c28604c4e86d",
                "title": "Uncertainty for Safe Utilization of Machine Learning in Medical Imaging, and Graphs in Biomedical Image Analysis: Second International Workshop, UNSURE 2020, and Third International Workshop, GRAIL 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 8, 2020, Proceedings",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "33971010",
                        "name": "M. Graham"
                    },
                    {
                        "authorId": "1697589",
                        "name": "C. Sudre"
                    },
                    {
                        "authorId": "51137172",
                        "name": "Thomas Varsavsky"
                    },
                    {
                        "authorId": "1492002647",
                        "name": "Petru-Daniel Tudosiu"
                    },
                    {
                        "authorId": "2126033",
                        "name": "P. Nachev"
                    },
                    {
                        "authorId": "143951081",
                        "name": "S. Ourselin"
                    },
                    {
                        "authorId": "145244249",
                        "name": "M. Cardoso"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The works that are most relevant to our method are Mixup (Thulasidasan et al., 2019; Zhang et al., 2018) and variance-weighted confidence-integrated Loss (VWCI, Seo et al. (2019)).",
                "Regarding mixing-based techniques as a strong data augmentation scheme, Thulasidasan et al. (2019) show that the data augmentation alone without mixed labels can substantially improve the prediction accuracy, but not the predictive uncertainty.",
                "Furthermore, Thulasidasan et al. (2019) reports that Mixup training encourages that the output of DNN, the estimated label distributions, serves as a better indicator of the actual likelihood of a correction prediction.",
                "The works that are most relevant to our method are Mixup (Thulasidasan et al., 2019; Zhang et al., 2018) and variance-weighted confidence-integrated Loss (VWCI, Seo et al.",
                "Notably, this simple learning procedure results in robustness toward adversarial examples (Zhang et al., 2018) and improving calibration (Thulasidasan et al., 2019).",
                "Recently, Thulasidasan et al. (2019) have empirically shown the network trained with Mixup gives better-calibrated results.",
                "For the quantitative analysis of the confidence calibration, we used two popular metrics, the expected calibration error (ECE, Naeini et al. (2015)) and the overconfidence error (OE, Thulasidasan et al. (2019)).",
                "Especially, using a large \u03b1 value degrades the accuracy largely in Mixup Thulasidasan et al. (2019); Zhang et al. (2018).",
                ", 2018) and improving calibration (Thulasidasan et al., 2019).",
                "One possible explanation for this is under-fitting since its complicate training prevents sufficient convergence for the conventional learning procedure (Thulasidasan et al., 2019).",
                "Lately, Thulasidasan et al. (2019) empirically show that the label smoothing effect is a key factor for achieving the accurate predictive uncertainty."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "0ecfa4bc9003abea8d1ee0b001b23f307f5eed3d",
                "externalIds": {
                    "CorpusId": 230335305
                },
                "corpusId": 230335305,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0ecfa4bc9003abea8d1ee0b001b23f307f5eed3d",
                "title": "LOGIT AS AUXILIARY WEAK-SUPERVISION",
                "abstract": "When a person identifies objects, he or she can think by associating objects to many classes and conclude by taking inter-class relations into account. This cognitive system can make a more reliable prediction. Inspired by these observations, we propose a new network training strategy to consider inter-class relations, namely LogitMix. Specifically, we use recent data augmentation techniques (e.g., Mixup, Manifold Mixup, or CutMix) as baselines for generating mixed samples. Then, LogitMix suggests using the mixed logit (i.e. the mixture of two logits) as an auxiliary training objective. Because using logit before softmax activation preserves rich class relationships, it can serve as a weak-supervision signal concerning interclass relations. Our experimental results demonstrate that LogitMix achieves state-of-the-art performance among recent data augmentation techniques in terms of both calibration error and prediction accuracy. The source code is attached as the supplementary material.",
                "year": 2020,
                "authors": []
            }
        },
        {
            "contexts": [
                "Thulasidasan et al. (2019) demonstrate that neural networks trained with mixup are significantly better calibrated under dataset shift, and are less prone to over-confident predictions on out-of-distribution data.",
                "(2018), can consistently improve classification accuracy and further has been shown to be able to help with calibration in (Thulasidasan et al., 2019).",
                "It has been shown by Mu\u0308ller et al. (2019); Thulasidasan et al. (2019) that label smoothing can also effectively improve the quality of a model\u2019s uncertainty estimates.",
                "This trade-off between clean accuracy and calibration of mixup is also observed in other datasets and networks in Figure 2(j) in Thulasidasan et al. (2019).",
                "Overview of mixup Mixup, originally proposed by Zhang et al. (2018), can consistently improve classification accuracy and further has been shown to be able to help with calibration in (Thulasidasan et al., 2019)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c8e422a895862828ee5f5507ad9ad624b5d8edf3",
                "externalIds": {
                    "CorpusId": 236924114
                },
                "corpusId": 236924114,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c8e422a895862828ee5f5507ad9ad624b5d8edf3",
                "title": "W HAT ARE EFFECTIVE LABELS FOR AUGMENTED DATA ? I MPROVING ROBUSTNESS WITH A UTO L ABEL",
                "abstract": "A wide breadth of research has devised data augmentation approaches that can improve both accuracy and generalization performance for neural networks. However, augmented data can end up being far from the clean data and what is the appropriate label is less clear. Despite this, most existing work simply reuses the original label from the clean data, and the choice of label accompanying the augmented data is relatively less explored. In this paper, we propose AutoLabel to automatically learn the labels for augmented data, based on the distance between the clean distribution and augmented distribution. AutoLabel is built on label smoothing and is guided by the calibration-performance over a hold-out validation set. We show that AutoLabel is a generic framework that can be easily applied to existing data augmentation methods, including AugMix, mixup, and adversarial training. Experiments on CIFAR-10, CIFAR-100 and ImageNet show that AutoLabel can improve models\u2019 accuracy and calibration performance, especially under distributional shift. Additionally, we demonstrate that AutoLabel can help adversarial training by bridging the gap between clean accuracy and adversarial robustness.",
                "year": 2020,
                "authors": []
            }
        },
        {
            "contexts": [
                "[19] show that mixup can improve the calibration of neural networks."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "72a502583abe34a83d950a31672d08b6db4aeddc",
                "externalIds": {
                    "CorpusId": 219632816
                },
                "corpusId": 219632816,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/72a502583abe34a83d950a31672d08b6db4aeddc",
                "title": "Scoring Confidence in Neural Networks",
                "abstract": "Scoring Confidence in Neural Networks",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2067028715",
                        "name": "N. Vemuri"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mixup [25] combines random pairs of images and their labels during training, originally aimed at increased performance but it has recently shown to improve the calibration of DNNs [23].",
                "Deep ensemble We train M standard DNNs independently of each other following [13] and combine the predictions as\np(y = k|x, \u03b8) = 1 M M\u2211 m=1 pm(y = k|x, \u03b8m) (5)\nMixup Recently proposed as a simple method by [25] for training better DNNs where two random input samples (xi, xj) and their corresponding labels (yi, yj) are combined using:\nx\u0303 = \u03bbxi + (1\u2212 \u03bb)xj y\u0303 = \u03bbyi + (1\u2212 \u03bb)yj\n(6)\nwhere \u03bb \u2208 [0, 1] determines the mixing ratio of the linear interpolation. \u03bb is drawn from a symmetric Beta distribution Beta(\u03b1, \u03b1), where \u03b1 controls the strength of the input interpolation and the label smoothing.",
                "Multiple popular methods have been proposed for quantifying predictive uncertainty for better calibration and robustness under distributional shifts and OOD inputs in deep neural networks (DNNs)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5f3ae4dd859c2ef7e1a6c695a7333db26072f4a2",
                "externalIds": {
                    "MAG": "3090505727",
                    "DBLP": "conf/miccai/ThagaardHVEHD20",
                    "DOI": "10.1007/978-3-030-59710-8_80",
                    "CorpusId": 221351684
                },
                "corpusId": 221351684,
                "publicationVenue": {
                    "id": "61a709e3-2060-423c-8de5-ffd3885aa31c",
                    "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                    "type": "conference",
                    "alternate_names": [
                        "Medical Image Computing and Computer-Assisted Intervention",
                        "MICCAI",
                        "Med Image Comput Comput Interv",
                        "Int Conf Med Image Comput Comput Interv"
                    ],
                    "url": "http://www.miccai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/5f3ae4dd859c2ef7e1a6c695a7333db26072f4a2",
                "title": "Can You Trust Predictive Uncertainty Under Real Dataset Shifts in Digital Pathology?",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1692314434",
                        "name": "J. Thagaard"
                    },
                    {
                        "authorId": "2142792",
                        "name": "S\u00f8ren Hauberg"
                    },
                    {
                        "authorId": "46555192",
                        "name": "B. Vegt"
                    },
                    {
                        "authorId": "3390025",
                        "name": "T. Ebstrup"
                    },
                    {
                        "authorId": "47907738",
                        "name": "J. D. Hansen"
                    },
                    {
                        "authorId": "2253200",
                        "name": "A. Dahl"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "34d23150150a2ed80fa3c32aa5baca7a4d22bd5e",
                "externalIds": {
                    "DBLP": "conf/aime/TsiligkaridisS20",
                    "MAG": "3088972517",
                    "DOI": "10.1007/978-3-030-59137-3_5",
                    "CorpusId": 221980919
                },
                "corpusId": 221980919,
                "publicationVenue": {
                    "id": "4e1eea23-99a8-4d19-a986-00ec1a3f1718",
                    "name": "Conference on Artificial Intelligence in Medicine in Europe",
                    "type": "conference",
                    "alternate_names": [
                        "Artif Intell Med Eur",
                        "AIME",
                        "Artificial Intelligence in Medicine in Europe",
                        "Conf Artif Intell Med Eur"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=110"
                },
                "url": "https://www.semanticscholar.org/paper/34d23150150a2ed80fa3c32aa5baca7a4d22bd5e",
                "title": "A Multi-task LSTM Framework for Improved Early Sepsis Prediction",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1902176",
                        "name": "Theodoros Tsiligkaridis"
                    },
                    {
                        "authorId": "40349968",
                        "name": "Jennifer Sloboda"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[15], follow-up work developed improved variants of temperature scaling [32] or new types of scaling [27, 28] as well as novel training procedures altogether [50]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c32eb0d283f81c0a6411ee72dacbfc453b41ef7d",
                "externalIds": {
                    "MAG": "3111327898",
                    "DBLP": "journals/corr/abs-2012-05329",
                    "CorpusId": 228083617
                },
                "corpusId": 228083617,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c32eb0d283f81c0a6411ee72dacbfc453b41ef7d",
                "title": "Know Your Limits: Uncertainty Estimation with ReLU Classifiers Fails at Reliable OOD Detection",
                "abstract": "A crucial requirement for reliable deployment of deep learning models for safety-critical applications is the ability to identify out-of-distribution (OOD) data points, samples which differ from the training data and on which a model might underperform. Previous work has attempted to tackle this problem using uncertainty estimation techniques. However, there is empirical evidence that a large family of these techniques do not detect OOD reliably in classification tasks.\r\nThis paper gives a theoretical explanation for said experimental findings and illustrates it on synthetic data. We prove that such techniques are not able to reliably identify OOD samples in a classification setting, since their level of confidence is generalized to unseen areas of the feature space. This result stems from the interplay between the representation of ReLU networks as piece-wise affine transformations, the saturating nature of activation functions like softmax, and the most widely-used uncertainty metrics.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "133864309",
                        "name": "Dennis Ulmer"
                    },
                    {
                        "authorId": "32346780",
                        "name": "G. Cin\u00e1"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, it has been observed that the success of mixup is highly sensitive to the shape of the mixing distribution [12, 9, 13].",
                "When used to train deep neural networks on classification tasks, mixup has been shown to achieve both better generalization and increased model calibration across a range of data domains: images, audio, text, and tabular data [1, 9]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3f2783c287d2bfeea307ddeb41ebd3a2e98525fd",
                "externalIds": {
                    "CorpusId": 249636222
                },
                "corpusId": 249636222,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3f2783c287d2bfeea307ddeb41ebd3a2e98525fd",
                "title": "LEARNABLE LOSS MIXUP FOR SPEECH ENHANCEMENT",
                "abstract": "Mixup [1] is a recently proposed learning paradigm that improves the generalization of deep neural networks by training them on virtual data sampled from linear interpolations of examples and their labels. However, applying it to speech enhancement is challenging, because mixup was not designed for non-classification tasks and its success is contingent on the shape of the mixing distribution. We propose a generalization of mixup that mixes the losses instead of the labels, and automatically learns a non-linear mixing function by conditioning on the mixed data. On the VCTK benchmark, our proposal significantly outperforms standard training, learnable label mixup, and linear loss mixup. It achieves 3.26 PESQ, surpassing the previous state-of-the-art by 6 points.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2134532944",
                        "name": "Oscar Chang"
                    },
                    {
                        "authorId": "3275309",
                        "name": "D. Tran"
                    },
                    {
                        "authorId": "145733034",
                        "name": "K. Koishida"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The trainable methods are not compared in this work because the literature shows that they have a worse or similar calibration performance with temperature scaling [16, 22, 33].",
                "In this section, we introduce some existing calibration methods, including temperature scaling [4, 7], entropy regularization [24], MMCE regularization [16], label smoothing [22, 32], and Mixup training [33, 39].",
                "[33] demonstrate that Mixup is also useful for neural network calibration."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": null,
                "externalIds": null,
                "corpusId": "221669362",
                "publicationVenue": null,
                "url": null,
                "title": "LIANG: IMPROVED TRAINABLE CALIBRATION METHOD FOR NEURAL NETWORKS 1 Improved Trainable Calibration Method for Neural Networks on Medical Imaging Classification1",
                "abstract": null,
                "year": 2020,
                "authors": []
            }
        }
    ]
}