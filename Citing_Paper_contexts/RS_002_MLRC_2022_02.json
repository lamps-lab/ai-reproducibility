{
    "offset": 0,
    "data": [
        {
            "contexts": [
                "Some of the DG methods proposed over the past ten years include domain alignment [16], meta-learning [10], style transfer [28], and regularization methods [14]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f31ea64b5343b0721dd4022ea14c5160040a6aea",
                "externalIds": {
                    "ArXiv": "2309.09670",
                    "DBLP": "journals/corr/abs-2309-09670",
                    "DOI": "10.48550/arXiv.2309.09670",
                    "CorpusId": 262044577
                },
                "corpusId": 262044577,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f31ea64b5343b0721dd4022ea14c5160040a6aea",
                "title": "DGM-DR: Domain Generalization with Mutual Information Regularized Diabetic Retinopathy Classification",
                "abstract": "The domain shift between training and testing data presents a significant challenge for training generalizable deep learning models. As a consequence, the performance of models trained with the independent and identically distributed (i.i.d) assumption deteriorates when deployed in the real world. This problem is exacerbated in the medical imaging context due to variations in data acquisition across clinical centers, medical apparatus, and patients. Domain generalization (DG) aims to address this problem by learning a model that generalizes well to any unseen target domain. Many domain generalization techniques were unsuccessful in learning domain-invariant representations due to the large domain shift. Furthermore, multiple tasks in medical imaging are not yet extensively studied in existing literature when it comes to DG point of view. In this paper, we introduce a DG method that re-establishes the model objective function as a maximization of mutual information with a large pretrained model to the medical imaging field. We re-visit the problem of DG in Diabetic Retinopathy (DR) classification to establish a clear benchmark with a correct model selection strategy and to achieve robust domain-invariant representation for an improved generalization. Moreover, we conduct extensive experiments on public datasets to show that our proposed method consistently outperforms the previous state-of-the-art by a margin of 5.25% in average accuracy and a lower standard deviation. Source code available at https://github.com/BioMedIA-MBZUAI/DGM-DR",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2217093473",
                        "name": "A. Matsun"
                    },
                    {
                        "authorId": "2243183150",
                        "name": "Dana O. Mohamed"
                    },
                    {
                        "authorId": "2243189140",
                        "name": "Sharon Chokuwa"
                    },
                    {
                        "authorId": "98576663",
                        "name": "Muhammad Ridzuan"
                    },
                    {
                        "authorId": "145628052",
                        "name": "Mohammad Yaqub"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We further perform a user study [1, 2, 4, 35, 45, 50, 54] to investigate user preference over different stylization results."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "41316b89eb0b611b4b7cab9fa7c14f2291da454a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-06004",
                    "ArXiv": "2309.06004",
                    "DOI": "10.1145/3581783.3611819",
                    "CorpusId": 261696717
                },
                "corpusId": 261696717,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/41316b89eb0b611b4b7cab9fa7c14f2291da454a",
                "title": "TSSAT: Two-Stage Statistics-Aware Transformation for Artistic Style Transfer",
                "abstract": "Artistic style transfer aims to create new artistic images by rendering a given photograph with the target artistic style. Existing methods learn styles simply based on global statistics or local patches, lacking careful consideration of the drawing process in practice. Consequently, the stylization results either fail to capture abundant and diversified local style patterns, or contain undesired semantic information of the style image and deviate from the global style distribution. To address this issue, we imitate the drawing process of humans and propose a Two-Stage Statistics-Aware Transformation (TSSAT) module, which first builds the global style foundation by aligning the global statistics of content and style features and then further enriches local style details by swapping the local statistics (instead of local features) in a patch-wise manner, significantly improving the stylization effects. Moreover, to further enhance both content and style representations, we introduce two novel losses: an attention-based content loss and a patch-based style loss, where the former enables better content preservation by enforcing the semantic relation in the content image to be retained during stylization, and the latter focuses on increasing the local style similarity between the style and stylized images. Extensive qualitative and quantitative experiments verify the effectiveness of our method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47666905",
                        "name": "Haibo Chen"
                    },
                    {
                        "authorId": "2239164541",
                        "name": "Lei Zhao"
                    },
                    {
                        "authorId": "46276037",
                        "name": "Jun Yu Li"
                    },
                    {
                        "authorId": "2118801183",
                        "name": "Jian Yang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "18ae481b160a46e342ea19f61b5e4ceda95848b3",
                "externalIds": {
                    "DOI": "10.1016/j.jtice.2023.105075",
                    "CorpusId": 260724633
                },
                "corpusId": 260724633,
                "publicationVenue": {
                    "id": "f7678ec3-0ff7-4596-b8bf-b95c037b1a29",
                    "name": "Journal of the Taiwan Institute of Chemical Engineers / Elsevier",
                    "type": "journal",
                    "alternate_names": [
                        "J Taiwan Inst Chem Eng",
                        "J Taiwan Inst Chem Eng  Elsevier",
                        "Journal of The Taiwan Institute of Chemical Engineers"
                    ],
                    "issn": "1876-1070",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/715607/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/18761070"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/18ae481b160a46e342ea19f61b5e4ceda95848b3",
                "title": "Intelligent identification method of chemical processes based on maximum mean discrepancy domain generalization",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2107917270",
                        "name": "Yaxin Wang"
                    },
                    {
                        "authorId": "10027191",
                        "name": "Baochang Xu"
                    },
                    {
                        "authorId": "2228204620",
                        "name": "Congrui Pang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent works [40] show that semantic information can be reflected via the order of pixels according to their gray value.",
                "Specifically, we\nre-implement the TENT [34], BIN [19], DSU [20], Frequency Amplitude Normalization (AmpNorm) [36,37], SAN [35] and EFDMix [40].",
                "re-implement the TENT [34], BIN [19], DSU [20], Frequency Amplitude Normalization (AmpNorm) [36,37], SAN [35] and EFDMix [40]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2929c66e3b2d5260d1ea05ccd09d98d3fda253ba",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-00188",
                    "ArXiv": "2309.00188",
                    "DOI": "10.48550/arXiv.2309.00188",
                    "CorpusId": 261494194
                },
                "corpusId": 261494194,
                "publicationVenue": {
                    "id": "61a709e3-2060-423c-8de5-ffd3885aa31c",
                    "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                    "type": "conference",
                    "alternate_names": [
                        "Medical Image Computing and Computer-Assisted Intervention",
                        "MICCAI",
                        "Med Image Comput Comput Interv",
                        "Int Conf Med Image Comput Comput Interv"
                    ],
                    "url": "http://www.miccai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/2929c66e3b2d5260d1ea05ccd09d98d3fda253ba",
                "title": "DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation",
                "abstract": "Nucleus segmentation is usually the first step in pathological image analysis tasks. Generalizable nucleus segmentation refers to the problem of training a segmentation model that is robust to domain gaps between the source and target domains. The domain gaps are usually believed to be caused by the varied image acquisition conditions, e.g., different scanners, tissues, or staining protocols. In this paper, we argue that domain gaps can also be caused by different foreground (nucleus)-background ratios, as this ratio significantly affects feature statistics that are critical to normalization layers. We propose a Distribution-Aware Re-Coloring (DARC) model that handles the above challenges from two perspectives. First, we introduce a re-coloring method that relieves dramatic image color variations between different domains. Second, we propose a new instance normalization method that is robust to the variation in foreground-background ratios. We evaluate the proposed methods on two H$\\&$E stained image datasets, named CoNSeP and CPM17, and two IHC stained image datasets, called DeepLIIF and BC-DeepLIIF. Extensive experimental results justify the effectiveness of our proposed DARC model. Codes are available at \\url{https://github.com/csccsccsccsc/DARC",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48848049",
                        "name": "Shengcong Chen"
                    },
                    {
                        "authorId": "144116132",
                        "name": "Changxing Ding"
                    },
                    {
                        "authorId": "2237424976",
                        "name": "Dacheng Tao"
                    },
                    {
                        "authorId": "2051536212",
                        "name": "Hao Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Inspired by recent studies [5,7,14,22,27,42], we believe that by introducing a suitable normalization strategy, it is possible to effectively balance the training stability and image generation quality of GANs."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0fe35c17baf4a451ed11981ac518b89abf618278",
                "externalIds": {
                    "PubMedCentral": "10490267",
                    "DOI": "10.3390/s23177338",
                    "CorpusId": 261166922,
                    "PubMed": "37687794"
                },
                "corpusId": 261166922,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0fe35c17baf4a451ed11981ac518b89abf618278",
                "title": "SUGAN: A Stable U-Net Based Generative Adversarial Network",
                "abstract": "As one of the representative models in the field of image generation, generative adversarial networks (GANs) face a significant challenge: how to make the best trade-off between the quality of generated images and training stability. The U-Net based GAN (U-Net GAN), a recently developed approach, can generate high-quality synthetic images by using a U-Net architecture for the discriminator. However, this model may suffer from severe mode collapse. In this study, a stable U-Net GAN (SUGAN) is proposed to mainly solve this problem. First, a gradient normalization module is introduced to the discriminator of U-Net GAN. This module effectively reduces gradient magnitudes, thereby greatly alleviating the problems of gradient instability and overfitting. As a result, the training stability of the GAN model is improved. Additionally, in order to solve the problem of blurred edges of the generated images, a modified residual network is used in the generator. This modification enhances its ability to capture image details, leading to higher-definition generated images. Extensive experiments conducted on several datasets show that the proposed SUGAN significantly improves over the Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) metrics compared with several state-of-the-art and classic GANs. The training process of our SUGAN is stable, and the quality and diversity of the generated samples are higher. This clearly demonstrates the effectiveness of our approach for image generation tasks. The source code and trained model of our SUGAN have been publicly released.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2233755740",
                        "name": "Shijie Cheng"
                    },
                    {
                        "authorId": "2233763632",
                        "name": "Lingfeng Wang"
                    },
                    {
                        "authorId": "39767557",
                        "name": "M. Zhang"
                    },
                    {
                        "authorId": "2233573561",
                        "name": "Cheng Zeng"
                    },
                    {
                        "authorId": "2218732291",
                        "name": "Yan Meng"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9dbeee78d7f1d9bf3c3b001632c8b4883d861094",
                "externalIds": {
                    "ArXiv": "2308.10285",
                    "DBLP": "journals/corr/abs-2308-10285",
                    "DOI": "10.48550/arXiv.2308.10285",
                    "CorpusId": 261049380
                },
                "corpusId": 261049380,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9dbeee78d7f1d9bf3c3b001632c8b4883d861094",
                "title": "DomainDrop: Suppressing Domain-Sensitive Channels for Domain Generalization",
                "abstract": "Deep Neural Networks have exhibited considerable success in various visual tasks. However, when applied to unseen test datasets, state-of-the-art models often suffer performance degradation due to domain shifts. In this paper, we introduce a novel approach for domain generalization from a novel perspective of enhancing the robustness of channels in feature maps to domain shifts. We observe that models trained on source domains contain a substantial number of channels that exhibit unstable activations across different domains, which are inclined to capture domain-specific features and behave abnormally when exposed to unseen target domains. To address the issue, we propose a DomainDrop framework to continuously enhance the channel robustness to domain shifts, where a domain discriminator is used to identify and drop unstable channels in feature maps of each network layer during forward propagation. We theoretically prove that our framework could effectively lower the generalization bound. Extensive experiments on several benchmarks indicate that our framework achieves state-of-the-art performance compared to other competing methods. Our code is available at https://github.com/lingeringlight/DomainDrop.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50115584",
                        "name": "Jintao Guo"
                    },
                    {
                        "authorId": "1785352346",
                        "name": "Lei Qi"
                    },
                    {
                        "authorId": "2475959",
                        "name": "Yinghuan Shi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e8411b1b42ca26939a42d9c8cd5a3a8ae0a4a15a",
                "externalIds": {
                    "ArXiv": "2308.07863",
                    "DBLP": "journals/corr/abs-2308-07863",
                    "DOI": "10.48550/arXiv.2308.07863",
                    "CorpusId": 260900064
                },
                "corpusId": 260900064,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e8411b1b42ca26939a42d9c8cd5a3a8ae0a4a15a",
                "title": "StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models",
                "abstract": "Content and style (C-S) disentanglement is a fundamental problem and critical challenge of style transfer. Existing approaches based on explicit definitions (e.g., Gram matrix) or implicit learning (e.g., GANs) are neither interpretable nor easy to control, resulting in entangled representations and less satisfying results. In this paper, we propose a new C-S disentangled framework for style transfer without using previous assumptions. The key insight is to explicitly extract the content information and implicitly learn the complementary style information, yielding interpretable and controllable C-S disentanglement and style transfer. A simple yet effective CLIP-based style disentanglement loss coordinated with a style reconstruction prior is introduced to disentangle C-S in the CLIP image space. By further leveraging the powerful style removal and generative ability of diffusion models, our framework achieves superior results than state of the art and flexible C-S disentanglement and trade-off control. Our work provides new insights into the C-S disentanglement in style transfer and demonstrates the potential of diffusion models for learning well-disentangled C-S characteristics.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2125067247",
                        "name": "Zhizhong Wang"
                    },
                    {
                        "authorId": "37310105",
                        "name": "Lei Zhao"
                    },
                    {
                        "authorId": "2034241827",
                        "name": "Wei Xing"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bd64b3b1cf174310ef648d65e65e8303fd1233cf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-02228",
                    "ArXiv": "2308.02228",
                    "DOI": "10.1145/3581783.3612451",
                    "CorpusId": 260611116
                },
                "corpusId": 260611116,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bd64b3b1cf174310ef648d65e65e8303fd1233cf",
                "title": "Painterly Image Harmonization using Diffusion Model",
                "abstract": "Painterly image harmonization aims to insert photographic objects into paintings and obtain artistically coherent composite images. Previous methods for this task mainly rely on inference optimization or generative adversarial network, but they are either very time-consuming or struggling at fine control of the foreground objects (e.g., texture and content details). To address these issues, we propose a novel Painterly Harmonization stable Diffusion model (PHDiffusion), which includes a lightweight adaptive encoder and a Dual Encoder Fusion (DEF) module. Specifically, the adaptive encoder and the DEF module first stylize foreground features within each encoder. Then, the stylized foreground features from both encoders are combined to guide the harmonization process. During training, besides the noise loss in diffusion model, we additionally employ content loss and two style losses, i.e., AdaIN style loss and contrastive style loss, aiming to balance the trade-off between style migration and content preservation. Compared with the state-of-the-art models from related fields, our PHDiffusion can stylize the foreground more sufficiently and simultaneously retain finer content. Our code and model are available at https://github.com/bcmi/PHDiffusion-Painterly-Image-Harmonization.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152518334",
                        "name": "Lingxiao Lu"
                    },
                    {
                        "authorId": "2118505938",
                        "name": "Jiangtong Li"
                    },
                    {
                        "authorId": "2109830545",
                        "name": "Junyan Cao"
                    },
                    {
                        "authorId": "1716055",
                        "name": "Li Niu"
                    },
                    {
                        "authorId": "2108911758",
                        "name": "Liqing Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "EFDMix [5] achieves precise feature distribution matching in the feature space using higher-order statistics and augments the training data with style transfer techniques to mitigate overfitting to the source domain.",
                "Following the methodology described in [5], we employ the OSNet architecture as the backbone for our experiments.",
                "Due to the inherent distribution disparities between multiple source domains and the target domain, domain generalization [1, 2, 5, 6] has emerged as a prominent research area.",
                "We compare our method with several SOTA methods, including feature-level perturbation methods such as [5, 44, 45, 46, 47, 48], as well as image-level perturbation methods like [2, 8, 13, 26, 25, 49, 50, 51].",
                "To mitigate the impact of domain shift, a multitude of domain generalization techniques have emerged [5, 6].",
                "ERM 71.8 76.1 44.6 36.0 57.1 MixStyle [44] 73.2 74.8 46.0 40.6 58.6 EFDMix [5] 75.3 77.4 48.0 44.2 61.2 ASA [52] 77.3 78.8 50.3 61.8 67.0 Pro-RandConv [53] - - - - 73.3 CPerb (ours) 85.1 86.1 66.6 72.6 77.6\nobservations can be made from this figure.",
                "Each column denotes a distinct domain, with the first column representing the source domain and the remaining three columns representing the target domains.\nwhen using Market1501 as the source domain and GRID as the target domain, where the CPerb framework achieves a significant 3.8% improvement in mean average precision (mAP) compared to the EFDMix [5].",
                "8% improvement in mean average precision (mAP) compared to the EFDMix [5]."
            ],
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1757e6bd8357f96f76bb2a63db592df838364eb2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-00918",
                    "ArXiv": "2308.00918",
                    "DOI": "10.48550/arXiv.2308.00918",
                    "CorpusId": 260379074
                },
                "corpusId": 260379074,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1757e6bd8357f96f76bb2a63db592df838364eb2",
                "title": "A Novel Cross-Perturbation for Single Domain Generalization",
                "abstract": "Single domain generalization aims to enhance the ability of the model to generalize to unknown domains when trained on a single source domain. However, the limited diversity in the training data hampers the learning of domain-invariant features, resulting in compromised generalization performance. To address this, data perturbation (augmentation) has emerged as a crucial method to increase data diversity. Nevertheless, existing perturbation methods often focus on either image-level or feature-level perturbations independently, neglecting their synergistic effects. To overcome these limitations, we propose CPerb, a simple yet effective cross-perturbation method. Specifically, CPerb utilizes both horizontal and vertical operations. Horizontally, it applies image-level and feature-level perturbations to enhance the diversity of the training data, mitigating the issue of limited diversity in single-source domains. Vertically, it introduces multi-route perturbation to learn domain-invariant features from different perspectives of samples with the same semantic category, thereby enhancing the generalization capability of the model. Additionally, we propose MixPatch, a novel feature-level perturbation method that exploits local image style information to further diversify the training data. Extensive experiments on various benchmark datasets validate the effectiveness of our method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2213838823",
                        "name": "Dongjia Zhao"
                    },
                    {
                        "authorId": "1785352346",
                        "name": "Lei Qi"
                    },
                    {
                        "authorId": "2226645663",
                        "name": "Xiao Shi"
                    },
                    {
                        "authorId": "2475959",
                        "name": "Yinghuan Shi"
                    },
                    {
                        "authorId": "2200850810",
                        "name": "Xin Geng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the texture transformation comparison, 76.9% of users prefer the results of SAdaIN over AdaIN, 71.2% prefer the results of SLST results over LST, and 64.1% prefer the results of SEFDM results over EFDM.",
                "We extend this approach to various methods such as AdaIN [13], linear style transfer (LST) [21] and EFDM [44].",
                "6 shows the results of the style transfer algorithms (e.g., AdaIN, LST, and EFDM), after utilizing the modules of semantic transfer.",
                "Similarly, semantic EFDM (SEFDM) is defined as SE (VS , VT , Useg) = \u22115 i=1 SE i ( V iS , V i T , U i seg ) :\nSLi =\n{ EHM ( V iS , V i T ) U iseg, if i 6= 5,\nV 5S , if i = 5, (7)\nTo increase the diversity of the texture, we also design a switch gate (SG) to stochastically exchange the part source feature V iS and the part target feature V i T in both the Eqs.",
                "2D style transfer has been extensively explored with many proposed methods such asAdaIN [13], LST [21], Adaattn[25], InST [40], and EFDM [45].",
                "It consisted of three main components: a comparison of shape transformation (five questions about NC, DSN, KPD, NT and ours), a comparison of texture transformation (five questions about AdaIN and ours, five questions about LST and ours, five questions about EFDM and ours), and a judgement of realism (five T/F questions).",
                "Decoder is identical to the decoder of AdaIN [13] , LST [21] and EFDM [44] for our SAdaIN, SLST and SEFDM, respectively.",
                "To show the transfer capacity of our method for 3D bird creation, we compare with four shape deformation methods (i.e., NC [37], DSN [34], KPD [17] and NT [15]) and three texture style transformation methods (i.e., AdaIN [13], LST [21] and EFDM [44]).",
                "We extend AdaIN [13], LST [21], and EFDM [44] into SAdaIN, SLST, and SEFDM respectively on the UV maps for the texture transfer.",
                "Third, we introduce a semantic UV texture transfer that exploits switched gates to respectively control whether the part textural transfer or not using AdaIN [13], LST [21] and EFDM [44].",
                "\u2022 We present a semantic UV transfer method to utilize switched gates to respectively control whether the part textural transfer or not by using AdaIN [13], LST [21], and EFDM [44], and employing semantic UV segmentation for the texture style transfer."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "dcf6c9c3ad607dccdccc4bf619d9350b2b42a6d7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-14127",
                    "ArXiv": "2307.14127",
                    "DOI": "10.48550/arXiv.2307.14127",
                    "CorpusId": 260164875
                },
                "corpusId": 260164875,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dcf6c9c3ad607dccdccc4bf619d9350b2b42a6d7",
                "title": "Creative Birds: Self-Supervised Single-View 3D Style Transfer",
                "abstract": "In this paper, we propose a novel method for single-view 3D style transfer that generates a unique 3D object with both shape and texture transfer. Our focus lies primarily on birds, a popular subject in 3D reconstruction, for which no existing single-view 3D transfer methods have been developed.The method we propose seeks to generate a 3D mesh shape and texture of a bird from two single-view images. To achieve this, we introduce a novel shape transfer generator that comprises a dual residual gated network (DRGNet), and a multi-layer perceptron (MLP). DRGNet extracts the features of source and target images using a shared coordinate gate unit, while the MLP generates spatial coordinates for building a 3D mesh. We also introduce a semantic UV texture transfer module that implements textural style transfer using semantic UV segmentation, which ensures consistency in the semantic meaning of the transferred regions. This module can be widely adapted to many existing approaches. Finally, our method constructs a novel 3D bird using a differentiable renderer. Experimental results on the CUB dataset verify that our method achieves state-of-the-art performance on the single-view 3D style transfer task. Code is available in https://github.com/wrk226/creative_birds.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108876305",
                        "name": "Renke Wang"
                    },
                    {
                        "authorId": "2224895559",
                        "name": "Guimin Que"
                    },
                    {
                        "authorId": "15841516",
                        "name": "Shuo Chen"
                    },
                    {
                        "authorId": "2144439048",
                        "name": "Xiang Li"
                    },
                    {
                        "authorId": "46276037",
                        "name": "Jun Yu Li"
                    },
                    {
                        "authorId": "2146236917",
                        "name": "Jian Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[12] propose a method called Exact Feature Distribution Matching (EFDM) that matches the empirical cumulative distribution functions of image features.",
                "In this paper, we propose a novel method for data augmentation in domain generalization, which differs from existing methods that perform augmentation at the image or feature level [9], [7], [12] to directly enrich image style information.",
                "We compare our NormAUG method with several augmentation-based methods, including EFDMix [12], FACT [9], RSC [13], and STNP [16]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "12e14e98149d9b5dfbd9433353f18f5ff7ca8d08",
                "externalIds": {
                    "ArXiv": "2307.13492",
                    "DBLP": "journals/corr/abs-2307-13492",
                    "DOI": "10.48550/arXiv.2307.13492",
                    "CorpusId": 260154731
                },
                "corpusId": 260154731,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/12e14e98149d9b5dfbd9433353f18f5ff7ca8d08",
                "title": "NormAUG: Normalization-guided Augmentation for Domain Generalization",
                "abstract": "Deep learning has made significant advancements in supervised learning. However, models trained in this setting often face challenges due to domain shift between training and test sets, resulting in a significant drop in performance during testing. To address this issue, several domain generalization methods have been developed to learn robust and domain-invariant features from multiple training domains that can generalize well to unseen test domains. Data augmentation plays a crucial role in achieving this goal by enhancing the diversity of the training data. In this paper, inspired by the observation that normalizing an image with different statistics generated by different batches with various domains can perturb its feature, we propose a simple yet effective method called NormAUG (Normalization-guided Augmentation). Our method includes two paths: the main path and the auxiliary (augmented) path. During training, the auxiliary path includes multiple sub-paths, each corresponding to batch normalization for a single domain or a random combination of multiple domains. This introduces diverse information at the feature level and improves the generalization of the main path. Moreover, our NormAUG method effectively reduces the existing upper boundary for generalization based on theoretical perspectives. During the test stage, we leverage an ensemble strategy to combine the predictions from the auxiliary path of our model, further boosting performance. Extensive experiments are conducted on multiple benchmark datasets to validate the effectiveness of our proposed method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1785352346",
                        "name": "Lei Qi"
                    },
                    {
                        "authorId": "2224837268",
                        "name": "Hongpeng Yang"
                    },
                    {
                        "authorId": "2475959",
                        "name": "Yinghuan Shi"
                    },
                    {
                        "authorId": "1735299",
                        "name": "Xin Geng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In this study, we alleviate domain discrepancy by first introducing unsupervised stain augmentation using our proposed RestainNet [14] to introduce domain specific samples in an unsuperivsed learning manner and then applying EFDMix [15] to enhance the cross-domain feature representation.",
                "\u2026feature clustering ResNet38 (ImageNet Pretrained) Imbalanced Training Samples RestainNet (Zhao et al., 2022)\nShared Weights\nDomain 1 Domain 2 Domain \ufffd Unsupervised Stain Augmentation (UnSA)\nInput Image\nResNet38 with EFDMix\n(Zhang et al., 2022)\nTraining Samples (1st Sampler)\nParent\nChildren\n1.",
                "Loss Functions\nTraining Phase\n\u2026\nImage Features\nMitosis Detection Pipeline\nTest Phase\nResNet38 with EFDMix\n(Zhang et al., 2022)\n\u2026\nPositive samples Negative samples\nFeature Representation\nPositive samples Negative samples\nFeature Representation (Parent)\nPositive samples Negative samples\nFeature\u2026",
                "\u2026Classes:\n\uff1aDropped negative samples \uff1aSelected negative samples\nPositive sample (Yellow boundary)\nNegative sample (Red boundary)\nResNet38 with EFDMix\n(Zhang et al., 2022)\nHematoxylin Channel\nPotential Nuclei\nInput Image\nTraining Samples (2nd Sampler)\nHematoxylin Channel Potential Nuclei\nFocal\u2026",
                "To tolerance the color variance across different institutions, we propose stain enhancement (SE) to augment the domain-relevant data by our previously proposed RestainNet [14] and enhance the cross-domain feature representation by EFDMix [15].",
                "Inspired by a style transfer method that augments cross-style features by mixing feature distributions, we apply an EFDMix [15] method to augment the feature representation of different stain styles."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "41dbc61d27e7bf7476fc820152b9f78361743700",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-05889",
                    "ArXiv": "2307.05889",
                    "DOI": "10.48550/arXiv.2307.05889",
                    "CorpusId": 259836944
                },
                "corpusId": 259836944,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/41dbc61d27e7bf7476fc820152b9f78361743700",
                "title": "Rethinking Mitosis Detection: Towards Diverse Data and Feature Representation",
                "abstract": "Mitosis detection is one of the fundamental tasks in computational pathology, which is extremely challenging due to the heterogeneity of mitotic cell. Most of the current studies solve the heterogeneity in the technical aspect by increasing the model complexity. However, lacking consideration of the biological knowledge and the complex model design may lead to the overfitting problem while limited the generalizability of the detection model. In this paper, we systematically study the morphological appearances in different mitotic phases as well as the ambiguous non-mitotic cells and identify that balancing the data and feature diversity can achieve better generalizability. Based on this observation, we propose a novel generalizable framework (MitDet) for mitosis detection. The data diversity is considered by the proposed diversity-guided sample balancing (DGSB). And the feature diversity is preserved by inter- and intra- class feature diversity-preserved module (InCDP). Stain enhancement (SE) module is introduced to enhance the domain-relevant diversity of both data and features simultaneously. Extensive experiments have demonstrated that our proposed model outperforms all the SOTA approaches in several popular mitosis detection datasets in both internal and external test sets using minimal annotation efforts with point annotations only. Comprehensive ablation studies have also proven the effectiveness of the rethinking of data and feature diversity balancing. By analyzing the results quantitatively and qualitatively, we believe that our proposed model not only achieves SOTA performance but also might inspire the future studies in new perspectives. Source code is at https://github.com/Onehour0108/MitDet.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144219950",
                        "name": "Hao Wang"
                    },
                    {
                        "authorId": "120803401",
                        "name": "Jiatai Lin"
                    },
                    {
                        "authorId": "2150383500",
                        "name": "Dan Li"
                    },
                    {
                        "authorId": "2152444080",
                        "name": "Jing Wang"
                    },
                    {
                        "authorId": "2112632877",
                        "name": "Bingchao Zhao"
                    },
                    {
                        "authorId": "49472989",
                        "name": "Zhenwei Shi"
                    },
                    {
                        "authorId": "2151989660",
                        "name": "Xipeng Pan"
                    },
                    {
                        "authorId": "2108883830",
                        "name": "Huadeng Wang"
                    },
                    {
                        "authorId": "2145729718",
                        "name": "Bingbing Li"
                    },
                    {
                        "authorId": "1873039",
                        "name": "C. Liang"
                    },
                    {
                        "authorId": "2146513340",
                        "name": "Guoqiang Han"
                    },
                    {
                        "authorId": "2155274512",
                        "name": "Li Liang"
                    },
                    {
                        "authorId": "2118642307",
                        "name": "Chu Han"
                    },
                    {
                        "authorId": "3168855",
                        "name": "Zaiyi Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Domain generalization (DG) Existing literature on DG strongly relies on supervised knowledge from source domain data, regardless of whether it originates from a single domain [39] or multiple domains [10,43,46,47], which may not be realistic in continually changing scenarios, as knowledge comes in a sequential manner."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "984b7a9c554d5d8d384e7c32a1fbb10f43635192",
                "externalIds": {
                    "ArXiv": "2307.05707",
                    "DBLP": "journals/corr/abs-2307-05707",
                    "DOI": "10.48550/arXiv.2307.05707",
                    "CorpusId": 259837333
                },
                "corpusId": 259837333,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/984b7a9c554d5d8d384e7c32a1fbb10f43635192",
                "title": "MoP-CLIP: A Mixture of Prompt-Tuned CLIP Models for Domain Incremental Learning",
                "abstract": "Despite the recent progress in incremental learning, addressing catastrophic forgetting under distributional drift is still an open and important problem. Indeed, while state-of-the-art domain incremental learning (DIL) methods perform satisfactorily within known domains, their performance largely degrades in the presence of novel domains. This limitation hampers their generalizability, and restricts their scalability to more realistic settings where train and test data are drawn from different distributions. To address these limitations, we present a novel DIL approach based on a mixture of prompt-tuned CLIP models (MoP-CLIP), which generalizes the paradigm of S-Prompting to handle both in-distribution and out-of-distribution data at inference. In particular, at the training stage we model the features distribution of every class in each domain, learning individual text and visual prompts to adapt to a given domain. At inference, the learned distributions allow us to identify whether a given test sample belongs to a known domain, selecting the correct prompt for the classification task, or from an unseen domain, leveraging a mixture of the prompt-tuned CLIP models. Our empirical evaluation reveals the poor performance of existing DIL methods under domain shift, and suggests that the proposed MoP-CLIP performs competitively in the standard DIL settings while outperforming state-of-the-art methods in OOD scenarios. These results demonstrate the superiority of MoP-CLIP, offering a robust and general solution to the problem of domain incremental learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2212310692",
                        "name": "Julien Nicolas"
                    },
                    {
                        "authorId": "51056617",
                        "name": "Florent Chiaroni"
                    },
                    {
                        "authorId": "9403301",
                        "name": "Imtiaz Masud Ziko"
                    },
                    {
                        "authorId": "1484496251",
                        "name": "Ola Ahmad"
                    },
                    {
                        "authorId": "1739646",
                        "name": "Christian Desrosiers"
                    },
                    {
                        "authorId": "144702316",
                        "name": "J. Dolz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AdaIN had a significant influence on subsequent research in style transfer, leading to the proposal of various general style transfer methods [9-16]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5c8e6b37100c158b8035873e0398903497e7de52",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-00225",
                    "ArXiv": "2307.00225",
                    "DOI": "10.48550/arXiv.2307.00225",
                    "CorpusId": 259316635
                },
                "corpusId": 259316635,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5c8e6b37100c158b8035873e0398903497e7de52",
                "title": "StyleStegan: Leak-free Style Transfer Based on Feature Steganography",
                "abstract": "In modern social networks, existing style transfer methods suffer from a serious content leakage issue, which hampers the ability to achieve serial and reversible stylization, thereby hindering the further propagation of stylized images in social networks. To address this problem, we propose a leak-free style transfer method based on feature steganography. Our method consists of two main components: a style transfer method that accomplishes artistic stylization on the original image and an image steganography method that embeds content feature secrets on the stylized image. The main contributions of our work are as follows: 1) We identify and explain the phenomenon of content leakage and its underlying causes, which arise from content inconsistencies between the original image and its subsequent stylized image. 2) We design a neural flow model for achieving loss-free and biased-free style transfer. 3) We introduce steganography to hide content feature information on the stylized image and control the subsequent usage rights. 4) We conduct comprehensive experimental validation using publicly available datasets MS-COCO and Wikiart. The results demonstrate that StyleStegan successfully mitigates the content leakage issue in serial and reversible style transfer tasks. The SSIM performance metrics for these tasks are 14.98% and 7.28% higher, respectively, compared to a suboptimal baseline model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2221028194",
                        "name": "Xiujian Liang"
                    },
                    {
                        "authorId": "2221131339",
                        "name": "Bingshan Liu"
                    },
                    {
                        "authorId": "102329059",
                        "name": "Qichao Ying"
                    },
                    {
                        "authorId": "3243117",
                        "name": "Zhenxing Qian"
                    },
                    {
                        "authorId": "152899522",
                        "name": "Xinpeng Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7997da9fe8bb22ecc8d6ed814dbf717ca92d9e76",
                "externalIds": {
                    "DOI": "10.1117/1.JEI.32.4.043011",
                    "CorpusId": 259900866
                },
                "corpusId": 259900866,
                "publicationVenue": {
                    "id": "c677ab24-0c04-487d-83e2-c252af9479c8",
                    "name": "Journal of Electronic Imaging (JEI)",
                    "type": "journal",
                    "alternate_names": [
                        "J Electron Imaging (JEI",
                        "Journal of Electronic Imaging",
                        "J Electron Imaging"
                    ],
                    "issn": "1017-9909",
                    "url": "https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging",
                    "alternate_urls": [
                        "http://electronicimaging.spiedigitallibrary.org/journal.aspx"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7997da9fe8bb22ecc8d6ed814dbf717ca92d9e76",
                "title": "Controllable face image editing in a disentanglement way",
                "abstract": "Abstract. The prevalence of deep learning has attracted interest in the face image manipulation domain, especially in face editing with disentanglement representation. However, how to realize controllable disentangled representation of face images still remains challenging. Current methods require extensive supervision and training, or images will have a significantly impaired quality. We present an approach that learns how to represent data in an ideal disentangled way, with minimal supervision. Specifically, we use a swapping autoencoder with identity and attribute branches to learn identity and attribute representations, respectively. In addition, we separate the process of disentanglement and synthesis by an advanced pre-trained unsupervised StyleGAN2 image generator to make the entire network structure focus on learning data disentanglement. The identity and attribute vectors from different images are combined into a new representation that is mapped by a linear mapper into the generator\u2019s latent space to generate a new hybrid image. In this way, we take advantage of StyleGAN2\u2019s most advanced quality and its expressive latent space without the pressure of training a decoder. Experimental results prove that our method successfully separates the identity and other attributes of face images, outperforms existing methods, and requires less training and supervision.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2223357271",
                        "name": "Shiyan Zhou"
                    },
                    {
                        "authorId": "2133842898",
                        "name": "Ke Wang"
                    },
                    {
                        "authorId": "2155661439",
                        "name": "Jun Zhang"
                    },
                    {
                        "authorId": "2205749",
                        "name": "Yi-Ben Xia"
                    },
                    {
                        "authorId": "144287224",
                        "name": "Peng Chen"
                    },
                    {
                        "authorId": "2155840236",
                        "name": "Bing Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, there has been increasing attention on the feature augmentation method that employs style representation [11], [12].",
                "Under the domain-invariant assumptions, there are many lines of research: contrastive learning [7], factor disentanglement [10], meta-learning [8], style augmentation in feature space [11], [12].",
                "[12] further expanded upon this research with the proposal"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "26c62c1f70303ee5c9fddffa36f3889e071a62c4",
                "externalIds": {
                    "DOI": "10.1109/UR57808.2023.10202303",
                    "CorpusId": 260740265
                },
                "corpusId": 260740265,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/26c62c1f70303ee5c9fddffa36f3889e071a62c4",
                "title": "A simple baseline for domain generalization of action recognition and a realistic out-of-domain scenario",
                "abstract": "In this study, we tackle the Domain Generalization (DG) challenge in the field of video action recognition. Previous works on DG for action recognition mostly focus on the Single Domain Generalization (SDG) problem, which is difficult to achieve due to domain biases present in datasets. Thus, we aim to address this challenge by re-defining DG for video action recognition. Another limitation of existing research is the lack of realistic out-of-domain scenarios. To overcome this, we introduce a new benchmark for DG in video action recognition named ENT dataset that contains realistic domain shifts. Additionally, we propose a straightforward baseline for DG in the video domain that helps improve the performance of action recognition models in unseen domains. Our proposed approach adapts the image-based methods to the video domain, leading to improved DG performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2135937738",
                        "name": "Hyungmin Kim"
                    },
                    {
                        "authorId": "2229000655",
                        "name": "Hobeum Jeon"
                    },
                    {
                        "authorId": "2143648980",
                        "name": "Dohyung Kim"
                    },
                    {
                        "authorId": "2192343875",
                        "name": "Jaehong Kim"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4a1647312ab7d0e2d0345cfa6ed477e048b43780",
                "externalIds": {
                    "DBLP": "conf/ijcnn/LiAS23",
                    "DOI": "10.1109/IJCNN54540.2023.10191267",
                    "CorpusId": 259905561
                },
                "corpusId": 259905561,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/4a1647312ab7d0e2d0345cfa6ed477e048b43780",
                "title": "Domain Generalization and Feature Fusion for Cross-domain Imperceptible Adversarial Attack Detection",
                "abstract": "Deep learning-based imperceptible adversarial attack detection methods have recently seen significant progress. However, the accuracy, latency, and computational cost of previous methods remain insufficient. Particularly, trained attack detection models can potentially be applied in previously unseen conditions, such as new datasets or attacks for real-world applications. Therefore, to improve domain generalization performance, we propose a new method for cross-domain imperceptible adversarial attack detection by leveraging domain generalization, where we train the model's feature extractor or detector with a partner well-tuned for different domains. Different from conventional domain generalization methods, we use the global loss and local loss to train each feature extractor or detector. Moreover, to efficiently re-use high-resolution feature maps from the feature extractor, we propose a feature fusion network, which exploits feature maps from images that are attacked with different error rates and helps extract rich features to further improve the attack detection accuracy. Extensive experiments on four public datasets are used to demonstrate the efficacy of the proposed method. The source code of the proposed method is available at https://github.com/Yukino-3/DTAD.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153684801",
                        "name": "Yi Li"
                    },
                    {
                        "authorId": "1719855",
                        "name": "P. Angelov"
                    },
                    {
                        "authorId": "2094514756",
                        "name": "N. Suri"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Data-based methods augment training data to prevent overfitting [51, 50, 62, 59, 6, 32, 7, 53, 54]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4de8d1fc869bdfb2fe2369deda886ffaf08a2c78",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-08713",
                    "ArXiv": "2306.08713",
                    "DOI": "10.48550/arXiv.2306.08713",
                    "CorpusId": 259164574
                },
                "corpusId": 259164574,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4de8d1fc869bdfb2fe2369deda886ffaf08a2c78",
                "title": "What can a cook in Italy teach a mechanic in India? Action Recognition Generalisation Over Scenarios and Locations",
                "abstract": "We propose and address a new generalisation problem: can a model trained for action recognition successfully classify actions when they are performed within a previously unseen scenario and in a previously unseen location? To answer this question, we introduce the Action Recognition Generalisation Over scenarios and locations dataset (ARGO1M), which contains 1.1M video clips from the large-scale Ego4D dataset, across 10 scenarios and 13 locations. We demonstrate recognition models struggle to generalise over 10 proposed test splits, each of an unseen scenario in an unseen location. We thus propose CIR, a method to represent each video as a Cross-Instance Reconstruction of videos from other domains. Reconstructions are paired with text narrations to guide the learning of a domain generalisable representation. We provide extensive analysis and ablations on ARGO1M that show CIR outperforms prior domain generalisation works on all test splits. Code and data: https://chiaraplizz.github.io/what-can-a-cook/.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1879516321",
                        "name": "Chiara Plizzari"
                    },
                    {
                        "authorId": "2682004",
                        "name": "Toby Perrett"
                    },
                    {
                        "authorId": "1752593147",
                        "name": "Barbara Caputo"
                    },
                    {
                        "authorId": "145089978",
                        "name": "D. Damen"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2022) and EFDMix (Zhang et al., 2022) have been also proposed.",
                ", 2022), EFDMix (Zhang et al., 2022), that also work in the style space as ours.",
                ", 2017) that the domain characteristic of data has a strong correlation with the feature statistics (or style statistics) of the early layers of CNNs, the authors of (Zhou et al., 2021; Li et al., 2022; Zhang et al., 2022; Kang et al., 2022) proposed to generate new style statistics during training via style augmentation.",
                "Based on EFDM, the authors of (Zhang et al., 2022) also propose EFDMix, which replaces the concept of AdaIN in MixStyle with EFDM, in a channel-wise manner as follows: EFDMix(x)\u03c4i = \u03bbx\u03c4i + (1\u2212 \u03bb)y\u03bai .",
                "As in the setup of (Zhang et al., 2022), we adopt Market1501 (Zheng et al., 2015) and GRID (Loy et al., 2009) datasets, and train the model in one dataset and test on the other one.",
                "Our work is built upon the official setup of EFDMix (Zhang et al., 2022).",
                "Recently, various style augmentation methods such as MixStyle (Zhou et al., 2021), DSU (Li et al., 2022), Style Neophile (Kang et al., 2022) and EFDMix (Zhang et al., 2022) have been also proposed.",
                "Other setups are exactly the same as in MixStyle (Zhou et al., 2021), DSU (Li et al., 2022) and EFDMix (Zhang et al., 2022) when implementing each module; each module is activated with probability 0.5.",
                "This parameter also appears in MixStyle (Zhou et al., 2021) and EFDMix (Zhang et al., 2022), and we set \u03c4 = 0.1 for all experiments as in these prior works.",
                "The term f(s)\u2212 \u27e8f(s)\u27e9 is introduced to facilitate backpropagation of sample s as in (Zhang et al., 2022).",
                "The authors of (Zhang et al., 2022) proposed EFDM to replace AdaIN in (3).",
                "Following the setups in (Zhou et al., 2021; Li et al., 2022; Zhang et al., 2022), we adopt ResNet-18 pre-trained on ImageNet as a backbone, where the results with ResNet50 are reported in Appendix.",
                "As in the setup of (Zhang et al., 2022), we adopt Market1501 (Zheng et al.",
                "\u2026characteristic of data has a strong correlation with the feature statistics (or style statistics) of the early layers of CNNs, the authors of (Zhou et al., 2021; Li et al., 2022; Zhang et al., 2022; Kang et al., 2022) proposed to generate new style statistics during training via style augmentation.",
                ", 2021) and EFDMix (Zhang et al., 2022), and we set \u03c4 = 0.",
                "First, we consider the state-of-the-art style augmentation schemes, MixStyle (Zhou et al., 2021), DSU (Li et al., 2022), EFDMix (Zhang et al., 2022), that also work in the style space as ours."
            ],
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "fd17c14180691ca6060bc152cf8c4366b00d28d4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-04911",
                    "ArXiv": "2306.04911",
                    "DOI": "10.48550/arXiv.2306.04911",
                    "CorpusId": 259108761
                },
                "corpusId": 259108761,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/fd17c14180691ca6060bc152cf8c4366b00d28d4",
                "title": "Test-Time Style Shifting: Handling Arbitrary Styles in Domain Generalization",
                "abstract": "In domain generalization (DG), the target domain is unknown when the model is being trained, and the trained model should successfully work on an arbitrary (and possibly unseen) target domain during inference. This is a difficult problem, and despite active studies in recent years, it remains a great challenge. In this paper, we take a simple yet effective approach to tackle this issue. We propose test-time style shifting, which shifts the style of the test sample (that has a large style gap with the source domains) to the nearest source domain that the model is already familiar with, before making the prediction. This strategy enables the model to handle any target domains with arbitrary style statistics, without additional model update at test-time. Additionally, we propose style balancing, which provides a great platform for maximizing the advantage of test-time style shifting by handling the DG-specific imbalance issues. The proposed ideas are easy to implement and successfully work in conjunction with various other DG schemes. Experimental results on different datasets show the effectiveness of our methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118946828",
                        "name": "Jun-Gyu Park"
                    },
                    {
                        "authorId": "14757897",
                        "name": "Dong-Jun Han"
                    },
                    {
                        "authorId": "2144259699",
                        "name": "Soyeong Kim"
                    },
                    {
                        "authorId": "145193260",
                        "name": "J. Moon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In this section, we provide a qualitative comparison of our proposed method with the three selected baseline methods: EFDM (Example-based Feature-driven Diffusion Model), CMD (In the light of feature distributions), and DSTN (Domain-Aware Universal Style Transfer).",
                "[30], proposed a novel approach for arbitrary style transfer that matches the distributions of features between the content and style images.",
                "Compared to EFDM, our method better preserves the content components of the content image, ensuring that the fine-grained details and structural information are maintained throughout the style transfer process.",
                "3 shows the various content and style images (columns 1,2 respectively), our approach (column 3), CMD (column 4), EFDM (column 5) and DSTN (column 6).",
                "(a) Content (b) Style (c) DEPM (Ours) (d) CMD [13] (e) EFDM [30] (f) DSTN [8]"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a3126ebce359c7fb0866d2fb8fc78c11096d32ac",
                "externalIds": {
                    "DBLP": "conf/cvpr/HamazaspyanN23",
                    "DOI": "10.1109/CVPRW59228.2023.00087",
                    "CorpusId": 260872589
                },
                "corpusId": 260872589,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a3126ebce359c7fb0866d2fb8fc78c11096d32ac",
                "title": "Diffusion-Enhanced PatchMatch: A Framework for Arbitrary Style Transfer with Diffusion Models",
                "abstract": "Diffusion models have gained immense popularity in recent years due to their impressive ability to generate high-quality images. The opportunities that diffusion models provide are numerous, from text-to-image synthesis to image restoration and enhancement, as well as image compression and inpainting. However, expressing image style in words can be a challenging task, making it difficult for diffusion models to generate images with specific style without additional optimization techniques. In this paper, we present a novel method, Diffusion-Enhanced PatchMatch (DEPM), that leverages Stable Diffusion for style transfer without any finetuning or pretraining. DEPM captures high-level style features while preserving the fine-grained texture details of the original image. By enabling the transfer of arbitrary styles during inference, our approach makes the process more flexible and efficient. Moreover, its optimization-free nature makes it accessible to a wide range of users.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2231511531",
                        "name": "Mark Hamazaspyan"
                    },
                    {
                        "authorId": "2042485075",
                        "name": "Shant Navasardyan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "By contrast, the perturbation can be performed in the feature space [25,9,22].",
                "Subsequently, more research efforts have been devoted to designing the search space that covers a larger area in the feature-style space [9,22].",
                "We used the same segmentation network and loss function to compare our TriD with seven DG methods, including (1) DCAC: dynamic structure [7], (2) SAN-SAW: based on normalization and whitening [16], (3) RandConv: input-space domain randomization [21], (4-6) MixStyle, EFDM, DSU: feature-space domain randomization [25,22,9] and (7) MaxStyle: adversarial noise [3]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "10a507500015378bcb942a7d2387235873f2e796",
                "externalIds": {
                    "ArXiv": "2305.19949",
                    "DBLP": "conf/miccai/ChenPYCX23",
                    "DOI": "10.48550/arXiv.2305.19949",
                    "CorpusId": 258987730
                },
                "corpusId": 258987730,
                "publicationVenue": {
                    "id": "61a709e3-2060-423c-8de5-ffd3885aa31c",
                    "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                    "type": "conference",
                    "alternate_names": [
                        "Medical Image Computing and Computer-Assisted Intervention",
                        "MICCAI",
                        "Med Image Comput Comput Interv",
                        "Int Conf Med Image Comput Comput Interv"
                    ],
                    "url": "http://www.miccai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/10a507500015378bcb942a7d2387235873f2e796",
                "title": "Treasure in Distribution: A Domain Randomization based Multi-Source Domain Generalization for 2D Medical Image Segmentation",
                "abstract": "Although recent years have witnessed the great success of convolutional neural networks (CNNs) in medical image segmentation, the domain shift issue caused by the highly variable image quality of medical images hinders the deployment of CNNs in real-world clinical applications. Domain generalization (DG) methods aim to address this issue by training a robust model on the source domain, which has a strong generalization ability. Previously, many DG methods based on feature-space domain randomization have been proposed, which, however, suffer from the limited and unordered search space of feature styles. In this paper, we propose a multi-source DG method called Treasure in Distribution (TriD), which constructs an unprecedented search space to obtain the model with strong robustness by randomly sampling from a uniform distribution. To learn the domain-invariant representations explicitly, we further devise a style-mixing strategy in our TriD, which mixes the feature styles by randomly mixing the augmented and original statistics along the channel wise and can be extended to other DG methods. Extensive experiments on two medical segmentation tasks with different modalities demonstrate that our TriD achieves superior generalization performance on unseen target-domain data. Code is available at https://github.com/Chen-Ziyang/TriD.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2184547815",
                        "name": "Ziyang Chen"
                    },
                    {
                        "authorId": "39324077",
                        "name": "Yongsheng Pan"
                    },
                    {
                        "authorId": "2170220899",
                        "name": "Yiwen Ye"
                    },
                    {
                        "authorId": "145319486",
                        "name": "Hengfei Cui"
                    },
                    {
                        "authorId": "2111026901",
                        "name": "Yong Xia"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "125daa57d30d9f4ed67b086ce851f43b9d4fbc96",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-07261",
                    "ArXiv": "2304.07261",
                    "DOI": "10.48550/arXiv.2304.07261",
                    "CorpusId": 258170432
                },
                "corpusId": 258170432,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/125daa57d30d9f4ed67b086ce851f43b9d4fbc96",
                "title": "Frequency Decomposition to Tap the Potential of Single Domain for Generalization",
                "abstract": "Domain generalization (DG), aiming at models able to work on multiple unseen domains, is a must-have characteristic of general artificial intelligence. DG based on single source domain training data is more challenging due to the lack of comparable information to help identify domain invariant features. In this paper, it is determined that the domain invariant features could be contained in the single source domain training samples, then the task is to find proper ways to extract such domain invariant features from the single source domain samples. An assumption is made that the domain invariant features are closely related to the frequency. Then, a new method that learns through multiple frequency domains is proposed. The key idea is, dividing the frequency domain of each original image into multiple subdomains, and learning features in the subdomain by a designed two branches network. In this way, the model is enforced to learn features from more samples of the specifically limited spectrum, which increases the possibility of obtaining the domain invariant features that might have previously been defiladed by easily learned features. Extensive experimental investigation reveals that 1) frequency decomposition can help the model learn features that are difficult to learn. 2) the proposed method outperforms the state-of-the-art methods of single-source domain generalization.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2149536435",
                        "name": "Qing-sheng Yang"
                    },
                    {
                        "authorId": "6030884",
                        "name": "Hongjing Niu"
                    },
                    {
                        "authorId": "2047529231",
                        "name": "Pengfei Xia"
                    },
                    {
                        "authorId": "39771572",
                        "name": "W. Zhang"
                    },
                    {
                        "authorId": "2156071890",
                        "name": "Bin Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "domain tasks [15, 43, 42, 5], which involves training a robust model capable of generalizing well to any unseen domains.",
                "To address the diversity of training samples, some augmentation schemes have been proposed to enrich training data [15, 43].",
                "For example, compared with recent methods such as EFDMix [43] and XDED [19], using PBN can increase their performance by +2.9% (57.3 vs. 54.4) and +2.3% (68.8 vs. 66.5), respectively, demonstrating the effectiveness of our proposed PBN.",
                "We employ OSNet [46] as the backbone and follow the experiment protocol of [43].",
                "For example, when integrating our PBN into EFDMix [43], it can still increase +2.",
                "Implementation Details: For the PACS dataset, we follow the same setting as in [19, 43], where we use ResNet-18 as the backbone.",
                "We compare our method with several state-of-the-art (SOTA) methods, including RSC [15], ADA [34], MEADA [44], EFDMix [43] and XDED [19], and integrate our PBN into these methods by replacing BN of the backbone.",
                "For example, when integrating our PBN into EFDMix [43], it can still increase +2.8% (64.0 vs. 61.2).",
                "For example, compared with recent methods such as EFDMix [43] and XDED [19], using PBN can increase their performance by +2.",
                "Moreover, some multi-source domain generalization methods, such as Exact Feature Distribution Matching [43] and Cross-Domain Ensemble Distillation [19], can also be applied to the single-domain task."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "cf428ebc75fa3309b76b758d4efa001a370e1628",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-02848",
                    "ArXiv": "2304.02848",
                    "DOI": "10.48550/arXiv.2304.02848",
                    "CorpusId": 257985079
                },
                "corpusId": 257985079,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cf428ebc75fa3309b76b758d4efa001a370e1628",
                "title": "Patch-aware Batch Normalization for Improving Cross-domain Robustness",
                "abstract": "Despite the significant success of deep learning in computer vision tasks, cross-domain tasks still present a challenge in which the model's performance will degrade when the training set and the test set follow different distributions. Most existing methods employ adversarial learning or instance normalization for achieving data augmentation to solve this task. In contrast, considering that the batch normalization (BN) layer may not be robust for unseen domains and there exist the differences between local patches of an image, we propose a novel method called patch-aware batch normalization (PBN). To be specific, we first split feature maps of a batch into non-overlapping patches along the spatial dimension, and then independently normalize each patch to jointly optimize the shared BN parameter at each iteration. By exploiting the differences between local patches of an image, our proposed PBN can effectively enhance the robustness of the model's parameters. Besides, considering the statistics from each patch may be inaccurate due to their smaller size compared to the global feature maps, we incorporate the globally accumulated statistics with the statistics from each batch to obtain the final statistics for normalizing each patch. Since the proposed PBN can replace the typical BN, it can be integrated into most existing state-of-the-art methods. Extensive experiments and analysis demonstrate the effectiveness of our PBN in multiple computer vision tasks, including classification, object detection, instance retrieval, and semantic segmentation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1785352346",
                        "name": "Lei Qi"
                    },
                    {
                        "authorId": "2213838823",
                        "name": "Dongjia Zhao"
                    },
                    {
                        "authorId": "2475959",
                        "name": "Yinghuan Shi"
                    },
                    {
                        "authorId": "1735299",
                        "name": "Xin Geng"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7e9bc1956a1eebc7550491523c0ebf797edcc44e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-00414",
                    "ArXiv": "2304.00414",
                    "DOI": "10.1109/CVPR52729.2023.00972",
                    "CorpusId": 257913465
                },
                "corpusId": 257913465,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7e9bc1956a1eebc7550491523c0ebf797edcc44e",
                "title": "Learning Dynamic Style Kernels for Artistic Style Transfer",
                "abstract": "Arbitrary style transfer has been demonstrated to be efficient in artistic image generation. Previous methods either globally modulate the content feature ignoring local details, or overly focus on the local structure details leading to style leakage. In contrast to the literature, we propose a new scheme \u201cstyle kernel\u201d that learns spatially adaptive kernels for per-pixel stylization, where the convolutional kernels are dynamically generated from the global style-content aligned feature and then the learned kernels are applied to modulate the content feature at each spatial position. This new scheme allows flexible both global and local interactions between the content and style features such that the wanted styles can be easily transferred to the content image while at the same time the content structure can be easily preserved. To further enhance the flexibility of our style transfer method, we propose a Style Alignment Encoding (SAE) module complemented with a Content-based Gating Modulation (CGM) module for learning the dynamic style kernels in focusing regions. Extensive experiments strongly demonstrate that our proposed method outperforms state-of-the-art methods and exhibits superior performance in terms of visual quality and efficiency.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51496098",
                        "name": "Wenju Xu"
                    },
                    {
                        "authorId": "48015811",
                        "name": "Chengjiang Long"
                    },
                    {
                        "authorId": "37221211",
                        "name": "Yongwei Nie"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3e90e30b2e8c11c4c459b32fbbe0cd0dbe95d2c3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-13845",
                    "ArXiv": "2303.13845",
                    "DOI": "10.48550/arXiv.2303.13845",
                    "CorpusId": 257757373
                },
                "corpusId": 257757373,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3e90e30b2e8c11c4c459b32fbbe0cd0dbe95d2c3",
                "title": "Anomaly Detection under Distribution Shift",
                "abstract": "Anomaly detection (AD) is a crucial machine learning task that aims to learn patterns from a set of normal training samples to identify abnormal samples in test data. Most existing AD studies assume that the training and test data are drawn from the same data distribution, but the test data can have large distribution shifts arising in many real-world applications due to different natural variations such as new lighting conditions, object poses, or background appearances, rendering existing AD methods ineffective in such cases. In this paper, we consider the problem of anomaly detection under distribution shift and establish performance benchmarks on four widely-used AD and out-of-distribution (OOD) generalization datasets. We demonstrate that simple adaptation of state-of-the-art OOD generalization methods to AD settings fails to work effectively due to the lack of labeled anomaly data. We further introduce a novel robust AD approach to diverse distribution shifts by minimizing the distribution gap between in-distribution and OOD normal samples in both the training and inference stages in an unsupervised way. Our extensive empirical results on the four datasets show that our approach substantially outperforms state-of-the-art AD methods and OOD generalization methods on data with various distribution shifts, while maintaining the detection accuracy on in-distribution data. Code and data are available at https://github.com/mala-lab/ADShift.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2149281913",
                        "name": "T. Cao"
                    },
                    {
                        "authorId": "2109445463",
                        "name": "Jiawen Zhu"
                    },
                    {
                        "authorId": "3224619",
                        "name": "Guansong Pang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "887f0451d95fd082cd662de564209f2f99ae3baa",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-13232",
                    "ArXiv": "2303.13232",
                    "DOI": "10.1109/CVPR52729.2023.01984",
                    "CorpusId": 257687658
                },
                "corpusId": 257687658,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/887f0451d95fd082cd662de564209f2f99ae3baa",
                "title": "Transforming Radiance Field with Lipschitz Network for Photorealistic 3D Scene Stylization",
                "abstract": "Recent advances in 3D scene representation and novel view synthesis have witnessed the rise of Neural Radiance Fields (NeRFs). Nevertheless, it is not trivial to exploit NeRF for the photorealistic 3D scene stylization task, which aims to generate visually consistent and photorealistic stylized scenes from novel views. Simply coupling NeRF with photorealistic style transfer (PST) will result in cross-view inconsistency and degradation of stylized view syntheses. Through a thorough analysis, we demonstrate that this non-trivial task can be simplified in a new light: When transforming the appearance representation of a pre-trained NeRF with Lipschitz mapping, the consistency and photorealism across source views will be seamlessly encoded into the syntheses. That motivates us to build a concise and flexible learning framework namely LipRF, which upgrades arbitrary 2D PST methods with Lipschitz mapping tailored for the 3D scene. Technically, LipRF first pre-trains a radiance field to reconstruct the 3D scene, and then emulates the style on each view by 2D PST as the prior to learn a Lipschitz network to stylize the pre-trained appearance. In view of that Lipschitz condition highly impacts the expressivity of the neural network, we devise an adaptive regularization to balance the reconstruction and stylization. A gradual gradient aggregation strategy is further introduced to optimize LipRF in a cost-efficient manner. We conduct extensive experiments to show the high quality and robust performance of LipRF on both photorealistic 3D stylization and object appearance editing.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2116459218",
                        "name": "Zicheng Zhang"
                    },
                    {
                        "authorId": "1604967498",
                        "name": "Yinglu Liu"
                    },
                    {
                        "authorId": "1881723",
                        "name": "Congying Han"
                    },
                    {
                        "authorId": "3202968",
                        "name": "Yingwei Pan"
                    },
                    {
                        "authorId": "7390729",
                        "name": "Tiande Guo"
                    },
                    {
                        "authorId": "145690248",
                        "name": "Ting Yao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f429439dc3b803ac6187859aeac339fd8911a1ea",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-11674",
                    "ArXiv": "2303.11674",
                    "DOI": "10.1109/CVPR52729.2023.02311",
                    "CorpusId": 257637058
                },
                "corpusId": 257637058,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f429439dc3b803ac6187859aeac339fd8911a1ea",
                "title": "ALOFT: A Lightweight MLP-Like Architecture with Dynamic Low-Frequency Transform for Domain Generalization",
                "abstract": "Domain generalization (DG) aims to learn a model that generalizes well to unseen target domains utilizing multiple source domains without re-training. Most existing DG works are based on convolutional neural networks (CNNs). However, the local operation of the convolution kernel makes the model focus too much on local representations (e.g., texture), which inherently causes the model more prone to overfit to the source domains and hampers its generalization ability. Recently, several MLP-based methods have achieved promising results in supervised learning tasks by learning global interactions among different patches of the image. Inspired by this, in this paper, we first analyze the difference between CNN and MLP methods in DG and find that MLP methods exhibit a better generalization ability because they can better capture the global representations (e.g., structure) than CNN methods. Then, based on a recent lightweight MLP method, we obtain a strong baseline that outperforms most state-of-the-art CNN-based methods. The baseline can learn global structure representations with a filter to suppress structureirrelevant information in the frequency space. Moreover, we propose a dynAmic LOw-Frequency spectrum Transform (ALOFT) that can perturb local texture features while preserving global structure features, thus enabling the filter to remove structure-irrelevant information sufficiently. Extensive experiments on four benchmarks have demonstrated that our method can achieve great performance improvement with a small number of parameters compared to SOTA CNN-based DG methods. Our code is available at https://github.com/lingeringlight/ALOFT/.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50115584",
                        "name": "Jintao Guo"
                    },
                    {
                        "authorId": "2212174035",
                        "name": "Na Wang"
                    },
                    {
                        "authorId": "1785352346",
                        "name": "Lei Qi"
                    },
                    {
                        "authorId": "2475959",
                        "name": "Yinghuan Shi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "077d32bbf68aa784ff2cdbdffafdd5483b4ea2ed",
                "externalIds": {
                    "ArXiv": "2303.12710",
                    "DBLP": "journals/corr/abs-2303-12710",
                    "DOI": "10.1145/3605548",
                    "CorpusId": 257663390
                },
                "corpusId": 257663390,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/077d32bbf68aa784ff2cdbdffafdd5483b4ea2ed",
                "title": "A Unified Arbitrary Style Transfer Framework via Adaptive Contrastive Learning",
                "abstract": "This work presents Unified Contrastive Arbitrary Style Transfer (UCAST), a novel style representation learning and transfer framework, that can fit in most existing arbitrary image style transfer models, such as CNN-based, ViT-based, and flow-based methods. As the key component in image style transfer tasks, a suitable style representation is essential to achieve satisfactory results. Existing approaches based on deep neural networks typically use second-order statistics to generate the output. However, these hand-crafted features computed from a single image cannot leverage style information sufficiently, which leads to artifacts such as local distortions and style inconsistency. To address these issues, we learn style representation directly from a large number of images based on contrastive learning by considering the relationships between specific styles and the holistic style distribution. Specifically, we present an adaptive contrastive learning scheme for style transfer by introducing an input-dependent temperature. Our framework consists of three key components: a parallel contrastive learning scheme for style representation and transfer, a domain enhancement (DE) module for effective learning of style distribution, and a generative network for style transfer. Qualitative and quantitative evaluations show the results of our approach are superior to those obtained via state-of-the-art methods. The code is available at https://github.com/zyxElsa/CAST_pytorch.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108078624",
                        "name": "Yu-xin Zhang"
                    },
                    {
                        "authorId": "1443761295",
                        "name": "Fan Tang"
                    },
                    {
                        "authorId": "40441149",
                        "name": "Weiming Dong"
                    },
                    {
                        "authorId": "3119608",
                        "name": "Haibin Huang"
                    },
                    {
                        "authorId": "151487472",
                        "name": "Chongyang Ma"
                    },
                    {
                        "authorId": "39945992",
                        "name": "Tong-Yee Lee"
                    },
                    {
                        "authorId": "2155590336",
                        "name": "Changsheng Xu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f37fae71760834924f287b71ad8f7bbd026ee95b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-01904",
                    "ArXiv": "2303.01904",
                    "DOI": "10.1109/CVPR52729.2023.01147",
                    "CorpusId": 257353304
                },
                "corpusId": 257353304,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f37fae71760834924f287b71ad8f7bbd026ee95b",
                "title": "EcoTTA: Memory-Efficient Continual Test-Time Adaptation via Self-Distilled Regularization",
                "abstract": "This paper presents a simple yet effective approach that improves continual test-time adaptation (TTA) in a memory-efficient manner. TTA may primarily be conducted on edge devices with limited memory, so reducing memory is crucial but has been overlooked in previous TTA studies. In addition, long-term adaptation often leads to catastrophic forgetting and error accumulation, which hinders applying TTA in real-world deployments. Our approach consists of two components to address these issues. First, we present lightweight meta networks that can adapt the frozen original networks to the target domain. This novel architecture minimizes memory consumption by decreasing the size of intermediate activations required for backpropagation. Second, our novel self-distilled regularization controls the output of the meta networks not to deviate significantly from the output of the frozen original networks, thereby preserving well-trained knowledge from the source domain. Without additional memory, this regularization prevents error accumulation and catastrophic forgetting, resulting in stable performance even in long-term test-time adaptation. We demonstrate that our simple yet effective strategy outperforms other state-of-the-art methods on various benchmarks for image classification and semantic segmentation tasks. Notably, our proposed method with ResNet-50 and WideResNet-40 takes 86% and 80% less memory than the recent state-of-the-art method, CoTTA.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144578615",
                        "name": "Jun S. Song"
                    },
                    {
                        "authorId": "2108472587",
                        "name": "Jungsoo Lee"
                    },
                    {
                        "authorId": "145017151",
                        "name": "In-So Kweon"
                    },
                    {
                        "authorId": "48919377",
                        "name": "Sungha Choi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To solve this problem, researchers have proposed two solutions, namely domain adaptation [21, 25, 28, 31] and domain generalization [3, 16, 38, 40]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8b34751ee96233d9fe23687d1da52c3e5af14da9",
                "externalIds": {
                    "DBLP": "journals/mta/TanT23",
                    "DOI": "10.1007/s11042-023-14397-y",
                    "CorpusId": 257237717
                },
                "corpusId": 257237717,
                "publicationVenue": {
                    "id": "477368e9-7a8e-475a-8c93-6d623797fd06",
                    "name": "Multimedia tools and applications",
                    "type": "journal",
                    "alternate_names": [
                        "Multimedia Tools and Applications",
                        "Multimedia Tool Appl",
                        "Multimedia tool appl"
                    ],
                    "issn": "1380-7501",
                    "url": "https://www.springer.com/computer/information+systems+and+applications/journal/11042",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11042"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8b34751ee96233d9fe23687d1da52c3e5af14da9",
                "title": "Improving generalization of image recognition with multi-branch generation network and contrastive learning",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2440954",
                        "name": "Zhiyuan Simon Tan"
                    },
                    {
                        "authorId": "2210125148",
                        "name": "Zhaofei Teng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The results indicate that improvement of the proposed AGFA over the existing DG methods is even more pronounced: averaged accuracies higher than the best prior method EFDMIX (Zhang et al., 2022) by about 10% for ResNet-18 and by about 7% for ResNet-50.",
                "Compared to the recent approaches MixStyle (Zhou et al., 2021b) and EFDMix (Zhang et al., 2022), our approach AGFA again shows higher performance even with the smaller ResNet-18 backbone."
            ],
            "intents": [
                "result"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8b2b1bd8601d61b5c5a1ea04f7f4ca674a4fe6b0",
                "externalIds": {
                    "DBLP": "conf/iclr/Kim0H23",
                    "ArXiv": "2302.12047",
                    "DOI": "10.48550/arXiv.2302.12047",
                    "CorpusId": 257102785
                },
                "corpusId": 257102785,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8b2b1bd8601d61b5c5a1ea04f7f4ca674a4fe6b0",
                "title": "Domain Generalisation via Domain Adaptation: An Adversarial Fourier Amplitude Approach",
                "abstract": "We tackle the domain generalisation (DG) problem by posing it as a domain adaptation (DA) task where we adversarially synthesise the worst-case target domain and adapt a model to that worst-case domain, thereby improving the model's robustness. To synthesise data that is challenging yet semantics-preserving, we generate Fourier amplitude images and combine them with source domain phase images, exploiting the widely believed conjecture from signal processing that amplitude spectra mainly determines image style, while phase data mainly captures image semantics. To synthesise a worst-case domain for adaptation, we train the classifier and the amplitude generator adversarially. Specifically, we exploit the maximum classifier discrepancy (MCD) principle from DA that relates the target domain performance to the discrepancy of classifiers in the model hypothesis space. By Bayesian hypothesis modeling, we express the model hypothesis space effectively as a posterior distribution over classifiers given the source domains, making adversarial MCD minimisation feasible. On the DomainBed benchmark including the large-scale DomainNet dataset, the proposed approach yields significantly improved domain generalisation performance over the state-of-the-art.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2918263",
                        "name": "Minyoung Kim"
                    },
                    {
                        "authorId": "2108338672",
                        "name": "Da Li"
                    },
                    {
                        "authorId": "1697755",
                        "name": "Timothy M. Hospedales"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Inspired by this observation, researchers have proposed to perform feature statistics perturbation to introduce style-augmented samples (Nuriel, Benaim, and Wolf 2021; Zhou et al. 2021c; Zhang et al. 2022; Li et al. 2022; Chen et al. 2022; Zhong et al. 2022).",
                "Our method adopts the data augmentation strategy, more specifically, feature-based augmentation (Nuriel, Benaim, and Wolf 2021; Zhou et al. 2021c; Li et al. 2022; Zhang et al. 2022).",
                "We closely follow (Zhou et al. 2021c; Zhang et al. 2022) to perform the cross-domain instance retrieval task on person re-identification (re-ID) datasets of Market1501 (Zheng et al. 2015) and Duke (Ristani et al. 2016; Zheng, Zheng, and Yang 2017).",
                "More importantly, our AdvStyle significantly outperforms other style augmentation competitors (Zhou et al. 2021c; Li et al. 2022; Zhang et al. 2022), validating the effectiveness of expanding the style space via adversarial training.",
                "Recently, performing style augmentation in feature space by conducting feature statistics perturbation has attracted increasing attention due to its simplicity and efficacy (Nuriel, Benaim, and Wolf 2021; Zhou et al. 2021c; Zhang et al. 2022; Li et al. 2022).",
                "Compared to the common augmentation strategies (Zhong et al.; Ghiasi, Lin, and Le 2018), the style augmentation methods (Zhou et al. 2021c; Li et al. 2022; Zhang et al. 2022) present clear advantages.",
                "Different from most data augmentation methods that introduce augmented samples in the image space (Zhang et al. 2020; Luo et al. 2020), we generate augmented samples in the feature space, which is more computationally efficient.",
                "Besides the first and second order statistics used in (Nuriel, Benaim, and Wolf 2021; Zhou et al. 2021c), Zhang et al. (Zhang et al. 2022) implicitly considered high-order statistics for more effective statistics perturbation.",
                "The adversarial training strategy was later adopted to align feature distributions in domain adaptation (Ganin and Lempitsky 2015; Ganin et al. 2016; Long et al. 2018), generative photo-realistic super-resolution (Ledig et al. 2017; Wang et al. 2018), data augmentation (Zhang et al. 2020; Volpi et al. 2018a; Luo et al. 2020), and so on.",
                "Based on such observation, researchers started to introduce style/distribution augmented training samples by feature statistics perturbation into DG model training (Nuriel, Benaim, and Wolf 2021; Zhou et al. 2021c; Li et al. 2022; Zhang et al. 2022).",
                "Particularly, we perform existing style augmentation-based DG methods (Nuriel, Benaim, and Wolf 2021; Zhou et al. 2021c; Li et al. 2022; Zhang et al. 2022) under the same setting for fair comparison.",
                "By expanding the training data with these style-augmented samples, improved generalization performance of DNN models has been observed (Nuriel, Benaim, and Wolf 2021; Zhou et al. 2021c; Zhang et al. 2022; Li et al. 2022).",
                "Our method also outperforms the recent work (Zhang et al. 2022) that explores broader style spaces by utilizing high-order batch statistics, revealing the advantage of exploring style spaces beyond batch statistics."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "837cf3fdf334129f7d64ec5a4903e51ea96e90f8",
                "externalIds": {
                    "ArXiv": "2301.12643",
                    "DBLP": "journals/corr/abs-2301-12643",
                    "DOI": "10.48550/arXiv.2301.12643",
                    "CorpusId": 256389872
                },
                "corpusId": 256389872,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/837cf3fdf334129f7d64ec5a4903e51ea96e90f8",
                "title": "Adversarial Style Augmentation for Domain Generalization",
                "abstract": "It is well-known that the performance of well-trained deep neural networks may degrade significantly when they are applied to data with even slightly shifted distributions. Recent studies have shown that introducing certain perturbation on feature statistics (\\eg, mean and standard deviation) during training can enhance the cross-domain generalization ability. Existing methods typically conduct such perturbation by utilizing the feature statistics within a mini-batch, limiting their representation capability. Inspired by the domain generalization objective, we introduce a novel Adversarial Style Augmentation (ASA) method, which explores broader style spaces by generating more effective statistics perturbation via adversarial training. Specifically, we first search for the most sensitive direction and intensity for statistics perturbation by maximizing the task loss. By updating the model against the adversarial statistics perturbation during training, we allow the model to explore the worst-case domain and hence improve its generalization performance. To facilitate the application of ASA, we design a simple yet effective module, namely AdvStyle, which instantiates the ASA method in a plug-and-play manner. We justify the efficacy of AdvStyle on tasks of cross-domain classification and instance retrieval. It achieves higher mean accuracy and lower performance fluctuation. Especially, our method significantly outperforms its competitors on the PACS dataset under the single source generalization setting, \\eg, boosting the classification accuracy from 61.2\\% to 67.1\\% with a ResNet50 backbone. Our code will be available at \\url{https://github.com/YBZh/AdvStyle}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108151704",
                        "name": "Yabin Zhang"
                    },
                    {
                        "authorId": "2070545950",
                        "name": "Bin Deng"
                    },
                    {
                        "authorId": "151482572",
                        "name": "Ruihuang Li"
                    },
                    {
                        "authorId": "2370507",
                        "name": "K. Jia"
                    },
                    {
                        "authorId": "2152838991",
                        "name": "Lei Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026DG works are typically based on accurate supervision knowledge of the source domain data, whether it is drawn from a single domain (Wang et al., 2021d; Li et al., 2021) or multiple domains (Yao et al., 2022; Zhang et al., 2022), which may not be achievable in continually changing scenarios.",
                "RaTP is compared with a comprehensive set of state-of-the-art works from Continual DA [CoTTA (Wang et al., 2022), AuCID (Rostami, 2021)], Source-Free DA [SHOT (Liang et al., 2020), GSFDA (Yang et al., 2021), BMD (Qu et al., 2022)], Test-Time/Online DA [Tent (Wang et al., 2020), T3A (Iwasawa & Matsuo, 2021)], Single DG [L2D (Wang et al., 2021d), PDEN (Li et al., 2021)], Unified DA&DG [SNR (Jin et al., 2021)], and Multiple DG [PCL (Yao et al., 2022), EFDM (Zhang et al., 2022)].",
                "For Single DG [L2D (Wang et al., 2021d), PDEN (Li et al., 2021)] and Multiple DG [PCL (Yao et al., 2022), EFDM (Zhang et al., 2022)], we use SHOT (Liang et al., 2020) to assign pseudo labels for the optimization on target domains.",
                "\u2026GSFDA (Yang et al., 2021), BMD (Qu et al., 2022)], Test-Time/Online DA [Tent (Wang et al., 2020), T3A (Iwasawa & Matsuo, 2021)], Single DG [L2D (Wang et al., 2021d), PDEN (Li et al., 2021)], Unified DA&DG [SNR (Jin et al., 2021)], and Multiple DG [PCL (Yao et al., 2022), EFDM (Zhang et al., 2022)].",
                "In EFDM, we use samples from current domain as the content images and randomly select images in the replay memory as the style images.",
                "According to the composition of the given source domain, DG can be divided into two types, Single Source (Wang et al., 2021d; Li et al., 2021) and Multi-Source (Yao et al., 2022; Zhang et al., 2022).",
                ", 2021) or multiple domains (Yao et al., 2022; Zhang et al., 2022), which may not be achievable in continually changing scenarios."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1432133bf51be0bcd42676ca916143e321e101f0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-10418",
                    "ArXiv": "2301.10418",
                    "DOI": "10.48550/arXiv.2301.10418",
                    "CorpusId": 256231061
                },
                "corpusId": 256231061,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1432133bf51be0bcd42676ca916143e321e101f0",
                "title": "DEJA VU: Continual Model Generalization For Unseen Domains",
                "abstract": "In real-world applications, deep learning models often run in non-stationary environments where the target data distribution continually shifts over time. There have been numerous domain adaptation (DA) methods in both online and offline modes to improve cross-domain adaptation ability. However, these DA methods typically only provide good performance after a long period of adaptation, and perform poorly on new domains before and during adaptation - in what we call the\"Unfamiliar Period\", especially when domain shifts happen suddenly and significantly. On the other hand, domain generalization (DG) methods have been proposed to improve the model generalization ability on unadapted domains. However, existing DG works are ineffective for continually changing domains due to severe catastrophic forgetting of learned knowledge. To overcome these limitations of DA and DG in handling the Unfamiliar Period during continual domain shift, we propose RaTP, a framework that focuses on improving models' target domain generalization (TDG) capability, while also achieving effective target domain adaptation (TDA) capability right after training on certain domains and forgetting alleviation (FA) capability on past domains. RaTP includes a training-free data augmentation module to prepare data for TDG, a novel pseudo-labeling mechanism to provide reliable supervision for TDA, and a prototype contrastive alignment algorithm to align different domains for achieving TDG, TDA and FA. Extensive experiments on Digits, PACS, and DomainNet demonstrate that RaTP significantly outperforms state-of-the-art works from Continual DA, Source-Free DA, Test-Time/Online DA, Single DG, Multiple DG and Unified DA&DG in TDG, and achieves comparable TDA and FA capabilities.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2116898205",
                        "name": "Chenxi Liu"
                    },
                    {
                        "authorId": "2108631414",
                        "name": "Lixu Wang"
                    },
                    {
                        "authorId": "145225199",
                        "name": "Lingjuan Lyu"
                    },
                    {
                        "authorId": "2118831761",
                        "name": "Chen Sun"
                    },
                    {
                        "authorId": "144129720",
                        "name": "Xiao Wang"
                    },
                    {
                        "authorId": "2152206866",
                        "name": "Qi Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "EFDM [22] implicitly performs the exact histogram matching on features to achieve style transfer.",
                "Some recent methods [7], [21], [22] utilize feature statistics to tackle the domain generalization problem."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6a7d81ada67f68cbbd94320eb80eb2e726eb886e",
                "externalIds": {
                    "ArXiv": "2301.06442",
                    "DBLP": "journals/corr/abs-2301-06442",
                    "DOI": "10.48550/arXiv.2301.06442",
                    "CorpusId": 255942180
                },
                "corpusId": 255942180,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6a7d81ada67f68cbbd94320eb80eb2e726eb886e",
                "title": "Modeling Uncertain Feature Representation for Domain Generalization",
                "abstract": "Though deep neural networks have achieved impressive success on various vision tasks, obvious performance degradation still exists when models are tested in out-of-distribution scenarios. In addressing this limitation, we ponder that the feature statistics (mean and standard deviation), which carry the domain characteristics of the training data, can be properly manipulated to improve the generalization ability of deep learning models. Existing methods commonly consider feature statistics as deterministic values measured from the learned features and do not explicitly model the uncertain statistics discrepancy caused by potential domain shifts during testing. In this paper, we improve the network generalization ability by modeling domain shifts with uncertainty (DSU), i.e., characterizing the feature statistics as uncertain distributions during training. Specifically, we hypothesize that the feature statistic, after considering the potential uncertainties, follows a multivariate Gaussian distribution. During inference, we propose an instance-wise adaptation strategy that can adaptively deal with the unforeseeable shift and further enhance the generalization ability of the trained model with negligible additional cost. We also conduct theoretical analysis on the aspects of generalization error bound and the implicit regularization effect, showing the efficacy of our method. Extensive experiments demonstrate that our method consistently improves the network generalization ability on multiple vision tasks, including image classification, semantic segmentation, instance retrieval, and pose estimation. Our methods are simple yet effective and can be readily integrated into networks without additional trainable parameters or loss constraints. Code will be released in https://github.com/lixiaotong97/DSU.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108689869",
                        "name": "Xiaotong Li"
                    },
                    {
                        "authorId": "1557412467",
                        "name": "Zixuan Hu"
                    },
                    {
                        "authorId": "9756930",
                        "name": "Jun Liu"
                    },
                    {
                        "authorId": "152988335",
                        "name": "Yixiao Ge"
                    },
                    {
                        "authorId": "153883459",
                        "name": "Yongxing Dai"
                    },
                    {
                        "authorId": "2055790840",
                        "name": "Ling-Yu Duan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several image classification works [13,35,45,48,56] have proposed strategies to improve the performance on unseen domains while training on a single source domain.",
                "While this has been well studied for image classification [13, 35, 45, 48, 56], it remains a nascent topic in object detection.",
                "In particular, [35,45,48] introduce data augmentation strategies where diverse input images are generated via adversarial training; [13, 56] propose normalization techniques to adapt the feature distribution to unseen domains."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9f0c857f24234f61282db2786cb8a2baeda9a7cd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-05499",
                    "ArXiv": "2301.05499",
                    "DOI": "10.1109/CVPR52729.2023.00314",
                    "CorpusId": 255825768
                },
                "corpusId": 255825768,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9f0c857f24234f61282db2786cb8a2baeda9a7cd",
                "title": "CLIP the Gap: A Single Domain Generalization Approach for Object Detection",
                "abstract": "Single Domain Generalization (SDG) tackles the problem of training a model on a single source domain so that it generalizes to any unseen target domain. While this has been well studied for image classification, the literature on SDG object detection remains almost non-existent. To address the challenges of simultaneously learning robust object localization and representation, we propose to leverage a pre-trained vision-language model to introduce semantic domain concepts via textual prompts. We achieve this via a semantic augmentation strategy acting on the features extracted by the detector backbone, as well as a text-based classification loss. Our experiments evidence the benefits of our approach, outperforming by 10% the only existing SDG object detection method, Single-DGOD [52], on their own diverse weather-driving benchmark.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2183082550",
                        "name": "Vidit Vidit"
                    },
                    {
                        "authorId": "35376394",
                        "name": "Martin Engilberge"
                    },
                    {
                        "authorId": "2862871",
                        "name": "M. Salzmann"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Input Reference Ours (\u03b1 = 0) Ours (\u03b1 = 1) AdaIN [5] WCT [8] LinearWCT [7] ArtFlow [1] EFDM [16] StyleSwap [2] Avatar-Net [13] SANet [11] AdaAttN [10] StyTR2 [3] Figure A.",
                "Input Reference Ours (\u03b1 = 0) Ours (\u03b1 = 1) AdaIN [5] WCT [8] LinearWCT [7] ArtFlow [1] EFDM [16] StyleSwap [2] Avatar-Net [13] SANet [11] AdaAttN [10] StyTR2 [3] Figure E.",
                "Input Reference Ours (\u03b1 = 0) Ours (\u03b1 = 1) AdaIN [5] WCT [8] LinearWCT [7] ArtFlow [1] EFDM [16] StyleSwap [2] Avatar-Net [13] SANet [11] AdaAttN [10] StyTR2 [3] Figure B.",
                "5 [16] Yabin Zhang, Minghan Li, Ruihuang Li, Kui Jia, and Lei Zhang.",
                "Input Reference Ours (\u03b1 = 0) Ours (\u03b1 = 1) AdaIN [5] WCT [8] LinearWCT [7] ArtFlow [1] EFDM [16] StyleSwap [2] Avatar-Net [13] SANet [11] AdaAttN [10] StyTR2 [3]"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "acc090b48431be4cb83d81bf94d955e2266f02af",
                "externalIds": {
                    "DBLP": "conf/cvpr/Huang00LP23",
                    "ArXiv": "2212.10431",
                    "DOI": "10.1109/CVPR52729.2023.00576",
                    "CorpusId": 254877300
                },
                "corpusId": 254877300,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/acc090b48431be4cb83d81bf94d955e2266f02af",
                "title": "QuantArt: Quantizing Image Style Transfer Towards High Visual Fidelity",
                "abstract": "The mechanism of existing style transfer algorithms is by minimizing a hybrid loss function to push the generated image toward high similarities in both content and style. However, this type of approach cannot guarantee visual fidelity, i.e., the generated artworks should be indistinguishable from real ones. In this paper, we devise a new style transfer framework called QunntArt for high visual-fidelity stylization. QuantArt pushes the latent representation of the generated artwork toward the centroids of the real artwork distribution with vector quantization. By fusing the quantized and continuous latent representations, QuantArt allows flexible control over the generated artworks in terms of content preservation, style similarity, and visual fidelity. Experiments on various style transfer settings show that our QuantArt framework achieves significantly higher visual fidelity compared with the existing style transfer methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48669017",
                        "name": "Siyu Huang"
                    },
                    {
                        "authorId": "1733982458",
                        "name": "Jie An"
                    },
                    {
                        "authorId": "1766333",
                        "name": "D. Wei"
                    },
                    {
                        "authorId": "2116782926",
                        "name": "Jiebo Luo"
                    },
                    {
                        "authorId": "143758236",
                        "name": "H. Pfister"
                    }
                ]
            }
        },
        {
            "contexts": [
                "9% [43] proposes to perform exact feature distribution matching in the image feature space and achieve one classification performance of 83."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4f8109ea85762a35080cfdbde61f6ba7379f40ae",
                "externalIds": {
                    "ArXiv": "2212.09950",
                    "DBLP": "journals/corr/abs-2212-09950",
                    "DOI": "10.48550/arXiv.2212.09950",
                    "CorpusId": 254877027
                },
                "corpusId": 254877027,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4f8109ea85762a35080cfdbde61f6ba7379f40ae",
                "title": "Domain Generalization with Correlated Style Uncertainty",
                "abstract": "Domain generalization (DG) approaches intend to extract domain invariant features that can lead to a more robust deep learning model. In this regard, style augmentation is a strong DG method taking advantage of instance-specific feature statistics containing informative style characteristics to synthetic novel domains. While it is one of the state-of-the-art methods, prior works on style augmentation have either disregarded the interdependence amongst distinct feature channels or have solely constrained style augmentation to linear interpolation. To address these research gaps, in this work, we introduce a novel augmentation approach, named Correlated Style Uncertainty (CSU), surpassing the limitations of linear interpolation in style statistic space and simultaneously preserving vital correlation information. Our method's efficacy is established through extensive experimentation on diverse cross-domain computer vision and medical imaging classification tasks: PACS, Office-Home, and Camelyon17 datasets, and the Duke-Market1501 instance retrieval task. The results showcase a remarkable improvement margin over existing state-of-the-art techniques. The source code is available https://github.com/freshman97/CSU.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2133195231",
                        "name": "Zheyu Zhang"
                    },
                    {
                        "authorId": "2152595129",
                        "name": "Bin Wang"
                    },
                    {
                        "authorId": "34665941",
                        "name": "Debesh Jha"
                    },
                    {
                        "authorId": "2935412",
                        "name": "Ugur Demir"
                    },
                    {
                        "authorId": "1717161",
                        "name": "Ulas Bagci"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As shown in Table 1, the LCT module outperformed the method without jump connection and the EFDM method with a higher SSIM score, which proved the effectiveness of the LCT module.",
                "Table 1: Quantitative comparison between LCT and other methods\nLCT EFDM Without skip\n0.612 0.255 0.578\nFigure 3: The effectiveness of GLFA\nFigure 4: The effectiveness of LCT\nAs shown in Figure 3, without the constraint of WCE, the local information of the style image cannot match the local information of the content image well.",
                "On the other hand, to prove the effectiveness of the LCT transform module, this article compares it with EFDM [36], a method that matches higher-order statistics of images.",
                "As shown in Figure 4, in the third and fourth columns of the first row, in the EFDM method, clouds in the sky show inconsistent stylization and inaccurate feature extraction ability (the lights of the building windows blur).",
                "In addition, the overall color of the style achieved by the EFDM method also differs from the original style image."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "0865c1f7fcc5866d2a5cb818ca530fe1ce3d0b58",
                "externalIds": {
                    "DBLP": "conf/icncc/JingJHWW22",
                    "DOI": "10.1145/3579895.3579909",
                    "CorpusId": 257927150
                },
                "corpusId": 257927150,
                "publicationVenue": {
                    "id": "12ed15a2-5218-48ca-a29f-e39af72dc504",
                    "name": "International Conference on Network, Communication and Computing",
                    "type": "conference",
                    "alternate_names": [
                        "ICNCC",
                        "International Conference Network, Communication and Computing",
                        "Int Conf Netw Commun Comput"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0865c1f7fcc5866d2a5cb818ca530fe1ce3d0b58",
                "title": "Global-Local Feature Alignment Loss for Photorealistic Style Transfer",
                "abstract": "The problem that needs to be solved for photorealistic style transfer lies in limiting the distortion of texture details of the generated image based on the typical style transfer network. Although the existing methods achieve better stylization results, they lack sufficient style information because they do not consider the feature map comprehensively enough, leading to exposure or artifacts. This article proposes a loss function based on the contrast learning method to constrain the network to extract local and global information effectively. It ensures the consistency of distribution among regional blocks generated based on anchor points and the consistency of comparison between anchor points of the resulting image and content image in their neighborhood. This ensures consistency between local and global information comparisons. To ensure that the network is simple and effective and that enough information is extracted, this article proposes a linear covariance transformation network to achieve faithful stylization by effectively fusing feature first-order statistics with second-order statistics. Experiments show that the proposed method can faithfully achieve realistic stylization and satisfying visual effects.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32739365",
                        "name": "Yuchao Jing"
                    },
                    {
                        "authorId": "46642850",
                        "name": "Guoquan Jiang"
                    },
                    {
                        "authorId": "1816039841",
                        "name": "Zhanqiang Huo"
                    },
                    {
                        "authorId": "2213110900",
                        "name": "Yijiang Wang"
                    },
                    {
                        "authorId": "2213851883",
                        "name": "Lili Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Holistic feature distribution matching [13, 24, 17, 47] and locality-aware feature matching [32, 12, 29, 46, 27] are two categories of existing approaches.",
                "In order to break through the theoretical and practical limitations of first-order and second-order statistics, high-order statistics are introduced in [17] and [47] to perform more exact distribution matching.",
                "Arbitrary style transfer [4, 13, 24, 32, 12, 22, 29, 42, 46, 19, 15, 7, 17, 27, 1, 40, 5, 6, 47, 48] has received increasing attention recently, depending on its advantage of using a single feed-forward neural model to transfer the style of an arbitrary image.",
                "There have been notable improvements in feature transformation modules [13, 32, 12, 29, 46, 7, 27, 17, 47], novel architectures [24, 31, 1, 40, 6], and practical objectives [19, 5, 48]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ab5f67fda257775fe166dda4573712ab24961ff0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-04105",
                    "ArXiv": "2212.04105",
                    "DOI": "10.48550/arXiv.2212.04105",
                    "CorpusId": 254408953
                },
                "corpusId": 254408953,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ab5f67fda257775fe166dda4573712ab24961ff0",
                "title": "All-to-key Attention for Arbitrary Style Transfer",
                "abstract": "Attention-based arbitrary style transfer studies have shown promising performance in synthesizing vivid local style details. They typically use the all-to-all attention mechanism -- each position of content features is fully matched to all positions of style features. However, all-to-all attention tends to generate distorted style patterns and has quadratic complexity, limiting the effectiveness and efficiency of arbitrary style transfer. In this paper, we propose a novel all-to-key attention mechanism -- each position of content features is matched to stable key positions of style features -- that is more in line with the characteristics of style transfer. Specifically, it integrates two newly proposed attention forms: distributed and progressive attention. Distributed attention assigns attention to key style representations that depict the style distribution of local regions; Progressive attention pays attention from coarse-grained regions to fine-grained key positions. The resultant module, dubbed StyA2K, shows extraordinary performance in preserving the semantic structure and rendering consistent style patterns. Qualitative and quantitative comparisons with state-of-the-art methods demonstrate the superior performance of our approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152186704",
                        "name": "Mingrui Zhu"
                    },
                    {
                        "authorId": "2189984763",
                        "name": "Xiao He"
                    },
                    {
                        "authorId": "144050305",
                        "name": "N. Wang"
                    },
                    {
                        "authorId": "2118777145",
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "authorId": "2164214077",
                        "name": "Xinbo Gao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u0303? with \ud835\udc46\ud835\udc37\ud835\udc64 using recently proposed work by Zhang [37] to get\ud835\udc47 (\ud835\udc52 \ud835\udc53 \ud835\udc51\ud835\udc5a)\n\ud835\udc37\ud835\udc64 (?",
                "higher order moments of ?\u0303? with SDw using recently proposed work by Zhang [37] to getT (e f dm) Dw (?\u0303?)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "932d7b95920fc91bb1c7fab5ed0213d04189f36d",
                "externalIds": {
                    "DBLP": "conf/icvgip/JambigiMC22",
                    "DOI": "10.1145/3571600.3571655",
                    "CorpusId": 258639800
                },
                "corpusId": 258639800,
                "publicationVenue": {
                    "id": "91e2dd1f-5b8e-4b17-aa1c-e099415b27e5",
                    "name": "Indian Conference on Computer Vision, Graphics & Image Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Indian Conf Comput Vis Graph  Image Process",
                        "Indian Conference on Computer Vision, Graphics and Image Processing",
                        "Indian Conf Comput Vis Graph Image Process",
                        "ICVGIP"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/932d7b95920fc91bb1c7fab5ed0213d04189f36d",
                "title": "G-PReDICT: Generalizable Person Re-ID using Domain Invariant Contrastive Techniques\u2731",
                "abstract": "Learning identity-aware, domain-invariant representations is crucial in solving domain generalizable person ReID (DG-ReID). Existing methods commonly use augmentation techniques either in feature space by mixing instance and batch normalization layers or in pixel space by adversarially generating pseudo domains. However, neither of these techniques guarantee identity preservation. Apart from increasing training data diversity, the augmented positive pairs also encode rich semantic relations which have not been fully explored. To address the above issues, we propose a novel framework for Generalizable Person Re-identification using Domain Invariant Contrastive Techniques (G-PReDICT). Specifically, we use simple yet effective perturbation strategies to hallucinate positive samples across domains by realistically modelling domain variations while preserving the target identities. We harness rich sample-sample relations between the hallucinated positive-negative pairs to learn domain-invariant representations using supervised contrastive learning. We also use a domain independent auxiliary task, i.e. attribute prediction to learn robust representations and introduce attribute annotations for two large scale public benchmarks i.e. CUHK-03 and MSMT17. Extensive experiments on standard benchmarks demonstrate the effectiveness of the proposed method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2139739053",
                        "name": "Chaitra Jambigi"
                    },
                    {
                        "authorId": "2138148082",
                        "name": "Umar Masud"
                    },
                    {
                        "authorId": "1429640900",
                        "name": "Anirban Chakraborty"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Among data augmentation methods, our method is related to MixStyle [71] and EFDMix [65] due to the use of style transfer (i.e., the AdaIN algorithm [15]) to create novel data samples.",
                "Among data augmentation methods, our method is related to MixStyle [71] and EFDMix [65] due to the use of style transfer (i.",
                "These methods have demonstrated their ability to not only transfer artistic styles but also effectively capture the semantic content of images [15, 58, 65], which we propose that they hold promise for fulfilling the front-door requirements.",
                "In addition, both MixStyle and EFDMix rely on statistical predictions, which are sensitive to domain shift [1, 32]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "fa2b08534993ab2c9448deecd718006b23f94650",
                "externalIds": {
                    "DBLP": "conf/kdd/NguyenDNDN23",
                    "ArXiv": "2212.03063",
                    "DOI": "10.1145/3580305.3599270",
                    "CorpusId": 259138417
                },
                "corpusId": 259138417,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/fa2b08534993ab2c9448deecd718006b23f94650",
                "title": "Causal Inference via Style Transfer for Out-of-distribution Generalisation",
                "abstract": "Out-of-distribution (OOD) generalisation aims to build a model that can generalise well on an unseen target domain using knowledge from multiple source domains. To this end, the model should seek the causal dependence between inputs and labels, which may be determined by the semantics of inputs and remain invariant across domains. However, statistical or non-causal methods often cannot capture this dependence and perform poorly due to not considering spurious correlations learnt from model training via unobserved confounders. A well-known existing causal inference method like back-door adjustment cannot be applied to remove spurious correlations as it requires the observation of confounders. In this paper, we propose a novel method that effectively deals with hidden confounders by successfully implementing front-door adjustment (FA). FA requires the choice of a mediator, which we regard as the semantic information of images that helps access the causal mechanism without the need for observing confounders. Further, we propose to estimate the combination of the mediator with other observed images in the front-door formula via style transfer algorithms. Our use of style transfer to estimate FA is novel and sensible for OOD generalisation, which we justify by extensive experimental results on widely used benchmark datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32125163",
                        "name": "Toan Q. Nguyen"
                    },
                    {
                        "authorId": "36072771",
                        "name": "Kien Do"
                    },
                    {
                        "authorId": "1779016",
                        "name": "D. Nguyen"
                    },
                    {
                        "authorId": "2159149299",
                        "name": "Bao Duong"
                    },
                    {
                        "authorId": "150322672",
                        "name": "T. Nguyen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ed345432a5179c998b0f81ab4a972199a5adf03b",
                "externalIds": {
                    "DBLP": "journals/eswa/ZhuQCX23",
                    "DOI": "10.2139/ssrn.4219027",
                    "CorpusId": 252319295
                },
                "corpusId": 252319295,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ed345432a5179c998b0f81ab4a972199a5adf03b",
                "title": "Emotional generative adversarial network for image emotion transfer",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110018837",
                        "name": "Siqi Zhu"
                    },
                    {
                        "authorId": "37178775",
                        "name": "Chunmei Qing"
                    },
                    {
                        "authorId": "2109063634",
                        "name": "Canqiang Chen"
                    },
                    {
                        "authorId": "9303726",
                        "name": "Xiangmin Xu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7b0183c518ebc2100569f1086fd6fedab8659d96",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-15313",
                    "ArXiv": "2211.15313",
                    "DOI": "10.48550/arXiv.2211.15313",
                    "CorpusId": 254044240
                },
                "corpusId": 254044240,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7b0183c518ebc2100569f1086fd6fedab8659d96",
                "title": "MicroAST: Towards Super-Fast Ultra-Resolution Arbitrary Style Transfer",
                "abstract": "Arbitrary style transfer (AST) transfers arbitrary artistic styles onto content images. Despite the recent rapid progress, existing AST methods are either incapable or too slow to run at ultra-resolutions (e.g., 4K) with limited resources, which heavily hinders their further applications. In this paper, we tackle this dilemma by learning a straightforward and lightweight model, dubbed MicroAST. The key insight is to completely abandon the use of cumbersome pre-trained Deep Convolutional Neural Networks (e.g., VGG) at inference. Instead, we design two micro encoders (content and style encoders) and one micro decoder for style transfer. The content encoder aims at extracting the main structure of the content image. The style encoder, coupled with a modulator, encodes the style image into learnable dual-modulation signals that modulate both intermediate features and convolutional filters of the decoder, thus injecting more sophisticated and flexible style signals to guide the stylizations. In addition, to boost the ability of the style encoder to extract more distinct and representative style signals, we also introduce a new style signal contrastive loss in our model. Compared to the state of the art, our MicroAST not only produces visually superior results but also is 5-73 times smaller and 6-18 times faster, for the first time enabling super-fast (about 0.5 seconds) AST at 4K ultra-resolutions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47196793",
                        "name": "Zhizhong Wang"
                    },
                    {
                        "authorId": "37310105",
                        "name": "Lei Zhao"
                    },
                    {
                        "authorId": null,
                        "name": "Zhiwen Zuo"
                    },
                    {
                        "authorId": "1384783474",
                        "name": "Ailin Li"
                    },
                    {
                        "authorId": "47666905",
                        "name": "Haibo Chen"
                    },
                    {
                        "authorId": "2034241827",
                        "name": "Wei Xing"
                    },
                    {
                        "authorId": "51260042",
                        "name": "Dongming Lu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fdf70e3275e7c6f6f5eff43a167b4812f68d7df6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-14594",
                    "ArXiv": "2211.14594",
                    "DOI": "10.48550/arXiv.2211.14594",
                    "CorpusId": 254044432
                },
                "corpusId": 254044432,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fdf70e3275e7c6f6f5eff43a167b4812f68d7df6",
                "title": "Direct-Effect Risk Minimization for Domain Generalization",
                "abstract": "We study the problem of out-of-distribution (o.o.d.) generalization where spurious correlations of attributes vary across training and test domains. This is known as the problem of correlation shift and has posed concerns on the reliability of machine learning. In this work, we introduce the concepts of direct and indirect effects from causal inference to the domain generalization problem. We argue that models that learn direct effects minimize the worst-case risk across correlation-shifted domains. To eliminate the indirect effects, our algorithm consists of two stages: in the first stage, we learn an indirect-effect representation by minimizing the prediction error of domain labels using the representation and the class labels; in the second stage, we remove the indirect effects learned in the first stage by matching each data with another data of similar indirect-effect representation but of different class labels in the training and validation phase. Our approach is shown to be compatible with existing methods and improve the generalization performance of them on correlation-shifted datasets. Experiments on 5 correlation-shifted datasets and the DomainBed benchmark verify the effectiveness of our approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2192674200",
                        "name": "Yuhui Li"
                    },
                    {
                        "authorId": "2192669149",
                        "name": "Zejia Wu"
                    },
                    {
                        "authorId": "2188875716",
                        "name": "Chao Zhang"
                    },
                    {
                        "authorId": "40975176",
                        "name": "Hongyang Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "831c240d7725b8e4ba3e4039f16a693253fab2ab",
                "externalIds": {
                    "DBLP": "conf/cvpr/ZhangHTHMDX23",
                    "ArXiv": "2211.13203",
                    "DOI": "10.1109/CVPR52729.2023.00978",
                    "CorpusId": 257427673
                },
                "corpusId": 257427673,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/831c240d7725b8e4ba3e4039f16a693253fab2ab",
                "title": "Inversion-based Style Transfer with Diffusion Models",
                "abstract": "The artistic style within a painting is the means of expression, which includes not only the painting material, colors, and brushstrokes, but also the high-level attributes, including semantic elements and object shapes. Previous arbitrary example-guided artistic image generation methods often fail to control shape changes or convey elements. Pre-trained text-to-image synthesis diffusion probabilistic models have achieved remarkable quality but often require extensive textual descriptions to accurately portray the attributes of a particular painting. The uniqueness of an artwork lies in the fact that it cannot be adequately explained with normal language. Our key idea is to learn the artistic style directly from a single painting and then guide the synthesis without providing complex textual descriptions. Specifically, we perceive style as a learnable textual description of a painting. We propose an inversion-based style transfer method (InST), which can efficiently and accurately learn the key information of an image, thus capturing and transferring the artistic style of a painting. We demonstrate the quality and efficiency of our method on numerous paintings of various artists and styles. Codes are available at https://github.com/zyxElsa/InST.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108078624",
                        "name": "Yu-xin Zhang"
                    },
                    {
                        "authorId": "2186281333",
                        "name": "Nisha Huang"
                    },
                    {
                        "authorId": "1443761295",
                        "name": "Fan Tang"
                    },
                    {
                        "authorId": "3119608",
                        "name": "Haibin Huang"
                    },
                    {
                        "authorId": "151487472",
                        "name": "Chongyang Ma"
                    },
                    {
                        "authorId": "40441149",
                        "name": "Weiming Dong"
                    },
                    {
                        "authorId": "2155590336",
                        "name": "Changsheng Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A commonly used invariance hypothesis is the texture shift hypothesis: a lot of domain shifts are primarily textures shifts, and using style transfer based data augmentation will improve the generalization, whether it is explicitly by training a model on stylized images [38, 19] or implicitly in the internal representation of the network [43, 27].",
                "While all the existing methods improve upon the standard training procedure (ERM) on PACS, only EFDM, spectral decoupling [28], and our method yield better results on Office-Home.",
                "If the methods did not have quantitative hyper-parameters, such as EFDM [43] with the\nchoice of mixing-layers depths, we used the ones proposed for the PACS experiments.",
                "This would explain why test-time batch normalization yield an large improvement on the PACS benchmark, as the simple use of test-time statistics, that encode textures [3], is enough to significantly bridge the domain gap and why the methods reaching the highest results [43, 27, 38] in the usual setting (without test-time batch normalization) are all style transfer based methods.",
                "As a result, a number of methods study single-source domain generalization [38, 31, 44, 43, 27].",
                "If the methods did not have quantitative hyper-parameters, such as EFDM [43] with the choice of mixing-layers depths, we used the ones proposed for the PACS experiments.",
                "Used alongside test-time batch normalization, our method reaches a performance similar to that of EFDM [43] on the PACS datasets, but exceeds it on the Office-Home datasets.",
                "We compare our approach with the standard training procedure (expected risk minimization, ERM), with several methods designed for single-source domain generalization [38, 43, 27, 36, 44, 31], with a method designed to reduce the shortcut learning phenomenon in deep networks [28] and a multi-source domain generalization algorithm that does not explicitly require several training domains [17]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "9336d5f6919ae6d1d1af5de963a5c87f50ba5a52",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-09834",
                    "ArXiv": "2210.09834",
                    "DOI": "10.48550/arXiv.2210.09834",
                    "CorpusId": 252967955
                },
                "corpusId": 252967955,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9336d5f6919ae6d1d1af5de963a5c87f50ba5a52",
                "title": "Learning Less Generalizable Patterns with an Asymmetrically Trained Double Classifier for Better Test-Time Adaptation",
                "abstract": "Deep neural networks often fail to generalize outside of their training distribution, in particular when only a single data domain is available during training. While test-time adaptation has yielded encouraging results in this setting, we argue that, to reach further improvements, these ap-proaches should be combined with training procedure mod-ifications aiming to learn a more diverse set of patterns. Indeed, test-time adaptation methods usually have to rely on a limited representation because of the shortcut learning phenomenon: only a subset of the available predictive patterns is learned with standard training. In this paper, we first show that the combined use of existing training-time strategies, and test-time batch normalization, a simple adaptation method, does not always improve upon the test-time adaptation alone on the PACS benchmark. Furthermore, experiments on Office-Home show that very few training-time methods improve upon standard training, with or without test-time batch normalization. We therefore propose a novel approach using a pair of classifiers and a shortcut patterns avoidance loss that mitigates the shortcut learning behavior by reducing the generalization ability of the secondary classifier, using the additional shortcut patterns avoidance loss that encourages the learning of samples specific patterns. The primary classifier is trained normally, resulting in the learning of both the natural and the more complex, less generalizable, features. Our experiments show that our method improves upon the state-of-the-art results on both benchmarks and",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "150318727",
                        "name": "Thomas Duboudin"
                    },
                    {
                        "authorId": "2183481965",
                        "name": "Emmanuel Dellandr'ea"
                    },
                    {
                        "authorId": "153801365",
                        "name": "Corentin Abgrall"
                    },
                    {
                        "authorId": "2112211438",
                        "name": "Gilles H'enaff"
                    },
                    {
                        "authorId": "2171552078",
                        "name": "Limin Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4664221ba7f7878773ffa19139849290b2d6b308",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-07571",
                    "ArXiv": "2210.07571",
                    "DOI": "10.48550/arXiv.2210.07571",
                    "CorpusId": 252907345
                },
                "corpusId": 252907345,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4664221ba7f7878773ffa19139849290b2d6b308",
                "title": "Mix and Reason: Reasoning over Semantic Topology with Data Mixing for Domain Generalization",
                "abstract": "Domain generalization (DG) enables generalizing a learning machine from multiple seen source domains to an unseen target one. The general objective of DG methods is to learn semantic representations that are independent of domain labels, which is theoretically sound but empirically challenged due to the complex mixture of common and domain-specific factors. Although disentangling the representations into two disjoint parts has been gaining momentum in DG, the strong presumption over the data limits its efficacy in many real-world scenarios. In this paper, we propose Mix and Reason (\\mire), a new DG framework that learns semantic representations via enforcing the structural invariance of semantic topology. \\mire\\ consists of two key components, namely, Category-aware Data Mixing (CDM) and Adaptive Semantic Topology Refinement (ASTR). CDM mixes two images from different domains in virtue of activation maps generated by two complementary classification losses, making the classifier focus on the representations of semantic objects. ASTR introduces relation graphs to represent semantic topology, which is progressively refined via the interactions between local feature aggregation and global cross-domain relational reasoning. Experiments on multiple DG benchmarks validate the effectiveness and robustness of the proposed \\mire.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145762695",
                        "name": "Chaoqi Chen"
                    },
                    {
                        "authorId": "2110211447",
                        "name": "Luyao Tang"
                    },
                    {
                        "authorId": "40405236",
                        "name": "Feng Liu"
                    },
                    {
                        "authorId": "22658530",
                        "name": "Gangming Zhao"
                    },
                    {
                        "authorId": "2108716197",
                        "name": "Yue Huang"
                    },
                    {
                        "authorId": "1841911",
                        "name": "Yizhou Yu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7ede79760454da631a20bce64c86c05612bbbbc3",
                "externalIds": {
                    "ArXiv": "2210.04155",
                    "DBLP": "journals/corr/abs-2210-04155",
                    "DOI": "10.48550/arXiv.2210.04155",
                    "CorpusId": 252780700,
                    "PubMed": "37440378"
                },
                "corpusId": 252780700,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7ede79760454da631a20bce64c86c05612bbbbc3",
                "title": "Constrained Maximum Cross-Domain Likelihood for Domain Generalization",
                "abstract": "As a recent noticeable topic, domain generalization aims to learn a generalizable model on multiple source domains, which is expected to perform well on unseen test domains. Great efforts have been made to learn domain-invariant features by aligning distributions across domains. However, existing works are often designed based on some relaxed conditions which are generally hard to satisfy and fail to realize the desired joint distribution alignment. In this article, we propose a novel domain generalization method, which originates from an intuitive idea that a domain-invariant classifier can be learned by minimizing the Kullback-Leibler (KL)-divergence between posterior distributions from different domains. To enhance the generalizability of the learned classifier, we formalize the optimization objective as an expectation computed on the ground-truth marginal distribution. Nevertheless, it also presents two obvious deficiencies, one of which is the side-effect of entropy increase in KL-divergence and the other is the unavailability of ground-truth marginal distributions. For the former, we introduce a term named maximum in-domain likelihood to maintain the discrimination of the learned domain-invariant representation space. For the latter, we approximate the ground-truth marginal distribution with source domains under a reasonable convex hull assumption. Finally, a constrained maximum cross-domain likelihood (CMCL) optimization problem is deduced, by solving which the joint distributions are naturally aligned. An alternating optimization strategy is carefully designed to approximately solve this optimization problem. Extensive experiments on four standard benchmark datasets, i.e., Digits-DG, PACS, Office-Home, and miniDomainNet, highlight the superior performance of our method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46698009",
                        "name": "Jianxin Lin"
                    },
                    {
                        "authorId": "2111321753",
                        "name": "Yongqiang Tang"
                    },
                    {
                        "authorId": "2110149124",
                        "name": "Junping Wang"
                    },
                    {
                        "authorId": "2108167359",
                        "name": "Wensheng Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "eac1c50b6c98b7fb139b0ef62a517822e069e3c0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-08473",
                    "ArXiv": "2209.08473",
                    "DOI": "10.48550/arXiv.2209.08473",
                    "CorpusId": 252368282
                },
                "corpusId": 252368282,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/eac1c50b6c98b7fb139b0ef62a517822e069e3c0",
                "title": "Bootstrap Generalization Ability from Loss Landscape Perspective",
                "abstract": "Domain generalization aims to learn a model that can generalize well on the unseen test dataset, i.e., out-of-distribution data, which has different distribution from the training dataset. To address domain generalization in computer vision, we introduce the loss landscape theory into this field. Specifically, we bootstrap the generalization ability of the deep learning model from the loss landscape perspective in four aspects, including backbone, regularization, training paradigm, and learning rate. We verify the proposed theory on the NICO++, PACS, and VLCS datasets by doing extensive ablation studies as well as visualizations. In addition, we apply this theory in the ECCV 2022 NICO Challenge1 and achieve the 3rd place without using any domain invariant methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2185449429",
                        "name": "Huanran Chen"
                    },
                    {
                        "authorId": "2185412500",
                        "name": "Shitong Shao"
                    },
                    {
                        "authorId": "48707632",
                        "name": "Ziyi Wang"
                    },
                    {
                        "authorId": "2185412274",
                        "name": "Zirui Shang"
                    },
                    {
                        "authorId": "2108458375",
                        "name": "Jin Chen"
                    },
                    {
                        "authorId": "2090273611",
                        "name": "Xiaofeng Ji"
                    },
                    {
                        "authorId": "2125709",
                        "name": "Xinxiao Wu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d07f137bdb29d9fb2bb01750f634d9dec8c078c9",
                "externalIds": {
                    "DBLP": "conf/ccis/LinTWZ22",
                    "ArXiv": "2209.08253",
                    "DOI": "10.1109/CCIS57298.2022.10016385",
                    "CorpusId": 252368240
                },
                "corpusId": 252368240,
                "publicationVenue": {
                    "id": "b8e3fb18-a48d-49f0-96bd-38cca7124b0d",
                    "name": "International Conference on Cloud Computing and Intelligence Systems",
                    "type": "conference",
                    "alternate_names": [
                        "CCIS",
                        "Int Conf Cloud Comput Intell Syst"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d07f137bdb29d9fb2bb01750f634d9dec8c078c9",
                "title": "Mitigating Both Covariate and Conditional Shift for Domain Generalization",
                "abstract": "Domain generalization (DG) aims to learn a model on several source domains, hoping that the model can generalize well to unseen target domains. The distribution shift between domains contains the covariate shift and conditional shift, both of which the model must be able to handle for better generalizability. In this paper, a novel DG method is proposed to deal with the distribution shift via Visual Alignment and Uncertainty-guided belief Ensemble (VAUE). Specifically, for the covariate shift, a visual alignment module is designed to align the distribution of image style to a common empirical Gaussian distribution so that the covariate shift can be eliminated in the visual space. For the conditional shift, we adopt an uncertainty-guided belief ensemble strategy based on subjective logic and Dempster-Shafer theory. The conditional distribution given a test sample is estimated by the dynamic combination of that of source domains. Comprehensive experiments are conducted to demonstrate the superior performance of the proposed method on four widely used datasets, i.e., Office-Home, VLCS, TerraIncognita, and PACS.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46698009",
                        "name": "Jianxin Lin"
                    },
                    {
                        "authorId": "2111321753",
                        "name": "Yongqiang Tang"
                    },
                    {
                        "authorId": "2110149124",
                        "name": "Junping Wang"
                    },
                    {
                        "authorId": "2108167359",
                        "name": "Wensheng Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "See the results of RSC [18], MixStyle [59] and EFDMix [51] in Figure 1a.",
                "Data augmentation methods aim to diversify the training data, which is often achieved by learning a generative model [57, 58] or mixing data at the input [43,44] or feature-level [51,59].",
                "In most cases, the two feature-based data augmentation methods, i.e., MixStyle and EFDMix, achieve better performance than RSC, a regularization method that mutes the most predictive subsets of neurons during training.",
                "For example, MixStyle and EFDMix can improve upon ERM when using MobileNetV3-Small but their performance plunges below ERM\u2019s when using two much smaller architectures specifically designed for MCUs.",
                "We choose top-performing DG methods that do not need domain labels to compare: ERM, RSC [18], MixStyle [59], and EFDMix [51]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "18b043cd8b6994c0bee4395596d899cab940a827",
                "externalIds": {
                    "ArXiv": "2209.07521",
                    "DBLP": "journals/corr/abs-2209-07521",
                    "DOI": "10.48550/arXiv.2209.07521",
                    "CorpusId": 252283920
                },
                "corpusId": 252283920,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/18b043cd8b6994c0bee4395596d899cab940a827",
                "title": "On-Device Domain Generalization",
                "abstract": "We present a systematic study of domain generalization (DG) for tiny neural networks. This problem is critical to on-device machine learning applications but has been overlooked in the literature where research has been merely focused on large models. Tiny neural networks have much fewer parameters and lower complexity and therefore should not be trained the same way as their large counterparts for DG applications. By conducting extensive experiments, we find that knowledge distillation (KD), a well-known technique for model compression, is much better for tackling the on-device DG problem than conventional DG methods. Another interesting observation is that the teacher-student gap on out-of-distribution data is bigger than that on in-distribution data, which highlights the capacity mismatch issue as well as the shortcoming of KD. We further propose a method called out-of-distribution knowledge distillation (OKD) where the idea is to teach the student how the teacher handles out-of-distribution data synthesized via disruptive data augmentation. Without adding any extra parameter to the model -- hence keeping the deployment cost unchanged -- OKD significantly improves DG performance for tiny neural networks in a variety of on-device DG scenarios for image and speech applications. We also contribute a scalable approach for synthesizing visual domain shifts, along with a new suite of DG datasets to complement existing testbeds.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9368124",
                        "name": "Kaiyang Zhou"
                    },
                    {
                        "authorId": "2145784327",
                        "name": "Yuanhan Zhang"
                    },
                    {
                        "authorId": "12862495",
                        "name": "Yuhang Zang"
                    },
                    {
                        "authorId": "2295601",
                        "name": "Jingkang Yang"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    },
                    {
                        "authorId": "2117940996",
                        "name": "Ziwei Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although dating from 2015, VGG-19 is still the go-to architecture for applications involving Gram matrices such as image style transfer (Zhang et al., 2022; H\u00f6llein et al., 2022; Xie et al., 2022)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fe2c21db202d71cff0af7dc8cce8c6e46ec21de1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-12625",
                    "ArXiv": "2208.12625",
                    "DOI": "10.48550/arXiv.2208.12625",
                    "CorpusId": 251881425
                },
                "corpusId": 251881425,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fe2c21db202d71cff0af7dc8cce8c6e46ec21de1",
                "title": "Take One Gram of Neural Features, Get Enhanced Group Robustness",
                "abstract": "Predictive performance of machine learning models trained with empirical risk minimization (ERM) can degrade considerably under distribution shifts. The presence of spurious correlations in training datasets leads ERM-trained models to display high loss when evaluated on minority groups not presenting such correlations. Extensive attempts have been made to develop methods improving worst-group robustness. However, they require group information for each training input or at least, a validation set with group labels to tune their hyperparameters, which may be expensive to get or unknown a priori. In this paper, we address the challenge of improving group robustness without group annotation during training or validation. To this end, we propose to partition the training dataset into groups based on Gram matrices of features extracted by an ``identification'' model and to apply robust optimization based on these pseudo-groups. In the realistic context where no group labels are available, our experiments show that our approach not only improves group robustness over ERM but also outperforms all recent baselines",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1753860506",
                        "name": "Simon Roburin"
                    },
                    {
                        "authorId": "1412921752",
                        "name": "Charles Corbi\u00e8re"
                    },
                    {
                        "authorId": "3242930",
                        "name": "Gilles Puy"
                    },
                    {
                        "authorId": "1728523",
                        "name": "Nicolas Thome"
                    },
                    {
                        "authorId": "3039779",
                        "name": "Matthieu Aubry"
                    },
                    {
                        "authorId": "3250857",
                        "name": "Renaud Marlet"
                    },
                    {
                        "authorId": "2173636176",
                        "name": "Patrick P'erez"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similarly, EFDM (Zhang et al. 2022) proposes to match the empirical Cumulative Distribution Functions (eCDFs) of image features, mapping the representation from unseen domains to the specific feature space."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "55b51b993600aa23d84e1e15fb53641100e9c772",
                "externalIds": {
                    "DBLP": "conf/aaai/PeiSXXM23",
                    "ArXiv": "2207.12194",
                    "DOI": "10.48550/arXiv.2207.12194",
                    "CorpusId": 251041085
                },
                "corpusId": 251041085,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/55b51b993600aa23d84e1e15fb53641100e9c772",
                "title": "Domain Decorrelation with Potential Energy Ranking",
                "abstract": "Machine learning systems, especially the methods based on deep learning, enjoy great success in modern computer vision tasks under ideal experimental settings. Generally, these classic deep learning methods are built on the i.i.d. assumption, supposing the training and test data are drawn from the same distribution independently and identically. However, the aforementioned i.i.d. assumption is, in general, unavailable in the real-world scenarios, and as a result, leads to sharp performance decay of deep learning algorithms. Behind this, domain shift is one of the primary factors to be blamed. In order to tackle this problem, we propose using Potential Energy Ranking (PoER) to decouple the object feature and the domain feature in given images, promoting the learning of label-discriminative representations while filtering out the irrelevant correlations between the objects and the background. PoER employs the ranking loss in shallow layers to make features with identical category and domain labels close to each other and vice versa. This makes the neural networks aware of both objects and background characteristics, which is vital for generating domain-invariant features. Subsequently, with the stacked convolutional blocks, PoER further uses the contrastive loss to make features within the same categories distribute densely no matter domains, filtering out the domain information progressively for feature alignment. PoER reports superior performance on domain generalization benchmarks, improving the average top-1 accuracy by at least 1.20% compared to the existing methods. Moreover, we use PoER in the ECCV 2022 NICO Challenge, achieving top place with only a vanilla ResNet-18 and winning the jury award. The code has been made publicly available at: https://github.com/ForeverPs/PoER.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146694400",
                        "name": "Sen Pei"
                    },
                    {
                        "authorId": "1516869541",
                        "name": "Jiaxing Sun"
                    },
                    {
                        "authorId": "1683738",
                        "name": "Shiming Xiang"
                    },
                    {
                        "authorId": "3182192",
                        "name": "Gaofeng Meng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In domain generalization, EFDMix [11] applies the EHM method to gain an exact match of the eCDF by performing ranked matching in the image feature space.",
                "(2) We adopt the EFDMix (Exact Feature Distribution Mixing) method [11] to generate two different enhancements by implicitly using higher order statistics to produce more diverse feature enhancements, which is the first application of EFDMix to vehicle re-identification so far.",
                "We show the overall framework of the proposed network in section A; in section B we introduce the IBN-Net; in section C we describe the EFDMix method [11]; finally, in section D we describe the SE and CA attention mechanisms; and in section F we introduce the loss function used.",
                "EFDM(x, y) \u2236 o\u03c4i = x\u03c4i + yki \u2212 \u3008x\u03c4i\u3009 (4) where\u3008x\u03c4i\u3009represents the stop-gradient operation [11].",
                "To overcome the problem of cross-domain performance degradation in vehicle reidentification, this paper introduces a new method EFDMix[11] that performs Exact Feature Distribution Matching (EFDM) based on the empirical Cumulative Distribution Function (eCDF) of exact matching image features."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c0b01fa169b4edfc166fdd27704b32f23502c8f5",
                "externalIds": {
                    "DOI": "10.1109/PRML56267.2022.9882239",
                    "CorpusId": 252223824
                },
                "corpusId": 252223824,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c0b01fa169b4edfc166fdd27704b32f23502c8f5",
                "title": "Vehicle Re-identification Approach Combining Multiple Attention Mechanisms and Style Transfer",
                "abstract": "To address the impact of inter-domain style differences on cross-domain performance degradation in vehicle re-identification applications, a vehicle re-identification method combining multiple attention mechanisms and style transfer is proposed. Firstly, IBN-Net is introduced on the basis of ResNet50 for improving the generalization ability of the network, secondly, EFDMix method is used to generate more diverse feature enhancements to improve the domain adaptation ability of the model, while multiple attention mechanisms are combined to extract more discriminative vehicle features and improve the feature representation ability of the model on different datasets, and finally, cross-entropy loss with label smoothing and supervised contrastive loss for vehicle sample classification and optimising the feature distance between categories. Single-domain vehicle re-identification experiments were conducted on two datasets, VeRi-776 and VehicleID, and the results showed that our proposed method outperformed existing methods in most performance metrics. Further cross-domain comparison experiments also demonstrate the advancedness of the proposed method in terms of cross-domain.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2106687637",
                        "name": "X. Pan"
                    },
                    {
                        "authorId": "2162744402",
                        "name": "Xianglan Liu"
                    },
                    {
                        "authorId": "2113349076",
                        "name": "B. Song"
                    },
                    {
                        "authorId": "2166511065",
                        "name": "Runqing Li"
                    },
                    {
                        "authorId": "2106696455",
                        "name": "L. Rong"
                    },
                    {
                        "authorId": "2154894910",
                        "name": "Yan Xu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "baea3ac7e64a620230b651810aef0151b4614387",
                "externalIds": {
                    "DBLP": "conf/cvpr/Zhang0XYS023",
                    "ArXiv": "2204.08040",
                    "DOI": "10.1109/CVPR52729.2023.01539",
                    "CorpusId": 248227486
                },
                "corpusId": 248227486,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/baea3ac7e64a620230b651810aef0151b4614387",
                "title": "NICO++: Towards Better Benchmarking for Domain Generalization",
                "abstract": "Despite the remarkable performance that modern deep neural networks have achieved on independent and identically distributed (I.I.D.) data, they can crash under distribution shifts. Most current evaluation methods for do-main generalization (DG) adopt the leave-one-out strategy as a compromise on the limited number of domains. We propose a large-scale benchmark with extensive labeled domains named $NICO^{++}$ along with more rational evaluation methods for comprehensively evaluating DC algorithms. To evaluate DG datasets, we propose two metrics to quantify covariate shift and concept shift, respectively. Two novel generalization bounds from the perspective of data construction are proposed to prove that limited concept shift and significant covariate shift favor the evaluation capability for generalization. Through extensive experiments, $NlCO^{++}$ shows its superior evaluation capability compared with current DG datasets and its contribution in alleviating unfairness caused by the leak of oracle knowledge in model selection. The data and code for the benchmark based on $NICO^{++}$ are available at https://github.com/xxgege/NICO-plus.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51258901",
                        "name": "Xingxuan Zhang"
                    },
                    {
                        "authorId": "48207021",
                        "name": "Linjun Zhou"
                    },
                    {
                        "authorId": "150287491",
                        "name": "Renzhe Xu"
                    },
                    {
                        "authorId": "2153522384",
                        "name": "Peng Cui"
                    },
                    {
                        "authorId": "24069072",
                        "name": "Zheyan Shen"
                    },
                    {
                        "authorId": "2143856875",
                        "name": "Haoxin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, existing DG methods, which mainly span domain invariant feature learning [16,34,35,41,42,69], gradient based meta-learning [9, 30, 31], and augmentation based generalization [50, 58], are devoted to learn semantic representations in virtue of one-vs-one consistency constraints."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "686d7d0f2d67c94704a269b9beb6f7a5cffbcaae",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-13006",
                    "ArXiv": "2203.13006",
                    "DOI": "10.1109/CVPR52688.2022.00698",
                    "CorpusId": 247627964
                },
                "corpusId": 247627964,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/686d7d0f2d67c94704a269b9beb6f7a5cffbcaae",
                "title": "Compound Domain Generalization via Meta-Knowledge Encoding",
                "abstract": "Domain generalization (DG) aims to improve the generalization performance for an unseen target domain by using the knowledge of multiple seen source domains. Mainstream DG methods typically assume that the domain label of each source sample is known a priori, which is challenged to be satisfied in many real-world applications. In this paper, we study a practical problem of compound DG, which relaxes the discrete domain assumption to the mixed source domains setting. On the other hand, current DG algorithms prioritize the focus on semantic invariance across domains (one-vs-one), while paying less attention to the holistic semantic structure (many-vs-many). Such holistic semantic structure, referred to as meta-knowledge here, is crucial for learning generalizable representations. To this end, we present COmpound domain generalization via Meta-knowledge ENcoding (COMEN), a general approach to automatically discover and model latent domains in two steps. Firstly, we introduce Style-induced Domain-specific Normalization (SDNorm) to re-normalize the multi-modal underlying distributions, thereby dividing the mixture of source domains into latent clusters. Secondly, we harness the prototype representations, the centroids of classes, to perform relational modeling in the embedding space with two parallel and complementary modules, which explicitly encode the semantic structure for the out-of-distribution generalization. Experiments on four standard DG benchmarks reveal that COMEN exceeds the state-of-the-art performance without the need of domain supervision.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145762695",
                        "name": "Chaoqi Chen"
                    },
                    {
                        "authorId": "2108930081",
                        "name": "Jiongcheng Li"
                    },
                    {
                        "authorId": "1763245",
                        "name": "Xiaoguang Han"
                    },
                    {
                        "authorId": "1753642167",
                        "name": "Xiaoqing Liu"
                    },
                    {
                        "authorId": "1841911",
                        "name": "Yizhou Yu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "12d20b0a56393273856a750b62cef0a184179ba9",
                "externalIds": {
                    "ArXiv": "2112.03676",
                    "DOI": "10.1145/3624015",
                    "CorpusId": 244920905
                },
                "corpusId": 244920905,
                "publicationVenue": {
                    "id": "bb2eb372-4df2-4181-9988-71aecb1dcc5e",
                    "name": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP)",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Multimedia Comput Commun Appl (TOMCCAP",
                        "ACM Transactions on Multimedia Computing, Communications, and Applications",
                        "ACM Trans Multimedia Comput Commun Appl"
                    ],
                    "issn": "1551-6857",
                    "url": "http://www.acm.org/tomccap/",
                    "alternate_urls": [
                        "http://tomccap.acm.org/",
                        "http://tomm.acm.org/",
                        "http://portal.acm.org/tomccap/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/12d20b0a56393273856a750b62cef0a184179ba9",
                "title": "PLACE dropout: A Progressive Layer-wise and Channel-wise Dropout for Domain Generalization",
                "abstract": "Domain generalization (DG) aims to learn a generic model from multiple observed source domains that generalizes well to arbitrary unseen target domains without further training. The major challenge in DG is that the model inevitably faces a severe overfitting issue due to the domain gap between source and target domains. To mitigate this problem, some dropout-based methods have been proposed to resist overfitting by discarding part of the representation of the intermediate layers. However, we observe that most of these methods only conduct the dropout operation in some specific layers, leading to an insufficient regularization effect on the model. We argue that applying dropout at multiple layers can produce stronger regularization effects, which could alleviate the overfitting problem on source domains more adequately than previous layer-specific dropout methods. In this paper, we develop a novel layer-wise and channel-wise dropout for DG, which randomly selects one layer and then randomly selects its channels to conduct dropout. Particularly, the proposed method can generate a variety of data variants to better deal with the overfitting issue. We also provide theoretical analysis for our dropout method and prove that it can effectively reduce the generalization error bound. Besides, we leverage the progressive scheme to increase the dropout ratio with the training progress, which can gradually boost the difficulty of training the model to enhance its robustness. Extensive experiments on three standard benchmark datasets have demonstrated that our method outperforms several state-of-the-art DG methods. Our code is available at https://github.com/lingeringlight/PLACEdropout.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50115584",
                        "name": "Jintao Guo"
                    },
                    {
                        "authorId": "1785352346",
                        "name": "Lei Qi"
                    },
                    {
                        "authorId": "2475959",
                        "name": "Yinghuan Shi"
                    },
                    {
                        "authorId": "145644819",
                        "name": "Yang Gao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "539e1fc503f525d3ef5b8a1976da04577279107a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-03924",
                    "ArXiv": "2102.03924",
                    "DOI": "10.1007/s10994-023-06324-x",
                    "CorpusId": 231846357
                },
                "corpusId": 231846357,
                "publicationVenue": {
                    "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
                    "name": "Machine-mediated learning",
                    "type": "journal",
                    "alternate_names": [
                        "Mach learn",
                        "Machine Learning",
                        "Mach Learn"
                    ],
                    "issn": "0732-6718",
                    "alternate_issns": [
                        "0885-6125"
                    ],
                    "url": "http://www.springer.com/computer/artificial/journal/10994",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10994",
                        "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/539e1fc503f525d3ef5b8a1976da04577279107a",
                "title": "Domain adversarial neural networks for domain generalization: when it works and how to improve",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51137683",
                        "name": "Anthony Sicilia"
                    },
                    {
                        "authorId": "121809235",
                        "name": "Xingchen Zhao"
                    },
                    {
                        "authorId": "3367790",
                        "name": "Seong Jae Hwang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Used alongside test-time batch normalization, our method reaches a performance similar to that of EFDM [44] on the PACS datasets but exceeds it on the Of\ufb01ce-Home datasets.",
                "Used alongside test-time batch normalization, our method reaches a performance similar to that of EFDM [44] on the PACS datasets but exceeds it on the Office-Home datasets.",
                "It can be done explicitly by training a model on stylized images [38, 20] or implicitly in the internal representation of the network [44, 28].",
                "We compare our approach with the standard training procedure (expected risk minimization, abbreviated ERM), with several methods designed for single-source domain generalization [38, 44, 28, 36, 45, 32], with Spectral Decoupling [29], a method designed to reduce the shortcutlearning phenomenon in deep networks, and with RSC [18], and InfoDrop [32], that are domain generalization algorithms which do not explicitly require several training domains.",
                "While all the existing methods improve upon the standard training procedure (ERM) on PACS, only EFDM, spectral decoupling [29], and our method yield better results on Of\ufb01ce-Home.",
                "It would also explain why the methods reaching the highest results [44, 28, 38] in the usual setting (without test-time batch normalization) are all style-transfer-based methods.",
                "As a result, some works study single-source domain generalization [38, 32, 45, 44, 28]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "50505a10213d59ff74000996f15ef302477062b0",
                "externalIds": {
                    "DBLP": "conf/visapp/DuboudinDAH023",
                    "DOI": "10.5220/0011893800003417",
                    "CorpusId": 257355772
                },
                "corpusId": 257355772,
                "publicationVenue": {
                    "id": "3e53351c-7355-4159-a3f2-f3b03a3aa989",
                    "name": "VISIGRAPP",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Comput Vis Imaging Comput Graph Theory Appl",
                        "International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications"
                    ],
                    "issn": "2184-4321",
                    "alternate_issns": [
                        "2184-5921"
                    ],
                    "url": "http://www.visigrapp.org/"
                },
                "url": "https://www.semanticscholar.org/paper/50505a10213d59ff74000996f15ef302477062b0",
                "title": "Learning Less Generalizable Patterns for Better Test-Time Adaptation",
                "abstract": "Deep neural networks often fail to generalize outside of their training distribution, particularly when only a single data domain is available during training. While test-time adaptation has yielded encouraging results in this setting, we argue that to reach further improvements, these approaches should be combined with training procedure mod-i\ufb01cations aiming to learn a more diverse set of patterns. Indeed, test-time adaptation methods usually have to rely on a limited representation because of the shortcut learning phenomenon: only a subset of the available predictive patterns is learned with standard training. In this paper, we \ufb01rst show that the combined use of existing training-time strategies and test-time batch normalization, a simple adaptation method, does not always improve upon the test-time adaptation alone on the PACS benchmark. Furthermore, experiments on Of\ufb01ce-Home show that very few training-time methods improve upon standard training, with or without test-time batch normalization. Therefore, we propose a novel approach that mitigates the shortcut learning behavior by having an additional classi\ufb01cation branch learn less predictive and generalizable patterns. Our experiments show that our method improves upon the state-of-the-art re-sults on both benchmarks and bene\ufb01ts the most to test-time batch normalization.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "150318727",
                        "name": "Thomas Duboudin"
                    },
                    {
                        "authorId": "1718878",
                        "name": "E. Dellandr\u00e9a"
                    },
                    {
                        "authorId": "153801365",
                        "name": "Corentin Abgrall"
                    },
                    {
                        "authorId": "2386962",
                        "name": "Gilles H\u00e9naff"
                    },
                    {
                        "authorId": "47818198",
                        "name": "Liming Luke Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8cc9c663cde7a7f8fbfdcc438a0a47030e82263a",
                "externalIds": {
                    "CorpusId": 259973042
                },
                "corpusId": 259973042,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8cc9c663cde7a7f8fbfdcc438a0a47030e82263a",
                "title": "L EARNING D OMAIN -A GNOSTIC R EPRESENTATION FOR D ISEASE D IAGNOSIS",
                "abstract": "In clinical environments, image-based diagnosis is desired to achieve robustness on multi-center samples. Toward this goal, a natural way is to capture only clinically disease-related features. However, such disease-related features are often entangled with center-effect, disabling robust transferring to unseen centers/domains. To disentangle disease-related features, we first leverage structural causal modeling to explicitly model disease-related and center-effects that are provable to be disentangled from each other. Guided by this, we propose a novel Domain Agnostic Representation Model (DarMo) based on variational Auto-Encoder. To facilitate disentanglement, we design domain-agnostic and domain-aware encoders to respectively capture disease-related features and varied center effects by incorporating a domain-aware batch normalization layer. Besides, we constrain the disease-related features to well predict the disease label as well as clinical attributes, by leveraging Graph Convolutional Network (GCN) into our decoder. The effectiveness and utility of our method are demonstrated by the superior performance over others on both public datasets and in-house datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1390842229",
                        "name": "Chu-ran Wang"
                    },
                    {
                        "authorId": "8283163",
                        "name": "Xinwei Sun"
                    },
                    {
                        "authorId": "9579109",
                        "name": "Fandong Zhang"
                    },
                    {
                        "authorId": "2761404",
                        "name": "Yizhou Yu"
                    },
                    {
                        "authorId": "2143470643",
                        "name": "Yizhou Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d7c28e55bd7ec272cc712b3c60d401a4cadd84ec",
                "externalIds": {
                    "DOI": "10.3788/gzxb20235204.0410003",
                    "CorpusId": 261307729
                },
                "corpusId": 261307729,
                "publicationVenue": {
                    "id": "09ee8237-7f80-4760-bd0b-cfea47b66dae",
                    "name": "Acta Photonica Sinica",
                    "type": "journal",
                    "alternate_names": [
                        "Acta Photonica Sin"
                    ],
                    "issn": "1004-4213",
                    "url": "http://www.oriprobe.com/journals/gzxb.html"
                },
                "url": "https://www.semanticscholar.org/paper/d7c28e55bd7ec272cc712b3c60d401a4cadd84ec",
                "title": "\u57fa\u4e8e\u6539\u8fdb\u7684\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u53ef\u89c1\u5149\u7ea2\u5916\u56fe\u50cf\u8f6c\u6362\u7b97\u6cd5",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2235565955",
                        "name": "\u9a6c\u5f97\u8349 MA Decao"
                    },
                    {
                        "authorId": "2146764317",
                        "name": "\u9c9c\u52c7 XIAN Yong"
                    },
                    {
                        "authorId": "2213557419",
                        "name": "\u82cf\u5a1f Su Juan"
                    },
                    {
                        "authorId": "2235566267",
                        "name": "\u674e\u5c11\u670b LI Shaopeng"
                    },
                    {
                        "authorId": "2128375347",
                        "name": "\u674e\u51b0 Li Bing"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In particular, MixStyle [25] generates novel domains by mixing implicit style information and demonstrates outstanding performances and EFDMix [27] provides more diverse feature augmentations by measuring accurate feature distributions.",
                "The purpose of data augmentation in DG is to enlarge the source domain distribution into a wider span [5], [8], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], which can improve the robustness of the models to novel domains.",
                "A recent data augmentation method, MixStyle [25], pAdaIN [24] and EFDMix [27], achieve significant improvement over the other baselines.",
                "Note that, the purpose of data augmentation techniques, MixStyle [25], pAdaIN [24] and EFDMix [27], are to diversify the source domain as aforementioned.",
                "Very recently, the generation-based approaches to define novel domains by synthesizing new images in the embedding space have been proposed [8], [23], [25], [27]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "dba88e0b21e177a57e728612b2b4e05e3db1fdf0",
                "externalIds": {
                    "DBLP": "journals/access/SeongCJH22",
                    "DOI": "10.1109/ACCESS.2022.3225970",
                    "CorpusId": 254333370
                },
                "corpusId": 254333370,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/dba88e0b21e177a57e728612b2b4e05e3db1fdf0",
                "title": "Pivot-Guided Embedding for Domain Generalization",
                "abstract": "Neural networks have suffered from a distribution gap between training and test data, known as domain shift. Domain generalization (DG) methods aim to learn domain invariant representations only with limited source domain data to cope with unseen target domains. The main assumption is that the model trained to extract semantically consistent features without any domain specific information is highly adaptable to the unseen target domain. Metric learning allows embedding representations to be class-separated and domain-mixed, which is an optimal condition for DG but has been downplayed in recent works. Even the most popular triplet embedding has limitations in forming an optimal embedding space for DG due to instability. In this paper, we present a novel deep metric learning method for domain invariant representations. Specifically, we propose Pivot-Guided Embedding (PGE), which explicitly forms the entire feature distribution of the embedding space with a novel pivot-guided attraction-repulsion mechanism, to address the instability problem that triplet embedding has. In particular, we leverage pivot features representing a coarse distribution of the entire space as reference points to guide other features toward domain invariant feature distribution. To this end, a pivot selection algorithm is presented to reliably reflect the entire feature distribution. Furthermore, we define Guide-Field, a subspace spanned by a subset of pivots chosen for individual samples, to guide each sample to domain invariant feature space. In a nutshell, the attraction-repulsion mechanism based on pivots, the reliable set of features representing the entire feature distribution, enables the model to extract domain invariant feature representations and also settles the instability problem of triplet loss. Experimental results on three different benchmarks validate the performance advantages of the proposed method over the state-of-the-art DG techniques.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "77363082",
                        "name": "Hyun Seok Seong"
                    },
                    {
                        "authorId": "2149220512",
                        "name": "Jaehyun Choi"
                    },
                    {
                        "authorId": "2097271999",
                        "name": "W. Jeong"
                    },
                    {
                        "authorId": "7212202",
                        "name": "Jae-Pil Heo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026such problems are found in numerous applications, including molecular docking (Gainza et al., 2020), image-based rendering (Fachada et al., 2021) , 3D reconstruction (Zhao et al., 2022), generative models (Dai & Hang, 2021) and style transfer (Zhang et al., 2022), in addition to countless others.",
                ", 2022), generative models (Dai & Hang, 2021) and style transfer (Zhang et al., 2022), in addition to countless others."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7e0342437c76baf9ba07b8c10153d4db0f4a7fda",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-14938",
                    "DOI": "10.48550/arXiv.2205.14938",
                    "CorpusId": 249191620
                },
                "corpusId": 249191620,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7e0342437c76baf9ba07b8c10153d4db0f4a7fda",
                "title": "Harnessing spectral representations for subgraph alignment",
                "abstract": "computational time of ZoomOut re\ufb01nement starting from an input functional map. All the experiments were performed on a Intel(R) Core(TM) i7-9700K CPU @ 3.60GHz.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2065528105",
                        "name": "Marco Pegoraro"
                    },
                    {
                        "authorId": "2053857470",
                        "name": "R. Marin"
                    },
                    {
                        "authorId": "14356390",
                        "name": "Arianna Rampini"
                    },
                    {
                        "authorId": "1972186",
                        "name": "S. Melzi"
                    },
                    {
                        "authorId": "1904895",
                        "name": "L. Cosmo"
                    },
                    {
                        "authorId": "2167025998",
                        "name": "Emaneule Rodola"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "13855416821771c85171e4e007606ca2bce7bc6b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-13203",
                    "DOI": "10.48550/arXiv.2211.13203",
                    "CorpusId": 253802114
                },
                "corpusId": 253802114,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/13855416821771c85171e4e007606ca2bce7bc6b",
                "title": "Inversion-Based Creativity Transfer with Diffusion Models",
                "abstract": "In this paper, we introduce the task of \u201cCreativity Trans-fer\u201d. The artistic creativity within a painting is the means of expression, which includes not only the painting material, colors, and brushstrokes, but also the high-level attributes including semantic elements, object shape, etc. Previous arbitrary example-guided artistic image generation methods (e.g., style transfer) often fail to control shape changes or convey semantic elements. The pre-trained text-to-image synthesis diffusion probabilistic models have achieved re-markable quality, but they often require extensive textual descriptions to accurately portray attributes of a particular painting. We believe that the uniqueness of an artwork lies precisely in the fact that it cannot be adequately explained with normal language. Our key idea is to learn artistic creativity directly from a single painting and then guide the synthesis without providing complex textual descriptions. Speci\ufb01cally, we assume creativity as a learnable textual description of a painting. We propose an attention-based inversion method, which can ef\ufb01ciently and accurately learn the holistic and detailed information of an image, thus cap-turing the complete artistic creativity of a painting. We demonstrate the quality and ef\ufb01ciency of our method on numerous paintings of various artists and styles. Code and models are available at https://github.com/ zyxElsa/creativity-transfer .",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108078624",
                        "name": "Yu-xin Zhang"
                    },
                    {
                        "authorId": "2186281333",
                        "name": "Nisha Huang"
                    },
                    {
                        "authorId": "1443761295",
                        "name": "Fan Tang"
                    },
                    {
                        "authorId": "3119608",
                        "name": "Haibin Huang"
                    },
                    {
                        "authorId": "151487472",
                        "name": "Chongyang Ma"
                    },
                    {
                        "authorId": "40441149",
                        "name": "Weiming Dong"
                    },
                    {
                        "authorId": "2155590336",
                        "name": "Changsheng Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Among data augmentation methods, our method is related to MixStyle [65] and EFDMix [59] due to the use of style transfer [13] to create novel data samples.",
                "In addition, both MixStyle and EFDMix rely on statistical prediction, which we believe, to be sensitive to domain shift [1, 28]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c2b8613eb0f7d5cdbf94b9a004fd92584f394024",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-03063",
                    "DOI": "10.48550/arXiv.2212.03063",
                    "CorpusId": 254275185
                },
                "corpusId": 254275185,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c2b8613eb0f7d5cdbf94b9a004fd92584f394024",
                "title": "Front-door Adjustment via Style Transfer for Out-of-distribution Generalisation",
                "abstract": "Out-of-distribution (OOD) generalisation aims to build a model that can well generalise its learnt knowledge from source domains to an unseen target domain. However, current image classi\ufb01cation models often perform poorly in the OOD setting due to statistically spurious correlations learning from model training. From causality-based perspective, we formulate the data generation process in OOD image classi\ufb01cation using a causal graph. On this graph, we show that prediction P ( Y | X ) of a label Y given an image X in statistical learning is formed by both causal effect P ( Y | do ( X )) and spurious effects caused by confounding features (e.g., background). Since the spurious features are domain-variant, the prediction P ( Y | X ) becomes unstable on unseen domains. In this paper, we propose to mitigate the spurious effect of confounders using front-door adjustment. In our method, the mediator variable is hypothesized as semantic features that are essential to determine a label for an image. Inspired by capability of style transfer in image generation, we interpret the combination of the mediator variable with different generated images in the front-door formula and propose novel algorithms to estimate it. Extensive experimental results on widely used benchmark datasets verify the effectiveness of our method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32125163",
                        "name": "Toan Q. Nguyen"
                    },
                    {
                        "authorId": "36072771",
                        "name": "Kien Do"
                    },
                    {
                        "authorId": "1779016",
                        "name": "D. Nguyen"
                    },
                    {
                        "authorId": "2159149299",
                        "name": "Bao Duong"
                    },
                    {
                        "authorId": "150322672",
                        "name": "T. Nguyen"
                    }
                ]
            }
        }
    ]
}