{
    "offset": 0,
    "data": [
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ef64008aabd82ea4aee2132a0bcec7b2eb5f3246",
                "externalIds": {
                    "DOI": "10.1016/j.aej.2023.09.002",
                    "CorpusId": 261795488
                },
                "corpusId": 261795488,
                "publicationVenue": {
                    "id": "b266d280-98ad-42f3-a028-bc3ab1233cea",
                    "name": "Alexandria Engineering Journal",
                    "type": "journal",
                    "alternate_names": [
                        "alexandria engineering journal",
                        "Alex Eng J",
                        "alex eng j"
                    ],
                    "issn": "1110-0168",
                    "url": "http://www.ees.elsevier.com/aej",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/alexandria-engineering-journal"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ef64008aabd82ea4aee2132a0bcec7b2eb5f3246",
                "title": "TrapezoidalNet: A new network architecture inspired from the numerical solution of ordinary differential equations",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "118549640",
                        "name": "Haoyu Chu"
                    },
                    {
                        "authorId": "46730712",
                        "name": "Shikui Wei"
                    },
                    {
                        "authorId": "2240238511",
                        "name": "Shunli Zhang"
                    },
                    {
                        "authorId": "2243577989",
                        "name": "Yao Zhao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ef50ab0ddc9e0bc496dc3a8d9a353016f4e8b16c",
                "externalIds": {
                    "DOI": "10.1016/j.chaos.2023.114057",
                    "CorpusId": 262173331
                },
                "corpusId": 262173331,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ef50ab0ddc9e0bc496dc3a8d9a353016f4e8b16c",
                "title": "Entropy structure informed learning for solving inverse problems of differential equations",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219696340",
                        "name": "Yan Jiang"
                    },
                    {
                        "authorId": "12835617",
                        "name": "Wuyue Yang"
                    },
                    {
                        "authorId": "2244353638",
                        "name": "Yi Zhu"
                    },
                    {
                        "authorId": "2218880286",
                        "name": "Liu Hong"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "fb4ca771de98cc6f3f2a8e2dbfa8420420c9399e",
                "externalIds": {
                    "DOI": "10.1016/j.neucom.2023.126884",
                    "CorpusId": 263811821
                },
                "corpusId": 263811821,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fb4ca771de98cc6f3f2a8e2dbfa8420420c9399e",
                "title": "Role of locality, fidelity and symmetry regularization in learning explainable representations",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2183810020",
                        "name": "Michele Ronco"
                    },
                    {
                        "authorId": "2256862557",
                        "name": "Gustau Camps-Valls"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Dynamical systems have been treated by learning the Lagrangian or Hamiltonian with correspondingly Lagrangian NNs [321\u2013323] and Hamiltonian NNs [324]."
            ],
            "citingPaper": {
                "paperId": "89e2413c3b5db4d48431e7d116c61ccff5e8d03e",
                "externalIds": {
                    "ArXiv": "2309.15421",
                    "CorpusId": 262940040
                },
                "corpusId": 262940040,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/89e2413c3b5db4d48431e7d116c61ccff5e8d03e",
                "title": "Deep Learning in Deterministic Computational Mechanics",
                "abstract": "The rapid growth of deep learning research, including within the field of computational mechanics, has resulted in an extensive and diverse body of literature. To help researchers identify key concepts and promising methodologies within this field, we provide an overview of deep learning in deterministic computational mechanics. Five main categories are identified and explored: simulation substitution, simulation enhancement, discretizations as neural networks, generative approaches, and deep reinforcement learning. This review focuses on deep learning methods rather than applications for computational mechanics, thereby enabling researchers to explore this field more effectively. As such, the review is not necessarily aimed at researchers with extensive knowledge of deep learning -- instead, the primary audience is researchers at the verge of entering this field or those who attempt to gain an overview of deep learning in computational mechanics. The discussed concepts are, therefore, explained as simple as possible.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "35562955",
                        "name": "L. Herrmann"
                    },
                    {
                        "authorId": "2244431426",
                        "name": "Stefan Kollmannsberger"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "54862991b85d492ed44db3e6d4cab4872aec1cb2",
                "externalIds": {
                    "ArXiv": "2309.13246",
                    "DBLP": "journals/corr/abs-2309-13246",
                    "DOI": "10.48550/arXiv.2309.13246",
                    "CorpusId": 262460301
                },
                "corpusId": 262460301,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/54862991b85d492ed44db3e6d4cab4872aec1cb2",
                "title": "Can I Trust the Explanations? Investigating Explainable Machine Learning Methods for Monotonic Models",
                "abstract": "In recent years, explainable machine learning methods have been very successful. Despite their success, most explainable machine learning methods are applied to black-box models without any domain knowledge. By incorporating domain knowledge, science-informed machine learning models have demonstrated better generalization and interpretation. But do we obtain consistent scientific explanations if we apply explainable machine learning methods to science-informed machine learning models? This question is addressed in the context of monotonic models that exhibit three different types of monotonicity. To demonstrate monotonicity, we propose three axioms. Accordingly, this study shows that when only individual monotonicity is involved, the baseline Shapley value provides good explanations; however, when strong pairwise monotonicity is involved, the Integrated gradients method provides reasonable explanations on average.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3430262",
                        "name": "Dangxing Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Given the positions and velocities as a function of time of a two-body system interacting with gravitational force, we trained Hamiltonian Neural Networks (HNN) (Greydanus et al., 2019) to match the data.",
                "Note that this set up is different from the original HNN paper where they train the network to match the velocity + acceleration, given position + velocity at every point in time.",
                "For the model, we are using Hamiltonian neural network (Greydanus et al., 2019) that consists of 6 linear layers with softplus activation except on the last linear layer."
            ],
            "citingPaper": {
                "paperId": "c170b8763cb9f53810fbf44717fb78d8c04bb023",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-12252",
                    "ArXiv": "2309.12252",
                    "DOI": "10.48550/arXiv.2309.12252",
                    "CorpusId": 262084002
                },
                "corpusId": 262084002,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c170b8763cb9f53810fbf44717fb78d8c04bb023",
                "title": "Parallelizing non-linear sequential models over the sequence length",
                "abstract": "Sequential models, such as Recurrent Neural Networks and Neural Ordinary Differential Equations, have long suffered from slow training due to their inherent sequential nature. For many years this bottleneck has persisted, as many thought sequential models could not be parallelized. We challenge this long-held belief with our parallel algorithm that accelerates GPU evaluation of sequential models by up to 3 orders of magnitude faster without compromising output accuracy. The algorithm does not need any special structure in the sequential models' architecture, making it applicable to a wide range of architectures. Using our method, training sequential models can be more than 10 times faster than the common sequential method without any meaningful difference in the training results. Leveraging this accelerated training, we discovered the efficacy of the Gated Recurrent Unit in a long time series classification problem with 17k time samples. By overcoming the training bottleneck, our work serves as the first step to unlock the potential of non-linear sequential models for long sequence problems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144833174",
                        "name": "Yi Heng Lim"
                    },
                    {
                        "authorId": "2243490246",
                        "name": "Qi Zhu"
                    },
                    {
                        "authorId": "2243336609",
                        "name": "Joshua Selfridge"
                    },
                    {
                        "authorId": "49443038",
                        "name": "M. F. Kasim"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "0fae1296a9ffa4dcbf310eb65d46089aa687a692",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-10418",
                    "ArXiv": "2309.10418",
                    "DOI": "10.48550/arXiv.2309.10418",
                    "CorpusId": 261945131
                },
                "corpusId": 261945131,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0fae1296a9ffa4dcbf310eb65d46089aa687a692",
                "title": "Graph Neural Networks for Dynamic Modeling of Roller Bearing",
                "abstract": "In the presented work, we propose to apply the framework of graph neural networks (GNNs) to predict the dynamics of a rolling element bearing. This approach offers generalizability and interpretability, having the potential for scalable use in real-time operational digital twin systems for monitoring the health state of rotating machines. By representing the bearing's components as nodes in a graph, the GNN can effectively model the complex relationships and interactions among them. We utilize a dynamic spring-mass-damper model of a bearing to generate the training data for the GNN. In this model, discrete masses represent bearing components such as rolling elements, inner raceways, and outer raceways, while a Hertzian contact model is employed to calculate the forces between these components. We evaluate the learning and generalization capabilities of the proposed GNN framework by testing different bearing configurations that deviate from the training configurations. Through this approach, we demonstrate the effectiveness of the GNN-based method in accurately predicting the dynamics of rolling element bearings, highlighting its potential for real-time health monitoring of rotating machinery.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2244667366",
                        "name": "Vinay Sharma"
                    },
                    {
                        "authorId": "2242002623",
                        "name": "Jens Ravesloot"
                    },
                    {
                        "authorId": "2683590",
                        "name": "C. Taal"
                    },
                    {
                        "authorId": "2248130898",
                        "name": "Olga Fink"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "1e81512aa187fc7372a6a9018ccf8d3c34eea3bf",
                "externalIds": {
                    "ArXiv": "2309.14350",
                    "CorpusId": 262822597
                },
                "corpusId": 262822597,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1e81512aa187fc7372a6a9018ccf8d3c34eea3bf",
                "title": "Training neural mapping schemes for satellite altimetry with simulation data",
                "abstract": "Satellite altimetry combined with data assimilation and optimal interpolation schemes have deeply renewed our ability to monitor sea surface dynamics. Recently, deep learning (DL) schemes have emerged as appealing solutions to address space-time interpolation problems. The scarcity of real altimetry dataset, in terms of space-time coverage of the sea surface, however impedes the training of state-of-the-art neural schemes on real-world case-studies. Here, we leverage both simulations of ocean dynamics and satellite altimeters to train simulation-based neural mapping schemes for the sea surface height and demonstrate their performance for real altimetry datasets. We analyze further how the ocean simulation dataset used during the training phase impacts this performance. This experimental analysis covers both the resolution from eddy-present configurations to eddy-rich ones, forced simulations vs. reanalyses using data assimilation and tide-free vs. tide-resolving simulations. Our benchmarking framework focuses on a Gulf Stream region for a realistic 5-altimeter constellation using NEMO ocean simulations and 4DVarNet mapping schemes. All simulation-based 4DVarNets outperform the operational observation-driven and reanalysis products, namely DUACS and GLORYS. The more realistic the ocean simulation dataset used during the training phase, the better the mapping. The best 4DVarNet mapping was trained from an eddy-rich and tide-free simulation datasets. It improves the resolved longitudinal scale from 151 kilometers for DUACS and 241 kilometers for GLORYS to 98 kilometers and reduces the root mean squared error (RMSE) by 23% and 61%. These results open research avenues for new synergies between ocean modelling and ocean observation using learning-based approaches.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1382652263",
                        "name": "Q. Febvre"
                    },
                    {
                        "authorId": "50520856",
                        "name": "J. Sommer"
                    },
                    {
                        "authorId": "9680774",
                        "name": "C. Ubelmann"
                    },
                    {
                        "authorId": "1731935",
                        "name": "Ronan Fablet"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "For example, Lagrangian [9]\u2013[13] and Hamiltonian formulations [14]\u2013[20] have been used to design neural network models to approximate dynamics of mechanical systems, where the equations of motions are enforced in the neural network architecture."
            ],
            "citingPaper": {
                "paperId": "7aa36227250d50758a26d48c251067fedac757b8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-09163",
                    "ArXiv": "2309.09163",
                    "DOI": "10.48550/arXiv.2309.09163",
                    "CorpusId": 262045133
                },
                "corpusId": 262045133,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7aa36227250d50758a26d48c251067fedac757b8",
                "title": "Hamiltonian Dynamics Learning from Point Cloud Observations for Nonholonomic Mobile Robot Control",
                "abstract": "Reliable autonomous navigation requires adapting the control policy of a mobile robot in response to dynamics changes in different operational conditions. Hand-designed dynamics models may struggle to capture model variations due to a limited set of parameters. Data-driven dynamics learning approaches offer higher model capacity and better generalization but require large amounts of state-labeled data. This paper develops an approach for learning robot dynamics directly from point-cloud observations, removing the need and associated errors of state estimation, while embedding Hamiltonian structure in the dynamics model to improve data efficiency. We design an observation-space loss that relates motion prediction from the dynamics model with motion prediction from point-cloud registration to train a Hamiltonian neural ordinary differential equation. The learned Hamiltonian model enables the design of an energy-shaping model-based tracking controller for rigid-body robots. We demonstrate dynamics learning and tracking control on a real nonholonomic wheeled robot.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2243183349",
                        "name": "Abdullah Altawaitan"
                    },
                    {
                        "authorId": "2242831838",
                        "name": "Jason Stanley"
                    },
                    {
                        "authorId": "1703620595",
                        "name": "Sambaran Ghosal"
                    },
                    {
                        "authorId": "2344318",
                        "name": "T. Duong"
                    },
                    {
                        "authorId": "2243186691",
                        "name": "Nikolay Atanasov"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "[2019a], Hamiltonian neural nets Greydanus et al. [2019]) have been successfully deployed in this domain. In these approaches, the spatial and temporal updates of a PDE model are matched with ground truth experimental observations. A loss function is de\ue000ned based on the mismatch between the simulation and the ground truth, and then the physics model is learned by back-propagation of error gradients of the loss function Xue et al. [2021a], Raissi et al.",
                "Physics model learning from data have been explored in Greydanus et al. [2019], Cranmer et al.",
                "[2019a], Hamiltonian neural nets Greydanus et al. [2019]) have been successfully deployed in this domain. In these approaches, the spatial and temporal updates of a PDE model are matched with ground truth experimental observations. A loss function is de\ue000ned based on the mismatch between the simulation and the ground truth, and then the physics model is learned by back-propagation of error gradients of the loss function Xue et al. [2021a], Raissi et al. [2019a]. Nevertheless, such learning processes are expensive because of the need to back-propagate gradients over spatial and temporal simulations involving millions of mutually interacting elements. One line of successful approaches to accelerate the learning of PDE models exploits the sparsity nature of system changes over time. For example, during the microstructure evolution of many engineering materials, only the boundary of the microstructure changes while large portion of the system remains unchanged. It is also assumed that the corresponding PDE models can be decomposed into af\ue000ne function of parameter functions and feature functions Nasim et al. [2022], Sima and Xue [2021]. The combination of decomposablity of the PDE model and sparse changes/updates over time together create opportunities for ef\ue000cient algorithms which handle learning in compressed spaces using random projections and/or locality sensitive hashing.",
                "Physics model learning from data have been explored in Greydanus et al. [2019], Cranmer et al. [2020b], Lutter et al. [2018], Niu et al.",
                "Physics model learning from data have been explored in Greydanus et al. [2019], Cranmer et al. [2020b], Lutter et al.",
                "[2019a], Hamiltonian neural nets Greydanus et al. [2019]) have been successfully deployed in this domain.",
                "[2019a], Hamiltonian neural nets Greydanus et al. [2019]) have been successfully deployed in this domain. In these approaches, the spatial and temporal updates of a PDE model are matched with ground truth experimental observations. A loss function is de\ue000ned based on the mismatch between the simulation and the ground truth, and then the physics model is learned by back-propagation of error gradients of the loss function Xue et al. [2021a], Raissi et al. [2019a]. Nevertheless, such learning processes are expensive because of the need to back-propagate gradients over spatial and temporal simulations involving millions of mutually interacting elements.",
                "[2019a], Hamiltonian neural nets Greydanus et al. [2019]) have been successfully deployed in this domain. In these approaches, the spatial and temporal updates of a PDE model are matched with ground truth experimental observations. A loss function is de\ue000ned based on the mismatch between the simulation and the ground truth, and then the physics model is learned by back-propagation of error gradients of the loss function Xue et al. [2021a], Raissi et al. [2019a]. Nevertheless, such learning processes are expensive because of the need to back-propagate gradients over spatial and temporal simulations involving millions of mutually interacting elements. One line of successful approaches to accelerate the learning of PDE models exploits the sparsity nature of system changes over time. For example, during the microstructure evolution of many engineering materials, only the boundary of the microstructure changes while large portion of the system remains unchanged. It is also assumed that the corresponding PDE models can be decomposed into af\ue000ne function of parameter functions and feature functions Nasim et al. [2022], Sima and Xue [2021]. The combination of decomposablity of the PDE model and sparse changes/updates over time together create opportunities for ef\ue000cient algorithms which handle learning in compressed spaces using random projections and/or locality sensitive hashing. Nevertheless, such decomposablity structure applies to a limited class of PDEs and sparsity structures may change with varying initial and boundary conditions (BC/IC). This paper propose a more general approach for ef\ue000ciently learning PDE models via random projection, by exploiting sparsity in both value domain and frequency domain, and also approximating nondecomposable functions with decomposable polynomials. We observe that, systems modeled by PDEs often have slow and gradual updates across wide regions in addition to a few rapid changes concentrated in small \u201cinterfacial\u201d regions. Such systems are frequently found in the real world. For example, during manufacturing processes such as laser sintering of powder materials into dense solids, grain boundary changes sharply at the interface area (sparse local change), while temperature rises gradually around the whole material (dense global change). Systems with dense global change and sharp interface change limit the application of existing approaches Nasim et al. [2022], Sima and Xue [2021] for ef\ue000ciently learning relevant PDE models.",
                "[2019a], Hamiltonian neural nets Greydanus et al. [2019]) have been successfully deployed in this domain. In these approaches, the spatial and temporal updates of a PDE model are matched with ground truth experimental observations. A loss function is de\ue000ned based on the mismatch between the simulation and the ground truth, and then the physics model is learned by back-propagation of error gradients of the loss function Xue et al. [2021a], Raissi et al. [2019a]. Nevertheless, such learning processes are expensive because of the need to back-propagate gradients over spatial and temporal simulations involving millions of mutually interacting elements. One line of successful approaches to accelerate the learning of PDE models exploits the sparsity nature of system changes over time. For example, during the microstructure evolution of many engineering materials, only the boundary of the microstructure changes while large portion of the system remains unchanged. It is also assumed that the corresponding PDE models can be decomposed into af\ue000ne function of parameter functions and feature functions Nasim et al. [2022], Sima and Xue [2021]."
            ],
            "citingPaper": {
                "paperId": "5ae70025b764c952c68d89128e105deab08b636f",
                "externalIds": {
                    "ArXiv": "2309.07344",
                    "DBLP": "journals/corr/abs-2309-07344",
                    "DOI": "10.48550/arXiv.2309.07344",
                    "CorpusId": 261822458
                },
                "corpusId": 261822458,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5ae70025b764c952c68d89128e105deab08b636f",
                "title": "Efficient Learning of PDEs via Taylor Expansion and Sparse Decomposition into Value and Fourier Domains",
                "abstract": "Accelerating the learning of Partial Differential Equations (PDEs) from experimental data will speed up the pace of scientific discovery. Previous randomized algorithms exploit sparsity in PDE updates for acceleration. However such methods are applicable to a limited class of decomposable PDEs, which have sparse features in the value domain. We propose Reel, which accelerates the learning of PDEs via random projection and has much broader applicability. Reel exploits the sparsity by decomposing dense updates into sparse ones in both the value and frequency domains. This decomposition enables efficient learning when the source of the updates consists of gradually changing terms across large areas (sparse in the frequency domain) in addition to a few rapid updates concentrated in a small set of\"interfacial\"regions (sparse in the value domain). Random projection is then applied to compress the sparse signals for learning. To expand the model applicability, Taylor series expansion is used in Reel to approximate the nonlinear PDE updates with polynomials in the decomposable form. Theoretically, we derive a constant factor approximation between the projected loss function and the original one with poly-logarithmic number of projected dimensions. Experimentally, we provide empirical evidence that our proposed Reel can lead to faster learning of PDE models (70-98% reduction in training time when the data is compressed to 1% of its original size) with comparable quality as the non-compressed models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2240533359",
                        "name": "Md Nasim"
                    },
                    {
                        "authorId": "2184948384",
                        "name": "Yexiang Xue"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recent advances have sought to alleviate these constraints by drawing inspiration from Hamiltonian systems [4, 5], a class of dynamical systems governed by Hamilton\u2019s equations."
            ],
            "citingPaper": {
                "paperId": "fe9e7e77b825845af9bd243484e7eae6fdda6ad2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-04885",
                    "ArXiv": "2309.04885",
                    "DOI": "10.48550/arXiv.2309.04885",
                    "CorpusId": 261682541
                },
                "corpusId": 261682541,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fe9e7e77b825845af9bd243484e7eae6fdda6ad2",
                "title": "Symplectic Structure-Aware Hamiltonian (Graph) Embeddings",
                "abstract": "In traditional Graph Neural Networks (GNNs), the assumption of a fixed embedding manifold often limits their adaptability to diverse graph geometries. Recently, Hamiltonian system-inspired GNNs are proposed to address the dynamic nature of such embeddings by incorporating physical laws into node feature updates. In this work, we present SAH-GNN, a novel approach that generalizes Hamiltonian dynamics for more flexible node feature updates. Unlike existing Hamiltonian-inspired GNNs, SAH-GNN employs Riemannian optimization on the symplectic Stiefel manifold to adaptively learn the underlying symplectic structure during training, circumventing the limitations of existing Hamiltonian GNNs that rely on a pre-defined form of standard symplectic structure. This innovation allows SAH-GNN to automatically adapt to various graph datasets without extensive hyperparameter tuning. Moreover, it conserves energy during training such that the implicit Hamiltonian system is physically meaningful. To this end, we empirically validate SAH-GNN's superior performance and adaptability in node classification tasks across multiple types of graph datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2239051861",
                        "name": "Jiaxu Liu"
                    },
                    {
                        "authorId": "2238955625",
                        "name": "Xinping Yi"
                    },
                    {
                        "authorId": "2239157687",
                        "name": "Tianle Zhang"
                    },
                    {
                        "authorId": "2107903140",
                        "name": "Xiaowei Huang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Researchers have employed machine learning models to discover Green\u2019s functions [11] [18], symmetries [11], Hamiltonian\u2019s [19], Dynamical Systems [13], Delay Dynamical Systems [31], and invariant quantities [40] directly from scientific data."
            ],
            "citingPaper": {
                "paperId": "2ae26ec2955057ddc37fb724b69449f48ba3a3cc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-04699",
                    "ArXiv": "2309.04699",
                    "DOI": "10.48550/arXiv.2309.04699",
                    "CorpusId": 261682112
                },
                "corpusId": 261682112,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2ae26ec2955057ddc37fb724b69449f48ba3a3cc",
                "title": "Weak-PDE-LEARN: A Weak Form Based Approach to Discovering PDEs From Noisy, Limited Data",
                "abstract": "We introduce Weak-PDE-LEARN, a Partial Differential Equation (PDE) discovery algorithm that can identify non-linear PDEs from noisy, limited measurements of their solutions. Weak-PDE-LEARN uses an adaptive loss function based on weak forms to train a neural network, $U$, to approximate the PDE solution while simultaneously identifying the governing PDE. This approach yields an algorithm that is robust to noise and can discover a range of PDEs directly from noisy, limited measurements of their solutions. We demonstrate the efficacy of Weak-PDE-LEARN by learning several benchmark PDEs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2007163797",
                        "name": "R. Stephany"
                    },
                    {
                        "authorId": "2238953405",
                        "name": "Christopher Earls"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "27776d49cdb637a0fc3af3ea9bc8d2c22cfdd968",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-02873",
                    "ArXiv": "2309.02873",
                    "DOI": "10.48550/arXiv.2309.02873",
                    "CorpusId": 261557619
                },
                "corpusId": 261557619,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/27776d49cdb637a0fc3af3ea9bc8d2c22cfdd968",
                "title": "Learning Hybrid Dynamics Models With Simulator-Informed Latent States",
                "abstract": "Dynamics model learning deals with the task of inferring unknown dynamics from measurement data and predicting the future behavior of the system. A typical approach to address this problem is to train recurrent models. However, predictions with these models are often not physically meaningful. Further, they suffer from deteriorated behavior over time due to accumulating errors. Often, simulators building on first principles are available being physically meaningful by design. However, modeling simplifications typically cause inaccuracies in these models. Consequently, hybrid modeling is an emerging trend that aims to combine the best of both worlds. In this paper, we propose a new approach to hybrid modeling, where we inform the latent states of a learned model via a black-box simulator. This allows to control the predictions via the simulator preventing them from accumulating errors. This is especially challenging since, in contrast to previous approaches, access to the simulator's latent states is not available. We tackle the task by leveraging observers, a well-known concept from control theory, inferring unknown latent states from observations and dynamics over time. In our learning-based setting, we jointly learn the dynamics and an observer that infers the latent states via the simulator. Thus, the simulator constantly corrects the latent states, compensating for modeling mismatch caused by learning. To maintain flexibility, we train an RNN-based residuum for the latent states that cannot be informed by the simulator.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2047583803",
                        "name": "K. Ensinger"
                    },
                    {
                        "authorId": "3001733",
                        "name": "Sebastian Ziesche"
                    },
                    {
                        "authorId": "2715093",
                        "name": "Sebastian Trimpe"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "For instance, Hamiltonian NN [47] draws inspiration from Hamiltonian mechanics and trains models to respect exact conservation laws, resulting in better inductive biases."
            ],
            "citingPaper": {
                "paperId": "2aa8e47f9b5db501c6b0b65f75388aae57606119",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-01909",
                    "ArXiv": "2309.01909",
                    "DOI": "10.48550/arXiv.2309.01909",
                    "CorpusId": 261530044
                },
                "corpusId": 261530044,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2aa8e47f9b5db501c6b0b65f75388aae57606119",
                "title": "A Survey on Physics Informed Reinforcement Learning: Review and Open Problems",
                "abstract": "The inclusion of physical information in machine learning frameworks has revolutionized many application areas. This involves enhancing the learning process by incorporating physical constraints and adhering to physical laws. In this work we explore their utility for reinforcement learning applications. We present a thorough review of the literature on incorporating physics information, as known as physics priors, in reinforcement learning approaches, commonly referred to as physics-informed reinforcement learning (PIRL). We introduce a novel taxonomy with the reinforcement learning pipeline as the backbone to classify existing works, compare and contrast them, and derive crucial insights. Existing works are analyzed with regard to the representation/ form of the governing physics modeled for integration, their specific contribution to the typical reinforcement learning architecture, and their connection to the underlying reinforcement learning pipeline stages. We also identify core learning architectures and physics incorporation biases (i.e., observational, inductive and learning) of existing PIRL approaches and use them to further categorize the works for better understanding and adaptation. By providing a comprehensive perspective on the implementation of the physics-informed capability, the taxonomy presents a cohesive approach to PIRL. It identifies the areas where this approach has been applied, as well as the gaps and opportunities that exist. Additionally, the taxonomy sheds light on unresolved issues and challenges, which can guide future research. This nascent field holds great potential for enhancing reinforcement learning algorithms by increasing their physical plausibility, precision, data efficiency, and applicability in real-world scenarios.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2065387130",
                        "name": "C. Banerjee"
                    },
                    {
                        "authorId": "49254601",
                        "name": "Kien Nguyen"
                    },
                    {
                        "authorId": "3140440",
                        "name": "C. Fookes"
                    },
                    {
                        "authorId": "145401977",
                        "name": "M. Raissi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Motivated by prior research [5, 6], this study aims to develop a method that captures the inductive bias of energy changes in rigid bodies as external conditions vary while preserving the high-precision modeling of 6-DoF equations for rigid bodies and the high-precision forward and backward sliding along the temporal dimension.",
                "Building upon, the authors in [5, 6] proposed a deep generative model termed the Hamiltonian generative network (HGN), which can learn the Hamiltonian dynamics of continuous-time evolution systems, exhibiting features such as time reversibility and smooth temporal interpolation.",
                "For instance, the authors in [5] devised a robust induction bias for energy conservation, yielding an intriguing byproduct: time reversibility."
            ],
            "citingPaper": {
                "paperId": "91cec28c19dc5288f1a9f669a4a0f42ca9e566af",
                "externalIds": {
                    "DBLP": "journals/complexity/FeiLSHLJLQ23",
                    "DOI": "10.1155/2023/8882781",
                    "CorpusId": 261599042
                },
                "corpusId": 261599042,
                "publicationVenue": {
                    "id": "8bc59e8b-e251-4201-839a-ec83ae78859d",
                    "name": "Complex",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Complex Sci",
                        "International Conference on Complex Sciences"
                    ],
                    "issn": "0806-1912",
                    "alternate_issns": [
                        "1538-6848"
                    ],
                    "url": "http://wo.uio.no/as/WebObjects/nettlogg.woa/1/wa/logg?logg=5904",
                    "alternate_urls": [
                        "http://www.wikicfp.com/cfp/program?id=545",
                        "https://www.complex.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/91cec28c19dc5288f1a9f669a4a0f42ca9e566af",
                "title": "Hamiltonian Neural Network 6-DoF Rigid-Body Dynamic Modeling Based on Energy Variation Estimation",
                "abstract": "This study introduces a novel deep modeling approach that utilizes Hamiltonian neural networks to address the challenges of modeling the six degrees of freedom rigid-body dynamics induced by control inputs in various domains such as aerospace, robotics, and automotive engineering. The proposed method is based on the principles of Hamiltonian dynamics and employs an inductive bias in the form of a constructed bias for both conservative and varying energies, effectively tackling the modeling issues arising from time-varying energy in controlled rigid-body dynamics. This constructed bias captures the information regarding the changes in the rigid body\u2019s energy. The presented method not only achieves highly accurate modeling but also preserves the inherent bidirectional time-sliding inference in Hamiltonian-based modeling approaches. Experimental results demonstrate that our method outperforms existing techniques in the time-varying six degrees of freedom dynamic modeling of aircraft and missile guidance, enabling high-precision modeling and feedback correction. The findings of our research hold significant potential for the kinematic modeling of time-varying energy systems, parallel system state prediction and control, inverse motion inference, and autonomous decision-making in military applications.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2238340189",
                        "name": "Simiao Fei"
                    },
                    {
                        "authorId": "2238387804",
                        "name": "Huo Lin"
                    },
                    {
                        "authorId": "1978589018",
                        "name": "Zhixiao Sun"
                    },
                    {
                        "authorId": "2238400934",
                        "name": "Wang He"
                    },
                    {
                        "authorId": "2238311818",
                        "name": "Yuanjie Lu"
                    },
                    {
                        "authorId": "2238335404",
                        "name": "He Jile"
                    },
                    {
                        "authorId": "2238340425",
                        "name": "Luo Qing"
                    },
                    {
                        "authorId": "2238341209",
                        "name": "Qihang Su"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "We adapt Hamiltonian neural networks (HNNs) [2], [3] (leftmost, Figure 2) for the task of regressing the Hamiltonian and vector field of a Hamiltonian system from random samples of the vector field.",
                "Hamiltonian neural networks leverage learning biases and use Hamilton\u2019s equations as soft constraints in the loss function of the neural network to favour convergence toward the Hamiltonian [2], [3].",
                "They use a learning bias [4] based on physics information regarding Hamilton\u2019s equations [2], [3] to aid the neural network in converging towards solutions that adhere to physics laws [4].",
                "Our work emulates theirs, by embedding Hamilton\u2019s equations within the loss function of a neural network to regress the Hamiltonian [3] and using automatic differentiation of the regressed Hamiltonian to yield the regressed vector field [3].",
                "[3] independently use physics-informed machine learning methods to regress the value of the Hamiltonian from multiple evenly-spaced samples along multiple Hamiltonian trajectories.",
                "A recent advancement in the modelling of dynamical systems is Hamiltonian neural networks [2], [3], which are physics-informed neural networks with learning biases given by Hamilton\u2019s equations and their corollaries [4]."
            ],
            "citingPaper": {
                "paperId": "8dd7bdabbb3617eade42908ff416f54993906c34",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-01069",
                    "ArXiv": "2309.01069",
                    "DOI": "10.48550/arXiv.2309.01069",
                    "CorpusId": 261531273
                },
                "corpusId": 261531273,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8dd7bdabbb3617eade42908ff416f54993906c34",
                "title": "Separable Hamiltonian Neural Networks",
                "abstract": "The modelling of dynamical systems from discrete observations is a challenge faced by modern scientific and engineering data systems. Hamiltonian systems are one such fundamental and ubiquitous class of dynamical systems. Hamiltonian neural networks are state-of-the-art models that unsupervised-ly regress the Hamiltonian of a dynamical system from discrete observations of its vector field under the learning bias of Hamilton's equations. Yet Hamiltonian dynamics are often complicated, especially in higher dimensions where the state space of the Hamiltonian system is large relative to the number of samples. A recently discovered remedy to alleviate the complexity between state variables in the state space is to leverage the additive separability of the Hamiltonian system and embed that additive separability into the Hamiltonian neural network. Following the nomenclature of physics-informed machine learning, we propose three separable Hamiltonian neural networks. These models embed additive separability within Hamiltonian neural networks. The first model uses additive separability to quadratically scale the amount of data for training Hamiltonian neural networks. The second model embeds additive separability within the loss function of the Hamiltonian neural network. The third model embeds additive separability through the architecture of the Hamiltonian neural network using conjoined multilayer perceptions. We empirically compare the three models against state-of-the-art Hamiltonian neural networks, and demonstrate that the separable Hamiltonian neural networks, which alleviate complexity between the state variables, are more effective at regressing the Hamiltonian and its vector field.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2041872418",
                        "name": "Zi-Yu Khoo"
                    },
                    {
                        "authorId": "2237809772",
                        "name": "Jonathan Sze Choong Low"
                    },
                    {
                        "authorId": "2237808597",
                        "name": "St'ephane Bressan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ee739ef43cd1043da1b6a3f8edb5e6dbeac38410",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-16331",
                    "ArXiv": "2308.16331",
                    "DOI": "10.48550/arXiv.2308.16331",
                    "CorpusId": 261394438
                },
                "corpusId": 261394438,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ee739ef43cd1043da1b6a3f8edb5e6dbeac38410",
                "title": "Symmetry Preservation in Hamiltonian Systems: Simulation and Learning",
                "abstract": "This work presents a general geometric framework for simulating and learning the dynamics of Hamiltonian systems that are invariant under a Lie group of transformations. This means that a group of symmetries is known to act on the system respecting its dynamics and, as a consequence, Noether's Theorem, conserved quantities are observed. We propose to simulate and learn the mappings of interest through the construction of $G$-invariant Lagrangian submanifolds, which are pivotal objects in symplectic geometry. A notable property of our constructions is that the simulated/learned dynamics also preserves the same conserved quantities as the original system, resulting in a more faithful surrogate of the original dynamics than non-symmetry aware methods, and in a more accurate predictor of non-observed trajectories. Furthermore, our setting is able to simulate/learn not only Hamiltonian flows, but any Lie group-equivariant symplectic transformation. Our designs leverage pivotal techniques and concepts in symplectic geometry and geometric mechanics: reduction theory, Noether's Theorem, Lagrangian submanifolds, momentum mappings, and coisotropic reduction among others. We also present methods to learn Poisson transformations while preserving the underlying geometry and how to endow non-geometric integrators with geometric properties. Thus, this work presents a novel attempt to harness the power of symplectic and Poisson geometry towards simulating and learning problems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2057187058",
                        "name": "M. Vaquero"
                    },
                    {
                        "authorId": "2225889796",
                        "name": "Jorge Cort'es"
                    },
                    {
                        "authorId": "2211477922",
                        "name": "David Mart\u00edn de Diego"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e331ce1df66f0d358c19808a1f0a32240716d024",
                "externalIds": {
                    "ArXiv": "2308.15653",
                    "CorpusId": 261338811
                },
                "corpusId": 261338811,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e331ce1df66f0d358c19808a1f0a32240716d024",
                "title": "Statistical methods for resolving poor uncertainty quantification in machine learning interatomic potentials",
                "abstract": "Machine learning interatomic potentials (MLIPs) are promising surrogates for quantum mechanics evaluations in ab-initio molecular dynamics simulations due to their ability to reproduce the energy and force landscape within chemical accuracy at four orders of magnitude less cost. While developing uncertainty quantification (UQ) tools for MLIPs is critical to build production MLIP datasets using active learning, only limited progress has been made and the most robust method, ensembling, still shows low correlation between high error and high uncertainty predictions. Here we develop a rigorous method rooted in statistics for determining an error cutoff that distinguishes regions of high and low UQ performance. The statistical cutoff illuminates that a main cause of the poor UQ performance is due to the machine learning model already describing the entire dataset and not having any datapoints with error greater than the statistical error distribution. Second, we extend the statistical analysis to create an interpretable connection between the error and uncertainty distributions to predict an uncertainty cutoff separating high and low errors. We showcase the statistical cutoff in active learning benchmarks on two datasets of varying chemical complexity for three common UQ methods: ensembling, sparse Gaussian processes, and latent distance metrics and compare them to the true error and random sampling, showing that the statistical cutoff is generalizable to a variety of different UQ methods and protocols and performs similarly to using the true error. Importantly, we conclude that utilizing this uncertainty cutoff enables using significantly lower cost uncertainty quantification tools such as sparse gaussian processes and latent distances compared to ensembling approaches for generating MLIP datasets at a fraction of the cost.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "116325274",
                        "name": "Emil Annevelink"
                    },
                    {
                        "authorId": "2631003",
                        "name": "V. Viswanathan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "To that end, we begin by highlighting the Hamiltonian neural network (HNN) framework [13], in which the central idea is energy-based modeling."
            ],
            "citingPaper": {
                "paperId": "89d480a9bf884e736c0779a23e8e67f81c484b8e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-13835",
                    "ArXiv": "2308.13835",
                    "DOI": "10.48550/arXiv.2308.13835",
                    "CorpusId": 261242428
                },
                "corpusId": 261242428,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/89d480a9bf884e736c0779a23e8e67f81c484b8e",
                "title": "Deep Learning for Structure-Preserving Universal Stable Koopman-Inspired Embeddings for Nonlinear Canonical Hamiltonian Dynamics",
                "abstract": "Discovering a suitable coordinate transformation for nonlinear systems enables the construction of simpler models, facilitating prediction, control, and optimization for complex nonlinear systems. To that end, Koopman operator theory offers a framework for global linearization for nonlinear systems, thereby allowing the usage of linear tools for design studies. In this work, we focus on the identification of global linearized embeddings for canonical nonlinear Hamiltonian systems through a symplectic transformation. While this task is often challenging, we leverage the power of deep learning to discover the desired embeddings. Furthermore, to overcome the shortcomings of Koopman operators for systems with continuous spectra, we apply the lifting principle and learn global cubicized embeddings. Additionally, a key emphasis is paid to enforce the bounded stability for the dynamics of the discovered embeddings. We demonstrate the capabilities of deep learning in acquiring compact symplectic coordinate transformation and the corresponding simple dynamical models, fostering data-driven learning of nonlinear canonical Hamiltonian systems, even those with continuous spectra.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48827566",
                        "name": "P. Goyal"
                    },
                    {
                        "authorId": "1446452284",
                        "name": "S\u00fcleyman Y\u0131ld\u0131z"
                    },
                    {
                        "authorId": "144175464",
                        "name": "P. Benner"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Representation models include Lagrangian Neural Networks (LNN) [8, 23], Hamiltonian neural networks (HNN) [12], and Neural ODE [4, 13]."
            ],
            "citingPaper": {
                "paperId": "2a2a853ad53746433f9d11b4e19995195e86f5ee",
                "externalIds": {
                    "ArXiv": "2308.13212",
                    "DBLP": "journals/corr/abs-2308-13212",
                    "DOI": "10.48550/arXiv.2308.13212",
                    "CorpusId": 261214421
                },
                "corpusId": 261214421,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2a2a853ad53746433f9d11b4e19995195e86f5ee",
                "title": "Physics-Inspired Neural Graph ODE for Long-term Dynamical Simulation",
                "abstract": "Simulating and modeling the long-term dynamics of multi-object physical systems is an essential and challenging task. Current studies model the physical systems utilizing Graph Neural Networks (GNNs) with equivariant properties. Specifically, they model the dynamics as a sequence of discrete states with a fixed time interval and learn a direct mapping for all the two adjacent states. However, this direct mapping overlooks the continuous nature between the two states. Namely, we have verified that there are countless possible trajectories between two discrete dynamic states in current GNN-based direct mapping models. This issue greatly hinders the model generalization ability, leading to poor performance of the long-term simulation. In this paper, to better model the latent trajectory through discrete supervision signals, we propose a Physics-Inspired Neural Graph ODE (PINGO) algorithm. In PINGO, to ensure the uniqueness of the trajectory, we construct a Physics-Inspired Neural ODE framework to update the latent trajectory. Meanwhile, to effectively capture intricate interactions among objects, we use a GNN-based model to parameterize Neural ODE in a plug-and-play manner. Furthermore, we prove that the discrepancy between the learned trajectory of PIGNO and the true trajectory can be theoretically bounded. Extensive experiments verify our theoretical findings and demonstrate that our model yields an order-of-magnitude improvement over the state-of-the-art baselines, especially on long-term predictions and roll-out errors.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152797198",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "2173905619",
                        "name": "Jiashun Cheng"
                    },
                    {
                        "authorId": "2234137843",
                        "name": "Haihong Zhao"
                    },
                    {
                        "authorId": "1754673",
                        "name": "Tingyang Xu"
                    },
                    {
                        "authorId": "144259957",
                        "name": "P. Zhao"
                    },
                    {
                        "authorId": "2173360",
                        "name": "F. Tsung"
                    },
                    {
                        "authorId": "2118371946",
                        "name": "Jia Li"
                    },
                    {
                        "authorId": "2188347331",
                        "name": "Yu Rong"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Previous efforts in learning dynamics from images [23, 47, 4, 54] consider only 2D planar systems (e.",
                "[23] predict symplectic gradients of a Hamiltonian system using a Hamiltonian parameterized by a neural network.",
                "This is a generalization of the existing literature where dynamics of canonical Hamiltonian systems are learned with the canonical symplectic form as the physics prior [23, 16, 13, 47].",
                "This formulation has been used by several authors to learn unknown dynamics: the Hamiltonian structure (canonical symplectic form) is used as a physics prior and the unknown dynamics are uncovered by learning the Hamiltonian [23, 56, 47].",
                "A growing body of work incorporates Hamiltonian and Lagrangian formalisms to improve the accuracy and interpretability of learned representations in neural network based dynamical systems forecasting [23, 13, 16].",
                "[13] improve the long-term prediction performance of [23] by minimizing the MSE over predicted state trajectories rather than one-step symplectic gradients."
            ],
            "citingPaper": {
                "paperId": "9a6b88c80decebcb322227432e3712a4af927cae",
                "externalIds": {
                    "ArXiv": "2308.14666",
                    "DBLP": "journals/corr/abs-2308-14666",
                    "DOI": "10.48550/arXiv.2308.14666",
                    "CorpusId": 259089395
                },
                "corpusId": 259089395,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9a6b88c80decebcb322227432e3712a4af927cae",
                "title": "Learning to predict 3D rotational dynamics from images of a rigid body with unknown mass distribution",
                "abstract": "In many real-world settings, image observations of freely rotating 3D rigid bodies, may be available when low-dimensional measurements are not. However, the high-dimensionality of image data precludes the use of classical estimation techniques to learn the dynamics. The usefulness of standard deep learning methods is also limited because an image of a rigid body reveals nothing about the distribution of mass inside the body, which, together with initial angular velocity, is what determines how the body will rotate. We present a physics-informed neural network model to estimate and predict 3D rotational dynamics from image sequences. We achieve this using a multi-stage prediction pipeline that maps individual images to a latent representation homeomorphic to $\\mathbf{SO}(3)$, computes angular velocities from latent pairs, and predicts future latent states using the Hamiltonian equations of motion. We demonstrate the efficacy of our approach on new rotating rigid-body datasets of sequences of synthetic images of rotating objects, including cubes, prisms and satellites, with unknown uniform and non-uniform mass distributions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "34819822",
                        "name": "J. Mason"
                    },
                    {
                        "authorId": "1405669277",
                        "name": "Christine Allen-Blanchette"
                    },
                    {
                        "authorId": "102569860",
                        "name": "Nick Zolman"
                    },
                    {
                        "authorId": "2178760411",
                        "name": "Elizabeth Davison"
                    },
                    {
                        "authorId": "47583055",
                        "name": "N. Leonard"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "98c05257d4bbc7c014f1639c1d7fa851af8c5ab1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-11216",
                    "ArXiv": "2308.11216",
                    "DOI": "10.48550/arXiv.2308.11216",
                    "CorpusId": 261064856
                },
                "corpusId": 261064856,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/98c05257d4bbc7c014f1639c1d7fa851af8c5ab1",
                "title": "Hamiltonian GAN",
                "abstract": "A growing body of work leverages the Hamiltonian formalism as an inductive bias for physically plausible neural network based video generation. The structure of the Hamiltonian ensures conservation of a learned quantity (e.g., energy) and imposes a phase-space interpretation on the low-dimensional manifold underlying the input video. While this interpretation has the potential to facilitate the integration of learned representations in downstream tasks, existing methods are limited in their applicability as they require a structural prior for the configuration space at design time. In this work, we present a GAN-based video generation pipeline with a learned configuration space map and Hamiltonian neural network motion model, to learn a representation of the configuration space from data. We train our model with a physics-inspired cyclic-coordinate loss function which encourages a minimal representation of the configuration space and improves interpretability. We demonstrate the efficacy and advantages of our approach on the Hamiltonian Dynamics Suite Toy Physics dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1405669277",
                        "name": "Christine Allen-Blanchette"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "1d80348b2fda9ffefbeb08c87520ca8fc62f37e2",
                "externalIds": {
                    "ArXiv": "2308.05082",
                    "DBLP": "journals/corr/abs-2308-05082",
                    "DOI": "10.48550/arXiv.2308.05082",
                    "CorpusId": 260735595
                },
                "corpusId": 260735595,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1d80348b2fda9ffefbeb08c87520ca8fc62f37e2",
                "title": "Learning of discrete models of variational PDEs from data",
                "abstract": "We show how to learn discrete field theories from observational data of fields on a space-time lattice. For this, we train a neural network model of a discrete Lagrangian density such that the discrete Euler--Lagrange equations are consistent with the given training data. We, thus, obtain a structure-preserving machine learning architecture. Lagrangian densities are not uniquely defined by the solutions of a field theory. We introduce a technique to derive regularisers for the training process which optimise numerical regularity of the discrete field theory. Minimisation of the regularisers guarantees that close to the training data the discrete field theory behaves robust and efficient when used in numerical simulations. Further, we show how to identify structurally simple solutions of the underlying continuous field theory such as travelling waves. This is possible even when travelling waves are not present in the training data. This is compared to data-driven model order reduction based approaches, which struggle to identify suitable latent spaces containing structurally simple solutions when these are not present in the training data. Ideas are demonstrated on examples based on the wave equation and the Schr\\\"odinger equation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51299251",
                        "name": "Christian Offen"
                    },
                    {
                        "authorId": "1419469651",
                        "name": "S. Ober-Blobaum"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "The latter in turn may be done by the combination of raw data and physical equations [27, 28], by enforcing a metriplectic structure to the model, related with the fulfillment of thermodynamic laws [29] or by defining the specific structure of the model [30, 31]."
            ],
            "citingPaper": {
                "paperId": "2cd98409836dafde0e04eb332430c7a165666e6b",
                "externalIds": {
                    "ArXiv": "2308.03915",
                    "DBLP": "journals/corr/abs-2308-03915",
                    "DOI": "10.48550/arXiv.2308.03915",
                    "CorpusId": 260704650
                },
                "corpusId": 260704650,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2cd98409836dafde0e04eb332430c7a165666e6b",
                "title": "Predicting and explaining nonlinear material response using deep Physically Guided Neural Networks with Internal Variables",
                "abstract": "Nonlinear materials are often difficult to model with classical state model theory because they have a complex and sometimes inaccurate physical and mathematical description or we simply do not know how to describe such materials in terms of relations between external and internal variables. In many disciplines, Neural Network methods have arisen as powerful tools to identify very complex and non-linear correlations. In this work, we use the very recently developed concept of Physically Guided Neural Networks with Internal Variables (PGNNIV) to discover constitutive laws using a model-free approach and training solely with measured force-displacement data. PGNNIVs make a particular use of the physics of the problem to enforce constraints on specific hidden layers and are able to make predictions without internal variable data. We demonstrate that PGNNIVs are capable of predicting both internal and external variables under unseen load scenarios, regardless of the nature of the material considered (linear, with hardening or softening behavior and hyperelastic), unravelling the constitutive law of the material hence explaining its nature altogether, placing the method in what is known as eXplainable Artificial Intelligence (XAI).",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2230392547",
                        "name": "Javier Orera-Echeverria"
                    },
                    {
                        "authorId": "2230412626",
                        "name": "Jacobo Ayensa-Jim'enez"
                    },
                    {
                        "authorId": "2194768",
                        "name": "M. Doblar\u00e9"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "8 Objective and Design of the Experiments We have chosen two Hamiltonian neural networks (HNNs)[19] from \u201dHamiltonian Neural Networks for Solving Equations of Motion\u201d by M.",
                "1 Introduction to Hamiltonian Neural Networks Hamiltonian neural networks (HNNs), as proposed in the literature [28, 19, 8, 7, 11], introduce a novel approach to solving differential equations that describe dynamical systems."
            ],
            "citingPaper": {
                "paperId": "0d2194a33325dffab42fb7c0994dae1539e9ac7d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-03128",
                    "ArXiv": "2308.03128",
                    "DOI": "10.48550/arXiv.2308.03128",
                    "CorpusId": 260683079
                },
                "corpusId": 260683079,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0d2194a33325dffab42fb7c0994dae1539e9ac7d",
                "title": "Iterative Magnitude Pruning as a Renormalisation Group: A Study in The Context of The Lottery Ticket Hypothesis",
                "abstract": "This thesis delves into the intricate world of Deep Neural Networks (DNNs), focusing on the exciting concept of the Lottery Ticket Hypothesis (LTH). The LTH posits that within extensive DNNs, smaller, trainable subnetworks termed\"winning tickets\", can achieve performance comparable to the full model. A key process in LTH, Iterative Magnitude Pruning (IMP), incrementally eliminates minimal weights, emulating stepwise learning in DNNs. Once we identify these winning tickets, we further investigate their\"universality\". In other words, we check if a winning ticket that works well for one specific problem could also work well for other, similar problems. We also bridge the divide between the IMP and the Renormalisation Group (RG) theory in physics, promoting a more rigorous understanding of IMP.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2227939328",
                        "name": "Abu-Al Hassan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "559f67393973ea5b49831022a72e231ccaa9f998",
                "externalIds": {
                    "DBLP": "conf/kdd/LiWL23",
                    "DOI": "10.1145/3580305.3599858",
                    "CorpusId": 260499691
                },
                "corpusId": 260499691,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/559f67393973ea5b49831022a72e231ccaa9f998",
                "title": "Learning Slow and Fast System Dynamics via Automatic Separation of Time Scales",
                "abstract": "Learning the underlying slow and fast dynamics of a system is instrumental for many practical applications related to the system. However, existing approaches are limited in discovering the appropriate time scale to separate the slow and fast variables and effectively learning their dynamics based on correct-dimensional representation vectors. In this paper, we introduce a framework that effectively learns slow and fast system dynamics in an integrated manner. We propose a novel intrinsic dimensionality (ID) driven learning method based on a time-lagged autoencoder framework to identify appropriate time scales to separate slow and fast variables and their IDs simultaneously. Further, we propose an integrated framework to concurrently learn the system's slow and fast dynamics, which is able to integrate prior knowledge of time scale and IDs and model the complex coupled slow and fast variables. Extensive experimental results on two representative dynamical systems show that our proposed framework is able to efficiently learn slow and fast system dynamics. Specifically, the long-time prediction performance is able to be improved by 36% on average compared with four representative baselines based on our proposed framework. Furthermore, our proposed system is able to extract interpretable slow and fast dynamics highly correlated with the known slow and fast variables in the dynamical systems. Our codes and datasets are open-sourced at: https://github.com/tsinghua-fib-lab/SlowFastSeparation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "153071890",
                        "name": "Ruikun Li"
                    },
                    {
                        "authorId": "2255821",
                        "name": "Huandong Wang"
                    },
                    {
                        "authorId": "2154403414",
                        "name": "Yong Li"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a4a9e888f547d3373feda26bf07c88bb670333dd",
                "externalIds": {
                    "ArXiv": "2308.02733",
                    "CorpusId": 260681182
                },
                "corpusId": 260681182,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a4a9e888f547d3373feda26bf07c88bb670333dd",
                "title": "Variable-moment fluid closures with Hamiltonian structure",
                "abstract": "Based on ideas due to Scovel-Weinstein, I present a general framework for constructing fluid moment closures of the Vlasov-Poisson system that exactly preserve that system's Hamiltonian structure. Notably, the technique applies in any space dimension and produces closures involving arbitrarily-large finite collections of moments. After selecting a desired collection of moments, the Poisson bracket for the closure is uniquely determined. Therefore data-driven fluid closures can be constructed by adjusting the closure Hamiltonian for compatibility with kinetic simulations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2264263",
                        "name": "J. Burby"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "in [42] introduce Hamiltonian Neural Networks (HNNs).",
                "[42] Hamiltonian mechanics neural network self-constructed trajectory data, [133], pixel observations modeling of problems where conservation of energy is important loss functions, MSE PGML"
            ],
            "citingPaper": {
                "paperId": "5815075c70bf5113d4ab2ab4fc078913674a27ce",
                "externalIds": {
                    "DOI": "10.1145/3611383",
                    "CorpusId": 260380494
                },
                "corpusId": 260380494,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5815075c70bf5113d4ab2ab4fc078913674a27ce",
                "title": "Machine Learning and Physics: A Survey of Integrated Models",
                "abstract": "Predictive modeling of various systems around the world is extremely essential from the physics and engineering perspectives. The recognition of different systems and the capacity to predict their future behavior can lead to numerous significant applications. For the most part, physics is frequently used to model different systems. Using physical modeling can also very well help the resolution of complexity and achieve superior performance with the emerging field of novel artificial intelligence and the challenges associated with it. Physical modeling provides data and knowledge that offer meaningful and complementary understanding about the system. So, by using enriched data and training phases, the overall general integrated model achieves enhanced accuracy. The effectiveness of hybrid physics-guided or machine learning-guided models has been validated by experimental results of diverse use cases. Increased accuracy, interpretability, and transparency are the results of such hybrid models. In this paper, we provide a detailed overview of how machine learning and physics can be integrated into an interactive approach. Regarding this, we propose a classification of possible interactions between physical modeling and machine learning techniques. Our classification includes three types of approaches: (1) physics-guided machine learning (2) machine learning-guided physics, and (3) mutually-guided physics and ML. We studied the models and specifications for each of these three approaches in-depth for this survey.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2226458277",
                        "name": "Azra Seyyedi"
                    },
                    {
                        "authorId": "2793286",
                        "name": "M. Bohlouli"
                    },
                    {
                        "authorId": "2199182748",
                        "name": "SeyedEhsan Nedaaee Oskoee"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "2ec52b6a447d8a98026d7d89fc4e6f4aa8236f84",
                "externalIds": {
                    "ArXiv": "2308.01084",
                    "DBLP": "journals/corr/abs-2308-01084",
                    "DOI": "10.48550/arXiv.2308.01084",
                    "CorpusId": 260378674
                },
                "corpusId": 260378674,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2ec52b6a447d8a98026d7d89fc4e6f4aa8236f84",
                "title": "Data-Driven Identification of Quadratic Symplectic Representations of Nonlinear Hamiltonian Systems",
                "abstract": "We present a framework for learning Hamiltonian systems using data. This work is based on the lifting hypothesis, which posits that nonlinear Hamiltonian systems can be written as nonlinear systems with cubic Hamiltonians. By leveraging this, we obtain quadratic dynamics that are Hamiltonian in a transformed coordinate system. To that end, for given generalized position and momentum data, we propose a methodology to learn quadratic dynamical systems, enforcing the Hamiltonian structure in combination with a symplectic auto-encoder. The enforced Hamiltonian structure exhibits long-term stability of the system, while the cubic Hamiltonian function provides relatively low model complexity. For low-dimensional data, we determine a higher-order transformed coordinate system, whereas, for high-dimensional data, we find a lower-order coordinate system with the desired properties. We demonstrate the proposed methodology by means of both low-dimensional and high-dimensional nonlinear Hamiltonian systems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1446452284",
                        "name": "S\u00fcleyman Y\u0131ld\u0131z"
                    },
                    {
                        "authorId": "48827566",
                        "name": "P. Goyal"
                    },
                    {
                        "authorId": "51954062",
                        "name": "Thomas Bendokat"
                    },
                    {
                        "authorId": "144175464",
                        "name": "P. Benner"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "By integrating knowledge from analytical mechanics, neural networks can learn the dynamics that adheres to physical laws, such as the conservation of energy, and even uncover these laws from data [8, 10, 28]."
            ],
            "citingPaper": {
                "paperId": "42aa9c9af54037cd019e95f12fc37373f9c9c221",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-13869",
                    "ArXiv": "2307.13869",
                    "DOI": "10.48550/arXiv.2307.13869",
                    "CorpusId": 260164906
                },
                "corpusId": 260164906,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/42aa9c9af54037cd019e95f12fc37373f9c9c221",
                "title": "Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory",
                "abstract": "Physics-informed neural networks (PINNs) offer a novel and efficient approach to solving partial differential equations (PDEs). Their success lies in the physics-informed loss, which trains a neural network to satisfy a given PDE at specific points and to approximate the solution. However, the solutions to PDEs are inherently infinite-dimensional, and the distance between the output and the solution is defined by an integral over the domain. Therefore, the physics-informed loss only provides a finite approximation, and selecting appropriate collocation points becomes crucial to suppress the discretization errors, although this aspect has often been overlooked. In this paper, we propose a new technique called good lattice training (GLT) for PINNs, inspired by number theoretic methods for numerical analysis. GLT offers a set of collocation points that are effective even with a small number of points and for multi-dimensional spaces. Our experiments demonstrate that GLT requires 2--20 times fewer collocation points (resulting in lower computational cost) than uniformly random sampling or Latin hypercube sampling, while achieving competitive performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144872058",
                        "name": "Takashi Matsubara"
                    },
                    {
                        "authorId": "2580628",
                        "name": "Takaharu Yaguchi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Another work in the same vein as our method is Hamiltonian Neural Networks [6], which parameterises a vector field which conserves energy by formulating it as the symplectic gradient of an energy function."
            ],
            "citingPaper": {
                "paperId": "96cbad7c44313982de419d0f82d46c007c644756",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-14439",
                    "ArXiv": "2307.14439",
                    "DOI": "10.48550/arXiv.2307.14439",
                    "CorpusId": 260203142
                },
                "corpusId": 260203142,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/96cbad7c44313982de419d0f82d46c007c644756",
                "title": "Fixed Integral Neural Networks",
                "abstract": "It is often useful to perform integration over learned functions represented by neural networks. However, this integration is usually performed numerically, as analytical integration over learned functions (especially neural networks) is generally viewed as intractable. In this work, we present a method for representing the analytical integral of a learned function $f$. This allows the exact integral of a neural network to be computed, and enables constrained neural networks to be parametrised by applying constraints directly to the integral. Crucially, we also introduce a method to constrain $f$ to be positive, a necessary condition for many applications (e.g. probability distributions, distance metrics, etc). Finally, we introduce several applications where our fixed-integral neural network (FINN) can be utilised.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2225232363",
                        "name": "Ryan Kortvelesy"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "1f87ba9d005c803cf6ae521b2bc4b692f28f8dcb",
                "externalIds": {
                    "DOI": "10.1109/PESGM52003.2023.10252786",
                    "CorpusId": 259104672
                },
                "corpusId": 259104672,
                "publicationVenue": {
                    "id": "b2cee8ae-3d79-4abb-bf4c-b31194b13960",
                    "name": "IEEE Power & Energy Society General Meeting",
                    "alternate_names": [
                        "IEEE Power  Energy Soc Gen Meet"
                    ],
                    "issn": "1944-9925",
                    "alternate_issns": [
                        "1944-9933"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000581",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=4584435"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1f87ba9d005c803cf6ae521b2bc4b692f28f8dcb",
                "title": "Learning Power System Dynamics with Nearly-Hamiltonian Neural Network",
                "abstract": "The ability to learn power system dynamic model and predict transient trajectories using data is crucial to realizing closed-loop control of the system with artificial intelligence. This paper proposes a Nearly-Hamiltonian neural network to predict transient trajectories and dynamic parameters of the power system by embedding energy conservation laws in the proposed neural network architecture. This inductive bias empowers the proposed model to learn the power system dynamics without explicitly using the exact functional form of the power system dynamic equations. The numerical study results on the single machine infinite bus system show that the proposed model produces accurate system trajectories and damping coefficient predictions. Furthermore, the proposed model significantly outperforms the baseline and Hamiltonian neural network.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2116576250",
                        "name": "Shao-Qun Zhang"
                    },
                    {
                        "authorId": "2159070",
                        "name": "N. Yu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "The latter\u2019s automatic encoding of priors enables dynamical behaviors to be learned by neural networks [6,7]."
            ],
            "citingPaper": {
                "paperId": "4502c63660f64f9b945d0eb5f6ef3c65b7dd9be2",
                "externalIds": {
                    "DOI": "10.1007/s11071-023-08672-8",
                    "CorpusId": 259851925
                },
                "corpusId": 259851925,
                "publicationVenue": {
                    "id": "10925c1c-0929-4ec5-8268-a8a52bd84631",
                    "name": "Nonlinear dynamics",
                    "type": "journal",
                    "alternate_names": [
                        "Nonlinear Dyn",
                        "Nonlinear Dynamics",
                        "Nonlinear dyn"
                    ],
                    "issn": "0924-090X",
                    "url": "http://www.springer.com/11071",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11071"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4502c63660f64f9b945d0eb5f6ef3c65b7dd9be2",
                "title": "Bidirectional dynamic neural networks with physical analyzability",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2223435842",
                        "name": "Changjun Li"
                    },
                    {
                        "authorId": "144344339",
                        "name": "F. Zhao"
                    },
                    {
                        "authorId": "2498428",
                        "name": "Xuguang Lan"
                    },
                    {
                        "authorId": "41170833",
                        "name": "Zhiqiang Tian"
                    },
                    {
                        "authorId": "145684851",
                        "name": "T. Tao"
                    },
                    {
                        "authorId": "2168017222",
                        "name": "Xuesong Mei"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "These approaches are known as Hamiltonian (HNN) [20, 21, 22, 14], and Lagrangian neural networks (LNN) [15, 16, 17], and Graph Neural ODEs [23, 18, 24].",
                "We observe that HGNN outperforms both HNN and HGN on both spring and pendulum systems.",
                "Note that the decoupling of kinetic and potential energies is implemented in HNN.",
                "5 shows the performance of HNN, HGN, and HGNN for spring and pendulum systems.",
                "While the performance of HNN has been demonstrated on several spring and pendulum systems, HGN [20] has been evaluated only on spring systems.",
                "Second, HGN [20] is a graph-based version of HNN, albeit without decoupling the kinetic and potential energies.",
                "The first, HNN [21], is a simple MLP that directly predicts the Hamiltonian of the system.",
                "Additional evaluation of the HGNN architecture is performed by comparing it with two baselines, namely, HNN (which is a physics-enforced MLP) and HGN, which does not decouple potential and kinetic energies (see Supplementary Materials) and on additional metrics such as energy and momentum error.",
                "[21] Samuel Greydanus, Misko Dzamba, and Jason Yosinski.",
                "Note that HNN is trained and evaluated on each of these systems separately, while HGN and HGNN are trained in only one system and inferred for all other systems by performing the forward simulation.",
                "We observe that HGNN significantly outperforms HGN and HNN in terms of rollout and energy error (see Supplementary Materials).",
                "\u2022HGNN\nParameter Value Node embedding dimension 5 Edge embedding dimension 5 Hidden layer neurons (MLP) 5\nNumber of hidden layers (MLP) 2 Activation function squareplus\nNumber of layers of message passing(pendulum) 2 Number of layers of message passing(spring) 1\nOptimizer ADAM Learning rate 1.0e\u22123\nBatch size 100\n\u2022HNN\nParameter Value Hidden layer neurons (MLP) 256\nNumber of hidden layers (MLP) 2 Activation function squareplus\nOptimizer ADAM Learning rate 1.0e\u22123\nBatch size 100\n\u2022HGN\nParameter Value Node embedding dimension 8 Edge embedding dimension 8 Hidden layer neurons (MLP) 16\nNumber of hidden layers (MLP) 2 Activation function squareplus\nNumber of layers of message passing 1 Optimizer ADAM\nLearning rate 1.0e\u22123 Batch size 100",
                "Despite best efforts, the HGN and HNN was unable to provide a forward trajectory for the hybrid system."
            ],
            "citingPaper": {
                "paperId": "5155214bbf623d05dc180c0d9f01e2c579ab3c0d",
                "externalIds": {
                    "ArXiv": "2307.05299",
                    "DBLP": "journals/corr/abs-2307-05299",
                    "DOI": "10.48550/arXiv.2307.05299",
                    "CorpusId": 259766711
                },
                "corpusId": 259766711,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5155214bbf623d05dc180c0d9f01e2c579ab3c0d",
                "title": "Discovering Symbolic Laws Directly from Trajectories with Hamiltonian Graph Neural Networks",
                "abstract": "The time evolution of physical systems is described by differential equations, which depend on abstract quantities like energy and force. Traditionally, these quantities are derived as functionals based on observables such as positions and velocities. Discovering these governing symbolic laws is the key to comprehending the interactions in nature. Here, we present a Hamiltonian graph neural network (HGNN), a physics-enforced GNN that learns the dynamics of systems directly from their trajectory. We demonstrate the performance of HGNN on n-springs, n-pendulums, gravitational systems, and binary Lennard Jones systems; HGNN learns the dynamics in excellent agreement with the ground truth from small amounts of data. We also evaluate the ability of HGNN to generalize to larger system sizes, and to hybrid spring-pendulum system that is a combination of two original systems (spring and pendulum) on which the models are trained independently. Finally, employing symbolic regression on the learned HGNN, we infer the underlying equations relating the energy functionals, even for complex systems such as the binary Lennard-Jones liquid. Our framework facilitates the interpretable discovery of interaction laws directly from physical system trajectories. Furthermore, this approach can be extended to other systems with topology-dependent dynamics, such as cells, polydisperse gels, or deformable bodies.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "15675757",
                        "name": "S. Bishnoi"
                    },
                    {
                        "authorId": "2131107456",
                        "name": "Ravinder Bhattoo"
                    },
                    {
                        "authorId": "2170276437",
                        "name": "Jayadeva"
                    },
                    {
                        "authorId": "1699732",
                        "name": "Sayan Ranu"
                    },
                    {
                        "authorId": "2144896197",
                        "name": "N. Krishnan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "c5c120bd24b9f5993201e0f6c5b1408aafe67b13",
                "externalIds": {
                    "DOI": "10.17163/ings.n30.2023.07",
                    "CorpusId": 259579896
                },
                "corpusId": 259579896,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c5c120bd24b9f5993201e0f6c5b1408aafe67b13",
                "title": "Evolutionary artificial neural network for temperature control in a batch polymerization reactor",
                "abstract": "The integration of artificial intelligence techniques introduces fresh perspectives in the implementation of these methods. This paper presents the combination of neural networks and evolutionary strategies to create what is known as evolutionary artificial neural networks (EANNs). In the process, the excitation function of neurons was modified to allow asexual reproduction. As a result, neurons evolved and developed significantly. The technique of a batch polymerization reactor temperature controller to produce polymethylmethacrylate (PMMA) by free radicals was compared with two different controls, such as PID and GMC, demonstrating that artificial intelligence-based controllers can be applied. These controllers provide better results than conventional controllers without creating transfer functions to the control process represented.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "82400569",
                        "name": "F. S\u00e1nchez-Ruiz"
                    },
                    {
                        "authorId": "2222160591",
                        "name": "Elizabeth Arg\u00fcelles Hernandez"
                    },
                    {
                        "authorId": "2141032607",
                        "name": "Jos\u00e9 Terrones-Salgado"
                    },
                    {
                        "authorId": "2222157962",
                        "name": "Luz Judith Fern\u00e1ndez Quiroz"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Work on discovering Lagrangian [10, 11] and Hamiltonian [12, 13] can also be found in the literature."
            ],
            "citingPaper": {
                "paperId": "f0d56f46f958202d3d2020b8e674fd1fcff96ec7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-15873",
                    "ArXiv": "2306.15873",
                    "DOI": "10.48550/arXiv.2306.15873",
                    "CorpusId": 259274773
                },
                "corpusId": 259274773,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f0d56f46f958202d3d2020b8e674fd1fcff96ec7",
                "title": "Discovering stochastic partial differential equations from limited data using variational Bayes inference",
                "abstract": "We propose a novel framework for discovering Stochastic Partial Differential Equations (SPDEs) from data. The proposed approach combines the concepts of stochastic calculus, variational Bayes theory, and sparse learning. We propose the extended Kramers-Moyal expansion to express the drift and diffusion terms of an SPDE in terms of state responses and use Spike-and-Slab priors with sparse learning techniques to efficiently and accurately discover the underlying SPDEs. The proposed approach has been applied to three canonical SPDEs, (a) stochastic heat equation, (b) stochastic Allen-Cahn equation, and (c) stochastic Nagumo equation. Our results demonstrate that the proposed approach can accurately identify the underlying SPDEs with limited data. This is the first attempt at discovering SPDEs from data, and it has significant implications for various scientific applications, such as climate modeling, financial forecasting, and chemical kinetics.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2196534611",
                        "name": "Yogesh Chandrakant Mathpati"
                    },
                    {
                        "authorId": "1389549891",
                        "name": "Tapas Tripura"
                    },
                    {
                        "authorId": "89478958",
                        "name": "R. Nayek"
                    },
                    {
                        "authorId": "3411759",
                        "name": "S. Chakraborty"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In a recent study [31], the idea of parameterization of a scalar function that represents the Hamiltonian in a conservative dynamic system by a neural network was explored."
            ],
            "citingPaper": {
                "paperId": "07a6b9cc0459cb9cbde03ac594fef0391fdccf47",
                "externalIds": {
                    "DOI": "10.1007/s11071-023-08618-0",
                    "CorpusId": 259677425
                },
                "corpusId": 259677425,
                "publicationVenue": {
                    "id": "10925c1c-0929-4ec5-8268-a8a52bd84631",
                    "name": "Nonlinear dynamics",
                    "type": "journal",
                    "alternate_names": [
                        "Nonlinear Dyn",
                        "Nonlinear Dynamics",
                        "Nonlinear dyn"
                    ],
                    "issn": "0924-090X",
                    "url": "http://www.springer.com/11071",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11071"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/07a6b9cc0459cb9cbde03ac594fef0391fdccf47",
                "title": "Hybrid modeling of a multidimensional coupled nonlinear system with integration of Hamiltonian mechanics",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "97588998",
                        "name": "A. Abbasi"
                    },
                    {
                        "authorId": "30849313",
                        "name": "P. N. Kambali"
                    },
                    {
                        "authorId": "144851044",
                        "name": "C. Nataraj"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "6cb1a49f248413f851eb6c345a2006aca8e96192",
                "externalIds": {
                    "DBLP": "conf/aaai/LiuSL23",
                    "DOI": "10.1609/aaai.v37i2.25265",
                    "CorpusId": 259720282
                },
                "corpusId": 259720282,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/6cb1a49f248413f851eb6c345a2006aca8e96192",
                "title": "Counterfactual Dynamics Forecasting - a New Setting of Quantitative Reasoning",
                "abstract": "Rethinking and introspection are important elements of human intelligence. To mimic these capabilities, counterfactual reasoning has attracted attention of AI researchers recently, which aims to forecast the alternative outcomes for hypothetical scenarios (\u201cwhat-if\u201d). However, most existing approaches focused on qualitative reasoning (e.g., casual-effect relationship). It lacks a well-defined description of the differences between counterfactuals and facts, as well as how these differences evolve over time. This paper defines a new problem formulation - counterfactual dynamics forecasting - which is described in middle-level abstraction under the structural causal models (SCM) framework and derived as ordinary differential equations (ODEs) as low-level quantitative computation. Based on it, we propose a method to infer counterfactual dynamics considering the factual dynamics as demonstration. Moreover, the evolution of differences between facts and counterfactuals are modelled by an explicit temporal component. The experimental results on two dynamical systems demonstrate the effectiveness of the proposed method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108172209",
                        "name": "Yanzhu Liu"
                    },
                    {
                        "authorId": "2000311149",
                        "name": "Ying Sun"
                    },
                    {
                        "authorId": "2109781618",
                        "name": "J. Lim"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Other approaches [1, 11, 17, 19, 33, 47] induce Hamiltonian and Lagrangian priors into neural networks exploiting the reformulations of Newton\u2019s equations of motion in energy-conservative dynamics."
            ],
            "citingPaper": {
                "paperId": "91721e0801310a00f3e750505e61519e1e0f985a",
                "externalIds": {
                    "ArXiv": "2306.12077",
                    "DBLP": "journals/corr/abs-2306-12077",
                    "DOI": "10.48550/arXiv.2306.12077",
                    "CorpusId": 259211859
                },
                "corpusId": 259211859,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/91721e0801310a00f3e750505e61519e1e0f985a",
                "title": "Learning Latent Dynamics via Invariant Decomposition and (Spatio-)Temporal Transformers",
                "abstract": "We propose a method for learning dynamical systems from high-dimensional empirical data that combines variational autoencoders and (spatio-)temporal attention within a framework designed to enforce certain scientifically-motivated invariances. We focus on the setting in which data are available from multiple different instances of a system whose underlying dynamical model is entirely unknown at the outset. The approach rests on a separation into an instance-specific encoding (capturing initial conditions, constants etc.) and a latent dynamics model that is itself universal across all instances/realizations of the system. The separation is achieved in an automated, data-driven manner and only empirical data are required as inputs to the model. The approach allows effective inference of system behaviour at any continuous time but does not require an explicit neural ODE formulation, which makes it efficient and highly scalable. We study behaviour through simple theoretical analyses and extensive experiments on synthetic and real-world datasets. The latter investigate learning the dynamics of complex systems based on finite data and show that the proposed approach can outperform state-of-the-art neural-dynamical models. We study also more general inductive bias in the context of transfer to data obtained under entirely novel system interventions. Overall, our results provide a promising new framework for efficiently learning dynamical models from heterogeneous data with potential applications in a wide range of fields including physics, medicine, biology and engineering.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2129251538",
                        "name": "Kai Lagemann"
                    },
                    {
                        "authorId": "40858217",
                        "name": "C. Lagemann"
                    },
                    {
                        "authorId": "113926067",
                        "name": "Swarnava Mukherjee"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Learning the dynamics of physical systems directly from their trajectory is an active area of research due to their potential applications in materials modeling [28], drug discovery [33], motion planning [25], robotics [29, 16], and even astrophysics [30].",
                "Among these, a family of models, such as Lagrangian or Hamiltonian neural networks [2, 1, 23, 29, 16] and Neural ODEs [3, 35, 17, 7], enforces the physics-based inductive biases in a strong sense.",
                ", particle-based systems [3, 32], atomistic dynamics [28, 19], physical systems [2, 29, 16], and articulated systems [1]."
            ],
            "citingPaper": {
                "paperId": "9e0adef9e8581d10ae2bf4659aa39fbee2cba5d6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-11435",
                    "ArXiv": "2306.11435",
                    "DOI": "10.48550/arXiv.2306.11435",
                    "CorpusId": 259203076
                },
                "corpusId": 259203076,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9e0adef9e8581d10ae2bf4659aa39fbee2cba5d6",
                "title": "Graph Neural Stochastic Differential Equations for Learning Brownian Dynamics",
                "abstract": "Neural networks (NNs) that exploit strong inductive biases based on physical laws and symmetries have shown remarkable success in learning the dynamics of physical systems directly from their trajectory. However, these works focus only on the systems that follow deterministic dynamics, for instance, Newtonian or Hamiltonian dynamics. Here, we propose a framework, namely Brownian graph neural networks (BROGNET), combining stochastic differential equations (SDEs) and GNNs to learn Brownian dynamics directly from the trajectory. We theoretically show that BROGNET conserves the linear momentum of the system, which in turn, provides superior performance on learning dynamics as revealed empirically. We demonstrate this approach on several systems, namely, linear spring, linear spring with binary particle types, and non-linear spring systems, all following Brownian dynamics at finite temperatures. We show that BROGNET significantly outperforms proposed baselines across all the benchmarked Brownian systems. In addition, we demonstrate zero-shot generalizability of BROGNET to simulate unseen system sizes that are two orders of magnitude larger and to different temperatures than those used during training. Altogether, our study contributes to advancing the understanding of the intricate dynamics of Brownian motion and demonstrates the effectiveness of graph neural networks in modeling such complex systems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "15675757",
                        "name": "S. Bishnoi"
                    },
                    {
                        "authorId": "2170276437",
                        "name": "Jayadeva"
                    },
                    {
                        "authorId": "1699732",
                        "name": "Sayan Ranu"
                    },
                    {
                        "authorId": "2144896197",
                        "name": "N. Krishnan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "bfd60ac391f0f8394ede3e94d5519da014ab72e7",
                "externalIds": {
                    "DBLP": "journals/mima/Li23",
                    "DOI": "10.1007/s11023-023-09639-9",
                    "CorpusId": 259459807
                },
                "corpusId": 259459807,
                "publicationVenue": {
                    "id": "76f182a2-ed15-4abe-add6-99f2dcd59171",
                    "name": "Minds and Machines",
                    "type": "journal",
                    "alternate_names": [
                        "Mind Mach"
                    ],
                    "issn": "0924-6495",
                    "url": "https://www.springer.com/computer/ai/journal/11023",
                    "alternate_urls": [
                        "http://www.springer.com/computer/ai/journal/11023",
                        "https://link.springer.com/journal/11023",
                        "https://www.springer.com/computer/ai/journal/11023/PSE"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bfd60ac391f0f8394ede3e94d5519da014ab72e7",
                "title": "Machines Learn Better with Better Data Ontology: Lessons from Philosophy of Induction and Machine Learning Practice",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2150383252",
                        "name": "Dan Li"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a1845843edfa890e83d613ab380387f701e4c921",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-09739",
                    "ArXiv": "2306.09739",
                    "DOI": "10.48550/arXiv.2306.09739",
                    "CorpusId": 259187905
                },
                "corpusId": 259187905,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a1845843edfa890e83d613ab380387f701e4c921",
                "title": "Stabilized Neural Differential Equations for Learning Constrained Dynamics",
                "abstract": "Many successful methods to learn dynamical systems from data have recently been introduced. However, assuring that the inferred dynamics preserve known constraints, such as conservation laws or restrictions on the allowed system states, remains challenging. We propose stabilized neural differential equations (SNDEs), a method to enforce arbitrary manifold constraints for neural differential equations. Our approach is based on a stabilization term that, when added to the original dynamics, renders the constraint manifold provably asymptotically stable. Due to its simplicity, our method is compatible with all common neural ordinary differential equation (NODE) models and broadly applicable. In extensive empirical evaluations, we demonstrate that SNDEs outperform existing methods while extending the scope of which types of constraints can be incorporated into NODE training.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220216257",
                        "name": "Alistair White"
                    },
                    {
                        "authorId": "19238593",
                        "name": "Niki Kilbertus"
                    },
                    {
                        "authorId": "35439538",
                        "name": "Maximilian Gelbrecht"
                    },
                    {
                        "authorId": "49920070",
                        "name": "N. Boers"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "3ec62b4e8c32567ce6621606d2bf2d04e380e49c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-09863",
                    "ArXiv": "2306.09863",
                    "DOI": "10.48550/arXiv.2306.09863",
                    "CorpusId": 259188073
                },
                "corpusId": 259188073,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3ec62b4e8c32567ce6621606d2bf2d04e380e49c",
                "title": "Transferability of Winning Lottery Tickets in Neural Network Differential Equation Solvers",
                "abstract": "Recent work has shown that renormalisation group theory is a useful framework with which to describe the process of pruning neural networks via iterative magnitude pruning. This report formally describes the link between RG theory and IMP and extends previous results around the Lottery Ticket Hypothesis and Elastic Lottery Hypothesis to Hamiltonian Neural Networks for solving differential equations. We find lottery tickets for two Hamiltonian Neural Networks and demonstrate transferability between the two systems, with accuracy being dependent on integration times. The universality of the two systems is then analysed using tools from an RG perspective.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220218937",
                        "name": "Edward Prideaux-Ghee"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "90ede06ec7692eee8b787036e0b3ddab7f2fbed5",
                "externalIds": {
                    "DOI": "10.1117/12.2664115",
                    "CorpusId": 259187058
                },
                "corpusId": 259187058,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/90ede06ec7692eee8b787036e0b3ddab7f2fbed5",
                "title": "Skeleton-based human action recognition with a physics-augmented encoder-decoder network",
                "abstract": "Human action recognition is important for many applications such as surveillance monitoring, safety, and healthcare. As 3D body skeletons can accurately characterize body actions and are robust to camera views, we propose a 3D skeleton-based human action method. Different from the existing skeleton-based methods that use only geometric features for action recognition, we propose a physics-augmented encoder and decoder model that produces physically plausible geometric features for human action recognition. Specifically, given the input skeleton sequence, the encoder performs a spatiotemporal graph convolution to produce spatiotemporal features for both predicting human actions and estimating the generalized positions and forces of body joints. The decoder, implemented as an ODE solver, takes the joint forces and solves the Euler-Lagrangian equation to reconstruct the skeletons in the next frame. By training the model to simultaneously minimize the action classification and the 3D skeleton reconstruction errors, the encoder is ensured to produce features that are consistent with both body skeletons and the underlying body dynamics as well as being discriminative. The physics-augmented spatiotemporal features are used for human action classification. We evaluate the proposed method on NTU-RGB+D, a large-scale dataset for skeleton-based action recognition. Compared with existing methods, our method achieves higher accuracy and better generalization ability.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2111304212",
                        "name": "Hongjian Guo"
                    },
                    {
                        "authorId": "2220211143",
                        "name": "Alexander Aved"
                    },
                    {
                        "authorId": "2220209155",
                        "name": "Collen Roller"
                    },
                    {
                        "authorId": "2191908153",
                        "name": "Erika Ardiles-Cruz"
                    },
                    {
                        "authorId": "2143639736",
                        "name": "Q. Ji"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "For example, data-driven machine learning algorithms that can preserve either the Hamiltonian structure [10; 11] or the Lagrangian structure [12], have been applied to a number of cases with success."
            ],
            "citingPaper": {
                "paperId": "cce8c60d919ab47a74205c1356843ba5b4a8c102",
                "externalIds": {
                    "ArXiv": "2306.08241",
                    "CorpusId": 259165512
                },
                "corpusId": 259165512,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cce8c60d919ab47a74205c1356843ba5b4a8c102",
                "title": "Entropy Structure Informed Learning for Inverse XDE Problems",
                "abstract": "Entropy, since its first discovery by Ludwig Boltzmann in 1877, has been widely applied in diverse disciplines, including thermodynamics, continuum mechanics, mathematical analysis, machine learning, etc. In this paper, we propose a new method for solving the inverse XDE (ODE, PDE, SDE) problems by utilizing the entropy balance equation instead of the original differential equations. This distinguishing feature constitutes a major difference between our current method and other previous classical methods (e.g. SINDy). Despite concerns about the potential information loss during the compression procedure from the original XDEs to single entropy balance equation, various examples from MM reactions, Schlogl model and chemical Lorenz equations in the form of ODEs to nonlinear porous medium equation and Fokker-Planck equation with a double-well potential in the PDE form all well confirm the accuracy, robustness and reliability of our method, as well as its comparable performance with respect to SINDy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219696340",
                        "name": "Yan Jiang"
                    },
                    {
                        "authorId": "12835617",
                        "name": "Wuyue Yang"
                    },
                    {
                        "authorId": "46758578",
                        "name": "Yi Zhu"
                    },
                    {
                        "authorId": "1410273073",
                        "name": "L. Hong"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "As baselines for comparison, we train a neural ODE (which has the same architecture as the SDE model but excludes the diffusion term), and an ensemble of 5 probabilistic (Gaussian) models [54].",
                "However, near the dataset, there is no clear pattern as to when it will produce stochastic versus deterministic predictions.\nslightly more accurate than those of the neural ODE.",
                "For example, many works use the Lagrangian, Hamiltonian, or Port-Hamiltonian formulation of dynamics to inform the structure of a neural ODE [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
                "While the neural ODE has no notion of prediction uncertainty, the proposed neural SDE yields uncertainty estimates that agree with the availability of the training data.",
                "The first row shows that neural SDE and ODE models have high prediction accuracy.",
                "We evaluate our approach by comparing its performance for modeling and model-based control tasks against state-of-the-art techniques such as probabilistic ensembles [45, 3, 54], system identificationbased algorithms [55, 56, 57], and neural ODEs [6, 1].",
                "Neural ordinary differential equations (ODEs) [6], which parametrize the right-hand side of a differential equation using a neural network, are a class of models that provide a natural mechanism for incorporating existing physics and engineering knowledge into neural networks [1, 7]."
            ],
            "citingPaper": {
                "paperId": "8b5e502384da02b730b6693a0a623bf41f4b91ce",
                "externalIds": {
                    "ArXiv": "2306.06335",
                    "DBLP": "journals/corr/abs-2306-06335",
                    "DOI": "10.48550/arXiv.2306.06335",
                    "CorpusId": 259138787
                },
                "corpusId": 259138787,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8b5e502384da02b730b6693a0a623bf41f4b91ce",
                "title": "How to Learn and Generalize From Three Minutes of Data: Physics-Constrained and Uncertainty-Aware Neural Stochastic Differential Equations",
                "abstract": "We present a framework and algorithms to learn controlled dynamics models using neural stochastic differential equations (SDEs) -- SDEs whose drift and diffusion terms are both parametrized by neural networks. We construct the drift term to leverage a priori physics knowledge as inductive bias, and we design the diffusion term to represent a distance-aware estimate of the uncertainty in the learned model's predictions -- it matches the system's underlying stochasticity when evaluated on states near those from the training dataset, and it predicts highly stochastic dynamics when evaluated on states beyond the training regime. The proposed neural SDEs can be evaluated quickly enough for use in model predictive control algorithms, or they can be used as simulators for model-based reinforcement learning. Furthermore, they make accurate predictions over long time horizons, even when trained on small datasets that cover limited regions of the state space. We demonstrate these capabilities through experiments on simulated robotic systems, as well as by using them to model and control a hexacopter's flight dynamics: A neural SDE trained using only three minutes of manually collected flight data results in a model-based control policy that accurately tracks aggressive trajectories that push the hexacopter's velocity and Euler angles to nearly double the maximum values observed in the training dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152050811",
                        "name": "Franck Djeumou"
                    },
                    {
                        "authorId": "1796254983",
                        "name": "Cyrus Neary"
                    },
                    {
                        "authorId": "3199888",
                        "name": "U. Topcu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "232327779834ab8b889e3dee1bf3eb871c8292a3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-03548",
                    "ArXiv": "2306.03548",
                    "DOI": "10.48550/arXiv.2306.03548",
                    "CorpusId": 259089097
                },
                "corpusId": 259089097,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/232327779834ab8b889e3dee1bf3eb871c8292a3",
                "title": "Learning Dynamical Systems from Noisy Data with Inverse-Explicit Integrators",
                "abstract": "We introduce the mean inverse integrator (MII), a novel approach to increase the accuracy when training neural networks to approximate vector fields of dynamical systems from noisy data. This method can be used to average multiple trajectories obtained by numerical integrators such as Runge-Kutta methods. We show that the class of mono-implicit Runge-Kutta methods (MIRK) has particular advantages when used in connection with MII. When training vector field approximations, explicit expressions for the loss functions are obtained when inserting the training data in the MIRK formulae, unlocking symmetric and high-order integrators that would otherwise be implicit for initial value problems. The combined approach of applying MIRK within MII yields a significantly lower error compared to the plain use of the numerical integrator without averaging the trajectories. This is demonstrated with experiments using data from several (chaotic) Hamiltonian systems. Additionally, we perform a sensitivity analysis of the loss functions under normally distributed perturbations, supporting the favorable performance of MII.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2210857577",
                        "name": "Haakon Noren"
                    },
                    {
                        "authorId": "102675722",
                        "name": "S\u00f8lve Eidnes"
                    },
                    {
                        "authorId": "2791391",
                        "name": "E. Celledoni"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Learning Fm [60] from few data is impractical [61], (II) precludes deriving ite on first principles, and using traditional image features would limit discriminating power for unknown patterns."
            ],
            "citingPaper": {
                "paperId": "8b5c45c8c887a9935749b52c41819cddc73d8570",
                "externalIds": {
                    "DOI": "10.1103/PhysRevLett.130.226201",
                    "CorpusId": 262046912,
                    "PubMed": "37327436"
                },
                "corpusId": 262046912,
                "publicationVenue": {
                    "id": "16c9f9d4-bee1-435d-8c85-22a3deba109d",
                    "name": "Physical Review Letters",
                    "type": "journal",
                    "alternate_names": [
                        "Phys Rev Lett"
                    ],
                    "issn": "0031-9007",
                    "url": "https://journals.aps.org/prl/",
                    "alternate_urls": [
                        "http://journals.aps.org/prl/",
                        "http://prl.aps.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8b5c45c8c887a9935749b52c41819cddc73d8570",
                "title": "Learning Complexity to Guide Light-Induced Self-Organized Nanopatterns.",
                "abstract": "Ultrafast laser irradiation can induce spontaneous self-organization of surfaces into dissipative structures with nanoscale reliefs. These surface patterns emerge from symmetry-breaking dynamical processes that occur in Rayleigh-B\u00e9nard-like instabilities. In this study, we demonstrate that the coexistence and competition between surface patterns of different symmetries in two dimensions can be numerically unraveled using the stochastic generalized Swift-Hohenberg model. We originally propose a deep convolutional network to identify and learn the dominant modes that stabilize for a given bifurcation and quadratic model coefficients. The model is scale-invariant and has been calibrated on microscopy measurements using a physics-guided machine learning strategy. Our approach enables the identification of experimental irradiation conditions for a desired self-organization pattern. It can be generally applied to predict structure formation in situations where the underlying physics can be approximately described by a self-organization process and data is sparse and nontime series. Our Letter paves the way for supervised local manipulation of matter using timely controlled optical fields in laser manufacturing.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2181235426",
                        "name": "Eduardo Brandao"
                    },
                    {
                        "authorId": "2086753852",
                        "name": "A. Nakhoul"
                    },
                    {
                        "authorId": "2243198910",
                        "name": "Stefan Du\ufb00ner"
                    },
                    {
                        "authorId": "2003050",
                        "name": "R. Emonet"
                    },
                    {
                        "authorId": "94098359",
                        "name": "F. Garrelie"
                    },
                    {
                        "authorId": "1749327",
                        "name": "Amaury Habrard"
                    },
                    {
                        "authorId": "2243201634",
                        "name": "Fran\u00e7ois Jacquenet"
                    },
                    {
                        "authorId": "66907761",
                        "name": "F. Pigeon"
                    },
                    {
                        "authorId": "1738336",
                        "name": "M. Sebban"
                    },
                    {
                        "authorId": "115246217",
                        "name": "J. Colombier"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "There are also numerous pieces of research [22, 45, 8, 43] focusing on recovering the Hamiltonian, and predicting the dynamics of certain physical systems based on observed trajectories."
            ],
            "citingPaper": {
                "paperId": "0a4736a98546de45759c0a260e0280e94e4c2591",
                "externalIds": {
                    "ArXiv": "2306.00191",
                    "DBLP": "journals/corr/abs-2306-00191",
                    "DOI": "10.48550/arXiv.2306.00191",
                    "CorpusId": 258999543
                },
                "corpusId": 258999543,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0a4736a98546de45759c0a260e0280e94e4c2591",
                "title": "Parameterized Wasserstein Hamiltonian Flow",
                "abstract": "In this work, we propose a numerical method to compute the Wasserstein Hamiltonian flow (WHF), which is a Hamiltonian system on the probability density manifold. Many well-known PDE systems can be reformulated as WHFs. We use parameterized function as push-forward map to characterize the solution of WHF, and convert the PDE to a finite-dimensional ODE system, which is a Hamiltonian system in the phase space of the parameter manifold. We establish error analysis results for the continuous time approximation scheme in Wasserstein metric. For the numerical implementation, we use neural networks as push-forward maps. We apply an effective symplectic scheme to solve the derived Hamiltonian ODE system so that the method preserves some important quantities such as total energy. The computation is done by fully deterministic symplectic integrator without any neural network training. Thus, our method does not involve direct optimization over network parameters and hence can avoid the error introduced by stochastic gradient descent (SGD) methods, which is usually hard to quantify and measure. The proposed algorithm is a sampling-based approach that scales well to higher dimensional problems. In addition, the method also provides an alternative connection between the Lagrangian and Eulerian perspectives of the original WHF through the parameterized ODE dynamics.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2119795510",
                        "name": "Hao Wu"
                    },
                    {
                        "authorId": "2108422879",
                        "name": "Shu Liu"
                    },
                    {
                        "authorId": "145315788",
                        "name": "X. Ye"
                    },
                    {
                        "authorId": "48053919",
                        "name": "Haomin Zhou"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Related approaches include energy-based model architectures such as Hamiltonian [36, 37] or Lagrangian [38, 39] neural networks, and their various extensions such as graph Hamiltonian neural networks (HNN) [40], Hamiltonian dynamics with dissipative forces [41], HNN with explicit constraints [42], or noncanonical Hamiltonian systems [43].",
                "Related approaches include energy-based model architectures such as Hamiltonian [36, 37] or Lagrangian [38, 39] neural networks, and their various extensions such as graph Hamiltonian neural networks (HNN) [40], Hamiltonian dynamics with dissipative forces [41], HNN with explicit constraints [42], or non-"
            ],
            "citingPaper": {
                "paperId": "d95c7f3a6ca2b98ba3c4422aabb33e3754cb1a47",
                "externalIds": {
                    "DBLP": "conf/amcc/NghiemDJ0SDCCPC23",
                    "ArXiv": "2306.13867",
                    "DOI": "10.23919/ACC55779.2023.10155901",
                    "CorpusId": 259251591
                },
                "corpusId": 259251591,
                "publicationVenue": {
                    "id": "fe4d09f8-d278-4bfb-b73a-1a6a0e22f6a3",
                    "name": "American Control Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Adv Comput Control",
                        "ACC",
                        "Advances in Computing and Communications",
                        "Adv Comput Commun",
                        "Am Control Conf",
                        "Advances in Computer and Communication",
                        "International Conference on Advanced Computer Control"
                    ],
                    "issn": "2767-2875",
                    "url": "http://a2c2.org/conferences/american-control-conferences",
                    "alternate_urls": [
                        "http://www.acc-rajagiri.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d95c7f3a6ca2b98ba3c4422aabb33e3754cb1a47",
                "title": "Physics-Informed Machine Learning for Modeling and Control of Dynamical Systems",
                "abstract": "Physics-informed machine learning (PIML) is a set of methods and tools that systematically integrate machine learning (ML) algorithms with physical constraints and abstract mathematical models developed in scientific and engineering domains. As opposed to purely data-driven methods, PIML models can be trained from additional information obtained by enforcing physical laws such as energy and mass conservation. More broadly, PIML models can include abstract properties and conditions such as stability, convexity, or invariance. The basic premise of PIML is that the integration of ML and physics can yield more effective, physically consistent, and data-efficient models. This paper aims to provide a tutorial-like overview of the recent advances in PIML for dynamical system modeling and control. Specifically, the paper covers an overview of the theory, fundamental concepts and methods, tools, and applications on topics of: 1) physics-informed learning for system identification; 2) physics-informed learning for control; 3) analysis and verification of PIML models; and 4) physics-informed digital twins. The paper is concluded with a perspective on open challenges and future research opportunities.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1679549",
                        "name": "Truong X. Nghiem"
                    },
                    {
                        "authorId": "2403282",
                        "name": "J\u00e1n Drgo\u0148a"
                    },
                    {
                        "authorId": "2247042795",
                        "name": "Colin N. Jones"
                    },
                    {
                        "authorId": "2170825281",
                        "name": "Zoltan Nagy"
                    },
                    {
                        "authorId": "2173701907",
                        "name": "Roland Schwan"
                    },
                    {
                        "authorId": "32553531",
                        "name": "Biswadip Dey"
                    },
                    {
                        "authorId": "35054948",
                        "name": "A. Chakrabarty"
                    },
                    {
                        "authorId": "2864341",
                        "name": "S. D. Cairano"
                    },
                    {
                        "authorId": "145991520",
                        "name": "J. Paulson"
                    },
                    {
                        "authorId": "3063090",
                        "name": "Andrea Carron"
                    },
                    {
                        "authorId": "2176899",
                        "name": "M. Zeilinger"
                    },
                    {
                        "authorId": "1403285856",
                        "name": "Wenceslao Shaw-Cortez"
                    },
                    {
                        "authorId": "1885215",
                        "name": "D. Vrabie"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "06ff959b7a5beb8a60cf65b5fe835006581bde70",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-18965",
                    "ArXiv": "2305.18965",
                    "DOI": "10.48550/arXiv.2305.18965",
                    "CorpusId": 258967801
                },
                "corpusId": 258967801,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/06ff959b7a5beb8a60cf65b5fe835006581bde70",
                "title": "Node Embedding from Neural Hamiltonian Orbits in Graph Neural Networks",
                "abstract": "In the graph node embedding problem, embedding spaces can vary significantly for different data types, leading to the need for different GNN model types. In this paper, we model the embedding update of a node feature as a Hamiltonian orbit over time. Since the Hamiltonian orbits generalize the exponential maps, this approach allows us to learn the underlying manifold of the graph in training, in contrast to most of the existing literature that assumes a fixed graph embedding manifold with a closed exponential map solution. Our proposed node embedding strategy can automatically learn, without extensive tuning, the underlying geometry of any given graph dataset even if it has diverse geometries. We test Hamiltonian functions of different forms and verify the performance of our approach on two graph node embedding downstream tasks: node classification and link prediction. Numerical experiments demonstrate that our approach adapts better to different types of graph datasets than popular state-of-the-art graph node embedding GNNs. The code is available at \\url{https://github.com/zknus/Hamiltonian-GNN}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "22758176",
                        "name": "Qiyu Kang"
                    },
                    {
                        "authorId": "2074108937",
                        "name": "Kai Zhao"
                    },
                    {
                        "authorId": "2006209232",
                        "name": "Yang Song"
                    },
                    {
                        "authorId": "2116422074",
                        "name": "Sijie Wang"
                    },
                    {
                        "authorId": "3118058",
                        "name": "Wee Peng Tay"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "763fd38d01fb5804aa576ebcb28d123e643d1874",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-18035",
                    "ArXiv": "2305.18035",
                    "DOI": "10.48550/arXiv.2305.18035",
                    "CorpusId": 258959254
                },
                "corpusId": 258959254,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/763fd38d01fb5804aa576ebcb28d123e643d1874",
                "title": "Physics-Informed Computer Vision: A Review and Perspectives",
                "abstract": "Incorporation of physical information in machine learning frameworks are opening and transforming many application domains. Here the learning process is augmented through the induction of fundamental knowledge and governing physical laws. In this work we explore their utility for computer vision tasks in interpreting and understanding visual data. We present a systematic literature review of formulation and approaches to computer vision tasks guided by physical laws. We begin by decomposing the popular computer vision pipeline into a taxonomy of stages and investigate approaches to incorporate governing physical equations in each stage. Existing approaches in each task are analyzed with regard to what governing physical processes are modeled, formulated and how they are incorporated, i.e. modify data (observation bias), modify networks (inductive bias), and modify losses (learning bias). The taxonomy offers a unified view of the application of the physics-informed capability, highlighting where physics-informed learning has been conducted and where the gaps and opportunities are. Finally, we highlight open problems and challenges to inform future research. While still in its early days, the study of physics-informed computer vision has the promise to develop better computer vision models that can improve physical plausibility, accuracy, data efficiency and generalization in increasingly realistic applications.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2065387130",
                        "name": "C. Banerjee"
                    },
                    {
                        "authorId": "49254601",
                        "name": "Kien Nguyen"
                    },
                    {
                        "authorId": "3140440",
                        "name": "C. Fookes"
                    },
                    {
                        "authorId": "102372728",
                        "name": "G. Karniadakis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Therefore, a compact and precise governing equation of action-value function is not available for regulating the training loss function, which thus hinders the applications of current frameworks of physics-enhanced DNN here [48, 51, 21, 20, 49, 31, 6, 45, 52, 23, 47, 11, 9, 14, 17, 36, 33, 35, 18, 46, 28].",
                "[17] Samuel Greydanus, Misko Dzamba, and Jason Yosinski.",
                "Current frameworks include physicsinformed NN [48, 51, 21, 20, 49, 31, 6, 45, 52, 23, 47, 11, 9, 14, 17], physics-guided NN architectures [36, 33, 35, 18, 46, 28] and physics-inspired neural operators [32, 29]."
            ],
            "citingPaper": {
                "paperId": "2726451ddd17c0e858572943b7937898d22e31cd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-16614",
                    "ArXiv": "2305.16614",
                    "DOI": "10.48550/arXiv.2305.16614",
                    "CorpusId": 258947355
                },
                "corpusId": 258947355,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2726451ddd17c0e858572943b7937898d22e31cd",
                "title": "Physical Deep Reinforcement Learning: Safety and Unknown Unknowns",
                "abstract": "In this paper, we propose the Phy-DRL: a physics-model-regulated deep reinforcement learning framework for safety-critical autonomous systems. The Phy-DRL is unique in three innovations: i) proactive unknown-unknowns training, ii) conjunctive residual control (i.e., integration of data-driven control and physics-model-based control) and safety- \\&stability-sensitive reward, and iii) physics-model-based neural network editing, including link editing and activation editing. Thanks to the concurrent designs, the Phy-DRL is able to 1) tolerate unknown-unknowns disturbances, 2) guarantee mathematically provable safety and stability, and 3) strictly comply with physical knowledge pertaining to Bellman equation and reward. The effectiveness of the Phy-DRL is finally validated by an inverted pendulum and a quadruped robot. The experimental results demonstrate that compared with purely data-driven DRL, Phy-DRL features remarkably fewer learning parameters, accelerated training and enlarged reward, while offering enhanced model robustness and safety assurance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1774928",
                        "name": "H. Cao"
                    },
                    {
                        "authorId": "1937682",
                        "name": "Y. Mao"
                    },
                    {
                        "authorId": "2919287",
                        "name": "L. Sha"
                    },
                    {
                        "authorId": "1749138",
                        "name": "M. Caccamo"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[19] Samuel Greydanus, Misko Dzamba, and Jason Yosinski.",
                "Structure-preserving dense networks: For dense networks, it is relatively straightforward to parameterize reversible dynamics, see for example: Hamiltonian neural networks [19, 20, 21, 22], Hamiltonian generative networks [23], Hamiltonian with Control (SymODEN) [24], Deep Lagrangian networks [25] and Lagrangian neural networks [26]."
            ],
            "citingPaper": {
                "paperId": "71395e0be94802971611ec88130af780d4ba90d0",
                "externalIds": {
                    "ArXiv": "2305.15616",
                    "DBLP": "journals/corr/abs-2305-15616",
                    "DOI": "10.48550/arXiv.2305.15616",
                    "CorpusId": 258887954
                },
                "corpusId": 258887954,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/71395e0be94802971611ec88130af780d4ba90d0",
                "title": "Reversible and irreversible bracket-based dynamics for deep graph neural networks",
                "abstract": "Recent works have shown that physics-inspired architectures allow the training of deep graph neural networks (GNNs) without oversmoothing. The role of these physics is unclear, however, with successful examples of both reversible (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomena producing comparable results despite diametrically opposed mechanisms, and further complications arising due to empirical departures from mathematical theory. This work presents a series of novel GNN architectures based upon structure-preserving bracket-based dynamical systems, which are provably guaranteed to either conserve energy or generate positive dissipation with increasing depth. It is shown that the theoretically principled framework employed here allows for inherently explainable constructions, which contextualize departures from theory in current architectures and better elucidate the roles of reversibility and irreversibility in network performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "115023648",
                        "name": "A. Gruber"
                    },
                    {
                        "authorId": "3041967",
                        "name": "Kookjin Lee"
                    },
                    {
                        "authorId": "1453887310",
                        "name": "N. Trask"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "(2020b;a) encode Hamiltonian dynamics and dissipative Hamiltonian dynamics into the structure of the neural ODE using Hamiltonian neural networks (Greydanus et al., 2019).",
                "Neural ODEs With Structure Greydanus et al. (2019) introduce the idea of adding a Hamiltonian structure to a neural network.",
                "For example, Zhong et al. (2020b;a) encode Hamiltonian dynamics and dissipative Hamiltonian dynamics into the structure of the neural ODE using Hamiltonian neural networks (Greydanus et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "bd5a854fe7f97a373a15ebf31264bb795adae877",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-13290",
                    "ArXiv": "2305.13290",
                    "DOI": "10.48550/arXiv.2305.13290",
                    "CorpusId": 258833343
                },
                "corpusId": 258833343,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bd5a854fe7f97a373a15ebf31264bb795adae877",
                "title": "Uncertainty and Structure in Neural Ordinary Differential Equations",
                "abstract": "Neural ordinary differential equations (ODEs) are an emerging class of deep learning models for dynamical systems. They are particularly useful for learning an ODE vector field from observed trajectories (i.e., inverse problems). We here consider aspects of these models relevant for their application in science and engineering. Scientific predictions generally require structured uncertainty estimates. As a first contribution, we show that basic and lightweight Bayesian deep learning techniques like the Laplace approximation can be applied to neural ODEs to yield structured and meaningful uncertainty quantification. But, in the scientific domain, available information often goes beyond raw trajectories, and also includes mechanistic knowledge, e.g., in the form of conservation laws. We explore how mechanistic knowledge and uncertainty quantification interact on two recently proposed neural ODE frameworks - symplectic neural ODEs and physical models augmented with neural ODEs. In particular, uncertainty reflects the effect of mechanistic information more directly than the predictive power of the trained model could. And vice versa, structure can improve the extrapolation abilities of neural ODEs, a fact that can be best assessed in practice through uncertainty estimates. Our experimental analysis demonstrates the effectiveness of the Laplace approach on both low dimensional ODE problems and a high dimensional partial differential equation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2066490213",
                        "name": "Katharina Ott"
                    },
                    {
                        "authorId": "2144071766",
                        "name": "Michael Tiemann"
                    },
                    {
                        "authorId": "2517795",
                        "name": "Philipp Hennig"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "It has applications in learning dynamics [52, 53, 54, 55], control [56, 57], generative modeling [51, 58], and joint shape encoding, reconstruction, and registration [59, 19, 29]."
            ],
            "citingPaper": {
                "paperId": "f72c4ad6f123f3c8e9c38448e086b963c66d1edc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-12854",
                    "ArXiv": "2305.12854",
                    "DOI": "10.48550/arXiv.2305.12854",
                    "CorpusId": 258832350
                },
                "corpusId": 258832350,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f72c4ad6f123f3c8e9c38448e086b963c66d1edc",
                "title": "RSA-INR: Riemannian Shape Autoencoding via 4D Implicit Neural Representations",
                "abstract": "Shape encoding and shape analysis are valuable tools for comparing shapes and for dimensionality reduction. A specific framework for shape analysis is the Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework, which is capable of shape matching and dimensionality reduction. Researchers have recently introduced neural networks into this framework. However, these works can not match more than two objects simultaneously or have suboptimal performance in shape variability modeling. The latter limitation occurs as the works do not use state-of-the-art shape encoding methods. Moreover, the literature does not discuss the connection between the LDDMM Riemannian distance and the Riemannian geometry for deep learning literature. Our work aims to bridge this gap by demonstrating how LDDMM can integrate Riemannian geometry into deep learning. Furthermore, we discuss how deep learning solves and generalizes shape matching and dimensionality reduction formulations of LDDMM. We achieve both goals by designing a novel implicit encoder for shapes. This model extends a neural network-based algorithm for LDDMM-based pairwise registration, results in a nonlinear manifold PCA, and adds a Riemannian geometry aspect to deep learning models for shape variability modeling. Additionally, we demonstrate that the Riemannian geometry component improves the reconstruction procedure of the implicit encoder in terms of reconstruction quality and stability to noise. We hope our discussion paves the way to more research into how Riemannian geometry, shape/image analysis, and deep learning can be combined.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2176001308",
                        "name": "Sven Dummer"
                    },
                    {
                        "authorId": "1742086",
                        "name": "N. Strisciuglio"
                    },
                    {
                        "authorId": "33882877",
                        "name": "C. Brune"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "HOGN [11] imports the Hamiltonian mechanics [21] as physics informed inductive biases into INs for more accurate particle system simulation."
            ],
            "citingPaper": {
                "paperId": "b8faa6683b9bea99f87387a6434eb064996007b2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-12334",
                    "ArXiv": "2305.12334",
                    "DOI": "10.48550/arXiv.2305.12334",
                    "CorpusId": 258832447
                },
                "corpusId": 258832447,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b8faa6683b9bea99f87387a6434eb064996007b2",
                "title": "Towards Complex Dynamic Physics System Simulation with Graph Neural ODEs",
                "abstract": "The great learning ability of deep learning models facilitates us to comprehend the real physical world, making learning to simulate complicated particle systems a promising endeavour. However, the complex laws of the physical world pose significant challenges to the learning based simulations, such as the varying spatial dependencies between interacting particles and varying temporal dependencies between particle system states in different time stamps, which dominate particles' interacting behaviour and the physical systems' evolution patterns. Existing learning based simulation methods fail to fully account for the complexities, making them unable to yield satisfactory simulations. To better comprehend the complex physical laws, this paper proposes a novel learning based simulation model- Graph Networks with Spatial-Temporal neural Ordinary Equations (GNSTODE)- that characterizes the varying spatial and temporal dependencies in particle systems using a united end-to-end framework. Through training with real-world particle-particle interaction observations, GNSTODE is able to simulate any possible particle systems with high precisions. We empirically evaluate GNSTODE's simulation performance on two real-world particle systems, Gravity and Coulomb, with varying levels of spatial and temporal dependencies. The results show that the proposed GNSTODE yields significantly better simulations than state-of-the-art learning based simulation methods, which proves that GNSTODE can serve as an effective solution to particle simulations in real-world application.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2216796907",
                        "name": "Guangsi Shi"
                    },
                    {
                        "authorId": "2589584",
                        "name": "Daokun Zhang"
                    },
                    {
                        "authorId": "2072905592",
                        "name": "Ming Jin"
                    },
                    {
                        "authorId": "2153326034",
                        "name": "Shirui Pan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "However, since many physical systems are dissipative, the assumption of HNNs limits its applicability to real-world systems.",
                "Efforts to build models that conserve the total energy of a system led to a body of work on Hamiltonian neural networks (HNN) (Greydanus et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "a8ba8b760fc49266c5aea128e9527d7ac746850e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-09689",
                    "ArXiv": "2305.09689",
                    "DOI": "10.48550/arXiv.2305.09689",
                    "CorpusId": 258740800
                },
                "corpusId": 258740800,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a8ba8b760fc49266c5aea128e9527d7ac746850e",
                "title": "Learning Switching Port-Hamiltonian Systems with Uncertainty Quantification",
                "abstract": "Switching physical systems are ubiquitous in modern control applications, for instance, locomotion behavior of robots and animals, power converters with switches and diodes. The dynamics and switching conditions are often hard to obtain or even inaccessible in case of a-priori unknown environments and nonlinear components. Black-box neural networks can learn to approximately represent switching dynamics, but typically require a large amount of data, neglect the underlying axioms of physics, and lack of uncertainty quantification. We propose a Gaussian process based learning approach enhanced by switching Port-Hamiltonian systems (GP-SPHS) to learn physical plausible system dynamics and identify the switching condition. The Bayesian nature of Gaussian processes uses collected data to form a distribution over all possible switching policies and dynamics that allows for uncertainty quantification. Furthermore, the proposed approach preserves the compositional nature of Port-Hamiltonian systems. A simulation with a hopping robot validates the effectiveness of the proposed approach.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "88691548",
                        "name": "Thomas Beckers"
                    },
                    {
                        "authorId": "1990956535",
                        "name": "Tom Z. Jiahao"
                    },
                    {
                        "authorId": "35896896",
                        "name": "George Pappas"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "[23, 16, 34], and a more general pseudo-Hamiltonian formulation than the port-Hamiltonian formulation of e.",
                "Much of the literature has focused on Hamiltonian or Lagrangian formulations of (1), beginning with [23, 8, 10].",
                "Hamiltonian neural networks (HNN) have received considerable attention since their introduction in [23], resulting in several extensions and generalizations [8, 22, 27].",
                "However, in the system we will consider, S is either of canonical form, as is assumed in the many recent works on Hamiltonian neural networks [23], or a non-canonical form that can be obtained from engineering knowledge."
            ],
            "citingPaper": {
                "paperId": "339e82264318f35539ea8e943b2a777c4bfba772",
                "externalIds": {
                    "ArXiv": "2305.06920",
                    "DBLP": "journals/corr/abs-2305-06920",
                    "DOI": "10.48550/arXiv.2305.06920",
                    "CorpusId": 258615186
                },
                "corpusId": 258615186,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/339e82264318f35539ea8e943b2a777c4bfba772",
                "title": "Pseudo-Hamiltonian system identification",
                "abstract": "Identifying the underlying dynamics of physical systems can be challenging when only provided with observational data. In this work, we consider systems that can be modelled as first-order ordinary differential equations. By assuming a certain pseudo-Hamiltonian formulation, we are able to learn the analytic terms of internal dynamics even if the model is trained on data where the system is affected by unknown damping and external disturbances. In cases where it is difficult to find analytic terms for the disturbances, a hybrid model that uses a neural network to learn these can still accurately identify the dynamics of the system as if under ideal conditions. This makes the models applicable in situations where other system identification models fail. Furthermore, we propose to use a fourth-order symmetric integration scheme in the loss function and avoid actual integration in the training, and demonstrate on varied examples how this leads to increased performance on noisy data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2216786480",
                        "name": "Sigurd Holmsen"
                    },
                    {
                        "authorId": "102675722",
                        "name": "S\u00f8lve Eidnes"
                    },
                    {
                        "authorId": "1388390391",
                        "name": "S. Riemer-S\u00f8rensen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Various strategies have been proposed such as the direct approximation of the infinitesimal dynamic [2], fix point methods mimicking infinitedepth networks [3], variational principles and conservative systems [4], the direct minimization of the differential equation residue [5] or also more generic data-driven frameworks to approximate mappings between Hilbert spaces [6, 7]."
            ],
            "citingPaper": {
                "paperId": "9a18fe280a67f50d72877f31e86d32a0e854928e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-05469",
                    "ArXiv": "2305.05469",
                    "DOI": "10.48550/arXiv.2305.05469",
                    "CorpusId": 258564244
                },
                "corpusId": 258564244,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9a18fe280a67f50d72877f31e86d32a0e854928e",
                "title": "Graph Neural Networks for Airfoil Design",
                "abstract": "The study of partial differential equations (PDE) through the framework of deep learning emerged a few years ago leading to the impressive approximations of simple dynamics. Graph neural networks (GNN) turned out to be very useful in those tasks by allowing the treatment of unstructured data often encountered in the field of numerical resolutions of PDE. However, the resolutions of harder PDE such as Navier-Stokes equations are still a challenging task and most of the work done on the latter concentrate either on simulating the flow around simple geometries or on qualitative results that looks physical for design purpose. In this study, we try to leverage the work done on deep learning for PDE and GNN by proposing an adaptation of a known architecture in order to tackle the task of approximating the solution of the two-dimensional steady-state incompressible Navier-Stokes equations over different airfoil geometries. In addition to that, we test our model not only on its performance over the volume but also on its performance to approximate surface quantities such as the wall shear stress or the isostatic pressure leading to the inference of global coefficients such as the lift and the drag of our airfoil in order to allow design exploration. This work takes place in a longer project that aims to approximate three dimensional steady-state solutions over industrial geometries.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2121331282",
                        "name": "F. Bonnet"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2021), physics-inspired inductive biases (Jonschkowski and Brock, 2015; Cranmer et al., 2020; Greydanus et al., 2019), unsupervised",
                "\u2026et al., 2016; Liu et al., 2019; Lyle et al., 2021), physics-inspired inductive biases (Jonschkowski and Brock, 2015; Cranmer et al., 2020; Greydanus et al., 2019), unsupervised\n1. https://github.com/sahandrez/homomorphic policy gradient\nPolicy Gradient Methods in the Presence of\u2026"
            ],
            "citingPaper": {
                "paperId": "3960ea842b8b89819f80cf2cd77adc321ff744a1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-05666",
                    "ArXiv": "2305.05666",
                    "DOI": "10.48550/arXiv.2305.05666",
                    "CorpusId": 258564493
                },
                "corpusId": 258564493,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3960ea842b8b89819f80cf2cd77adc321ff744a1",
                "title": "Policy Gradient Methods in the Presence of Symmetries and State Abstractions",
                "abstract": "Reinforcement learning on high-dimensional and complex problems relies on abstraction for improved efficiency and generalization. In this paper, we study abstraction in the continuous-control setting, and extend the definition of MDP homomorphisms to the setting of continuous state and action spaces. We derive a policy gradient theorem on the abstract MDP for both stochastic and deterministic policies. Our policy gradient results allow for leveraging approximate symmetries of the environment for policy optimization. Based on these theorems, we propose a family of actor-critic algorithms that are able to learn the policy and the MDP homomorphism map simultaneously, using the lax bisimulation metric. Finally, we introduce a series of environments with continuous symmetries to further demonstrate the ability of our algorithm for action abstraction in the presence of such symmetries. We demonstrate the effectiveness of our method on our environments, as well as on challenging visual control tasks from the DeepMind Control Suite. Our method's ability to utilize MDP homomorphisms for representation learning leads to improved performance, and the visualizations of the latent space clearly demonstrate the structure of the learned abstraction.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1784317",
                        "name": "P. Panangaden"
                    },
                    {
                        "authorId": "1417282075",
                        "name": "S. Rezaei-Shoshtari"
                    },
                    {
                        "authorId": "2004617613",
                        "name": "Rosie Zhao"
                    },
                    {
                        "authorId": "2462512",
                        "name": "D. Meger"
                    },
                    {
                        "authorId": "144368601",
                        "name": "Doina Precup"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Learning the symplectic character (if it exists) of a physical system (including particles in potential fields, pendulums of various complexities) can be done utilizing neural networks, see, for example, [15, 16]."
            ],
            "citingPaper": {
                "paperId": "59749a7cdbb11de19fee2f50222633bcc9fc59b1",
                "externalIds": {
                    "ArXiv": "2305.05540",
                    "CorpusId": 258564553
                },
                "corpusId": 258564553,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/59749a7cdbb11de19fee2f50222633bcc9fc59b1",
                "title": "Direct Poisson neural networks: Learning non-symplectic mechanical systems",
                "abstract": "In this paper, we present neural networks learning mechanical systems that are both symplectic (for instance particle mechanics) and non-symplectic (for instance rotating rigid body). Mechanical systems have Hamiltonian evolution, which consists of two building blocks: a Poisson bracket and an energy functional. We feed a set of snapshots of a Hamiltonian system to our neural network models which then find both the two building blocks. In particular, the models distinguish between symplectic systems (with non-degenerate Poisson brackets) and non-symplectic systems (degenerate brackets). In contrast with earlier works, our approach does not assume any further a priori information about the dynamics except its Hamiltonianity, and it returns Poisson brackets that satisfy Jacobi identity. Finally, the models indicate whether a system of equations is Hamiltonian or not.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2129453533",
                        "name": "Martin vS'ipka"
                    },
                    {
                        "authorId": "12727047",
                        "name": "M. Pavelka"
                    },
                    {
                        "authorId": "102567376",
                        "name": "Ougul Esen"
                    },
                    {
                        "authorId": "1802115",
                        "name": "M. Grmela"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Hamiltonian neural networks (HNNs) (Greydanus et al., 2019) are a state-of-the-art grey-box modeling tool in which Hamiltonian mechanics is embedded as prior knowledge in the neural network.",
                "Greydanus et al. (2019) proposed to learn the Hamiltonian, as an energy-like scalar value in an unsupervised manner.",
                "After the introduction of HNNs by Greydanus et al. (2019), many researchers from different fields studied its application and proposed extensions to the original idea."
            ],
            "citingPaper": {
                "paperId": "e0dfbae90cafa8756755e26e9ebd6a84df034a56",
                "externalIds": {
                    "ArXiv": "2305.01338",
                    "DBLP": "journals/corr/abs-2305-01338",
                    "DOI": "10.48550/arXiv.2305.01338",
                    "CorpusId": 258437262
                },
                "corpusId": 258437262,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e0dfbae90cafa8756755e26e9ebd6a84df034a56",
                "title": "Physics-Informed Learning Using Hamiltonian Neural Networks with Output Error Noise Models",
                "abstract": "In order to make data-driven models of physical systems interpretable and reliable, it is essential to include prior physical knowledge in the modeling framework. Hamiltonian Neural Networks (HNNs) implement Hamiltonian theory in deep learning and form a comprehensive framework for modeling autonomous energy-conservative systems. Despite being suitable to estimate a wide range of physical system behavior from data, classical HNNs are restricted to systems without inputs and require noiseless state measurements and information on the derivative of the state to be available. To address these challenges, this paper introduces an Output Error Hamiltonian Neural Network (OE-HNN) modeling approach to address the modeling of physical systems with inputs and noisy state measurements. Furthermore, it does not require the state derivatives to be known. Instead, the OE-HNN utilizes an ODE-solver embedded in the training process, which enables the OE-HNN to learn the dynamics from noisy state measurements. In addition, extending HNNs based on the generalized Hamiltonian theory enables to include external inputs into the framework which are important for engineering applications. We demonstrate via simulation examples that the proposed OE-HNNs results in superior modeling performance compared to classical HNNs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2100517697",
                        "name": "Sarvin Moradi"
                    },
                    {
                        "authorId": "32472333",
                        "name": "N. Jaensson"
                    },
                    {
                        "authorId": "2215866736",
                        "name": "Roland T'oth"
                    },
                    {
                        "authorId": "1767103",
                        "name": "M. Schoukens"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Networks [5, 20] where a structural feature of the dynamics (symplecticity, or their Hamiltonian nature) is imposed in addition to the demand for accurate prediction."
            ],
            "citingPaper": {
                "paperId": "0fc99868ba89f3b66b7ff9519268634942cb74e9",
                "externalIds": {
                    "ArXiv": "2304.14214",
                    "DBLP": "journals/corr/abs-2304-14214",
                    "DOI": "10.48550/arXiv.2304.14214",
                    "CorpusId": 258352243
                },
                "corpusId": 258352243,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0fc99868ba89f3b66b7ff9519268634942cb74e9",
                "title": "Some of the variables, some of the parameters, some of the times, with some physics known: Identification with partial information",
                "abstract": "Experimental data is often comprised of variables measured independently, at different sampling rates (non-uniform ${\\Delta}$t between successive measurements); and at a specific time point only a subset of all variables may be sampled. Approaches to identifying dynamical systems from such data typically use interpolation, imputation or subsampling to reorganize or modify the training data $\\textit{prior}$ to learning. Partial physical knowledge may also be available $\\textit{a priori}$ (accurately or approximately), and data-driven techniques can complement this knowledge. Here we exploit neural network architectures based on numerical integration methods and $\\textit{a priori}$ physical knowledge to identify the right-hand side of the underlying governing differential equations. Iterates of such neural-network models allow for learning from data sampled at arbitrary time points $\\textit{without}$ data modification. Importantly, we integrate the network with available partial physical knowledge in\"physics informed gray-boxes\"; this enables learning unknown kinetic rates or microbial growth functions while simultaneously estimating experimental parameters.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "148088192",
                        "name": "S. Malani"
                    },
                    {
                        "authorId": "31588522",
                        "name": "Tom S. Bertalan"
                    },
                    {
                        "authorId": "2053996298",
                        "name": "Tianqi Cui"
                    },
                    {
                        "authorId": "46444330",
                        "name": "J. Avalos"
                    },
                    {
                        "authorId": "2134669612",
                        "name": "Michael Betenbaugh"
                    },
                    {
                        "authorId": "3439407",
                        "name": "I. Kevrekidis"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "884f603df065faa5ff3abea84f5bd1fd696bcfcb",
                "externalIds": {
                    "ArXiv": "2304.14374",
                    "DBLP": "journals/corr/abs-2304-14374",
                    "DOI": "10.48550/arXiv.2304.14374",
                    "CorpusId": 258352371
                },
                "corpusId": 258352371,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/884f603df065faa5ff3abea84f5bd1fd696bcfcb",
                "title": "Pseudo-Hamiltonian neural networks for learning partial differential equations",
                "abstract": "Pseudo-Hamiltonian neural networks (PHNN) were recently introduced for learning dynamical systems that can be modelled by ordinary differential equations. In this paper, we extend the method to partial differential equations. The resulting model is comprised of up to three neural networks, modelling terms representing conservation, dissipation and external forces, and discrete convolution operators that can either be learned or be given as input. We demonstrate numerically the superior performance of PHNN compared to a baseline model that models the full dynamics by a single neural network. Moreover, since the PHNN model consists of three parts with different physical interpretations, these can be studied separately to gain insight into the system, and the learned model is applicable also if external forces are removed or changed.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "102675722",
                        "name": "S\u00f8lve Eidnes"
                    },
                    {
                        "authorId": "143662496",
                        "name": "K. Lye"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "2f403d194b42d10c3a438736388c8812831b1361",
                "externalIds": {
                    "ArXiv": "2304.12944",
                    "DBLP": "conf/icml/SongKSW23",
                    "DOI": "10.48550/arXiv.2304.12944",
                    "CorpusId": 258309133
                },
                "corpusId": 258309133,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2f403d194b42d10c3a438736388c8812831b1361",
                "title": "Latent Traversals in Generative Models as Potential Flows",
                "abstract": "Despite the significant recent progress in deep generative models, the underlying structure of their latent spaces is still poorly understood, thereby making the task of performing semantically meaningful latent traversals an open research challenge. Most prior work has aimed to solve this challenge by modeling latent structures linearly, and finding corresponding linear directions which result in `disentangled' generations. In this work, we instead propose to model latent structures with a learned dynamic potential landscape, thereby performing latent traversals as the flow of samples down the landscape's gradient. Inspired by physics, optimal transport, and neuroscience, these potential landscapes are learned as physically realistic partial differential equations, thereby allowing them to flexibly vary over both space and time. To achieve disentanglement, multiple potentials are learned simultaneously, and are constrained by a classifier to be distinct and semantically self-consistent. Experimentally, we demonstrate that our method achieves both more qualitatively and quantitatively disentangled trajectories than state-of-the-art baselines. Further, we demonstrate that our method can be integrated as a regularization term during training, thereby acting as an inductive bias towards the learning of structured representations, ultimately improving model likelihood on similarly structured data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152601878",
                        "name": "Yue Song"
                    },
                    {
                        "authorId": "2215270642",
                        "name": "Andy Keller"
                    },
                    {
                        "authorId": "1429806753",
                        "name": "N. Sebe"
                    },
                    {
                        "authorId": "1678311",
                        "name": "M. Welling"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Many of the examples cited above involve conservation of quantities based on the symmetry of the equations of motion, such as conservation of energy and momentum [13], or the inclusion of previously derived differential equations [11] as components of the ML training.",
                "[13, 49]) proposed that including conserved quantities such as energy/momentum may help to improve the application of neural networks to physical systems.",
                "Enforcing conservation of quantities such as momentum, mass, or energy [11, 13, 22, 23] for dissipative systems in isolation may not be"
            ],
            "citingPaper": {
                "paperId": "cb524c48e77b8df6db148187ef461cc9ba43758b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-12865",
                    "ArXiv": "2304.12865",
                    "DOI": "10.48550/arXiv.2304.12865",
                    "CorpusId": 258309763
                },
                "corpusId": 258309763,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cb524c48e77b8df6db148187ef461cc9ba43758b",
                "title": "Constraining Chaos: Enforcing dynamical invariants in the training of recurrent neural networks",
                "abstract": "Drawing on ergodic theory, we introduce a novel training method for machine learning based forecasting methods for chaotic dynamical systems. The training enforces dynamical invariants--such as the Lyapunov exponent spectrum and fractal dimension--in the systems of interest, enabling longer and more stable forecasts when operating with limited data. The technique is demonstrated in detail using the recurrent neural network architecture of reservoir computing. Results are given for the Lorenz 1996 chaotic dynamical system and a spectral quasi-geostrophic model, both typical test cases for numerical weather prediction.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1420417375",
                        "name": "Jason A. Platt"
                    },
                    {
                        "authorId": "31549390",
                        "name": "S. Penny"
                    },
                    {
                        "authorId": "2111858422",
                        "name": "T. A. Smith"
                    },
                    {
                        "authorId": "2218345527",
                        "name": "Tse-Chun Chen"
                    },
                    {
                        "authorId": "48489623",
                        "name": "H. Abarbanel"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Using neural networks embedded with physical information will significantly alleviate the above three problems, such as Hamiltonian neural networks [19], Lagrangian neural networks [20], Physics-informed neural networks [21], Neural ODEs [22], etc."
            ],
            "citingPaper": {
                "paperId": "0d478aa09664b7417afe05ffa9880ae27bdb314a",
                "externalIds": {
                    "ArXiv": "2304.10842",
                    "CorpusId": 258291948
                },
                "corpusId": 258291948,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0d478aa09664b7417afe05ffa9880ae27bdb314a",
                "title": "Residual-Based Multi-peak Sampling Algorithm in Inverse Problems of Dynamical Systems",
                "abstract": "Stochastic differential equations can describe a wide range of dynamical systems, and obtaining the governing equations of these systems is the premise of studying the nonlinear dynamic behavior of the system. Neural networks are currently the most popular approach in the inverse problem of dynamical systems. In order to obtain accurate dynamical equations, neural networks need a large amount of trajectory data as a training set. To address this shortcoming, we propose a residual-based multi-peaks sampling algorithm. Evaluate the training results of each epoch of neural network, calculate the probability density function $P(x)$ of the residual, perform sampling where the $P(x)$ is large, and add samples to the training set to retrain the neural network. In order to prevent the neural network from falling into the trap of overfitting, we discretize the sampling points. We conduct case studies using two classical nonlinear dynamical systems and perform bifurcation and first escape probability analyzes of the fitted equations. Results show that our proposed sampling strategy requires only 20$\\sim $30\\% of the sample points of the original method to reconstruct the stochastic dynamical behavior of the system. Finally, the algorithm is tested by adding interference noise to the data, and the results show that the sampling strategy has better numerical robustness and stability.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "9253703",
                        "name": "X. An"
                    },
                    {
                        "authorId": "2068127194",
                        "name": "Lin Du"
                    },
                    {
                        "authorId": "2075350374",
                        "name": "Zichen Deng"
                    },
                    {
                        "authorId": "2215229404",
                        "name": "Yu-jia Zhang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "09241be23b36faae69925755ea6813bcc500d951",
                "externalIds": {
                    "ArXiv": "2304.04885",
                    "DBLP": "journals/cma/AhmedS23",
                    "DOI": "10.1016/j.camwa.2023.06.038",
                    "CorpusId": 258059972
                },
                "corpusId": 258059972,
                "publicationVenue": {
                    "id": "635a0827-5219-4c75-84a2-fab5be8f309f",
                    "name": "Computers and Mathematics with Applications",
                    "type": "journal",
                    "alternate_names": [
                        "Computers & Mathematics With Applications",
                        "Computers & Mathematics with Applications",
                        "Comput Math Appl",
                        "Comput  Math Appl"
                    ],
                    "issn": "0898-1221",
                    "alternate_issns": [
                        "0097-4943"
                    ],
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/301/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/08981221",
                        "https://www.journals.elsevier.com/computers-and-mathematics-with-applications/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/09241be23b36faae69925755ea6813bcc500d951",
                "title": "Forward sensitivity analysis and mode dependent control for closure modeling of Galerkin systems",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109162036",
                        "name": "Shady E. Ahmed"
                    },
                    {
                        "authorId": "3071838",
                        "name": "O. San"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "To promote physical feasibility of the solution, many works impose equality or inequality system constraints by i) encoding hard constraints inside NN layers (e.g. using sigmoid layer to encode technical limits of upper and lower bounds), ii) applying prior on the NN architecture (e.g., Hamiltonian [25] and Lagrangian neural networks [26]), iii) augmenting the objective function with penalty terms in a supervised [13] or unsupervised [17] [14] way, iv) projecting outputs [16] to the feasible domain, or v) combining many different strategies.",
                ", Hamiltonian [25] and Lagrangian neural networks [26]), iii) augmenting the objective function with penalty terms in a supervised [13] or unsupervised [17] [14] way, iv) projecting outputs [16] to the feasible domain, or v) combining many different strategies."
            ],
            "citingPaper": {
                "paperId": "44fe0a95f756374abb3dff8febd22d641ef275d6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-06727",
                    "ArXiv": "2304.06727",
                    "DOI": "10.48550/arXiv.2304.06727",
                    "CorpusId": 258170365
                },
                "corpusId": 258170365,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/44fe0a95f756374abb3dff8febd22d641ef275d6",
                "title": "Contingency Analyses with Warm Starter using Probabilistic Graphical Model",
                "abstract": "Cyberthreats are an increasingly common risk to the power grid and can thwart secure grid operation. We propose to extend contingency analysis (CA) that is currently used to secure the grid against natural threats to protect against cyberthreats. However, unlike traditional N-1 or N-2 contingencies, cyberthreats (e.g., MadIoT) require CA to solve harder N-k (with k>>2) contingencies in a practical amount of time. Purely physics-based solvers, while robust, are slow and may not solve N-k contingencies in a timely manner, whereas the emerging data-driven alternatives to power grid analytics are fast but not sufficiently generalizable, interpretable, or scalable. To address these challenges, we propose a novel conditional Gaussian Random Field-based data-driven method that is bothfast and robust. It achieves speedup by improving starting points for the physical solvers. To improve the physical interpretability and generalizability, the proposed method incorporates domain knowledge by considering the graphical nature of the grid topology. To improve scalability, the method applies physics-informed regularization that reduces the model size and complexity. Experiments validate that simulating MadIoT-induced N-k contingencies with our warm starter requires 5x fewer iterations for a realistic 2000-bus system.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144496881",
                        "name": "Shimiao Li"
                    },
                    {
                        "authorId": "145190652",
                        "name": "Amritanshu Pandey"
                    },
                    {
                        "authorId": "2093783115",
                        "name": "L. Pileggi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "More work is required to implement such physics-aware reasoning layers [220]\u2013[224] into onboard neural-inertial navigation models."
            ],
            "citingPaper": {
                "paperId": "3c3b88d6dc126a9c3dee7387f3253b0963b5c743",
                "externalIds": {
                    "DBLP": "conf/plans/SahaDSGJS23",
                    "DOI": "10.1109/PLANS53410.2023.10139997",
                    "CorpusId": 259122943,
                    "PubMed": "37736264"
                },
                "corpusId": 259122943,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3c3b88d6dc126a9c3dee7387f3253b0963b5c743",
                "title": "Inertial Navigation on Extremely Resource-Constrained Platforms: Methods, Opportunities and Challenges",
                "abstract": "Inertial navigation provides a small footprint, low-power, and low-cost pathway for localization in GPS-denied environments on extremely resource-constrained Internet-of-Things (IoT) platforms. Traditionally, application-specific heuristics and physics-based kinematic models are used to mitigate the curse of drift in inertial odometry. These techniques, albeit lightweight, fail to handle domain shifts and environmental non-linearities. Recently, deep neural-inertial sequence learning has shown superior odometric resolution in capturing non-linear motion dynamics without human knowledge over heuristic-based methods. These AI-based techniques are data-hungry, suffer from excessive resource usage, and cannot guarantee following the underlying system physics. This paper highlights the unique methods, opportunities, and challenges in porting real-time AI-enhanced inertial navigation algorithms onto IoT platforms. First, we discuss how platform-aware neural architecture search coupled with ultra-lightweight model backbones can yield neural-inertial odometry models that are 31\u2013134 x smaller yet achieve or exceed the localization resolution of state-of-the-art AI-enhanced techniques. The framework can generate models suitable for locating humans, animals, underwater sensors, aerial vehicles, and precision robots. Next, we showcase how techniques from neurosymbolic AI can yield physics-informed and interpretable neural-inertial navigation models. Afterward, we present opportunities for fine-tuning pre-trained odometry models in a new domain with as little as 1 minute of labeled data, while discussing inexpensive data collection and labeling techniques. Finally, we identify several open research challenges that demand careful consideration moving forward.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144049820",
                        "name": "Swapnil Sayan Saha"
                    },
                    {
                        "authorId": "2004834847",
                        "name": "Yayun Du"
                    },
                    {
                        "authorId": "2353239",
                        "name": "S. Sandha"
                    },
                    {
                        "authorId": "2094951808",
                        "name": "L. Garcia"
                    },
                    {
                        "authorId": "144667695",
                        "name": "M. Jawed"
                    },
                    {
                        "authorId": "1702254",
                        "name": "M. Srivastava"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[6, 22]) or higher-order explicit Runge-Kutta methods (e.",
                "Recently, researchers have focused on leveraging a continuous-time representation to incorporate physical inductive biases such as symplectic structure [6, 22, 50], the Onsager principle [53], the GENERIC formalism [54] and time-reversal symmetry [29], to name a few, into the learning model."
            ],
            "citingPaper": {
                "paperId": "d520d069f58460a8c806f7591d4498e34831d68f",
                "externalIds": {
                    "ArXiv": "2303.17824",
                    "DBLP": "journals/corr/abs-2303-17824",
                    "DOI": "10.48550/arXiv.2303.17824",
                    "CorpusId": 257900843
                },
                "corpusId": 257900843,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d520d069f58460a8c806f7591d4498e34831d68f",
                "title": "Implementation and (Inverse Modified) Error Analysis for implicitly-templated ODE-nets",
                "abstract": "We focus on learning unknown dynamics from data using ODE-nets templated on implicit numerical initial value problem solvers. First, we perform Inverse Modified error analysis of the ODE-nets using unrolled implicit schemes for ease of interpretation. It is shown that training an ODE-net using an unrolled implicit scheme returns a close approximation of an Inverse Modified Differential Equation (IMDE). In addition, we establish a theoretical basis for hyper-parameter selection when training such ODE-nets, whereas current strategies usually treat numerical integration of ODE-nets as a black box. We thus formulate an adaptive algorithm which monitors the level of error and adapts the number of (unrolled) implicit solution iterations during the training process, so that the error of the unrolled approximation is less than the current learning loss. This helps accelerate training, while maintaining accuracy. Several numerical experiments are performed to demonstrate the advantages of the proposed algorithm compared to nonadaptive unrollings, and validate the theoretical analysis. We also note that this approach naturally allows for incorporating partially known physical terms in the equations, giving rise to what is termed ``gray box\"identification.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2430811",
                        "name": "Aiqing Zhu"
                    },
                    {
                        "authorId": "31588522",
                        "name": "Tom S. Bertalan"
                    },
                    {
                        "authorId": "145801437",
                        "name": "Beibei Zhu"
                    },
                    {
                        "authorId": "1805110",
                        "name": "Yifa Tang"
                    },
                    {
                        "authorId": "3439407",
                        "name": "I. Kevrekidis"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "26bff5c4fdc34b066663e309855fe1adce15eee3",
                "externalIds": {
                    "ArXiv": "2303.15779",
                    "CorpusId": 257771480
                },
                "corpusId": 257771480,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/26bff5c4fdc34b066663e309855fe1adce15eee3",
                "title": "Learnability of Linear Port-Hamiltonian Systems",
                "abstract": "A complete structure-preserving learning scheme for single-input/single-output (SISO) linear port-Hamiltonian systems is proposed. The construction is based on the solution, when possible, of the unique identification problem for these systems, in ways that reveal fundamental relationships between classical notions in control theory and crucial properties in the machine learning context, like structure-preservation and expressive power. In the canonical case, it is shown that the set of uniquely identified systems can be explicitly characterized as a smooth manifold endowed with global Euclidean coordinates, which allows concluding that the parameter complexity necessary for the replication of the dynamics is only $O(n)$ and not $O(n^2)$, as suggested by the standard parametrization of these systems. Furthermore, it is shown that linear port-Hamiltonian systems can be learned while remaining agnostic about the dimension of the underlying data-generating system. Numerical experiments show that this methodology can be used to efficiently estimate linear port-Hamiltonian systems out of input-output realizations, making the contributions in this paper the first example of a structure-preserving machine learning paradigm for linear port-Hamiltonian systems based on explicit representations of this model category.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "143690686",
                        "name": "J. Ortega"
                    },
                    {
                        "authorId": "14351413",
                        "name": "Daiying Yin"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "To the best of our knowledge, this is the first demonstration of the remarkable capacity of numerical integrators of order p > 4 to facilitate the training of Hamiltonian neural networks [9] from sparse datasets, to do accurate interpolation and extrapolation in time.",
                "We follow the idea of Hamiltonian neural networks [9] aiming at approximating the Hamiltonian, H : R \u2192 R, such that H\u03b8 is a neural network and f is approximated by f\u03b8(y) := J\u2207H\u03b8(y).",
                "Hamiltonian neural networks [9] aim at learning energy-preserving dynamical systems from data by approximating the Hamiltonian using neural networks."
            ],
            "citingPaper": {
                "paperId": "ed6511cf6cad615b833503e1f8b27783f95e8461",
                "externalIds": {
                    "DBLP": "conf/gsi/Noren23",
                    "ArXiv": "2303.03769",
                    "DOI": "10.48550/arXiv.2303.03769",
                    "CorpusId": 257378291
                },
                "corpusId": 257378291,
                "publicationVenue": {
                    "id": "edcc5a21-61ec-4fa0-9447-4a8f6e1a5a2d",
                    "name": "International Conference on Geometric Science of Information",
                    "type": "conference",
                    "alternate_names": [
                        "GSI",
                        "Int Conf Geom Sci Inf"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ed6511cf6cad615b833503e1f8b27783f95e8461",
                "title": "Learning Hamiltonian Systems with Mono-Implicit Runge-Kutta Methods",
                "abstract": "Numerical integrators could be used to form interpolation conditions when training neural networks to approximate the vector field of an ordinary differential equation (ODE) from data. When numerical one-step schemes such as the Runge-Kutta methods are used to approximate the temporal discretization of an ODE with a known vector field, properties such as symmetry and stability are much studied. Here, we show that using mono-implicit Runge-Kutta methods of high order allows for accurate training of Hamiltonian neural networks on small datasets. This is demonstrated by numerical experiments where the Hamiltonian of the chaotic double pendulum in addition to the Fermi-Pasta-Ulam-Tsingou system is learned from data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2210857577",
                        "name": "Haakon Noren"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recently, authors have proposed neural network models that use physics-based inductive biases, also known as grey-box models (Lutter et al., 2019a;b; Greydanus et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "1484cc1fc1d7ed1b9e77b230aaee01f4b29e9327",
                "externalIds": {
                    "ArXiv": "2303.03955",
                    "DBLP": "conf/iclr/PalenicekLC023",
                    "DOI": "10.48550/arXiv.2303.03955",
                    "CorpusId": 259373058
                },
                "corpusId": 259373058,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1484cc1fc1d7ed1b9e77b230aaee01f4b29e9327",
                "title": "Diminishing Return of Value Expansion Methods in Model-Based Reinforcement Learning",
                "abstract": "Model-based reinforcement learning is one approach to increase sample efficiency. However, the accuracy of the dynamics model and the resulting compounding error over modelled trajectories are commonly regarded as key limitations. A natural question to ask is: How much more sample efficiency can be gained by improving the learned dynamics models? Our paper empirically answers this question for the class of model-based value expansion methods in continuous control problems. Value expansion methods should benefit from increased model accuracy by enabling longer rollout horizons and better value function approximations. Our empirical study, which leverages oracle dynamics models to avoid compounding model errors, shows that (1) longer horizons increase sample efficiency, but the gain in improvement decreases with each additional expansion step, and (2) the increased model accuracy only marginally increases the sample efficiency compared to learned models with identical horizons. Therefore, longer horizons and increased model accuracy yield diminishing returns in terms of sample efficiency. These improvements in sample efficiency are particularly disappointing when compared to model-free value expansion methods. Even though they introduce no computational overhead, we find their performance to be on-par with model-based value expansion methods. Therefore, we conclude that the limitation of model-based value expansion methods is not the model accuracy of the learned models. While higher model accuracy is beneficial, our experiments show that even a perfect model will not provide an un-rivalled sample efficiency but that the bottleneck lies elsewhere.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1751630928",
                        "name": "Daniel Palenicek"
                    },
                    {
                        "authorId": "49199170",
                        "name": "M. Lutter"
                    },
                    {
                        "authorId": "2067739440",
                        "name": "Jo\u00e3o Carvalho"
                    },
                    {
                        "authorId": "2107720654",
                        "name": "Jan Peters"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u20262017a; Raissi, 2018; Jiang et al., 2019) or with missing terms (Yin et al., 2021), and (c) different domain-specific physical constraints such as energy conservation (Greydanus et al., 2019; Cranmer et al., 2020a), symmetries (Wang et al., 2020b; Finzi et al., 2021; Brandstetter et al., 2022a).",
                ", 2021), and (c) different domain-specific physical constraints such as energy conservation (Greydanus et al., 2019; Cranmer et al., 2020a), symmetries (Wang et al."
            ],
            "citingPaper": {
                "paperId": "29beea4d41678cf3cd1864731c031d5b39360798",
                "externalIds": {
                    "ArXiv": "2303.03181",
                    "DBLP": "journals/corr/abs-2303-03181",
                    "DOI": "10.48550/arXiv.2303.03181",
                    "CorpusId": 257365782
                },
                "corpusId": 257365782,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/29beea4d41678cf3cd1864731c031d5b39360798",
                "title": "MetaPhysiCa: OOD Robustness in Physics-informed Machine Learning",
                "abstract": "A fundamental challenge in physics-informed machine learning (PIML) is the design of robust PIML methods for out-of-distribution (OOD) forecasting tasks. These OOD tasks require learning-to-learn from observations of the same (ODE) dynamical system with different unknown ODE parameters, and demand accurate forecasts even under out-of-support initial conditions and out-of-support ODE parameters. In this work we propose a solution for such tasks, which we define as a meta-learning procedure for causal structure discovery (including invariant risk minimization). Using three different OOD tasks, we empirically observe that the proposed approach significantly outperforms existing state-of-the-art PIML and deep learning methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145872582",
                        "name": "S Chandra Mouli"
                    },
                    {
                        "authorId": "145844377",
                        "name": "M. A. Alam"
                    },
                    {
                        "authorId": "2144726453",
                        "name": "Bruno Ribeiro"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Greydanus et al. (2019) introduce the Hamiltonian NN (HNN), which trains an NN to predict the dynamics with an auxiliary loss based on Hamilton\u2019s equations.",
                "Recently, numerous data-driven approaches have been introduced to learn Hamiltonian systems with neural networks (Greydanus et al., 2019; Cranmer et al., 2020; Zhong et al., 2019; Finzi et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "6c542af2ce928b96fe2431cab07f7568991b7d65",
                "externalIds": {
                    "ArXiv": "2303.01925",
                    "DBLP": "journals/corr/abs-2303-01925",
                    "DOI": "10.48550/arXiv.2303.01925",
                    "CorpusId": 257353725
                },
                "corpusId": 257353725,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6c542af2ce928b96fe2431cab07f7568991b7d65",
                "title": "Learning Energy Conserving Dynamics Efficiently with Hamiltonian Gaussian Processes",
                "abstract": "Hamiltonian mechanics is one of the cornerstones of natural sciences. Recently there has been significant interest in learning Hamiltonian systems in a free-form way directly from trajectory data. Previous methods have tackled the problem of learning from many short, low-noise trajectories, but learning from a small number of long, noisy trajectories, whilst accounting for model uncertainty has not been addressed. In this work, we present a Gaussian process model for Hamiltonian systems with efficient decoupled parameterisation, and introduce an energy-conserving shooting method that allows robust inference from both short and long trajectories. We demonstrate the method's success in learning Hamiltonian systems in various data settings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2068076423",
                        "name": "M. Ross"
                    },
                    {
                        "authorId": "34066178",
                        "name": "Markus Heinonen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e6b9ee5dc6cd8021ac33c32beaef68174c5d0c70",
                "externalIds": {
                    "DOI": "10.1007/s00466-023-02288-w",
                    "CorpusId": 257339675
                },
                "corpusId": 257339675,
                "publicationVenue": {
                    "id": "f1a44872-70ad-4c33-b8bb-8618e573cf3f",
                    "name": "Computational Mechanics",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Mech"
                    ],
                    "issn": "0178-7675",
                    "url": "http://www.springer.com/466",
                    "alternate_urls": [
                        "https://link.springer.com/journal/466"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e6b9ee5dc6cd8021ac33c32beaef68174c5d0c70",
                "title": "A structure-preserving neural differential operator with embedded Hamiltonian constraints for modeling structural dynamics",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1691715793",
                        "name": "David A. Najera-Flores"
                    },
                    {
                        "authorId": "46379606",
                        "name": "M. Todd"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "8980297f3425e6aa7e9597c75b65cf66669e9e9c",
                "externalIds": {
                    "ArXiv": "2303.01030",
                    "DBLP": "journals/corr/abs-2303-01030",
                    "DOI": "10.48550/arXiv.2303.01030",
                    "CorpusId": 257280180
                },
                "corpusId": 257280180,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8980297f3425e6aa7e9597c75b65cf66669e9e9c",
                "title": "Node Embedding from Hamiltonian Information Propagation in Graph Neural Networks",
                "abstract": "Graph neural networks (GNNs) have achieved success in various inference tasks on graph-structured data. However, common challenges faced by many GNNs in the literature include the problem of graph node embedding under various geometries and the over-smoothing problem. To address these issues, we propose a novel graph information propagation strategy called Hamiltonian Dynamic GNN (HDG) that uses a Hamiltonian mechanics approach to learn node embeddings in a graph. The Hamiltonian energy function in HDG is learnable and can adapt to the underlying geometry of any given graph dataset. We demonstrate the ability of HDG to automatically learn the underlying geometry of graph datasets, even those with complex and mixed geometries, through comprehensive evaluations against state-of-the-art baselines on various downstream tasks. We also verify that HDG is stable against small perturbations and can mitigate the over-smoothing problem when stacking many layers.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "22758176",
                        "name": "Qiyu Kang"
                    },
                    {
                        "authorId": "2074108937",
                        "name": "Kai Zhao"
                    },
                    {
                        "authorId": "2006209232",
                        "name": "Yang Song"
                    },
                    {
                        "authorId": "2116422074",
                        "name": "Sijie Wang"
                    },
                    {
                        "authorId": "2164990082",
                        "name": "Rui She"
                    },
                    {
                        "authorId": "3118058",
                        "name": "Wee Peng Tay"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "75b65b2031b6927ab820f8a61cb844e2fb4d971c",
                "externalIds": {
                    "DOI": "10.1098/rspa.2022.0576",
                    "CorpusId": 257233532
                },
                "corpusId": 257233532,
                "publicationVenue": {
                    "id": "b61ce141-a434-431b-a154-68fc26e348f3",
                    "name": "Proceedings of the Royal Society A",
                    "type": "journal",
                    "alternate_names": [
                        "Proc R Soc A",
                        "Proc R Soc Math Phys Eng Sci",
                        "Proceedings of The Royal Society A: Mathematical, Physical and Engineering Sciences"
                    ],
                    "issn": "1364-5021",
                    "url": "https://www.jstor.org/journal/procmathphysengi",
                    "alternate_urls": [
                        "http://rspa.royalsocietypublishing.org/content/by/year",
                        "http://rspa.royalsocietypublishing.org/about",
                        "http://rspa.royalsocietypublishing.org/",
                        "https://royalsocietypublishing.org/journal/rspa"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/75b65b2031b6927ab820f8a61cb844e2fb4d971c",
                "title": "Physics-informed dynamic mode decomposition",
                "abstract": "In this work, we demonstrate how physical principles\u2014such as symmetries, invariances and conservation laws\u2014can be integrated into the dynamic mode decomposition (DMD). DMD is a widely used data analysis technique that extracts low-rank modal structures and dynamics from high-dimensional measurements. However, DMD can produce models that are sensitive to noise, fail to generalize outside the training data and violate basic physical laws. Our physics-informed DMD (piDMD) optimization, which may be formulated as a Procrustes problem, restricts the family of admissible models to a matrix manifold that respects the physical structure of the system. We focus on five fundamental physical principles\u2014conservation, self-adjointness, localization, causality and shift-equivariance\u2014and derive several closed-form solutions and efficient algorithms for the corresponding piDMD optimizations. With fewer degrees of freedom, piDMD models are less prone to overfitting, require less training data, and are often less computationally expensive to build than standard DMD models. We demonstrate piDMD on a range of problems, including energy-preserving fluid flow, the Schr\u00f6dinger equation, solute advection-diffusion and three-dimensional transitional channel flow. In each case, piDMD outperforms standard DMD algorithms in metrics such as spectral identification, state prediction and estimation of optimal forcings and responses.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "83535783",
                        "name": "Peter J. Baddoo"
                    },
                    {
                        "authorId": "108252626",
                        "name": "Benjam\u00edn Herrmann"
                    },
                    {
                        "authorId": "3190206",
                        "name": "B. McKeon"
                    },
                    {
                        "authorId": "46720950",
                        "name": "J. Nathan Kutz"
                    },
                    {
                        "authorId": "3083169",
                        "name": "S. Brunton"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[144] can learn an arbitrary conservation law based on Hamiltonian mechanics."
            ],
            "citingPaper": {
                "paperId": "58e24c21be4370215d8e17a250c6ef91fb1cc7cf",
                "externalIds": {
                    "DOI": "10.1007/s11044-023-09884-x",
                    "CorpusId": 257250933
                },
                "corpusId": 257250933,
                "publicationVenue": {
                    "id": "8b1141ff-10b2-47c8-897d-b229aa06098a",
                    "name": "Multibody system dynamics",
                    "type": "journal",
                    "alternate_names": [
                        "Multibody System Dynamics",
                        "Multibody syst dyn",
                        "Multibody Syst Dyn"
                    ],
                    "issn": "1384-5640",
                    "url": "https://link.springer.com/journal/11044"
                },
                "url": "https://www.semanticscholar.org/paper/58e24c21be4370215d8e17a250c6ef91fb1cc7cf",
                "title": "Multibody dynamics and control using machine learning",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2060967018",
                        "name": "Arash Hashemi"
                    },
                    {
                        "authorId": "40211939",
                        "name": "Grzegorz Orzechowski"
                    },
                    {
                        "authorId": "5460011",
                        "name": "A. Mikkola"
                    },
                    {
                        "authorId": "144304939",
                        "name": "J. McPhee"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Finally, Greydanus & Sosanya (2022) extended Hamiltonian Neural Networks (Greydanus et al., 2019) to model both curl- and divergence-free dynamics simultaneously.",
                "Finally, a recent work from the deep learning community (Greydanus & Sosanya, 2022) extended Hamiltonian Neural Networks (Greydanus et al., 2019) in such a way that, the authors suggest, allows one to model both curl- and divergence-free dynamics simultaneously, for example for reconstructing\u2026",
                "Finally, a recent work from the deep learning community (Greydanus & Sosanya, 2022) extended Hamiltonian Neural Networks (Greydanus et al., 2019) in such a way that, the authors suggest, allows one to model both curl- and divergence-free dynamics simultaneously, for example for reconstructing surface flows from a noisy ocean current dataset.",
                "Finally, Greydanus & Sosanya\n(2022) extended Hamiltonian Neural Networks (Greydanus et al., 2019) to model both curl- and divergence-free dynamics simultaneously."
            ],
            "citingPaper": {
                "paperId": "f8f054c915df72f98070437d4530e8eceedfb7e4",
                "externalIds": {
                    "ArXiv": "2302.10364",
                    "DBLP": "conf/icml/BerlinghieriTBG23",
                    "DOI": "10.48550/arXiv.2302.10364",
                    "CorpusId": 257050347
                },
                "corpusId": 257050347,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f8f054c915df72f98070437d4530e8eceedfb7e4",
                "title": "Gaussian processes at the Helm(holtz): A more fluid model for ocean currents",
                "abstract": "Given sparse observations of buoy velocities, oceanographers are interested in reconstructing ocean currents away from the buoys and identifying divergences in a current vector field. As a first and modular step, we focus on the time-stationary case - for instance, by restricting to short time periods. Since we expect current velocity to be a continuous but highly non-linear function of spatial location, Gaussian processes (GPs) offer an attractive model. But we show that applying a GP with a standard stationary kernel directly to buoy data can struggle at both current reconstruction and divergence identification, due to some physically unrealistic prior assumptions. To better reflect known physical properties of currents, we propose to instead put a standard stationary kernel on the divergence and curl-free components of a vector field obtained through a Helmholtz decomposition. We show that, because this decomposition relates to the original vector field just via mixed partial derivatives, we can still perform inference given the original data with only a small constant multiple of additional computational expense. We illustrate the benefits of our method with theory and experiments on synthetic and real ocean data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2208973678",
                        "name": "Renato Berlinghieri"
                    },
                    {
                        "authorId": "80066929",
                        "name": "Brian L. Trippe"
                    },
                    {
                        "authorId": "2060175778",
                        "name": "David R. Burt"
                    },
                    {
                        "authorId": "46261388",
                        "name": "Ryan Giordano"
                    },
                    {
                        "authorId": "153226288",
                        "name": "K. Srinivasan"
                    },
                    {
                        "authorId": "6386240",
                        "name": "Tamay Ozgokmen"
                    },
                    {
                        "authorId": "1482562642",
                        "name": "Junfei Xia"
                    },
                    {
                        "authorId": "1840690",
                        "name": "Tamara Broderick"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "The quality of the learned model can greatly be improved when prior geometric knowledge about the dynamical system is taken into account such as conservation laws [16,14,5,9,2], symmetries [10,8,7], equilibrium points [19], or asymptotic behaviour of its motions."
            ],
            "citingPaper": {
                "paperId": "5f297be92aaafe68e03fc8f73744307ced7c85d0",
                "externalIds": {
                    "DBLP": "conf/gsi/OffenO23",
                    "ArXiv": "2302.08232",
                    "DOI": "10.1007/978-3-031-38271-0_57",
                    "CorpusId": 256900898
                },
                "corpusId": 256900898,
                "publicationVenue": {
                    "id": "edcc5a21-61ec-4fa0-9447-4a8f6e1a5a2d",
                    "name": "International Conference on Geometric Science of Information",
                    "type": "conference",
                    "alternate_names": [
                        "GSI",
                        "Int Conf Geom Sci Inf"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5f297be92aaafe68e03fc8f73744307ced7c85d0",
                "title": "Learning discrete Lagrangians for variationalPDEs from data and detection of travelling waves",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51299251",
                        "name": "Christian Offen"
                    },
                    {
                        "authorId": "1419469651",
                        "name": "S. Ober-Blobaum"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "fc2fd34a7b38f365e905c3878681b63c8bcecbf8",
                "externalIds": {
                    "PubMedCentral": "9929450",
                    "DOI": "10.1038/s41598-023-29186-8",
                    "CorpusId": 256851744,
                    "PubMed": "36788325"
                },
                "corpusId": 256851744,
                "publicationVenue": {
                    "id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                    "name": "Scientific Reports",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Rep"
                    ],
                    "issn": "2045-2322",
                    "url": "http://www.nature.com/srep/",
                    "alternate_urls": [
                        "http://www.nature.com/srep/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fc2fd34a7b38f365e905c3878681b63c8bcecbf8",
                "title": "Symplectic encoders for physics-constrained variational dynamics inference",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2133300239",
                        "name": "Kiran Bacsa"
                    },
                    {
                        "authorId": "67285417",
                        "name": "Zhilu Lai"
                    },
                    {
                        "authorId": "2157222439",
                        "name": "Wei Liu"
                    },
                    {
                        "authorId": "2139759982",
                        "name": "Michael Todd"
                    },
                    {
                        "authorId": "66773557",
                        "name": "E. Chatzi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "2021; Pathak et al., 2022; Bi et al., 2022; Lam et al., 2022; Nguyen et al., 2023), molecular dynamics (Mardt et al., 2018; Zhong et al., 2019; Greydanus et al., 2019; Mattheakis et al., 2019; Li et al., 2020a), or astrophysics (Tamayo et al., 2016; Cranmer et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "5f6c801ad83eb5266e08c67aaa1dda28597500a5",
                "externalIds": {
                    "ArXiv": "2302.06594",
                    "DBLP": "journals/corr/abs-2302-06594",
                    "DOI": "10.48550/arXiv.2302.06594",
                    "CorpusId": 256827197
                },
                "corpusId": 256827197,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5f6c801ad83eb5266e08c67aaa1dda28597500a5",
                "title": "Geometric Clifford Algebra Networks",
                "abstract": "We propose Geometric Clifford Algebra Networks (GCANs) for modeling dynamical systems. GCANs are based on symmetry group transformations using geometric (Clifford) algebras. We first review the quintessence of modern (plane-based) geometric algebra, which builds on isometries encoded as elements of the $\\mathrm{Pin}(p,q,r)$ group. We then propose the concept of group action layers, which linearly combine object transformations using pre-specified group actions. Together with a new activation and normalization scheme, these layers serve as adjustable $\\textit{geometric templates}$ that can be refined via gradient descent. Theoretical advantages are strongly reflected in the modeling of three-dimensional rigid body transformations as well as large-scale fluid dynamics simulations, showing significantly improved performance over traditional methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "146543374",
                        "name": "David Ruhe"
                    },
                    {
                        "authorId": "38303675",
                        "name": "Jayesh K. Gupta"
                    },
                    {
                        "authorId": "148073179",
                        "name": "Steven De Keninck"
                    },
                    {
                        "authorId": "1678311",
                        "name": "M. Welling"
                    },
                    {
                        "authorId": "78843496",
                        "name": "Johannes Brandstetter"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Table 6 and Table 7 show the result comparison between our proposed method and another DNN-based method HNN (Greydanus et al., 2019) on the two examples.",
                "On the other hand, the approach of data-driven dynamical modeling tries to learn a dynamical system from data, which often generate models that are prone to violation of physics laws as demonstrated in (Greydanus et al., 2019).",
                "\u2026(2019) learns the dynamical system and a Lyapunov function to ensure the exponential stability of predicted system; Hamiltonian neural network (HNN, Greydanus et al. (2019)) targets at the Hamiltonian mechanics, directly learns the Hamiltonian and uses the symplectic vector field to approximate\u2026",
                "For contrastive learning with a single conservation value (e.g. ideal spring mass system), HNN performs slightly better than ConCerNet.",
                "Average R(2) comparison with prior work (HNN (Greydanus et al., 2019)) in conservation property learning Task Conservation ConCerNet HNN Ideal spring mass system x[1](2) + x[2](2) = C 0.",
                "HNN is not applicable to the other two experiments, because they are not Hamiltonian systems.",
                "ConCerNet empirically learns the Angular momentum function and HNN learns the Hamiltonian value.",
                "Simulation error comparison with DNN-based prior work (HNN (Greydanus et al., 2019)) Task Mean square error Violation of conservation laws Baseline NN ConCerNet HNN Baseline NN ConCerNet HNN Ideal spring mass 0.",
                "Existing work includes: Kolter & Manek (2019) learns the dynamical system and a Lyapunov function to ensure the exponential stability of predicted system; Hamiltonian neural network (HNN, Greydanus et al. (2019)) targets at the Hamiltonian mechanics, directly learns the Hamiltonian and uses the symplectic vector field to approximate the dynamics; Lagrangian neural network (LNN, Cranmer et al. (2020)) extends the work of HNN to Lagrangian mechanics.",
                "For example, Greydanus et al. (2019) enforces the Hamiltonian to be conserved in Hamiltonian systems, Cranmer et al. (2020) further extends it to Lagrangian dynamics.",
                "Besides the abovementioned HNN and LNN, a few recent works (Zhang et al., 2018; Liu & Tegmark, 2021; Ha & Jeong, 2021; Liu et al., 2022; Udrescu & Tegmark, 2020) have explored automated approaches to extract the conservation laws from data.",
                ", 2016)) and a DNN-based prior work (HNN, (Greydanus et al., 2019)) and delay the results to Appendix B.",
                "We also compare ConCerNet with one classical modeling method (SINDy, (Brunton et al., 2016)) and a DNN-based prior work (HNN, (Greydanus et al., 2019)) and delay the results to Appendix B.2, where ConCerNet shows similar performance but ConCerNet is more generally applicable."
            ],
            "citingPaper": {
                "paperId": "6dce0657dc212bdef52160ae615d0837d5ed3e8b",
                "externalIds": {
                    "DBLP": "conf/icml/ZhangWDMDN23",
                    "ArXiv": "2302.05783",
                    "DOI": "10.48550/arXiv.2302.05783",
                    "CorpusId": 256827344
                },
                "corpusId": 256827344,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6dce0657dc212bdef52160ae615d0837d5ed3e8b",
                "title": "ConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction",
                "abstract": "Deep neural networks (DNN) have shown great capacity of modeling a dynamical system; nevertheless, they usually do not obey physics constraints such as conservation laws. This paper proposes a new learning framework named ConCerNet to improve the trustworthiness of the DNN based dynamics modeling to endow the invariant properties. ConCerNet consists of two steps: (i) a contrastive learning method to automatically capture the system invariants (i.e. conservation properties) along the trajectory observations; (ii) a neural projection layer to guarantee that the learned dynamics models preserve the learned invariants. We theoretically prove the functional relationship between the learned latent representation and the unknown system invariant function. Experiments show that our method consistently outperforms the baseline neural networks in both coordinate error and conservation metrics by a large margin. With neural network based parameterization and no dependence on prior knowledge, our method can be extended to complex and large-scale dynamics by leveraging an autoencoder.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2205625633",
                        "name": "Wang Zhang"
                    },
                    {
                        "authorId": "27836724",
                        "name": "Tsui-Wei Weng"
                    },
                    {
                        "authorId": "3225635",
                        "name": "Subhro Das"
                    },
                    {
                        "authorId": "1808668",
                        "name": "A. Megretski"
                    },
                    {
                        "authorId": "103706192",
                        "name": "Lucani E. Daniel"
                    },
                    {
                        "authorId": "144274166",
                        "name": "Lam M. Nguyen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Initial works related to the discovery of Lagrangian can be linked to Hamiltonian Neural Networks (HNN) [12, 13].",
                "For learning the Hamiltonian and Lagrangian directly in Cartesian coordinates, Constrained Hamiltonian Neural Networks (CHNNs) and Constrained Lagrangian Neural Networks (CLNNs) were proposed in [16]."
            ],
            "citingPaper": {
                "paperId": "4a2620bb33e4ae7ce398a443df2cdc01a0351f67",
                "externalIds": {
                    "ArXiv": "2302.04400",
                    "DBLP": "journals/corr/abs-2302-04400",
                    "DOI": "10.48550/arXiv.2302.04400",
                    "CorpusId": 256697122
                },
                "corpusId": 256697122,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4a2620bb33e4ae7ce398a443df2cdc01a0351f67",
                "title": "Discovering interpretable Lagrangian of dynamical systems from data",
                "abstract": "A complete understanding of physical systems requires models that are accurate and obeys natural conservation laws. Recent trends in representation learning involve learning Lagrangian from data rather than the direct discovery of governing equations of motion. The generalization of equation discovery techniques has huge potential; however, existing Lagrangian discovery frameworks are black-box in nature. This raises a concern about the reusability of the discovered Lagrangian. In this article, we propose a novel data-driven machine-learning algorithm to automate the discovery of interpretable Lagrangian from data. The Lagrangian are derived in interpretable forms, which also allows the automated discovery of conservation laws and governing equations of motion. The architecture of the proposed framework is designed in such a way that it allows learning the Lagrangian from a subset of the underlying domain and then generalizing for an infinite-dimensional system. The fidelity of the proposed framework is exemplified using examples described by systems of ordinary differential equations and partial differential equations where the Lagrangian and conserved quantities are known.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1389549891",
                        "name": "Tapas Tripura"
                    },
                    {
                        "authorId": "3411759",
                        "name": "S. Chakraborty"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Specifically, some differential equations are chaotic (Greydanus et al., 2019), i.e. sensitive to the initial values of inputs, which is hard for PDDM to fundamentally model the ubiquitous and elusive randomness of chaos with only finite data, leading to poor generalization (Abu-Mostafa et al.,\u2026",
                "Specifically, some differential equations are chaotic (Greydanus et al., 2019), i."
            ],
            "citingPaper": {
                "paperId": "0b90f5c635131a6d918dbc6943540ee0691b6817",
                "externalIds": {
                    "ArXiv": "2302.10184",
                    "DBLP": "journals/corr/abs-2302-10184",
                    "DOI": "10.48550/arXiv.2302.10184",
                    "CorpusId": 257050822
                },
                "corpusId": 257050822,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0b90f5c635131a6d918dbc6943540ee0691b6817",
                "title": "On Robust Numerical Solver for ODE via Self-Attention Mechanism",
                "abstract": "With the development of deep learning techniques, AI-enhanced numerical solvers are expected to become a new paradigm for solving differential equations due to their versatility and effectiveness in alleviating the accuracy-speed trade-off in traditional numerical solvers. However, this paradigm still inevitably requires a large amount of high-quality data, whose acquisition is often very expensive in natural science and engineering problems. Therefore, in this paper, we explore training efficient and robust AI-enhanced numerical solvers with a small data size by mitigating intrinsic noise disturbances. We first analyze the ability of the self-attention mechanism to regulate noise in supervised learning and then propose a simple-yet-effective numerical solver, AttSolver, which introduces an additive self-attention mechanism to the numerical solution of differential equations based on the dynamical system perspective of the residual neural network. Our results on benchmarks, ranging from high-dimensional problems to chaotic systems, demonstrate the effectiveness of AttSolver in generally improving the performance of existing traditional numerical solvers without any elaborated model crafting. Finally, we analyze the convergence, generalization, and robustness of the proposed method experimentally and theoretically.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109670338",
                        "name": "Zhongzhan Huang"
                    },
                    {
                        "authorId": "73445023",
                        "name": "Mingfu Liang"
                    },
                    {
                        "authorId": "2181395240",
                        "name": "Liang Lin"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4fd703e3571fe469226effb3ffc303a8bcd193e0",
                "externalIds": {
                    "ArXiv": "2302.01955",
                    "DBLP": "journals/corr/abs-2302-01955",
                    "DOI": "10.48550/arXiv.2302.01955",
                    "CorpusId": 256615358
                },
                "corpusId": 256615358,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4fd703e3571fe469226effb3ffc303a8bcd193e0",
                "title": "Fixed-kinetic Neural Hamiltonian Flows for enhanced interpretability and reduced complexity",
                "abstract": "Normalizing Flows (NF) are Generative models which are particularly robust and allow for exact sampling of the learned distribution. They however require the design of an invertible mapping, whose Jacobian determinant has to be computable. Recently introduced, Neural Hamiltonian Flows (NHF) are based on Hamiltonian dynamics-based Flows, which are continuous, volume-preserving and invertible and thus make for natural candidates for robust NF architectures. In particular, their similarity to classical Mechanics could lead to easier interpretability of the learned mapping. However, despite being Physics-inspired architectures, the originally introduced NHF architecture still poses a challenge to interpretability. For this reason, in this work, we introduce a fixed kinetic energy version of the NHF model. Inspired by physics, our approach improves interpretability and requires less parameters than previously proposed architectures. We then study the robustness of the NHF architectures to the choice of hyperparameters. We analyze the impact of the number of leapfrog steps, the integration time and the number of neurons per hidden layer, as well as the choice of prior distribution, on sampling a multimodal 2D mixture. The NHF architecture is robust to these choices, especially the fixed-kinetic energy model. Finally, we adapt NHF to the context of Bayesian inference and illustrate our method on sampling the posterior distribution of two cosmological parameters knowing type Ia supernovae observations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2204576085",
                        "name": "Vincent Souveton"
                    },
                    {
                        "authorId": "1795342",
                        "name": "A. Guillin"
                    },
                    {
                        "authorId": "74359295",
                        "name": "J. Jasche"
                    },
                    {
                        "authorId": "152410445",
                        "name": "G. Lavaux"
                    },
                    {
                        "authorId": "14788259",
                        "name": "Manon Michel"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Subsequently, extensive work has been conducted on parametrizing the continuous dynamics of hidden states using an ODE (Greydanus et al., 2019; Lu et al., 2019b; Liu et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "1095fbfe9feb9117819a6d899faf624f2e943e73",
                "externalIds": {
                    "ArXiv": "2302.00854",
                    "DBLP": "journals/corr/abs-2302-00854",
                    "DOI": "10.48550/arXiv.2302.00854",
                    "CorpusId": 256503713
                },
                "corpusId": 256503713,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1095fbfe9feb9117819a6d899faf624f2e943e73",
                "title": "Learning PDE Solution Operator for Continuous Modeling of Time-Series",
                "abstract": "Learning underlying dynamics from data is important and challenging in many real-world scenarios. Incorporating differential equations (DEs) to design continuous networks has drawn much attention recently, however, most prior works make specific assumptions on the type of DEs, making the model specialized for particular problems. This work presents a partial differential equation (PDE) based framework which improves the dynamics modeling capability. Building upon the recent Fourier neural operator, we propose a neural operator that can handle time continuously without requiring iterative operations or specific grids of temporal discretization. A theoretical result demonstrating its universality is provided. We also uncover an intrinsic property of neural operators that improves data efficiency and model generalization by ensuring stability. Our model achieves superior accuracy in dealing with time-dependent PDEs compared to existing models. Furthermore, several numerical pieces of evidence validate that our method better represents a wide range of dynamics and outperforms state-of-the-art DE-based models in real-time-series applications. Our framework opens up a new way for a continuous representation of neural networks that can be readily adopted for real-world applications.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "32849209",
                        "name": "Yesom Park"
                    },
                    {
                        "authorId": "2149219640",
                        "name": "Jaemoo Choi"
                    },
                    {
                        "authorId": "14122974",
                        "name": "Changyeon Yoon"
                    },
                    {
                        "authorId": "2116334602",
                        "name": "Changhoon Song"
                    },
                    {
                        "authorId": "2259103",
                        "name": "Myung-joo Kang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Furthermore, classical mechanics is already used as a framework to encode conserved quantities in continuous-valued data [65, 66]."
            ],
            "citingPaper": {
                "paperId": "14eb1b5d846973785c33fe433a9c43d73be326f2",
                "externalIds": {
                    "ArXiv": "2302.01365",
                    "CorpusId": 256597912
                },
                "corpusId": 256597912,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/14eb1b5d846973785c33fe433a9c43d73be326f2",
                "title": "Contextuality and inductive bias in quantum machine learning",
                "abstract": "Generalisation in machine learning often relies on the ability to encode structures present in data into an inductive bias of the model class. To understand the power of quantum machine learning, it is therefore crucial to identify the types of data structures that lend themselves naturally to quantum models. In this work we look to quantum contextuality -- a form of nonclassicality with links to computational advantage -- for answers to this question. We introduce a framework for studying contextuality in machine learning, which leads us to a definition of what it means for a learning model to be contextual. From this, we connect a central concept of contextuality, called operational equivalence, to the ability of a model to encode a linearly conserved quantity in its label space. A consequence of this connection is that contextuality is tied to expressivity: contextual model classes that encode the inductive bias are generally more expressive than their noncontextual counterparts. To demonstrate this, we construct an explicit toy learning problem -- based on learning the payoff behaviour of a zero-sum game -- for which this is the case. By leveraging tools from geometric quantum machine learning, we then describe how to construct quantum learning models with the associated inductive bias, and show through our toy problem that they outperform their corresponding classical surrogate models. This suggests that understanding learning problems of this form may lead to useful insights about the power of quantum machine learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "31225573",
                        "name": "Joseph Bowles"
                    },
                    {
                        "authorId": "2093117546",
                        "name": "Victoria J. Wright"
                    },
                    {
                        "authorId": "31812830",
                        "name": "M. Farkas"
                    },
                    {
                        "authorId": "3399181",
                        "name": "N. Killoran"
                    },
                    {
                        "authorId": "3048564",
                        "name": "M. Schuld"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "68a0e0ac127f129742352f6076e4413629ab5168",
                "externalIds": {
                    "ArXiv": "2302.01413",
                    "CorpusId": 256597871
                },
                "corpusId": 256597871,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/68a0e0ac127f129742352f6076e4413629ab5168",
                "title": "Solving two-dimensional quantum eigenvalue problems using physics-informed machine learning",
                "abstract": "A particle confined to an impassable box is a paradigmatic and exactly solvable one-dimensional quantum system modeled by an infinite square well potential. Here we explore some of its infinitely many generalizations to two dimensions, including particles confined to rectangle, elliptic, triangle, and cardioid-shaped boxes, using physics-informed neural networks. In particular, we generalize an unsupervised learning algorithm to find the particles' eigenvalues and eigenfunctions. During training, the neural network adjusts its weights and biases, one of which is the energy eigenvalue, so its output approximately solves the Schr\\\"odinger equation with normalized and mutually orthogonal eigenfunctions. The same procedure solves the Helmholtz equation for the harmonics and vibration modes of waves on drumheads or transverse magnetic modes of electromagnetic cavities. Related applications include dynamical billiards, quantum chaos, and Laplacian spectra.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1443437285",
                        "name": "Elliott G. Holliday"
                    },
                    {
                        "authorId": "2468955",
                        "name": "J. Lindner"
                    },
                    {
                        "authorId": "3067672",
                        "name": "W. Ditto"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "as a differentiable computational graph [11], [15], [16], [17] or used separately in an error-learning scheme [3], [11], [18]."
            ],
            "citingPaper": {
                "paperId": "cf2b01c7956e3734a4ed4c63e6a444c2446d2068",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-12921",
                    "DOI": "10.1109/LRA.2022.3222951",
                    "CorpusId": 253661859
                },
                "corpusId": 253661859,
                "publicationVenue": {
                    "id": "93c335b7-edf4-45f5-8ddc-7c5835154945",
                    "name": "IEEE Robotics and Automation Letters",
                    "alternate_names": [
                        "IEEE Robot Autom Lett"
                    ],
                    "issn": "2377-3766",
                    "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=7083369"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cf2b01c7956e3734a4ed4c63e6a444c2446d2068",
                "title": "Hybrid Learning of Time-Series Inverse Dynamics Models for Locally Isotropic Robot Motion",
                "abstract": "Applications of force control and motion planning often rely on an inverse dynamics model to represent the high-dimensional dynamic behavior of robots during motion. The widespread occurrence of low-velocity, small-scale, locally isotropic motion (LIMO) typically complicates the identification of appropriate models due to the exaggeration of dynamic effects and sensory perturbation caused by complex friction and phenomena of hysteresis, e.g., pertaining to joint elasticity. We propose a hybrid model learning base architecture combining a rigid body dynamics model identified by parametric regression and time-series neural network architectures based on multilayer-perceptron, LSTM, and Transformer topologies. Further, we introduce a novel joint-wise rotational history encoding, reinforcing temporal information to effectively model dynamic hysteresis. The models are evaluated on a KUKA iiwa 14 during algorithmically generated locally isotropic movements. Together with the rotational encoding, the proposed architectures outperform state-of-the-art baselines by a magnitude of 10$^{3}$ yielding an RMSE of 0.14 Nm. Leveraging the hybrid structure and time-series encoding capabilities, our approach allows for accurate torque estimation, indicating its applicability in critically force-sensitive applications during motion sequences exceeding the capacity of conventional inverse dynamics models while retaining trainability in face of scarce data and explainability due to the employed physics model prior.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1390119731",
                        "name": "Tolga-Can Callar"
                    },
                    {
                        "authorId": "1381869412",
                        "name": "Sven B\u00f6ttger"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "c766175d1996f31675cc2dd1125ae94f46bc2890",
                "externalIds": {
                    "DBLP": "journals/cce/GallupGP23",
                    "DOI": "10.1016/j.compchemeng.2022.108111",
                    "CorpusId": 255895090
                },
                "corpusId": 255895090,
                "publicationVenue": {
                    "id": "e4866174-bf20-4fac-a8e3-2f02202f3cd8",
                    "name": "Computers and Chemical Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Comput  Chem Eng",
                        "Comput Chem Eng",
                        "Computers & Chemical Engineering",
                        "Computer-aided chemical engineering",
                        "Comput chem eng"
                    ],
                    "issn": "0098-1354",
                    "alternate_issns": [
                        "1570-7946"
                    ],
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/349/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/bookseries/15707946",
                        "http://www.sciencedirect.com/science/journal/00981354"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c766175d1996f31675cc2dd1125ae94f46bc2890",
                "title": "Physics-guided neural networks with engineering domain knowledge for hybrid process modeling",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "95718138",
                        "name": "E. Gallup"
                    },
                    {
                        "authorId": "2201172843",
                        "name": "Tyler Gallup"
                    },
                    {
                        "authorId": "1946757",
                        "name": "Kody M. Powell"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "N-Body Trajectory We test our model as well as the baselines, Augerino and SymmetryGAN, on the simulated n-body trajectory dataset from Hamiltonian NN (Greydanus et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "9c4427faea5ae85e4fed79d78564d523fc9c5201",
                "externalIds": {
                    "ArXiv": "2302.00236",
                    "DBLP": "journals/corr/abs-2302-00236",
                    "DOI": "10.48550/arXiv.2302.00236",
                    "CorpusId": 256459428
                },
                "corpusId": 256459428,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9c4427faea5ae85e4fed79d78564d523fc9c5201",
                "title": "Generative Adversarial Symmetry Discovery",
                "abstract": "Despite the success of equivariant neural networks in scientific applications, they require knowing the symmetry group a priori. However, it may be difficult to know which symmetry to use as an inductive bias in practice. Enforcing the wrong symmetry could even hurt the performance. In this paper, we propose a framework, LieGAN, to automatically discover equivariances from a dataset using a paradigm akin to generative adversarial training. Specifically, a generator learns a group of transformations applied to the data, which preserve the original distribution and fool the discriminator. LieGAN represents symmetry as interpretable Lie algebra basis and can discover various symmetries such as the rotation group $\\mathrm{SO}(n)$, restricted Lorentz group $\\mathrm{SO}(1,3)^+$ in trajectory prediction and top-quark tagging tasks. The learned symmetry can also be readily used in several existing equivariant neural networks to improve accuracy and generalization in prediction.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145743311",
                        "name": "Jianwei Yang"
                    },
                    {
                        "authorId": "153401894",
                        "name": "R. Walters"
                    },
                    {
                        "authorId": "9716460",
                        "name": "Nima Dehmamy"
                    },
                    {
                        "authorId": "2023052",
                        "name": "Rose Yu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "eb62f0a1aa1bce9ebfbe167286b98b47ba1e44a4",
                "externalIds": {
                    "DBLP": "journals/kais/ChoiCHLLP23",
                    "DOI": "10.1007/s10115-023-01829-2",
                    "CorpusId": 256492729
                },
                "corpusId": 256492729,
                "publicationVenue": {
                    "id": "1f55639d-134e-44ae-b050-ccf2a6676bc5",
                    "name": "Knowledge and Information Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Inf Syst"
                    ],
                    "issn": "0219-3116",
                    "url": "https://link.springer.com/journal/10115"
                },
                "url": "https://www.semanticscholar.org/paper/eb62f0a1aa1bce9ebfbe167286b98b47ba1e44a4",
                "title": "Climate modeling with neural advection\u2013diffusion equation",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "123881593",
                        "name": "Hwan-Kyu Choi"
                    },
                    {
                        "authorId": "2112184918",
                        "name": "Jeongwhan Choi"
                    },
                    {
                        "authorId": "2305736",
                        "name": "JeeHyun Hwang"
                    },
                    {
                        "authorId": "2120214038",
                        "name": "Kookjin Lee"
                    },
                    {
                        "authorId": "2148665454",
                        "name": "Dongeun Lee"
                    },
                    {
                        "authorId": "5166698",
                        "name": "Noseong Park"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recent approaches have been interested in combining deep learning algorithms with physical knowledge (Greydanus et al., 2019; Brunton et al., 2020; Willard et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "6195e952748fb9252320e7d02f0c5eca510f35c9",
                "externalIds": {
                    "ArXiv": "2301.11647",
                    "DBLP": "conf/icml/BleisteinFJG23",
                    "DOI": "10.48550/arXiv.2301.11647",
                    "CorpusId": 256358622
                },
                "corpusId": 256358622,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6195e952748fb9252320e7d02f0c5eca510f35c9",
                "title": "Learning the Dynamics of Sparsely Observed Interacting Systems",
                "abstract": "We address the problem of learning the dynamics of an unknown non-parametric system linking a target and a feature time series. The feature time series is measured on a sparse and irregular grid, while we have access to only a few points of the target time series. Once learned, we can use these dynamics to predict values of the target from the previous values of the feature time series. We frame this task as learning the solution map of a controlled differential equation (CDE). By leveraging the rich theory of signatures, we are able to cast this non-linear problem as a high-dimensional linear regression. We provide an oracle bound on the prediction error which exhibits explicit dependencies on the individual-specific sampling schemes. Our theoretical results are illustrated by simulations which show that our method outperforms existing algorithms for recovering the full time series while being computationally cheap. We conclude by demonstrating its potential on real-world epidemiological data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2203246062",
                        "name": "Linus Bleistein"
                    },
                    {
                        "authorId": "1438299422",
                        "name": "Adeline Fermanian"
                    },
                    {
                        "authorId": "2560595",
                        "name": "A. Jannot"
                    },
                    {
                        "authorId": "2991128",
                        "name": "Agathe Guilloux"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "bd9b3f0a0d663d27703e095e23b04b836fcd9094",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-07928",
                    "ArXiv": "2301.07928",
                    "DOI": "10.1063/5.0142969",
                    "CorpusId": 255999847,
                    "PubMed": "37276568"
                },
                "corpusId": 255999847,
                "publicationVenue": {
                    "id": "30c0ded7-c8b4-473c-bbc0-f237234ac1a6",
                    "name": "Chaos",
                    "type": "journal",
                    "issn": "1054-1500",
                    "url": "http://chaos.aip.org/",
                    "alternate_urls": [
                        "https://aip.scitation.org/journal/cha"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bd9b3f0a0d663d27703e095e23b04b836fcd9094",
                "title": "Hamiltonian Neural Networks with Automatic Symmetry Detection",
                "abstract": "Recently, Hamiltonian neural networks (HNNs) have been introduced to incorporate prior physical knowledge when learning the dynamical equations of Hamiltonian systems. Hereby, the symplectic system structure is preserved despite the data-driven modeling approach. However, preserving symmetries requires additional attention. In this research, we enhance HNN with a Lie algebra framework to detect and embed symmetries in the neural network. This approach allows us to simultaneously learn the symmetry group action and the total energy of the system. As illustrating examples, a pendulum on a cart and a two-body problem from astrodynamics are considered.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2090843477",
                        "name": "Eva Dierkes"
                    },
                    {
                        "authorId": "51299251",
                        "name": "Christian Offen"
                    },
                    {
                        "authorId": "1419469651",
                        "name": "S. Ober-Blobaum"
                    },
                    {
                        "authorId": "2462599",
                        "name": "K. Fla\u00dfkamp"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "9e4412fd2e4c62b5ec63cef27e48fc92093c3ff5",
                "externalIds": {
                    "DOI": "10.3389/fmars.2022.1034188",
                    "CorpusId": 255968683
                },
                "corpusId": 255968683,
                "publicationVenue": {
                    "id": "1257031a-9c82-43ab-b4e5-1c8749f9dd94",
                    "name": "Frontiers in Marine Science",
                    "type": "journal",
                    "alternate_names": [
                        "Front Mar Sci"
                    ],
                    "issn": "2296-7745",
                    "url": "https://www.frontiersin.org/journals/marine-science",
                    "alternate_urls": [
                        "http://www.frontiersin.org/Marine_Science/archive",
                        "http://www.frontiersin.org/Marine_Science"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9e4412fd2e4c62b5ec63cef27e48fc92093c3ff5",
                "title": "Deep blue artificial intelligence for knowledge discovery of the intermediate ocean",
                "abstract": "Oceans at a depth ranging from ~100 to ~1000-m (defined as the intermediate water here), though poorly understood compared to the sea surface, is a critical layer of the Earth system where many important oceanographic processes take place. Advances in ocean observation and computer technology have allowed ocean science to enter the era of big data (to be precise, big data for the surface layer, small data for the bottom layer, and the intermediate layer sits in between) and greatly promoted our understanding of near-surface ocean phenomena. During the past few decades, however, the intermediate ocean is also undergoing profound changes because of global warming, the research and prediction of which are of intensive concern. Due to the lack of three-dimensional ocean theories and field observations, how to remotely sense the intermediate ocean from space becomes a very attractive but challenging scientific issue. With the rapid development of the next generation of information technology, artificial intelligence (AI) has built a new bridge from data science to marine science (called Deep Blue AI, DBAI), which acts as a powerful weapon to extend the paradigm of modern oceanography in the era of the metaverse. This review first introduces the basic prior knowledge of water movement in the ~100 m ocean and vertical stratification within the ~1000-m depths as well as the data resources provided by satellite remote sensing, field observation, and model reanalysis for DBAI. Then, three universal DBAI methodologies, namely, associative statistical, physically informed, and mathematically driven neural networks, are elucidated in the context of intermediate ocean remote sensing. Finally, the unique advantages and potentials of DBAI in data mining and knowledge discovery are demonstrated in a top-down way of \u201csurface-to-interior\u201d via several typical examples in physical and biological oceanography.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2116389326",
                        "name": "Ge Chen"
                    },
                    {
                        "authorId": "29305014",
                        "name": "Baoxiang Huang"
                    },
                    {
                        "authorId": "2118580575",
                        "name": "Jie Yang"
                    },
                    {
                        "authorId": "1688358",
                        "name": "Milena Radenkovic"
                    },
                    {
                        "authorId": "2087750228",
                        "name": "Linyao Ge"
                    },
                    {
                        "authorId": "153522637",
                        "name": "Chuanchuan Cao"
                    },
                    {
                        "authorId": "2186395375",
                        "name": "Xiaoyan Chen"
                    },
                    {
                        "authorId": "49351289",
                        "name": "Linghui Xia"
                    },
                    {
                        "authorId": "2067641108",
                        "name": "Guiyan Han"
                    },
                    {
                        "authorId": "2186375863",
                        "name": "Ying Ma"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "7db517a20c74fc35243548d96614363162ad0169",
                "externalIds": {
                    "ArXiv": "2301.07503",
                    "DBLP": "journals/corr/abs-2301-07503",
                    "DOI": "10.48550/arXiv.2301.07503",
                    "CorpusId": 255998217
                },
                "corpusId": 255998217,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7db517a20c74fc35243548d96614363162ad0169",
                "title": "Model-agnostic machine learning of conservation laws from data",
                "abstract": "We present a machine learning based method for learning first integrals of systems of ordinary differential equations from given trajectory data. The method is model-agnostic in that it does not require explicit knowledge of the underlying system of differential equations that generated the trajectories. As a by-product, once the first integrals have been learned, also the system of differential equations will be known. We illustrate our method by considering several classical problems from the mathematical sciences.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2052308520",
                        "name": "Shivam Arora"
                    },
                    {
                        "authorId": "2327234",
                        "name": "Alexander Bihlo"
                    },
                    {
                        "authorId": "153413258",
                        "name": "Rudiger Brecht"
                    },
                    {
                        "authorId": "66582620",
                        "name": "P. Holba"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026Lagrangian and Hamiltonian neural networks (Cranmer et al., 2020; Lutter et al., 2019; Zhong and Leonard, 2020; Allen-Blanchette et al., 2020; Greydanus et al., 2019; Matsubara et al., 2020; Toth et al., 2020; Finzi et al., 2020), and it also enables learning control-oriented dynamics models\u2026",
                "This approach allows for the inclusion of general forms physics knowledge into data-driven models , such as for so-called Lagrangian and Hamiltonian neural networks (Cranmer et al., 2020; Lutter et al., 2019; Zhong and Leonard, 2020; Allen-Blanchette et al., 2020; Greydanus et al., 2019; Matsubara et al., 2020; Toth et al., 2020; Finzi et al., 2020), and it also enables learning control-oriented dynamics models (Zhong et al."
            ],
            "citingPaper": {
                "paperId": "55bd634767dba7fc7736186c6d03732206e0d53d",
                "externalIds": {
                    "ArXiv": "2301.03565",
                    "DBLP": "journals/corr/abs-2301-03565",
                    "DOI": "10.48550/arXiv.2301.03565",
                    "CorpusId": 255545787
                },
                "corpusId": 255545787,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/55bd634767dba7fc7736186c6d03732206e0d53d",
                "title": "Physics-Informed Kernel Embeddings: Integrating Prior System Knowledge with Data-Driven Control",
                "abstract": "Data-driven control algorithms use observations of system dynamics to construct an implicit model for the purpose of control. However, in practice, data-driven techniques often require excessive sample sizes, which may be infeasible in real-world scenarios where only limited observations of the system are available. Furthermore, purely data-driven methods often neglect useful a priori knowledge, such as approximate models of the system dynamics. We present a method to incorporate such prior knowledge into data-driven control algorithms using kernel embeddings, a nonparametric machine learning technique based in the theory of reproducing kernel Hilbert spaces. Our proposed approach incorporates prior knowledge of the system dynamics as a bias term in the kernel learning problem. We formulate the biased learning problem as a least-squares problem with a regularization term that is informed by the dynamics, that has an efficiently computable, closed-form solution. Through numerical experiments, we empirically demonstrate the improved sample efficiency and out-of-sample generalization of our approach over a purely data-driven baseline. We demonstrate an application of our method to control through a target tracking problem with nonholonomic dynamics, and on spring-mass-damper and F-16 aircraft state prediction tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2057725021",
                        "name": "Adam J. Thorpe"
                    },
                    {
                        "authorId": "1796254983",
                        "name": "Cyrus Neary"
                    },
                    {
                        "authorId": "2152050811",
                        "name": "Franck Djeumou"
                    },
                    {
                        "authorId": "1722063",
                        "name": "Meeko Oishi"
                    },
                    {
                        "authorId": "3199888",
                        "name": "U. Topcu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "The main assumption that we make is that the energy of the system is conserved for short periods of time thanks to the energy injected by the motor, which allows us to use the HNN [Greydanus et al., 2019].",
                "This approach in developing arbitrary coordinates has been proposed in the original HNN paper [Greydanus et al., 2019].",
                "used by Hamiltonian neural networks is to learn a parametric function in the form of a neural network for the Hamiltonian itself [Greydanus et al., 2019].",
                "This work leverages the Hamiltonian neural network (HNN) [Greydanus et al., 2019] to learn the Hamiltonian equations of energyconserving dynamical systems from noisy data.",
                "number of systems [Greydanus et al., 2019], including an ideal mass-spring system."
            ],
            "citingPaper": {
                "paperId": "f614d21b9eac018f2ee131a3d2b9b746940b2398",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-02243",
                    "ArXiv": "2301.02243",
                    "DOI": "10.48550/arXiv.2301.02243",
                    "CorpusId": 255522636
                },
                "corpusId": 255522636,
                "publicationVenue": {
                    "id": "8ef5945c-5b25-4774-b55a-15cd5450f6e4",
                    "name": "International Conference on Pattern Recognition Applications and Methods",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Pattern Recognit Appl Method",
                        "ICPRAM"
                    ],
                    "url": "http://icpram.org/"
                },
                "url": "https://www.semanticscholar.org/paper/f614d21b9eac018f2ee131a3d2b9b746940b2398",
                "title": "Machine Fault Classification using Hamiltonian Neural Networks",
                "abstract": "A new approach is introduced to classify faults in rotating machinery based on the total energy signature estimated from sensor measurements. The overall goal is to go beyond using black-box models and incorporate additional physical constraints that govern the behavior of mechanical systems. Observational data is used to train Hamiltonian neural networks that describe the conserved energy of the system for normal and various abnormal regimes. The estimated total energy function, in the form of the weights of the Hamiltonian neural network, serves as the new feature vector to discriminate between the faults using off-the-shelf classification models. The experimental results are obtained using the MaFaulDa database, where the proposed model yields a promising area under the curve (AUC) of $0.78$ for the binary classification (normal vs abnormal) and $0.84$ for the multi-class problem (normal, and $5$ different abnormal regimes).",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "98084408",
                        "name": "Jer-Sheng Shen"
                    },
                    {
                        "authorId": "1492004686",
                        "name": "Jawad Chowdhury"
                    },
                    {
                        "authorId": "2195972947",
                        "name": "Sourav Banerjee"
                    },
                    {
                        "authorId": "2980488",
                        "name": "G. Terejanu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Examples are hamiltonian, symplectic, and lagrangian neural networks [31, 32, 33] and the physics-informed neural networks [34], aiming at exploiting the best of both worlds, namely the expressive power of nonlinear function approximators with grounded physical knowledge."
            ],
            "citingPaper": {
                "paperId": "187c02ed65f48a8080bb16adc9cdfae3dbb52668",
                "externalIds": {
                    "ArXiv": "2212.14253",
                    "DBLP": "journals/corr/abs-2212-14253",
                    "DOI": "10.1002/oca.3025",
                    "CorpusId": 255341005
                },
                "corpusId": 255341005,
                "publicationVenue": {
                    "id": "1552b1c3-793b-41bc-8910-1b8edc374f09",
                    "name": "Optimal control applications & methods",
                    "type": "journal",
                    "alternate_names": [
                        "Optim control appl  method",
                        "Optimal Control Applications & Methods",
                        "Optim Control Appl  Method"
                    ],
                    "issn": "0143-2087",
                    "url": "http://www3.interscience.wiley.com/cgi-bin/jhome/2133/",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/10991514"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/187c02ed65f48a8080bb16adc9cdfae3dbb52668",
                "title": "Discovering Efficient Periodic Behaviours in Mechanical Systems via Neural Approximators",
                "abstract": "It is well known that conservative mechanical systems exhibit local oscillatory behaviours due to their elastic and gravitational potentials, which completely characterise these periodic motions together with the inertial properties of the system. The classification of these periodic behaviours and their geometric characterisation are in an on-going secular debate, which recently led to the so-called eigenmanifold theory. The eigenmanifold characterises nonlinear oscillations as a generalisation of linear eigenspaces. With the motivation of performing periodic tasks efficiently, we use tools coming from this theory to construct an optimization problem aimed at inducing desired closed-loop oscillations through a state feedback law. We solve the constructed optimization problem via gradient-descent methods involving neural networks. Extensive simulations show the validity of the approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1490934784",
                        "name": "Yannik P. Wotte"
                    },
                    {
                        "authorId": "2176001308",
                        "name": "Sven Dummer"
                    },
                    {
                        "authorId": "118468227",
                        "name": "N. Botteghi"
                    },
                    {
                        "authorId": "2065340953",
                        "name": "C. Brune"
                    },
                    {
                        "authorId": "1711747",
                        "name": "S. Stramigioli"
                    },
                    {
                        "authorId": "21857982",
                        "name": "Federico Califano"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "For example, Hamiltonian neural networks can model dynamics that obey exact conservation laws [14, 9], and monotonic neural networks can model monotonically increasing dynamics [1, 45, 56].",
                "HNN is Hamiltonian neural networks [14], which models the Hamiltonian by a neural network.",
                "Hamiltonian and generalized Hamiltonian neural networks can place physics-inspired priors on systems [14, 9].",
                "Neural network-based methods for modeling continuous-time ODEs require derivative regression [14] or computationally expensive numerical integration to solve the ODEs [7, 40, 51, 30, 8]."
            ],
            "citingPaper": {
                "paperId": "90813609def379739274d4db8337b155ac43e48e",
                "externalIds": {
                    "ArXiv": "2212.13033",
                    "DBLP": "journals/corr/abs-2212-13033",
                    "DOI": "10.48550/arXiv.2212.13033",
                    "CorpusId": 255125509
                },
                "corpusId": 255125509,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/90813609def379739274d4db8337b155ac43e48e",
                "title": "Modeling Nonlinear Dynamics in Continuous Time with Inductive Biases on Decay Rates and/or Frequencies",
                "abstract": "We propose a neural network-based model for nonlinear dynamics in continuous time that can impose inductive biases on decay rates and/or frequencies. Inductive biases are helpful for training neural networks especially when training data are small. The proposed model is based on the Koopman operator theory, where the decay rate and frequency information is used by restricting the eigenvalues of the Koopman operator that describe linear evolution in a Koopman space. We use neural networks to find an appropriate Koopman space, which are trained by minimizing multi-step forecasting and backcasting errors using irregularly sampled time-series data. Experiments on various time-series datasets demonstrate that the proposed method achieves higher forecasting performance given a single short training sequence than the existing methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2664600",
                        "name": "Tomoharu Iwata"
                    },
                    {
                        "authorId": "1704932",
                        "name": "Y. Kawahara"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "28267f3b28715648c126f41c4a27c6f2ffca615d",
                "externalIds": {
                    "ArXiv": "2212.12380",
                    "DBLP": "journals/corr/abs-2212-12380",
                    "DOI": "10.48550/arXiv.2212.12380",
                    "CorpusId": 255096494
                },
                "corpusId": 255096494,
                "publicationVenue": {
                    "id": "00a6fb8b-7f32-4ae0-a417-8033f5f369f9",
                    "name": "Applied Energy",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Energy"
                    ],
                    "issn": "0306-2619",
                    "url": "https://www.journals.elsevier.com/applied-energy",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/03062619"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/28267f3b28715648c126f41c4a27c6f2ffca615d",
                "title": "Towards Scalable Physically Consistent Neural Networks: an Application to Data-driven Multi-zone Thermal Building Models",
                "abstract": "With more and more data being collected, data-driven modeling methods have been gaining in popularity in recent years. While physically sound, classical gray-box models are often cumbersome to identify and scale, and their accuracy might be hindered by their limited expressiveness. On the other hand, classical black-box methods, typically relying on Neural Networks (NNs) nowadays, often achieve impressive performance, even at scale, by deriving statistical patterns from data. However, they remain completely oblivious to the underlying physical laws, which may lead to potentially catastrophic failures if decisions for real-world physical systems are based on them. Physically Consistent Neural Networks (PCNNs) were recently developed to address these aforementioned issues, ensuring physical consistency while still leveraging NNs to attain state-of-the-art accuracy. In this work, we scale PCNNs to model building temperature dynamics and propose a thorough comparison with classical gray-box and black-box methods. More precisely, we design three distinct PCNN extensions, thereby exemplifying the modularity and flexibility of the architecture, and formally prove their physical consistency. In the presented case study, PCNNs are shown to achieve state-of-the-art accuracy, even outperforming classical NN-based models despite their constrained structure. Our investigations furthermore provide a clear illustration of NNs achieving seemingly good performance while remaining completely physics-agnostic, which can be misleading in practice. While this performance comes at the cost of computational complexity, PCNNs on the other hand show accuracy improvements of 17-35% compared to all other physically consistent methods, paving the way for scalable physically consistent models with state-of-the-art performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2143195427",
                        "name": "Loris Di Natale"
                    },
                    {
                        "authorId": "2378010",
                        "name": "B. Svetozarevic"
                    },
                    {
                        "authorId": "134579497",
                        "name": "Philipp Heer"
                    },
                    {
                        "authorId": "2115201467",
                        "name": "Colin Jones"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "[11] assume the underlying system is measure-preserving, and their network learns the Hamiltonian during training."
            ],
            "citingPaper": {
                "paperId": "a0d3246b4e5fb719234dfd6c608263fdf4528bd0",
                "externalIds": {
                    "ArXiv": "2212.12086",
                    "DBLP": "journals/corr/abs-2212-12086",
                    "DOI": "10.48550/arXiv.2212.12086",
                    "CorpusId": 255096316
                },
                "corpusId": 255096316,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a0d3246b4e5fb719234dfd6c608263fdf4528bd0",
                "title": "Eigenvalue initialisation and regularisation for Koopman autoencoders",
                "abstract": ",",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118098570",
                        "name": "Jack W. Miller"
                    },
                    {
                        "authorId": "2072014174",
                        "name": "Charles O'Neill"
                    },
                    {
                        "authorId": "88640650",
                        "name": "N. Constantinou"
                    },
                    {
                        "authorId": "1927247",
                        "name": "Omri Azencot"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "The propagator objective is used within algorithms such as dynamic mode decomposition (DMD) [61] and sparse identification of nonlinear dynamics (SINDy) [10] and for training certain neural networks such as Hamiltonian neural networks [29]."
            ],
            "citingPaper": {
                "paperId": "6eb434661938b64cb44f63444a3ae356b05de3a9",
                "externalIds": {
                    "ArXiv": "2212.13902",
                    "CorpusId": 256105637
                },
                "corpusId": 256105637,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6eb434661938b64cb44f63444a3ae356b05de3a9",
                "title": "Likelihood-based generalization of Markov parameter estimation and multiple shooting objectives in system identification",
                "abstract": "This paper considers the problem of system identi\ufb01cation (ID) of linear and nonlinear non-autonomous systems from noisy and sparse data. We propose and analyze an objective function derived from a Bayesian formulation for learning a hidden Markov model with stochastic dynamics. We then analyze this objective function in the context of several state-of-the-art approaches for both linear and nonlinear system ID. In the former, we analyze least squares approaches for Markov parameter estimation, and in the latter, we analyze the multiple shooting approach. We demonstrate the limitations of the optimization problems posed by these existing methods by showing that they can be seen as special cases of the proposed optimization objective under certain simplifying assumptions: conditional independence of data and zero model error. Furthermore, we observe that our proposed approach has improved smoothness and inherent regularization that make it well-suited for system ID and provide mathematical explanations for these characteristics\u2019 origins. Finally, numerical simulations demonstrate a mean squared error over 8.7 times lower compared to multiple shooting when data are noisy and/or sparse. Moreover, the proposed approach can identify accurate and generalizable models even when there are more parameters than data or when the underlying system exhibits chaotic behavior.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2093676645",
                        "name": "Nicholas Galioto"
                    },
                    {
                        "authorId": "2439226",
                        "name": "A. Gorodetsky"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Section 7.2 of Finzi et al. (2021) points to the paper by Greydanus et al. (2019), where the authors look to learn the Hamiltonian of a system coming from Hamiltonian mechanics."
            ],
            "citingPaper": {
                "paperId": "0c1680a4f437a1b37dab45ef0947e7eba665df58",
                "externalIds": {
                    "ArXiv": "2212.08630",
                    "DBLP": "journals/corr/abs-2212-08630",
                    "DOI": "10.48550/arXiv.2212.08630",
                    "CorpusId": 254823179
                },
                "corpusId": 254823179,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0c1680a4f437a1b37dab45ef0947e7eba665df58",
                "title": "Brauer's Group Equivariant Neural Networks",
                "abstract": "We provide a full characterisation of all of the possible group equivariant neural networks whose layers are some tensor power of $\\mathbb{R}^{n}$ for three symmetry groups that are missing from the machine learning literature: $O(n)$, the orthogonal group; $SO(n)$, the special orthogonal group; and $Sp(n)$, the symplectic group. In particular, we find a spanning set of matrices for the learnable, linear, equivariant layer functions between such tensor power spaces in the standard basis of $\\mathbb{R}^{n}$ when the group is $O(n)$ or $SO(n)$, and in the symplectic basis of $\\mathbb{R}^{n}$ when the group is $Sp(n)$.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2163390645",
                        "name": "Edward Pearce-Crump"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In addition to physical inductive biases encodable via differentiable equations, there have also been recently developed methods to effectively impose energy conservation on learnt representations (Greydanus et al., 2019; Cranmer et al., 2020).",
                "\u2026in machine learning, e.g. ranging from convolutional neural networks (LeCun et al., 1998) and geometrically-invariant networks (Giles & Maxwell, 1987) through to recent examples such as Hamiltonian neural networks (Greydanus et al., 2019) and physics-informed neural networks (Raissi et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "ea4284f26942208f245763ffa1d3d804c0db6b61",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-07462",
                    "ArXiv": "2212.07462",
                    "DOI": "10.48550/arXiv.2212.07462",
                    "CorpusId": 254685913
                },
                "corpusId": 254685913,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ea4284f26942208f245763ffa1d3d804c0db6b61",
                "title": "Harmonic (Quantum) Neural Networks",
                "abstract": "Harmonic functions are abundant in nature, appearing in limiting cases of Maxwell's, Navier-Stokes equations, the heat and the wave equation. Consequently, there are many applications of harmonic functions from industrial process optimisation to robotic path planning and the calculation of first exit times of random walks. Despite their ubiquity and relevance, there have been few attempts to incorporate inductive biases towards harmonic functions in machine learning contexts. In this work, we demonstrate effective means of representing harmonic functions in neural networks and extend such results also to quantum neural networks to demonstrate the generality of our approach. We benchmark our approaches against (quantum) physics-informed neural networks, where we show favourable performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "123402741",
                        "name": "Atiyo Ghosh"
                    },
                    {
                        "authorId": "35636440",
                        "name": "A. Gentile"
                    },
                    {
                        "authorId": "35056184",
                        "name": "M. Dagrada"
                    },
                    {
                        "authorId": "2195974609",
                        "name": "Chul Lee"
                    },
                    {
                        "authorId": "2195883178",
                        "name": "S. Kim"
                    },
                    {
                        "authorId": "2196074496",
                        "name": "Hyukgeun Cha"
                    },
                    {
                        "authorId": "2196249687",
                        "name": "Yunjun Choi"
                    },
                    {
                        "authorId": "2195805272",
                        "name": "Brad Kim"
                    },
                    {
                        "authorId": "144969941",
                        "name": "J. Kye"
                    },
                    {
                        "authorId": "13620411",
                        "name": "V. Elfving"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "67131cb9cba4a8c5e6db2014cbc32ebe6c4952d8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-06929",
                    "ArXiv": "2212.06929",
                    "DOI": "10.1103/PhysRevB.106.214307",
                    "CorpusId": 254636214
                },
                "corpusId": 254636214,
                "publicationVenue": {
                    "id": "52113867-f77b-4f26-a1cf-8e577dd325ea",
                    "name": "Physical review B",
                    "type": "journal",
                    "alternate_names": [
                        "Phys rev B",
                        "Phys Rev B",
                        "Physical Review B"
                    ],
                    "issn": "2469-9950",
                    "alternate_issns": [
                        "1098-0121",
                        "0556-2805"
                    ],
                    "url": "https://journals.aps.org/prb",
                    "alternate_urls": [
                        "https://journals.aps.org/prb/",
                        "http://journals.aps.org/prb/",
                        "http://prola.aps.org/",
                        "https://www.tib.eu/de/openurl/search?amp;DlicenseModel=nl&issn=1098-0121,0163-1829",
                        "http://prb.aps.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/67131cb9cba4a8c5e6db2014cbc32ebe6c4952d8",
                "title": "Generating extreme quantum scattering in graphene with machine learning",
                "abstract": "Graphene quantum dots provide a platform for manipulating electron behaviors in two-dimensional (2D) Dirac materials. Most previous works were of the\"forward\"type in that the objective was to solve various confinement, transport and scattering problems with given structures that can be generated by, e.g., applying an external electrical field. There are applications such as cloaking or superscattering where the challenging problem of inverse design needs to be solved: finding a quantum-dot structure according to certain desired functional characteristics. A brute-force search of the system configuration based directly on the solutions of the Dirac equation is computational infeasible. We articulate a machine-learning approach to addressing the inverse-design problem where artificial neural networks subject to physical constraints are exploited to replace the rigorous Dirac equation solver. In particular, we focus on the problem of designing a quantum dot structure to generate both cloaking and superscattering in terms of the scattering efficiency as a function of the energy. We construct a physical loss function that enables accurate prediction of the scattering characteristics. We demonstrate that, in the regime of Klein tunneling, the scattering efficiency can be designed to vary over two orders of magnitudes, allowing any scattering curve to be generated from a proper combination of the gate potentials. Our physics-based machine-learning approach can be a powerful design tool for 2D Dirac material-based electronics.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "93112518",
                        "name": "Chen-Di Han"
                    },
                    {
                        "authorId": "144769611",
                        "name": "Y. Lai"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "On the contrary, several existing works [1, 5, 6, 8, 24] embed specialized physical laws as hard constraints into neural networks, and enforce the model to must satisfy physical constraints."
            ],
            "citingPaper": {
                "paperId": "ba019f83ca27cd0fc0445092ada26a5a3d82d9c7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-04058",
                    "ArXiv": "2212.04058",
                    "DOI": "10.48550/arXiv.2212.04058",
                    "CorpusId": 254408488
                },
                "corpusId": 254408488,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ba019f83ca27cd0fc0445092ada26a5a3d82d9c7",
                "title": "AutoPINN: When AutoML Meets Physics-Informed Neural Networks",
                "abstract": "Physics-Informed Neural Networks (PINNs) have recently been proposed to solve scientific and engineering problems, where physical laws are introduced into neural networks as prior knowledge. With the embedded physical laws, PINNs enable the estimation of critical parameters, which are unobservable via physical tools, through observable variables. For example, Power Electronic Converters (PECs) are essential building blocks for the green energy transition. PINNs have been applied to estimate the capacitance, which is unobservable during PEC operations, using current and voltage, which can be observed easily during operations. The estimated capacitance facilitates self-diagnostics of PECs. Existing PINNs are often manually designed, which is time-consuming and may lead to suboptimal performance due to a large number of design choices for neural network architectures and hyperparameters. In addition, PINNs are often deployed on different physical devices, e.g., PECs, with limited and varying resources. Therefore, it requires designing different PINN models under different resource constraints, making it an even more challenging task for manual design. To contend with the challenges, we propose Automated Physics-Informed Neural Networks (AutoPINN), a framework that enables the automated design of PINNs by combining AutoML and PINNs. Specifically, we first tailor a search space that allows finding high-accuracy PINNs for PEC internal parameter estimation. We then propose a resource-aware search strategy to explore the search space to find the best PINN model under different resource constraints. We experimentally demonstrate that AutoPINN is able to find more accurate PINN models than human-designed, state-of-the-art PINN models using fewer resources.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1716206822",
                        "name": "Xinle Wu"
                    },
                    {
                        "authorId": "2679444",
                        "name": "Dalin Zhang"
                    },
                    {
                        "authorId": "2194662156",
                        "name": "Miao Zhang"
                    },
                    {
                        "authorId": "2637615",
                        "name": "Chenjuan Guo"
                    },
                    {
                        "authorId": "2145074538",
                        "name": "Shuai Zhao"
                    },
                    {
                        "authorId": "153533769",
                        "name": "Yi Zhang"
                    },
                    {
                        "authorId": "2182152839",
                        "name": "Huai Wang"
                    },
                    {
                        "authorId": "37606919",
                        "name": "B. Yang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Results for the data-driven learning of Hamiltonian systems with neural networks have been established in [9], [10], and extended to PHS, see [11], [12]."
            ],
            "citingPaper": {
                "paperId": "aaf636bc5b2153aaa732956382a7d5eaf858dd8e",
                "externalIds": {
                    "DBLP": "conf/cdc/0001SPP22",
                    "ArXiv": "2305.09017",
                    "DOI": "10.1109/CDC51059.2022.9992733",
                    "CorpusId": 255597248
                },
                "corpusId": 255597248,
                "publicationVenue": {
                    "id": "ab066af1-bfee-42da-84bb-42f7e199d0d0",
                    "name": "IEEE Conference on Decision and Control",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Decision and Control",
                        "IEEE Conf Decis Control",
                        "Conf Decis Control",
                        "CDC"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=403"
                },
                "url": "https://www.semanticscholar.org/paper/aaf636bc5b2153aaa732956382a7d5eaf858dd8e",
                "title": "Gaussian Process Port-Hamiltonian Systems: Bayesian Learning with Physics Prior",
                "abstract": "Data-driven approaches achieve remarkable results for the modeling of complex dynamics based on collected data. However, these models often neglect basic physical principles which determine the behavior of any real-world system. This omission is unfavorable in two ways: The models are not as data-efficient as they could be by incorporating physical prior knowledge, and the model itself might not be physically correct. We propose Gaussian Process Port-Hamiltonian systems (GPPHS) as a physics-informed Bayesian learning approach with uncertainty quantification. The Bayesian nature of GP-PHS uses collected data to form a distribution over all possible Hamiltonians instead of a single point estimate. Due to the underlying physics model, a GP-PHS generates passive systems with respect to designated inputs and outputs. Further, the proposed approach preserves the compositional nature of Port-Hamiltonian systems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "88691548",
                        "name": "Thomas Beckers"
                    },
                    {
                        "authorId": "66517049",
                        "name": "Jacob H. Seidman"
                    },
                    {
                        "authorId": "3410970",
                        "name": "P. Perdikaris"
                    },
                    {
                        "authorId": "143770945",
                        "name": "George J. Pappas"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "One typical example is Hamiltonian neural networks (HNNs) [2] which aim at training models that respect exact conservation laws.",
                "Authors in [2] proposed to learn Hamiltonians from data using artificial neural networks (ANNs)."
            ],
            "citingPaper": {
                "paperId": "4655b5c3d571f16e81154a885ab662d0e090b8f8",
                "externalIds": {
                    "DBLP": "conf/cdc/BaoTKV22",
                    "DOI": "10.1109/CDC51059.2022.9992803",
                    "CorpusId": 255598667
                },
                "corpusId": 255598667,
                "publicationVenue": {
                    "id": "ab066af1-bfee-42da-84bb-42f7e199d0d0",
                    "name": "IEEE Conference on Decision and Control",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Decision and Control",
                        "IEEE Conf Decis Control",
                        "Conf Decis Control",
                        "CDC"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=403"
                },
                "url": "https://www.semanticscholar.org/paper/4655b5c3d571f16e81154a885ab662d0e090b8f8",
                "title": "Physics-guided and Energy-based Learning of Interconnected Systems: from Lagrangian to Port-Hamiltonian Systems",
                "abstract": "This paper presents a framework for physics-informed energy-based neural network (NN) design to learn models of interconnected systems under the port-Hamiltonian (pH) formalism. In particular, this paper focuses on mechanical systems and incorporates the physical knowledge of Lagrangians into the neural networks to facilitate learning of equations of motion from the data. Moreover, the transformation from the Lagrangian mechanics to the Hamiltonian mechanics is incorporated into the NN architecture and learned from the data such that the learned model is compatible with the pH framework. Then, the structure of input-state-output pH models is imposed on the NN, which guarantees the dissipativity of the learned model. Furthermore, modeling interconnected systems is facilitated by the compositionality property of the pH systems. Additionally, the consistency between the Hamiltonian and Lagrangian is employed for the energy estimation to enable energy-based control. The proposed approach is shown to be computationally more efficient than the existing Lagrangian-based NN design approaches. Furthermore, the learned models with energy estimation are employed for energy-based model predictive control (MPC) design purpose. Experimental results using single (and double) inverted pendulum on carts show that the proposed learning-based approach can achieve an improved performance of model identification compared to the Lagrangian neural networks, accurate estimation of energies and strong control performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2075376698",
                        "name": "Yajie Bao"
                    },
                    {
                        "authorId": "2033829493",
                        "name": "Vaishnavi Thesma"
                    },
                    {
                        "authorId": "143676613",
                        "name": "A. Kelkar"
                    },
                    {
                        "authorId": "2230164",
                        "name": "J. Mohammadpour"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026been growing interest in developing better deep neural network based dynamics models for physical systems through better inductive biases (Lutter et al., 2019a; Greydanus et al., 2019; Lutter et al., 2019b; Zhong et al., 2019, 2020; Cranmer et al., 2020; Finzi et al., 2020; Zhong et al., 2021a).",
                "Recently there has been growing interest in developing better deep neural network based dynamics models for physical systems through better inductive biases (Lutter et al., 2019a; Greydanus et al., 2019; Lutter et al., 2019b; Zhong et al., 2019, 2020; Cranmer et al., 2020; Finzi et al., 2020; Zhong et al., 2021a)."
            ],
            "citingPaper": {
                "paperId": "4c62574c2900a8201b5201aab062fe5ee9625297",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-02179",
                    "ArXiv": "2212.02179",
                    "DOI": "10.48550/arXiv.2212.02179",
                    "CorpusId": 254246239
                },
                "corpusId": 254246239,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4c62574c2900a8201b5201aab062fe5ee9625297",
                "title": "Physics-Informed Model-Based Reinforcement Learning",
                "abstract": "We apply reinforcement learning (RL) to robotics tasks. One of the drawbacks of traditional RL algorithms has been their poor sample efficiency. One approach to improve the sample efficiency is model-based RL. In our model-based RL algorithm, we learn a model of the environment, essentially its transition dynamics and reward function, use it to generate imaginary trajectories and backpropagate through them to update the policy, exploiting the differentiability of the model. Intuitively, learning more accurate models should lead to better model-based RL performance. Recently, there has been growing interest in developing better deep neural network based dynamics models for physical systems, by utilizing the structure of the underlying physics. We focus on robotic systems undergoing rigid body motion without contacts. We compare two versions of our model-based RL algorithm, one which uses a standard deep neural network based dynamics model and the other which uses a much more accurate, physics-informed neural network based dynamics model. We show that, in model-based RL, model accuracy mainly matters in environments that are sensitive to initial conditions, where numerical errors accumulate fast. In these environments, the physics-informed version of our algorithm achieves significantly better average-return and sample efficiency. In environments that are not sensitive to initial conditions, both versions of our algorithm achieve similar average-return, while the physics-informed version achieves better sample efficiency. We also show that, in challenging environments, physics-informed model-based RL achieves better average-return than state-of-the-art model-free RL algorithms such as Soft Actor-Critic, as it computes the policy-gradient analytically, while the latter estimates it through sampling.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "153519852",
                        "name": "Adithya Ramesh"
                    },
                    {
                        "authorId": "1723632",
                        "name": "Balaraman Ravindran"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Physics informed neural networks for dynamics models: Although our focus is on enforcing constraints, we also briefly discuss related ideas in physics-informed neural networks (Raissi et al., 2019; M\u00e1rquez-Neila et al., 2017; Lu et al., 2021a; Lutter et al., 2019; Cranmer et al., 2020; Greydanus et al., 2019).",
                "\u2026neural networks for dynamics models: Although our focus is on enforcing constraints, we also briefly discuss related ideas in physics-informed neural networks (Raissi et al., 2019; Ma\u0301rquez-Neila et al., 2017; Lu et al., 2021a; Lutter et al., 2019; Cranmer et al., 2020; Greydanus et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "14e169b2a6efe24228ee261bd449e995f461a6dc",
                "externalIds": {
                    "DBLP": "conf/l4dc/SridharDW023",
                    "ArXiv": "2212.01346",
                    "DOI": "10.48550/arXiv.2212.01346",
                    "CorpusId": 254221150
                },
                "corpusId": 254221150,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/14e169b2a6efe24228ee261bd449e995f461a6dc",
                "title": "Guaranteed Conformance of Neurosymbolic Models to Natural Constraints",
                "abstract": "Deep neural networks have emerged as the workhorse for a large section of robotics and control applications, especially as models for dynamical systems. Such data-driven models are in turn used for designing and verifying autonomous systems. They are particularly useful in modeling medical systems where data can be leveraged to individualize treatment. In safety-critical applications, it is important that the data-driven model is conformant to established knowledge from the natural sciences. Such knowledge is often available or can often be distilled into a (possibly black-box) model. For instance, an F1 racing car should conform to Newton's laws (which are encoded within a unicycle model). In this light, we consider the following problem - given a model $M$ and a state transition dataset, we wish to best approximate the system model while being a bounded distance away from $M$. We propose a method to guarantee this conformance. Our first step is to distill the dataset into a few representative samples called memories, using the idea of a growing neural gas. Next, using these memories we partition the state space into disjoint subsets and compute bounds that should be respected by the neural network in each subset. This serves as a symbolic wrapper for guaranteed conformance. We argue theoretically that this only leads to a bounded increase in approximation error; which can be controlled by increasing the number of memories. We experimentally show that on three case studies (Car Model, Drones, and Artificial Pancreas), our constrained neurosymbolic models conform to specified models (each encoding various constraints) with order-of-magnitude improvements compared to the augmented Lagrangian and vanilla training methods. Our code can be found at: https://github.com/kaustubhsridhar/Constrained_Models",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2065757795",
                        "name": "Kaustubh Sridhar"
                    },
                    {
                        "authorId": "8235781",
                        "name": "Souradeep Dutta"
                    },
                    {
                        "authorId": "4726675",
                        "name": "James Weimer"
                    },
                    {
                        "authorId": "144637634",
                        "name": "Insup Lee"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "It is worth noting that the meta-trained model requires significantly fewer adaptation steps compared to the randomly initialized model, which corresponds to the vanilla HNN model [8].",
                "Prior works [8, 9, 23] have utilized tanh or softplus activations, hypothesizing that the relu activation may hinder parameter optimization due to its piecewise linear nature, as the HNN loss defined in Equation.",
                "1, HNN predicts the dynamics of the system incorporating the symplectic gradient inside the loss function as follows [8]."
            ],
            "citingPaper": {
                "paperId": "20336ad994a2a43c7d9d7a865661059840bbae72",
                "externalIds": {
                    "ArXiv": "2212.01168",
                    "CorpusId": 254220969
                },
                "corpusId": 254220969,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/20336ad994a2a43c7d9d7a865661059840bbae72",
                "title": "Identifying Generalized Neural Representation Across Hamiltonian Manifolds via Meta-learning",
                "abstract": "Recent advancements in deep learning for physics have focused on discovering shared representations of target systems by incorporating physics priors or inductive biases into neural networks. However, these approaches are system-specific and do not allow for easy adaptation to new physical systems governed by different laws. For example, a neural network trained on a mass-spring system cannot accurately predict the behavior of a two-body system or any other system with different governing physics. In this work, we model our system with a graph neural network and employ a meta-learning algorithm to enable the model to gain experience over a distribution of tasks and make it adapt to new physics. Our approach aims to learn a general representation across the various Hamiltonian manifolds, which is a common feature of the data distribution of Hamiltonian systems. We train our model using a dataset of different physical systems, each governed by its own inherent dynamics, and evaluate its performance on a new type of dynamical system with unknown physics. Our results show that the meta-trained model effectively adapts to the new system, which was unseen during the meta-training phase. Furthermore, we analyze the representation learned by the meta-trained neural network to identify a generalizable representation of Hamilton's equation that is shared across various physical systems. Our findings suggest that the meta-learned model can capture the generalizable representation across Hamiltonian manifolds inherent in dynamical systems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2218131153",
                        "name": "Y. Song"
                    },
                    {
                        "authorId": "2136396863",
                        "name": "Hawoong Jeong"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "contexts": [
                "\u2026bias have recently shown promise in learning dynamical system models that respect physical laws and that generalize beyond the training dataset (Djeumou et al., 2022a; Menda et al., 2019; Gupta et al., 2020; Cranmer et al., 2020; Greydanus et al., 2019; Finzi et al., 2020; Zhong et al., 2021a).",
                "Note that in this section, similar to all existing works involving port-Hamiltonian neural networks (Greydanus et al., 2019; Zhong et al., 2020; Desai et al., 2021; Eidnes et al., 2023), we assume that the interaction term J(x) is known a priori.",
                "Deep learning methods that use physics-based knowledge as inductive bias have recently shown promise in learning dynamical system models that respect physical laws and that generalize beyond the training dataset (Djeumou et al., 2022a; Menda et al., 2019; Gupta et al., 2020; Cranmer et al., 2020; Greydanus et al., 2019; Finzi et al., 2020; Zhong et al., 2021a).",
                "Of particular relevance to our work, Hamiltonian neural networks use the Hamiltonian formulation of dynamics to inform the structure of a neural ODE (Greydanus et al., 2019; Matsubara et al., 2020; Toth et al., 2020; Finzi et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "21bac88339555940ba094324dca70a9c6cceac77",
                "externalIds": {
                    "ArXiv": "2212.00893",
                    "DBLP": "conf/l4dc/NearyT23",
                    "DOI": "10.48550/arXiv.2212.00893",
                    "CorpusId": 254220970
                },
                "corpusId": 254220970,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/21bac88339555940ba094324dca70a9c6cceac77",
                "title": "Compositional Learning of Dynamical System Models Using Port-Hamiltonian Neural Networks",
                "abstract": "Many dynamical systems -- from robots interacting with their surroundings to large-scale multiphysics systems -- involve a number of interacting subsystems. Toward the objective of learning composite models of such systems from data, we present i) a framework for compositional neural networks, ii) algorithms to train these models, iii) a method to compose the learned models, iv) theoretical results that bound the error of the resulting composite models, and v) a method to learn the composition itself, when it is not known a priori. The end result is a modular approach to learning: neural network submodels are trained on trajectory data generated by relatively simple subsystems, and the dynamics of more complex composite systems are then predicted without requiring additional data generated by the composite systems themselves. We achieve this compositionality by representing the system of interest, as well as each of its subsystems, as a port-Hamiltonian neural network (PHNN) -- a class of neural ordinary differential equations that uses the port-Hamiltonian systems formulation as inductive bias. We compose collections of PHNNs by using the system's physics-informed interconnection structure, which may be known a priori, or may itself be learned from data. We demonstrate the novel capabilities of the proposed framework through numerical examples involving interacting spring-mass-damper systems. Models of these systems, which include nonlinear energy dissipation and control inputs, are learned independently. Accurate compositions are learned using an amount of training data that is negligible in comparison with that required to train a new model from scratch. Finally, we observe that the composite PHNNs enjoy properties of port-Hamiltonian systems, such as cyclo-passivity -- a property that is useful for control purposes.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1796254983",
                        "name": "Cyrus Neary"
                    },
                    {
                        "authorId": "3199888",
                        "name": "U. Topcu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Scientific Knowledge Mathematical Equations [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] [50]\u2013[52] [53] [54] [55]",
                "Hamiltonian functionality that enforces energy conservation has attracted much attention [24], [25], [30], [31]."
            ],
            "citingPaper": {
                "paperId": "e6efc429bef27fb22c62909e55091e907d7b42ae",
                "externalIds": {
                    "ArXiv": "2212.00017",
                    "DBLP": "journals/corr/abs-2212-00017",
                    "DOI": "10.48550/arXiv.2212.00017",
                    "CorpusId": 254125341
                },
                "corpusId": 254125341,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e6efc429bef27fb22c62909e55091e907d7b42ae",
                "title": "Knowledge-augmented Deep Learning and Its Applications: A Survey",
                "abstract": ",",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "90146345",
                        "name": "Zijun Cui"
                    },
                    {
                        "authorId": "2007655125",
                        "name": "Tian Gao"
                    },
                    {
                        "authorId": "2940762",
                        "name": "Kartik Talamadupula"
                    },
                    {
                        "authorId": "2193256378",
                        "name": "Qiang Ji"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Many physics-informed approaches have recently been proposed to learn Hamiltonian dynamics and symplectic maps (Lutter et al., 2019b; Greydanus et al., 2019; Bertalan et al., 2019; Jin et al., 2020; Burby et al., 2020; Chen et al., 2020; Cranmer et al., 2020; Zhong et al., 2020a,b, 2021; Marco and\u2026",
                "Many physics-informed approaches have recently been proposed to learn Hamiltonian dynamics and symplectic maps (Lutter et al., 2019b; Greydanus et al., 2019; Bertalan et al., 2019; Jin et al., 2020; Burby et al., 2020; Chen et al., 2020; Cranmer et al., 2020; Zhong et al., 2020a,b, 2021; Marco and M\u00e9hats, 2021; Rath et al., 2021; Chen et al., 2021; Offen and Ober-Bl\u00f6baum, 2022; Santos et al., 2022; Valperga et al., 2022; Mathiesen et al., 2022; Duruisseaux et al., 2023a)."
            ],
            "citingPaper": {
                "paperId": "306269bb2495a0664c38f216936d1b1e9f3732d1",
                "externalIds": {
                    "ArXiv": "2211.16006",
                    "DBLP": "journals/corr/abs-2211-16006",
                    "DOI": "10.48550/arXiv.2211.16006",
                    "CorpusId": 254069658
                },
                "corpusId": 254069658,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/306269bb2495a0664c38f216936d1b1e9f3732d1",
                "title": "Lie Group Forced Variational Integrator Networks for Learning and Control of Robot Systems",
                "abstract": "Incorporating prior knowledge of physics laws and structural properties of dynamical systems into the design of deep learning architectures has proven to be a powerful technique for improving their computational efficiency and generalization capacity. Learning accurate models of robot dynamics is critical for safe and stable control. Autonomous mobile robots, including wheeled, aerial, and underwater vehicles, can be modeled as controlled Lagrangian or Hamiltonian rigid-body systems evolving on matrix Lie groups. In this paper, we introduce a new structure-preserving deep learning architecture, the Lie group Forced Variational Integrator Network (LieFVIN), capable of learning controlled Lagrangian or Hamiltonian dynamics on Lie groups, either from position-velocity or position-only data. By design, LieFVINs preserve both the Lie group structure on which the dynamics evolve and the symplectic structure underlying the Hamiltonian or Lagrangian systems of interest. The proposed architecture learns surrogate discrete-time flow maps allowing accurate and fast prediction without numerical-integrator, neural-ODE, or adjoint techniques, which are needed for vector fields. Furthermore, the learnt discrete-time dynamics can be utilized with computationally scalable discrete-time (optimal) control strategies.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2032462536",
                        "name": "Valentin Duruisseaux"
                    },
                    {
                        "authorId": "2344318",
                        "name": "T. Duong"
                    },
                    {
                        "authorId": "2087376",
                        "name": "M. Leok"
                    },
                    {
                        "authorId": "50365495",
                        "name": "Nikolay A. Atanasov"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "For instance regularization terms can be introduced into the loss function to penalize the NN that would otherwise not satisfy physical constraints [6, 10].",
                "A typical approach to embed the conservation property into the network model is to add a regularization term to the loss function [6, 10, 22]."
            ],
            "citingPaper": {
                "paperId": "047ff1a87d02373c48643b30354d6b36201bc9e0",
                "externalIds": {
                    "ArXiv": "2211.14375",
                    "DBLP": "journals/corr/abs-2211-14375",
                    "DOI": "10.48550/arXiv.2211.14375",
                    "CorpusId": 254043960
                },
                "corpusId": 254043960,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/047ff1a87d02373c48643b30354d6b36201bc9e0",
                "title": "Designing Neural Networks for Hyperbolic Conservation Laws",
                "abstract": "We propose a new data-driven method to learn the dynamics of an unknown hyperbolic system of conservation laws using deep neural networks. Inspired by classical methods in numerical conservation laws, we develop a new conservative form network (CFN) in which the network learns the \ufb02ux function of the unknown system. Our numerical examples demonstrate that the CFN yields signi\ufb01cantly better prediction accuracy than what is obtained using a standard non-conservative form network, even when it is enhanced with constraints to promote conservation. In particular, solutions obtained using the CFN consistently capture the correct shock propagation speed without introducing non-physical oscillations into the solution. They are furthermore robust to noisy and sparse observation environments",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2117102278",
                        "name": "Zhen Chen"
                    },
                    {
                        "authorId": "31015751",
                        "name": "A. Gelb"
                    },
                    {
                        "authorId": "3128294",
                        "name": "Yoonsang Lee"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "They usually use a higher-order numerical integration scheme to update the latent state:\nu\u0302(t)\u2190 Encode(u(t)) u\u0302(t+ \u2206t)\u2190 u\u0302(t) + \u222b t+\u2206t t F(u\u0302(s))ds u(t+ \u2206t)\u2190 Decode(u\u0302(t+ \u2206t))\nHamiltonian Neural Network: Hamiltonian Neural Networks (HNNs) [Greydanus et al., 2019] models the Hamiltonian in a latent space explicitly and updates the latent coordinates via Hamilton\u2019s equations.\nq\u0302(t), p\u0302(t)\u2190 Encode(u(t)) q\u0302(t+ \u2206t)\u2190 q\u0302(t) + \u222b t+\u2206t t \u2202 \u2202p\u0302 H(q\u0302(s), p\u0302(s))ds\np\u0302(t+ \u2206t)\u2190 p\u0302(t)\u2212 \u222b t+\u2206t t \u2202 \u2202q\u0302 H(q\u0302(s), p\u0302(s))ds u(t+ \u2206t)\u2190 Decode(q\u0302(t+ \u2206t), p\u0302(t+ \u2206t))\nWherever applicable, we used the Dormand-Prince 5(4) solver, a 5th order Runge-Kutta method for numerical integration.",
                "(1) Previous efforts towards modelling Hamiltonian systems [Greydanus et al., 2019, Cranmer et al., 2020, Jin et al., 2020, Chen et al., 2018, Kidger, 2022] have seen success with neural networks trained via backpropagation on the trajectory prediction objective:\n\u03c6\u2217 = arg min \u03c6 \u2211 t \u2016M\u03c6(u(t),\u2206t)\u2212\u2026",
                ", 2018], and the physicsinspired Hamiltonian Neural Networks (HNN) [Greydanus et al., 2019].",
                "We compare the Action-Angle Network to three strong baseline models: the Euler Update Network (EUN), the Neural Ordinary Differential Equations (Neural ODE) [Chen et al., 2018], and the physicsinspired Hamiltonian Neural Networks (HNN) [Greydanus et al., 2019].",
                "\u2026state:\nu\u0302(t)\u2190 Encode(u(t)) u\u0302(t+ \u2206t)\u2190 u\u0302(t) + \u222b t+\u2206t t F(u\u0302(s))ds u(t+ \u2206t)\u2190 Decode(u\u0302(t+ \u2206t))\nHamiltonian Neural Network: Hamiltonian Neural Networks (HNNs) [Greydanus et al., 2019] models the Hamiltonian in a latent space explicitly and updates the latent coordinates via Hamilton\u2019s\u2026",
                "Figure 3a shows that the Action-Angle Network can be queried much faster than the Neural ODE and the HNN, with an inference time that is independent of \u2206t. Figure 3b depicts the prediction error as a function of \u2206t, showing that the Action-Angle Network also scales much better with the jump size \u2206t, even for jump sizes larger than those seen during training \u2206tmax = 10."
            ],
            "citingPaper": {
                "paperId": "f77128747418c02074f05cc87e230f8f1228035c",
                "externalIds": {
                    "ArXiv": "2211.15338",
                    "DBLP": "journals/corr/abs-2211-15338",
                    "DOI": "10.48550/arXiv.2211.15338",
                    "CorpusId": 254043980
                },
                "corpusId": 254043980,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f77128747418c02074f05cc87e230f8f1228035c",
                "title": "Learning Integrable Dynamics with Action-Angle Networks",
                "abstract": "Machine learning has become increasingly popular for efficiently modelling the dynamics of complex physical systems, demonstrating a capability to learn effective models for dynamics which ignore redundant degrees of freedom. Learned simulators typically predict the evolution of the system in a step-by-step manner with numerical integration techniques. However, such models often suffer from instability over long roll-outs due to the accumulation of both estimation and integration error at each prediction step. Here, we propose an alternative construction for learned physical simulators that are inspired by the concept of action-angle coordinates from classical mechanics for describing integrable systems. We propose Action-Angle Networks, which learn a nonlinear transformation from input coordinates to the action-angle space, where evolution of the system is linear. Unlike traditional learned simulators, Action-Angle Networks do not employ any higher-order numerical integration methods, making them extremely efficient at modelling the dynamics of integrable physical systems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "102488653",
                        "name": "Ameya Daigavane"
                    },
                    {
                        "authorId": "2192604719",
                        "name": "Arthur Kosmala"
                    },
                    {
                        "authorId": "32122523",
                        "name": "M. Cranmer"
                    },
                    {
                        "authorId": "5485763",
                        "name": "T. Smidt"
                    },
                    {
                        "authorId": "2149614112",
                        "name": "S. Ho"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "as a differentiable computational graph [11], [15]\u2013[17] or used separately in an error-learning scheme [3], [11], [18]."
            ],
            "citingPaper": {
                "paperId": "bf1bc61f468c7181deeab6715797fdbccb821cd5",
                "externalIds": {
                    "ArXiv": "2211.12921",
                    "DOI": "10.1109/LRA.2022.3222951",
                    "CorpusId": 254274820
                },
                "corpusId": 254274820,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bf1bc61f468c7181deeab6715797fdbccb821cd5",
                "title": "Hybrid Learning of Time-Series Inverse Dynamics Models for Locally Isotropic Robot Motion",
                "abstract": "Applications of force control and motion planning often rely on an inverse dynamics model to represent the high-dimensional dynamic behavior of robots during motion. The widespread occurrence of low-velocity, small-scale, locally isotropic motion (LIMO) typically complicates the identification of appropriate models due to the exaggeration of dynamic effects and sensory perturbation caused by complex friction and phenomena of hysteresis, e.g., pertaining to joint elasticity. We propose a hybrid model learning base architecture combining a rigid body dynamics model identified by parametric regression and time-series neural network architectures based on multilayer-perceptron, LSTM, and Transformer topologies. Further, we introduce novel joint-wise rotational history encoding, reinforcing temporal information to effectively model dynamic hysteresis. The models are evaluated on a KUKA iiwa 14 during algorithmically generated locally isotropic movements. Together with the rotational encoding, the proposed architectures outperform state-of-the-art baselines by a magnitude of 10$^3$ yielding an RMSE of 0.14 Nm. Leveraging the hybrid structure and time-series encoding capabilities, our approach allows for accurate torque estimation, indicating its applicability in critically force-sensitive applications during motion sequences exceeding the capacity of conventional inverse dynamics models while retaining trainability in face of scarce data and explainability due to the employed physics model prior.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2193620127",
                        "name": "Tolga-Can cCallar"
                    },
                    {
                        "authorId": "2193620125",
                        "name": "Sven Bottger"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Using the latter approach Greydanus et al. (2019) proposed the Hamiltonian Neural Network (HNN), closely followed by the Lagrangian Neural Networks (LNN) by Cranmer et al. (2020) leading to a rapid growth of research interest in this topic."
            ],
            "citingPaper": {
                "paperId": "495b334362b47d2ead7a73a56bc66a8b7fc801a7",
                "externalIds": {
                    "ArXiv": "2211.10830",
                    "DBLP": "journals/corr/abs-2211-10830",
                    "DOI": "10.48550/arXiv.2211.10830",
                    "CorpusId": 253735442
                },
                "corpusId": 253735442,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/495b334362b47d2ead7a73a56bc66a8b7fc801a7",
                "title": "Discrete Lagrangian Neural Networks with Automatic Symmetry Discovery",
                "abstract": ": By one of the most fundamental principles in physics, a dynamical system will exhibit those motions which extremise an action functional. This leads to the formation of the Euler-Lagrange equations, which serve as a model of how the system will behave in time. If the dynamics exhibit additional symmetries, then the motion ful\ufb01ls additional conservation laws, such as conservation of energy (time invariance), momentum (translation invariance), or angular momentum (rotational invariance). To learn a system representation, one could learn the discrete Euler-Lagrange equations, or alternatively, learn the discrete Lagrangian function L d which de\ufb01nes them. Based on ideas from Lie group theory, in this work we introduce a framework to learn a discrete Lagrangian along with its symmetry group from discrete observations of motions and, therefore, identify conserved quantities. The learning process does not restrict the form of the Lagrangian, does not require velocity or momentum observations or predictions and incorporates a cost term which safeguards against unwanted solutions and against potential numerical issues in forward simulations. The learnt discrete quantities are related to their continuous analogues using variational backward error analysis and numerical results demonstrate the improvement such models can have both qualitatively and quantitatively even in the presence of noise.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1942896694",
                        "name": "Yana Lishkova"
                    },
                    {
                        "authorId": "1397984251",
                        "name": "P. Scherer"
                    },
                    {
                        "authorId": "2008686725",
                        "name": "Steffen Ridderbusch"
                    },
                    {
                        "authorId": "1708741",
                        "name": "M. Jamnik"
                    },
                    {
                        "authorId": "2075355155",
                        "name": "Pietro Lio'"
                    },
                    {
                        "authorId": "1419469651",
                        "name": "S. Ober-Blobaum"
                    },
                    {
                        "authorId": "51299251",
                        "name": "Christian Offen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Moreover, NLD can be viewed in a line of physics inspired neural network models such as (Cranmer et al., 2020; Greydanus et al., 2019; Toth et al., 2020; Botev et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "8ed31496eea24fb64e465cea58f1a0a4a92b4470",
                "externalIds": {
                    "ArXiv": "2211.09537",
                    "DBLP": "journals/corr/abs-2211-09537",
                    "DOI": "10.48550/arXiv.2211.09537",
                    "CorpusId": 253581210
                },
                "corpusId": 253581210,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8ed31496eea24fb64e465cea58f1a0a4a92b4470",
                "title": "Neural Langevin Dynamics: towards interpretable Neural Stochastic Differential Equations",
                "abstract": "Neural Stochastic Di\ufb00erential Equations (NSDE) have been trained as both Variational Autoencoders, and as GANs. However, the resulting Stochastic Di\ufb00erential Equations can be hard to interpret or analyse due to the generic nature of the drift and di\ufb00usion \ufb01elds. By restricting our NSDE to be of the form of Langevin dynamics, and training it as a VAE, we obtain NSDEs that lend themselves to more elaborate analysis and to a wider range of visualisation techniques than a generic NSDE. More speci\ufb01cally, we obtain an energy landscape, the minima of which are in one-to-one correspondence with latent states underlying the used data. This not only allows us to detect states underlying the data dynamics in an unsupervised manner, but also to infer the distribution of time spent in each state according to the learned SDE. More in general, restricting an NSDE to Langevin dynamics enables the use of a large set of tools from computational molecular dynamics for the analysis of the obtained results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2191075252",
                        "name": "Simon M. Koop"
                    },
                    {
                        "authorId": "3266992",
                        "name": "M. Peletier"
                    },
                    {
                        "authorId": "9032664",
                        "name": "J. Portegies"
                    },
                    {
                        "authorId": "49917515",
                        "name": "V. Menkovski"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "fe9cb4ed5c0252e3bff836e3243771e057661b25",
                "externalIds": {
                    "DBLP": "journals/jcphy/XiaoF23",
                    "ArXiv": "2211.08149",
                    "DOI": "10.1016/j.jcp.2023.112317",
                    "CorpusId": 253523453
                },
                "corpusId": 253523453,
                "publicationVenue": {
                    "id": "58606051-2e63-4034-8496-9d4ed773bb49",
                    "name": "Journal of Computational Physics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Phys"
                    ],
                    "issn": "0021-9991",
                    "url": "http://www.elsevier.com/locate/jcp",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00219991",
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622866/description#description",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fe9cb4ed5c0252e3bff836e3243771e057661b25",
                "title": "RelaxNet: A structure-preserving neural network to approximate the Boltzmann collision operator",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "30163976",
                        "name": "Tianbai Xiao"
                    },
                    {
                        "authorId": "152432498",
                        "name": "M. Frank"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "There is a great deal of work designing specific neural architectures that naturally obey Hamiltonian equations and Lagrangian equations [67], [68], [134], [135].",
                "For PDEs with Dirichlet conditions, they sample a dataset of collocation points from \u2126 and \u2202\u2126, i.e.{xi} \u2282 \u2126\n7 Neural Solver Method Description Representatives Loss Reweighting Grad Norm GradientPathologiesPINNs [43] NTK Reweighting PINNsNTK [44] Variance Reweighting Inverse-Dirichlet PINNs [45] Novel Optimization Targets Numerical Differentiation DGM [46], CAN-PINN [47], cvPINNs [48] Variantional Formulation vPINN [49], hp-PINN [50], VarNet [51], WAN [52] Regularization gPINNs [53], Sobolev Training [54]\nNovel Architectures\nAdaptive Activation LAAF-PINNs [55], [56], SReLU [57] Feature Preprocessing Fourier Embedding [58], Prior Dictionary Embedding [59] Boundary Encoding TFC-based [60], CENN [61], PFNN [62], HCNet [63]\nSequential Architecture PhyCRNet [64], PhyLSTM [65] AR-DenseED [66], HNN [67], HGN [68] Convolutional Architecture PhyGeoNet [69], PhyCRNet [64], PPNN [70]\nDomain Decomposition XPINNs [71], cPINNs [72], FBPINNs [73], Shukla et al. [74]\nOther Learning Paradigms Transfer Learning Desai et al. [75], MF-PIDNN [76]Meta-Learning Psaros et al. [77], NRPINNs [78]\nTABLE 2: An overview of variants of PINNs.",
                "Hamiltonian neural networks (HNN) [67], [136] represent the Hamiltonian with a neural network Hw(q,p).",
                "Novel Architectures Adaptive Activation LAAF-PINNs [55], [56], SReLU [57] Feature Preprocessing Fourier Embedding [58], Prior Dictionary Embedding [59] Boundary Encoding TFC-based [60], CENN [61], PFNN [62], HCNet [63] Sequential Architecture PhyCRNet [64], PhyLSTM [65] AR-DenseED [66], HNN [67], HGN [68] Convolutional Architecture PhyGeoNet [69], PhyCRNet [64], PPNN [70] Domain Decomposition XPINNs [71], cPINNs [72], FBPINNs [73], Shukla et al.",
                "HNN\n(Greydanus et. al.)"
            ],
            "citingPaper": {
                "paperId": "0c5c5f100dec9c758abf4dcc526a6883671fd3bd",
                "externalIds": {
                    "ArXiv": "2211.08064",
                    "DBLP": "journals/corr/abs-2211-08064",
                    "DOI": "10.48550/arXiv.2211.08064",
                    "CorpusId": 253522948
                },
                "corpusId": 253522948,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0c5c5f100dec9c758abf4dcc526a6883671fd3bd",
                "title": "Physics-Informed Machine Learning: A Survey on Problems, Methods and Applications",
                "abstract": "Recent advances of data-driven machine learning have revolutionized fields like computer vision, reinforcement learning, and many scientific and engineering domains. In many real-world and scientific problems, systems that generate data are governed by physical laws. Recent work shows that it provides potential benefits for machine learning models by incorporating the physical prior and collected data, which makes the intersection of machine learning and physics become a prevailing paradigm. By integrating the data and mathematical physics models seamlessly, it can guide the machine learning model towards solutions that are physically plausible, improving accuracy and efficiency even in uncertain and high-dimensional contexts. In this survey, we present this learning paradigm called Physics-Informed Machine Learning (PIML) which is to build a model that leverages empirical data and available physical prior knowledge to improve performance on a set of tasks that involve a physical mechanism. We systematically review the recent development of physics-informed machine learning from three perspectives of machine learning tasks, representation of physical prior, and methods for incorporating physical prior. We also propose several important open research problems based on the current trends in the field. We argue that encoding different forms of physical prior into model architectures, optimizers, inference algorithms, and significant domain-specific applications like inverse engineering design and robotic control is far from being fully explored in the field of physics-informed machine learning. We believe that the interdisciplinary research of physics-informed machine learning will significantly propel research progress, foster the creation of more effective machine learning models, and also offer invaluable assistance in addressing long-standing problems in related disciplines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "95460807",
                        "name": "Zhongkai Hao"
                    },
                    {
                        "authorId": "104037450",
                        "name": "Songming Liu"
                    },
                    {
                        "authorId": null,
                        "name": "Yichi Zhang"
                    },
                    {
                        "authorId": "2072399434",
                        "name": "Chengyang Ying"
                    },
                    {
                        "authorId": "2190929908",
                        "name": "Yao Feng"
                    },
                    {
                        "authorId": "2093561216",
                        "name": "Hang Su"
                    },
                    {
                        "authorId": "2146280496",
                        "name": "Jun Zhu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "For time series modelling, recurrent architectures [13, 3] or physically inspired models [7] are often considered with success.",
                "See for instance [13, 1, 7, 3, 6] just to name a few recent work."
            ],
            "citingPaper": {
                "paperId": "c345e96c0c949ee13c52afadb2846d2f70805da3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-06972",
                    "ArXiv": "2211.06972",
                    "DOI": "10.48550/arXiv.2211.06972",
                    "CorpusId": 253510250
                },
                "corpusId": 253510250,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c345e96c0c949ee13c52afadb2846d2f70805da3",
                "title": "Experimental study of Neural ODE training with adaptive solver for dynamical systems modeling",
                "abstract": "Neural Ordinary Differential Equations (ODEs) was recently introduced as a new family of neural network models, which relies on black-box ODE solvers for inference and training. Some ODE solvers called adaptive can adapt their evaluation strategy depending on the complexity of the problem at hand, opening great perspectives in machine learning. However, this paper describes a simple set of experiments to show why adaptive solvers cannot be seamlessly leveraged as a black-box for dynamical systems modelling. By taking the Lorenz'63 system as a showcase, we show that a naive application of the Fehlberg's method does not yield the expected results. Moreover, a simple workaround is proposed that assumes a tighter interaction between the solver and the training strategy. The code is available on github: https://github.com/Allauzen/adaptive-step-size-neural-ode",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2311059",
                        "name": "A. Allauzen"
                    },
                    {
                        "authorId": "2190750352",
                        "name": "Thiago Petrilli Maffei Dardis"
                    },
                    {
                        "authorId": "2190750217",
                        "name": "Hannah Plath"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "As a countermeasure to the brittleness of NN-based models, there has been increasing interest in incorporating prior knowledge \u2013 also known as inductive bias \u2013 into NNs to ensure physical consistency, leading to Hamiltonian NNs (Chen et al., 2019; Greydanus et al., 2019; Finzi et al., 2020), Lagrangian NNs (Cranmer et al., 2020), or Poisson NNs (Jin et al., 2022), amongst others.",
                "As a countermeasure to the brittleness of NN-based models, there has been increasing interest in incorporating prior knowledge \u2013 also known as inductive bias \u2013 into NNs to ensure physical consistency, leading to Hamiltonian NNs (Chen et al., 2019; Greydanus et al., 2019; Finzi et al., 2020), Lagrangian NNs (Cranmer et al.",
                "In other words, NODEs learn the parameters of an ODE to fit data, making them particularly suitable to model complex dynamical systems (Greydanus et al., 2019; Rubanova et al., 2019).",
                "\u2026interest in incorporating prior knowledge \u2013 also known as inductive bias \u2013 into NNs to ensure physical consistency, leading to Hamiltonian NNs (Chen et al., 2019; Greydanus et al., 2019; Finzi et al., 2020), Lagrangian NNs (Cranmer et al., 2020), or Poisson NNs (Jin et al., 2022), amongst others."
            ],
            "citingPaper": {
                "paperId": "3288f73d10949c26ddfce6a559c79f88dd2a68a4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-06130",
                    "ArXiv": "2211.06130",
                    "DOI": "10.48550/arXiv.2211.06130",
                    "CorpusId": 253499138
                },
                "corpusId": 253499138,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3288f73d10949c26ddfce6a559c79f88dd2a68a4",
                "title": "Physically Consistent Neural ODEs for Learning Multi-Physics Systems",
                "abstract": ": Despite the immense success of neural networks in modeling system dynamics from data, they often remain physics-agnostic black boxes. In the particular case of physical systems, they might consequently make physically inconsistent predictions, which makes them unreliable in practice. In this paper, we leverage the framework of Irreversible port-Hamiltonian Systems (IPHS), which can describe most multi-physics systems, and rely on Neural Ordinary Di\ufb00erential Equations (NODEs) to learn their parameters from data. Since IPHS models are consistent with the \ufb01rst and second principles of thermodynamics by design, so are the proposed Physically Consistent NODEs (PC-NODEs). Furthermore, the NODE training procedure allows us to seamlessly incorporate prior knowledge of the system properties in the learned dynamics. We demonstrate the e\ufb00ectiveness of the proposed method by learning the thermodynamics of a building from the real-world measurements and the dynamics of a simulated gas-piston system. Thanks to the modularity and \ufb02exibility of the IPHS framework, PC-NODEs can be extended to learn physically consistent models of multi-physics distributed systems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "30426382",
                        "name": "M. Zakwan"
                    },
                    {
                        "authorId": "2143195427",
                        "name": "Loris Di Natale"
                    },
                    {
                        "authorId": "2378010",
                        "name": "B. Svetozarevic"
                    },
                    {
                        "authorId": "134579497",
                        "name": "Philipp Heer"
                    },
                    {
                        "authorId": "2148365672",
                        "name": "Colin N. Jones"
                    },
                    {
                        "authorId": "1404093319",
                        "name": "G. Ferrari-Trecate"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "(2019); Greydanus et al. (2019). Further, the potential energy is predicted using the GNN and the diagonal mass matrix is trained as a learnable parameter.",
                "\u2026Chen et al. (2018); Gruver et al. (2021); Bishnoi et al. (2022),Lagrangian (LNNs) Cranmer et al. (2020a); Finzi et al. (2020); Lutter et al. (2019); Bhattoo et al. (2022), and Hamiltonian neural networks (HNNs) Sanchez-Gonzalez et al. (2019); Greydanus et al. (2019); Zhong et al. (2020, 2019).",
                "Specifically, these biases allow the MLP to preserve the characteristics of physical systems, such as energy and momentum conservation, and thus lead to a realistic realization of a trajectory of the system Greydanus et al. (2019); Cranmer et al. (2020a); Zhong and Leonard (2020).",
                "\u2022HGNN: HGNN refers to Hamiltonian graph neural network, where the structure of the Hamiltonian of a system is exploited to decouple it into potential and kinetic energies Bhattoo et al. (2022); Sanchez-Gonzalez et al. (2019); Greydanus et al. (2019)."
            ],
            "citingPaper": {
                "paperId": "d7952de05e25e90a08b79431116ca6d2ec85e710",
                "externalIds": {
                    "ArXiv": "2211.05520",
                    "DBLP": "journals/corr/abs-2211-05520",
                    "DOI": "10.48550/arXiv.2211.05520",
                    "CorpusId": 253447034
                },
                "corpusId": 253447034,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d7952de05e25e90a08b79431116ca6d2ec85e710",
                "title": "Unravelling the Performance of Physics-informed Graph Neural Networks for Dynamical Systems",
                "abstract": "Recently, graph neural networks have been gaining a lot of attention to simulate dynamical systems due to their inductive nature leading to zero-shot generalizability. Similarly, physics-informed inductive biases in deep-learning frameworks have been shown to give superior performance in learning the dynamics of physical systems. There is a growing volume of literature that attempts to combine these two approaches. Here, we evaluate the performance of thirteen different graph neural networks, namely, Hamiltonian and Lagrangian graph neural networks, graph neural ODE, and their variants with explicit constraints and different architectures. We briefly explain the theoretical formulation highlighting the similarities and differences in the inductive biases and graph architecture of these systems. We evaluate these models on spring, pendulum, gravitational, and 3D deformable solid systems to compare the performance in terms of rollout error, conserved quantities such as energy and momentum, and generalizability to unseen system sizes. Our study demonstrates that GNNs with additional inductive biases, such as explicit constraints and decoupling of kinetic and potential energies, exhibit significantly enhanced performance. Further, all the physics-informed GNNs exhibit zero-shot generalizability to system sizes an order of magnitude larger than the training system, thus providing a promising route to simulate large-scale realistic systems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "11409168",
                        "name": "A. Thangamuthu"
                    },
                    {
                        "authorId": "2113299739",
                        "name": "Gunjan Kumar"
                    },
                    {
                        "authorId": "15675757",
                        "name": "S. Bishnoi"
                    },
                    {
                        "authorId": "2131107456",
                        "name": "Ravinder Bhattoo"
                    },
                    {
                        "authorId": "2144896197",
                        "name": "N. Krishnan"
                    },
                    {
                        "authorId": "1699732",
                        "name": "Sayan Ranu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "7516b1c8c11e6e713e9a2f544d5b4c7469231066",
                "externalIds": {
                    "DBLP": "journals/fi/ZhuJLF22",
                    "PubMedCentral": "9680843",
                    "DOI": "10.3389/frobt.2022.968305",
                    "CorpusId": 253399649,
                    "PubMed": "36425848"
                },
                "corpusId": 253399649,
                "publicationVenue": {
                    "id": "2ee61499-676f-46c2-afde-d4c0cb4393e6",
                    "name": "Frontiers in Robotics and AI",
                    "type": "journal",
                    "alternate_names": [
                        "Front Robot AI"
                    ],
                    "issn": "2296-9144",
                    "url": "https://www.frontiersin.org/journals/robotics-and-ai",
                    "alternate_urls": [
                        "http://www.frontiersin.org/Robotics_and_AI/archive",
                        "http://www.frontiersin.org/Robotics_and_AI/about",
                        "http://www.frontiersin.org/Robotics_and_AI"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7516b1c8c11e6e713e9a2f544d5b4c7469231066",
                "title": "NN-Poly: Approximating common neural networks with Taylor polynomials to imbue dynamical system constraints",
                "abstract": "Recent advances in deep learning have bolstered our ability to forecast the evolution of dynamical systems, but common neural networks do not adhere to physical laws, critical information that could lead to sounder state predictions. This contribution addresses this concern by proposing a neural network to polynomial (NN-Poly) approximation, a method that furnishes algorithmic guarantees of adhering to physics while retaining state prediction accuracy. To achieve these goals, this article shows how to represent a trained fully connected perceptron, convolution, and recurrent neural networks of various activation functions as Taylor polynomials of arbitrary order. This solution is not only analytic in nature but also least squares optimal. The NN-Poly system identification or state prediction method is evaluated against a single-layer neural network and a polynomial trained on data generated by dynamic systems. Across our test cases, the proposed method maintains minimal root mean-squared state error, requires few parameters to form, and enables model structure for verification and safety. Future work will incorporate safety constraints into state predictions, with this new model structure and test high-dimensional dynamical system data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51027983",
                        "name": "Frances Zhu"
                    },
                    {
                        "authorId": "2190184096",
                        "name": "Dongheng Jing"
                    },
                    {
                        "authorId": "39593375",
                        "name": "Frederick A. Leve"
                    },
                    {
                        "authorId": "145927985",
                        "name": "S. Ferrari"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "09118140c70e91112039b6c6b6896cf1bc3fe3a1",
                "externalIds": {
                    "PubMedCentral": "9640874",
                    "DBLP": "journals/evi/Yao23",
                    "DOI": "10.1007/s12065-022-00789-w",
                    "CorpusId": 253418136,
                    "PubMed": "36406009"
                },
                "corpusId": 253418136,
                "publicationVenue": {
                    "id": "283554cb-7ba2-4666-bb9b-ce759f2ee4fd",
                    "name": "Evolutionary Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "Evol Intell"
                    ],
                    "issn": "1864-5909",
                    "url": "https://www.springer.com/engineering/computational+intelligence+and+complexity/journal/12065",
                    "alternate_urls": [
                        "https://link.springer.com/journal/12065"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/09118140c70e91112039b6c6b6896cf1bc3fe3a1",
                "title": "Epistemic neural network based evaluation of online teaching status during epidemic period",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2061174411",
                        "name": "Ni-Chun Yao"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Tasks For physical simulation tasks, we use the setting of pixel pendulum and real pendulum in the paper of HNN (Greydanus et al., 2019).",
                "In the experimental part, we verify that NODA provides a more efficient modeling than HNN, and we can use prior knowledge or transfer learning to further boost its training.",
                "For physical simulation tasks, we choose AE and Hamiltonian neural network (HNN) (Greydanus et al., 2019) as baseline methods.",
                "The testing loss curves of HNN, AE and NODA over two physical environments are shown in Figure 2 (a) and (b).",
                "For example, Lagrangian neural networks (Lutter et al., 2019; Cranmer et al., 2020) and Hamiltonian neural networks (Greydanus et al., 2019) can be used to simulate dynamic systems.",
                ", 2020) and Hamiltonian neural networks (Greydanus et al., 2019) can be used to simulate dynamic systems.",
                "In each experiment, the number of NODA\u2019s parameters equals to that of AE\u2019s parameters, and it is no more than that of HNN\u2019s parameters."
            ],
            "citingPaper": {
                "paperId": "cce3bf78011c718791c9d0719f782eb5d1ea8d5a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-00942",
                    "ArXiv": "2211.00942",
                    "DOI": "10.48550/arXiv.2211.00942",
                    "CorpusId": 253255038
                },
                "corpusId": 253255038,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cce3bf78011c718791c9d0719f782eb5d1ea8d5a",
                "title": "Model-based Reinforcement Learning with a Hamiltonian Canonical ODE Network",
                "abstract": "Model-based reinforcement learning usually suffers from a high sample complexity in training the world model, especially for the environments with complex dynamics. To make the training for general physical environments more ef\ufb01cient, we introduce Hamiltonian canonical ordinary differential equations into the learning process, which inspires a novel model of neural ordinary differential auto-encoder (NODA). NODA can model the physical world by nature and is \ufb02exible to impose Hamiltonian mechanics (e.g., the dimension of the physical equations) which can further accelerate training of the environment models. It can consequentially em-power an RL agent with the robust extrapolation using a small amount of samples as well as the guarantee on the physical plausibility. Theoretically, we prove that NODA has uniform bounds for multi-step transition errors and value errors under certain conditions. Extensive experiments show that NODA can learn the environment dynamics effectively with a high sample ef\ufb01ciency, making it possible to facilitate reinforcement learning agents at the early stage.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115389182",
                        "name": "Yao Feng"
                    },
                    {
                        "authorId": "1588138822",
                        "name": "Yuhong Jiang"
                    },
                    {
                        "authorId": "2093561216",
                        "name": "Hang Su"
                    },
                    {
                        "authorId": "143848636",
                        "name": "Dong Yan"
                    },
                    {
                        "authorId": "2146280496",
                        "name": "Jun Zhu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "eb87f128bf2556a2684f37b8171d2aaac353f4d3",
                "externalIds": {
                    "DBLP": "journals/entropy/YouZPLC22",
                    "PubMedCentral": "9689721",
                    "DOI": "10.3390/e24111651",
                    "CorpusId": 253571199,
                    "PubMed": "36421506"
                },
                "corpusId": 253571199,
                "publicationVenue": {
                    "id": "8270cfe1-3713-4325-a7bd-c6a87eed889e",
                    "name": "Entropy",
                    "type": "journal",
                    "issn": "1099-4300",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-155606",
                    "alternate_urls": [
                        "http://www.mdpi.com/journal/entropy/",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-155606",
                        "https://www.mdpi.com/journal/entropy"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/eb87f128bf2556a2684f37b8171d2aaac353f4d3",
                "title": "Spatiotemporal Transformer Neural Network for Time-Series Forecasting",
                "abstract": "Predicting high-dimensional short-term time-series is a difficult task due to the lack of sufficient information and the curse of dimensionality. To overcome these problems, this study proposes a novel spatiotemporal transformer neural network (STNN) for efficient prediction of short-term time-series with three major features. Firstly, the STNN can accurately and robustly predict a high-dimensional short-term time-series in a multi-step-ahead manner by exploiting high-dimensional/spatial information based on the spatiotemporal information (STI) transformation equation. Secondly, the continuous attention mechanism makes the prediction results more accurate than those of previous studies. Thirdly, we developed continuous spatial self-attention, temporal self-attention, and transformation attention mechanisms to create a bridge between effective spatial information and future temporal evolution information. Fourthly, we show that the STNN model can reconstruct the phase space of the dynamical system, which is explored in the time-series prediction. The experimental results demonstrate that the STNN significantly outperforms the existing methods on various benchmarks and real-world systems in the multi-step-ahead prediction of a short-term time-series.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1947879835",
                        "name": "Yujie You"
                    },
                    {
                        "authorId": "2108007081",
                        "name": "Le Zhang"
                    },
                    {
                        "authorId": "2228727150",
                        "name": "Tao Peng"
                    },
                    {
                        "authorId": "2108164991",
                        "name": "Suran Liu"
                    },
                    {
                        "authorId": "2145146596",
                        "name": "Luonan Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Compared to traditional discrete layers, Neural ODEs [24] demonstrate that continuous modeling of neural network can better learn the continuous structures [22, 56] with infinite depth [44] and constant memory cost [69]."
            ],
            "citingPaper": {
                "paperId": "255d74d345f8fdb5b3bb5030c73a5cabee611a45",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-15812",
                    "ArXiv": "2210.15812",
                    "DOI": "10.48550/arXiv.2210.15812",
                    "CorpusId": 253223909
                },
                "corpusId": 253223909,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/255d74d345f8fdb5b3bb5030c73a5cabee611a45",
                "title": "Differentiable Analog Quantum Computing for Optimization and Control",
                "abstract": "We formulate the first differentiable analog quantum computing framework with a specific parameterization design at the analog signal (pulse) level to better exploit near-term quantum devices via variational methods. We further propose a scalable approach to estimate the gradients of quantum dynamics using a forward pass with Monte Carlo sampling, which leads to a quantum stochastic gradient descent algorithm for scalable gradient-based training in our framework. Applying our framework to quantum optimization and control, we observe a significant advantage of differentiable analog quantum computing against SOTAs based on parameterized digital quantum circuits by orders of magnitude.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "153221430",
                        "name": "Jiaqi Leng"
                    },
                    {
                        "authorId": "2111033663",
                        "name": "Yuxiang Peng"
                    },
                    {
                        "authorId": "51300425",
                        "name": "Yi-Ling Qiao"
                    },
                    {
                        "authorId": "2115913407",
                        "name": "Ming-Chyuan Lin"
                    },
                    {
                        "authorId": "33948760",
                        "name": "Xiaodi Wu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "7d6d90f7159c544ad2d050c87ba0404411b756c2",
                "externalIds": {
                    "DOI": "10.1109/ICUS55513.2022.9986797",
                    "CorpusId": 255266739
                },
                "corpusId": 255266739,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7d6d90f7159c544ad2d050c87ba0404411b756c2",
                "title": "Neural Mixed Platoon Controller Design",
                "abstract": "Vehicle platooning can be formulated as an optimal control problem and many solving paradigms, such as Pontryagin's maximum principle-based and dynamical programming methods, have been recently developed. However, these methods usually rely on solving a group of necessary conditions or Hamilton-Jacobi-Bellman (HJB) partial differential equations, which is hard to calculate. Besides, due to the heterogeneous dynamics of different vehicles in a mixed and complex platoon which comprises of not only connected autonomous vehicles (CAVs), but also human-driven vehicles (HDVs), it is also challenging to coordinate the behaviors of different vehicles in an unified control framework. Here we provide a Neural Mixed Platoon Control (NMPC) framework, a novel control design for mixed vehicle platooning based on a neural ordinary differential equation (NODE). We first formulate an optimal control model that incorporates the heterogeneous dynamics of a leading CAV and several following HDVs. We use a neural network to parameterize a state-feedback controller and join the neural controller and the mixed platooning dynamics into the NODE solver to create a closed-loop and learnable controlled system. The resulting system can learn optimal control inputs driving the mixed platoon to evolve from a given beginning condition to the target state within a finite duration in an unsupervised manner. Finally, simulation results validate our suggested method's usefulness in terms of space headway and velocity tracking.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1379963984",
                        "name": "Ailing Xie"
                    },
                    {
                        "authorId": "2196231",
                        "name": "Jianshan Zhou"
                    },
                    {
                        "authorId": "2992987",
                        "name": "Daxin Tian"
                    },
                    {
                        "authorId": "2716574",
                        "name": "Xuting Duan"
                    },
                    {
                        "authorId": "143876012",
                        "name": "Zhengguo Sheng"
                    },
                    {
                        "authorId": "2210035",
                        "name": "Dezong Zhao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e3a7cde8082cd96c5902e9135dd49f0ad427b536",
                "externalIds": {
                    "DBLP": "journals/amses/ChinestaC22",
                    "DOI": "10.1186/s40323-022-00234-8",
                    "CorpusId": 253155609
                },
                "corpusId": 253155609,
                "publicationVenue": {
                    "id": "7ae89e69-1422-4aaa-a84b-18c5730a1a95",
                    "name": "Advanced Modeling and Simulation in Engineering Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Adv Model Simul Eng Sci"
                    ],
                    "issn": "2213-7467",
                    "url": "http://link.springer.com/journal/volumesAndIssues/40323",
                    "alternate_urls": [
                        "https://amses-journal.springeropen.com",
                        "https://amses-journal.springeropen.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e3a7cde8082cd96c5902e9135dd49f0ad427b536",
                "title": "Empowering engineering with data, machine learning and artificial intelligence: a short introductive review",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2734584",
                        "name": "F. Chinesta"
                    },
                    {
                        "authorId": "3246447",
                        "name": "E. Cueto"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "1fadcc635180e0ce440dda6b9462e715a5a46270",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-05972",
                    "ArXiv": "2210.05972",
                    "DOI": "10.48550/arXiv.2210.05972",
                    "CorpusId": 252846027
                },
                "corpusId": 252846027,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1fadcc635180e0ce440dda6b9462e715a5a46270",
                "title": "Unsupervised Learning of Equivariant Structure from Sequences",
                "abstract": "In this study, we present meta-sequential prediction (MSP), an unsupervised framework to learn the symmetry from the time sequence of length at least three. Our method leverages the stationary property (e.g. constant velocity, constant acceleration) of the time sequence to learn the underlying equivariant structure of the dataset by simply training the encoder-decoder model to be able to predict the future observations. We will demonstrate that, with our framework, the hidden disentangled structure of the dataset naturally emerges as a by-product by applying simultaneous block-diagonalization to the transition operators in the latent space, the procedure which is commonly used in representation theory to decompose the feature-space based on the type of response to group actions. We will showcase our method from both empirical and theoretical perspectives. Our result suggests that finding a simple structured relation and learning a model with extrapolation capability are two sides of the same coin. The code is available at https://github.com/takerum/meta_sequential_prediction.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3213400",
                        "name": "Takeru Miyato"
                    },
                    {
                        "authorId": "2877296",
                        "name": "Masanori Koyama"
                    },
                    {
                        "authorId": "1693668",
                        "name": "K. Fukumizu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4684fa0607b989a64f7d45da4019df970f071d0c",
                "externalIds": {
                    "ArXiv": "2210.05087",
                    "PubMedCentral": "10206110",
                    "DBLP": "journals/corr/abs-2210-05087",
                    "DOI": "10.1038/s41598-023-34862-w",
                    "CorpusId": 252815816,
                    "PubMed": "37221253"
                },
                "corpusId": 252815816,
                "publicationVenue": {
                    "id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                    "name": "Scientific Reports",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Rep"
                    ],
                    "issn": "2045-2322",
                    "url": "http://www.nature.com/srep/",
                    "alternate_urls": [
                        "http://www.nature.com/srep/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4684fa0607b989a64f7d45da4019df970f071d0c",
                "title": "Approximation of nearly-periodic symplectic maps via structure-preserving neural networks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2032462536",
                        "name": "Valentin Duruisseaux"
                    },
                    {
                        "authorId": "2264263",
                        "name": "J. Burby"
                    },
                    {
                        "authorId": "1429560602",
                        "name": "Q. Tang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "INTRODUCTION Physics-informed machine learning is applied to numerous highly complex problems, such as turbulence and climate modeling [7], [44], [45], model predictive control [3], [33], and Hamiltonian system dynamics [16]."
            ],
            "citingPaper": {
                "paperId": "4e2e72f2dcde473555a231d839a26f51839a8d54",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-03426",
                    "ArXiv": "2210.03426",
                    "DOI": "10.48550/arXiv.2210.03426",
                    "CorpusId": 252762483
                },
                "corpusId": 252762483,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4e2e72f2dcde473555a231d839a26f51839a8d54",
                "title": "Certified machine learning: Rigorous a posteriori error bounds for PDE defined PINNs",
                "abstract": "Prediction error quantification in machine learning has been left out of most methodological investigations of neural networks, for both purely data-driven and physics-informed approaches. Beyond statistical investigations and generic results on the approximation capabilities of neural networks, we present a rigorous upper bound on the prediction error of physics-informed neural networks. This bound can be calculated without the knowledge of the true solution and only with a priori available information about the characteristics of the underlying dynamical system governed by a partial differential equation. We apply this a posteriori error bound exemplarily to four problems: the transport equation, the heat equation, the Navier-Stokes equation and the Klein-Gordon equation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2050455195",
                        "name": "Birgit Hillebrecht"
                    },
                    {
                        "authorId": "2000025",
                        "name": "B. Unger"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2021), and endowing NeuralODEs with mathematical structures that the system must satisfy (Greydanus et al., 2019; Finzi et al., 2020).",
                "\u2026on this idea, including blending NeuralODEs with partial information on the form of the governing equation to produce \u201dgrey-box\u201d dynamics model (Rackauckas et al., 2021), and endowing NeuralODEs with mathematical structures that the system must satisfy (Greydanus et al., 2019; Finzi et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "52d5bc698684d676a407c879fc826d2d5596211b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01407",
                    "ArXiv": "2210.01407",
                    "DOI": "10.48550/arXiv.2210.01407",
                    "CorpusId": 252693347
                },
                "corpusId": 252693347,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/52d5bc698684d676a407c879fc826d2d5596211b",
                "title": "Homotopy-based training of NeuralODEs for accurate dynamics discovery",
                "abstract": "Neural Ordinary Differential Equations (NeuralODEs) present an attractive way to extract dynamical laws from time series data, as they bridge neural networks with the differential equation-based modeling paradigm of the physical sciences. However, these models often display long training times and suboptimal results, especially for longer duration data. While a common strategy in the literature imposes strong constraints to the NeuralODE architecture to inherently promote stable model dynamics, such methods are ill-suited for dynamics discovery as the unknown governing equation is not guaranteed to satisfy the assumed constraints. In this paper, we develop a new training method for NeuralODEs, based on synchronization and homotopy optimization, that does not require changes to the model architecture. We show that synchronizing the model dynamics and the training data tames the originally irregular loss landscape, which homotopy optimization can then leverage to enhance training. Through benchmark experiments, we demonstrate our method achieves competitive or better training loss while often requiring less than half the number of training epochs compared to other model-agnostic techniques. Furthermore, models trained with our method display better extrapolation capabilities, highlighting the effectiveness of our method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1741512778",
                        "name": "Joon-Hyuk Ko"
                    },
                    {
                        "authorId": "2186874805",
                        "name": "Hankyul Koh"
                    },
                    {
                        "authorId": "2052428450",
                        "name": "Nojun Park"
                    },
                    {
                        "authorId": "4388050",
                        "name": "W. Jhe"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "HNN (Greydanus et al., 2019) assumes the target system to be a Hamiltonian system in the canonical form, thereby guaranteeing various properties of Hamiltonian systems by definition, including the conservation of energy and preservation of the symplectic structure in continuous time (Hairer et al.",
                "We implemented all codes by modifying the officially released codes of HNN (Greydanus et al., 2019) 1 and DGNet (Matsubara et al., 2020)2.",
                "Following HNN (Greydanus et al., 2019) and DGNet (Matsubara et al.",
                "We implemented all codes by modifying the officially released codes of HNN (Greydanus et al., 2019) 1 and DGNet (Matsubara et al.",
                "2-pend 2-body\nModel 1-step\u2193 VPT\u2191 1-step\u2193 VPT\u2191\nNODE 0.82 \u00b10.020 0.110 \u00b10.035 144.21 \u00b112.65 0.134 \u00b10.014 HNN (Greydanus et al., 2019) 6220.26 \u00b191.57 0.002 \u00b10.000 5.17 \u00b10.570 0.362 \u00b10.026 CHNN (Finzi et al., 2020b) 0.07 \u00b10.000 0.928 \u00b10.036 (not working)\nNODE+cFINDE 0.71 \u00b10.040 0.461 \u00b10.071 163.64\u2026",
                "Greydanus et al. (2019) proposed the Hamiltonian neural network (HNN), which employs a neural network to approximate Hamilton\u2019s equation, thereby conserving the system energy called the Hamiltonian.",
                "HNN (Greydanus et al., 2019) assumes the target system to be a Hamiltonian system in the canonical form, thereby guaranteeing various properties of Hamiltonian systems by definition, including the conservation of energy and preservation of the symplectic structure in continuous time (Hairer et al.,\u2026",
                "The HNN was developed to model Hamiltonian systems in the canonical forms (Greydanus et al., 2019).",
                "Following HNN (Greydanus et al., 2019) and DGNet (Matsubara et al., 2020), we used fullyconnected neural networks with two hidden layers.",
                ", 2018) HNN (Greydanus et al., 2019) X LieConv (Finzi et al."
            ],
            "citingPaper": {
                "paperId": "b170b752042eee58985968080dc7c9a86374587b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-00272",
                    "ArXiv": "2210.00272",
                    "DOI": "10.48550/arXiv.2210.00272",
                    "CorpusId": 252682977
                },
                "corpusId": 252682977,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b170b752042eee58985968080dc7c9a86374587b",
                "title": "FINDE: Neural Differential Equations for Finding and Preserving Invariant Quantities",
                "abstract": "Many real-world dynamical systems are associated with first integrals (a.k.a. invariant quantities), which are quantities that remain unchanged over time. The discovery and understanding of first integrals are fundamental and important topics both in the natural sciences and in industrial applications. First integrals arise from the conservation laws of system energy, momentum, and mass, and from constraints on states; these are typically related to specific geometric structures of the governing equations. Existing neural networks designed to ensure such first integrals have shown excellent accuracy in modeling from data. However, these models incorporate the underlying structures, and in most situations where neural networks learn unknown systems, these structures are also unknown. This limitation needs to be overcome for scientific discovery and modeling of unknown systems. To this end, we propose first integral-preserving neural differential equation (FINDE). By leveraging the projection method and the discrete gradient method, FINDE finds and preserves first integrals from data, even in the absence of prior knowledge about underlying structures. Experimental results demonstrate that FINDE can predict future states of target systems much longer and find various quantities consistent with well-known first integrals in a unified manner.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144872058",
                        "name": "Takashi Matsubara"
                    },
                    {
                        "authorId": "2580628",
                        "name": "Takaharu Yaguchi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "[1], Greydanus and Sosanya [37] can be interpreted as an application of the explicit Euler integrator David and M\u00e9hats [10].",
                "Popular integrators such as forward Euler, Runge-Kutta 4 (RK4) and Leapfrog [1, 6, 36] unfortunately do no maintain the manifold structure.",
                ", [1, 3, 6, 15]); however, the Newtonian (point-mass) gravity considered is already well understood.",
                "One important class of systems to be learned have dynamics described by physical laws, whose structure can be exploited by learning the Hamiltonian of the system instead of the vector field [1, 2].",
                "Note that we do not assume access to the true derivatives q\u0307k,j and \u1e57k,j used in the loss function of some works [1, 37, 38].",
                "[1] Samuel Greydanus, Misko Dzamba, and Jason Yosinski.",
                "Seminal work initially approximated the time derivative via finite differences and then matched it with a learned (Hamiltonian) vector field[1, 2]."
            ],
            "citingPaper": {
                "paperId": "0fbf6903a41c9752f5b8bbe607d8219a1520af0f",
                "externalIds": {
                    "ArXiv": "2210.00090",
                    "DBLP": "journals/corr/abs-2210-00090",
                    "DOI": "10.48550/arXiv.2210.00090",
                    "CorpusId": 252683623
                },
                "corpusId": 252683623,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0fbf6903a41c9752f5b8bbe607d8219a1520af0f",
                "title": "Data-driven discovery of non-Newtonian astronomy via learning non-Euclidean Hamiltonian",
                "abstract": "Incorporating the Hamiltonian structure of physical dynamics into deep learning models provides a powerful way to improve the interpretability and prediction accuracy. While previous works are mostly limited to the Euclidean spaces, their extension to the Lie group manifold is needed when rotations form a key component of the dynamics, such as the higher-order physics beyond simple point-mass dynamics for N-body celestial interactions. Moreover, the multiscale nature of these processes presents a challenge to existing methods as a long time horizon is required. By leveraging a symplectic Lie-group manifold preserving integrator, we present a method for data-driven discovery of non-Newtonian astronomy. Preliminary results show the importance of both these properties in training stability and prediction accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1923350663",
                        "name": "Oswin So"
                    },
                    {
                        "authorId": "2108421697",
                        "name": "Gongjie Li"
                    },
                    {
                        "authorId": "1751063",
                        "name": "Evangelos A. Theodorou"
                    },
                    {
                        "authorId": "46699279",
                        "name": "Molei Tao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5abd34dc1461f9ab64771e2eed3f1cf69b5ddc0d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-13511",
                    "ArXiv": "2209.13511",
                    "DOI": "10.48550/arXiv.2209.13511",
                    "CorpusId": 252545049
                },
                "corpusId": 252545049,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5abd34dc1461f9ab64771e2eed3f1cf69b5ddc0d",
                "title": "Phy-Taylor: Physics-Model-Based Deep Neural Networks",
                "abstract": "Purely data-driven deep neural networks (DNNs) applied to physical engineering systems can infer relations that violate physics laws, thus leading to unexpected consequences. To address this challenge, we propose a physics-model-based DNN framework, called Phy-Taylor, that accelerates learning compliant representations with physical knowledge. The Phy-Taylor framework makes two key contributions; it introduces a new architectural Physics-compatible neural network (PhN), and features a novel compliance mechanism, we call {\\em Physics-guided Neural Network Editing\\}. The PhN aims to directly capture nonlinearities inspired by physical quantities, such as kinetic energy, potential energy, electrical power, and aerodynamic drag force. To do so, the PhN augments neural network layers with two key components: (i) monomials of Taylor series expansion of nonlinear functions capturing physical knowledge, and (ii) a suppressor for mitigating the influence of noise. The neural-network editing mechanism further modifies network links and activation functions consistently with physical knowledge. As an extension, we also propose a self-correcting Phy-Taylor framework that introduces two additional capabilities: (i) physics-model-based safety relationship learning, and (ii) automatic output correction when violations of safety occur. Through experiments, we show that (by expressing hard-to-learn nonlinearities directly and by constraining dependencies) Phy-Taylor features considerably fewer parameters, and a remarkably accelerated training process, while offering enhanced model robustness and accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1937682",
                        "name": "Y. Mao"
                    },
                    {
                        "authorId": "2919287",
                        "name": "L. Sha"
                    },
                    {
                        "authorId": "3395273",
                        "name": "Huajie Shao"
                    },
                    {
                        "authorId": "30955167",
                        "name": "Yuliang Gu"
                    },
                    {
                        "authorId": "2109090616",
                        "name": "Qixin Wang"
                    },
                    {
                        "authorId": "1730531",
                        "name": "T. Abdelzaher"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In addition, such learning models have been further extended to incorporate the physical inductive bias of the underlying problems [3, 5, 19, 28, 36, 54, 55, 56]."
            ],
            "citingPaper": {
                "paperId": "9c0321a430c8ed933cb1f993010bd724f6860b38",
                "externalIds": {
                    "ArXiv": "2209.12123",
                    "DBLP": "journals/corr/abs-2209-12123",
                    "DOI": "10.48550/arXiv.2209.12123",
                    "CorpusId": 252531675
                },
                "corpusId": 252531675,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9c0321a430c8ed933cb1f993010bd724f6860b38",
                "title": "Error analysis based on inverse modified differential equations for discovery of dynamics using linear multistep methods and deep learning",
                "abstract": "Along with the practical success of the discovery of dynamics using deep learning, the theoretical analysis of this approach has attracted increasing attention. Prior works have established the grid error estimation with auxiliary conditions for the discovery of dynamics using linear multistep methods and deep learning. And we extend the existing error analysis in this work. We first introduce the concept of inverse modified differential equations (IMDE) for linear multistep methods and show that the learned model returns a close approximation of the IMDE. Based on the IMDE, we prove that the error between the discovered system and the target system is bounded by the sum of the LMM discretization error and the learning loss. Furthermore, the learning loss is quantified by combining the approximation and generalization theories of neural networks, and thereby we obtain the priori error estimates for the discovery of dynamics using linear multistep methods. Several numerical experiments are performed to verify the theoretical analysis.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2430811",
                        "name": "Aiqing Zhu"
                    },
                    {
                        "authorId": "2112452889",
                        "name": "Sidi Wu"
                    },
                    {
                        "authorId": "1805110",
                        "name": "Yifa Tang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "The layer weights are initialised with a random normal distribution similarly to [13]: the standard deviation of the normal distribution is set to 2/nh for the first hidden layer D1, 1/ \u221a nh for the second hidden layer D2 and \u221a nh for the output layer D3; the biases of all layers are initialised to zero.",
                "In [30] the GNN architecture is combined with the Hamiltonian neural network approach [13], and it is demonstrated that this can lead to more accurate predictions.",
                "In contrast to our work, they use the Hamiltonian formulation in [13].",
                "1 Hamiltonian- and Lagrangian neural networks For Hamiltonian systems a completely different approach is pursued in [13]: a neural network is trained to learn a scalar valued function HNN(q, p) which approximates the true Hamiltonian H(q, p) as a function of the generalised coordinates q \u2208 R and conjugate momenta p \u2208 R; here and in the following d is the dimension of the dynamical system.",
                "While the authors on [13] do not exploit this since they use a non-symplectic fourth order Runge Kutta integrator, in [14] it is argued that symplectic Neural Networks (SRNNs) show superior performance.",
                "To achieve this, we use the Lagrangian formalism and represent the Lagrangian LNN by a neural network as in [13].",
                "Removing the symmetry-enforcing layer S results in the standard architecture already introduced in [13]."
            ],
            "citingPaper": {
                "paperId": "af901507585dc722397e5e07355f646401c5329d",
                "externalIds": {
                    "ArXiv": "2209.11661",
                    "DOI": "10.1016/j.jcp.2023.112234",
                    "CorpusId": 252519584
                },
                "corpusId": 252519584,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/af901507585dc722397e5e07355f646401c5329d",
                "title": "Exact conservation laws for neural network integrators of dynamical systems",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2061778041",
                        "name": "E. Muller"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "eba59ef649b1f8c203ff3ee5a2f630e840385e25",
                "externalIds": {
                    "ArXiv": "2209.11355",
                    "DBLP": "journals/corr/abs-2209-11355",
                    "DOI": "10.48550/arXiv.2209.11355",
                    "CorpusId": 252519570
                },
                "corpusId": 252519570,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/eba59ef649b1f8c203ff3ee5a2f630e840385e25",
                "title": "Learning Interpretable Dynamics from Images of a Freely Rotating 3D Rigid Body",
                "abstract": "In many real-world settings, image observations of freely rotating 3D rigid bodies, such as satellites, may be available when low-dimensional measurements are not. However, the high-dimensionality of image data precludes the use of classical estimation techniques to learn the dynamics and a lack of interpretability reduces the usefulness of standard deep learning methods. In this work, we present a physics-informed neural network model to estimate and predict 3D rotational dynamics from image sequences. We achieve this using a multi-stage prediction pipeline that maps individual images to a latent representation homeomorphic to $\\mathbf{SO}(3)$, computes angular velocities from latent pairs, and predicts future latent states using the Hamiltonian equations of motion with a learned representation of the Hamiltonian. We demonstrate the efficacy of our approach on a new rotating rigid-body dataset with sequences of rotating cubes and rectangular prisms with uniform and non-uniform density.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "34819822",
                        "name": "J. Mason"
                    },
                    {
                        "authorId": "1405669277",
                        "name": "Christine Allen-Blanchette"
                    },
                    {
                        "authorId": "102569860",
                        "name": "Nick Zolman"
                    },
                    {
                        "authorId": "2178760411",
                        "name": "Elizabeth Davison"
                    },
                    {
                        "authorId": "3301461",
                        "name": "Naomi Ehrich Leonard"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "0081ceae9b6125954e057698ba3a65824d3f89a7",
                "externalIds": {
                    "DBLP": "conf/mlmi2/WenLQ22",
                    "DOI": "10.1145/3568199.3568209",
                    "CorpusId": 257354094
                },
                "corpusId": 257354094,
                "publicationVenue": {
                    "id": "0675e80f-4226-4332-87ac-0277bfb95a5c",
                    "name": "Machine Learning for Multimodal Interaction",
                    "type": "conference",
                    "alternate_names": [
                        "MLMI",
                        "Mach Learn Multimodal Interact"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0081ceae9b6125954e057698ba3a65824d3f89a7",
                "title": "Learning Symplectic Dynamics via Generating Recurrent Neural Network",
                "abstract": "Although Hamiltonian Neural Network (HNN) achieve good accuracy for various numeral solvers, if theoretic results of Hamiltonian systems are applicable to HNN, it is paramount to find the exactly symplectic map in discrete time. In this paper, consider a time series data generated by a latent symplectic map, we propose a novel neural network that directly learns the latent symplectic map whether its existence or not and does not rely on vector filed. To ensure this, we convert the symplectic map to a generating function, which inherently presents a time-series property. Therefore, we approximate symplectic map by a RNNs based on a generating function (hence the name GRNN). In the learning process, each state passing of GRNN need a fixed point to expand around, which make the local prediction error at each step to accumulate in a controlled fashion. By co-training the GRNN and HNN, our numerical experiments show that such structures can certainly be learned from data and obtain the better performance comparing with other methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "66157475",
                        "name": "Guoquan Wen"
                    },
                    {
                        "authorId": "2210776038",
                        "name": "Duo Li"
                    },
                    {
                        "authorId": "2083944208",
                        "name": "Fengqing Qin"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "To this extent, three broad approaches have been proposed, namely, Lagrangian neural networks (Lnn) [9, 5, 7], Hamiltonian neural networks (Hnn) [10, 11, 3, 12], and neural ODE (Node) [13, 14].",
                "Although the idea of graph-based modeling have been suggested for physical systems [16, 11], the inductive biases induced due to different graph structures and their consequences on the dynamics remain poorly explored."
            ],
            "citingPaper": {
                "paperId": "521e45bed5410b41eece700cb89614270a81412e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-10740",
                    "ArXiv": "2209.10740",
                    "DOI": "10.48550/arXiv.2209.10740",
                    "CorpusId": 252438817
                },
                "corpusId": 252438817,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/521e45bed5410b41eece700cb89614270a81412e",
                "title": "Enhancing the Inductive Biases of Graph Neural ODE for Modeling Dynamical Systems",
                "abstract": "Neural networks with physics based inductive biases such as Lagrangian neural networks (LNN), and Hamiltonian neural networks (HNN) learn the dynamics of physical systems by encoding strong inductive biases. Alternatively, Neural ODEs with appropriate inductive biases have also been shown to give similar performances. However, these models, when applied to particle based systems, are transductive in nature and hence, do not generalize to large system sizes. In this paper, we present a graph based neural ODE, GNODE, to learn the time evolution of dynamical systems. Further, we carefully analyse the role of different inductive biases on the performance of GNODE. We show that, similar to LNN and HNN, encoding the constraints explicitly can significantly improve the training efficiency and performance of GNODE significantly. Our experiments also assess the value of additional inductive biases, such as Newtons third law, on the final performance of the model. We demonstrate that inducing these biases can enhance the performance of model by orders of magnitude in terms of both energy violation and rollout error. Interestingly, we observe that the GNODE trained with the most effective inductive biases, namely MCGNODE, outperforms the graph versions of LNN and HNN, namely, Lagrangian graph networks (LGN) and Hamiltonian graph networks (HGN) in terms of energy violation error by approx 4 orders of magnitude for a pendulum system, and approx 2 orders of magnitude for spring systems. These results suggest that competitive performances with energy conserving neural networks can be obtained for NODE based systems by inducing appropriate inductive biases.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "15675757",
                        "name": "S. Bishnoi"
                    },
                    {
                        "authorId": "2131107456",
                        "name": "Ravinder Bhattoo"
                    },
                    {
                        "authorId": "1699732",
                        "name": "Sayan Ranu"
                    },
                    {
                        "authorId": "2144896197",
                        "name": "N. Krishnan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "fda29ca609bafeea97525edd4f0b0a8ba402e576",
                "externalIds": {
                    "ArXiv": "2209.10633",
                    "DBLP": "journals/corr/abs-2209-10633",
                    "DOI": "10.48550/arXiv.2209.10633",
                    "CorpusId": 252438926
                },
                "corpusId": 252438926,
                "publicationVenue": {
                    "id": "7023c73b-da73-4f6e-8811-e15480606a9c",
                    "name": "Journal of Data Science",
                    "type": "journal",
                    "alternate_names": [
                        "J Data Sci",
                        "Journal of data science",
                        "J data sci"
                    ],
                    "issn": "1680-743X",
                    "url": "https://www.sinica.edu.tw/",
                    "alternate_urls": [
                        "https://www.sinica.edu.tw/~jds/",
                        "https://www.sinica.edu.tw/~jds/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fda29ca609bafeea97525edd4f0b0a8ba402e576",
                "title": "Neural Generalized Ordinary Differential Equations with Layer-varying Parameters",
                "abstract": "Deep residual networks (ResNets) have shown state-of-the-art performance in various real-world applications. Recently, the ResNets model was reparameterized and interpreted as solutions to a continuous ordinary differential equation or Neural-ODE model. In this study, we propose a neural generalized ordinary differential equation (Neural-GODE) model with layer-varying parameters to further extend the Neural-ODE to approximate the discrete ResNets. Specifically, we use nonparametric B-spline functions to parameterize the Neural-GODE so that the trade-off between the model complexity and computational efficiency can be easily balanced. It is demonstrated that ResNets and Neural-ODE models are special cases of the proposed Neural-GODE model. Based on two benchmark datasets, MNIST and CIFAR-10, we show that the layer-varying Neural-GODE is more flexible and general than the standard Neural-ODE. Furthermore, the Neural-GODE enjoys the computational and memory benefits while performing comparably to ResNets in prediction accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111359554",
                        "name": "Duo Yu"
                    },
                    {
                        "authorId": "47355902",
                        "name": "Hongyu Miao"
                    },
                    {
                        "authorId": "30120418",
                        "name": "Hulin Wu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "6ac4b06adbb762b06fe6d309e3b1b6714a074a43",
                "externalIds": {
                    "ArXiv": "2209.09349",
                    "DBLP": "journals/corr/abs-2209-09349",
                    "DOI": "10.48550/arXiv.2209.09349",
                    "CorpusId": 252383380
                },
                "corpusId": 252383380,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6ac4b06adbb762b06fe6d309e3b1b6714a074a43",
                "title": "Physics-Informed Machine Learning of Dynamical Systems for Efficient Bayesian Inference",
                "abstract": "Although the no-u-turn sampler (NUTS) is a widely adopted method for performing Bayesian inference, it requires numerous posterior gradients which can be expensive to compute in practice. Recently, there has been a significant interest in physics-based machine learning of dynamical (or Hamiltonian) systems and Hamiltonian neural networks (HNNs) is a noteworthy architecture. But these types of architectures have not been applied to solve Bayesian inference problems efficiently. We propose the use of HNNs for performing Bayesian inference efficiently without requiring numerous posterior gradients. We introduce latent variable outputs to HNNs (L-HNNs) for improved expressivity and reduced integration errors. We integrate L-HNNs in NUTS and further propose an online error monitoring scheme to prevent sampling degeneracy in regions where L-HNNs may have little training data. We demonstrate L-HNNs in NUTS with online error monitoring considering several complex high-dimensional posterior densities and compare its performance to NUTS.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "101835579",
                        "name": "Somayajulu L. N. Dhulipala"
                    },
                    {
                        "authorId": "152579565",
                        "name": "Yifeng Che"
                    },
                    {
                        "authorId": "2270463",
                        "name": "M. Shields"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "18e5ee0ea920ed3af114ed5b1efe6cb5358252d3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-07081",
                    "ArXiv": "2209.07081",
                    "DOI": "10.48550/arXiv.2209.07081",
                    "CorpusId": 252280344
                },
                "corpusId": 252280344,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/18e5ee0ea920ed3af114ed5b1efe6cb5358252d3",
                "title": "DEQGAN: Learning the Loss Function for PINNs with Generative Adversarial Networks",
                "abstract": "Solutions to differential equations are of significant scientific and engineering relevance. Physics-Informed Neural Networks (PINNs) have emerged as a promising method for solving differential equations, but they lack a theoretical justification for the use of any particular loss function. This work presents Differential Equation GAN (DEQGAN), a novel method for solving differential equations using generative adversarial networks to\"learn the loss function\"for optimizing the neural network. Presenting results on a suite of twelve ordinary and partial differential equations, including the nonlinear Burgers', Allen-Cahn, Hamilton, and modified Einstein's gravity equations, we show that DEQGAN can obtain multiple orders of magnitude lower mean squared errors than PINNs that use $L_2$, $L_1$, and Huber loss functions. We also show that DEQGAN achieves solution accuracies that are competitive with popular numerical methods. Finally, we present two methods to improve the robustness of DEQGAN to different hyperparameter settings.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2164385011",
                        "name": "Blake Bullwinkel"
                    },
                    {
                        "authorId": "1825788803",
                        "name": "Dylan Randle"
                    },
                    {
                        "authorId": "1735677",
                        "name": "P. Protopapas"
                    },
                    {
                        "authorId": "144633639",
                        "name": "David Sondak"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "2d1b83626d9f55b94335d24037a253a9ac35d63d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-07364",
                    "ArXiv": "2209.07364",
                    "DOI": "10.48550/arXiv.2209.07364",
                    "CorpusId": 252283953
                },
                "corpusId": 252283953,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2d1b83626d9f55b94335d24037a253a9ac35d63d",
                "title": "Continuous MDP Homomorphisms and Homomorphic Policy Gradient",
                "abstract": "Abstraction has been widely studied as a way to improve the efficiency and generalization of reinforcement learning algorithms. In this paper, we study abstraction in the continuous-control setting. We extend the definition of MDP homomorphisms to encompass continuous actions in continuous state spaces. We derive a policy gradient theorem on the abstract MDP, which allows us to leverage approximate symmetries of the environment for policy optimization. Based on this theorem, we propose an actor-critic algorithm that is able to learn the policy and the MDP homomorphism map simultaneously, using the lax bisimulation metric. We demonstrate the effectiveness of our method on benchmark tasks in the DeepMind Control Suite. Our method's ability to utilize MDP homomorphisms for representation learning leads to improved performance when learning from pixel observations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1417282075",
                        "name": "S. Rezaei-Shoshtari"
                    },
                    {
                        "authorId": "2004617613",
                        "name": "Rosie Zhao"
                    },
                    {
                        "authorId": "1784317",
                        "name": "P. Panangaden"
                    },
                    {
                        "authorId": "2462512",
                        "name": "D. Meger"
                    },
                    {
                        "authorId": "144368601",
                        "name": "Doina Precup"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "This setting is similar to the approach considered in [6] or [32] where the authors use nonsymplectic integrators for training structure-preserving neural networks and then integrate the learned Hamiltonian models using symplectic integrators."
            ],
            "citingPaper": {
                "paperId": "082ebd0a989cce0457aa472c602c5ddb574915d3",
                "externalIds": {
                    "ArXiv": "2209.07646",
                    "DBLP": "journals/corr/abs-2209-07646",
                    "DOI": "10.1109/CDC51059.2022.9992571",
                    "CorpusId": 252355099
                },
                "corpusId": 252355099,
                "publicationVenue": {
                    "id": "ab066af1-bfee-42da-84bb-42f7e199d0d0",
                    "name": "IEEE Conference on Decision and Control",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Decision and Control",
                        "IEEE Conf Decis Control",
                        "Conf Decis Control",
                        "CDC"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=403"
                },
                "url": "https://www.semanticscholar.org/paper/082ebd0a989cce0457aa472c602c5ddb574915d3",
                "title": "Bayesian Identification of Nonseparable Hamiltonian Systems Using Stochastic Dynamic Models",
                "abstract": "This paper proposes a probabilistic Bayesian formulation for system identification (ID) and estimation of nonseparable Hamiltonian systems using stochastic dynamic models. Nonseparable Hamiltonian systems arise in models from diverse science and engineering applications such as astrophysics, robotics, vortex dynamics, charged particle dynamics, and quantum mechanics. The numerical experiments demonstrate that the proposed method recovers dynamical systems with higher accuracy and reduced predictive uncertainty compared to state-of-the-art approaches. The results further show that accurate predictions far outside the training time interval in the presence of sparse and noisy measurements are possible, which lends robustness and generalizability to the proposed approach. A quantitative benefit is prediction accuracy with less than 10% relative error for more than 12 times longer than a comparable least-squares-based method on a benchmark problem.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47557147",
                        "name": "Harsh Sharma"
                    },
                    {
                        "authorId": "2093676645",
                        "name": "Nicholas Galioto"
                    },
                    {
                        "authorId": "2439226",
                        "name": "A. Gorodetsky"
                    },
                    {
                        "authorId": "2053218326",
                        "name": "B. Kramer"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "From black-box architectures using simple multilayer perceptrons [15] to grey-box architectures that aim to preserve physical invariants [16], [17], these approaches vary by the level of inductive bias they introduce."
            ],
            "citingPaper": {
                "paperId": "8e500a8448018b5d06e519878f8f78a047da32db",
                "externalIds": {
                    "ArXiv": "2209.05712",
                    "DBLP": "conf/icra/AloraCSHP23",
                    "DOI": "10.1109/ICRA48891.2023.10160418",
                    "CorpusId": 252367304
                },
                "corpusId": 252367304,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8e500a8448018b5d06e519878f8f78a047da32db",
                "title": "Data-Driven Spectral Submanifold Reduction for Nonlinear Optimal Control of High-Dimensional Robots",
                "abstract": "Modeling and control of high-dimensional, nonlinear robotic systems remains a challenging task. While various model- and learning-based approaches have been proposed to address these challenges, they broadly lack generalizability to different control tasks and rarely preserve the structure of the dynamics. In this work, we propose a new, data-driven approach for extracting control-oriented, low-dimensional models from data using Spectral Submanifold Reduction (SSMR). In contrast to other data-driven methods which fit dynamical models to training trajectories, we identify the dynamics on generic, low-dimensional attractors embedded in the full phase space of the robotic system. This allows us to obtain computationally-tractable models for control which preserve the system's dominant dynamics and better track trajectories radically different from the training data. We demonstrate the superior performance and generalizability of SSMR in dynamic trajectory tracking tasks vis-\u00e1-vis the state of the art, including Koopman operator-based approaches.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8383806",
                        "name": "J. I. Alora"
                    },
                    {
                        "authorId": "101002420",
                        "name": "Mattia Cenedese"
                    },
                    {
                        "authorId": "1868195",
                        "name": "E. Schmerling"
                    },
                    {
                        "authorId": "6314087",
                        "name": "G. Haller"
                    },
                    {
                        "authorId": "1696085",
                        "name": "M. Pavone"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Structure-preserving algorithms have a rich history in geometric integration [30] and have recently come to the fold in data-driven problems [33, 19, 40, 28, 32]."
            ],
            "citingPaper": {
                "paperId": "ff4bdf1d22447d2f7c294517c0aecb6a579beefb",
                "externalIds": {
                    "ArXiv": "2209.02244",
                    "DBLP": "journals/corr/abs-2209-02244",
                    "DOI": "10.48550/arXiv.2209.02244",
                    "CorpusId": 252089488
                },
                "corpusId": 252089488,
                "publicationVenue": {
                    "id": "fb39a672-cb3a-430f-af86-316a481db9ed",
                    "name": "SIAM Journal on Numerical Analysis",
                    "type": "journal",
                    "alternate_names": [
                        "SIAM J Numer Anal"
                    ],
                    "issn": "0036-1429",
                    "url": "https://www.jstor.org/journal/siamjnumeanal",
                    "alternate_urls": [
                        "https://epubs.siam.org/journal/sjnaam",
                        "http://www.jstor.org/journals/00361429.html",
                        "http://www.siam.org/journals/sinum.php"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ff4bdf1d22447d2f7c294517c0aecb6a579beefb",
                "title": "The mpEDMD Algorithm for Data-Driven Computations of Measure-Preserving Dynamical Systems",
                "abstract": "Koopman operators globally linearize nonlinear dynamical systems and their spectral information is a powerful tool for the analysis and decomposition of nonlinear dynamical systems. However, Koopman operators are infinite-dimensional, and computing their spectral information is a considerable challenge. We introduce measure-preserving extended dynamic mode decomposition ($\\texttt{mpEDMD}$), the first truncation method whose eigendecomposition converges to the spectral quantities of Koopman operators for general measure-preserving dynamical systems. $\\texttt{mpEDMD}$ is a data-driven algorithm based on an orthogonal Procrustes problem that enforces measure-preserving truncations of Koopman operators using a general dictionary of observables. It is flexible and easy to use with any pre-existing DMD-type method, and with different types of data. We prove convergence of $\\texttt{mpEDMD}$ for projection-valued and scalar-valued spectral measures, spectra, and Koopman mode decompositions. For the case of delay embedding (Krylov subspaces), our results include the first convergence rates of the approximation of spectral measures as the size of the dictionary increases. We demonstrate $\\texttt{mpEDMD}$ on a range of challenging examples, its increased robustness to noise compared with other DMD-type methods, and its ability to capture the energy conservation and cascade of experimental measurements of a turbulent boundary layer flow with Reynolds number $>6\\times 10^4$ and state-space dimension $>10^5$.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51445731",
                        "name": "Matthew J. Colbrook"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Table 1 Difference between HNN [2] and SymODEN [3] and the proposed method.",
                "Table 1 shows the differences between the conventional method (HNN [2] and SymODEN [3]) and the proposed method.",
                "In such methods, some have been proposed to use energy conservation laws for model estimation: Greydanus et al. [2] proposed Hamiltonian Neural Networks (HNN), which estimates the Hamiltonian of a conservative system by utilizing the law of conservation of energy.",
                "Note that, HNN is not included in the comparison because it cannot handle inputs.",
                "[2] proposed Hamiltonian Neural Networks (HNN), which estimates the Hamiltonian of a conservative system by utilizing the law of conservation of energy."
            ],
            "citingPaper": {
                "paperId": "6bc00abab51ac8ef3b91752e6620b570697dd50d",
                "externalIds": {
                    "DBLP": "conf/sice/NakanoAAA22",
                    "DOI": "10.23919/SICE56594.2022.9905855",
                    "CorpusId": 252756714
                },
                "corpusId": 252756714,
                "publicationVenue": {
                    "id": "507291fc-56b6-404d-9bea-fad5c7d765fc",
                    "name": "Annual Conference of the Society of Instrument and Control Engineers of Japan",
                    "type": "conference",
                    "alternate_names": [
                        "Society of Instrument and Control Engineers of Japan",
                        "Annu Conf Soc Instrum Control Eng Jpn",
                        "SICE",
                        "Soc Instrum Control Eng Jpn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6bc00abab51ac8ef3b91752e6620b570697dd50d",
                "title": "Model Estimation Ensuring Passivity by Using Port-Hamiltonian Model and Deep Learning",
                "abstract": "An accurate model is necessary for highly accurate control, but it is not always easy to obtain the model via first principles. One of the methods for creating models is to represent the model by neural networks and train them in accordance with the data. However, the model created by machine learning cannot always satisfy the physical properties of the system. If some prior knowledge can be imposed on the estimation, it can be beneficial in the application of the obtained model and the reduction of the burden needed for the training. In this paper, we propose the new method to reflect the passivity of the system by using a port-Hamiltonian form. The effectiveness of the proposed method is shown via numerical examples.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2187167648",
                        "name": "Hiroyasu Nakano"
                    },
                    {
                        "authorId": "2926759",
                        "name": "Ryo Ariizumi"
                    },
                    {
                        "authorId": "2671046",
                        "name": "T. Asai"
                    },
                    {
                        "authorId": "2203949",
                        "name": "S. Azuma"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "For trajectory data, there exist many methods developed such as Hamiltonian neural networks [18], Hidden Markov Model (HMM) [19], Kalman Filter (KF) [20], Particle Filter (PF) [21] and related works [16, 22]."
            ],
            "citingPaper": {
                "paperId": "7b6b8ea7c1e5c3467992022a78ee7381862fb103",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-02628",
                    "ArXiv": "2209.02628",
                    "DOI": "10.48550/arXiv.2209.02628",
                    "CorpusId": 252089489
                },
                "corpusId": 252089489,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7b6b8ea7c1e5c3467992022a78ee7381862fb103",
                "title": "Weak Collocation Regression method: fast reveal hidden stochastic dynamics from high-dimensional aggregate data",
                "abstract": "Revealing hidden dynamics from the stochastic data is a challenging problem as randomness takes part in the evolution of the data. The problem becomes exceedingly complex when the trajectories of the stochastic data are absent in many scenarios. Here we present an approach to e\ufb00ectively modeling the dynamics of the stochastic data without trajectories based on the weak form of the Fokker-Planck (FP) equation, which governs the evolution of the density function in the Brownian process. Taking the collocations of Gaussian functions as the test functions in the weak form of the FP equation, we transfer the derivatives to the Gaussian functions and thus approximate the weak form by the expectational sum of the data. With a dictionary representation of the unknown terms, a linear system is built and then solved by the regression, revealing the unknown dynamics of the data. Hence, we name the method with the Weak Collocation Regression (WCR) method for its three key components: weak form, collocation of Gaussian kernels, and regression. The numerical experiments show that our method is \ufb02exible and fast, which reveals the dynamics within seconds in multi-dimensional problems and can be easily extended to high-dimensional data such as 20 dimensions. WCR can also correctly identify the hidden dynamics of the complex tasks with variable-dependent di\ufb00usion and coupled drift, and the performance is robust, achieving high accuracy in the case with noise added.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2184273996",
                        "name": "Liwei Lu"
                    },
                    {
                        "authorId": "49512135",
                        "name": "Zhijun Zeng"
                    },
                    {
                        "authorId": "2117864178",
                        "name": "Yan Jiang"
                    },
                    {
                        "authorId": "2117913533",
                        "name": "Yi Zhu"
                    },
                    {
                        "authorId": "153086928",
                        "name": "Pipi Hu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "It has also been shown that deep learning is also capable of approximating invariant quantities from dynamical systems such as the Hamiltonian [3] and the Lagrangian [4]."
            ],
            "citingPaper": {
                "paperId": "4cb7ffb5dfa4330c7fcc70afce6f8129c1699c42",
                "externalIds": {
                    "ArXiv": "2209.03248",
                    "DBLP": "journals/corr/abs-2209-03248",
                    "PubMedCentral": "10188520",
                    "DOI": "10.1038/s41598-023-34931-0",
                    "CorpusId": 252111158,
                    "PubMed": "37193704"
                },
                "corpusId": 252111158,
                "publicationVenue": {
                    "id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                    "name": "Scientific Reports",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Rep"
                    ],
                    "issn": "2045-2322",
                    "url": "http://www.nature.com/srep/",
                    "alternate_urls": [
                        "http://www.nature.com/srep/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4cb7ffb5dfa4330c7fcc70afce6f8129c1699c42",
                "title": "Sparse identification of Lagrangian for nonlinear dynamical systems via proximal gradient method",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2184247544",
                        "name": "Adam Purnomo"
                    },
                    {
                        "authorId": "3262458",
                        "name": "M. Hayashibe"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Much of this work has focused on specific formalisms of governing equations, such as Hamiltonians, which provide a consistent approach for many physical dynamical systems and the easy incorporation of priors to shrink the solution space (Greydanus et al., 2019; DiPietro et al., 2020; Chen et al., 2019).",
                "\u2026of this work has focused on specific formalisms of governing equations, such as Hamiltonians, which provide a consistent approach for many physical dynamical systems and the easy incorporation of priors to shrink the solution space (Greydanus et al., 2019; DiPietro et al., 2020; Chen et al., 2019).",
                "Greydanus et al. (2019) propose Hamiltonian Neural Networks, which parameterize Hamiltonians using black-box deep neural networks.",
                "The first approach emphasizes prediction, relying upon black-box machine learning techniques that can time-evolve the dynamical system with very high accuracy but offer no insight into its underlying governing equations (Greydanus et al., 2019; Chen et al., 2019; Raissi et al., 2020; Breen et al., 2020; Cranmer et al., 2020a).",
                "\u2026prediction, relying upon black-box machine learning techniques that can time-evolve the dynamical system with very high accuracy but offer no insight into its underlying governing equations (Greydanus et al., 2019; Chen et al., 2019; Raissi et al., 2020; Breen et al., 2020; Cranmer et al., 2020a)."
            ],
            "citingPaper": {
                "paperId": "46b9763e5d463bf8bd943309455fa6734f58b235",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-01521",
                    "ArXiv": "2209.01521",
                    "DOI": "10.48550/arXiv.2209.01521",
                    "CorpusId": 252090329
                },
                "corpusId": 252090329,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/46b9763e5d463bf8bd943309455fa6734f58b235",
                "title": "Symplectically Integrated Symbolic Regression of Hamiltonian Dynamical Systems",
                "abstract": "Here we present Symplectically Integrated Symbolic Regression (SISR), a novel technique for learning physical governing equations from data. SISR employs a deep symbolic regression approach, using a multi-layer LSTM-RNN with mutation to probabilistically sample Hamiltonian symbolic expressions. Using symplectic neural networks, we develop a model-agnostic approach for extracting meaningful physical priors from the data that can be imposed on-the-fly into the RNN output, limiting its search space. Hamiltonians generated by the RNN are optimized and assessed using a fourth-order symplectic integration scheme; prediction performance is used to train the LSTM-RNN to generate increasingly better functions via a risk-seeking policy gradients approach. Employing these techniques, we extract correct governing equations from oscillator, pendulum, two-body, and three-body gravitational systems with noisy and extremely small datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1396992658",
                        "name": "Daniel M. DiPietro"
                    },
                    {
                        "authorId": "2184192853",
                        "name": "Bo Zhu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "LGNN for n-pendulum and n-spring systems In order to evaluate the performance of LGNN, we first consider two standard systems, that have been widely studied in the literature, namely, n-pendulum and n-spring systems [4, 6, 8, 9], with n= (3,4,5).",
                "GNS uses the position and velocity of the particles to directly predict their updates for a future timestep [4]."
            ],
            "citingPaper": {
                "paperId": "b03df6544ff0eaf40b5ff8fc7434ce7c79cf367e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-01476",
                    "ArXiv": "2209.01476",
                    "DOI": "10.1088/2632-2153/acb03e",
                    "CorpusId": 252089116
                },
                "corpusId": 252089116,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b03df6544ff0eaf40b5ff8fc7434ce7c79cf367e",
                "title": "Learning the dynamics of particle-based systems with Lagrangian graph neural networks",
                "abstract": "Physical systems are commonly represented as a combination of particles, the individual dynamics of which govern the system dynamics. However, traditional approaches require the knowledge of several abstract quantities such as the energy or force to infer the dynamics of these particles. Here, we present a framework, namely, Lagrangian graph neural network (LGnn), that provides a strong inductive bias to learn the Lagrangian of a particle-based system directly from the trajectory. We test our approach on challenging systems with constraints and drag\u2014LGnn outperforms baselines such as feed-forward Lagrangian neural network (Lnn) with improved performance. We also show the zero-shot generalizability of the system by simulating systems two orders of magnitude larger than the trained one and also hybrid systems that are unseen by the model, a unique feature. The graph architecture of LGnn significantly simplifies the learning in comparison to Lnn with \u223c25 times better performance on \u223c20 times smaller amounts of data. Finally, we show the interpretability of LGnn, which directly provides physical insights on drag and constraint forces learned by the model. LGnn can thus provide a fillip toward understanding the dynamics of physical systems purely from observable quantities.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2131107456",
                        "name": "Ravinder Bhattoo"
                    },
                    {
                        "authorId": "1699732",
                        "name": "Sayan Ranu"
                    },
                    {
                        "authorId": "2144896197",
                        "name": "N. Krishnan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "cae2adb6642a38e2dd80ee975a65b26788a891c6",
                "externalIds": {
                    "ArXiv": "2209.00905",
                    "CorpusId": 257913495
                },
                "corpusId": 257913495,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cae2adb6642a38e2dd80ee975a65b26788a891c6",
                "title": "From latent dynamics to meaningful representations",
                "abstract": "While representation learning has been central to the rise of machine learning and artificial intelligence, a key problem remains in making the learnt representations meaningful. For this the typical approach is to regularize the learned representation through prior probability distributions. However such priors are usually unavailable or ad hoc. To deal with this, we propose a dynamics-constrained representation learning framework. Instead of using predefined probabilities, we restrict the latent representation to follow specific dynamics, which is a more natural constraint for representation learning in dynamical systems. Our belief stems from a fundamental observation in physics that though different systems can have different marginalized probability distributions, they typically obey the same dynamics, such as Newton's and Schrodinger's equations. We validate our framework for different systems including a real-world fluorescent DNA movie dataset. We show that our algorithm can uniquely identify an uncorrelated, isometric and meaningful latent representation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51224208",
                        "name": "Dedi Wang"
                    },
                    {
                        "authorId": "2108927025",
                        "name": "Yihang Wang"
                    },
                    {
                        "authorId": "2055729877",
                        "name": "Luke J. Evans"
                    },
                    {
                        "authorId": "46736401",
                        "name": "P. Tiwary"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "deep learning have seen neural network designs that can better model the physical world (Greydanus et al., 2019), measure uncertainties in their predictions (Louart & Couillet, 2018), and mitigate the risks attached with their tendency to be overconfident regarding a predicted confidence interval (Pereira & Thomas, 2020)."
            ],
            "citingPaper": {
                "paperId": "33c99524a9406843bef6debc122f204145f840e8",
                "externalIds": {
                    "ArXiv": "2209.00517",
                    "DBLP": "journals/corr/abs-2209-00517",
                    "DOI": "10.48550/arXiv.2209.00517",
                    "CorpusId": 251979520
                },
                "corpusId": 251979520,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/33c99524a9406843bef6debc122f204145f840e8",
                "title": "The Neural Process Family: Survey, Applications and Perspectives",
                "abstract": "The standard approaches to neural network implementation yield powerful function approximation capabilities but are limited in their abilities to learn meta representations and reason probabilistic uncertainties in their predictions. Gaussian processes, on the other hand, adopt the Bayesian learning scheme to estimate such uncertainties but are constrained by their efficiency and approximation capacity. The Neural Processes Family (NPF) intends to offer the best of both worlds by leveraging neural networks for meta-learning predictive uncertainties. Such potential has brought substantial research activity to the family in recent years. Therefore, a comprehensive survey of NPF models is needed to organize and relate their motivation, methodology, and experiments. This paper intends to address this gap while digging deeper into the formulation, research themes, and applications concerning the family members. We shed light on their potential to bring several recent advances in other deep learning domains under one umbrella. We then provide a rigorous taxonomy of the family and empirically demonstrate their capabilities for modeling data generating functions operating on 1-d, 2-d, and 3-d input domains. We conclude by discussing our perspectives on the promising directions that can fuel the research advances in the field. Code for our experiments will be made available at https://github.com/srvCodes/neural-processes-survey.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "41017210",
                        "name": "Saurav Jha"
                    },
                    {
                        "authorId": "2183355008",
                        "name": "Dong Gong"
                    },
                    {
                        "authorId": "2108028098",
                        "name": "Xuesong Wang"
                    },
                    {
                        "authorId": "145369890",
                        "name": "Richard E. Turner"
                    },
                    {
                        "authorId": "145095579",
                        "name": "L. Yao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e054283301505bd003b4ca34516368cf27955db9",
                "externalIds": {
                    "ArXiv": "2208.12104",
                    "PubMedCentral": "9639201",
                    "DBLP": "journals/corr/abs-2208-12104",
                    "DOI": "10.1021/acs.jpclett.2c02632",
                    "CorpusId": 251800121,
                    "PubMed": "36279418"
                },
                "corpusId": 251800121,
                "publicationVenue": {
                    "id": "45c7ef3c-e436-428b-a856-c8d5b31b603b",
                    "name": "Journal of Physical Chemistry Letters",
                    "type": "journal",
                    "alternate_names": [
                        "J Phys Chem Lett"
                    ],
                    "issn": "1948-7185",
                    "url": "https://pubs.acs.org/journal/jpclcd",
                    "alternate_urls": [
                        "https://pubs.acs.org/loi/jpclcd",
                        "http://pubs.acs.org/journals/jpclcd"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e054283301505bd003b4ca34516368cf27955db9",
                "title": "Algorithmic Differentiation for Automated Modeling of Machine Learned Force Fields",
                "abstract": "Reconstructing force fields (FFs) from atomistic simulation data is a challenge since accurate data can be highly expensive. Here, machine learning (ML) models can help to be data economic as they can be successfully constrained using the underlying symmetry and conservation laws of physics. However, so far, every descriptor newly proposed for an ML model has required a cumbersome and mathematically tedious remodeling. We therefore propose using modern techniques from algorithmic differentiation within the ML modeling process, effectively enabling the usage of novel descriptors or models fully automatically at an order of magnitude higher computational efficiency. This paradigmatic approach enables not only a versatile usage of novel representations and the efficient computation of larger systems\u2014all of high value to the FF community\u2014but also the simple inclusion of further physical knowledge, such as higher-order information (e.g., Hessians, more complex partial differential equations constraints etc.), even beyond the presented FF domain.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "96848301",
                        "name": "Niklas Schmitz"
                    },
                    {
                        "authorId": "2113612432",
                        "name": "Klaus-Robert M\u00fcller"
                    },
                    {
                        "authorId": "7631063",
                        "name": "Stefan Chmiela"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "For each case, we compared the performance of COMET with other methods: (1) simple neural ODE (NODE) [10], (2) Hamiltonian neural network (HNN) [6] with the coordinates given in each case below, (3) neural symplectic form (NSF) [7], and (4) Lagrangian neural network (LNN) [8].",
                "In contrast to Hamiltonian-based networks [6, 7, 8, 9], COMET is not constrained to Hamiltonian systems and its coordinate choice, making it generally applicable to a wider range of systems as shown in Table 1.",
                "NODE [10] HNN [6] NSF [7] LNN [8] COMET",
                "Case NODE [10] HNN [6] NSF [7] LNN [8] COMET",
                "With the recent emergence of employing neural networks for scientific discovery and learning systems\u2019 behaviour from observational data [3, 4, 5, 6], naturally it raises a question, \"can we find constants of motion of dynamical systems from their data and exploit them to make a better prediction?\"",
                "Hamiltonian neural network (HNN) [6] is an attempt to solve this conservation problem by learning the Hamiltonian and calculate the state dynamics from the Hamiltonian.",
                "A large body of literature has been moving into this direction by learning the Hamiltonian [6, 7] or its variations [8, 9] of a system."
            ],
            "citingPaper": {
                "paperId": "cb07726f2e6b9fb8ed68080f6972a72718d28b7d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-10387",
                    "ArXiv": "2208.10387",
                    "DOI": "10.48550/arXiv.2208.10387",
                    "CorpusId": 251719156
                },
                "corpusId": 251719156,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/cb07726f2e6b9fb8ed68080f6972a72718d28b7d",
                "title": "Constants of motion network",
                "abstract": "The beauty of physics is that there is usually a conserved quantity in an always-changing system, known as the constant of motion. Finding the constant of motion is important in understanding the dynamics of the system, but typically requires mathematical proficiency and manual analytical work. In this paper, we present a neural network that can simultaneously learn the dynamics of the system and the constants of motion from data. By exploiting the discovered constants of motion, it can produce better predictions on dynamics and can work on a wider range of systems than Hamiltonian-based neural networks. In addition, the training progresses of our method can be used as an indication of the number of constants of motion in a system which could be useful in studying a novel physical system.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49443038",
                        "name": "M. F. Kasim"
                    },
                    {
                        "authorId": "144833174",
                        "name": "Yi Heng Lim"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Some approaches incorporate the neural network with Hamiltonian mechanics [1], Lagrangian mechanics [2], and solid mechanics [3]."
            ],
            "citingPaper": {
                "paperId": "8ff0d109aef4a8ff744d2b9521609878d5190711",
                "externalIds": {
                    "DBLP": "conf/icit2/WangK22",
                    "DOI": "10.1109/ICIT48603.2022.10002761",
                    "CorpusId": 255478746
                },
                "corpusId": 255478746,
                "publicationVenue": {
                    "id": "6c1a00cb-4b59-44d6-a887-bfd7c5b3768f",
                    "name": "International Conference on Industrial Technology",
                    "type": "conference",
                    "alternate_names": [
                        "ICIT",
                        "Int Conf Ind Technol",
                        "IEEE Int Conf Integr Technol",
                        "IEEE International Conference on Integration Technology"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8ff0d109aef4a8ff744d2b9521609878d5190711",
                "title": "Neural Encoding of Mass Matrices of Articulated Rigid-body Systems in Cholesky-Decomposed Form",
                "abstract": "This paper provides guidelines for training a neural network (NN) to encode the mass matrices of articulated rigid-body systems in Cholesky-decomposed form. To store a mass matrix in Cholesky-decomposed form via an NN is expectedly computationally efficient for applications like control and simulation. It is known that training an NN to approximate the mass matrix can be affected by many factors, but most of the causalities have not been discussed yet, and the training takes a very long time. This paper discusses how different training strategies affect the training results. We also propose a norm-based cost function, and we investigate how the order of the norm of the cost function influences the training result. The results show that the NN is trained fast and becomes accurate in different scenarios. NNs achieved approximation error less than 1% during the testing stage in the 2-DOF system after 8 minutes of training; approximation error less than 1% during the testing stage of the 3-DOF system after 58 minutes of training; approximation error less than 2% in the 4-DOF systems after 11 hours and 3 minutes, and 14 hours and 32 minutes of training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2154767695",
                        "name": "Shih-Ming Wang"
                    },
                    {
                        "authorId": "47966131",
                        "name": "Ryo Kikuuwe"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "53e2d66345658fed6c1957e3e8d72bf69c784344",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-06120",
                    "ArXiv": "2208.06120",
                    "DOI": "10.48550/arXiv.2208.06120",
                    "CorpusId": 251554563
                },
                "corpusId": 251554563,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/53e2d66345658fed6c1957e3e8d72bf69c784344",
                "title": "Bayesian Inference with Latent Hamiltonian Neural Networks",
                "abstract": "When sampling for Bayesian inference, one popular approach is to use Hamiltonian Monte Carlo (HMC) and specifically the No-U-Turn Sampler (NUTS) which automatically decides the end time of the Hamiltonian trajectory. However, HMC and NUTS can require numerous numerical gradients of the target density, and can prove slow in practice. We propose Hamiltonian neural networks (HNNs) with HMC and NUTS for solving Bayesian inference problems. Once trained, HNNs do not require numerical gradients of the target density during sampling. Moreover, they satisfy important properties such as perfect time reversibility and Hamiltonian conservation, making them well-suited for use within HMC and NUTS because stationarity can be shown. We also propose an HNN extension called latent HNNs (L-HNNs), which are capable of predicting latent variable outputs. Compared to HNNs, L-HNNs offer improved expressivity and reduced integration errors. Finally, we employ L-HNNs in NUTS with an online error monitoring scheme to prevent sample degeneracy in regions of low probability density. We demonstrate L-HNNs in NUTS with online error monitoring on several examples involving complex, heavy-tailed, and high-local-curvature probability densities. Overall, L-HNNs in NUTS with online error monitoring satisfactorily inferred these probability densities. Compared to traditional NUTS, L-HNNs in NUTS with online error monitoring required 1--2 orders of magnitude fewer numerical gradients of the target density and improved the effective sample size (ESS) per gradient by an order of magnitude.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "101835579",
                        "name": "Somayajulu L. N. Dhulipala"
                    },
                    {
                        "authorId": "152579565",
                        "name": "Yifeng Che"
                    },
                    {
                        "authorId": "2270463",
                        "name": "M. Shields"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "962f706170e3cdb464a6e9b2a155d47d6abd4405",
                "externalIds": {
                    "PubMedCentral": "10502038",
                    "ArXiv": "2208.03680",
                    "DOI": "10.1038/s41598-023-42194-y",
                    "CorpusId": 261660743,
                    "PubMed": "37709820"
                },
                "corpusId": 261660743,
                "publicationVenue": {
                    "id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                    "name": "Scientific Reports",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Rep"
                    ],
                    "issn": "2045-2322",
                    "url": "http://www.nature.com/srep/",
                    "alternate_urls": [
                        "http://www.nature.com/srep/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/962f706170e3cdb464a6e9b2a155d47d6abd4405",
                "title": "On fast simulation of dynamical system with neural vector enhanced numerical solver",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109670338",
                        "name": "Zhongzhan Huang"
                    },
                    {
                        "authorId": "116746634",
                        "name": "Senwei Liang"
                    },
                    {
                        "authorId": "2238920724",
                        "name": "Hong Zhang"
                    },
                    {
                        "authorId": null,
                        "name": "Haizhao Yang"
                    },
                    {
                        "authorId": "2181395240",
                        "name": "Liang Lin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "If the dynamics are learned using Hamiltonian neural network [2], the dynamics will be equal to equation 3.",
                "A popular workaround is to learn the Hamiltonian [2, 6, 7] or Lagrangian [3] of the energy-conserving system with a neural network, then get the dynamics as the derivatives of the learned quantity.",
                "Similar to [2] and [13], we tested our model with pixel data.",
                "The states\u2019 dynamics of Hamiltonian systems can be written as [2, 13]",
                "Motivated by Hamiltonian mechanics, Hamiltonian Neural Network (HNN) was proposed in [2]."
            ],
            "citingPaper": {
                "paperId": "e142b0e670725afc66c6d4ce926b0738f1a8ad2b",
                "externalIds": {
                    "ArXiv": "2208.02632",
                    "DBLP": "journals/corr/abs-2208-02632",
                    "DOI": "10.48550/arXiv.2208.02632",
                    "CorpusId": 251320548
                },
                "corpusId": 251320548,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e142b0e670725afc66c6d4ce926b0738f1a8ad2b",
                "title": "Unifying physical systems' inductive biases in neural ODE using dynamics constraints",
                "abstract": "Conservation of energy is at the core of many physical phenomena and dynamical systems. There have been a significant number of works in the past few years aimed at predicting the trajectory of motion of dynamical systems using neural networks while adhering to the law of conservation of energy. Most of these works are inspired by classical mechanics such as Hamiltonian and Lagrangian mechanics as well as Neural Ordinary Differential Equations. While these works have been shown to work well in specific domains respectively, there is a lack of a unifying method that is more generally applicable without requiring significant changes to the neural network architectures. In this work, we aim to address this issue by providing a simple method that could be applied to not just energy-conserving systems, but also dissipative systems, by including a different inductive bias in different cases in the form of a regularisation term in the loss function. The proposed method does not require changing the neural network architecture and could form the basis to validate a novel idea, therefore showing promises to accelerate research in this direction.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144833174",
                        "name": "Yi Heng Lim"
                    },
                    {
                        "authorId": "49443038",
                        "name": "M. F. Kasim"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "8112e4b959e69f6224938de86d48a4957e01b49a",
                "externalIds": {
                    "DBLP": "journals/aei/NiLLL22",
                    "DOI": "10.1016/j.aei.2022.101661",
                    "CorpusId": 249859261
                },
                "corpusId": 249859261,
                "publicationVenue": {
                    "id": "ec497fa8-833a-4d68-873a-539c20989c22",
                    "name": "Advanced Engineering Informatics",
                    "type": "journal",
                    "alternate_names": [
                        "Adv Eng Informatics"
                    ],
                    "issn": "1474-0346",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622240/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/14740346"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8112e4b959e69f6224938de86d48a4957e01b49a",
                "title": "A mechanism informed neural network for predicting machining deformation of annular parts",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2072723917",
                        "name": "Yang Ni"
                    },
                    {
                        "authorId": "2201624163",
                        "name": "Yingguang Li"
                    },
                    {
                        "authorId": "47535323",
                        "name": "Changqing Liu"
                    },
                    {
                        "authorId": "40913460",
                        "name": "X. Liu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5c40b5b58eee489181f8c718b41a0a0d392e76c5",
                "externalIds": {
                    "DBLP": "journals/entropy/BrandaoCDEGHJNS22",
                    "PubMedCentral": "9407468",
                    "DOI": "10.3390/e24081096",
                    "CorpusId": 251504514,
                    "PubMed": "36010759"
                },
                "corpusId": 251504514,
                "publicationVenue": {
                    "id": "8270cfe1-3713-4325-a7bd-c6a87eed889e",
                    "name": "Entropy",
                    "type": "journal",
                    "issn": "1099-4300",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-155606",
                    "alternate_urls": [
                        "http://www.mdpi.com/journal/entropy/",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-155606",
                        "https://www.mdpi.com/journal/entropy"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5c40b5b58eee489181f8c718b41a0a0d392e76c5",
                "title": "Learning PDE to Model Self-Organization of Matter",
                "abstract": "A self-organization hydrodynamic process has recently been proposed to partially explain the formation of femtosecond laser-induced nanopatterns on Nickel, which have important applications in optics, microbiology, medicine, etc. Exploring laser pattern space is difficult, however, which simultaneously (i) motivates using machine learning (ML) to search for novel patterns and (ii) hinders it, because of the few data available from costly and time-consuming experiments. In this paper, we use ML to predict novel patterns by integrating partial physical knowledge in the form of the Swift-Hohenberg (SH) partial differential equation (PDE). To do so, we propose a framework to learn with few data, in the absence of initial conditions, by benefiting from background knowledge in the form of a PDE solver. We show that in the case of a self-organization process, a feature mapping exists in which initial conditions can safely be ignored and patterns can be described in terms of PDE parameters alone, which drastically simplifies the problem. In order to apply this framework, we develop a second-order pseudospectral solver of the SH equation which offers a good compromise between accuracy and speed. Our method allows us to predict new nanopatterns in good agreement with experimental data. Moreover, we show that pattern features are related, which imposes constraints on novel pattern design, and suggest an efficient procedure of acquiring experimental data iteratively to improve the generalization of the learned model. It also allows us to identify the limitations of the SH equation as a partial model and suggests an improvement to the physical model itself.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2181235426",
                        "name": "Eduardo Brandao"
                    },
                    {
                        "authorId": "115246217",
                        "name": "J. Colombier"
                    },
                    {
                        "authorId": "1762557",
                        "name": "S. Duffner"
                    },
                    {
                        "authorId": "2003050",
                        "name": "R. Emonet"
                    },
                    {
                        "authorId": "94098359",
                        "name": "F. Garrelie"
                    },
                    {
                        "authorId": "1749327",
                        "name": "Amaury Habrard"
                    },
                    {
                        "authorId": "1785777",
                        "name": "F. Jacquenet"
                    },
                    {
                        "authorId": "2086753852",
                        "name": "A. Nakhoul"
                    },
                    {
                        "authorId": "1738336",
                        "name": "M. Sebban"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "790a2c6d9e8a8831312e9bf24fdb73f800b30a47",
                "externalIds": {
                    "DBLP": "journals/ras/YangSS22",
                    "DOI": "10.1016/j.robot.2022.104258",
                    "CorpusId": 251969137
                },
                "corpusId": 251969137,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/790a2c6d9e8a8831312e9bf24fdb73f800b30a47",
                "title": "Learning differentiable dynamics models for shape control of deformable linear objects",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144145722",
                        "name": "Yuxuan Yang"
                    },
                    {
                        "authorId": "3128110",
                        "name": "J. A. Stork"
                    },
                    {
                        "authorId": "144504646",
                        "name": "Todor Stoyanov"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "introduced the so-called Hamiltonian Neural Networks for canonical, discrete systems, in which the loss term is of the form [72]"
            ],
            "citingPaper": {
                "paperId": "25080ff7886630ea8c5ac9cb35033be89acbeb38",
                "externalIds": {
                    "ArXiv": "2207.12749",
                    "DBLP": "journals/corr/abs-2207-12749",
                    "DOI": "10.48550/arXiv.2207.12749",
                    "CorpusId": 251067028
                },
                "corpusId": 251067028,
                "publicationVenue": {
                    "id": "f9c1272f-e8c2-4e8c-bdae-fc9c2bb2cb85",
                    "name": "Archives of Computational Methods in Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Arch Comput Method Eng"
                    ],
                    "issn": "1134-3060",
                    "url": "http://www.cimne.com/arcme/",
                    "alternate_urls": [
                        "http://www.springer.com/journal/11831",
                        "https://www.springer.com/journal/11831",
                        "https://link.springer.com/journal/11831"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/25080ff7886630ea8c5ac9cb35033be89acbeb38",
                "title": "Thermodynamics of learning physical phenomena",
                "abstract": "Thermodynamics could be seen as an expression of physics at a high epistemic level. As such, its potential as an inductive bias to help machine learning procedures attain accurate and credible predictions has been recently realized in many fields. We review how thermodynamics provides helpful insights in the learning process. At the same time, we study the influence of aspects such as the scale at which a given phenomenon is to be described, the choice of relevant variables for this description or the different techniques available for the learning process.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3246447",
                        "name": "E. Cueto"
                    },
                    {
                        "authorId": "2734584",
                        "name": "F. Chinesta"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "cc9825f744b08114a315ddebea3d99396490bded",
                "externalIds": {
                    "DOI": "10.1038/s42254-022-00497-5",
                    "CorpusId": 251084701
                },
                "corpusId": 251084701,
                "publicationVenue": {
                    "id": "3639d55b-36ef-4fa6-97fd-1bdc155f9081",
                    "name": "Nature Reviews Physics",
                    "alternate_names": [
                        "Nat Rev Phys"
                    ],
                    "issn": "2522-5820",
                    "url": "https://www.nature.com/natrevphys/"
                },
                "url": "https://www.semanticscholar.org/paper/cc9825f744b08114a315ddebea3d99396490bded",
                "title": "How machines could teach physicists new scientific concepts",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39958530",
                        "name": "I. Georgescu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "We can see from Figure 50 that Adam and the BrAVO algorithms can achieve good training and test losses on this system identification problem using the Hamiltonian-based neural ODE network from [27] (with 231,310 parameters), inspired by [40; 91]."
            ],
            "citingPaper": {
                "paperId": "1857ae8c4f9039fea56013b3e162b6eaf301b330",
                "externalIds": {
                    "ArXiv": "2207.11460",
                    "DOI": "10.1080/10556788.2023.2214837",
                    "CorpusId": 251040881
                },
                "corpusId": 251040881,
                "publicationVenue": {
                    "id": "cc88086b-4917-4bde-8ab6-5558ec0c8f44",
                    "name": "Optimization Methods and Software",
                    "type": "journal",
                    "alternate_names": [
                        "Optim Method  Softw",
                        "Optimization Methods & Software",
                        "Optim Method Softw"
                    ],
                    "issn": "1026-7670",
                    "url": "https://www.tandfonline.com/loi/goms20"
                },
                "url": "https://www.semanticscholar.org/paper/1857ae8c4f9039fea56013b3e162b6eaf301b330",
                "title": "Practical perspectives on symplectic accelerated optimization",
                "abstract": "Geometric numerical integration has recently been exploited to design symplectic accelerated optimization algorithms by simulating the Lagrangian and Hamiltonian systems from the variational framework introduced in Wibisono et al. In this paper, we discuss practical considerations which can significantly boost the computational performance of these optimization algorithms, and considerably simplify the tuning process. In particular, we investigate how momentum restarting schemes ameliorate computational efficiency and robustness by reducing the undesirable effect of oscillations, and ease the tuning process by making time-adaptivity superfluous. We also discuss how temporal looping helps avoiding instability issues caused by numerical precision, without harming the computational efficiency of the algorithms. Finally, we compare the efficiency and robustness of different geometric integration techniques, and study the effects of the different parameters in the algorithms to inform and simplify tuning in practice. From this paper emerge symplectic accelerated optimization algorithms whose computational efficiency, stability and robustness have been improved, and which are now much simpler to use and tune for practical applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2032462536",
                        "name": "Valentin Duruisseaux"
                    },
                    {
                        "authorId": "2087376",
                        "name": "M. Leok"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Our approach leverages known equations for computing a coarse-grid prior; which is complementary to using known equations as soft [109, 74, 142, 137, 144, 139] or hard constraints [50, 89, 8, 39, 7, 61] as these methods can still be used to constrain the learned parametrization."
            ],
            "citingPaper": {
                "paperId": "9072b3b1d6e34b5d0077d88e16e5b11ff14e9ed7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-11417",
                    "ArXiv": "2207.11417",
                    "DOI": "10.48550/arXiv.2207.11417",
                    "CorpusId": 251040875
                },
                "corpusId": 251040875,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9072b3b1d6e34b5d0077d88e16e5b11ff14e9ed7",
                "title": "Multiscale Neural Operator: Learning Fast and Grid-independent PDE Solvers",
                "abstract": "Numerical simulations in climate, chemistry, or astrophysics are computationally too expensive for uncertainty quantification or parameter-exploration at high-resolution. Reduced-order or surrogate models are multiple orders of magnitude faster, but traditional surrogates are inflexible or inaccurate and pure machine learning (ML)-based surrogates too data-hungry. We propose a hybrid, flexible surrogate model that exploits known physics for simulating large-scale dynamics and limits learning to the hard-to-model term, which is called parametrization or closure and captures the effect of fine- onto large-scale dynamics. Leveraging neural operators, we are the first to learn grid-independent, non-local, and flexible parametrizations. Our \\textit{multiscale neural operator} is motivated by a rich literature in multiscale modeling, has quasilinear runtime complexity, is more accurate or flexible than state-of-the-art parametrizations and demonstrated on the chaotic equation multiscale Lorenz96.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "81551971",
                        "name": "Bj\u00f6rn L\u00fctjens"
                    },
                    {
                        "authorId": "2055703171",
                        "name": "Catherine H. Crawford"
                    },
                    {
                        "authorId": "49054886",
                        "name": "C. Watson"
                    },
                    {
                        "authorId": "2150315516",
                        "name": "C. Hill"
                    },
                    {
                        "authorId": "2054846636",
                        "name": "Dava Newman"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "38a4d598484a7d0c44f72951af56055412c6eed8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-09344",
                    "ArXiv": "2207.09344",
                    "DOI": "10.48550/arXiv.2207.09344",
                    "CorpusId": 250644128
                },
                "corpusId": 250644128,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/38a4d598484a7d0c44f72951af56055412c6eed8",
                "title": "Online Dynamics Learning for Predictive Control with an Application to Aerial Robots",
                "abstract": "In this work, we consider the task of improving the accuracy of dynamic models for model predictive control (MPC) in an online setting. Although prediction models can be learned and applied to model-based controllers, these models are often learned offline. In this offline setting, training data is first collected and a prediction model is learned through an elaborated training procedure. However, since the model is learned offline, it does not adapt to disturbances or model errors observed during deployment. To improve the adaptiveness of the model and the controller, we propose an online dynamics learning framework that continually improves the accuracy of the dynamic model during deployment. We adopt knowledge-based neural ordinary differential equations (KNODE) as the dynamic models, and use techniques inspired by transfer learning to continually improve the model accuracy. We demonstrate the efficacy of our framework with a quadrotor, and verify the framework in both simulations and physical experiments. Results show that our approach can account for disturbances that are possibly time-varying, while maintaining good trajectory tracking performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1990956535",
                        "name": "Tom Z. Jiahao"
                    },
                    {
                        "authorId": "2044355187",
                        "name": "K. Y. Chee"
                    },
                    {
                        "authorId": "144062128",
                        "name": "M. A. Hsieh"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "6f82db7d18eb1a0373494d631842e6092fb0b2b8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-07920",
                    "ArXiv": "2207.07920",
                    "DOI": "10.1109/IROS47612.2022.9981303",
                    "CorpusId": 250626777
                },
                "corpusId": 250626777,
                "publicationVenue": {
                    "id": "37275deb-3fcf-4d16-ae77-95db9899b1f3",
                    "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "IROS",
                        "Intelligent Robots and Systems",
                        "Intell Robot Syst",
                        "IEEE/RJS Int Conf Intell Robot Syst"
                    ],
                    "url": "http://www.iros.org/"
                },
                "url": "https://www.semanticscholar.org/paper/6f82db7d18eb1a0373494d631842e6092fb0b2b8",
                "title": "Physics Embedded Neural Network Vehicle Model and Applications in Risk-Aware Autonomous Driving Using Latent Features",
                "abstract": "Non-holonomic vehicle motion has been studied extensively using physics-based models. Common approaches when using these models interpret the wheel/ground interactions using a linear tire model and thus may not fully capture the nonlinear and complex dynamics under various environments. On the other hand, neural network models have been widely employed in this domain, demonstrating powerful function approximation capabilities. However, these black-box learning strategies completely abandon the existing knowledge of well-known physics. In this paper, we seamlessly combine deep learning with a fully differentiable physics model to endow the neural network with available prior knowledge. The proposed model shows better generalization performance than the vanilla neural network model by a large margin. We also show that the latent features of our model can accurately represent lateral tire forces without the need for any additional training. Lastly, We develop a risk-aware model predictive controller using proprioceptive information derived from the latent features. We validate our idea in two autonomous driving tasks under unknown friction, outperforming the baseline control framework.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1675257694",
                        "name": "Taekyung Kim"
                    },
                    {
                        "authorId": "66406739",
                        "name": "Ho-Woon Lee"
                    },
                    {
                        "authorId": "2148959310",
                        "name": "Wonsuk Lee"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "42225a54689510b22edc93b8d57d9217cae582b8",
                "externalIds": {
                    "DBLP": "journals/jcphy/LiLTY23",
                    "ArXiv": "2207.06012",
                    "DOI": "10.1016/j.jcp.2023.111952",
                    "CorpusId": 250493183
                },
                "corpusId": 250493183,
                "publicationVenue": {
                    "id": "58606051-2e63-4034-8496-9d4ed773bb49",
                    "name": "Journal of Computational Physics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Phys"
                    ],
                    "issn": "0021-9991",
                    "url": "http://www.elsevier.com/locate/jcp",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00219991",
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622866/description#description",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/42225a54689510b22edc93b8d57d9217cae582b8",
                "title": "NySALT: Nystr\u00f6m-type inference-based schemes adaptive to large time-stepping",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145247019",
                        "name": "Xingjie Li"
                    },
                    {
                        "authorId": "40464284",
                        "name": "F. Lu"
                    },
                    {
                        "authorId": "46699279",
                        "name": "Molei Tao"
                    },
                    {
                        "authorId": "20380455",
                        "name": "F. Ye"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2020; 2021) or data-driven modeling of physical quantities (Greydanus et al., 2019; Cranmer et al., 2020; Lee et al., 2021).",
                "Followup work used those models and has shown success in diverse tasks, such as image classification (Zhuang et al., 2020; 2021) or data-driven modeling of physical quantities (Greydanus et al., 2019; Cranmer et al., 2020; Lee et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "1a237b2189bd79f7826b066df5428cf6d680261b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-06066",
                    "ArXiv": "2207.06066",
                    "DOI": "10.48550/arXiv.2207.06066",
                    "CorpusId": 250492925
                },
                "corpusId": 250492925,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1a237b2189bd79f7826b066df5428cf6d680261b",
                "title": "AdamNODEs: When Neural ODE Meets Adaptive Moment Estimation",
                "abstract": "Recent work by Xia et al. leveraged the continuous-limit of the classical momentum accelerated gradient descent and proposed heavy-ball neural ODEs. While this model offers computational efficiency and high utility over vanilla neural ODEs, this approach often causes the overshooting of internal dynamics, leading to unstable training of a model. Prior work addresses this issue by using ad-hoc approaches, e.g., bounding the internal dynamics using specific activation functions, but the resulting models do not satisfy the exact heavy-ball ODE. In this work, we propose adaptive momentum estimation neural ODEs (AdamNODEs) that adaptively control the acceleration of the classical momentum-based approach. We find that its adjoint states also satisfy AdamODE and do not require ad-hoc solutions that the prior work employs. In evaluation, we show that AdamNODEs achieve the lowest training loss and efficacy over existing neural ODEs. We also show that AdamNODEs have better training stability than classical momentum-based neural ODEs. This result sheds some light on adapting the techniques proposed in the optimization community to improving the training and inference of neural ODEs further. Our code is available at https://github.com/pmcsh04/AdamNODE.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2176079830",
                        "name": "Suneghyeon Cho"
                    },
                    {
                        "authorId": "2111053680",
                        "name": "Sanghyun Hong"
                    },
                    {
                        "authorId": "3041967",
                        "name": "Kookjin Lee"
                    },
                    {
                        "authorId": "5166698",
                        "name": "Noseong Park"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "0ef49ef3144efcbaed5f963d3091500f1d4fc175",
                "externalIds": {
                    "DOI": "10.3390/app12147014",
                    "CorpusId": 250513252
                },
                "corpusId": 250513252,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0ef49ef3144efcbaed5f963d3091500f1d4fc175",
                "title": "A Deformation Force Monitoring Method for Aero-Engine Casing Machining Based on Deep Autoregressive Network and Kalman Filter",
                "abstract": "Aero-engine casing is a kind of thin-walled rotary part for which serious deformation often occurs during its machining process. As deformation force is an important physical quantity associated with deformation, the utilization of deformation force to control the deformation has been suggested. However, due to the complex machining characteristics of an aero-engine casing, obtaining a stable and reliable deformation force can be quite difficult. To address this issue, this paper proposes a deformation force monitoring method via a pre-support force probabilistic decision model based on deep autoregressive neural network and Kalman filter, for which a set of sophisticated clamping devices with force sensors are specifically developed. In the proposed method, the pre-support force is determined by the predicted value of the deformation force and the equivalent flexibility of the part, while the measurement errors and the reality gaps are reduced by Kalman filter via fusing the predicted and measured data. Both computer simulation and physical machining experiments are carried out and their results give a positive confirmation on the effectiveness of the proposed method. The results are as follows. In the simulation experiments, when the confidence is 84.1%, the success rate of deformation force monitoring is increased by about 30% compared with the traditional approach, and the final impact of clamping deformation of the proposed method is less than 0.003 mm. In the real machining experiments, the results show that the calculation error of deformation by the proposed method based on monitoring the deformation force is less than 0.008 mm.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "10439487",
                        "name": "Haonan Guo"
                    },
                    {
                        "authorId": "2201624163",
                        "name": "Yingguang Li"
                    },
                    {
                        "authorId": "47535323",
                        "name": "Changqing Liu"
                    },
                    {
                        "authorId": "2072724421",
                        "name": "Yang Ni"
                    },
                    {
                        "authorId": "2055743278",
                        "name": "K. Tang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Using neural networks to learn Hamiltonian dynamics is an earlier idea that was proposed in Greydanus et al. (2019).",
                "To enforce invertibility, we express the flow f\u03b8 as a conservative operator using Hamiltonian neural networks inspired from Greydanus et al. (2019)."
            ],
            "citingPaper": {
                "paperId": "558e25f70d3cfc1bf8af0fd8c6358e381605c1f6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-03843",
                    "ArXiv": "2207.03843",
                    "DOI": "10.48550/arXiv.2207.03843",
                    "CorpusId": 250408020
                },
                "corpusId": 250408020,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/558e25f70d3cfc1bf8af0fd8c6358e381605c1f6",
                "title": "Continuous Methods : Hamiltonian Domain Translation",
                "abstract": "This paper proposes a novel approach to domain translation. Leveraging established parallels between generative models and dynamical systems, we propose a reformulation of the Cycle-GAN architecture. By embedding our model with a Hamiltonian structure, we obtain a continuous, expressive and most importantly invertible generative model for domain translation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155649051",
                        "name": "Emmanuel Menier"
                    },
                    {
                        "authorId": "89936237",
                        "name": "M. Bucci"
                    },
                    {
                        "authorId": "26631868",
                        "name": "Mouadh Yagoubi"
                    },
                    {
                        "authorId": "3083383",
                        "name": "L. Mathelin"
                    },
                    {
                        "authorId": "2066691430",
                        "name": "M. Schoenauer"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Many ideas were explored, such as imposing soft physical constraints [43, 48, 34, 28, 56] in the training loss function or hard constraints in the network architectures [6, 12, 27, 16, 36, 41]."
            ],
            "citingPaper": {
                "paperId": "710fc9418b14803aea4e8ccf3fbe703a525a5ce3",
                "externalIds": {
                    "ArXiv": "2207.03790",
                    "DBLP": "conf/eccv/GuenRT22",
                    "DOI": "10.48550/arXiv.2207.03790",
                    "CorpusId": 250408080
                },
                "corpusId": 250408080,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/710fc9418b14803aea4e8ccf3fbe703a525a5ce3",
                "title": "Complementing Brightness Constancy with Deep Networks for Optical Flow Prediction",
                "abstract": ". State-of-the-art methods for optical flow estimation rely on deep learning, which require complex sequential training schemes to reach optimal performances on real-world data. In this work, we introduce the COMBO deep network that explicitly exploits the brightness constancy (BC) model used in traditional methods. Since BC is an ap-proximate physical model violated in several situations, we propose to train a physically-constrained network complemented with a data-driven network. We introduce a unique and meaningful flow decomposition between the physical prior and the data-driven complement, including an uncertainty quantification of the BC model. We derive a joint training scheme for learning the different components of the decomposition ensur-ing an optimal cooperation, in a supervised but also in a semi-supervised context. Experiments show that COMBO can improve performances over state-of-the-art supervised networks, e.g. RAFT, reaching state-of-the-art results on several benchmarks. We highlight how COMBO can leverage the BC model and adapt to its limitations. Finally, we show that our semi-supervised method can significantly simplify the training procedure.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3965182",
                        "name": "V. Guen"
                    },
                    {
                        "authorId": "2034274729",
                        "name": "Cl\u00e9ment Rambour"
                    },
                    {
                        "authorId": "1728523",
                        "name": "Nicolas Thome"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Some works aimed at directly learning the system\u2019s underlying Hamiltonian (Chen et al., 2020; Greydanus et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "1069bb04b5b1a81fef2b435dab135455a8814e13",
                "externalIds": {
                    "ArXiv": "2207.02542",
                    "DBLP": "conf/icml/BrennerHMBMKD22",
                    "DOI": "10.48550/arXiv.2207.02542",
                    "CorpusId": 250311297
                },
                "corpusId": 250311297,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1069bb04b5b1a81fef2b435dab135455a8814e13",
                "title": "Tractable Dendritic RNNs for Reconstructing Nonlinear Dynamical Systems",
                "abstract": "In many scienti\ufb01c disciplines, we are interested in inferring the nonlinear dynamical system underlying a set of observed time series, a challenging task in the face of chaotic behavior and noise. Pre-vious deep learning approaches toward this goal often suffered from a lack of interpretability and tractability. In particular, the high-dimensional latent spaces often required for a faithful embedding, even when the underlying dynamics lives on a lower-dimensional manifold, can hamper theoretical analysis. Motivated by the emerg-ing principles of dendritic computation, we aug-ment a dynamically interpretable and mathematically tractable piecewise-linear (PL) recurrent neural network (RNN) by a linear spline basis expansion. We show that this approach retains all the theoretically appealing properties of the simple PLRNN, yet boosts its capacity for approximating arbitrary nonlinear dynamical systems in comparatively low dimensions. We employ two frameworks for training the system, one combining back-propagation-through-time (BPTT) with teacher forcing, and another based on fast and scalable variational inference. We show that the dendritically expanded PLRNN achieves better reconstructions with fewer parameters and dimensions on various dynamical systems benchmarks and compares favorably to other methods, while retaining a tractable and interpretable structure.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "105021256",
                        "name": "Manuela Brenner"
                    },
                    {
                        "authorId": "145060037",
                        "name": "F. Hess"
                    },
                    {
                        "authorId": "2132538877",
                        "name": "Jonas M. Mikhaeil"
                    },
                    {
                        "authorId": "87881370",
                        "name": "Leonard Bereska"
                    },
                    {
                        "authorId": "104047138",
                        "name": "Z. Monfared"
                    },
                    {
                        "authorId": "2804346",
                        "name": "Po-Chen Kuo"
                    },
                    {
                        "authorId": "2361055",
                        "name": "D. Durstewitz"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "48b3bcffdbab96f6a5af68cffea0beaac6d9f6bf",
                "externalIds": {
                    "ArXiv": "2207.00687",
                    "DOI": "10.1088/2632-2153/ace8f0",
                    "CorpusId": 250264676
                },
                "corpusId": 250264676,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/48b3bcffdbab96f6a5af68cffea0beaac6d9f6bf",
                "title": "Machine-learning Kohn\u2013Sham potential from dynamics in time-dependent Kohn\u2013Sham systems",
                "abstract": "The construction of a better exchange-correlation potential in time-dependent density functional theory (TDDFT) can improve the accuracy of TDDFT calculations and provide more accurate predictions of the properties of many-electron systems. Here, we propose a machine learning method to develop the energy functional and the Kohn\u2013Sham potential of a time-dependent Kohn\u2013Sham (TDKS) system is proposed. The method is based on the dynamics of the Kohn\u2013Sham system and does not require any data on the exact Kohn\u2013Sham potential for training the model. We demonstrate the results of our method with a 1D harmonic oscillator example and a 1D two-electron example. We show that the machine-learned Kohn\u2013Sham potential matches the exact Kohn\u2013Sham potential in the absence of memory effect. Our method can still capture the dynamics of the Kohn\u2013Sham system in the presence of memory effects. The machine learning method developed in this article provides insight into making better approximations of the energy functional and the Kohn\u2013Sham potential in the TDKS system.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146158226",
                        "name": "Jun Yang"
                    },
                    {
                        "authorId": "2253922",
                        "name": "J. Whitfield"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "f51873e0fe8affb3e33af0c8a2ca9594a3b393a1",
                "externalIds": {
                    "DOI": "10.1038/s43588-022-00281-6",
                    "CorpusId": 251087119
                },
                "corpusId": 251087119,
                "publicationVenue": {
                    "id": "4cbc2987-6254-4668-b85b-aeabd7ff62ef",
                    "name": "Nature Computational Science",
                    "type": "journal",
                    "alternate_names": [
                        "Nat Comput Sci"
                    ],
                    "issn": "2662-8457"
                },
                "url": "https://www.semanticscholar.org/paper/f51873e0fe8affb3e33af0c8a2ca9594a3b393a1",
                "title": "Automated discovery of fundamental variables hidden in experimental data",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8786274",
                        "name": "Boyuan Chen"
                    },
                    {
                        "authorId": "2000347101",
                        "name": "Kuang Huang"
                    },
                    {
                        "authorId": "1452353235",
                        "name": "Sunand Raghupathi"
                    },
                    {
                        "authorId": "2146376367",
                        "name": "I. Chandratreya"
                    },
                    {
                        "authorId": "2072520338",
                        "name": "Qi Du"
                    },
                    {
                        "authorId": "51022452",
                        "name": "H. Lipson"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "35a00c7d62120292a7bc58874f266a423171a86e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-12209",
                    "ArXiv": "2207.12209",
                    "DOI": "10.48550/arXiv.2207.12209",
                    "CorpusId": 251040981
                },
                "corpusId": 251040981,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/35a00c7d62120292a7bc58874f266a423171a86e",
                "title": "Lagrangian Density Space-Time Deep Neural Network Topology",
                "abstract": "As a network-based functional approximator, we have proposed a\"Lagrangian Density Space-Time Deep Neural Networks\"(LDDNN) topology. It is qualified for unsupervised training and learning to predict the dynamics of underlying physical science governed phenomena. The prototypical network respects the fundamental conservation laws of nature through the succinctly described Lagrangian and Hamiltonian density of the system by a given data-set of generalized nonlinear partial differential equations. The objective is to parameterize the Lagrangian density over a neural network and directly learn from it through data instead of hand-crafting an exact time-dependent\"Action solution\"of Lagrangian density for the physical system. With this novel approach, can understand and open up the information inference aspect of the\"Black-box deep machine learning representation\"for the physical dynamics of nature by constructing custom-tailored network interconnect topologies, activation, and loss/cost functions based on the underlying physical differential operators. This article will discuss statistical physics interpretation of neural networks in the Lagrangian and Hamiltonian domains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3475948",
                        "name": "B. Bishnoi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Systems identification of frictionless dynamical systems such as pendulums and molecular dynamics through deep learning has seen a surge in recent years, particularly Hamiltonian Neural Networks (Greydanus, Dzamba, and Yosinski 2019).",
                "We test the performance on the two classical systems, undamped spring-mass and pendulum (Greydanus, Dzamba, and Yosinski 2019)."
            ],
            "citingPaper": {
                "paperId": "d51d703711621e3ff03d4566589fdedd1d133347",
                "externalIds": {
                    "DBLP": "conf/aaai/Mathiesen0H22",
                    "DOI": "10.1609/aaai.v36i4.20381",
                    "CorpusId": 247847114
                },
                "corpusId": 247847114,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d51d703711621e3ff03d4566589fdedd1d133347",
                "title": "Hyperverlet: A Symplectic Hypersolver for Hamiltonian Systems",
                "abstract": "Hamiltonian systems represent an important class of dynamical systems such as pendulums, molecular dynamics, and cosmic systems. The choice of solvers is significant to the accuracy when simulating Hamiltonian systems, where symplectic solvers show great significance. Recent advances in neural network-based hypersolvers, though achieve competitive results, still lack the symplecity necessary for reliable simulations, especially over long time horizons. To alleviate this, we introduce Hyperverlet, a new hypersolver composing the traditional, symplectic velocity Verlet and symplectic neural network-based solvers. More specifically, we propose a parameterization of symplectic neural networks and prove that hyperbolic tangent is r-finite expanding the set of allowable activation functions for symplectic neural networks, improving the accuracy. Extensive experiments on a spring-mass and a pendulum system justify the design choices and suggest that Hyperverlet outperforms both traditional solvers and hypersolvers.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2007535300",
                        "name": "Frederik Baymler Mathiesen"
                    },
                    {
                        "authorId": "16234870",
                        "name": "Bin-Yan Yang"
                    },
                    {
                        "authorId": "8520788",
                        "name": "Jilin Hu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "333d60524f15d3b5793fe1ab414ad7041e110652",
                "externalIds": {
                    "DOI": "10.1155/2022/5010251",
                    "CorpusId": 250132334
                },
                "corpusId": 250132334,
                "publicationVenue": {
                    "id": "501c1070-b5d2-4ff0-ad6f-8769a0a1e13f",
                    "name": "Wireless Communications and Mobile Computing",
                    "type": "journal",
                    "alternate_names": [
                        "Wirel Commun Mob Comput"
                    ],
                    "issn": "1530-8669",
                    "url": "https://www.hindawi.com/journals/wcmc/",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/15308677",
                        "http://www.interscience.wiley.com/jpages/1530-8669/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/333d60524f15d3b5793fe1ab414ad7041e110652",
                "title": "A Convex Relaxation Approach for Learning the Robust Koopman Operator",
                "abstract": "Although data-driven models, especially deep learning, have achieved astonishing results on many prediction tasks for nonlinear sequences, challenges still remain in finding an appropriate way to embed prior knowledge of physical dynamics in these models. In this work, we introduce a convex relaxation approach for learning robust Koopman operators of nonlinear dynamical systems, which are intended to construct approximate space spanned by eigenfunctions of the Koopman operator. Different from the classical dynamic mode decomposition, we use the layer weights of neural networks as eigenfunctions of the Koopman operator, providing intrinsic coordinates that globally linearize the dynamics. We find that the approximation of space can be regarded as an orthogonal Procrustes problem on the Stiefel manifold, which is highly sensitive to noise. The key contribution of this paper is to demonstrate that strict orthogonal constraint can be replaced by its convex relaxation, and the performance of the model can be improved without increasing the complexity when dealing with both clean and noisy data. After that, the overall model can be optimized via backpropagation in an end-to-end manner. The comparisons of the proposed method against several state-of-the-art competitors are shown on nonlinear oscillators and the lid-driven cavity flow.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2218132144",
                        "name": "Yong Su"
                    },
                    {
                        "authorId": "2115758438",
                        "name": "Jiawei Jin"
                    },
                    {
                        "authorId": "2067862129",
                        "name": "Weilong Peng"
                    },
                    {
                        "authorId": "2055743227",
                        "name": "Keke Tang"
                    },
                    {
                        "authorId": "2108486669",
                        "name": "Asad Khan"
                    },
                    {
                        "authorId": "9766344",
                        "name": "Simin An"
                    },
                    {
                        "authorId": "1710882345",
                        "name": "Meng Xing"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "We include the two most representative methods for comparison, HNN and LNN (Greydanus et al., 2019; Cranmer et al., 2020).",
                "To reduce the difficulty in training, researchers started to design physics-informed neural networks such as DeLaN, HNN, and LNN (Lutter et al., 2018; Greydanus et al., 2019; Cranmer et al., 2020).",
                "The baseline model is a three-layer MLP directly modelling the evolution function f in Equation (1) following HNN and LNN (Greydanus et al., 2019; Cranmer et al., 2020).",
                "The fusion of physical knowledge and neural networks casts a light on this problem, which leads to physics-informed neural networks such as Hamiltonian Neural Networks (HNNs, Greydanus et al., 2019) or Lagrangian Neural Networks (LNNs, Cranmer et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "92c103f29680a312b3dd78ede234221817038cec",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-12325",
                    "ArXiv": "2206.12325",
                    "DOI": "10.48550/arXiv.2206.12325",
                    "CorpusId": 250048528
                },
                "corpusId": 250048528,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/92c103f29680a312b3dd78ede234221817038cec",
                "title": "ModLaNets: Learning Generalisable Dynamics via Modularity and Physical Inductive Bias",
                "abstract": "Deep learning models are able to approximate one specific dynamical system but struggle at learning generalisable dynamics, where dynamical systems obey the same laws of physics but contain different numbers of elements (e.g., double- and triple-pendulum systems). To relieve this issue, we proposed the Modular Lagrangian Network (ModLaNet), a structural neural network framework with modularity and physical inductive bias. This framework models the energy of each element using modularity and then construct the target dynamical system via Lagrangian mechanics. Modularity is beneficial for reusing trained networks and reducing the scale of networks and datasets. As a result, our framework can learn from the dynamics of simpler systems and extend to more complex ones, which is not feasible using other relevant physics-informed neural networks. We examine our framework for modelling double-pendulum or three-body systems with small training datasets, where our models achieve the best data efficiency and accuracy performance compared with counterparts. We also reorganise our models as extensions to model multi-pendulum and multi-body systems, demonstrating the intriguing reusable feature of our framework.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1897570842",
                        "name": "Yupu Lu"
                    },
                    {
                        "authorId": "2108835388",
                        "name": "Shi-Min Lin"
                    },
                    {
                        "authorId": "1390855802",
                        "name": "Guanqi Chen"
                    },
                    {
                        "authorId": "1943594",
                        "name": "Jia-Yu Pan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": true,
            "contexts": [
                "The majority of related work [1, 8, 9, 10, 11] use a variational autoencoder (VAE) framework to represent the state in a latent space embedding.",
                "The Hamiltonian expresses the total energy of the system H(q,p) = T (q,p) + V (q) [1, 8].",
                "[1] introduced Hamiltonian neural networks.",
                "References [1] Samuel Greydanus, Misko Dzamba, and Jason Yosinski.",
                "This is similar to other reported results in literature [1, 9, 8, 10]."
            ],
            "citingPaper": {
                "paperId": "7972612f6675bc44a52f8ec9211275b2cead9948",
                "externalIds": {
                    "ArXiv": "2206.11030",
                    "DBLP": "journals/corr/abs-2206-11030",
                    "DOI": "10.48550/arXiv.2206.11030",
                    "CorpusId": 249926374
                },
                "corpusId": 249926374,
                "publicationVenue": {
                    "id": "75d7a8c1-d871-42db-a8e4-7cf5146fdb62",
                    "name": "Social Science Research Network",
                    "type": "journal",
                    "alternate_names": [
                        "SSRN, Social Science Research Network (SSRN) home page",
                        "SSRN Electronic Journal",
                        "Soc Sci Res Netw",
                        "SSRN",
                        "SSRN Home Page",
                        "SSRN Electron J",
                        "Social Science Electronic Publishing presents Social Science Research Network"
                    ],
                    "issn": "1556-5068",
                    "url": "http://www.ssrn.com/",
                    "alternate_urls": [
                        "www.ssrn.com/",
                        "https://fatcat.wiki/container/tol7woxlqjeg5bmzadeg6qrg3e",
                        "https://www.wikidata.org/wiki/Q53949192",
                        "www.ssrn.com/en",
                        "http://www.ssrn.com/en/",
                        "http://umlib.nl/ssrn",
                        "umlib.nl/ssrn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7972612f6675bc44a52f8ec9211275b2cead9948",
                "title": "KeyCLD: Learning Constrained Lagrangian Dynamics in Keypoint Coordinates from Images",
                "abstract": "We present KeyCLD, a framework to learn Lagrangian dynamics from images. Learned keypoints represent semantic landmarks in images and can directly represent state dynamics. Interpreting this state as Cartesian coordinates coupled with explicit holonomic constraints, allows expressing the dynamics with a constrained Lagrangian. Our method explicitly models kinetic and potential energy, thus allowing energy based control. We are the first to demonstrate learning of Lagrangian dynamics from images on the dm_control pendulum, cartpole and acrobot environments. This is a step forward towards learning Lagrangian dynamics from real-world images, since previous work in literature was only applied to minimalistic images with monochromatic shapes on empty backgrounds. Please refer to our project page for code and additional results: https://rdaems.github.io/keycld/",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2172201770",
                        "name": "Rembert Daems"
                    },
                    {
                        "authorId": "2172162987",
                        "name": "Jeroen Taets"
                    },
                    {
                        "authorId": "8508453",
                        "name": "F. Wyffels"
                    },
                    {
                        "authorId": "3318300",
                        "name": "G. Crevecoeur"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e5aeec169d2c4d5e8e730a3c382006ba983b8709",
                "externalIds": {
                    "PubMedCentral": "9207538",
                    "DOI": "10.1098/rsta.2021.0213",
                    "CorpusId": 249853630,
                    "PubMed": "35719077"
                },
                "corpusId": 249853630,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e5aeec169d2c4d5e8e730a3c382006ba983b8709",
                "title": "Data-driven prediction in dynamical systems: recent developments",
                "abstract": "In recent years, we have witnessed a significant shift toward ever-more complex and ever-larger-scale systems in the majority of the grand societal challenges tackled in applied sciences. The need to comprehend and predict the dynamics of complex systems have spurred developments in large-scale simulations and a multitude of methods across several disciplines. The goals of understanding and prediction in complex dynamical systems, however, have been hindered by high dimensionality, complexity and chaotic behaviours. Recent advances in data-driven techniques and machine-learning approaches have revolutionized how we model and analyse complex systems. The integration of these techniques with dynamical systems theory opens up opportunities to tackle previously unattainable challenges in modelling and prediction of dynamical systems. While data-driven prediction methods have made great strides in recent years, it is still necessary to develop new techniques to improve their applicability to a wider range of complex systems in science and engineering. This focus issue shares recent developments in the field of complex dynamical systems with emphasis on data-driven, data-assisted and artificial intelligence-based discovery of dynamical systems. This article is part of the theme issue 'Data-driven prediction in dynamical systems'.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50816543",
                        "name": "Amin Ghadami"
                    },
                    {
                        "authorId": "3167255",
                        "name": "B. Epureanu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Learning Hamiltonian Systems Hamiltonian system is an important category in ordinary differential equations and there have been satisfactory works on learning Hamiltonian systems (Bertalan et al., 2019; Chen & Tao, 2021; Greydanus et al., 2019; Jin et al., 2020).",
                "In order to substantiate this claim, we use Neural ODE to learn several benchmark problems that are widely investigated for the discovery of hidden dynamics (Du et al., 2021; Greydanus et al., 2019; Keller & Du, 2021; Yu et al., 2021).",
                "Greydanus et al. (2019) observed drifting of the predicted trajectory when learning a Hamiltonian system using Neural ODE.",
                "Since both true f and the IMDE fh are inaccessible in practice, we consider several benchmark problems that are widely investigated for the discovery of hidden dynamics (Du et al., 2021; Greydanus et al., 2019; Keller & Du, 2021; Yu et al., 2021).",
                "Experimental Details Since both true f and the IMDE fh are inaccessible in practice, we consider several benchmark problems that are widely investigated for the discovery of hidden dynamics (Du et al., 2021; Greydanus et al., 2019; Keller & Du, 2021; Yu et al., 2021).",
                "Hamiltonian system is an important category in ordinary differential equations and there have been satisfactory works on learning Hamiltonian systems (Bertalan et al., 2019; Chen & Tao, 2021; Greydanus et al., 2019; Jin et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "d51dfe0ba4f502e4ed36a92c8adb1234e2140c0f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-07335",
                    "ArXiv": "2206.07335",
                    "DOI": "10.48550/arXiv.2206.07335",
                    "CorpusId": 249674482
                },
                "corpusId": 249674482,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d51dfe0ba4f502e4ed36a92c8adb1234e2140c0f",
                "title": "On Numerical Integration in Neural Ordinary Differential Equations",
                "abstract": "The combination of ordinary differential equations and neural networks, i.e., neural ordinary differential equations (Neural ODE), has been widely studied from various angles. However, deciphering the numerical integration in Neural ODE is still an open challenge, as many researches demonstrated that numerical integration significantly affects the performance of the model. In this paper, we propose the inverse modified differential equations (IMDE) to clarify the influence of numerical integration on training Neural ODE models. IMDE is determined by the learning task and the employed ODE solver. It is shown that training a Neural ODE model actually returns a close approximation of the IMDE, rather than the true ODE. With the help of IMDE, we deduce that (i) the discrepancy between the learned model and the true ODE is bounded by the sum of discretization error and learning loss; (ii) Neural ODE using non-symplectic numerical integration fail to learn conservation laws theoretically. Several experiments are performed to numerically verify our theoretical analysis.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2430811",
                        "name": "Aiqing Zhu"
                    },
                    {
                        "authorId": "121687074",
                        "name": "Pengzhan Jin"
                    },
                    {
                        "authorId": "145801437",
                        "name": "Beibei Zhu"
                    },
                    {
                        "authorId": "1805110",
                        "name": "Yifa Tang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "We generated the non-chaotic two-body systems and chaotic three-body systems in a relatively stable near-circular way [52], where the trajectories are obtained with the Explicit Runge-Kutta method [53]."
            ],
            "citingPaper": {
                "paperId": "323c220fb19295e0ada03d7c0f7975e5c97db749",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-06131",
                    "ArXiv": "2206.06131",
                    "DOI": "10.1101/2022.06.10.495595",
                    "CorpusId": 249625897,
                    "PubMed": "37309509"
                },
                "corpusId": 249625897,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/323c220fb19295e0ada03d7c0f7975e5c97db749",
                "title": "Seeing the forest and the tree: Building representations of both individual and collective dynamics with transformers",
                "abstract": "Complex time-varying systems are often studied by abstracting away from the dynamics of individual components to build a model of the population-level dynamics from the start. However, when building a population-level description, it can be easy to lose sight of each individual and how they contribute to the larger picture. In this paper, we present a novel transformer architecture for learning from time-varying data that builds descriptions of both the individual as well as the collective population dynamics. Rather than combining all of our data into our model at the onset, we develop a separable architecture that operates on individual time-series first before passing them forward; this induces a permutation-invariance property and can be used to transfer across systems of different size and order. After demonstrating that our model can be applied to successfully recover complex interactions and dynamics in many-body systems, we apply our approach to populations of neurons in the nervous system. On neural activity datasets, we show that our model not only yields robust decoding performance, but also provides impressive performance in transfer across recordings of different animals without any neuron-level correspondence. By enabling flexible pre-training that can be transferred to neural recordings of different size and order, our work provides a first step towards creating a foundation model for neural decoding.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112342083",
                        "name": "Ran Liu"
                    },
                    {
                        "authorId": "2039962468",
                        "name": "Mehdi Azabou"
                    },
                    {
                        "authorId": "150206486",
                        "name": "M. Dabagia"
                    },
                    {
                        "authorId": "31227090",
                        "name": "Jingyun Xiao"
                    },
                    {
                        "authorId": "1746363",
                        "name": "Eva L. Dyer"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In [15], without the knowledge about the Hamiltonian parameters, unsupervised learning has been applied to learn a parametric function according to the symplectic gradient of the Hamiltonian function."
            ],
            "citingPaper": {
                "paperId": "18c705659ceab9d83ccb665a5887ee44e6cc0d08",
                "externalIds": {
                    "DBLP": "conf/amcc/KennyYPD22",
                    "DOI": "10.23919/ACC53348.2022.9867713",
                    "CorpusId": 252104829
                },
                "corpusId": 252104829,
                "publicationVenue": {
                    "id": "fe4d09f8-d278-4bfb-b73a-1a6a0e22f6a3",
                    "name": "American Control Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Adv Comput Control",
                        "ACC",
                        "Advances in Computing and Communications",
                        "Adv Comput Commun",
                        "Am Control Conf",
                        "Advances in Computer and Communication",
                        "International Conference on Advanced Computer Control"
                    ],
                    "issn": "2767-2875",
                    "url": "http://a2c2.org/conferences/american-control-conferences",
                    "alternate_urls": [
                        "http://www.acc-rajagiri.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/18c705659ceab9d83ccb665a5887ee44e6cc0d08",
                "title": "Feature Learning for Optimal Control with B-spline Representations",
                "abstract": "The paper develops a feature learning-based method to solve optimal control problems using B-splines to approximate the optimal solutions. The feature learning-based optimal control method can quickly generate near-optimal trajectories for general optimal control problems subject to system dynamics and constraints. The steps in the proposed method are as follows: Firstly, by representing the state and control variables with B-spline functions, the optimal control problem is converted into an approximate nonlinear programming (NLP) problem, where parameters of the B-splines are identified as features of the optimal solution. Secondly, for a specific problem with designated inputs, a dataset of the optimal trajectories under varying inputs is generated by solving the corresponding NLP problem offline. Finally, the neural network is applied to map the relationship between the designated inputs and identified features, represented by the parameters of B-splines and time variables. To show the effectiveness and efficiency of the proposed method for solving the optimal control problems, extensive simulation cases are presented and analyzed.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2148745684",
                        "name": "Vinay Kenny"
                    },
                    {
                        "authorId": "3433875",
                        "name": "Sixiong You"
                    },
                    {
                        "authorId": "30661195",
                        "name": "Chaoying Pei"
                    },
                    {
                        "authorId": "1878564",
                        "name": "R. Dai"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In [5], a framework for discovering the Hamiltonian dynamics from time derivatives of the observed system coordinates is proposed."
            ],
            "citingPaper": {
                "paperId": "8ee56eeba52972426217eb63cd9d17f2c1a87de0",
                "externalIds": {
                    "DBLP": "conf/amcc/SirichotiyakulA22",
                    "DOI": "10.23919/ACC53348.2022.9867143",
                    "CorpusId": 252102259
                },
                "corpusId": 252102259,
                "publicationVenue": {
                    "id": "fe4d09f8-d278-4bfb-b73a-1a6a0e22f6a3",
                    "name": "American Control Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Adv Comput Control",
                        "ACC",
                        "Advances in Computing and Communications",
                        "Adv Comput Commun",
                        "Am Control Conf",
                        "Advances in Computer and Communication",
                        "International Conference on Advanced Computer Control"
                    ],
                    "issn": "2767-2875",
                    "url": "http://a2c2.org/conferences/american-control-conferences",
                    "alternate_urls": [
                        "http://www.acc-rajagiri.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8ee56eeba52972426217eb63cd9d17f2c1a87de0",
                "title": "Robust Data-Driven Passivity-Based Control of Underactuated Systems via Neural Approximators and Bayesian Inference",
                "abstract": "We synthesize controllers for underactuated robotic systems using data-driven approaches. Inspired by techniques from classical passivity theory, the control law is parametrized by the gradient of an energy-like (Lyapunov) function, which is represented by a neural network. With the control task encoded as the objective of the optimization, we systematically identify the optimal neural net parameters using gradient-based techniques. The proposed method is validated on the cart-pole swing-up task, both in simulation and on a real system. Additionally, we address questions about controller\u2019s robustness against model uncertainties and measurement noise, using a Bayesian approach to infer a probability distribution over the parameters of the controller. The proposed robustness improvement technique is demonstrated on the simple pendulum system.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "89668696",
                        "name": "Wankun Sirichotiyakul"
                    },
                    {
                        "authorId": "82745720",
                        "name": "N. Ashenafi"
                    },
                    {
                        "authorId": "11495649",
                        "name": "A. Satici"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "[19], via the SymODEN [39, 38] and port-Hamiltonian neural network [14] frameworks, and is defined for systems on any manifold.",
                "Hamiltonian neural network (HNN) is a hybrid machine learning framework imposing hard constraints on a data-driven model [19].",
                "Following [19, 8, 27], we use fully connected neural networks with two hidden layers of 100 neurons each to estimate the Hamiltonian and the external force.",
                "Furthermore, we use the hyperbolic tangent (tanh) and Rectified Linear Unit (ReLU) as activation functions for the first and second hidden layer, respectively, while [19, 8, 27] use tanh for both.",
                "The HNNs of [19] use a neural network \u0124\u03b8 with weights \u03b8 to approximate the Hamiltonian H(q, p) of a system.",
                "4 Implementation and hyperparameters Following [19, 8, 27], we use fully connected neural networks with two hidden layers of 100 neurons each to estimate the Hamiltonian and the external force.",
                "2 Choice of discretization method in training Instead of training on the integration scheme as we do, works like [19, 8, 14] either assume that derivatives of the state variables are known or perform one or more integration steps at each training step."
            ],
            "citingPaper": {
                "paperId": "7d14d0167c2c1f1a3c3aaaeb2670bfdbbdeb5faf",
                "externalIds": {
                    "ArXiv": "2206.02660",
                    "DOI": "10.1016/j.physd.2023.133673",
                    "CorpusId": 256105403
                },
                "corpusId": 256105403,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7d14d0167c2c1f1a3c3aaaeb2670bfdbbdeb5faf",
                "title": "Pseudo-Hamiltonian neural networks with state-dependent external forces",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "102675722",
                        "name": "S\u00f8lve Eidnes"
                    },
                    {
                        "authorId": "1560989357",
                        "name": "Alexander J. Stasik"
                    },
                    {
                        "authorId": "1491380459",
                        "name": "Camilla Sterud"
                    },
                    {
                        "authorId": "1404060050",
                        "name": "Eivind B\u00f8hn"
                    },
                    {
                        "authorId": "1388390391",
                        "name": "S. Riemer-S\u00f8rensen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ddcf1fe8b78805f1edbeef00bce224cc69d3415a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-01298",
                    "ArXiv": "2206.01298",
                    "DOI": "10.48550/arXiv.2206.01298",
                    "CorpusId": 249375207
                },
                "corpusId": 249375207,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ddcf1fe8b78805f1edbeef00bce224cc69d3415a",
                "title": "PNODE: A memory-efficient neural ODE framework based on high-level adjoint differentiation",
                "abstract": "Neural ordinary differential equations (neural ODEs) have emerged as a novel network architecture that bridges dynamical systems and deep learning. However, the gradient obtained with the continuous adjoint method in the vanilla neural ODE is not reverse-accurate. Other approaches suffer either from an excessive memory requirement due to deep computational graphs or from limited choices for the time integration scheme, hampering their application to large-scale complex dynamical systems. To achieve accurate gradients without compromising memory efficiency and flexibility, we present a new neural ODE framework, PNODE, based on high-level discrete adjoint algorithmic differentiation. By leveraging discrete adjoint time integrators and advanced checkpointing strategies tailored for these integrators, PNODE can provide a balance between memory and computational costs, while computing the gradients consistently and accurately. We provide an open-source implementation based on PyTorch and PETSc, one of the most commonly used portable, scalable scientific computing libraries. We demonstrate the performance through extensive numerical experiments on image classification and continuous normalizing flow problems. We show that PNODE achieves the highest memory efficiency when compared with other reverse-accurate methods. On the image classification problems, PNODE is up to two times faster than the vanilla neural ODE and up to 2.3 times faster than the best existing reverse-accurate method. We also show that PNODE enables the use of the implicit time integration methods that are needed for stiff dynamical systems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "104693914",
                        "name": "Hong Zhang"
                    },
                    {
                        "authorId": "123903443",
                        "name": "Wenjun Zhao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "reasoning [233] [234], signal temporal logic [232], and physics-aware embeddings [235] [236] [237] [238] for robust complex event processing within the laws and bounds of physics."
            ],
            "citingPaper": {
                "paperId": "0c8a4edb0035e3ef942645b3f9879b6d883343aa",
                "externalIds": {
                    "ArXiv": "2205.14550",
                    "DBLP": "journals/corr/abs-2205-14550",
                    "DOI": "10.1109/JSEN.2022.3210773",
                    "CorpusId": 249192002,
                    "PubMed": "36439060"
                },
                "corpusId": 249192002,
                "publicationVenue": {
                    "id": "b210fd3d-11d7-478e-a0aa-7e3d2a4f482d",
                    "name": "IEEE Sensors Journal",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Sens J"
                    ],
                    "issn": "1530-437X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=7361",
                    "alternate_urls": [
                        "http://ieee-sensors.org/sensors-journal/",
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?puNumber=7361",
                        "http://www.ieee-sensors.org/journals",
                        "https://ieee-sensors.org/sensors-journal/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0c8a4edb0035e3ef942645b3f9879b6d883343aa",
                "title": "Machine Learning for Microcontroller-Class Hardware: A Review",
                "abstract": "The advancements in machine learning (ML) opened a new opportunity to bring intelligence to the low-end Internet-of-Things (IoT) nodes, such as microcontrollers. Conventional ML deployment has high memory and computes footprint hindering their direct deployment on ultraresource-constrained microcontrollers. This article highlights the unique requirements of enabling onboard ML for microcontroller-class devices. Researchers use a specialized model development workflow for resource-limited applications to ensure that the compute and latency budget is within the device limits while still maintaining the desired performance. We characterize a closed-loop widely applicable workflow of ML model development for microcontroller-class devices and show that several classes of applications adopt a specific instance of it. We present both qualitative and numerical insights into different stages of model development by showcasing several use cases. Finally, we identify the open research challenges and unsolved questions demanding careful considerations moving forward.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144049820",
                        "name": "Swapnil Sayan Saha"
                    },
                    {
                        "authorId": "2353239",
                        "name": "S. Sandha"
                    },
                    {
                        "authorId": "1702254",
                        "name": "M. Srivastava"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Experimentaly, Neural ODEs have been successful in a various range of applications, among which physical modelling [Greydanus et al., 2019, Cranmer et al., 2019] and generative modeling [Chen et al., 2018, Grathwohl et al., 2018].",
                "Experimentaly, Neural ODEs have been successful in a various range of applications, among which physical modelling (Greydanus et al., 2019; Cranmer et al., 2019) and generative modeling (Chen et al."
            ],
            "citingPaper": {
                "paperId": "5ae2e7b99cc0413a6d36b6535937cf1ab2086484",
                "externalIds": {
                    "ArXiv": "2205.14612",
                    "DBLP": "conf/nips/SanderAP22",
                    "DOI": "10.48550/arXiv.2205.14612",
                    "CorpusId": 249192166
                },
                "corpusId": 249192166,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5ae2e7b99cc0413a6d36b6535937cf1ab2086484",
                "title": "Do Residual Neural Networks discretize Neural Ordinary Differential Equations?",
                "abstract": "Neural Ordinary Differential Equations (Neural ODEs) are the continuous analog of Residual Neural Networks (ResNets). We investigate whether the discrete dynamics defined by a ResNet are close to the continuous one of a Neural ODE. We first quantify the distance between the ResNet's hidden state trajectory and the solution of its corresponding Neural ODE. Our bound is tight and, on the negative side, does not go to 0 with depth N if the residual functions are not smooth with depth. On the positive side, we show that this smoothness is preserved by gradient descent for a ResNet with linear residual functions and small enough initial loss. It ensures an implicit regularization towards a limit Neural ODE at rate 1 over N, uniformly with depth and optimization time. As a byproduct of our analysis, we consider the use of a memory-free discrete adjoint method to train a ResNet by recovering the activations on the fly through a backward pass of the network, and show that this method theoretically succeeds at large depth if the residual functions are Lipschitz with the input. We then show that Heun's method, a second order ODE integration scheme, allows for better gradient estimation with the adjoint method when the residual functions are smooth with depth. We experimentally validate that our adjoint method succeeds at large depth, and that Heun method needs fewer layers to succeed. We finally use the adjoint method successfully for fine-tuning very deep ResNets without memory consumption in the residual layers.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2068668635",
                        "name": "Michael E. Sander"
                    },
                    {
                        "authorId": "1763708",
                        "name": "Pierre Ablin"
                    },
                    {
                        "authorId": "2134774604",
                        "name": "Gabriel Peyr'e"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Physics-inspired neural networks such as Deep Lagriangen Networks (DeLaN) [13] or Hamiltonian networks [5] guarantee physically plausible dynamic models, which conserve energy."
            ],
            "citingPaper": {
                "paperId": "beece4b96592d125a8700839d39ecec43cc00a13",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-13804",
                    "ArXiv": "2205.13804",
                    "DOI": "10.48550/arXiv.2205.13804",
                    "CorpusId": 248942160
                },
                "corpusId": 248942160,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/beece4b96592d125a8700839d39ecec43cc00a13",
                "title": "End-to-End Learning of Hybrid Inverse Dynamics Models for Precise and Compliant Impedance Control",
                "abstract": "It is well-known that inverse dynamics models can improve tracking performance in robot control. These models need to precisely capture the robot dynamics, which consist of well-understood components, e.g., rigid body dynamics, and effects that remain challenging to capture, e.g., stick-slip friction and mechanical flexibilities. Such effects exhibit hysteresis and partial observability, rendering them, particularly challenging to model. Hence, hybrid models, which combine a physical prior with data-driven approaches are especially well-suited in this setting. We present a novel hybrid model formulation that enables us to identify fully physically consistent inertial parameters of a rigid body dynamics model which is paired with a recurrent neural network architecture, allowing us to capture unmodeled partially observable effects using the network memory. We compare our approach against state-of-the-art inverse dynamics models on a 7 degree of freedom manipulator. Using data sets obtained through an optimal experiment design approach, we study the accuracy of offline torque prediction and generalization capabilities of joint learning methods. In control experiments on the real system, we evaluate the model as a feed-forward term for impedance control and show the feedback gains can be drastically reduced to achieve a given tracking accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2165874245",
                        "name": "Moritz Reuss"
                    },
                    {
                        "authorId": "3481858",
                        "name": "N. V. Duijkeren"
                    },
                    {
                        "authorId": "145369877",
                        "name": "R. Krug"
                    },
                    {
                        "authorId": "2070414791",
                        "name": "P. Becker"
                    },
                    {
                        "authorId": "7155274",
                        "name": "Vaisakh Shaj"
                    },
                    {
                        "authorId": "26599977",
                        "name": "G. Neumann"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "This line of work originates from Lutter et al. (2019); Greydanus et al. (2019); Cranmer et al. (2020) (see Zhong et al. (2021) for an overview) and has been extended to NODEs for Hamiltonian and port-Hamiltonian systems (Zhong et al., 2020; Massaroli et al., 2020a; Zakwan et al., 2022), but also\u2026",
                "It also does not conserve energy, which is not surprising when no structure is imposed, as discussed e.g., in Greydanus et al. (2019)."
            ],
            "citingPaper": {
                "paperId": "b1aeafbf4fbd9b0055f33795c059a5769cbbeb55",
                "externalIds": {
                    "ArXiv": "2205.12550",
                    "DBLP": "journals/tmlr/BuissonFenetMTM23",
                    "CorpusId": 252907957
                },
                "corpusId": 252907957,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b1aeafbf4fbd9b0055f33795c059a5769cbbeb55",
                "title": "Recognition Models to Learn Dynamics from Partial Observations with Neural ODEs",
                "abstract": "Identifying dynamical systems from experimental data is a notably difficult task. Prior knowledge generally helps, but the extent of this knowledge varies with the application, and customized models are often needed. Neural ordinary differential equations can be written as a flexible framework for system identification and can incorporate a broad spectrum of physical insight, giving physical interpretability to the resulting latent space. In the case of partial observations, however, the data points cannot directly be mapped to the latent state of the ODE. Hence, we propose to design recognition models, in particular inspired by nonlinear observer theory, to link the partial observations to the latent state. We demonstrate the performance of the proposed approach on numerical simulations and on an experimental dataset from a robotic exoskeleton.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1410600781",
                        "name": "Mona Buisson-Fenet"
                    },
                    {
                        "authorId": "84320271",
                        "name": "V. Morgenthaler"
                    },
                    {
                        "authorId": "2715093",
                        "name": "Sebastian Trimpe"
                    },
                    {
                        "authorId": "2086729",
                        "name": "F. D. Meglio"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "56119684a856c24d30c0f5f5224768021383032a",
                "externalIds": {
                    "DOI": "10.1007/s40747-022-00769-8",
                    "CorpusId": 249084540
                },
                "corpusId": 249084540,
                "publicationVenue": {
                    "id": "d30c9917-b233-46f4-a644-8e5cdf6d6c5e",
                    "name": "Complex & Intelligent Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Complex  Intell Syst"
                    ],
                    "issn": "2199-4536",
                    "url": "https://link.springer.com/journal/40747",
                    "alternate_urls": [
                        "http://link.springer.com/journal/40747"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/56119684a856c24d30c0f5f5224768021383032a",
                "title": "Trajectory prediction based on conditional Hamiltonian generative network for incomplete observation image sequences",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48015884",
                        "name": "Kui Qian"
                    },
                    {
                        "authorId": "2087843319",
                        "name": "Lei Tian"
                    },
                    {
                        "authorId": "2164514204",
                        "name": "Aiguo Song"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Hamiltonian dynamics (Greydanus et al., 2019) describes a system\u2019s total energyH(q,p) as a function of its canonical coordinates q and momenta p, e.",
                "Hamiltonian dynamics (Greydanus et al., 2019) describes a system\u2019s total energyH(q,p) as a function of its canonical coordinates q and momenta p, e.g., each particles\u2019 position and momentum."
            ],
            "citingPaper": {
                "paperId": "8552ccfd023ceb9f94e35668601badaabef8ec92",
                "externalIds": {
                    "ArXiv": "2205.07266",
                    "CorpusId": 255546261
                },
                "corpusId": 255546261,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8552ccfd023ceb9f94e35668601badaabef8ec92",
                "title": "Discovering and Explaining the Representation Bottleneck of Graph Neural Networks from Multi-order Interactions",
                "abstract": "Graph neural networks (GNNs) mainly rely on the message-passing paradigm to propagate node features and build interactions, and different graph learning tasks require different ranges of node interactions. In this work, we explore the capacity of GNNs to capture interactions between nodes under contexts with different complexities. We discover that GNNs are usually unable to capture the most informative kinds of interaction styles for diverse graph learning tasks, and thus name this phenomenon as GNNs' representation bottleneck. As a response, we demonstrate that the inductive bias introduced by existing graph construction mechanisms can prevent GNNs from learning interactions of the most appropriate complexity, i.e., resulting in the representation bottleneck. To address that limitation, we propose a novel graph rewiring approach based on interaction patterns learned by GNNs to adjust the receptive fields of each node dynamically. Extensive experiments on both real-world and synthetic datasets prove the effectiveness of our algorithm to alleviate the representation bottleneck and its superiority to enhance the performance of GNNs over state-of-the-art graph rewiring baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152167768",
                        "name": "Fang Wu"
                    },
                    {
                        "authorId": "2118155623",
                        "name": "Siyuan Li"
                    },
                    {
                        "authorId": "47767812",
                        "name": "Lirong Wu"
                    },
                    {
                        "authorId": "9215251",
                        "name": "Dragomir R. Radev"
                    },
                    {
                        "authorId": "1390908654",
                        "name": "Stan Z. Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "To promote physical feasibility of the solution, many works impose equality or inequality system constraints by i) encoding hard constraints inside layers (e.g. sigmoid layer to encode technical limits of upper and lower bounds), ii) applying prior on the NN architecture (e.g., Hamiltonian [24] and Lagrangian neural networks [25]), iii) augmenting the objective function with penalty terms (supervised [4] or unsupervised [8] [5]), iv) projecting outputs [7] to the feasible domain, or v) combining many different strategies.",
                ", Hamiltonian [24] and Lagrangian neural networks [25]), iii) augmenting the objective function with penalty terms (supervised [4] or unsupervised [8] [5]), iv) projecting outputs [7] to the feasible domain, or v) combining many different strategies."
            ],
            "citingPaper": {
                "paperId": "ac60bd7c64ba12eef0b5a70c37904f33ae6afded",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-03673",
                    "ArXiv": "2205.03673",
                    "DOI": "10.48550/arXiv.2205.03673",
                    "CorpusId": 248572165
                },
                "corpusId": 248572165,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ac60bd7c64ba12eef0b5a70c37904f33ae6afded",
                "title": "Towards Practical Physics-Informed ML Design and Evaluation for Power Grid",
                "abstract": "When applied to a real-world safety critical system like the power grid, general machine learning methods suffer from expensive training, non-physical solutions, and limited interpretability. To address these challenges for power grids, many recent works have explored the inclusion of grid physics (i.e., domain expertise) into their method design, primarily through including system constraints and technical limits, reducing search space and defining meaningful features in latent space. Yet, there is no general methodology to evaluate the practicality of these approaches in power grid tasks, and limitations exist regarding scalability, generalization, interpretability, etc. This work formalizes a new concept of physical interpretability which assesses how a ML model makes predictions in a physically meaningful way and introduces an evaluation methodology that identifies a set of attributes that a practical method should satisfy. Inspired by the evaluation attributes, the paper further develops a novel contingency analysis warm starter for MadIoT cyberattack, based on a conditional Gaussian random field. This method serves as an instance of an ML model that can incorporate diverse domain knowledge and improve on these identified attributes. Experiments validate that the warm starter significantly boosts the efficiency of contingency analysis for MadIoT attack even with shallow NN architectures.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144496881",
                        "name": "Shimiao Li"
                    },
                    {
                        "authorId": "145190652",
                        "name": "Amritanshu Pandey"
                    },
                    {
                        "authorId": "2093783115",
                        "name": "L. Pileggi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Other research focused on efficient simulations by learning conservation laws (Cranmer et al., 2020; Greydanus et al., 2019), or aimed at correcting iterative solvers (Hsieh et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "b5e32b5f89a8116b8a511c7cb840206628bb6620",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-01222",
                    "ArXiv": "2205.01222",
                    "DOI": "10.48550/arXiv.2205.01222",
                    "CorpusId": 248505945
                },
                "corpusId": 248505945,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b5e32b5f89a8116b8a511c7cb840206628bb6620",
                "title": "Leveraging Stochastic Predictions of Bayesian Neural Networks for Fluid Simulations",
                "abstract": "We investigate uncertainty estimation and multimodality via the non-deterministic predictions of Bayesian neural networks (BNNs) in fluid simulations. To this end, we deploy BNNs in three challenging experimental test-cases of increasing complexity: We show that BNNs, when used as surrogate models for steady-state fluid flow predictions, provide accurate physical predictions together with sensible estimates of uncertainty. Further, we experiment with perturbed temporal sequences from Navier-Stokes simulations and evaluate the capabilities of BNNs to capture multimodal evolutions. While our findings indicate that this is problematic for large perturbations, our results show that the networks learn to correctly predict high uncertainties in such situations. Finally, we study BNNs in the context of solver interactions with turbulent plasma flows. We find that BNN-based corrector networks can stabilize coarse-grained simulations and successfully create multimodal trajectories.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "80580587",
                        "name": "Maximilian Mueller"
                    },
                    {
                        "authorId": "46424015",
                        "name": "R. Greif"
                    },
                    {
                        "authorId": "2758452",
                        "name": "F. Jenko"
                    },
                    {
                        "authorId": "2125721010",
                        "name": "Nils Thuerey"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "For example, [16, 10] and [52] use a neural network to parameterize the Hamiltonian of a system, which relates the total energy to the change of the state.",
                "Several of the previously mentioned works model physical systems using Lagrangian or Hamiltonian energy formulations [30, 16, 11, 10, 52, 58, 28, 59], or other general physics models [26].",
                "While earlier works [30, 16, 42, 11, 58, 10, 20, 43] require coordinate data, i."
            ],
            "citingPaper": {
                "paperId": "458fa7e578f54573ce313308b215b7d8e0abe33a",
                "externalIds": {
                    "DBLP": "conf/wacv/HofherrKBC23",
                    "ArXiv": "2204.14030",
                    "DOI": "10.1109/WACV56688.2023.00213",
                    "CorpusId": 248476227
                },
                "corpusId": 248476227,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/458fa7e578f54573ce313308b215b7d8e0abe33a",
                "title": "Neural Implicit Representations for Physical Parameter Inference from a Single Video",
                "abstract": "Neural networks have recently been used to analyze diverse physical systems and to identify the underlying dynamics. While existing methods achieve impressive results, they are limited by their strong demand for training data and their weak generalization abilities to out-of-distribution data. To overcome these limitations, we propose to combine neural implicit representations for appearance modeling with neural ordinary differential equations (ODEs) for modelling planar physical phenomena to obtain a dynamic scene representation that can be identified directly from visual observations. Our proposed model combines several unique advantages: (i) Contrary to existing approaches that require large training datasets, we are able to identify physical parameters from only a single video. (ii) The use of neural implicit representations enables the processing of high-resolution videos and the synthesis of photo-realistic images. (iii) The embedded neural ODE has a known parametric form that allows for the identification of interpretable physical parameters, and (iv) long-term prediction in state space. (v) Furthermore, the photo-realistic rendering of novel scenes with modified physical parameters becomes possible.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "22248888",
                        "name": "F. Hofherr"
                    },
                    {
                        "authorId": "1990661065",
                        "name": "Lukas Koestler"
                    },
                    {
                        "authorId": "39600032",
                        "name": "Florian Bernard"
                    },
                    {
                        "authorId": "1695302",
                        "name": "D. Cremers"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Recent works [18, 19, 20, 21, 22, 23, 24] mainly focus on approximating Hamiltonian vector field from phase space data by means of using numerical integration to reconstruct symplectic map."
            ],
            "citingPaper": {
                "paperId": "fea226726119c857874608b0b08a0fd3aae8f738",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-13843",
                    "ArXiv": "2204.13843",
                    "DOI": "10.48550/arXiv.2204.13843",
                    "CorpusId": 248476029
                },
                "corpusId": 248476029,
                "publicationVenue": {
                    "id": "0dd1e980-f66a-40ab-9f04-626424b1a7d8",
                    "name": "Journal of Computational and Applied Mathematics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Appl Math"
                    ],
                    "issn": "0377-0427",
                    "alternate_issns": [
                        "0771-050X"
                    ],
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/505613/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/03770427",
                        "https://www.journals.elsevier.com/journal-of-computational-and-applied-mathematics/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fea226726119c857874608b0b08a0fd3aae8f738",
                "title": "VPNets: Volume-preserving neural networks for learning source-free dynamics",
                "abstract": "We propose volume-preserving networks (VPNets) for learning unknown source-free dynamical systems using trajectory data. We propose three modules and combine them to obtain two network architectures, coined R-VPNet and LA-VPNet. The distinct feature of the proposed models is that they are intrinsic volume-preserving. In addition, the corresponding approximation theorems are proved, which theoretically guarantee the expressivity of the proposed VPNets to learn source-free dynamics. The effectiveness, generalization ability and structure-preserving property of the VP-Nets are demonstrated by numerical experiments.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2430811",
                        "name": "Aiqing Zhu"
                    },
                    {
                        "authorId": "145801437",
                        "name": "Beibei Zhu"
                    },
                    {
                        "authorId": "2107988113",
                        "name": "Jiawei Zhang"
                    },
                    {
                        "authorId": "1805110",
                        "name": "Yifa Tang"
                    },
                    {
                        "authorId": "2150169893",
                        "name": "Jian Liu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "(Greydanus et al., 2019; Desai et al., 2021) learn a Hamiltonian function using a neural network."
            ],
            "citingPaper": {
                "paperId": "20f363809f9ef69593129e6af63da8bd2413b9cc",
                "externalIds": {
                    "DBLP": "conf/l4dc/ValpergaWTKL22",
                    "ArXiv": "2204.12323",
                    "CorpusId": 248392372
                },
                "corpusId": 248392372,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/20f363809f9ef69593129e6af63da8bd2413b9cc",
                "title": "Learning Reversible Symplectic Dynamics",
                "abstract": "Time-reversal symmetry arises naturally as a structural property in many dynamical systems of interest. While the importance of hard-wiring symmetry is increasingly recognized in machine learning, to date this has eluded time-reversibility. In this paper we propose a new neural network architecture for learning time-reversible dynamical systems from data. We focus in particular on an adaptation to symplectic systems, because of their importance in physics-informed learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2163522170",
                        "name": "Riccardo Valperga"
                    },
                    {
                        "authorId": "39399700",
                        "name": "K. Webster"
                    },
                    {
                        "authorId": "145169196",
                        "name": "D. Turaev"
                    },
                    {
                        "authorId": "2061475214",
                        "name": "Victoria G Klein"
                    },
                    {
                        "authorId": "144645873",
                        "name": "J. Lamb"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Moreover, we attain additional novelty as these results are obtained without needing to impose a specific structure on the state-space (such as in Greydanus et al. (2019); Cranmer et al. (2020)) obtaining a practically widely applicable method.",
                "An adjacent research direction is the method/models which consider CT dynamics and directly use the state derivatives and even often the noiseless states to formulate structured and interpretable models such as Hamiltonian Neural Networks (HNN) (Greydanus et al., 2019), Lagrangian Neural Networks (LNN) (Cranmer et al., 2020) and Sparse Identification of Nonlinear Dynamics (SINDy) (Brunton et al., 2016).",
                "\u2026state derivatives and even often the noiseless states to formulate structured and interpretable models such as Hamiltonian Neural Networks (HNN) (Greydanus et al., 2019), Lagrangian Neural Networks (LNN) (Cranmer et al., 2020) and Sparse Identification of Nonlinear Dynamics (SINDy) (Brunton et\u2026",
                "An adjacent research direction is the method/models which consider CT dynamics and directly use the state derivatives and even often the noiseless states to formulate structured and interpretable models such as Hamiltonian Neural Networks (HNN) (Greydanus et al., 2019), Lagrangian Neural Networks (LNN) (Cranmer et al."
            ],
            "citingPaper": {
                "paperId": "9a52c83a06b6796b3d1783e4abdbe683bcc908d4",
                "externalIds": {
                    "DBLP": "conf/iclr/BeintemaST23",
                    "ArXiv": "2204.09405",
                    "CorpusId": 256105801
                },
                "corpusId": 256105801,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9a52c83a06b6796b3d1783e4abdbe683bcc908d4",
                "title": "Continuous-time identification of dynamic state-space models by deep subspace encoding",
                "abstract": "Continuous-time (CT) modeling has proven to provide improved sample efficiency and interpretability in learning the dynamical behavior of physical systems compared to discrete-time (DT) models. However, even with numerous recent developments, the CT nonlinear state-space (NL-SS) model identification problem remains to be solved in full, considering common experimental aspects such as the presence of external inputs, measurement noise, latent states, and general robustness. This paper presents a novel estimation method that addresses all these aspects and that can obtain state-of-the-art results on multiple benchmarks with compact fully connected neural networks capturing the CT dynamics. The proposed estimation method called the subspace encoder approach (SUBNET) ascertains these results by efficiently approximating the complete simulation loss by evaluating short simulations on subsections of the data, by using an encoder function to estimate the initial state for each subsection and a novel state-derivative normalization to ensure stability and good numerical conditioning of the training process. We prove that the use of subsections increases cost function smoothness together with the necessary requirements for the existence of the encoder function and we show that the proposed state-derivative normalization is essential for reliable estimation of CT NL-SS models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1602277127",
                        "name": "G. Beintema"
                    },
                    {
                        "authorId": "1767103",
                        "name": "M. Schoukens"
                    },
                    {
                        "authorId": "2188839999",
                        "name": "R. T'oth"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "118d777ecdb359c32649da0f7576af150d51eab4",
                "externalIds": {
                    "ArXiv": "2204.08155",
                    "DBLP": "journals/corr/abs-2204-08155",
                    "DOI": "10.48550/arXiv.2204.08155",
                    "CorpusId": 248227539
                },
                "corpusId": 248227539,
                "publicationVenue": {
                    "id": "88f368e9-1c91-453a-b74e-b3421b0b3dd7",
                    "name": "Communication on Applied Mathematics and Computation",
                    "type": "journal",
                    "alternate_names": [
                        "Communications on Applied Mathematics and Computation",
                        "Commun Appl Math Comput"
                    ],
                    "issn": "1006-6330",
                    "alternate_issns": [
                        "2661-8893"
                    ],
                    "url": "https://link.springer.com/journal/volumesAndIssues/42967"
                },
                "url": "https://www.semanticscholar.org/paper/118d777ecdb359c32649da0f7576af150d51eab4",
                "title": "A Dynamical System-Based Framework for Dimension Reduction",
                "abstract": "We propose a novel framework for learning a low-dimensional representation of data based on nonlinear dynamical systems, which we call the dynamical dimension reduction (DDR). In the DDR model, each point is evolved via a nonlinear flow towards a lower-dimensional subspace; the projection onto the subspace gives the low-dimensional embedding. Training the model involves identifying the nonlinear flow and the subspace. Following the equation discovery method, we represent the vector field that defines the flow using a linear combination of dictionary elements, where each element is a pre-specified linear/nonlinear candidate function. A regularization term for the average total kinetic energy is also introduced and motivated by the optimal transport theory. We prove that the resulting optimization problem is well-posed and establish several properties of the DDR method. We also show how the DDR method can be trained using a gradient-based optimization method, where the gradients are computed using the adjoint method from the optimal control theory. The DDR method is implemented and compared on synthetic and example data sets to other dimension reduction methods, including the PCA, t -SNE, and Umap.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2067177324",
                        "name": "Ryeongkyung Yoon"
                    },
                    {
                        "authorId": "2877186",
                        "name": "B. Osting"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e7a47ebb237e17aa7141c5bcab4b96d6496129df",
                "externalIds": {
                    "ArXiv": "2204.08414",
                    "DBLP": "journals/corr/abs-2204-08414",
                    "DOI": "10.48550/arXiv.2204.08414",
                    "CorpusId": 248227607
                },
                "corpusId": 248227607,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e7a47ebb237e17aa7141c5bcab4b96d6496129df",
                "title": "STONet: A Neural-Operator-Driven Spatio-temporal Network",
                "abstract": "Graph-based spatio-temporal neural networks are effective to model the spatial dependency among discrete points sampled irregularly from unstructured grids, thanks to the great expressiveness of graph neural networks. However, these models are usually spatially-transductive \u2013 only \ufb01tting the signals for discrete spatial nodes fed in models but unable to generalize to \u2018unseen\u2019 spatial points with zero-shot. In comparison, for forecasting tasks on continuous space such as temperature prediction on the earth\u2019s surface, the spatially-inductive property allows the model to generalize to any point in the spatial domain, demonstrating models\u2019 ability to learn the underlying mechanisms or physics laws of the systems, rather than simply \ufb01t the signals. Besides, in temporal domains, irregularly-sampled time series, e.g. data with missing values, urge models to be temporally-continuous. Motivated by the two issues, we pro-pose a spatio-temporal framework based on neural operators for PDEs, which learn the underlying mechanisms governing the dynamics of spatially-continuous physical quantities. Experiments show our model\u2019s improved performance on forecasting spatially-continuous physic quantities, and its superior generalization to unseen spatial points and ability to handle temporally-irregular data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Haitao Lin"
                    },
                    {
                        "authorId": "150116926",
                        "name": "Guojiang Zhao"
                    },
                    {
                        "authorId": "47767812",
                        "name": "Lirong Wu"
                    },
                    {
                        "authorId": "1390908654",
                        "name": "Stan Z. Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "1, we compare the trajectories for the mass-spring system estimated with the proposed DHH approach and with the HNN from [11] in which the derivatives are estimated using finite differences.",
                "The modeling assumption of Hamiltonian neural networks [11] is that the observed state s = (q,p) of a dynamical system evolves according to Hamilton\u2019s equations:",
                "One prominent research direction that emerged recently in the literature is modeling Hamiltonian systems with neural networks [11].",
                "Next, we quantatively compare the proposed approach against the following baselines: 1) HNN [11] with derivatives calculated as finite differences, 2) HNN [11] with derivatives provided by the simulator, 3) NSSNN [28], 4) Neural ODE [3] and 5) DHPMs [23].",
                "The original HNN model [11] had the limitation of assuming the knowledge of the state derivatives with respect to time or approximating those using finite differences.",
                "We test our method on the following four physical systems from [11]:",
                "The original HNN model [11] was trained by minimizing the loss",
                ", [7, 11, 14, 15, 19]) is therefore a promising line of research with many potential applications."
            ],
            "citingPaper": {
                "paperId": "1c9488198aff2fffbbe06aafe45d330a4ddb775a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-05077",
                    "ArXiv": "2204.05077",
                    "DOI": "10.48550/arXiv.2204.05077",
                    "CorpusId": 248085350
                },
                "corpusId": 248085350,
                "publicationVenue": {
                    "id": "3e64b1c1-745f-4edf-bd92-b8ef122bb49c",
                    "name": "International Conference on Artificial Neural Networks",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Artif Neural Netw",
                        "ICANN"
                    ],
                    "url": "http://www.e-nns.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1c9488198aff2fffbbe06aafe45d330a4ddb775a",
                "title": "Learning Trajectories of Hamiltonian Systems with Neural Networks",
                "abstract": "Modeling of conservative systems with neural networks is an area of active research. A popular approach is to use Hamiltonian neural networks (HNNs) which rely on the assumptions that a conservative system is described with Hamilton's equations of motion. Many recent works focus on improving the integration schemes used when training HNNs. In this work, we propose to enhance HNNs with an estimation of a continuous-time trajectory of the modeled system using an additional neural network, called a deep hidden physics model in the literature. We demonstrate that the proposed integration scheme works well for HNNs, especially with low sampling rates, noisy and irregular observations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2132485673",
                        "name": "Katsiaryna Haitsiukevich"
                    },
                    {
                        "authorId": "145096481",
                        "name": "A. Ilin"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4a864d3dcbdd5d1a6ac2533be2fbff60c70637d1",
                "externalIds": {
                    "PubMedCentral": "10460398",
                    "ArXiv": "2204.04348",
                    "DOI": "10.1038/s41598-023-40766-6",
                    "CorpusId": 257772087,
                    "PubMed": "37634029"
                },
                "corpusId": 257772087,
                "publicationVenue": {
                    "id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                    "name": "Scientific Reports",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Rep"
                    ],
                    "issn": "2045-2322",
                    "url": "http://www.nature.com/srep/",
                    "alternate_urls": [
                        "http://www.nature.com/srep/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4a864d3dcbdd5d1a6ac2533be2fbff60c70637d1",
                "title": "Neuronal diversity can improve machine learning for physics and beyond",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145638518",
                        "name": "A. Choudhary"
                    },
                    {
                        "authorId": "2162047367",
                        "name": "Anil Radhakrishnan"
                    },
                    {
                        "authorId": "2468955",
                        "name": "J. Lindner"
                    },
                    {
                        "authorId": "2053142",
                        "name": "S. Sinha"
                    },
                    {
                        "authorId": "3067672",
                        "name": "W. Ditto"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "We implement Hamiltonian neural networks (HNNs; Greydanus et al. 2019; SanchezGonzalez et al. 2019) with scalar-based MLPs for this learning task."
            ],
            "citingPaper": {
                "paperId": "6ef8388468b3206b605475775b877f9ef073e729",
                "externalIds": {
                    "ArXiv": "2204.00887",
                    "DBLP": "journals/corr/abs-2204-00887",
                    "DOI": "10.48550/arXiv.2204.00887",
                    "CorpusId": 247939698
                },
                "corpusId": 247939698,
                "publicationVenue": {
                    "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                    "name": "Journal of machine learning research",
                    "type": "journal",
                    "alternate_names": [
                        "Journal of Machine Learning Research",
                        "J mach learn res",
                        "J Mach Learn Res"
                    ],
                    "issn": "1532-4435",
                    "alternate_issns": [
                        "1533-7928"
                    ],
                    "url": "http://www.ai.mit.edu/projects/jmlr/",
                    "alternate_urls": [
                        "http://jmlr.csail.mit.edu/",
                        "http://www.jmlr.org/",
                        "http://portal.acm.org/affiliated/jmlr"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6ef8388468b3206b605475775b877f9ef073e729",
                "title": "Dimensionless machine learning: Imposing exact units equivariance",
                "abstract": "Units equivariance (or units covariance) is the exact symmetry that follows from the requirement that relationships among measured quantities of physics relevance must obey self-consistent dimensional scalings. Here, we express this symmetry in terms of a (non-compact) group action, and we employ dimensional analysis and ideas from equivariant machine learning to provide a methodology for exactly units-equivariant machine learning: For any given learning task, we first construct a dimensionless version of its inputs using classic results from dimensional analysis, and then perform inference in the dimensionless space. Our approach can be used to impose units equivariance across a broad range of machine learning methods which are equivariant to rotations and other groups. We discuss the in-sample and out-of-sample prediction accuracy gains one can obtain in contexts like symbolic regression and emulation, where symmetry is important. We illustrate our approach with simple numerical examples involving dynamical systems in physics and ecology.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144790990",
                        "name": "Soledad Villar"
                    },
                    {
                        "authorId": "29892797",
                        "name": "Weichi Yao"
                    },
                    {
                        "authorId": "144735014",
                        "name": "D. Hogg"
                    },
                    {
                        "authorId": "1401969150",
                        "name": "Ben Blum-Smith"
                    },
                    {
                        "authorId": "4184955",
                        "name": "Bianca Dumitrascu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "69a03ced4dd5395f8c89c80a4ec062319d056504",
                "externalIds": {
                    "DBLP": "journals/cma/CuiWX22",
                    "DOI": "10.1016/j.camwa.2022.02.004",
                    "CorpusId": 247133276
                },
                "corpusId": 247133276,
                "publicationVenue": {
                    "id": "635a0827-5219-4c75-84a2-fab5be8f309f",
                    "name": "Computers and Mathematics with Applications",
                    "type": "journal",
                    "alternate_names": [
                        "Computers & Mathematics With Applications",
                        "Computers & Mathematics with Applications",
                        "Comput Math Appl",
                        "Comput  Math Appl"
                    ],
                    "issn": "0898-1221",
                    "alternate_issns": [
                        "0097-4943"
                    ],
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/301/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/08981221",
                        "https://www.journals.elsevier.com/computers-and-mathematics-with-applications/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/69a03ced4dd5395f8c89c80a4ec062319d056504",
                "title": "An efficient neural network method with plane wave activation functions for solving Helmholtz equation",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49446670",
                        "name": "Tao Cui"
                    },
                    {
                        "authorId": "2137317162",
                        "name": "Ziming Wang"
                    },
                    {
                        "authorId": "2451845",
                        "name": "Xueshuang Xiang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Examples of real-world object-centric data include trajectory data from multi-agent systems [109, 110, 111], position and velocity data from networks of motion sensors [110, 112], and molecule data [53].",
                "For example, DeLaN [117] and HNN [109] contain modules estimating the Lagrangian/Hamiltonian function of the system and following modules deriving the prediction.",
                "Similarly, [109] (HNN) models the Hamiltonian function with a neural network.",
                "Second, existing works such as [117, 109, 123, 60, 140, 43, 34] heavily rely on heterogeneous domain-specific datasets, which greatly increases the difficulty of fairly comparing different PIML methods.",
                "Energy Conservation Law [117][118][109][154][134]",
                "[109] designs models for forecasting n-body systems that learn and respect exact conservation laws - Hamiltonian mechanics - in an unsupervised manner."
            ],
            "citingPaper": {
                "paperId": "f4eb1de428295e9743a0b4754776813df6e951da",
                "externalIds": {
                    "ArXiv": "2203.16797",
                    "DBLP": "journals/corr/abs-2203-16797",
                    "DOI": "10.48550/arXiv.2203.16797",
                    "CorpusId": 247839241
                },
                "corpusId": 247839241,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f4eb1de428295e9743a0b4754776813df6e951da",
                "title": "When Physics Meets Machine Learning: A Survey of Physics-Informed Machine Learning",
                "abstract": "Physics-informed machine learning (PIML), referring to the combination of prior knowledge of physics, which is the high level abstraction of natural phenomenons and human behaviours in the long history, with data-driven machine learning models, has emerged as an effective way to mitigate the shortage of training data, to increase models' generalizability and to ensure the physical plausibility of results. In this paper, we survey an abundant number of recent works in PIML and summarize them from three aspects: (1) motivations of PIML, (2) physics knowledge in PIML, (3) methods of physics knowledge integration in PIML. We also discuss current challenges and corresponding research opportunities in PIML.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "27737939",
                        "name": "Chuizheng Meng"
                    },
                    {
                        "authorId": "145260557",
                        "name": "Sungyong Seo"
                    },
                    {
                        "authorId": "120783624",
                        "name": "Defu Cao"
                    },
                    {
                        "authorId": "2160884182",
                        "name": "Sam Griesemer"
                    },
                    {
                        "authorId": "2156649635",
                        "name": "Yan Liu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "f5e1c1c1ba6985094a844a11326232419e7eb775",
                "externalIds": {
                    "DBLP": "journals/mlst/TeruyaTMHO22",
                    "DOI": "10.1088/2632-2153/ac61eb",
                    "CorpusId": 247806223
                },
                "corpusId": 247806223,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f5e1c1c1ba6985094a844a11326232419e7eb775",
                "title": "ARTS: autonomous research topic selection system using word embeddings and network analysis",
                "abstract": "The materials science research process has become increasingly autonomous due to the remarkable progress in artificial intelligence. However, autonomous research topic selection (ARTS) has not yet been fully explored due to the difficulty of estimating its promise and the lack of previous research. This paper introduces an ARTS system that autonomously selects potential research topics that are likely to reveal new scientific facts yet have not been the subject of much previous research by analyzing vast numbers of articles. Potential research topics are selected by analyzing the difference between two research concept networks constructed from research information in articles: one that represents the promise of research topics and is constructed from word embeddings, and one that represents known facts and past research activities and is constructed from statistical information on the appearance patterns of research concepts. The ARTS system is also equipped with functions to search and visualize information about selected research topics to assist in the final determination of a research topic by a scientist. We developed the ARTS system using approximately 100\u200900 articles published in the Computational Materials Science journal. The results of our evaluation demonstrated that research topics studied after 2016 could be generated autonomously from an analysis of the articles published before 2015. This suggests that potential research topics can be effectively selected by using the ARTS system.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2160772260",
                        "name": "Eri Teruya"
                    },
                    {
                        "authorId": "2160776777",
                        "name": "Tadashi Takeuchi"
                    },
                    {
                        "authorId": "68974966",
                        "name": "Hidekazu Morita"
                    },
                    {
                        "authorId": "2160776614",
                        "name": "Takayuki Hayashi"
                    },
                    {
                        "authorId": "2160774973",
                        "name": "Kanta Ono"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "The continuous-time nature of NODEs makes them particularly suitable for learning complex dynamical systems [9], [10] and allows borrowing tools from the system theory to analyze NN properties."
            ],
            "citingPaper": {
                "paperId": "18a133cbcd3048e085e7273d0e63c22eb99cc864",
                "externalIds": {
                    "DBLP": "journals/csysl/ZakwanXF23",
                    "ArXiv": "2203.11805",
                    "DOI": "10.1109/lcsys.2022.3186959",
                    "CorpusId": 247597056
                },
                "corpusId": 247597056,
                "publicationVenue": {
                    "id": "b84ebb03-add3-4e2c-b380-26ecc87c7011",
                    "name": "IEEE Control Systems Letters",
                    "alternate_names": [
                        "IEEE Control Syst Lett"
                    ],
                    "issn": "2475-1456",
                    "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=7782633",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7782633"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/18a133cbcd3048e085e7273d0e63c22eb99cc864",
                "title": "Robust Classification Using Contractive Hamiltonian Neural ODEs",
                "abstract": "Deep neural networks can be fragile and sensitive to small input perturbations that might cause a significant change in the output. In this letter, we employ contraction theory to improve the robustness of neural ODEs (NODEs). A dynamical system is contractive if all solutions with different initial conditions converge to each other exponentially fast. As a consequence, perturbations in initial conditions become less and less relevant over time. Since in NODEs the input data corresponds to the initial condition of dynamical systems, we show contractivity can mitigate the effect of input perturbations. More precisely, inspired by NODEs with Hamiltonian dynamics, we propose a class of contractive Hamiltonian NODEs (CH-NODEs). By properly tuning a scalar parameter, CH-NODEs ensure contractivity by design and can be trained using standard backpropagation. Moreover, CH-NODEs enjoy built-in guarantees of non-exploding gradients, which ensure a well-posed training process. Finally, we demonstrate the robustness of CH-NODEs on the MNIST image classification problem with noisy test data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "30426382",
                        "name": "M. Zakwan"
                    },
                    {
                        "authorId": "151484848",
                        "name": "Liang Xu"
                    },
                    {
                        "authorId": "1404093319",
                        "name": "G. Ferrari-Trecate"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "fbe5e38d08568565634e228a35942c8d2306dbf6",
                "externalIds": {
                    "ArXiv": "2203.11546",
                    "DOI": "10.1016/j.eml.2022.101925",
                    "CorpusId": 247597058
                },
                "corpusId": 247597058,
                "publicationVenue": {
                    "id": "83cc7477-4e74-4bda-be77-f63728aebc31",
                    "name": "Extreme Mechanics Letters",
                    "type": "journal",
                    "alternate_names": [
                        "Extreme Mech Lett"
                    ],
                    "issn": "2352-4316",
                    "url": "https://www.journals.elsevier.com/extreme-mechanics-letters",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/23524316"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fbe5e38d08568565634e228a35942c8d2306dbf6",
                "title": "Rapidly encoding generalizable dynamics in a Euclidean symmetric neural network",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108511478",
                        "name": "Qiaofeng Li"
                    },
                    {
                        "authorId": "2118914662",
                        "name": "Tianyi Wang"
                    },
                    {
                        "authorId": "1686063",
                        "name": "V. Roychowdhury"
                    },
                    {
                        "authorId": "144667695",
                        "name": "M. Jawed"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In recent years, the use of physical loss functions has proven beneficial for the training procedure, yielding substantial improvements over purely supervised training approaches (Tompson et al., 2017; Wu & Tegmark, 2019; Greydanus et al., 2019).",
                "Additional works have shown the advantages of physical loss formulations (Greydanus et al., 2019; Cranmer et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "49228c2db9ba4d622f9351c83d67a3a86b22f3ec",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-10131",
                    "ArXiv": "2203.10131",
                    "DOI": "10.48550/arXiv.2203.10131",
                    "CorpusId": 247595088
                },
                "corpusId": 247595088,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/49228c2db9ba4d622f9351c83d67a3a86b22f3ec",
                "title": "Half-Inverse Gradients for Physical Deep Learning",
                "abstract": "Recent works in deep learning have shown that integrating differentiable physics simulators into the training process can greatly improve the quality of results. Although this combination represents a more complex optimization task than supervised neural network training, the same gradient-based optimizers are typically employed to minimize the loss function. However, the integrated physics solvers have a profound effect on the gradient flow as manipulating scales in magnitude and direction is an inherent property of many physical processes. Consequently, the gradient flow is often highly unbalanced and creates an environment in which existing gradient-based optimizers perform poorly. In this work, we analyze the characteristics of both physical and neural network optimizations to derive a new method that does not suffer from this phenomenon. Our method is based on a half-inversion of the Jacobian and combines principles of both classical network and physics optimizers to solve the combined optimization task. Compared to state-of-the-art neural network optimizers, our method converges more quickly and yields better solutions, which we demonstrate on three complex learning problems involving nonlinear oscillators, the Schroedinger equation and the Poisson problem.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2004343490",
                        "name": "Patrick Schnell"
                    },
                    {
                        "authorId": "13094542",
                        "name": "P. Holl"
                    },
                    {
                        "authorId": "2125721010",
                        "name": "Nils Thuerey"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Neural ODEs allow training continuous deep neural networks in an end-to-end manner, which have been successfully applied in numerous tasks, such as density estimation [2, 3], time-series modeling [4, 5], physics-based models [6, 7] and some others [8, 9, 10]."
            ],
            "citingPaper": {
                "paperId": "9fcb6ef4acbb13a8cc84cc3c8f83b270632ba0f3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-05103",
                    "ArXiv": "2203.05103",
                    "DOI": "10.48550/arXiv.2203.05103",
                    "CorpusId": 247362931
                },
                "corpusId": 247362931,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9fcb6ef4acbb13a8cc84cc3c8f83b270632ba0f3",
                "title": "Improving Neural ODEs via Knowledge Distillation",
                "abstract": "Neural Ordinary Differential Equations (Neural ODEs) construct the continuous dynamics of hidden units using ordinary differential equations specified by a neural network, demonstrating promising results on many tasks. However, Neural ODEs still do not perform well on image recognition tasks. The possible reason is that the one-hot encoding vector commonly used in Neural ODEs can not provide enough supervised information. We propose a new training based on knowledge distillation to construct more powerful and robust Neural ODEs fitting image recognition tasks. Specially, we model the training of Neural ODEs into a teacher-student learning process, in which we propose ResNets as the teacher model to provide richer supervised information. The experimental results show that the new training manner can improve the classification accuracy of Neural ODEs by 24% on CIFAR10 and 5% on SVHN. In addition, we also quantitatively discuss the effect of both knowledge distillation and time horizon in Neural ODEs on robustness against adversarial examples. The experimental analysis concludes that introducing the knowledge distillation and increasing the time horizon can improve the robustness of Neural ODEs against adversarial examples.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "118549640",
                        "name": "Haoyu Chu"
                    },
                    {
                        "authorId": "46730712",
                        "name": "Shikui Wei"
                    },
                    {
                        "authorId": "2117522973",
                        "name": "Qiming Lu"
                    },
                    {
                        "authorId": "2143397594",
                        "name": "Yao Zhao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "0706441f4a7bddaba85a7689c66e32d7c7ae31a3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-01874",
                    "ArXiv": "2203.01874",
                    "DOI": "10.1109/TAI.2022.3179681",
                    "CorpusId": 247222725
                },
                "corpusId": 247222725,
                "publicationVenue": {
                    "id": "3c27e831-750f-45bc-9914-2148a5259eba",
                    "name": "IEEE Transactions on Artificial Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Artif Intell"
                    ],
                    "issn": "2691-4581",
                    "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9078688"
                },
                "url": "https://www.semanticscholar.org/paper/0706441f4a7bddaba85a7689c66e32d7c7ae31a3",
                "title": "Thermodynamics-informed graph neural networks",
                "abstract": "In this paper we present a deep learning method to predict the temporal evolution of dissipative dynamic systems. We propose using both geometric and thermodynamic inductive biases to improve accuracy and generalization of the resulting integration scheme. The first is achieved with Graph Neural Networks, which induces a non-Euclidean geometrical prior with permutation invariant node and edge update functions. The second bias is forced by learning the GENERIC structure of the problem, an extension of the Hamiltonian formalism, to model more general non-conservative dynamics. Several examples are provided in both Eulerian and Lagrangian description in the context of fluid and solid mechanics respectively, achieving relative mean errors of less than 3% in all the tested examples. Two ablation studies are provided based on recent works in both physics-informed and geometric deep learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2157042146",
                        "name": "Quercus Hern'andez"
                    },
                    {
                        "authorId": "2157041576",
                        "name": "Alberto Bad'ias"
                    },
                    {
                        "authorId": "2734584",
                        "name": "F. Chinesta"
                    },
                    {
                        "authorId": "3246447",
                        "name": "E. Cueto"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "The idea of learning Hamiltonian dynamics by machine learning models dates back to the 1990s [Howse et al., 1995; Seung et al., 1997], recently a ground-breaking study of learning a deep neural network for Hamiltonian, Hamiltonian Neural Networks (HNN) [Greydanus et al., 2019] emerges.",
                "Method Hamiltonian Type Loss Form Input Form Separablity Assumption Integration HNN [Greydanus et al., 2019] Standard Pointwise Canonical/Pixel No Euler DHNN [Greydanus and Sosanya, 2022] Generalized Pointwise Canonical No Euler GHNN [Course et al.",
                "\u2026[Zhong et al., 2021] is perhaps the most relevant to our work, as it benchmarks ten energy-conserving neural network models, including HNN [Greydanus et al., 2019], SymODEN [Zhong et al., 2019] and CHNN [Celledoni et al., 2022], and also Lagrangian models like Deep Lagrange Network\u2026",
                ", 2021] is perhaps the most relevant to our work, as it benchmarks ten energy-conserving neural network models, including HNN [Greydanus et al., 2019], SymODEN [Zhong et al.",
                ", 1997], recently a ground-breaking study of learning a deep neural network for Hamiltonian, Hamiltonian Neural Networks (HNN) [Greydanus et al., 2019] emerges."
            ],
            "citingPaper": {
                "paperId": "ae54927b924debd9343e6b93fca40dd58295dacb",
                "externalIds": {
                    "ArXiv": "2203.00128",
                    "DBLP": "journals/corr/abs-2203-00128",
                    "DOI": "10.48550/arXiv.2203.00128",
                    "CorpusId": 247187915
                },
                "corpusId": 247187915,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ae54927b924debd9343e6b93fca40dd58295dacb",
                "title": "Learning Neural Hamiltonian Dynamics: A Methodological Overview",
                "abstract": "The past few years have witnessed an increased interest in learning Hamiltonian dynamics in deep learning frameworks. As an inductive bias based on physical laws, Hamiltonian dynamics endow neural networks with accurate long-term prediction, interpretability, and data-efficient learning. However, Hamiltonian dynamics also bring energy conservation or dissipation assumptions on the input data and additional computational overhead. In this paper, we systematically survey recently proposed Hamiltonian neural network models, with a special emphasis on methodologies. In general, we discuss the major contributions of these models, and compare them in four overlapping directions: 1) generalized Hamiltonian system; 2) symplectic integration, 3) generalized input form, and 4) extended problem settings. We also provide an outlook of the fundamental challenges and emerging opportunities in this area.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2141931645",
                        "name": "Zhijie Chen"
                    },
                    {
                        "authorId": "2067624423",
                        "name": "Mingquan Feng"
                    },
                    {
                        "authorId": "3063894",
                        "name": "Junchi Yan"
                    },
                    {
                        "authorId": "145203884",
                        "name": "H. Zha"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recently NODEs are beginning to be deployed in system identification tasks [19], [20], [21], [22]."
            ],
            "citingPaper": {
                "paperId": "bd10c7eddff12566bbbadaf8db0aa53987356281",
                "externalIds": {
                    "ArXiv": "2203.00120",
                    "DBLP": "conf/amcc/RahmanDTS22",
                    "DOI": "10.23919/ACC53348.2022.9867586",
                    "CorpusId": 247187685
                },
                "corpusId": 247187685,
                "publicationVenue": {
                    "id": "fe4d09f8-d278-4bfb-b73a-1a6a0e22f6a3",
                    "name": "American Control Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Adv Comput Control",
                        "ACC",
                        "Advances in Computing and Communications",
                        "Adv Comput Commun",
                        "Am Control Conf",
                        "Advances in Computer and Communication",
                        "International Conference on Advanced Computer Control"
                    ],
                    "issn": "2767-2875",
                    "url": "http://a2c2.org/conferences/american-control-conferences",
                    "alternate_urls": [
                        "http://www.acc-rajagiri.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bd10c7eddff12566bbbadaf8db0aa53987356281",
                "title": "Neural Ordinary Differential Equations for Nonlinear System Identification",
                "abstract": "Neural ordinary differential equations (NODE) have been recently proposed as a promising approach for nonlinear system identification tasks. In this work, we systematically compare their predictive performance with current state-of-the-art nonlinear and classical linear methods. In particular, we present a quantitative study comparing NODE\u2019s performance against neural state-space models and classical linear system identification methods. We evaluate the inference speed and prediction performance of each method on open-loop errors across eight different dynamical systems. The experiments show that NODEs can consistently improve the prediction accuracy by an order of magnitude compared to benchmark methods. Besides improved accuracy, we also observed that NODEs are less sensitive to hyperparameters compared to neural state-space models. On the other hand, these performance gains come with a slight increase of computation at the inference time.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145093338",
                        "name": "Aowabin Rahman"
                    },
                    {
                        "authorId": "2403282",
                        "name": "J\u00e1n Drgo\u0148a"
                    },
                    {
                        "authorId": "145655596",
                        "name": "Aaron Tuor"
                    },
                    {
                        "authorId": "1387586666",
                        "name": "J. Strube"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Several architectures and methods regarding graph networks were proposed for leaning and inference of dynamics of physical systems [Battaglia et al., 2016, Greydanus et al., 2019, Sanchez-Gonzalez et al., 2018, Chang et al., 2016]."
            ],
            "citingPaper": {
                "paperId": "1b0040c1b8538b48d4db4a8b48d00450cfe863f3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-13775",
                    "ArXiv": "2202.13775",
                    "DOI": "10.1016/j.ijmecsci.2022.107835",
                    "CorpusId": 247158582
                },
                "corpusId": 247158582,
                "publicationVenue": {
                    "id": "afc91e07-d0c2-47d5-a923-18eea5079def",
                    "name": "International Journal of Mechanical Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Mech Sci"
                    ],
                    "issn": "0020-7403",
                    "url": "https://www.journals.elsevier.com/international-journal-of-mechanical-sciences",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00207403"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1b0040c1b8538b48d4db4a8b48d00450cfe863f3",
                "title": "Learning the nonlinear dynamics of soft mechanical metamaterials with graph networks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "89799181",
                        "name": "Tianju Xue"
                    },
                    {
                        "authorId": "3109395",
                        "name": "S. Adriaenssens"
                    },
                    {
                        "authorId": "2056610127",
                        "name": "S. Mao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "One can model the driving force of the system directly [11, 13], focus on the Hamiltonian [14, 15], or the Lagrangian [6, 16]."
            ],
            "citingPaper": {
                "paperId": "9355f1c0117dc4cb9b5fb24803083f702433ac24",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-11447",
                    "ArXiv": "2202.11447",
                    "DOI": "10.1088/1367-2630/ac7c2d",
                    "CorpusId": 247058313
                },
                "corpusId": 247058313,
                "publicationVenue": {
                    "id": "8a4f69c8-3ddc-4669-921a-79403732a17e",
                    "name": "New Journal of Physics",
                    "type": "journal",
                    "alternate_names": [
                        "New J Phys"
                    ],
                    "issn": "1367-2630",
                    "url": "http://iopscience.iop.org/1367-2630",
                    "alternate_urls": [
                        "https://iopscience.iop.org/journal/1367-2630",
                        "http://njp.org/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9355f1c0117dc4cb9b5fb24803083f702433ac24",
                "title": "Reconstruction of observed mechanical motions with artificial intelligence tools",
                "abstract": "The goal of this paper is to determine the laws of observed trajectories assuming that there is a mechanical system in the background and using these laws to continue the observed motion in a plausible way. The laws are represented by neural networks with a limited number of parameters. The training of the networks follows the extreme learning machine idea. We determine laws for different levels of embedding, thus we can represent not only the equation of motion but also the symmetries of different kinds. In the recursive numerical evolution of the system, we require the fulfillment of all the observed laws, within the determined numerical precision. In this way, we can successfully reconstruct both integrable and chaotic motions, as we demonstrate in the example of the gravity pendulum and the double pendulum.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "15823876",
                        "name": "A. Jakov\u00e1c"
                    },
                    {
                        "authorId": "98510344",
                        "name": "M. T. Kurbucz"
                    },
                    {
                        "authorId": "102897395",
                        "name": "P. P\u00f3sfay"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "At the same time, the deep learning community has developed powerful tools by adapting physical modeling concepts to deep learning, for instance the Neural ODE approach was proposed for the modeling of continuous transformations ([42]), stable neural architectures were also developed by embedding neural networks with invariant structures ([36, 50, 48])."
            ],
            "citingPaper": {
                "paperId": "412a25f5a0f98a01b53a0b9f2c845de4e2ee26c2",
                "externalIds": {
                    "ArXiv": "2202.10746",
                    "DOI": "10.1016/j.cma.2023.115985",
                    "CorpusId": 253734732
                },
                "corpusId": 253734732,
                "publicationVenue": {
                    "id": "3bfaa538-a67d-47d7-bfda-6f82748e9a29",
                    "name": "Computer Methods in Applied Mechanics and Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Method Appl Mech Eng"
                    ],
                    "issn": "0045-7825",
                    "url": "https://www.journals.elsevier.com/computer-methods-in-applied-mechanics-and-engineering",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00457825"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/412a25f5a0f98a01b53a0b9f2c845de4e2ee26c2",
                "title": "CD-ROM: Complemented Deep - Reduced order model",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155649051",
                        "name": "Emmanuel Menier"
                    },
                    {
                        "authorId": "89936237",
                        "name": "M. Bucci"
                    },
                    {
                        "authorId": "26631868",
                        "name": "Mouadh Yagoubi"
                    },
                    {
                        "authorId": "3083383",
                        "name": "L. Mathelin"
                    },
                    {
                        "authorId": "2066691430",
                        "name": "M. Schoenauer"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "As an example, inspired by HNN [11], HOGN [25] models the evolution of interacting systems by Hamiltonian equations to obtain energy conservation."
            ],
            "citingPaper": {
                "paperId": "124ec00db85c0cc7059f6aba76764d22d3f5d463",
                "externalIds": {
                    "DBLP": "conf/nips/Han0XR22",
                    "ArXiv": "2202.10643",
                    "CorpusId": 247025701
                },
                "corpusId": 247025701,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/124ec00db85c0cc7059f6aba76764d22d3f5d463",
                "title": "Equivariant Graph Hierarchy-Based Neural Networks",
                "abstract": "Equivariant Graph neural Networks (EGNs) are powerful in characterizing the dynamics of multi-body physical systems. Existing EGNs conduct flat message passing, which, yet, is unable to capture the spatial/dynamical hierarchy for complex systems particularly, limiting substructure discovery and global information fusion. In this paper, we propose Equivariant Hierarchy-based Graph Networks (EGHNs) which consist of the three key components: generalized Equivariant Matrix Message Passing (EMMP) , E-Pool and E-UpPool. In particular, EMMP is able to improve the expressivity of conventional equivariant message passing, E-Pool assigns the quantities of the low-level nodes into high-level clusters, while E-UpPool leverages the high-level information to update the dynamics of the low-level nodes. As their names imply, both E-Pool and E-UpPool are guaranteed to be equivariant to meet physic symmetry. Considerable experimental evaluations verify the effectiveness of our EGHN on several applications including multi-object dynamics simulation, motion capture, and protein dynamics modeling.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2161440",
                        "name": "J. Han"
                    },
                    {
                        "authorId": "48537464",
                        "name": "Yu Rong"
                    },
                    {
                        "authorId": "1754673",
                        "name": "Tingyang Xu"
                    },
                    {
                        "authorId": "2323566",
                        "name": "Fuchun Sun"
                    },
                    {
                        "authorId": "123175679",
                        "name": "Wen-bing Huang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "3 can automatically capture continuous dynamics [39, 40, 41, 42, 43]."
            ],
            "citingPaper": {
                "paperId": "0dfa7e909940fb1840831614e593714a5544ec02",
                "externalIds": {
                    "ArXiv": "2202.08494",
                    "DBLP": "journals/corr/abs-2202-08494",
                    "CorpusId": 246904572
                },
                "corpusId": 246904572,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0dfa7e909940fb1840831614e593714a5544ec02",
                "title": "Learning continuous models for continuous physics",
                "abstract": "Dynamical systems that evolve continuously over time are ubiquitous throughout science and engineering. Machine learning (ML) provides data-driven approaches to model and predict the dynamics of such systems. A core issue with this approach is that ML models are typically trained on discrete data, using ML methodologies that are not aware of underlying continuity properties, which results in models that often do not capture the underlying continuous dynamics of a system of interest. As a result, these ML models are of limited use for for many scientific and engineering applications. To address this challenge, we develop a convergence test based on numerical analysis theory. Our test verifies whether a model has learned a function that accurately approximates a system's underlying continuous dynamics. Models that fail this test fail to capture relevant dynamics, rendering them of limited utility for many scientific prediction tasks; while models that pass this test enable both better interpolation and better extrapolation in multiple ways. Our results illustrate how principled numerical analysis methods can be coupled with existing ML training/testing methodologies to validate models for science and engineering applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "15408096",
                        "name": "A. Krishnapriyan"
                    },
                    {
                        "authorId": "40897456",
                        "name": "A. Queiruga"
                    },
                    {
                        "authorId": "2371914",
                        "name": "N. Benjamin Erichson"
                    },
                    {
                        "authorId": "1717098",
                        "name": "Michael W. Mahoney"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "More recent neural models learn Hamiltonians directly (Greydanus et al., 2019) or indirectly (Sanchez-Gonzalez et al.",
                "More recent neural models learn Hamiltonians directly (Greydanus et al., 2019) or indirectly (Sanchez-Gonzalez et al., 2019; Cranmer et al., 2020; Finzi et al., 2020b).",
                "This has been exploited by the ML4Physics community with Lagrangian Neural Networks (Greydanus et al., 2019), and with classical solvers this is the basis of the finite volume method and symplectic integrators."
            ],
            "citingPaper": {
                "paperId": "ebd042387a2dde05ab18b23820030841bb671966",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-07643",
                    "ArXiv": "2202.07643",
                    "CorpusId": 246863584
                },
                "corpusId": 246863584,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ebd042387a2dde05ab18b23820030841bb671966",
                "title": "Lie Point Symmetry Data Augmentation for Neural PDE Solvers",
                "abstract": "Neural networks are increasingly being used to solve partial differential equations (PDEs), replacing slower numerical solvers. However, a critical issue is that neural PDE solvers require high-quality ground truth data, which usually must come from the very solvers they are designed to replace. Thus, we are presented with a proverbial chicken-and-egg problem. In this paper, we present a method, which can partially alleviate this problem, by improving neural PDE solver sample complexity -- Lie point symmetry data augmentation (LPSDA). In the context of PDEs, it turns out that we are able to quantitatively derive an exhaustive list of data transformations, based on the Lie point symmetry group of the PDEs in question, something not possible in other application areas. We present this framework and demonstrate how it can easily be deployed to improve neural PDE solver sample complexity by an order of magnitude.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "78843496",
                        "name": "Johannes Brandstetter"
                    },
                    {
                        "authorId": "1678311",
                        "name": "M. Welling"
                    },
                    {
                        "authorId": "3471551",
                        "name": "Daniel E. Worrall"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "3f8dae850dfc1163990f9b513164b42908515a08",
                "externalIds": {
                    "ArXiv": "2202.04836",
                    "DBLP": "journals/corr/abs-2202-04836",
                    "CorpusId": 246706127
                },
                "corpusId": 246706127,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3f8dae850dfc1163990f9b513164b42908515a08",
                "title": "Deconstructing the Inductive Biases of Hamiltonian Neural Networks",
                "abstract": "Physics-inspired neural networks (NNs), such as Hamiltonian or Lagrangian NNs, dramatically outperform other learned dynamics models by leveraging strong inductive biases. These models, however, are challenging to apply to many real world systems, such as those that don't conserve energy or contain contacts, a common setting for robotics and reinforcement learning. In this paper, we examine the inductive biases that make physics-inspired models successful in practice. We show that, contrary to conventional wisdom, the improved generalization of HNNs is the result of modeling acceleration directly and avoiding artificial complexity from the coordinate system, rather than symplectic structure or energy conservation. We show that by relaxing the inductive biases of these models, we can match or exceed performance on energy-conserving systems while dramatically improving performance on practical, non-conservative systems. We extend this approach to constructing transition models for common Mujoco environments, showing that our model can appropriately balance inductive biases with the flexibility required for model-based control.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "13297813",
                        "name": "Nate Gruver"
                    },
                    {
                        "authorId": "51007156",
                        "name": "Marc Finzi"
                    },
                    {
                        "authorId": "2067201658",
                        "name": "S. Stanton"
                    },
                    {
                        "authorId": "145771261",
                        "name": "A. Wilson"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a6eb9ecef16437bc76efbe0b016cea3bbdbf8f87",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-04109",
                    "ArXiv": "2202.04109",
                    "DOI": "10.1609/aaai.v37i7.26007",
                    "CorpusId": 246679857
                },
                "corpusId": 246679857,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a6eb9ecef16437bc76efbe0b016cea3bbdbf8f87",
                "title": "Learning Similarity Metrics for Volumetric Simulations with Multiscale CNNs",
                "abstract": "Simulations that produce three-dimensional data are ubiquitous in science, ranging from fluid flows to plasma physics. We propose a similarity model based on entropy, which allows for the creation of physically meaningful ground truth distances for the similarity assessment of scalar and vectorial data, produced from transport and motion-based simulations. Utilizing two data acquisition methods derived from this model, we create collections of fields from numerical PDE solvers and existing simulation data repositories. Furthermore, a multiscale CNN architecture that computes a volumetric similarity metric (VolSiM) is proposed. To the best of our knowledge this is the first learning method inherently designed to address the challenges arising for the similarity assessment of high-dimensional simulation data. Additionally, the tradeoff between a large batch size and an accurate correlation computation for correlation-based loss functions is investigated, and the metric's invariance with respect to rotation and scale operations is analyzed. Finally, the robustness and generalization of VolSiM is evaluated on a large range of test data, as well as a particularly challenging turbulence case study, that is close to potential real-world applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2084296966",
                        "name": "Georg Kohl"
                    },
                    {
                        "authorId": "91908601",
                        "name": "Li-Wei Chen"
                    },
                    {
                        "authorId": "2125721010",
                        "name": "Nils Thuerey"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ff09e4fd2dfe64b0deffe501c2eb6977d8071cc4",
                "externalIds": {
                    "PubMedCentral": "8814210",
                    "DOI": "10.1038/s41598-022-05696-9",
                    "CorpusId": 246529297,
                    "PubMed": "35115591"
                },
                "corpusId": 246529297,
                "publicationVenue": {
                    "id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                    "name": "Scientific Reports",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Rep"
                    ],
                    "issn": "2045-2322",
                    "url": "http://www.nature.com/srep/",
                    "alternate_urls": [
                        "http://www.nature.com/srep/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ff09e4fd2dfe64b0deffe501c2eb6977d8071cc4",
                "title": "Classification of regular and chaotic motions in Hamiltonian systems with deep learning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2854024",
                        "name": "A. Celletti"
                    },
                    {
                        "authorId": "145572720",
                        "name": "C. Gale\u015f"
                    },
                    {
                        "authorId": "1403319981",
                        "name": "V. Rodr\u00edguez-Fern\u00e1ndez"
                    },
                    {
                        "authorId": "1712566",
                        "name": "M. Vasile"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "51b718c3b198db408b90aa6da21402b71c477b24",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-00299",
                    "PubMedCentral": "9802333",
                    "ArXiv": "2202.00299",
                    "DOI": "10.1093/pnasnexus/pgac264",
                    "CorpusId": 246441886,
                    "PubMed": "36712322"
                },
                "corpusId": 246441886,
                "publicationVenue": {
                    "id": "0473502f-71fa-4945-8e3a-5c0618a4ec32",
                    "name": "PNAS Nexus",
                    "type": "journal",
                    "issn": "2752-6542",
                    "url": "https://academic.oup.com/pnasnexus"
                },
                "url": "https://www.semanticscholar.org/paper/51b718c3b198db408b90aa6da21402b71c477b24",
                "title": "Learning physics-consistent particle interactions",
                "abstract": "Abstract Interacting particle systems play a key role in science and engineering. Access to the governing particle interaction law is fundamental for a complete understanding of such systems. However, the inherent system complexity keeps the particle interaction hidden in many cases. Machine learning methods have the potential to learn the behavior of interacting particle systems by combining experiments with data analysis methods. However, most existing algorithms focus on learning the kinetics at the particle level. Learning pairwise interaction, e.g., pairwise force or pairwise potential energy, remains an open challenge. Here, we propose an algorithm that adapts the Graph Networks framework, which contains an edge part to learn the pairwise interaction and a node part to model the dynamics at particle level. Different from existing approaches that use neural networks in both parts, we design a deterministic operator in the node part that allows to precisely infer the pairwise interactions that are consistent with underlying physical laws by only being trained to predict the particle acceleration. We test the proposed methodology on multiple datasets and demonstrate that it achieves superior performance in inferring correctly the pairwise interactions while also being consistent with the underlying physics on all the datasets. While the previously proposed approaches are able to be applied as simulators, they fail to infer physically consistent particle interactions that satisfy Newton\u2019s laws. Moreover, the proposed physics-induced graph network for particle interaction also outperforms the other baseline models in terms of generalization ability to larger systems and robustness to significant levels of noise. The developed methodology can support a better understanding and discovery of the underlying particle interaction laws, and hence, guide the design of materials with targeted properties.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40592359",
                        "name": "Zhichao Han"
                    },
                    {
                        "authorId": "32670344",
                        "name": "David S. Kammer"
                    },
                    {
                        "authorId": "2757308",
                        "name": "Olga Fink"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a9d19372562fb6e5bd31feba5c12feb435acee99",
                "externalIds": {
                    "ArXiv": "2202.01889",
                    "DBLP": "journals/corr/abs-2202-01889",
                    "CorpusId": 246608231
                },
                "corpusId": 246608231,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a9d19372562fb6e5bd31feba5c12feb435acee99",
                "title": "Generalizing to New Physical Systems via Context-Informed Dynamics Model",
                "abstract": "Data-driven approaches to modeling physical systems fail to generalize to unseen systems that share the same general dynamics with the learning domain, but correspond to different physical contexts. We propose a new framework for this key problem, context-informed dynamics adaptation (CoDA), which takes into account the distributional shift across systems for fast and efficient adaptation to new dynamics. CoDA leverages multiple environments, each associated to a different dynamic, and learns to condition the dynamics model on contextual parameters, specific to each environment. The conditioning is performed via a hypernetwork, learned jointly with a context vector from observed data. The proposed formulation constrains the search hypothesis space to foster fast adaptation and better generalization across environments. We theoretically motivate our approach and show state-of-the-art generalization results on a set of nonlinear dynamics, representative of a variety of application domains. We also show, on these systems, that new system parameters can be inferred from context vectors with minimal supervision. Code is available at https://github.com/yuan-yin/CoDA .",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2583878",
                        "name": "Matthieu Kirchmeyer"
                    },
                    {
                        "authorId": "2109472874",
                        "name": "Yuan Yin"
                    },
                    {
                        "authorId": "1853488882",
                        "name": "J\u00e9r\u00e9mie Don\u00e0"
                    },
                    {
                        "authorId": "1800361",
                        "name": "Nicolas Baskiotis"
                    },
                    {
                        "authorId": "1792962",
                        "name": "A. Rakotomamonjy"
                    },
                    {
                        "authorId": "1741426",
                        "name": "P. Gallinari"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "8c81d3e19ed73d565ade3a34f16d20908eaf02ad",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-13254",
                    "ArXiv": "2201.13254",
                    "DOI": "10.1016/j.cam.2022.114608",
                    "CorpusId": 246430715
                },
                "corpusId": 246430715,
                "publicationVenue": {
                    "id": "0dd1e980-f66a-40ab-9f04-626424b1a7d8",
                    "name": "Journal of Computational and Applied Mathematics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Appl Math"
                    ],
                    "issn": "0377-0427",
                    "alternate_issns": [
                        "0771-050X"
                    ],
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/505613/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/03770427",
                        "https://www.journals.elsevier.com/journal-of-computational-and-applied-mathematics/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8c81d3e19ed73d565ade3a34f16d20908eaf02ad",
                "title": "Learning Hamiltonians of constrained mechanical systems",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2791391",
                        "name": "E. Celledoni"
                    },
                    {
                        "authorId": "48618060",
                        "name": "A. Leone"
                    },
                    {
                        "authorId": "2082098916",
                        "name": "Davide Murari"
                    },
                    {
                        "authorId": "1868160",
                        "name": "B. Owren"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "bb782d04b0c9b4f1f1a28eb43c1347f2d9f937d5",
                "externalIds": {
                    "ArXiv": "2201.11969",
                    "DBLP": "journals/corr/abs-2201-11969",
                    "CorpusId": 246411585
                },
                "corpusId": 246411585,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/bb782d04b0c9b4f1f1a28eb43c1347f2d9f937d5",
                "title": "Approximately Equivariant Networks for Imperfectly Symmetric Dynamics",
                "abstract": "Incorporating symmetry as an inductive bias into neural network architecture has led to improvements in generalization, data efficiency, and physical consistency in dynamics modeling. Methods such as CNNs or equivariant neural networks use weight tying to enforce symmetries such as shift invariance or rotational equivariance. However, despite the fact that physical laws obey many symmetries, real-world dynamical data rarely conforms to strict mathematical symmetry either due to noisy or incomplete data or to symmetry breaking features in the underlying dynamical system. We explore approximately equivariant networks which are biased towards preserving symmetry but are not strictly constrained to do so. By relaxing equivariance constraints, we find that our models can outperform both baselines with no symmetry bias and baselines with overly strict symmetry in both simulated turbulence domains and real-world multi-stream jet flow.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151036763",
                        "name": "Rui Wang"
                    },
                    {
                        "authorId": "153401894",
                        "name": "R. Walters"
                    },
                    {
                        "authorId": "2023052",
                        "name": "Rose Yu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "601f5986c0ea500ba17280ce73d5fe92428ad44b",
                "externalIds": {
                    "DBLP": "journals/apin/QianT22",
                    "DOI": "10.1007/s10489-021-02902-5",
                    "CorpusId": 246204559
                },
                "corpusId": 246204559,
                "publicationVenue": {
                    "id": "2c01e8b4-1888-4cf0-91e6-6b1213cdca16",
                    "name": "Applied intelligence (Boston)",
                    "type": "journal",
                    "alternate_names": [
                        "Applied Intelligence",
                        "Appl Intell",
                        "Appl intell t"
                    ],
                    "issn": "0924-669X",
                    "url": "https://link.springer.com/journal/10489"
                },
                "url": "https://www.semanticscholar.org/paper/601f5986c0ea500ba17280ce73d5fe92428ad44b",
                "title": "Data-driven physical law learning model for chaotic robot dynamics prediction",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48015884",
                        "name": "Kui Qian"
                    },
                    {
                        "authorId": "2115832570",
                        "name": "Lei Tian"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "This work is generalized to include all forms of Lagrangians in Cranmer et al. (2020) and extended to Hamiltonian mechanics in Greydanus et al. (2019)."
            ],
            "citingPaper": {
                "paperId": "470377f9b3ad6447ac2c2239324ba42932539f3d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-08281",
                    "ArXiv": "2201.08281",
                    "CorpusId": 246063521
                },
                "corpusId": 246063521,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/470377f9b3ad6447ac2c2239324ba42932539f3d",
                "title": "Symplectic Momentum Neural Networks - Using Discrete Variational Mechanics as a prior in Deep Learning",
                "abstract": "With deep learning gaining attention from the research community for prediction and control of real physical systems, learning important representations is becoming now more than ever mandatory. It is of extreme importance that deep learning representations are coherent with physics. When learning from discrete data this can be guaranteed by including some sort of prior into the learning, however, not all discretization priors preserve important structures from the physics. In this paper, we introduce Symplectic Momentum Neural Networks (SyMo) as models from a discrete formulation of mechanics for non-separable mechanical systems. The combination of such formulation leads SyMos to be constrained towards preserving important geometric structures such as momentum and a symplectic form and learn from limited data. Furthermore, it allows to learn dynamics only from the poses as training data. We extend SyMos to include variational integrators within the learning framework by developing an implicit root-find layer which leads to End-to-End Symplectic Momentum Neural Networks (E2E-SyMo). Through experimental results, using the pendulum and cartpole, we show that such combination not only allows these models to learn from limited data but also provides the models with the capability of preserving the symplectic form and show better long-term behaviour.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143864538",
                        "name": "Saul Santos"
                    },
                    {
                        "authorId": "35956718",
                        "name": "Monica Ekal"
                    },
                    {
                        "authorId": "145187072",
                        "name": "R. Ventura"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "44c0f92d2daf9c90939f132a0f0ffef84405d28a",
                "externalIds": {
                    "DOI": "10.3390/app12031050",
                    "CorpusId": 246202422
                },
                "corpusId": 246202422,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/44c0f92d2daf9c90939f132a0f0ffef84405d28a",
                "title": "Certifiable AI",
                "abstract": "Implicit stochastic models, including both \u2018deep neural networks\u2019 (dNNs) and the more recent unsupervised foundational models, cannot be explained. That is, it cannot be determined how they work, because the interactions of the millions or billions of terms that are contained in their equations cannot be captured in the form of a causal model. Because users of stochastic AI systems would like to understand how they operate in order to be able to use them safely and reliably, there has emerged a new field called \u2018explainable AI\u2019 (XAI). When we examine the XAI literature, however, it becomes apparent that its protagonists have redefined the term \u2018explanation\u2019 to mean something else, namely: \u2018interpretation\u2019. Interpretations are indeed sometimes possible, but we show that they give at best only a subjective understanding of how a model works. We propose an alternative to XAI, namely certified AI (CAI), and describe how an AI can be specified, realized, and tested in order to become certified. The resulting approach combines ontologies and formal logic with statistical learning to obtain reliable AI systems which can be safely used in technical applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "120743060",
                        "name": "J. Landgrebe"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "We can make further progress if we take advantage of the knowledge about the underlying data generation process and encode such information in the neural network architectures and the loss functions [49, 88, 40, 70]."
            ],
            "citingPaper": {
                "paperId": "9a3668275cf463d9862b63780d36ab4a281ccd00",
                "externalIds": {
                    "DBLP": "journals/siamsc/MengZDK22",
                    "ArXiv": "2201.05475",
                    "DOI": "10.1137/22m1472206",
                    "CorpusId": 245986526
                },
                "corpusId": 245986526,
                "publicationVenue": {
                    "id": "0e3b51a7-21d8-477c-8918-14a55f087532",
                    "name": "SIAM Journal on Scientific Computing",
                    "type": "journal",
                    "alternate_names": [
                        "SIAM J Sci Comput"
                    ],
                    "issn": "1064-8275",
                    "url": "http://www.siam.org/journals/sisc.php",
                    "alternate_urls": [
                        "https://epubs.siam.org/journal/sjoce3"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9a3668275cf463d9862b63780d36ab4a281ccd00",
                "title": "SympOCnet: Solving optimal control problems with applications to high-dimensional multi-agent path planning problems",
                "abstract": "Solving high-dimensional optimal control problems in real-time is an important but challenging problem, with applications to multi-agent path planning problems, which have drawn increased attention given the growing popularity of drones in recent years. In this paper, we propose a novel neural network method called SympOCnet that applies the Symplectic network to solve high-dimensional optimal control problems with state constraints. We present several numerical results on path planning problems in two-dimensional and three-dimensional spaces. Specifically, we demonstrate that our SympOCnet can solve a problem with more than 500 dimensions in 1.5 hours on a single GPU, which shows the effectiveness and efficiency of SympOCnet. The proposed method is scalable and has the potential to solve truly high-dimensional path planning problems in real-time.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "148220583",
                        "name": "Tingwei Meng"
                    },
                    {
                        "authorId": "2128201890",
                        "name": "Zhen Zhang"
                    },
                    {
                        "authorId": "2221048",
                        "name": "J. Darbon"
                    },
                    {
                        "authorId": "1720124",
                        "name": "G. Karniadakis"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "For example, recent works have used NODEs as a means to incorporate physics-based knowledge into the learning of dynamical systems [Djeumou et al., 2022; Menda et al., 2019; Gupta et al., 2020; Cranmer et al., 2020; Greydanus et al., 2019; Finzi et al., 2020; Zhong et al., 2021].",
                "For example, recent works have used NODEs as a means to incorporate physics-based knowledge into the learning of dynamical systems [Djeumou et al., 2022a; Menda et al., 2019; Gupta et al., 2020; Cranmer et al., 2020; Greydanus et al., 2019; Finzi et al., 2020; Zhong et al., 2021]."
            ],
            "citingPaper": {
                "paperId": "97546d8cfa57522899ab5d88700fdb2e107102ae",
                "externalIds": {
                    "DBLP": "conf/ijcai/DjeumouNGPT22",
                    "ArXiv": "2201.05715",
                    "DOI": "10.24963/ijcai.2022/405",
                    "CorpusId": 246015580
                },
                "corpusId": 246015580,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/97546d8cfa57522899ab5d88700fdb2e107102ae",
                "title": "Taylor-Lagrange Neural Ordinary Differential Equations: Toward Fast Training and Evaluation of Neural ODEs",
                "abstract": "Neural ordinary differential equations (NODEs) -- parametrizations of differential equations using neural networks -- have shown tremendous promise in learning models of unknown continuous-time dynamical systems from data. However, every forward evaluation of a NODE requires numerical integration of the neural network used to capture the system dynamics, making their training prohibitively expensive. Existing works rely on off-the-shelf adaptive step-size numerical integration schemes, which often require an excessive number of evaluations of the underlying dynamics network to obtain sufficient accuracy for training. By contrast, we accelerate the evaluation and the training of NODEs by proposing a data-driven approach to their numerical integration. The proposed Taylor-Lagrange NODEs (TL-NODEs) use a fixed-order Taylor expansion for numerical integration, while also learning to estimate the expansion's approximation error. As a result, the proposed approach achieves the same accuracy as adaptive step-size schemes while employing only low-order Taylor expansions, thus greatly reducing the computational cost necessary to integrate the NODE. A suite of numerical experiments, including modeling dynamical systems, image classification, and density estimation, demonstrate that TL-NODEs can be trained more than an order of magnitude faster than state-of-the-art approaches, without any loss in performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1475901145",
                        "name": "Franck Djeumou"
                    },
                    {
                        "authorId": "1796254983",
                        "name": "Cyrus Neary"
                    },
                    {
                        "authorId": "1709823",
                        "name": "\u00c9. Goubault"
                    },
                    {
                        "authorId": "2307593",
                        "name": "S. Putot"
                    },
                    {
                        "authorId": "3199888",
                        "name": "U. Topcu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "b63eace4b98c8572b6e32187c4a0f362fa05cfb7",
                "externalIds": {
                    "ArXiv": "2201.04339",
                    "DBLP": "journals/corr/abs-2201-04339",
                    "CorpusId": 245877652
                },
                "corpusId": 245877652,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b63eace4b98c8572b6e32187c4a0f362fa05cfb7",
                "title": "Physics-guided Learning-based Adaptive Control on the SE(3) Manifold",
                "abstract": "In real-world robotics applications, accurate models of robot dynamics are critical for safe and stable control in rapidly changing operational conditions. This motivates the use of machine learning techniques to approximate robot dynamics and their disturbances over a training set of state-control trajectories. This paper demonstrates that inductive biases arising from physics laws can be used to improve the data efficiency and accuracy of the approximated dynamics model. For example, the dynamics of many robots, including ground, aerial, and underwater vehicles, are described using their $SE(3)$ pose and satisfy conservation of energy principles. We design a physically plausible model of the robot dynamics by imposing the structure of Hamilton's equations of motion in the design of a neural ordinary differential equation (ODE) network. The Hamiltonian structure guarantees satisfaction of $SE(3)$ kinematic constraints and energy conservation by construction. It also allows us to derive an energy-based adaptive controller that achieves trajectory tracking while compensating for disturbances. Our learning-based adaptive controller is verified on an under-actuated quadrotor robot.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2344318",
                        "name": "T. Duong"
                    },
                    {
                        "authorId": "50365495",
                        "name": "Nikolay A. Atanasov"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "791c3296f01da9b6cec66f97eb0093afa16a2020",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-14014",
                    "ArXiv": "2112.14014",
                    "CorpusId": 245537502
                },
                "corpusId": 245537502,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/791c3296f01da9b6cec66f97eb0093afa16a2020",
                "title": "An Error Analysis Framework for Neural Network Modeling of Dynamical Systems",
                "abstract": "We propose a theoretical framework for investigating a modeling error caused by numerical integration in the learning process of dynamics. Recently, learning equations of motion to describe dynamics from data using neural networks has been attracting attention. During such training, numerical integration is used to compare the data with the solution of the neural network model; however, discretization errors due to numerical integration prevent the model from being trained correctly. In this study, we formulate the modeling error using the Dahlquist test equation that is commonly used in the analysis of numerical methods and apply it to some of the Runge--Kutta methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1572323664",
                        "name": "Shunpei Terakawa"
                    },
                    {
                        "authorId": "144872058",
                        "name": "Takashi Matsubara"
                    },
                    {
                        "authorId": "2580628",
                        "name": "Takaharu Yaguchi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In contrast to Lagrangian Shadow Integration, these approaches typically assume that the symplectic structure of the system is known and assume that observations of momentum data is available in addition to position data: Hamiltonian Neural Networks [12] learn the Hamiltonian of a system from position and momentum data of trajectories."
            ],
            "citingPaper": {
                "paperId": "550db29f3020335931ff4292a8f2c15c9276859a",
                "externalIds": {
                    "DBLP": "journals/jcam/Ober-BlobaumO23",
                    "ArXiv": "2112.12619",
                    "DOI": "10.1016/j.cam.2022.114780",
                    "CorpusId": 250089444
                },
                "corpusId": 250089444,
                "publicationVenue": {
                    "id": "0dd1e980-f66a-40ab-9f04-626424b1a7d8",
                    "name": "Journal of Computational and Applied Mathematics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Appl Math"
                    ],
                    "issn": "0377-0427",
                    "alternate_issns": [
                        "0771-050X"
                    ],
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/505613/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/03770427",
                        "https://www.journals.elsevier.com/journal-of-computational-and-applied-mathematics/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/550db29f3020335931ff4292a8f2c15c9276859a",
                "title": "Variational learning of Euler-Lagrange dynamics from data",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1419469651",
                        "name": "S. Ober-Blobaum"
                    },
                    {
                        "authorId": "51299251",
                        "name": "Christian Offen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Takens theorem has been fundamental to several system identification methods for general dynamical systems already [24], recently also involvingmachine learning [25\u2013 28], with a focus on PDE [29, 30] or special structure such as (classical) Hamiltonian dynamics [31, 32]."
            ],
            "citingPaper": {
                "paperId": "4a73ee5cdfa5eeae377b1645e8f7858e9d84ac9d",
                "externalIds": {
                    "DBLP": "journals/qip/GutierrezDM23",
                    "ArXiv": "2112.09021",
                    "DOI": "10.1007/s11128-023-04008-y",
                    "CorpusId": 245218787
                },
                "corpusId": 245218787,
                "publicationVenue": {
                    "id": "f509d2f6-f64c-4a9d-a468-4d226978713f",
                    "name": "Quantum Information Processing",
                    "type": "journal",
                    "alternate_names": [
                        "Quantum Inf Process"
                    ],
                    "issn": "1570-0755",
                    "url": "https://link.springer.com/journal/11128"
                },
                "url": "https://www.semanticscholar.org/paper/4a73ee5cdfa5eeae377b1645e8f7858e9d84ac9d",
                "title": "Quantum process tomography of unitary maps from time-delayed measurements",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145753043",
                        "name": "Irene L'opez Guti'errez"
                    },
                    {
                        "authorId": "144399320",
                        "name": "Felix Dietrich"
                    },
                    {
                        "authorId": "47681680",
                        "name": "C. Mendl"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Zhong et al. (2021) further introduce an explicit module to handle contacts inside a Lagrangian or Hamiltonian system.",
                "Imposing Hamiltonian (Greydanus et al., 2019; Sanchez-Gonzalez et al., 2019; Chen et al., 2019) and Lagrangian (Lutter et al., 2019; Cranmer et al., 2020; Finzi et al., 2020) mechanics in learned simulators offers unique speed/accuracy tradeoffs and can preserve symmetries more effectively.",
                "Imposing Hamiltonian (Greydanus et al., 2019; Sanchez-Gonzalez et al., 2019; Chen et al., 2019) and Lagrangian (Lutter et al."
            ],
            "citingPaper": {
                "paperId": "e0ee02a573b3d83fec55ed5d7c80f1afa055a7b4",
                "externalIds": {
                    "ArXiv": "2112.09161",
                    "DBLP": "journals/corr/abs-2112-09161",
                    "CorpusId": 245329451
                },
                "corpusId": 245329451,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e0ee02a573b3d83fec55ed5d7c80f1afa055a7b4",
                "title": "Constraint-based graph network simulator",
                "abstract": "In the area of physical simulations, nearly all neural-network-based methods directly predict future states from the input states. However, many traditional simulation engines instead model the constraints of the system and select the state which satis\ufb01es them. Here we present a framework for constraint-based learned simulation, where a scalar constraint function is implemented as a graph neural network, and future predictions are computed by solving the optimization prob-lem de\ufb01ned by the learned constraint. Our model achieves comparable or better accuracy to top learned simulators on a variety of challenging physical domains, and offers several unique advantages. We can improve the simulation accuracy on a larger system by applying more solver iterations at test time. We also can incorporate novel hand-designed constraints at test time and simulate new dynamics which were not present in the training data. Our constraint-based framework shows how key techniques from traditional simulation and numerical methods can be leveraged as inductive biases in machine learning simulators.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40959192",
                        "name": "Yulia Rubanova"
                    },
                    {
                        "authorId": "1398105826",
                        "name": "Alvaro Sanchez-Gonzalez"
                    },
                    {
                        "authorId": "2054956",
                        "name": "T. Pfaff"
                    },
                    {
                        "authorId": "2019153",
                        "name": "P. Battaglia"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In contrast to [6], [8], [11], [12] we take the prior physical knowledge as external input into account which rather guides the PGNN than forces it to strictly obey a priori presumed dynamics.",
                "Similar to the notion of incorporating physical relations as an additional loss term, there are also approaches that exclusively rely on a physical loss function as error function evaluated on estimated and targeted outputs, such as the Hamiltonian function [12], the Lagrangian formulation of a system [11], [15] or the governing equations [6]."
            ],
            "citingPaper": {
                "paperId": "63d38b16f00d3103741b6dc3c4361488f48eaeed",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-08148",
                    "ArXiv": "2112.08148",
                    "DOI": "10.1109/AIRC56195.2022.9836982",
                    "CorpusId": 245144607
                },
                "corpusId": 245144607,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/63d38b16f00d3103741b6dc3c4361488f48eaeed",
                "title": "Composed Physics- and Data-driven System Identification for Non-autonomous Systems in Control Engineering",
                "abstract": "In control design most control strategies are model-based and require accurate models to be applied successfully. Due to simplifications and the model-reality-gap physics-derived models frequently exhibit deviations from real-world-systems. Likewise, purely data-driven methods often do not generalise well enough and may violate physical laws. Recently Physics-Guided Neural Networks (PGNN) and physics-inspired loss functions separately have shown promising results to conquer these drawbacks. In this contribution we extend existing methods towards the identification of non-autonomous systems and propose a combined approach PGNN-L, which uses a PGNN and a physics-inspired loss term (-L) to successfully identify the system's dynamics, while maintaining the consistency with physical laws. The proposed method is demonstrated on two real-world nonlinear systems and outperforms existing techniques regarding complexity and reliability.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2148665126",
                        "name": "Ricarda-Samantha G\u00f6tte"
                    },
                    {
                        "authorId": "39977156",
                        "name": "Julia Timmermann"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Neural networks can parametrize the equations of motion for physical systems, which can have conserved quantities resulting from symmetries [4, 5].",
                "[4] Sam Greydanus, Misko Dzamba, and Jason Yosinski, \u201cHamiltonian neural networks,\u201d (2019), arXiv:1906."
            ],
            "citingPaper": {
                "paperId": "763c6f289683b4fcd1e460fd9aa3c01020a0f51f",
                "externalIds": {
                    "ArXiv": "2112.05722",
                    "DOI": "10.1103/PhysRevD.105.096031",
                    "CorpusId": 245117306
                },
                "corpusId": 245117306,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/763c6f289683b4fcd1e460fd9aa3c01020a0f51f",
                "title": "Symmetry discovery with deep learning",
                "abstract": "What are the symmetries of a dataset? Whereas the symmetries of an individual data element can be characterized by its invariance under various transformations, the symmetries of an ensemble of data elements are ambiguous due to Jacobian factors introduced while changing coordinates. In this paper, we provide a rigorous statistical definition of the symmetries of a dataset, which involves inertial reference densities, in analogy to inertial frames in classical mechanics. We then propose SymmetryGAN as a novel and powerful approach to automatically discover symmetries using a deep learning method based on generative adversarial networks (GANs). When applied to Gaussian examples, SymmetryGAN shows excellent empirical performance, in agreement with expectations from the analytic loss landscape. SymmetryGAN is then applied to simulated dijet events from the Large Hadron Collider (LHC) to demonstrate the potential utility of this method in high energy collider physics applications. Going beyond symmetry discovery, we consider procedures to infer the underlying symmetry group from empirical data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1885318652",
                        "name": "Krish Desai"
                    },
                    {
                        "authorId": "3085579",
                        "name": "B. Nachman"
                    },
                    {
                        "authorId": "49534895",
                        "name": "J. Thaler"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "c094e07cc5d76e3facb963ad8061dc74595b28ed",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-04307",
                    "ArXiv": "2112.04307",
                    "CorpusId": 244954605
                },
                "corpusId": 244954605,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c094e07cc5d76e3facb963ad8061dc74595b28ed",
                "title": "Physics-informed dynamic mode decomposition (piDMD)",
                "abstract": "In this work, we demonstrate how physical principles -- such as symmetries, invariances, and conservation laws -- can be integrated into the dynamic mode decomposition (DMD). DMD is a widely-used data analysis technique that extracts low-rank modal structures and dynamics from high-dimensional measurements. However, DMD frequently produces models that are sensitive to noise, fail to generalize outside the training data, and violate basic physical laws. Our physics-informed DMD (piDMD) optimization, which may be formulated as a Procrustes problem, restricts the family of admissible models to a matrix manifold that respects the physical structure of the system. We focus on five fundamental physical principles -- conservation, self-adjointness, localization, causality, and shift-invariance -- and derive several closed-form solutions and efficient algorithms for the corresponding piDMD optimizations. With fewer degrees of freedom, piDMD models are less prone to overfitting, require less training data, and are often less computationally expensive to build than standard DMD models. We demonstrate piDMD on a range of challenging problems in the physical sciences, including energy-preserving fluid flow, travelling-wave systems, the Schr\\\"odinger equation, solute advection-diffusion, a system with causal dynamics, and three-dimensional transitional channel flow. In each case, piDMD significantly outperforms standard DMD in metrics such as spectral identification, state prediction, and estimation of optimal forcings and responses.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "83535783",
                        "name": "Peter J. Baddoo"
                    },
                    {
                        "authorId": "108252626",
                        "name": "Benjam\u00edn Herrmann"
                    },
                    {
                        "authorId": "3190206",
                        "name": "B. McKeon"
                    },
                    {
                        "authorId": "144484982",
                        "name": "J. Kutz"
                    },
                    {
                        "authorId": "3083169",
                        "name": "S. Brunton"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "This trend already began several years ago with the emergence of physics-guided machine learning [26] and the creation of specific network structures that represent known physical systems [18, 27, 28].",
                "In recent years, a new field emerged in the Machine Learning community to tackle the generalization issue of neural networks and create new NN architectures bound to follow given physical laws, such as Hamiltonian NNs [28] or Lagrangian NNs [27], later generalized by Djeumou et al.",
                "In recent years, a new field emerged in the Machine Learning community to tackle the generalization issue of neural networks and create new NN architectures bound to follow given physical laws, such as Hamiltonian NNs [28] or Lagrangian NNs [27], later generalized by Djeumou et al. [18]."
            ],
            "citingPaper": {
                "paperId": "23f4c82747f905f383f82cb030be23aca52ad2c3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-03212",
                    "ArXiv": "2112.03212",
                    "DOI": "10.1016/j.apenergy.2022.119806",
                    "CorpusId": 244908748
                },
                "corpusId": 244908748,
                "publicationVenue": {
                    "id": "00a6fb8b-7f32-4ae0-a417-8033f5f369f9",
                    "name": "Applied Energy",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Energy"
                    ],
                    "issn": "0306-2619",
                    "url": "https://www.journals.elsevier.com/applied-energy",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/03062619"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/23f4c82747f905f383f82cb030be23aca52ad2c3",
                "title": "Physically Consistent Neural Networks for building thermal modeling: theory and analysis",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2143195427",
                        "name": "Loris Di Natale"
                    },
                    {
                        "authorId": "2378010",
                        "name": "B. Svetozarevic"
                    },
                    {
                        "authorId": "134579497",
                        "name": "Philipp Heer"
                    },
                    {
                        "authorId": "2115201230",
                        "name": "C. Jones"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "0fabd55d41e116f50b02e6e3144adaa4fe84591f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-03383",
                    "ArXiv": "2112.03383",
                    "DOI": "10.1063/5.0083060",
                    "CorpusId": 244920677,
                    "PubMed": "35428386"
                },
                "corpusId": 244920677,
                "publicationVenue": {
                    "id": "1bb63b2b-3f57-4387-aaf6-b2a33dfcdcc5",
                    "name": "Journal of Chemical Physics",
                    "type": "journal",
                    "alternate_names": [
                        "J Chem Phys"
                    ],
                    "issn": "0021-9606",
                    "url": "http://jcp.aip.org/",
                    "alternate_urls": [
                        "https://aip.scitation.org/journal/jcp"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0fabd55d41e116f50b02e6e3144adaa4fe84591f",
                "title": "Graph Neural Networks Accelerated Molecular Dynamics",
                "abstract": "Molecular Dynamics (MD) simulation is a powerful tool for understanding the dynamics and structure of matter. Since the resolution of MD is atomic-scale, achieving long timescale simulations with femtosecond integration is very expensive. In each MD step, numerous iterative computations are performed to calculate energy based on different types of interaction and their corresponding spatial gradients. These repetitive computations can be learned and surrogated by a deep learning model, such as a Graph Neural Network (GNN). In this work, we developed a GNN Accelerated MD (GAMD) model that directly predicts forces, given the state of the system (atom positions, atom types), bypassing the evaluation of potential energy. By training the GNN on a variety of data sources (simulation data derived from classical MD and density functional theory), we show that GAMD can predict the dynamics of two typical molecular systems, Lennard-Jones system and water system, in the NVT ensemble with velocities regulated by a thermostat. We further show that GAMD's learning and inference are agnostic to the scale, where it can scale to much larger systems at test time. We also perform a comprehensive benchmark test comparing our implementation of GAMD to production-level MD software, showing GAMD's competitive performance on the large-scale simulation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1752871628",
                        "name": "Zijie Li"
                    },
                    {
                        "authorId": "1999900316",
                        "name": "Kazem Meidani"
                    },
                    {
                        "authorId": "1752533621",
                        "name": "Prakarsh Yadav"
                    },
                    {
                        "authorId": "3614493",
                        "name": "A. Farimani"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recent works have encoded Hamiltonian and Lagrangian mechanics into neural models [22, 30, 11, 19], with gains in data-efficiency in physical and robotics systems, including some modeling controlled or dissipative systems [60, 15].",
                "Given the parameters in [22] the energy is H = 3(1 \u2212 cos q) + p(2), with p(2) \u2212 3 \u00b7 cos q being a simpler equivalent.",
                "[22] propose the setting of an ideal spring and ideal pendulum, which will allow us to understand the behavior of Noether Networks for scientific data where we know a useful conserved quantity: the energy."
            ],
            "citingPaper": {
                "paperId": "44ab05013cbfa2012985cbba8d51bfeeabc45db4",
                "externalIds": {
                    "ArXiv": "2112.03321",
                    "DBLP": "conf/nips/AletDZTKF21",
                    "CorpusId": 244462384
                },
                "corpusId": 244462384,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/44ab05013cbfa2012985cbba8d51bfeeabc45db4",
                "title": "Noether Networks: Meta-Learning Useful Conserved Quantities",
                "abstract": "Progress in machine learning (ML) stems from a combination of data availability, computational resources, and an appropriate encoding of inductive biases. Useful biases often exploit symmetries in the prediction problem, such as convolutional networks relying on translation equivariance. Automatically discovering these useful symmetries holds the potential to greatly improve the performance of ML systems, but still remains a challenge. In this work, we focus on sequential prediction problems and take inspiration from Noether's theorem to reduce the problem of finding inductive biases to meta-learning useful conserved quantities. We propose Noether Networks: a new type of architecture where a meta-learned conservation loss is optimized inside the prediction function. We show, theoretically and experimentally, that Noether Networks improve prediction quality, providing a general framework for discovering inductive biases in sequential problems.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "27587911",
                        "name": "Ferran Alet"
                    },
                    {
                        "authorId": "2135026719",
                        "name": "Dylan D. Doblar"
                    },
                    {
                        "authorId": "2064472884",
                        "name": "Allan Zhou"
                    },
                    {
                        "authorId": "1763295",
                        "name": "J. Tenenbaum"
                    },
                    {
                        "authorId": "1392876047",
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Greydanus et al. (2019) use NNs to predict Hamiltonian from phase-space coordinates s = (p,q) and their derivatives."
            ],
            "citingPaper": {
                "paperId": "71ab40c75c75d41f1780436682723699a828d72c",
                "externalIds": {
                    "ArXiv": "2112.01641",
                    "DBLP": "conf/nips/0008S22",
                    "CorpusId": 252873675
                },
                "corpusId": 252873675,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/71ab40c75c75d41f1780436682723699a828d72c",
                "title": "Hamiltonian Latent Operators for content and motion disentanglement in image sequences",
                "abstract": "We introduce \\textit{HALO} -- a deep generative model utilising HAmiltonian Latent Operators to reliably disentangle content and motion information in image sequences. The \\textit{content} represents summary statistics of a sequence, and \\textit{motion} is a dynamic process that determines how information is expressed in any part of the sequence. By modelling the dynamics as a Hamiltonian motion, important desiderata are ensured: (1) the motion is reversible, (2) the symplectic, volume-preserving structure in phase space means paths are continuous and are not divergent in the latent space. Consequently, the nearness of sequence frames is realised by the nearness of their coordinates in the phase space, which proves valuable for disentanglement and long-term sequence generation. The sequence space is generally comprised of different types of dynamical motions. To ensure long-term separability and allow controlled generation, we associate every motion with a unique Hamiltonian that acts in its respective subspace. We demonstrate the utility of \\textit{HALO} by swapping the motion of a pair of sequences, controlled generation, and image rotations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2149889657",
                        "name": "Asif Khan"
                    },
                    {
                        "authorId": "1728216",
                        "name": "A. Storkey"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "7502bf121c7103660b3f7ff1b5033b705263c246",
                "externalIds": {
                    "DBLP": "conf/nips/FinziBW21",
                    "ArXiv": "2112.01388",
                    "CorpusId": 244799202
                },
                "corpusId": 244799202,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7502bf121c7103660b3f7ff1b5033b705263c246",
                "title": "Residual Pathway Priors for Soft Equivariance Constraints",
                "abstract": "There is often a trade-off between building deep learning systems that are expressive enough to capture the nuances of the reality, and having the right inductive biases for efficient learning. We introduce Residual Pathway Priors (RPPs) as a method for converting hard architectural constraints into soft priors, guiding models towards structured solutions, while retaining the ability to capture additional complexity. Using RPPs, we construct neural network priors with inductive biases for equivariances, but without limiting flexibility. We show that RPPs are resilient to approximate or misspecified symmetries, and are as effective as fully constrained models even when symmetries are exact. We showcase the broad applicability of RPPs with dynamical systems, tabular data, and reinforcement learning. In Mujoco locomotion tasks, where contact forces and directional rewards violate strict equivariance assumptions, the RPP outperforms baseline model-free RL agents, and also improves the learned transition models for model-based RL.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51007156",
                        "name": "Marc Finzi"
                    },
                    {
                        "authorId": "2031420788",
                        "name": "Gregory W. Benton"
                    },
                    {
                        "authorId": "145771261",
                        "name": "A. Wilson"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Hamiltonian neural network [1] Hamiltonian neural networks learn the Hamiltonian instead of the ode."
            ],
            "citingPaper": {
                "paperId": "beea82abc52e851956666e5a5bd3d83323f5e508",
                "externalIds": {
                    "DOI": "10.1002/pamm.202100116",
                    "CorpusId": 245233488
                },
                "corpusId": 245233488,
                "publicationVenue": {
                    "id": "4168c391-0e2d-498e-bcfb-07c8bce9218b",
                    "name": "Pamm",
                    "type": "journal"
                },
                "url": "https://www.semanticscholar.org/paper/beea82abc52e851956666e5a5bd3d83323f5e508",
                "title": "Learning Mechanical Systems by Hamiltonian Neural Networks",
                "abstract": "The great success of machine learning in image processing and related fields also motivated its application to dynamical system identification. In particular, neural networks are trained to learn equations of motion and thus provide an alternative to first\u2010principle modeling. While these black\u2010box algorithms are quite flexible regarding the system structure, they often have difficulties in learning basic physics laws which are intrinsic system properties. Recently, Hamiltonian neural networks (HNN) were introduced to explicitly learn the total energy of a system in order to overcome the lack of physical rules. However, Hamiltonian systems often also contain other structures such as symmetries in terms of further invariances. In this contribution, we extend HNN such they respect system invariances in addition to the Hamiltonian. The proposed extension leads to a trade off between energy conservation and conservation of invariance properties, which we investigate exemplarily.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2090843477",
                        "name": "Eva Dierkes"
                    },
                    {
                        "authorId": "2462599",
                        "name": "K. Fla\u00dfkamp"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Thereafter, Greydanus [23] built a neural network to parameterize the Hamiltonian and learn it directly from data."
            ],
            "citingPaper": {
                "paperId": "5b06c6d063a5c8fe21b3f1141dc5d7bccd0ef7ff",
                "externalIds": {
                    "DOI": "10.1007/s10409-021-01151-6",
                    "CorpusId": 246760931
                },
                "corpusId": 246760931,
                "publicationVenue": {
                    "id": "35c72f2e-4d19-47a9-9d6e-9e7bd93d979d",
                    "name": "Acta Mechanica Sinica",
                    "type": "journal",
                    "alternate_names": [
                        "Acta Mech Sin"
                    ],
                    "issn": "0459-1879",
                    "alternate_issns": [
                        "0567-7718"
                    ],
                    "url": "http://www.springer.com/engineering/journal/10409",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10409"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5b06c6d063a5c8fe21b3f1141dc5d7bccd0ef7ff",
                "title": "Aerodynamic modeling using an end-to-end learning attitude dynamics network for flight control",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9232618",
                        "name": "Tun Zhao"
                    },
                    {
                        "authorId": "2116718468",
                        "name": "Gong Chen"
                    },
                    {
                        "authorId": "2144550576",
                        "name": "Xiao Wang"
                    },
                    {
                        "authorId": "31040202",
                        "name": "Enmi Yong"
                    },
                    {
                        "authorId": "9929542",
                        "name": "W. Qian"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e4362bbfe42f4f5048ca52ef47494009c40e5497",
                "externalIds": {
                    "DBLP": "journals/neco/ItohIM22",
                    "DOI": "10.1162/neco_a_01465",
                    "CorpusId": 245286576,
                    "PubMed": "34915580"
                },
                "corpusId": 245286576,
                "publicationVenue": {
                    "id": "69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
                    "name": "Neural Computation",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Comput"
                    ],
                    "issn": "0899-7667",
                    "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6720226",
                        "http://www.mitpressjournals.org/loi/neco",
                        "https://www.mitpressjournals.org/loi/neco"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e4362bbfe42f4f5048ca52ef47494009c40e5497",
                "title": "Implicit Contact Dynamics Modeling With Explicit Inertia Matrix Representation for Real-Time, Model-Based Control in Physical Environment",
                "abstract": "Abstract Model-based control has great potential for use in real robots due to its high sampling efficiency. Nevertheless, dealing with physical contacts and generating accurate motions are inevitable for practical robot control tasks, such as precise manipulation. For a real-time, model-based approach, the difficulty of contact-rich tasks that requires precise movement lies in the fact that a model needs to accurately predict forthcoming contact events within a limited length of time rather than detect them afterward with sensors. Therefore, in this study, we investigate whether and how neural network models can learn a task-related model useful enough for model-based control, that is, a model predicting future states, including contact events. To this end, we propose a structured neural network model predictive control (SNN-MPC) method, whose neural network architecture is designed with explicit inertia matrix representation. To train the proposed network, we develop a two-stage modeling procedure for contact-rich dynamics from a limited number of samples. As a contact-rich task, we take up a trackball manipulation task using a physical 3-DoF finger robot. The results showed that the SNN-MPC outperformed MPC with a conventional fully connected network model on the manipulation task.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2072440367",
                        "name": "Takeshi D. Itoh"
                    },
                    {
                        "authorId": "144946848",
                        "name": "K. Ishihara"
                    },
                    {
                        "authorId": "50414121",
                        "name": "J. Morimoto"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Another type of models including HNN [10], SRNN [11], GFNN [12] and others [15\u201319], manged to incorporate physical knowledge and solve mechanical problems by learning Hamiltonian or other physical quantities."
            ],
            "citingPaper": {
                "paperId": "385f4b919171f35824f45469a4de56cc915b7970",
                "externalIds": {
                    "ArXiv": "2111.15176",
                    "DBLP": "journals/corr/abs-2111-15176",
                    "CorpusId": 244729236
                },
                "corpusId": 244729236,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/385f4b919171f35824f45469a4de56cc915b7970",
                "title": "Learning Large-Time-Step Molecular Dynamics with Graph Neural Networks",
                "abstract": "Molecular dynamics (MD) simulation predicts the trajectory of atoms by solving Newton's equation of motion with a numeric integrator. Due to physical constraints, the time step of the integrator need to be small to maintain sufficient precision. This limits the efficiency of simulation. To this end, we introduce a graph neural network (GNN) based model, MDNet, to predict the evolution of coordinates and momentum with large time steps. In addition, MDNet can easily scale to a larger system, due to its linear complexity with respect to the system size. We demonstrate the performance of MDNet on a 4000-atom system with large time steps, and show that MDNet can predict good equilibrium and transport properties, well aligned with standard MD simulations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2053658872",
                        "name": "Tian Zheng"
                    },
                    {
                        "authorId": "2153577134",
                        "name": "Weihao Gao"
                    },
                    {
                        "authorId": "2146309022",
                        "name": "Chong Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "To compare with existing works, we use the same initial condition as Greydanus et al. [2019]. We carry out the simulation for 1,000 time steps, and WH-NIH yields a relative energy error of dE/E0 \u223c 10\u22129 by the end of the simulation. This result is about 7 orders of magnitude more accurate comparing to Greydanus et al. [2019].",
                "Following Greydanus et al. [2019], we use a simple multi-layer perceptron (MLP) backbone network Hinter,\u03b8 to serve as a function approximator of Hinter.",
                "To compare with existing works, we use the same initial condition as Greydanus et al. [2019]. We carry out the simulation for 1,000 time steps, and WH-NIH yields a relative energy error of dE/E0 \u223c 10\u22129 by the end of the simulation."
            ],
            "citingPaper": {
                "paperId": "b7930e96318bf45b213b535f15a7a2cbf655f842",
                "externalIds": {
                    "ArXiv": "2111.15631",
                    "DBLP": "journals/corr/abs-2111-15631",
                    "CorpusId": 244729473
                },
                "corpusId": 244729473,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b7930e96318bf45b213b535f15a7a2cbf655f842",
                "title": "Neural Symplectic Integrator with Hamiltonian Inductive Bias for the Gravitational $N$-body Problem",
                "abstract": "The gravitational $N$-body problem, which is fundamentally important in astrophysics to predict the motion of $N$ celestial bodies under the mutual gravity of each other, is usually solved numerically because there is no known general analytical solution for $N>2$. Can an $N$-body problem be solved accurately by a neural network (NN)? Can a NN observe long-term conservation of energy and orbital angular momentum? Inspired by Wistom&Holman (1991)'s symplectic map, we present a neural $N$-body integrator for splitting the Hamiltonian into a two-body part, solvable analytically, and an interaction part that we approximate with a NN. Our neural symplectic $N$-body code integrates a general three-body system for $10^{5}$ steps without diverting from the ground truth dynamics obtained from a traditional $N$-body integrator. Moreover, it exhibits good inductive bias by successfully predicting the evolution of $N$-body systems that are no part of the training set.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "15590614",
                        "name": "M. Cai"
                    },
                    {
                        "authorId": "1843702",
                        "name": "S. Zwart"
                    },
                    {
                        "authorId": "29980657",
                        "name": "Damian Podareanu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "f2d8432ee59fe6832635cad9cbdbc0f1efd1916e",
                "externalIds": {
                    "DOI": "10.3390/electronics10232963",
                    "CorpusId": 244753676
                },
                "corpusId": 244753676,
                "publicationVenue": {
                    "id": "ccd8e532-73c6-414f-bc91-271bbb2933e2",
                    "name": "Electronics",
                    "type": "journal",
                    "issn": "1450-5843",
                    "alternate_issns": [
                        "2079-9292",
                        "0883-4989"
                    ],
                    "url": "http://www.electronics.etfbl.net/",
                    "alternate_urls": [
                        "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-247562",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-247562",
                        "https://www.mdpi.com/journal/electronics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f2d8432ee59fe6832635cad9cbdbc0f1efd1916e",
                "title": "End-Effector Force and Joint Torque Estimation of a 7-DoF Robotic Manipulator Using Deep Learning",
                "abstract": "When a mobile robotic manipulator interacts with other robots, people, or the environment in general, the end-effector forces need to be measured to assess if a task has been completed successfully. Traditionally used force or torque estimation methods are usually based on observers, which require knowledge of the robot dynamics. Contrary to this, our approach involves two methods based on deep neural networks: robot end-effector force estimation and joint torque estimation. These methods require no knowledge of robot dynamics and are computationally effective but require a force sensor under the robot base. Several different architectures were considered for the tasks, and the best ones were identified among those tested. First, the data for training the networks were obtained in simulation. The trained networks showed reasonably good performance, especially using the LSTM architecture (with a root mean squared error (RMSE) of 0.1533 N for end-effector force estimation and 0.5115 Nm for joint torque estimation). Afterward, data were collected on a real Franka Emika Panda robot and then used to train the same networks for joint torque estimation. The obtained results are slightly worse than in simulation (0.5115 Nm vs. 0.6189 Nm, according to the RMSE metric) but still reasonably good, showing the validity of the proposed approach.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "23701134",
                        "name": "Stanko Kru\u017ei\u0107"
                    },
                    {
                        "authorId": "2728418",
                        "name": "J. Musi\u0107"
                    },
                    {
                        "authorId": "3352088",
                        "name": "R. Kamnik"
                    },
                    {
                        "authorId": "2403147",
                        "name": "V. Papi\u0107"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "b4ecde129fb11af90e691f4bc1f023f23a6f950b",
                "externalIds": {
                    "ArXiv": "2111.14053",
                    "DBLP": "journals/corr/abs-2111-14053",
                    "CorpusId": 244714755
                },
                "corpusId": 244714755,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b4ecde129fb11af90e691f4bc1f023f23a6f950b",
                "title": "Towards Conditional Generation of Minimal Action Potential Pathways for Molecular Dynamics",
                "abstract": "In this paper, we utilized generative models, and reformulate it for problems in molecular dynamics (MD) simulation, by introducing an MD potential energy component to our generative model. By incorporating potential energy as calculated from TorchMD into a conditional generative framework, we attempt to construct a low-potential energy route of transformation between the helix~$\\rightarrow$~coil structures of a protein. We show how to add an additional loss function to conditional generative models, motivated by potential energy of molecular configurations, and also present an optimization technique for such an augmented loss function. Our results show the benefit of this additional loss term on synthesizing realistic molecular trajectories.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1413478602",
                        "name": "J. Cava"
                    },
                    {
                        "authorId": "91231641",
                        "name": "J. Vant"
                    },
                    {
                        "authorId": "2069439842",
                        "name": "Nicholas Ho"
                    },
                    {
                        "authorId": "2142456123",
                        "name": "Ankita Shulka"
                    },
                    {
                        "authorId": "143655174",
                        "name": "P. Turaga"
                    },
                    {
                        "authorId": "1786514",
                        "name": "Ross Maciejewski"
                    },
                    {
                        "authorId": "2217949",
                        "name": "A. Singharoy"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "are often governed by latent differential equations that can be discovered with data driven methods [7, 14, 49, 52]."
            ],
            "citingPaper": {
                "paperId": "17773066cf244e82503e7283dcf1981bfe4ef0a6",
                "externalIds": {
                    "ArXiv": "2111.13207",
                    "DBLP": "journals/corr/abs-2111-13207",
                    "CorpusId": 244709251
                },
                "corpusId": 244709251,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/17773066cf244e82503e7283dcf1981bfe4ef0a6",
                "title": "Characteristic Neural Ordinary Differential Equations",
                "abstract": "We propose Characteristic-Neural Ordinary Differential Equations (C-NODEs), a framework for extending Neural Ordinary Differential Equations (NODEs) beyond ODEs. While NODEs model the evolution of a latent variables as the solution to an ODE, C-NODE models the evolution of the latent variables as the solution of a family of first-order quasi-linear partial differential equations (PDEs) along curves on which the PDEs reduce to ODEs, referred to as characteristic curves. This in turn allows the application of the standard frameworks for solving ODEs, namely the adjoint method. Learning optimal characteristic curves for given tasks improves the performance and computational efficiency, compared to state of the art NODE models. We prove that the C-NODE framework extends the classical NODE on classification tasks by demonstrating explicit C-NODE representable functions not expressible by NODEs. Additionally, we present C-NODE-based continuous normalizing flows, which describe the density evolution of latent variables along multiple dimensions. Empirical results demonstrate the improvements provided by the proposed method for classification and density estimation on CIFAR-10, SVHN, and MNIST datasets under a similar computational budget as the existing NODE methods. The results also provide empirical evidence that the learned curves improve the efficiency of the system through a lower number of parameters and function evaluations compared with baselines.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2152775979",
                        "name": "Xingzi Xu"
                    },
                    {
                        "authorId": "2065152192",
                        "name": "Ali Hasan"
                    },
                    {
                        "authorId": "1808335",
                        "name": "Khalil Elkhalil"
                    },
                    {
                        "authorId": "143798670",
                        "name": "Jie Ding"
                    },
                    {
                        "authorId": "1780864",
                        "name": "V. Tarokh"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "This work is similar to [27], where inductive biases based on the underlying physics laws are coded directly into the network."
            ],
            "citingPaper": {
                "paperId": "faac86f5b12c6fc7d141f60b35a54b35fe3d0c97",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-12066",
                    "ArXiv": "2111.12066",
                    "DOI": "10.1016/j.apenergy.2022.118852",
                    "CorpusId": 244488678
                },
                "corpusId": 244488678,
                "publicationVenue": {
                    "id": "00a6fb8b-7f32-4ae0-a417-8033f5f369f9",
                    "name": "Applied Energy",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Energy"
                    ],
                    "issn": "0306-2619",
                    "url": "https://www.journals.elsevier.com/applied-energy",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/03062619"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/faac86f5b12c6fc7d141f60b35a54b35fe3d0c97",
                "title": "Physics Informed Neural Networks for Control Oriented Thermal Modeling of Buildings",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2141630405",
                        "name": "Gargya Gokhale"
                    },
                    {
                        "authorId": "3239050",
                        "name": "B. Claessens"
                    },
                    {
                        "authorId": "2489892",
                        "name": "Chris Develder"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Physics-informed deep learning models: Deep learning models can enforce physical constraints partially through the loss function [28, 39, 40] or changes in neural network architecture [41]."
            ],
            "citingPaper": {
                "paperId": "c178795c4aa9d0ccc5aec98eb264032536e3c7f7",
                "externalIds": {
                    "ArXiv": "2111.11185",
                    "CorpusId": 244478225
                },
                "corpusId": 244478225,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c178795c4aa9d0ccc5aec98eb264032536e3c7f7",
                "title": "Incomplete to complete multiphysics forecasting -- a hybrid approach for learning unknown phenomena",
                "abstract": "Modeling complex dynamical systems with only partial knowledge of their physical mechanisms is a crucial problem across all scientific and engineering disciplines. Purely data-driven approaches, which only make use of an artificial neural network and data, often fail to accurately simulate the evolution of the system dynamics over a sufficiently long time and in a physically consistent manner. Therefore, we propose a hybrid approach that uses a neural network model in combination with an incomplete partial differential equations (PDE) solver that provides known, but incomplete physical information. In this study, we demonstrate that the results obtained from the incomplete PDEs can be efficiently corrected at every time step by the proposed hybrid neural network - PDE solver model, so that the effect of the unknown physics present in the system is correctly accounted for. For validation purposes, the obtained simulations of the hybrid model are successfully compared against results coming from the complete set of PDEs describing the full physics of the considered system. We demonstrate the validity of the proposed approach on a reactive flow, an archetypal multi-physics system that combines fluid mechanics and chemistry, the latter being the physics considered unknown. Experiments are made on planar and Bunsen-type flames at various operating conditions. The hybrid neural network - PDE approach correctly models the flame evolution of the cases under study for significantly long time windows, yields improved generalization, and allows for larger simulation time steps.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40900886",
                        "name": "Nilam Tathawadekar"
                    },
                    {
                        "authorId": "11426389",
                        "name": "N. Doan"
                    },
                    {
                        "authorId": "82005245",
                        "name": "Camilo F. Silva"
                    },
                    {
                        "authorId": "2125721010",
                        "name": "Nils Thuerey"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "To overcome these problems, we therefore combine latest advances in physics-enhanced Neural Networks based on dynamic invariants such as the Hamiltonian [1, 2, 3] or Lagrangian [4, 5] NNs with additional inductive bias to gain better predictive capabilities and to reduce the requisite training data.",
                "Greydanus [1] proposed to learn the Hamiltonian on the basis of N observations of the system\u2019s state variables z = (q,p) as well as their time derivatives \u017c = (q\u0307, \u1e57), i = 1, .",
                "The results of a black-box MLP model [1, 3] serve as baseline in Tables 1, 2."
            ],
            "citingPaper": {
                "paperId": "72a3e30782e715a633a35b423ac96036bd6897ab",
                "externalIds": {
                    "ArXiv": "2111.10329",
                    "DBLP": "journals/corr/abs-2111-10329",
                    "CorpusId": 244463107
                },
                "corpusId": 244463107,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/72a3e30782e715a633a35b423ac96036bd6897ab",
                "title": "Physics-enhanced Neural Networks in the Small Data Regime",
                "abstract": "Identifying the dynamics of physical systems requires a machine learning model that can assimilate observational data, but also incorporate the laws of physics. Neural Networks based on physical principles such as the Hamiltonian or Lagrangian NNs have recently shown promising results in generating extrapolative predictions and accurately representing the system's dynamics. We show that by additionally considering the actual energy level as a regularization term during training and thus using physical information as inductive bias, the results can be further improved. Especially in the case where only small amounts of data are available, these improvements can significantly enhance the predictive capability. We apply the proposed regularization term to a Hamiltonian Neural Network (HNN) and Constrained Hamiltonian Neural Network (CHHN) for a single and double pendulum, generate predictions under unseen initial conditions and report significant gains in predictive accuracy.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2141595883",
                        "name": "Jonas Eichelsd\u00f6rfer"
                    },
                    {
                        "authorId": "113831649",
                        "name": "Sebastian Kaltenbach"
                    },
                    {
                        "authorId": "2415520",
                        "name": "P. Koutsourelakis"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "77aedde9f05d4e203ac27086bb4913c3b4cbf5b8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-06011",
                    "ArXiv": "2111.06011",
                    "DOI": "10.1109/ICDM51629.2021.00033",
                    "CorpusId": 243985883
                },
                "corpusId": 243985883,
                "publicationVenue": {
                    "id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
                    "name": "Industrial Conference on Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Ind Conf Data Min",
                        "ICDM"
                    ],
                    "url": "http://www.data-mining-forum.de/"
                },
                "url": "https://www.semanticscholar.org/paper/77aedde9f05d4e203ac27086bb4913c3b4cbf5b8",
                "title": "Climate Modeling with Neural Diffusion Equations",
                "abstract": "Owing to the remarkable development of deep learning technology, there have been a series of efforts to build deep learning-based climate models. Whereas most of them utilize recurrent neural networks and/or graph neural networks, we design a novel climate model based on the two concepts, the neural ordinary differential equation (NODE) and the diffusion equation. Many physical processes involving a Brownian motion of particles can be described by the diffusion equation and as a result, it is widely used for modeling climate. On the other hand, neural ordinary differential equations (NODEs) are to learn a latent governing equation of ODE from data. In our presented method, we combine them into a single framework and propose a concept, called neural diffusion equation (NDE). Our NDE, equipped with the diffusion equation and one more additional neural network to model inherent uncertainty, can learn an appropriate latent governing equation that best describes a given climate dataset. In our experiments with two real-world and one synthetic datasets and eleven baselines, our method consistently outperforms existing baselines by non-trivial margins.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2305736",
                        "name": "JeeHyun Hwang"
                    },
                    {
                        "authorId": "67027632",
                        "name": "Jeongwhan Choi"
                    },
                    {
                        "authorId": "123881593",
                        "name": "Hwan-Kyu Choi"
                    },
                    {
                        "authorId": "2120214038",
                        "name": "Kookjin Lee"
                    },
                    {
                        "authorId": "2148665454",
                        "name": "Dongeun Lee"
                    },
                    {
                        "authorId": "5166698",
                        "name": "Noseong Park"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recently, a body of work has emerged that brings these well-established principles of modelling dynamics from physics \u2013 such as the conservation of energy, and numerical formulations from the theory of differential equations \u2013 to neural network architectures [49, 20, 9, 3, 58, 8, 4, 31, 50, 43, 56, 12, 17, 24, 14, 32, 11, 45, 57, 54, 48, 21]."
            ],
            "citingPaper": {
                "paperId": "785139d4fdc017fe43ee986ebd89d7bdb1211843",
                "externalIds": {
                    "ArXiv": "2111.05986",
                    "DBLP": "journals/corr/abs-2111-05986",
                    "CorpusId": 243985811
                },
                "corpusId": 243985811,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/785139d4fdc017fe43ee986ebd89d7bdb1211843",
                "title": "SyMetric: Measuring the Quality of Learnt Hamiltonian Dynamics Inferred from Vision",
                "abstract": "A recently proposed class of models attempts to learn latent dynamics from high-dimensional observations, like images, using priors informed by Hamiltonian mechanics. While these models have important potential applications in areas like robotics or autonomous driving, there is currently no good way to evaluate their performance: existing methods primarily rely on image reconstruction quality, which does not always reflect the quality of the learnt latent dynamics. In this work, we empirically highlight the problems with the existing measures and develop a set of new measures, including a binary indicator of whether the underlying Hamiltonian dynamics have been faithfully captured, which we call Symplecticity Metric or SyMetric. Our measures take advantage of the known properties of Hamiltonian dynamics and are more discriminative of the model's ability to capture the underlying dynamics than reconstruction error. Using SyMetric, we identify a set of architectural choices that significantly improve the performance of a previously proposed model for inferring latent dynamics from pixels, the Hamiltonian Generative Network (HGN). Unlike the original HGN, the new HGN++ is able to discover an interpretable phase space with physically meaningful latents on some datasets. Furthermore, it is stable for significantly longer rollouts on a diverse range of 13 datasets, producing rollouts of essentially infinite length both forward and backwards in time with no degradation in quality on a subset of the datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "39051054",
                        "name": "I. Higgins"
                    },
                    {
                        "authorId": "5721359",
                        "name": "Peter Wirnsberger"
                    },
                    {
                        "authorId": "2689633",
                        "name": "Andrew Jaegle"
                    },
                    {
                        "authorId": "3436640",
                        "name": "Aleksandar Botev"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "09865ffe9d8f9a0438100de39bcf6131fa55ce11",
                "externalIds": {
                    "DBLP": "conf/nips/BotevJWHH21",
                    "ArXiv": "2111.05458",
                    "CorpusId": 237368146
                },
                "corpusId": 237368146,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/09865ffe9d8f9a0438100de39bcf6131fa55ce11",
                "title": "Which priors matter? Benchmarking models for learning latent dynamics",
                "abstract": "Learning dynamics is at the heart of many important applications of machine learning (ML), such as robotics and autonomous driving. In these settings, ML algorithms typically need to reason about a physical system using high dimensional observations, such as images, without access to the underlying state. Recently, several methods have proposed to integrate priors from classical mechanics into ML models to address the challenge of physical reasoning from images. In this work, we take a sober look at the current capabilities of these models. To this end, we introduce a suite consisting of 17 datasets with visual observations based on physical systems exhibiting a wide range of dynamics. We conduct a thorough and detailed comparison of the major classes of physically inspired methods alongside several strong baselines. While models that incorporate physical priors can often learn latent spaces with desirable properties, our results demonstrate that these methods fail to significantly improve upon standard techniques. Nonetheless, we find that the use of continuous and time-reversible dynamics benefits models of all classes.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3436640",
                        "name": "Aleksandar Botev"
                    },
                    {
                        "authorId": "2689633",
                        "name": "Andrew Jaegle"
                    },
                    {
                        "authorId": "5721359",
                        "name": "Peter Wirnsberger"
                    },
                    {
                        "authorId": "1897926",
                        "name": "Daniel Hennes"
                    },
                    {
                        "authorId": "39051054",
                        "name": "I. Higgins"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Besides analytical methods, many authors introduced provably stable neural architectures [Haber et al., 2019, Greydanus et al., 2019, Cranmer et al., 2020] or stability constraints [John et al., 2017]."
            ],
            "citingPaper": {
                "paperId": "3e1d263859d0b525d47c70ddf903dbd0ad200059",
                "externalIds": {
                    "ArXiv": "2111.04601",
                    "DBLP": "conf/nips/DrgonaMZLH21",
                    "CorpusId": 243847549
                },
                "corpusId": 243847549,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3e1d263859d0b525d47c70ddf903dbd0ad200059",
                "title": "On the Stochastic Stability of Deep Markov Models",
                "abstract": "Deep Markov models (DMM) are generative models that are scalable and expressive generalization of Markov models for representation, learning, and inference problems. However, the fundamental stochastic stability guarantees of such models have not been thoroughly investigated. In this paper, we provide sufficient conditions of DMM's stochastic stability as defined in the context of dynamical systems and propose a stability analysis method based on the contraction of probabilistic maps modeled by deep neural networks. We make connections between the spectral properties of neural network's weights and different types of used activation functions on the stability and overall dynamic behavior of DMMs with Gaussian distributions. Based on the theory, we propose a few practical methods for designing constrained DMMs with guaranteed stability. We empirically substantiate our theoretical results via intuitive numerical experiments using the proposed stability constraints.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2403282",
                        "name": "J\u00e1n Drgo\u0148a"
                    },
                    {
                        "authorId": "145117720",
                        "name": "Sayak Mukherjee"
                    },
                    {
                        "authorId": "2144129676",
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "authorId": "2108861107",
                        "name": "Frank Liu"
                    },
                    {
                        "authorId": "3285377",
                        "name": "M. Halappanavar"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "First, Section 4.4.1 describes how energy-conserving dynamics can be enforced by encoding the problem using Hamiltonian or Lagrangian mechanics.",
                "For instance, we relate energy-conserving numerical solvers to Hamiltonian NNs, whose goal is to encode energy conservation, and we discuss concepts such as numerical stability and solver convergence, which are crucial in long-term prediction using NNs.",
                "The main advantage of Hamiltonian [41, 124] NNs and the closely related",
                "In recent years, the research community turned its attention to deriving these types of scalar valued energy functions by means of data-driven methods [41, 77, 142].",
                "We start with the Hamiltonian defined as\nH (x ) = T (x ) \u2212V (x ), (14)\nwhere x = [q,p] represents the concatenated state vector of generalized coordinates q and generalized momentap.",
                "Despite their mathematical elegance, deriving analytical Hamiltonian and Lagrangian functions for complex dynamical systems is a grueling task.",
                "The main advantage of Hamiltonian [41, 124] NNs and the closely related Lagrangian [21, 77] NNs is that they naturally incorporate the preservation of energy into the network structure itself.",
                "A similar concept to that of Hamiltonian and Lagrangian NNs involves learning neural surrogates for potential energy functionsV (x ) of a dynamical system, where the primary difference with Hamiltonians and Lagrangians is that the kinetic terms are\nACM Computing Surveys, Vol. 55, No. 11, Article 236.",
                "Specifically, the goal is to train an NN to approximate the Hamiltonian/Lagrangian of the system, as shown in Figure 18.",
                "In physics, a special class of closely related functions, called Hamiltonian and Lagrangian functions, has been developed for describing the total energy of a system.",
                "4.4.1 Hamiltonian and Lagrangian Networks.",
                "To improve the performance, others have introduced various inductive biases such as Hamiltonian NODE architecture [142] or penalizing higher-order derivatives of the NODEs in the\nACM Computing Surveys, Vol. 55, No. 11, Article 236.",
                "Both Hamiltonian H and Lagrangian L are defined as a sum of total kineticT and potential energy V of the system."
            ],
            "citingPaper": {
                "paperId": "0ffddec4d715c86a9e41ca61124fb47941cf7dfd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-01495",
                    "ArXiv": "2111.01495",
                    "DOI": "10.1145/3567591",
                    "CorpusId": 240420173
                },
                "corpusId": 240420173,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0ffddec4d715c86a9e41ca61124fb47941cf7dfd",
                "title": "Constructing Neural Network Based Models for Simulating Dynamical Systems",
                "abstract": "Dynamical systems see widespread use in natural sciences like physics, biology, and chemistry, as well as engineering disciplines such as circuit analysis, computational fluid dynamics, and control. For simple systems, the differential equations governing the dynamics can be derived by applying fundamental physical laws. However, for more complex systems, this approach becomes exceedingly difficult. Data-driven modeling is an alternative paradigm that seeks to learn an approximation of the dynamics of a system using observations of the true system. In recent years, there has been an increased interest in applying data-driven modeling techniques to solve a wide range of problems in physics and engineering. This article provides a survey of the different ways to construct models of dynamical systems using neural networks. In addition to the basic overview, we review the related literature and outline the most significant challenges from numerical simulations that this modeling paradigm must overcome. Based on the reviewed literature and identified challenges, we provide a discussion on promising research areas.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2121348428",
                        "name": "C. Legaard"
                    },
                    {
                        "authorId": "1392677248",
                        "name": "Thomas Schranz"
                    },
                    {
                        "authorId": "40085274",
                        "name": "G. Schweiger"
                    },
                    {
                        "authorId": "2136392921",
                        "name": "J'an Drgovna"
                    },
                    {
                        "authorId": "101678794",
                        "name": "Basak Falay"
                    },
                    {
                        "authorId": "145746046",
                        "name": "C. Gomes"
                    },
                    {
                        "authorId": "3074923",
                        "name": "Alexandros Iosifidis"
                    },
                    {
                        "authorId": "46220390",
                        "name": "M. Abkar"
                    },
                    {
                        "authorId": "1792089",
                        "name": "P. Larsen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Finally, we implemented and compared with the equivariant version of Hamiltonian neural network of Greydanus et al. (2019) (see EqHNN in table."
            ],
            "citingPaper": {
                "paperId": "1ccb211c3816e39ce71e46d4f9eba70823ad3da6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-01892",
                    "ArXiv": "2111.01892",
                    "CorpusId": 241035462
                },
                "corpusId": 241035462,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1ccb211c3816e39ce71e46d4f9eba70823ad3da6",
                "title": "Equivariant Deep Dynamical Model for Motion Prediction",
                "abstract": "Learning representations through deep generative modeling is a powerful approach for dynamical modeling to discover the most simplified and compressed underlying description of the data, to then use it for other tasks such as prediction. Most learning tasks have intrinsic symmetries, i.e., the input transformations leave the output unchanged, or the output undergoes a similar transformation. The learning process is, however, usually uninformed of these symmetries. Therefore, the learned representations for individually transformed inputs may not be meaningfully related. In this paper, we propose an SO(3) equivariant deep dynamical model (EqDDM) for motion prediction that learns a structured representation of the input space in the sense that the embedding varies with symmetry transformations. EqDDM is equipped with equivariant networks to parameterize the state-space emission and transition models. We demonstrate the superior predictive performance of the proposed model on various motion data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2975755",
                        "name": "Bahar Azari"
                    },
                    {
                        "authorId": "2064391855",
                        "name": "D. Erdo\u011fmu\u015f"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "We can for example restrict the policy to obey stable system dynamics derived from first principles (Greydanus et al., 2019; Lutter et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "e7eba2aa3c625beec289cb14914e7b5d36469d04",
                "externalIds": {
                    "PubMedCentral": "9038844",
                    "ArXiv": "2111.00956",
                    "DBLP": "journals/corr/abs-2111-00956",
                    "DOI": "10.3389/frobt.2022.799893",
                    "CorpusId": 240354417,
                    "PubMed": "35494543"
                },
                "corpusId": 240354417,
                "publicationVenue": {
                    "id": "2ee61499-676f-46c2-afde-d4c0cb4393e6",
                    "name": "Frontiers in Robotics and AI",
                    "type": "journal",
                    "alternate_names": [
                        "Front Robot AI"
                    ],
                    "issn": "2296-9144",
                    "url": "https://www.frontiersin.org/journals/robotics-and-ai",
                    "alternate_urls": [
                        "http://www.frontiersin.org/Robotics_and_AI/archive",
                        "http://www.frontiersin.org/Robotics_and_AI/about",
                        "http://www.frontiersin.org/Robotics_and_AI"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e7eba2aa3c625beec289cb14914e7b5d36469d04",
                "title": "Robot Learning From Randomized Simulations: A Review",
                "abstract": "The rise of deep learning has caused a paradigm shift in robotics research, favoring methods that require large amounts of data. Unfortunately, it is prohibitively expensive to generate such data sets on a physical platform. Therefore, state-of-the-art approaches learn in simulation where data generation is fast as well as inexpensive and subsequently transfer the knowledge to the real robot (sim-to-real). Despite becoming increasingly realistic, all simulators are by construction based on models, hence inevitably imperfect. This raises the question of how simulators can be modified to facilitate learning robot control policies and overcome the mismatch between simulation and reality, often called the \u201creality gap.\u201d We provide a comprehensive review of sim-to-real research for robotics, focusing on a technique named \u201cdomain randomization\u201d which is a method for learning from randomized simulations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "81555516",
                        "name": "Fabio Muratore"
                    },
                    {
                        "authorId": "153279354",
                        "name": "Fabio Ramos"
                    },
                    {
                        "authorId": "1713189",
                        "name": "Greg Turk"
                    },
                    {
                        "authorId": "70461341",
                        "name": "Wenhao Yu"
                    },
                    {
                        "authorId": "1713430",
                        "name": "M. Gienger"
                    },
                    {
                        "authorId": "145197867",
                        "name": "Jan Peters"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "[5, 17, 46] introduce non-regression loss functions inspired by Hamiltonian mechanics [19]."
            ],
            "citingPaper": {
                "paperId": "862b436e4385aa1984ceef922fad6ce9725af3e3",
                "externalIds": {
                    "ArXiv": "2110.14392",
                    "DBLP": "conf/bmvc/PourheydariRFFN22",
                    "CorpusId": 239998098
                },
                "corpusId": 239998098,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/862b436e4385aa1984ceef922fad6ce9725af3e3",
                "title": "TaylorSwiftNet: Taylor Driven Temporal Modeling for Swift Future Frame Prediction",
                "abstract": "While recurrent neural networks (RNNs) demonstrate outstanding capabilities for future video frame prediction, they model dynamics in a discrete time space, i.e., they predict the frames sequentially with a fixed temporal step. RNNs are therefore prone to accumulate the error as the number of future frames increases. In contrast, partial differential equations (PDEs) model physical phenomena like dynamics in a continuous time space. However, the estimated PDE for frame forecasting needs to be numerically solved, which is done by discretization of the PDE and diminishes most of the advantages compared to discrete models. In this work, we, therefore, propose to approximate the motion in a video by a continuous function using the Taylor series. To this end, we introduce TaylorSwiftNet, a novel convolutional neural network that learns to estimate the higher order terms of the Taylor series for a given input video. TaylorSwiftNet can swiftly predict future frames in parallel and it allows to change the temporal resolution of the forecast frames on-the-fly. The experimental results on various datasets demonstrate the superiority of our model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2135541738",
                        "name": "Mohammad Pourheydari"
                    },
                    {
                        "authorId": null,
                        "name": "Mohsen Fayyaz"
                    },
                    {
                        "authorId": "2025664854",
                        "name": "Emad Bahrami Rad"
                    },
                    {
                        "authorId": "1811235696",
                        "name": "M. Noroozi"
                    },
                    {
                        "authorId": "145689714",
                        "name": "Juergen Gall"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "babe0882cfde20cde8a7d8f3e9e4b4213724fd20",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-12773",
                    "ArXiv": "2110.12773",
                    "DOI": "10.1038/s42254-022-00441-7",
                    "CorpusId": 239768180
                },
                "corpusId": 239768180,
                "publicationVenue": {
                    "id": "3639d55b-36ef-4fa6-97fd-1bdc155f9081",
                    "name": "Nature Reviews Physics",
                    "alternate_names": [
                        "Nat Rev Phys"
                    ],
                    "issn": "2522-5820",
                    "url": "https://www.nature.com/natrevphys/"
                },
                "url": "https://www.semanticscholar.org/paper/babe0882cfde20cde8a7d8f3e9e4b4213724fd20",
                "title": "Scientific machine learning benchmarks",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "89275999",
                        "name": "J. Thiyagalingam"
                    },
                    {
                        "authorId": "46233324",
                        "name": "M. Shankar"
                    },
                    {
                        "authorId": "145943296",
                        "name": "Geoffrey Fox"
                    },
                    {
                        "authorId": "152181346",
                        "name": "Tony (Anthony) John Grenville Hey"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "266691c02a8715e99570d013a4661bc64adc8811",
                "externalIds": {
                    "ArXiv": "2110.12690",
                    "DBLP": "conf/icml/MeunierDAA22",
                    "CorpusId": 246441766
                },
                "corpusId": 246441766,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/266691c02a8715e99570d013a4661bc64adc8811",
                "title": "A Dynamical System Perspective for Lipschitz Neural Networks",
                "abstract": "The Lipschitz constant of neural networks has been established as a key quantity to enforce the robustness to adversarial examples. In this paper, we tackle the problem of building $1$-Lipschitz Neural Networks. By studying Residual Networks from a continuous time dynamical system perspective, we provide a generic method to build $1$-Lipschitz Neural Networks and show that some previous approaches are special cases of this framework. Then, we extend this reasoning and show that ResNet flows derived from convex potentials define $1$-Lipschitz transformations, that lead us to define the {\\em Convex Potential Layer} (CPL). A comprehensive set of experiments on several datasets demonstrates the scalability of our architecture and the benefits as an $\\ell_2$-provable defense against adversarial examples.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47516106",
                        "name": "Laurent Meunier"
                    },
                    {
                        "authorId": "2134988135",
                        "name": "Blaise Delattre"
                    },
                    {
                        "authorId": "145533439",
                        "name": "Alexandre Araujo"
                    },
                    {
                        "authorId": "2311059",
                        "name": "A. Allauzen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "715b024db316c42659b63f98e00aa168e23b4702",
                "externalIds": {
                    "ArXiv": "2110.12422",
                    "DBLP": "journals/corr/abs-2110-12422",
                    "CorpusId": 239768433
                },
                "corpusId": 239768433,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/715b024db316c42659b63f98e00aa168e23b4702",
                "title": "A Differentiable Newton-Euler Algorithm for Real-World Robotics",
                "abstract": "Obtaining dynamics models is essential for robotics to achieve accurate model-based controllers and simulators for planning. The dynamics models are typically obtained using model specification of the manufacturer or simple numerical methods such as linear regression. However, this approach does not guarantee physically plausible parameters and can only be applied to kinematic chains consisting of rigid bodies. In this article, we describe a differentiable simulator that can be used to identify the system parameters of real-world mechanical systems with complex friction models, holonomic as well as non-holonomic constraints. To guarantee physically consistent parameters, we utilize virtual parameters and gradient-based optimization. The described Differentiable Newton-Euler Algorithm (DiffNEA) can be applied to a class of dynamical systems and guarantees physically plausible predictions. The extensive experimental evaluation shows, that the proposed model learning approach learns accurate dynamics models of systems with complex friction and non-holonomic constraints. Especially in the offline reinforcement learning experiments, the identified DiffNEA models excel. For the challenging ball in a cup task, these models solve the task using model-based offline reinforcement learning on the physical system. The black-box baselines fail on this task in simulation and on the physical system despite using more data for learning the model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49199170",
                        "name": "M. Lutter"
                    },
                    {
                        "authorId": "17160667",
                        "name": "Johannes Silberbauer"
                    },
                    {
                        "authorId": "31349388",
                        "name": "Joe Watson"
                    },
                    {
                        "authorId": "145197867",
                        "name": "Jan Peters"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "For example, numerous works show that energy conserving trajectories can be effectively learnt from data by enforcing known energy constraints such as Hamiltonians (Greydanus et al., 2019; Sanchez-Gonzalez et al., 2019), Lagrangians (Cranmer et al., 2020) and variational integrators (Saemundsson et al., 2020; Desai et al., 2021b) into networks.",
                "\u2026works show that energy conserving trajectories can be effectively learnt from data by enforcing known energy constraints such as Hamiltonians (Greydanus et al., 2019; Sanchez-Gonzalez et al., 2019), Lagrangians (Cranmer et al., 2020) and variational integrators (Saemundsson et al., 2020;\u2026"
            ],
            "citingPaper": {
                "paperId": "9ce3a95b1fdde6c8383ce4a9cf19e0d208b8c62a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-11286",
                    "ArXiv": "2110.11286",
                    "CorpusId": 239050073
                },
                "corpusId": 239050073,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9ce3a95b1fdde6c8383ce4a9cf19e0d208b8c62a",
                "title": "One-Shot Transfer Learning of Physics-Informed Neural Networks",
                "abstract": "Solving differential equations efficiently and accurately sits at the heart of progress in many areas of scientific research, from classical dynamical systems to quantum mechanics. There is a surge of interest in using Physics-Informed Neural Networks (PINNs) to tackle such problems as they provide numerous benefits over traditional numerical approaches. Despite their potential benefits for solving differential equations, transfer learning has been under explored. In this study, we present a general framework for transfer learning PINNs that results in one-shot inference for linear systems of both ordinary and partial differential equations. This means that highly accurate solutions to many unknown differential equations can be obtained instantaneously without retraining an entire network. We demonstrate the efficacy of the proposed deep learning approach by solving several real-world problems, such as first- and second-order linear ordinary equations, the Poisson equation, and the time-dependent Schrodinger complex-value partial differential equation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144446378",
                        "name": "Shaan Desai"
                    },
                    {
                        "authorId": "145324933",
                        "name": "M. Mattheakis"
                    },
                    {
                        "authorId": "2124445841",
                        "name": "H. Joy"
                    },
                    {
                        "authorId": "1735677",
                        "name": "P. Protopapas"
                    },
                    {
                        "authorId": "143841496",
                        "name": "Stephen J. Roberts"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "For example physical conservation laws can be learned [1, 2]."
            ],
            "citingPaper": {
                "paperId": "40da9383263b97e98b638f2d3cdff39527484bac",
                "externalIds": {
                    "ArXiv": "2110.10721",
                    "DBLP": "journals/corr/abs-2110-10721",
                    "DOI": "10.1103/PhysRevA.105.042403",
                    "CorpusId": 239049957
                },
                "corpusId": 239049957,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/40da9383263b97e98b638f2d3cdff39527484bac",
                "title": "Learning quantum dynamics with latent neural ODEs",
                "abstract": "The core objective of machine-assisted scientific discovery is to learn physical laws from experimental data without prior knowledge of the systems in question. In the area of quantum physics, making progress towards these goals is significantly more challenging due to the curse of dimensionality as well as the counter-intuitive nature of quantum mechanics. Here, we present the QNODE, a latent neural ODE trained on expectation values of closed and open quantum systems dynamics. It can learn to generate such measurement data and extrapolate outside of its training region that satisfies the von Neumann and time-local Lindblad master equations for closed and open quantum systems respectively in an unsupervised means. Furthermore, the QNODE rediscovers quantum mechanical laws such as the Heisenberg's uncertainty principle in a data-driven way, without any constraint or guidance. Additionally, we show that trajectories that are generated from the QNODE that are close in its latent space have similar quantum dynamics while preserving the physics of the training system.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2107810007",
                        "name": "M. Choi"
                    },
                    {
                        "authorId": "1492180970",
                        "name": "Daniel Flam-Shepherd"
                    },
                    {
                        "authorId": "32185825",
                        "name": "T. Kyaw"
                    },
                    {
                        "authorId": "1422193589",
                        "name": "A. Aspuru\u2010Guzik"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "c3d9eb1e31bfc69c5c4ede3056c2710dec4a8f5c",
                "externalIds": {
                    "DBLP": "journals/ijcv/KandukuriAMS22",
                    "MAG": "3205069776",
                    "DOI": "10.1007/s11263-021-01493-5",
                    "CorpusId": 244615302
                },
                "corpusId": 244615302,
                "publicationVenue": {
                    "id": "939ee07c-6009-43f8-b884-69238b40659e",
                    "name": "International Journal of Computer Vision",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Comput Vis"
                    ],
                    "issn": "0920-5691",
                    "url": "https://www.springer.com/computer/image+processing/journal/11263",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11263",
                        "http://link.springer.com/journal/11263"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c3d9eb1e31bfc69c5c4ede3056c2710dec4a8f5c",
                "title": "Physical Representation Learning and Parameter Identification from Video Using Differentiable Physics",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1946865681",
                        "name": "Rama Krishna Kandukuri"
                    },
                    {
                        "authorId": "31039722",
                        "name": "Jan Achterhold"
                    },
                    {
                        "authorId": "2072391584",
                        "name": "Michael Moeller"
                    },
                    {
                        "authorId": "2065472725",
                        "name": "Joerg Stueckler"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "A different way of formulating rigid body dynamics has been investigated in Greydanus et al. (2019) using energy conservation laws."
            ],
            "citingPaper": {
                "paperId": "7dd07bbf6bb370e30189fecdd65b76d95c0cfc85",
                "externalIds": {
                    "DOI": "10.1007/s11263-021-01493-5",
                    "CorpusId": 255097649
                },
                "corpusId": 255097649,
                "publicationVenue": {
                    "id": "939ee07c-6009-43f8-b884-69238b40659e",
                    "name": "International Journal of Computer Vision",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Comput Vis"
                    ],
                    "issn": "0920-5691",
                    "url": "https://www.springer.com/computer/image+processing/journal/11263",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11263",
                        "http://link.springer.com/journal/11263"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7dd07bbf6bb370e30189fecdd65b76d95c0cfc85",
                "title": "Physical Representation Learning and Parameter Identification from Video Using Differentiable Physics",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1946865681",
                        "name": "Rama Krishna Kandukuri"
                    },
                    {
                        "authorId": "31039722",
                        "name": "Jan Achterhold"
                    },
                    {
                        "authorId": "2155273305",
                        "name": "Michael Moeller"
                    },
                    {
                        "authorId": "2065472725",
                        "name": "Joerg Stueckler"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Also, [Greydanus et al., 2019] used a Hamiltonian prior on a NN and studied conservative systems such as a mass-spring system and a pendulum (ideal and real data)."
            ],
            "citingPaper": {
                "paperId": "d8a84f8f1c0446de528b8f47d796aa313e4aa7d4",
                "externalIds": {
                    "ArXiv": "2110.06917",
                    "DBLP": "journals/corr/abs-2110-06917",
                    "CorpusId": 238744007
                },
                "corpusId": 238744007,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d8a84f8f1c0446de528b8f47d796aa313e4aa7d4",
                "title": "Extracting Dynamical Models from Data",
                "abstract": "The problem of determining the underlying dynamics of a system when only given data of its state over time has challenged scientists for decades. In this paper, the approach of using machine learning to model the {\\em updates} of the phase space variables is introduced; this is done as a function of the phase space variables. (More generally, the modeling is done over the jet space of the variables.) This approach is shown to accurately replicate the dynamics for the examples of the harmonic oscillator, the pendulum, and the Duffing oscillator; the underlying differential equation is also accurately recovered in each example. In addition, the results in no way depend on how the data is sampled over time (i.e., regularly or irregularly). It is demonstrated that this approach (named\"FJet\") is similar to the model resulting from a Taylor series expansion of the Runge-Kutta (RK) numerical integration scheme. This analogy confers the advantage of explicitly revealing the appropriate functions to use in the modeling, as well as revealing the error estimate of the updates. Thus, this new approach can be thought of as a way to determine the coefficients of an RK scheme by machine learning. Finally, it is shown in the undamped harmonic oscillator example that the stability of the updates is stable for $10^9$ times longer than with $4$th-order RK.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48345857",
                        "name": "M. Zimmer"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "95088cb4b573cde65e748d9764dbbced0157309c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-05167",
                    "ArXiv": "2110.05167",
                    "CorpusId": 238583011
                },
                "corpusId": 238583011,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/95088cb4b573cde65e748d9764dbbced0157309c",
                "title": "Robust and Scalable SDE Learning: A Functional Perspective",
                "abstract": "Stochastic differential equations provide a rich class of flexible generative models, capable of describing a wide range of spatio-temporal processes. A host of recent work looks to learn data-representing SDEs, using neural networks and other flexible function approximators. Despite these advances, learning remains computationally expensive due to the sequential nature of SDE integrators. In this work, we propose an importance-sampling estimator for probabilities of observations of SDEs for the purposes of learning. Crucially, the approach we suggest does not rely on such integrators. The proposed method produces lower-variance gradient estimates compared to algorithms based on SDE integrators and has the added advantage of being embarrassingly parallelizable. This facilitates the effective use of large-scale parallel hardware for massive decreases in computation time.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2070859762",
                        "name": "Scott A. Cameron"
                    },
                    {
                        "authorId": "2131832022",
                        "name": "Tyron Cameron"
                    },
                    {
                        "authorId": "38166712",
                        "name": "Arnu Pretorius"
                    },
                    {
                        "authorId": "2256679917",
                        "name": "Stephen J. Roberts"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "9c1ceb96ef4b0cbf46d01b89d40da0c9ff52c675",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-05266",
                    "ArXiv": "2110.05266",
                    "CorpusId": 238583409
                },
                "corpusId": 238583409,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9c1ceb96ef4b0cbf46d01b89d40da0c9ff52c675",
                "title": "Chaos as an interpretable benchmark for forecasting and data-driven modelling",
                "abstract": "The striking fractal geometry of strange attractors underscores the generative nature of chaos: like probability distributions, chaotic systems can be repeatedly measured to produce arbitrarily-detailed information about the underlying attractor. Chaotic systems thus pose a unique challenge to modern statistical learning techniques, while retaining quantifiable mathematical properties that make them controllable and interpretable as benchmarks. Here, we present a growing database currently comprising 131 known chaotic dynamical systems spanning fields such as astrophysics, climatology, and biochemistry. Each system is paired with precomputed multivariate and univariate time series. Our dataset has comparable scale to existing static time series databases; however, our systems can be re-integrated to produce additional datasets of arbitrary length and granularity. Our dataset is annotated with known mathematical properties of each system, and we perform feature analysis to broadly categorize the diverse dynamics present across the collection. Chaotic systems inherently challenge forecasting models, and across extensive benchmarks we correlate forecasting performance with the degree of chaos present. We also exploit the unique generative properties of our dataset in several proof-of-concept experiments: surrogate transfer learning to improve time series classification, importance sampling to accelerate model training, and benchmarking symbolic regression algorithms.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49738547",
                        "name": "W. Gilpin"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "9c23b4f8439565873a82eb40ef1a04d734f6d274",
                "externalIds": {
                    "PubMedCentral": "8989982",
                    "ArXiv": "2110.04170",
                    "DOI": "10.1038/s41598-022-09938-8",
                    "CorpusId": 238531250,
                    "PubMed": "35393511"
                },
                "corpusId": 238531250,
                "publicationVenue": {
                    "id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                    "name": "Scientific Reports",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Rep"
                    ],
                    "issn": "2045-2322",
                    "url": "http://www.nature.com/srep/",
                    "alternate_urls": [
                        "http://www.nature.com/srep/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9c23b4f8439565873a82eb40ef1a04d734f6d274",
                "title": "Multi-fidelity information fusion with concatenated neural networks",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "11347090",
                        "name": "Suraj Pawar"
                    },
                    {
                        "authorId": "3071838",
                        "name": "O. San"
                    },
                    {
                        "authorId": "1958218",
                        "name": "P. Vedula"
                    },
                    {
                        "authorId": "49219413",
                        "name": "A. Rasheed"
                    },
                    {
                        "authorId": "1834820",
                        "name": "T. Kvamsdal"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "51b489ece8d86e73e85e9d095b4a73620d5c2029",
                "externalIds": {
                    "ArXiv": "2110.03903",
                    "DBLP": "journals/corr/abs-2110-03903",
                    "CorpusId": 238531598
                },
                "corpusId": 238531598,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/51b489ece8d86e73e85e9d095b4a73620d5c2029",
                "title": "Kinematically consistent recurrent neural networks for learning inverse problems in wave propagation",
                "abstract": "Although machine learning (ML) is increasingly employed recently for mechanistic problems, the black-box nature of conventional ML architectures lacks the physical knowledge to infer unforeseen input conditions. This implies both severe overfitting during a dearth of training data and inadequate physical interpretability, which motivates us to propose a new kinematically consistent, physics-based ML model. In particular, we attempt to perform physically interpretable learning of inverse problems in wave propagation without suffering overfitting restrictions. Towards this goal, we employ long short-term memory (LSTM) networks endowed with a physical, hyperparameter-driven regularizer, performing penalty-based enforcement of the characteristic geometries. Since these characteristics are the kinematical invariances of wave propagation phenomena, maintaining their structure provides kinematical consistency to the network. Even with modest training data, the kinematically consistent network can reduce the $L_1$ and $L_\\infty$ error norms of the plain LSTM predictions by about 45% and 55%, respectively. It can also increase the horizon of the plain LSTM's forecasting by almost two times. To achieve this, an optimal range of the physical hyperparameter, analogous to an artificial bulk modulus, has been established through numerical experiments. The efficacy of the proposed method in alleviating overfitting, and the physical interpretability of the learning mechanism, are also discussed. Such an application of kinematically consistent LSTM networks for wave propagation learning is presented here for the first time.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "72216413",
                        "name": "Wrik Mallik"
                    },
                    {
                        "authorId": "6330065",
                        "name": "R. Jaiman"
                    },
                    {
                        "authorId": "71859411",
                        "name": "J. Jelovica"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recent studies on Lagrangian (LNN) and Hamiltonian neural networks show that one of these invariant quantities, energy, can be learned directly from the data enabling realistic simulation of systems [6, 7, 4, 8, 9, 10, 11]."
            ],
            "citingPaper": {
                "paperId": "be8ae1565c7534c95eb5febce8f586abe669cff3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-03266",
                    "ArXiv": "2110.03266",
                    "CorpusId": 238419621
                },
                "corpusId": 238419621,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/be8ae1565c7534c95eb5febce8f586abe669cff3",
                "title": "Lagrangian Neural Network with Differentiable Symmetries and Relational Inductive Bias",
                "abstract": "Realistic models of physical world rely on differentiable symmetries that, in turn, correspond to conservation laws. Recent works on Lagrangian and Hamiltonian neural networks show that the underlying symmetries of a system can be easily learned by a neural network when provided with an appropriate inductive bias. However, these models still suffer from issues such as inability to generalize to arbitrary system sizes, poor interpretability, and most importantly, inability to learn translational and rotational symmetries, which lead to the conservation laws of linear and angular momentum, respectively. Here, we present a momentum conserving Lagrangian neural network (MCLNN) that learns the Lagrangian of a system, while also preserving the translational and rotational symmetries. We test our approach on linear and non-linear spring systems, and a gravitational system, demonstrating the energy and momentum conservation. We also show that the model developed can generalize to systems of any arbitrary size. Finally, we discuss the interpretability of the MCLNN, which directly provides physical insights into the interactions of multi-particle systems.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2131107456",
                        "name": "Ravinder Bhattoo"
                    },
                    {
                        "authorId": "1699732",
                        "name": "Sayan Ranu"
                    },
                    {
                        "authorId": "2144896197",
                        "name": "N. Krishnan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "For the HNN case, we parameterize the Hamiltonian function of the dynamical system with a neural network, and we use a Hamiltonian integrator to predict the dynamics [10].",
                "Inspired by [7] we consider two approaches to model f in equation (6): Neural ordinary differential equations (N-ODEs) [3], and Hamiltonian neural networks (HNNs) [10, 15]."
            ],
            "citingPaper": {
                "paperId": "bfa81f5a418d5b60a3b34d5baca12b7bc9849e98",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-03761",
                    "ArXiv": "2110.03761",
                    "CorpusId": 238531466
                },
                "corpusId": 238531466,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bfa81f5a418d5b60a3b34d5baca12b7bc9849e98",
                "title": "A simple equivariant machine learning method for dynamics based on scalars",
                "abstract": "Physical systems obey strict symmetry principles. We expect that machine learning methods that intrinsically respect these symmetries should have higher prediction accuracy and better generalization in prediction of physical dynamics. In this work we implement a principled model based on invariant scalars, and release open-source code. We apply this Scalars method to a simple chaotic dynamical system, the springy double pendulum. We show that the Scalars method outperforms state-of-the-art approaches for learning the properties of physical systems with symmetries, both in terms of accuracy and speed. Because the method incorporates the fundamental symmetries, we expect it to generalize to different settings, such as changes in the force laws in the system.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "29892797",
                        "name": "Weichi Yao"
                    },
                    {
                        "authorId": "1463037423",
                        "name": "Kate Storey-Fisher"
                    },
                    {
                        "authorId": "144735014",
                        "name": "D. Hogg"
                    },
                    {
                        "authorId": "144790990",
                        "name": "Soledad Villar"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "120fd9fb41c0e6c1b7869fbb09e3f6c6a9113086",
                "externalIds": {
                    "DBLP": "journals/ijrr/LutterP23",
                    "ArXiv": "2110.01894",
                    "DOI": "10.1177/02783649231169492",
                    "CorpusId": 238353891
                },
                "corpusId": 238353891,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/120fd9fb41c0e6c1b7869fbb09e3f6c6a9113086",
                "title": "Combining physics and deep learning to learn continuous-time dynamics models",
                "abstract": "Deep learning has been widely used within learning algorithms for robotics. One disadvantage of deep networks is that these networks are black-box representations. Therefore, the learned approximations ignore the existing knowledge of physics or robotics. Especially for learning dynamics models, these black-box models are not desirable as the underlying principles are well understood and the standard deep networks can learn dynamics that violate these principles. To learn dynamics models with deep networks that guarantee physically plausible dynamics, we introduce physics-inspired deep networks that combine first principles from physics with deep learning. We incorporate Lagrangian mechanics within the model learning such that all approximated models adhere to the laws of physics and conserve energy. Deep Lagrangian Networks (DeLaN) parametrize the system energy using two networks. The parameters are obtained by minimizing the squared residual of the Euler\u2013Lagrange differential equation. Therefore, the resulting model does not require specific knowledge of the individual system, is interpretable, and can be used as a forward, inverse, and energy model. Previously these properties were only obtained when using system identification techniques that require knowledge of the kinematic structure. We apply DeLaN to learning dynamics models and apply these models to control simulated and physical rigid body systems. The results show that the proposed approach obtains dynamics models that can be applied to physical systems for real-time control. Compared to standard deep networks, the physics-inspired models learn better models and capture the underlying structure of the dynamics.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49199170",
                        "name": "M. Lutter"
                    },
                    {
                        "authorId": "145197867",
                        "name": "Jan Peters"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ae9983abc02b900e069e5677fce5f91ec28dec6c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-01954",
                    "ArXiv": "2110.01954",
                    "DOI": "10.1109/TPAMI.2022.3215769",
                    "CorpusId": 238354229,
                    "PubMed": "36260585"
                },
                "corpusId": 238354229,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ae9983abc02b900e069e5677fce5f91ec28dec6c",
                "title": "Continuous-Time Fitted Value Iteration for Robust Policies",
                "abstract": "Solving the Hamilton-Jacobi-Bellman equation is important in many domains including control, robotics and economics. Especially for continuous control, solving this differential equation and its extension the Hamilton-Jacobi-Isaacs equation, is important as it yields the optimal policy that achieves the maximum reward on a give task. In the case of the Hamilton-Jacobi-Isaacs equation, which includes an adversary controlling the environment and minimizing the reward, the obtained policy is also robust to perturbations of the dynamics. In this paper we propose continuous fitted value iteration (cFVI) and robust fitted value iteration (rFVI). These algorithms leverage the non-linear control-affine dynamics and separable state and action reward of many continuous control problems to derive the optimal policy and optimal adversary in closed form. This analytic expression simplifies the differential equations and enables us to solve for the optimal value function using value iteration for continuous actions and states as well as the adversarial case. Notably, the resulting algorithms do not require discretization of states or actions. We apply the resulting algorithms to the Furuta pendulum and cartpole. We show that both algorithms obtain the optimal policy. The robustness Sim2Real experiments on the physical systems show that the policies successfully achieve the task in the real-world. When changing the masses of the pendulum, we observe that robust value iteration is more robust compared to deep reinforcement learning algorithm and the non-robust version of the algorithm. Videos of the experiments are shown at https://sites.google.com/view/rfvi.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49199170",
                        "name": "M. Lutter"
                    },
                    {
                        "authorId": "29505409",
                        "name": "B. Belousov"
                    },
                    {
                        "authorId": "1712535",
                        "name": "Shie Mannor"
                    },
                    {
                        "authorId": "145197953",
                        "name": "D. Fox"
                    },
                    {
                        "authorId": "2054554660",
                        "name": "Animesh Garg"
                    },
                    {
                        "authorId": "2107720654",
                        "name": "Jan Peters"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4f8a7257c9aa203bb7d824afdf31d0eddaf89a71",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-02083",
                    "ArXiv": "2110.02083",
                    "DOI": "10.1007/s10409-021-01143-6",
                    "CorpusId": 238353800
                },
                "corpusId": 238353800,
                "publicationVenue": {
                    "id": "35c72f2e-4d19-47a9-9d6e-9e7bd93d979d",
                    "name": "Acta Mechanica Sinica",
                    "type": "journal",
                    "alternate_names": [
                        "Acta Mech Sin"
                    ],
                    "issn": "0459-1879",
                    "alternate_issns": [
                        "0567-7718"
                    ],
                    "url": "http://www.springer.com/engineering/journal/10409",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10409"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4f8a7257c9aa203bb7d824afdf31d0eddaf89a71",
                "title": "Applying machine learning to study fluid mechanics",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3083169",
                        "name": "S. Brunton"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a533ffae7151cb72909b9822537ba2d80afc7197",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-01408",
                    "ArXiv": "2110.01408",
                    "CorpusId": 238259512
                },
                "corpusId": 238259512,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a533ffae7151cb72909b9822537ba2d80afc7197",
                "title": "Paradigm Shift Through the Integration of Physical Methodology and Data Science",
                "abstract": "Data science methodologies, which have undergone significant developments recently, provide flexible representational performance and fast computational means to address the challenges faced by traditional scientific methodologies while revealing unprecedented challenges such as the interpretability of computations and the demand for extrapolative predictions on the amount of data. Methods that integrate traditional physical and data science methodologies are new methods of mathematical analysis that complement both methodologies and are being studied in various scientific fields. This paper highlights the significance and importance of such integrated methods from the viewpoint of scientific theory. Additionally, a comprehensive survey of specific methods and applications are conducted, and the current state of the art in relevant research fields are summarized.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "153044775",
                        "name": "T. Miyamoto"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Methods for incorporating prior knowledge also exist for neural networks, such as [15], inspired by Hamiltonian mechanics."
            ],
            "citingPaper": {
                "paperId": "4343148197d55ba04550274897db520bf69dd2be",
                "externalIds": {
                    "DBLP": "conf/iros/DernerKB21",
                    "DOI": "10.1109/IROS51168.2021.9635831",
                    "CorpusId": 245264611
                },
                "corpusId": 245264611,
                "publicationVenue": {
                    "id": "37275deb-3fcf-4d16-ae77-95db9899b1f3",
                    "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "IROS",
                        "Intelligent Robots and Systems",
                        "Intell Robot Syst",
                        "IEEE/RJS Int Conf Intell Robot Syst"
                    ],
                    "url": "http://www.iros.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4343148197d55ba04550274897db520bf69dd2be",
                "title": "Guiding Robot Model Construction with Prior Features",
                "abstract": "Virtually all robot control methods benefit from the availability of an accurate mathematical model of the robot. However, obtaining a sufficient amount of informative data for constructing dynamic models can be difficult, especially when the models are to be learned during robot deployment. Under such circumstances, standard data-driven model learning techniques often yield models that do not comply with the physics of the robot. We extend a symbolic regression algorithm based on Single Node Genetic Programming by including the prior model information into the model construction process. In this way, symbolic regression automatically builds models that compensate for theoretical or empirical model deficiencies. We experimentally demonstrate the approach on two real-world systems: the TurtleBot 2 mobile robot and the Parrot Bebop 2 drone. The results show that the proposed model-learning algorithm produces realistic models that fit well the training data even when using small training sets. Passing the prior model information to the algorithm significantly improves the model accuracy while speeding up the search.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "31533806",
                        "name": "Erik Derner"
                    },
                    {
                        "authorId": "3225363",
                        "name": "Ji\u0159\u00ed Kubal\u00edk"
                    },
                    {
                        "authorId": "1705222",
                        "name": "Robert Babu\u0161ka"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e016609913e538c34becc9d71fe03506d937bb17",
                "externalIds": {
                    "ArXiv": "2109.12659",
                    "CorpusId": 237940236
                },
                "corpusId": 237940236,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e016609913e538c34becc9d71fe03506d937bb17",
                "title": "Learning the GENERIC evolution",
                "abstract": "We propose a novel approach for learning the evolution that employs differentiable neural networks to approximate the full GENERIC structure. Instead of manually choosing the fitted parameters, we learn the whole model together with the evolution equations. We can reconstruct the energy and entropy functions for the system under various assumptions and accurately capture systems behaviour for a double thermoelastic pendulum and a rigid body.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2129453533",
                        "name": "Martin vS'ipka"
                    },
                    {
                        "authorId": "12727047",
                        "name": "M. Pavelka"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "2d9af525f54141ffe3cfee274899013546b88437",
                "externalIds": {
                    "DBLP": "journals/csysl/DuongA22",
                    "ArXiv": "2109.09974",
                    "DOI": "10.1109/lcsys.2022.3177156",
                    "CorpusId": 247597120
                },
                "corpusId": 247597120,
                "publicationVenue": {
                    "id": "b84ebb03-add3-4e2c-b380-26ecc87c7011",
                    "name": "IEEE Control Systems Letters",
                    "alternate_names": [
                        "IEEE Control Syst Lett"
                    ],
                    "issn": "2475-1456",
                    "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=7782633",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7782633"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2d9af525f54141ffe3cfee274899013546b88437",
                "title": "Adaptive Control of SE(3) Hamiltonian Dynamics With Learned Disturbance Features",
                "abstract": "Adaptive control is a critical component of reliable robot autonomy in rapidly changing operational conditions. Adaptive control designs benefit from a disturbance model, which is often unavailable in practice. This motivates the use of machine learning techniques to learn disturbance features from training data offline, which can subsequently be employed to compensate the disturbances online. This letter develops geometric adaptive control with a learned disturbance model for rigid-body systems, such as ground, aerial, and underwater vehicles, that satisfy Hamilton\u2019s equations of motion over the $SE{(}3{)}$ manifold. Our design consists of an offline disturbance model identification stage, using a Hamiltonian-based neural ordinary differential equation (ODE) network trained from state-control trajectory data, and an online adaptive control stage, estimating and compensating the disturbances based on geometric tracking errors. We demonstrate our adaptive geometric controller in trajectory tracking simulations of fully-actuated pendulum and under-actuated quadrotor systems.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2344318",
                        "name": "T. Duong"
                    },
                    {
                        "authorId": "50365495",
                        "name": "Nikolay A. Atanasov"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In contrast to learning the flow of the Hamiltonian dynamics, alternatively, the Hamiltonian itself can be learned, see [8], or the framework of Neural ODEs [15] can be adapted for large-scale nonseparable Hamiltonian systems [13].",
                "In addition to that, structure-preserving neural networks have been shown to generalize better than regular neural networks and produce qualitatively better predictions [8, 9, 10, 11, 12, 13]."
            ],
            "citingPaper": {
                "paperId": "7f030362b51d2a40382099dc6d32d77696a44d94",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-09151",
                    "ArXiv": "2109.09151",
                    "DOI": "10.1016/j.jcp.2023.111911",
                    "CorpusId": 237572178
                },
                "corpusId": 237572178,
                "publicationVenue": {
                    "id": "58606051-2e63-4034-8496-9d4ed773bb49",
                    "name": "Journal of Computational Physics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Phys"
                    ],
                    "issn": "0021-9991",
                    "url": "http://www.elsevier.com/locate/jcp",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00219991",
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622866/description#description",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7f030362b51d2a40382099dc6d32d77696a44d94",
                "title": "Locally-symplectic neural networks for learning volume-preserving dynamics",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7832894",
                        "name": "J. Baj\u0101rs"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Hamiltonian and Lagrangian Networks: Hamiltonian Neural Networks (Greydanus, Dzamba, and Yosinski 2019)\nIntelligence (www.aaai.org)."
            ],
            "citingPaper": {
                "paperId": "514cb41750495f2d25faf7615e0623a7b6f6e126",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-07359",
                    "ArXiv": "2109.07359",
                    "CorpusId": 237513376
                },
                "corpusId": 237513376,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/514cb41750495f2d25faf7615e0623a7b6f6e126",
                "title": "Modular Neural Ordinary Differential Equations",
                "abstract": "The laws of physics have been written in the language of dif-ferential equations for centuries. Neural Ordinary Differen-tial Equations (NODEs) are a new machine learning architecture which allows these differential equations to be learned from a dataset. These have been applied to classical dynamics simulations in the form of Lagrangian Neural Net-works (LNNs) and Second Order Neural Differential Equations (SONODEs). However, they either cannot represent the most general equations of motion or lack interpretability. In this paper, we propose Modular Neural ODEs, where each force component is learned with separate modules. We show how physical priors can be easily incorporated into these models. Through a number of experiments, we demonstrate these result in better performance, are more interpretable, and add flexibility due to their modularity.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2129480010",
                        "name": "Max Zhu"
                    },
                    {
                        "authorId": "2075355146",
                        "name": "P. Lio"
                    },
                    {
                        "authorId": "1562181219",
                        "name": "Jacob Moss"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026works either use the Lagrangian or the Hamiltonian formulation of dynamics to inform the structure of a neural ODE, as in (Cranmer et al. 2020; Lutter, Ritter, and Peters 2019; Roehrl et al. 2020) vs. (Greydanus, Dzamba, and Yosinski 2019; Matsubara, Ishikawa, and Yaguchi 2020; Toth et al. 2020)."
            ],
            "citingPaper": {
                "paperId": "d270b89b8161fd114daca4b4ceff849423a08a30",
                "externalIds": {
                    "ArXiv": "2109.06407",
                    "DBLP": "journals/corr/abs-2109-06407",
                    "CorpusId": 237503331
                },
                "corpusId": 237503331,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d270b89b8161fd114daca4b4ceff849423a08a30",
                "title": "Neural Networks with Physics-Informed Architectures and Constraints for Dynamical Systems Modeling",
                "abstract": "Effective inclusion of physics-based knowledge into deep neural network models of dynamical systems can greatly improve data efficiency and generalization. Such a-priori knowledge might arise from physical principles (e.g., conservation laws) or from the system's design (e.g., the Jacobian matrix of a robot), even if large portions of the system dynamics remain unknown. We develop a framework to learn dynamics models from trajectory data while incorporating a-priori system knowledge as inductive bias. More specifically, the proposed framework uses physics-based side information to inform the structure of the neural network itself, and to place constraints on the values of the outputs and the internal states of the model. It represents the system's vector field as a composition of known and unknown functions, the latter of which are parametrized by neural networks. The physics-informed constraints are enforced via the augmented Lagrangian method during the model's training. We experimentally demonstrate the benefits of the proposed approach on a variety of dynamical systems -- including a benchmark suite of robotics environments featuring large state spaces, non-linear dynamics, external forces, contact forces, and control inputs. By exploiting a-priori system knowledge during training, the proposed approach learns to predict the system dynamics two orders of magnitude more accurately than a baseline approach that does not include prior knowledge, given the same training dataset.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1475901145",
                        "name": "Franck Djeumou"
                    },
                    {
                        "authorId": "1796254983",
                        "name": "Cyrus Neary"
                    },
                    {
                        "authorId": "1709823",
                        "name": "\u00c9. Goubault"
                    },
                    {
                        "authorId": "2307593",
                        "name": "S. Putot"
                    },
                    {
                        "authorId": "3199888",
                        "name": "U. Topcu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "d0316b77addf8c377538ca1533a5f7052caeaf94",
                "externalIds": {
                    "ArXiv": "2109.07573",
                    "DBLP": "journals/corr/abs-2109-07573",
                    "CorpusId": 237532722
                },
                "corpusId": 237532722,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d0316b77addf8c377538ca1533a5f7052caeaf94",
                "title": "Differentiable Physics: A Position Piece",
                "abstract": "Differentiable physics provides a new approach for modeling and understanding the physical systems by pairing the new technology of differentiable programming with classical numerical methods for physical simulation. We survey the rapidly growing literature of differentiable physics techniques and highlight methods for parameter estimation, learning representations, solving differential equations, and developing what we call scientific foundation models using data and inductive priors. We argue that differentiable physics offers a new paradigm for modeling physical phenomena by combining classical analytic solutions with numerical methodology using the bridge of differentiable programming.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2378027",
                        "name": "Bharath Ramsundar"
                    },
                    {
                        "authorId": "32415194",
                        "name": "Dilip Krishnamurthy"
                    },
                    {
                        "authorId": "2631003",
                        "name": "V. Viswanathan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Unsupervised system identification from vision is a recent area of research that removes the requirements for trajectory data, with approaches including unsupervised physical parameter estimation [24, 31, 40], structured latent space learning [19, 25, 32], and Hamiltonian/Lagrangian learning [18, 51, 58]."
            ],
            "citingPaper": {
                "paperId": "19408c8b95b477e359b593420077d8cebfc3d5dd",
                "externalIds": {
                    "ArXiv": "2109.05928",
                    "DBLP": "conf/l4dc/JaquesABH22",
                    "CorpusId": 237491851
                },
                "corpusId": 237491851,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/19408c8b95b477e359b593420077d8cebfc3d5dd",
                "title": "Vision-based system identification and 3D keypoint discovery using dynamics constraints",
                "abstract": "This paper introduces V-SysId, a novel method that enables simultaneous keypoint discovery, 3D system identification, and extrinsic camera calibration from an unlabeled video taken from a static camera, using only the family of equations of motion of the object of interest as weak supervision. V-SysId takes keypoint trajectory proposals and alternates between maximum likelihood parameter estimation and extrinsic camera calibration, before applying a suitable selection criterion to identify the track of interest. This is then used to train a keypoint tracking model using supervised learning. Results on a range of settings (robotics, physics, physiology) highlight the utility of this approach.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50425877",
                        "name": "Miguel Jaques"
                    },
                    {
                        "authorId": "67246377",
                        "name": "Martin Asenov"
                    },
                    {
                        "authorId": "145841847",
                        "name": "Michael Burke"
                    },
                    {
                        "authorId": "1697755",
                        "name": "Timothy M. Hospedales"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "For generating data, we base our implementation on the code from Greydanus et al. (2019). We generate 800 training trajectories, 160 validation trajectories, and 160 test trajectories for varying initial conditions.",
                "For generating data, we base our implementation on the code from (Greydanus, Dzamba, and Yosinski 2019).",
                "Hamiltonian structure-preserving parameterization As a special case of the GENERIC formalism, we consider the Hamiltonian structure-preserving parameterization technique, which is originally proposed in Hamiltonian neural networks (HNNs) Greydanus et al. (2019): parameterizing the Hamiltonian function H(q, p) as H\u0398(q, p) such that H\u0398 = (\u03c6(q, p)\u039e).",
                "Parameterization techniques that preserve physical\nstructure include Hamiltonian neural networks (Greydanus, Dzamba, and Yosinski 2019; Toth et al. 2019), Lagrangian neural networks (Cranmer et al. 2020; Lutter, Ritter, and Peters 2018), port-Hamiltonian neural networks (Desai et al. 2021), and\u2026",
                "In the following, we consider the Hamiltonian structurepreserving parameterization technique proposed in Hamiltonian neural networks (HNNs) (Greydanus, Dzamba, and Yosinski 2019): parameterizing the Hamiltonian function H(q, p) asH\u0398(q, p) such that\nH\u0398 = (\u03c6(q, p)T\u039e)T. (5)\nWith the above\u2026"
            ],
            "citingPaper": {
                "paperId": "66b2509feac026d77bc0d2a3066c76918b77b9d5",
                "externalIds": {
                    "ArXiv": "2109.05364",
                    "DBLP": "journals/corr/abs-2109-05364",
                    "CorpusId": 237490407
                },
                "corpusId": 237490407,
                "publicationVenue": {
                    "id": "bed36c1a-3a44-42af-bd5a-4ffda6aa9fa2",
                    "name": "Mathematical and Scientific Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "MSML",
                        "Math Sci Mach Learn"
                    ],
                    "url": "http://msml-conf.org/"
                },
                "url": "https://www.semanticscholar.org/paper/66b2509feac026d77bc0d2a3066c76918b77b9d5",
                "title": "Structure-preserving Sparse Identification of Nonlinear Dynamics for Data-driven Modeling",
                "abstract": "Discovery of dynamical systems from data forms the foundation for data-driven modeling and recently, structure-preserving geometric perspectives have been shown to provide improved forecasting, stability, and physical realizability guarantees. We present here a unification of the Sparse Identification of Nonlinear Dynamics (SINDy) formalism with neural ordinary differential equations. The resulting framework allows learning of both\"black-box\"dynamics and learning of structure preserving bracket formalisms for both reversible and irreversible dynamics. We present a suite of benchmarks demonstrating effectiveness and structure preservation, including for chaotic systems.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3041967",
                        "name": "Kookjin Lee"
                    },
                    {
                        "authorId": "38723595",
                        "name": "Nathaniel Trask"
                    },
                    {
                        "authorId": "153799305",
                        "name": "P. Stinis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Some approaches have focused on embedding specialized physical constraints into NNs, such as conservation of energy or momentum [4, 12] or multiscale features [34]."
            ],
            "citingPaper": {
                "paperId": "3c4372b125d0744bb68bfca9f5d6b0abb85dd182",
                "externalIds": {
                    "ArXiv": "2109.01050",
                    "DBLP": "journals/corr/abs-2109-01050",
                    "CorpusId": 237386144
                },
                "corpusId": 237386144,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3c4372b125d0744bb68bfca9f5d6b0abb85dd182",
                "title": "Characterizing possible failure modes in physics-informed neural networks",
                "abstract": "Recent work in scientific machine learning has developed so-called physics-informed neural network (PINN) models. The typical approach is to incorporate physical domain knowledge as soft constraints on an empirical loss function and use existing machine learning methodologies to train the model. We demonstrate that, while existing PINN methodologies can learn good models for relatively trivial problems, they can easily fail to learn relevant physical phenomena for even slightly more complex problems. In particular, we analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. We provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. Importantly, we show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that the PINN's setup makes the loss landscape very hard to optimize. We then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where the PINN's loss term starts from a simple PDE regularization, and becomes progressively more complex as the NN gets trained. The second approach is to pose the problem as a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that we can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "15408096",
                        "name": "A. Krishnapriyan"
                    },
                    {
                        "authorId": "10419477",
                        "name": "A. Gholami"
                    },
                    {
                        "authorId": "2390798",
                        "name": "Shandian Zhe"
                    },
                    {
                        "authorId": "152534535",
                        "name": "Robert M. Kirby"
                    },
                    {
                        "authorId": "1717098",
                        "name": "Michael W. Mahoney"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ea1384cafd31d0c4d821469bb6261f95b86a69ac",
                "externalIds": {
                    "ArXiv": "2109.00092",
                    "DBLP": "journals/corr/abs-2109-00092",
                    "DOI": "10.1098/rsta.2021.0207",
                    "CorpusId": 237373722,
                    "PubMed": "35719066"
                },
                "corpusId": 237373722,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ea1384cafd31d0c4d821469bb6261f95b86a69ac",
                "title": "GFINNs: GENERIC formalism informed neural networks for deterministic and stochastic dynamical systems",
                "abstract": "We propose the GENERIC formalism informed neural networks (GFINNs) that obey the symmetric degeneracy conditions of the GENERIC formalism. GFINNs comprise two modules, each of which contains two components. We model each component using a neural network whose architecture is designed to satisfy the required conditions. The component-wise architecture design provides flexible ways of leveraging available physics information into neural networks. We prove theoretically that GFINNs are sufficiently expressive to learn the underlying equations, hence establishing the universal approximation theorem. We demonstrate the performance of GFINNs in three simulation problems: gas containers exchanging heat and volume, thermoelastic double pendulum and the Langevin dynamics. In all the examples, GFINNs outperform existing methods, hence demonstrating good accuracy in predictions for both deterministic and stochastic systems. This article is part of the theme issue \u2018Data-driven prediction in dynamical systems\u2019.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116702527",
                        "name": "Zhen Zhang"
                    },
                    {
                        "authorId": "3366546",
                        "name": "Yeonjong Shin"
                    },
                    {
                        "authorId": "7964513",
                        "name": "G. Em Karniadakis"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In addition, Greydanus et al. (2019) have used Hamiltonian Neural Networks to define reduced-order models based on learned conservation laws."
            ],
            "citingPaper": {
                "paperId": "1e9dd654200a62ca89a75d9856c6d1c33c9edb5d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-12453",
                    "ArXiv": "2108.12453",
                    "CorpusId": 237353404
                },
                "corpusId": 237353404,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1e9dd654200a62ca89a75d9856c6d1c33c9edb5d",
                "title": "Convolutional Autoencoders for Reduced-Order Modeling",
                "abstract": "In the construction of reduced-order models for dynamical systems, linear projection methods, such as proper orthogonal decompositions, are commonly employed. However, for many dynamical systems, the lower dimensional representation of the state space can most accurately be described by a \\textit{nonlinear} manifold. Previous research has shown that deep learning can provide an efficient method for performing nonlinear dimension reduction, though they are dependent on the availability of training data and are often problem-specific \\citep[see][]{carlberg_ca}. Here, we utilize randomized training data to create and train convolutional autoencoders that perform nonlinear dimension reduction for the wave and Kuramoto-Shivasinsky equations. Moreover, we present training methods that are independent of full-order model samples and use the manifold least-squares Petrov-Galerkin projection method to define a reduced-order model for the heat, wave, and Kuramoto-Shivasinsky equations using the same autoencoder.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145086550",
                        "name": "Sreeram Venkat"
                    },
                    {
                        "authorId": "2108984905",
                        "name": "Ralph C. Smith"
                    },
                    {
                        "authorId": "145204338",
                        "name": "C. Kelley"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e179bccc99bacf35018e42c58013043f4df7b7e9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-11417",
                    "ArXiv": "2108.11417",
                    "CorpusId": 237304254
                },
                "corpusId": 237304254,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e179bccc99bacf35018e42c58013043f4df7b7e9",
                "title": "Unsupervised Reservoir Computing for Solving Ordinary Differential Equations",
                "abstract": "There is a wave of interest in using unsupervised neural networks for solving differential equations. The existing methods are based on feed-forward networks, {while} recurrent neural network differential equation solvers have not yet been reported. We introduce an unsupervised reservoir computing (RC), an echo-state recurrent neural network capable of discovering approximate solutions that satisfy ordinary differential equations (ODEs). We suggest an approach to calculate time derivatives of recurrent neural network outputs without using backpropagation. The internal weights of an RC are fixed, while only a linear output layer is trained, yielding efficient training. However, RC performance strongly depends on finding the optimal hyper-parameters, which is a computationally expensive process. We use Bayesian optimization to efficiently discover optimal sets in a high-dimensional hyper-parameter space and numerically show that one set is robust and can be used to solve an ODE for different initial conditions and time ranges. A closed-form formula for the optimal output weights is derived to solve first order linear equations in a backpropagation-free learning process. We extend the RC approach by solving nonlinear system of ODEs using a hybrid optimization method consisting of gradient descent and Bayesian optimization. Evaluation of linear and nonlinear systems of equations demonstrates the efficiency of the RC ODE solver.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145324933",
                        "name": "M. Mattheakis"
                    },
                    {
                        "authorId": "2124445841",
                        "name": "H. Joy"
                    },
                    {
                        "authorId": "1735677",
                        "name": "P. Protopapas"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "fd8bfb547d302b8b4e7a560a72a0b317611407a4",
                "externalIds": {
                    "PubMedCentral": "8419223",
                    "DBLP": "journals/ficn/PizloB21",
                    "DOI": "10.3389/fncom.2021.681162",
                    "CorpusId": 237262253,
                    "PubMed": "34497499"
                },
                "corpusId": 237262253,
                "publicationVenue": {
                    "id": "8c456f98-9892-42ac-9b16-418755f01550",
                    "name": "Frontiers in Computational Neuroscience",
                    "type": "journal",
                    "alternate_names": [
                        "Front Comput Neurosci"
                    ],
                    "issn": "1662-5188",
                    "url": "http://www.frontiersin.org/computational_neuroscience",
                    "alternate_urls": [
                        "http://www.frontiersin.org/computationalneuroscience/",
                        "https://www.frontiersin.org/journals/computational-neuroscience"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fd8bfb547d302b8b4e7a560a72a0b317611407a4",
                "title": "The Concept of Symmetry and the Theory of Perception",
                "abstract": "Perceptual constancy refers to the fact that the perceived geometrical and physical characteristics of objects remain constant despite transformations of the objects such as rigid motion. Perceptual constancy is essential in everything we do, like recognition of familiar objects and scenes, planning and executing visual navigation, visuomotor coordination, and many more. Perceptual constancy would not exist without the geometrical and physical permanence of objects: their shape, size, and weight. Formally, perceptual constancy and permanence of objects are invariants, also known in mathematics and physics as symmetries. Symmetries of the Laws of Physics received a central status due to mathematical theorems of Emmy Noether formulated and proved over 100 years ago. These theorems connected symmetries of the physical laws to conservation laws through the least-action principle. We show how Noether's theorem is applied to mirror-symmetrical objects and establishes mental shape representation (perceptual conservation) through the application of a simplicity (least-action) principle. This way, the formalism of Noether's theorem provides a computational explanation of the relation between the physical world and its mental representation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3100774",
                        "name": "Z. Pizlo"
                    },
                    {
                        "authorId": "2100713",
                        "name": "J. A. Barros"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4ba4b939dfa9ff1c9a993c8f55b868891df7dd79",
                "externalIds": {
                    "MAG": "3197979987",
                    "ArXiv": "2205.08861",
                    "DOI": "10.1016/bs.agph.2021.06.003",
                    "CorpusId": 239681616
                },
                "corpusId": 239681616,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4ba4b939dfa9ff1c9a993c8f55b868891df7dd79",
                "title": "An introduction to variational inference in geophysical inverse problems",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2149172220",
                        "name": "Xin Zhang"
                    },
                    {
                        "authorId": "144891436",
                        "name": "M. Nawaz"
                    },
                    {
                        "authorId": "2145743499",
                        "name": "Xuebin Zhao"
                    },
                    {
                        "authorId": "152203022",
                        "name": "A. Curtis"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a2bcbab742b25b5edfd90154aa38935e0efb1906",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-04899",
                    "ArXiv": "2108.04899",
                    "CorpusId": 236976125
                },
                "corpusId": 236976125,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a2bcbab742b25b5edfd90154aa38935e0efb1906",
                "title": "Analysis of ODE2VAE with Examples",
                "abstract": "Deep generative models aim to learn underlying distributions that generate the observed data. Given the fact that the generative distribution may be complex and intractable, deep latent variable models use probabilistic frameworks to learn more expressive joint probability distributions over the data and their low-dimensional hidden variables. Learning complex probability distributions over sequential data without any supervision is a difficult task for deep generative models. Ordinary Differential Equation Variational Auto-Encoder (ODE2VAE) is a deep latent variable model that aims to learn complex distributions over high-dimensional sequential data and their low-dimensional representations. ODE2VAE infers continuous latent dynamics of the high-dimensional input in a low-dimensional hierarchical latent space. The hierarchical organization of the continuous latent space embeds a physics-guided inductive bias in the model. In this paper, we analyze the latent representations inferred by the ODE2VAE model over three different physical motion datasets: bouncing balls, projectile motion, and simple pendulum. Through our experiments, we explore the effects of the physics-guided inductive bias of the ODE2VAE model over the learned dynamical latent representations. We show that the model is able to learn meaningful latent representations to an extent without any supervision.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2000083157",
                        "name": "Batuhan Koyuncu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "(1) One line of work aims to understand how neural networks can be structured and trained to reproduce known physical system behavior, with the goal of designing general methods applicable in a variety of settings [8, 42, 4, 41, 37, 38, 26, 10, 50, 51].",
                "Past works have chosen systems of the types featured here: simple oscillators (both spring and pendulum [8]), particle systems with various interaction laws (gravity, spring forces, charges, cloth simulations, etc."
            ],
            "citingPaper": {
                "paperId": "60d77e04d6449d59b510fbf342fd1b3bde752082",
                "externalIds": {
                    "DBLP": "conf/nips/OtnessGBPPSZ21",
                    "ArXiv": "2108.07799",
                    "CorpusId": 237056825
                },
                "corpusId": 237056825,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/60d77e04d6449d59b510fbf342fd1b3bde752082",
                "title": "An Extensible Benchmark Suite for Learning to Simulate Physical Systems",
                "abstract": "Simulating physical systems is a core component of scientific computing, encompassing a wide range of physical domains and applications. Recently, there has been a surge in data-driven methods to complement traditional numerical simulations methods, motivated by the opportunity to reduce computational costs and/or learn new physical models leveraging access to large collections of data. However, the diversity of problem settings and applications has led to a plethora of approaches, each one evaluated on a different setup and with different evaluation metrics. We introduce a set of benchmark problems to take a step towards unified benchmarks and evaluation protocols. We propose four representative physical systems, as well as a collection of both widely used classical time integrators and representative data-driven methods (kernel-based, MLP, CNN, nearest neighbors). Our framework allows evaluating objectively and systematically the stability, accuracy, and computational efficiency of data-driven methods. Additionally, it is configurable to permit adjustments for accommodating other learning tasks and for establishing a foundation for future developments in machine learning for scientific computing.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "36031867",
                        "name": "Karl Otness"
                    },
                    {
                        "authorId": "117194411",
                        "name": "Arvi Gjoka"
                    },
                    {
                        "authorId": "143627859",
                        "name": "Joan Bruna"
                    },
                    {
                        "authorId": "3241132",
                        "name": "Daniele Panozzo"
                    },
                    {
                        "authorId": "2863679",
                        "name": "B. Peherstorfer"
                    },
                    {
                        "authorId": "17001536",
                        "name": "T. Schneider"
                    },
                    {
                        "authorId": "145516498",
                        "name": "D. Zorin"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "We mention here related work that embeds symmetries and invariances into neural network architectures: [41] embedded even/odd symmetry of a function and energy conservation into a neural network by adding special hub layers; [42] propose gauge equivariant CNN layers to capture rotational symmetry; [43] structures their networks following a Hamiltonian in order to learn physically conserved quantities and symmetries."
            ],
            "citingPaper": {
                "paperId": "b01175f87707a35fc6b7675ffb8549c78cfe6697",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-02537",
                    "ArXiv": "2108.02537",
                    "DOI": "10.1103/physrevresearch.4.023118",
                    "CorpusId": 236924702
                },
                "corpusId": 236924702,
                "publicationVenue": {
                    "id": "349f119f-f4ee-48cf-aedb-89bcb56ab8e3",
                    "name": "Physical Review Research",
                    "type": "journal",
                    "alternate_names": [
                        "Phys Rev Res"
                    ],
                    "issn": "2643-1564",
                    "url": "https://journals.aps.org/prresearch"
                },
                "url": "https://www.semanticscholar.org/paper/b01175f87707a35fc6b7675ffb8549c78cfe6697",
                "title": "Redatuming physical systems using symmetric autoencoders",
                "abstract": "This paper considers physical systems described by hidden states and indirectly observed through repeated measurements corrupted by unmodeled nuisance parameters. A network-based representation learns to disentangle the coherent information (relative to the state) from the incoherent nuisance information (relative to the sensing). Instead of physical models, the representation uses symmetry and stochastic regularization to inform an autoencoder architecture called SymAE. It enables redatuming, i.e., creating virtual data instances where the nuisances are uniformized across measurements.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "103860006",
                        "name": "P. Bharadwaj"
                    },
                    {
                        "authorId": "1825674252",
                        "name": "Matthew Li"
                    },
                    {
                        "authorId": "2991849",
                        "name": "L. Demanet"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "8b2e68e4084828703746ffd1d540e3736f87802e",
                "externalIds": {
                    "MAG": "3188882577",
                    "ArXiv": "2108.02492",
                    "DBLP": "journals/corr/abs-2108-02492",
                    "DOI": "10.1063/5.0065913",
                    "CorpusId": 236924292,
                    "PubMed": "35105124"
                },
                "corpusId": 236924292,
                "publicationVenue": {
                    "id": "30c0ded7-c8b4-473c-bbc0-f237234ac1a6",
                    "name": "Chaos",
                    "type": "journal",
                    "issn": "1054-1500",
                    "url": "http://chaos.aip.org/",
                    "alternate_urls": [
                        "https://aip.scitation.org/journal/cha"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8b2e68e4084828703746ffd1d540e3736f87802e",
                "title": "Symplectic integration of learned Hamiltonian systems",
                "abstract": "Hamiltonian systems are differential equations that describe systems in classical mechanics, plasma physics, and sampling problems. They exhibit many structural properties, such as a lack of attractors and the presence of conservation laws. To predict Hamiltonian dynamics based on discrete trajectory observations, the incorporation of prior knowledge about Hamiltonian structure greatly improves predictions. This is typically done by learning the system's Hamiltonian and then integrating the Hamiltonian vector field with a symplectic integrator. For this, however, Hamiltonian data need to be approximated based on trajectory observations. Moreover, the numerical integrator introduces an additional discretization error. In this article, we show that an inverse modified Hamiltonian structure adapted to the geometric integrator can be learned directly from observations. A separate approximation step for the Hamiltonian data is avoided. The inverse modified data compensate for the discretization error such that the discretization error is eliminated. The technique is developed for Gaussian processes.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51299251",
                        "name": "Christian Offen"
                    },
                    {
                        "authorId": "1419469651",
                        "name": "S. Ober-Blobaum"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "cf8bf68b10a79a386fe4b295289880c83ec89d2c",
                "externalIds": {
                    "MAG": "3187503080",
                    "DOI": "10.1146/annurev-environ-020220-061831",
                    "CorpusId": 238321691
                },
                "corpusId": 238321691,
                "publicationVenue": {
                    "id": "a356c48c-2a9e-497d-bb64-e34edaed7caa",
                    "name": "Annual Review Environment and Resources",
                    "type": "journal",
                    "alternate_names": [
                        "Annu Rev Environ Resour",
                        "Annual Review of Environment and Resources"
                    ],
                    "issn": "1543-5938",
                    "url": "https://www.annualreviews.org/journal/energy",
                    "alternate_urls": [
                        "https://www.annualreviews.org/loi/energy",
                        "http://arjournals.annualreviews.org/loi/energy",
                        "http://arjournals.annualreviews.org/loi/energy?cookieSet=1",
                        "http://www.annualreviews.org/loi/energy"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cf8bf68b10a79a386fe4b295289880c83ec89d2c",
                "title": "Machine Learning for Sustainable Energy Systems",
                "abstract": "In recent years, machine learning has proven to be a powerful tool for deriving insights from data. In this review, we describe ways in which machine learning has been leveraged to facilitate the development and operation of sustainable energy systems. We first provide a taxonomy of machine learning paradigms and techniques, along with a discussion of their strengths and limitations. We then provide an overview of existing research using machine learning for sustainable energy production, delivery, and storage. Finally, we identify gaps in this literature, propose future research directions, and discuss important considerations for deployment. Expected final online publication date for the Annual Review of Environment and Resources, Volume 46 is October 2021. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49698491",
                        "name": "P. Donti"
                    },
                    {
                        "authorId": "145116464",
                        "name": "J. Z. Kolter"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "However, in contrast we do not propagate the system by Hamiltonian dynamics, but learn a (deterministic) flow, as in Hamiltonian flows [23, 24].",
                "[23] Samuel Greydanus, Misko Dzamba, and Jason Yosinski."
            ],
            "citingPaper": {
                "paperId": "78f17d91c0d2df45d778bd8484f1260e5a2f61c5",
                "externalIds": {
                    "ArXiv": "2108.01590",
                    "DOI": "10.1103/physrevresearch.4.l042005",
                    "CorpusId": 236881209
                },
                "corpusId": 236881209,
                "publicationVenue": {
                    "id": "349f119f-f4ee-48cf-aedb-89bcb56ab8e3",
                    "name": "Physical Review Research",
                    "type": "journal",
                    "alternate_names": [
                        "Phys Rev Res"
                    ],
                    "issn": "2643-1564",
                    "url": "https://journals.aps.org/prresearch"
                },
                "url": "https://www.semanticscholar.org/paper/78f17d91c0d2df45d778bd8484f1260e5a2f61c5",
                "title": "Temperature steerable flows and Boltzmann generators",
                "abstract": "Boltzmann generators approach the sampling problem in many-body physics by combining a normalizing flow and a statistical reweighting method to generate samples in thermodynamic equilibrium. The equilibrium distribution is usually defined by an energy function and a thermodynamic state. Here we propose temperature-steerable flows (TSF) which are able to generate a family of probability densities parametrized by a choosable temperature parameter. TSFs can be embedded in generalized ensemble sampling frameworks to sample a physical system across multiple thermodynamic states.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "116366008",
                        "name": "Manuel Dibak"
                    },
                    {
                        "authorId": "1380082499",
                        "name": "Leon Klein"
                    },
                    {
                        "authorId": "2078973772",
                        "name": "Andreas Kr\u00e4mer"
                    },
                    {
                        "authorId": "2122593997",
                        "name": "Frank No\u00e9"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "For this pendulum system, we have dp dt = \u22122mglsin(q), wherem = 1 (mass), g = 3 (gravity constant), l = 1 (length of the pendulum) (as in [Greydanus et al., 2019]).",
                "Robust Mimicking from Domain Knowledge Model We investigate a pendulum system [Greydanus et al., 2019] with mass m and length l, and collect the data and model\nfitting results during the training process.",
                "\u2022 Spring [Greydanus et al., 2019].",
                "4 RQ2: How the Domain Knowledge Helps Robust Mimicking from Domain Knowledge Model We investigate a pendulum system [Greydanus et al., 2019] with mass m and length l, and collect the data and model fitting results during the training process.",
                "\u2022 Pend [Greydanus et al., 2019].",
                "For instance, an ideal pendulum system can be described by the following equation [Greydanus et al., 2019]."
            ],
            "citingPaper": {
                "paperId": "15fd3bd06f0bf505956b2987e294bd469c18cb79",
                "externalIds": {
                    "DBLP": "conf/ijcai/ZhengL0JC0L21",
                    "DOI": "10.24963/ijcai.2021/228",
                    "CorpusId": 237100167
                },
                "corpusId": 237100167,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/15fd3bd06f0bf505956b2987e294bd469c18cb79",
                "title": "Knowledge-based Residual Learning",
                "abstract": "Small data has been a barrier for many machine learning tasks, especially when applied in scientific domains. Fortunately, we can utilize domain knowledge to make up the lack of data. Hence, in this paper, we propose a hybrid model KRL that treats domain knowledge model as a weak learner and uses another neural net model to boost it. We prove that KRL is guaranteed to improve over pure domain knowledge model and pure neural net model under certain loss functions. Extensive experiments have shown the superior performance of KRL over baselines. In addition, several case studies have explained how the domain knowledge can assist the prediction.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "22698655",
                        "name": "Guanjie Zheng"
                    },
                    {
                        "authorId": "2118484750",
                        "name": "Chang Liu"
                    },
                    {
                        "authorId": "1474226770",
                        "name": "Hua Wei"
                    },
                    {
                        "authorId": "83816065",
                        "name": "P. Jenkins"
                    },
                    {
                        "authorId": "150946029",
                        "name": "Chacha Chen"
                    },
                    {
                        "authorId": "2055590125",
                        "name": "Tao Wen"
                    },
                    {
                        "authorId": "2109640666",
                        "name": "Z. Li"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "29428ff15dc70c3e95a4dd4706b5e17ce9024ca5",
                "externalIds": {
                    "ArXiv": "2107.12996",
                    "DOI": "10.1016/j.physd.2021.133122",
                    "CorpusId": 245081026
                },
                "corpusId": 245081026,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/29428ff15dc70c3e95a4dd4706b5e17ce9024ca5",
                "title": "Hamiltonian operator inference: Physics-preserving learning of reduced-order models for canonical Hamiltonian systems",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47557147",
                        "name": "Harsh Sharma"
                    },
                    {
                        "authorId": null,
                        "name": "Zhu Wang"
                    },
                    {
                        "authorId": "2053218326",
                        "name": "B. Kramer"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "49470fd8cf877ed7e7a9c724945f45904172a9d3",
                "externalIds": {
                    "ArXiv": "2108.02318",
                    "DBLP": "journals/corr/abs-2108-02318",
                    "PubMedCentral": "8866480",
                    "DOI": "10.1038/s41467-022-28571-7",
                    "CorpusId": 236924457,
                    "PubMed": "35197449"
                },
                "corpusId": 236924457,
                "publicationVenue": {
                    "id": "43b3f0f9-489a-4566-8164-02fafde3cd98",
                    "name": "Nature Communications",
                    "type": "journal",
                    "alternate_names": [
                        "Nat Commun"
                    ],
                    "issn": "2041-1723",
                    "url": "https://www.nature.com/ncomms/",
                    "alternate_urls": [
                        "http://www.nature.com/ncomms/about/index.html",
                        "http://www.nature.com/ncomms/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/49470fd8cf877ed7e7a9c724945f45904172a9d3",
                "title": "Forecasting the outcome of spintronic experiments with Neural Ordinary Differential Equations",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118652759",
                        "name": "Xing Chen"
                    },
                    {
                        "authorId": "39269248",
                        "name": "Flavio Abreu Araujo"
                    },
                    {
                        "authorId": "34802117",
                        "name": "M. Riou"
                    },
                    {
                        "authorId": "46926922",
                        "name": "J. Torrejon"
                    },
                    {
                        "authorId": "48712320",
                        "name": "D. Ravelosona"
                    },
                    {
                        "authorId": "144754286",
                        "name": "W. Kang"
                    },
                    {
                        "authorId": "145487746",
                        "name": "Weisheng Zhao"
                    },
                    {
                        "authorId": "2595706",
                        "name": "J. Grollier"
                    },
                    {
                        "authorId": "1793153",
                        "name": "D. Querlioz"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "43aa49823fb62d906d39210b00026e1f348db456",
                "externalIds": {
                    "ArXiv": "2112.10765",
                    "DBLP": "journals/corr/abs-2112-10765",
                    "DOI": "10.1109/INDIN45523.2021.9557382",
                    "CorpusId": 238750331
                },
                "corpusId": 238750331,
                "publicationVenue": {
                    "id": "b6744d41-3aae-472d-98cf-21f4373115b2",
                    "name": "International Conference on Industrial Informatics",
                    "type": "conference",
                    "alternate_names": [
                        "INDIN",
                        "Int Conf Ind Informatics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/43aa49823fb62d906d39210b00026e1f348db456",
                "title": "A Grid-Structured Model of Tubular Reactors",
                "abstract": "We propose a grid-like computational model of tubular reactors. The architecture is inspired by the computations performed by solvers of partial differential equations which describe the dynamics of the chemical process inside a tubular reactor. The proposed model may be entirely based on the known form of the partial differential equations or it may contain generic machine learning components such as multi-layer perceptrons. We show that the proposed model can be trained using limited amounts of data to describe the state of a fixed-bed catalytic reactor. The trained model can reconstruct unmeasured states such as the catalyst activity using the measurements of inlet concentrations and temperatures along the reactor.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2132485673",
                        "name": "Katsiaryna Haitsiukevich"
                    },
                    {
                        "authorId": "145293685",
                        "name": "S. Bergman"
                    },
                    {
                        "authorId": "91937377",
                        "name": "C. D. A. Filho"
                    },
                    {
                        "authorId": "2478137",
                        "name": "F. Corona"
                    },
                    {
                        "authorId": "145096481",
                        "name": "A. Ilin"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "bc96f3f29b5b7076460f75aed67077ac04ff69e3",
                "externalIds": {
                    "DBLP": "journals/tog/RomeroCPO21",
                    "DOI": "10.1145/3450626.3459875",
                    "CorpusId": 236094962
                },
                "corpusId": 236094962,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bc96f3f29b5b7076460f75aed67077ac04ff69e3",
                "title": "Learning contact corrections for handle-based subspace dynamics",
                "abstract": "This paper introduces a novel subspace method for the simulation of dynamic deformations. The method augments existing linear handle-based subspace formulations with nonlinear learning-based corrections parameterized by the same subspace. Together, they produce a compact nonlinear model that combines the fast dynamics and overall contact-based interaction of subspace methods, with the highly detailed deformations of learning-based methods. We propose a formulation of the model with nonlinear corrections applied on the local undeformed setting, and decoupling internal and external contact-driven corrections. We define a simple mapping of these corrections to the global setting, an efficient implementation for dynamic simulation, and a training pipeline to generate examples that efficiently cover the interaction space. Altogether, the method achieves unprecedented combination of speed and contact-driven deformation detail.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2053850657",
                        "name": "Cristian Romero"
                    },
                    {
                        "authorId": "1863006",
                        "name": "D. Casas"
                    },
                    {
                        "authorId": "144414497",
                        "name": "Jes\u00fas P\u00e9rez"
                    },
                    {
                        "authorId": "1704342",
                        "name": "M. Otaduy"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "2f6f3d8c0db77b5c1fc5a663b723e17380c63a6f",
                "externalIds": {
                    "DOI": "10.1145/3476576.3476703",
                    "CorpusId": 236004155
                },
                "corpusId": 236004155,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2f6f3d8c0db77b5c1fc5a663b723e17380c63a6f",
                "title": "Learning contact corrections for handle-based subspace dynamics",
                "abstract": "This paper introduces a novel subspace method for the simulation of dynamic deformations. The method augments existing linear handle-based subspace formulations with nonlinear learning-based corrections parameterized by the same subspace. Together, they produce a compact nonlinear model that combines the fast dynamics and overall contact-based interaction of subspace methods, with the highly detailed deformations of learning-based methods. We propose a formulation of the model with nonlinear corrections applied on the local undeformed setting, and decoupling internal and external contact-driven corrections. We define a simple mapping of these corrections to the global setting, an efficient implementation for dynamic simulation, and a training pipeline to generate examples that efficiently cover the interaction space. Altogether, the method achieves unprecedented combination of speed and contact-driven deformation detail.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2053850657",
                        "name": "Cristian Romero"
                    },
                    {
                        "authorId": "1863006",
                        "name": "D. Casas"
                    },
                    {
                        "authorId": "144414497",
                        "name": "Jes\u00fas P\u00e9rez"
                    },
                    {
                        "authorId": "1704342",
                        "name": "M. Otaduy"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "In [9] and [6] it has been shown that by learning the Hamiltonian or the Lagrangian of a system, it is possible to accurately predict the temporal dynamics and conserve energy.",
                "Using the Hamiltonian formalism, [6] showed that a NN with parameters \u03b8 can be used to learn a Hamiltonian H\u03b8(q,p) given q and p as inputs to the network.",
                "1(c) [6] take q and p as inputs and are trained to yield the time derivatives of the input, with the HNN learning an intermediate Hamiltonian and employing backpropagation to compute the final output.",
                "However, their performance in learning and generalizing the long-term behaviour of dynamic systems governed by known physical laws from state data has often been limited [6, 7].",
                "Recently, the authors of [6] demonstrated that the dynamics of an energy conserving autonomous system can be accurately learned by guiding a neural network to prear X iv :2 10 7.",
                "Concretely, it has been shown that physicallyinformed learning biases embedded in networks, such as Hamiltonian mechanics [6, 8], Lagrangians [9, 10], Ordinary Differential Equations (ODEs) [11], physicsinformed networks [12, 13], generative networks [14], and Graph Neural Networks [15, 16] can significantly improve learning and generalization over vanilla neural networks in complex physical domains."
            ],
            "citingPaper": {
                "paperId": "ea8b902ca4c33236435dacdf30cea0b964c9c1eb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-08024",
                    "ArXiv": "2107.08024",
                    "DOI": "10.1103/PhysRevE.104.034312",
                    "CorpusId": 236034262,
                    "PubMed": "34654178"
                },
                "corpusId": 236034262,
                "publicationVenue": {
                    "id": "19842b7b-a4d1-4f9a-9714-87d878cf6e73",
                    "name": "Physical Review E",
                    "type": "journal",
                    "alternate_names": [
                        "Phys rev E",
                        "Physical review. E",
                        "Phys Rev E"
                    ],
                    "issn": "1539-3755",
                    "alternate_issns": [
                        "1550-2376",
                        "2470-0045"
                    ],
                    "url": "https://journals.aps.org/pre/",
                    "alternate_urls": [
                        "http://pre.aps.org/",
                        "http://journals.aps.org/pre/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ea8b902ca4c33236435dacdf30cea0b964c9c1eb",
                "title": "Port-Hamiltonian Neural Networks for Learning Explicit Time-Dependent Dynamical Systems",
                "abstract": "Accurately learning the temporal behavior of dynamical systems requires models with well-chosen learning biases. Recent innovations embed the Hamiltonian and Lagrangian formalisms into neural networks and demonstrate a significant improvement over other approaches in predicting trajectories of physical systems. These methods generally tackle autonomous systems that depend implicitly on time or systems for which a control signal is known a priori. Despite this success, many real world dynamical systems are nonautonomous, driven by time-dependent forces and experience energy dissipation. In this study, we address the challenge of learning from such nonautonomous systems by embedding the port-Hamiltonian formalism into neural networks, a versatile framework that can capture energy dissipation and time-dependent control forces. We show that the proposed port-Hamiltonian neural network can efficiently learn the dynamics of nonlinear physical systems of practical interest and accurately recover the underlying stationary Hamiltonian, time-dependent force, and dissipative coefficient. A promising outcome of our network is its ability to learn and predict chaotic systems such as the Duffing equation, for which the trajectories are typically hard to learn.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144446378",
                        "name": "Shaan Desai"
                    },
                    {
                        "authorId": "145324933",
                        "name": "M. Mattheakis"
                    },
                    {
                        "authorId": "144633639",
                        "name": "David Sondak"
                    },
                    {
                        "authorId": "1735677",
                        "name": "P. Protopapas"
                    },
                    {
                        "authorId": "143841496",
                        "name": "Stephen J. Roberts"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", bioinformatics [1], computer visions [2,3], finance [4,5], and physics [6, 7]."
            ],
            "citingPaper": {
                "paperId": "70e7e64eddc5011447b7a1412dbe03726c2aecd7",
                "externalIds": {
                    "PubMedCentral": "8789868",
                    "ArXiv": "2107.05808",
                    "DOI": "10.1038/s41598-022-05061-w",
                    "CorpusId": 235829247,
                    "PubMed": "35079045"
                },
                "corpusId": 235829247,
                "publicationVenue": {
                    "id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                    "name": "Scientific Reports",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Rep"
                    ],
                    "issn": "2045-2322",
                    "url": "http://www.nature.com/srep/",
                    "alternate_urls": [
                        "http://www.nature.com/srep/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/70e7e64eddc5011447b7a1412dbe03726c2aecd7",
                "title": "Natural quantum reservoir computing for temporal information processing",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2150936482",
                        "name": "Yudai Suzuki"
                    },
                    {
                        "authorId": "2088245576",
                        "name": "Qi Gao"
                    },
                    {
                        "authorId": "4063322",
                        "name": "K. Pradel"
                    },
                    {
                        "authorId": "3016926",
                        "name": "K. Yasuoka"
                    },
                    {
                        "authorId": "51014591",
                        "name": "N. Yamamoto"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "7ac3e74f927b01f2b54661b09d9014a4e3a77bf8",
                "externalIds": {
                    "ArXiv": "2107.01272",
                    "DBLP": "journals/corr/abs-2107-01272",
                    "CorpusId": 235732074
                },
                "corpusId": 235732074,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7ac3e74f927b01f2b54661b09d9014a4e3a77bf8",
                "title": "Physics-Guided Deep Learning for Dynamical Systems: A survey",
                "abstract": "Modeling complex physical dynamics is a fundamental task in science and engineering. Traditional physics-based models are sample efficient, and interpretable but often rely on rigid assumptions. Furthermore, direct numerical approximation is usually computationally intensive, requiring significant computational resources and expertise, and many real-world systems do not have fully-known governing laws. While deep learning (DL) provides novel alternatives for efficiently recognizing complex patterns and emulating nonlinear dynamics, its predictions do not necessarily obey the governing laws of physical systems, nor do they generalize well across different systems. Thus, the study of physics-guided DL emerged and has gained great progress. Physics-guided DL aims to take the best from both physics-based modeling and state-of-the-art DL models to better solve scientific problems. In this paper, we provide a structured overview of existing methodologies of integrating prior physical knowledge or physics-based modeling into DL, with a special emphasis on learning dynamical systems. We also discuss the fundamental challenges and emerging opportunities in the area.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2151036763",
                        "name": "Rui Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "f119c18272b7a720e776d7657fed13ee10890254",
                "externalIds": {
                    "DBLP": "journals/jcphy/ZhaoBB21",
                    "MAG": "3138736071",
                    "DOI": "10.1016/J.JCP.2021.110279",
                    "CorpusId": 234354821
                },
                "corpusId": 234354821,
                "publicationVenue": {
                    "id": "58606051-2e63-4034-8496-9d4ed773bb49",
                    "name": "Journal of Computational Physics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Phys"
                    ],
                    "issn": "0021-9991",
                    "url": "http://www.elsevier.com/locate/jcp",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00219991",
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622866/description#description",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f119c18272b7a720e776d7657fed13ee10890254",
                "title": "Image inversion and uncertainty quantification for constitutive laws of pattern formation",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47941240",
                        "name": "Hongbo Zhao"
                    },
                    {
                        "authorId": "144960825",
                        "name": "R. Braatz"
                    },
                    {
                        "authorId": "2964245",
                        "name": "M. Bazant"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "da713054aab2e9d06146beea5093b3a29cbd8609",
                "externalIds": {
                    "DBLP": "journals/sensors/MahonyCKCWR21",
                    "PubMedCentral": "8271830",
                    "DOI": "10.3390/s21134486",
                    "CorpusId": 235708740,
                    "PubMed": "34209075"
                },
                "corpusId": 235708740,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/da713054aab2e9d06146beea5093b3a29cbd8609",
                "title": "Representation Learning for Fine-Grained Change Detection",
                "abstract": "Fine-grained change detection in sensor data is very challenging for artificial intelligence though it is critically important in practice. It is the process of identifying differences in the state of an object or phenomenon where the differences are class-specific and are difficult to generalise. As a result, many recent technologies that leverage big data and deep learning struggle with this task. This review focuses on the state-of-the-art methods, applications, and challenges of representation learning for fine-grained change detection. Our research focuses on methods of harnessing the latent metric space of representation learning techniques as an interim output for hybrid human-machine intelligence. We review methods for transforming and projecting embedding space such that significant changes can be communicated more effectively and a more comprehensive interpretation of underlying relationships in sensor data is facilitated. We conduct this research in our work towards developing a method for aligning the axes of latent embedding space with meaningful real-world metrics so that the reasoning behind the detection of change in relation to past observations may be revealed and adjusted. This is an important topic in many fields concerned with producing more meaningful and explainable outputs from deep learning and also for providing means for knowledge injection and model calibration in order to maintain user confidence.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3692743",
                        "name": "Niall O' Mahony"
                    },
                    {
                        "authorId": "145911154",
                        "name": "S. Campbell"
                    },
                    {
                        "authorId": "15909050",
                        "name": "L. Krpalkova"
                    },
                    {
                        "authorId": "133574842",
                        "name": "A. Carvalho"
                    },
                    {
                        "authorId": "145975195",
                        "name": "Joseph Walsh"
                    },
                    {
                        "authorId": "50207148",
                        "name": "D. Riordan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "cfc5695b9d8c0793b7bd767897ab8f5048f01e06",
                "externalIds": {
                    "DBLP": "journals/pami/MoyaBGCC23",
                    "ArXiv": "2106.13301",
                    "DOI": "10.1109/TPAMI.2022.3160100",
                    "CorpusId": 235652364,
                    "PubMed": "35316181"
                },
                "corpusId": 235652364,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cfc5695b9d8c0793b7bd767897ab8f5048f01e06",
                "title": "Physics Perception in Sloshing Scenes With Guaranteed Thermodynamic Consistency",
                "abstract": "Physics perception very often faces the problem that only limited data or partial measurements on the scene are available. In this work, we propose a strategy to learn the full state of sloshing liquids from measurements of the free surface. Our approach is based on recurrent neural networks (RNN) that project the limited information available to a reduced-order manifold to not only reconstruct the unknown information but also be capable of performing fluid reasoning about future scenarios in real-time. To obtain physically consistent predictions, we train deep neural networks on the reduced-order manifold that, through the employ of inductive biases, ensure the fulfillment of the principles of thermodynamics. RNNs learn from history the required hidden information to correlate the limited information with the latent space where the simulation occurs. Finally, a decoder returns data to the high-dimensional manifold, to provide the user with insightful information in the form of augmented reality. This algorithm is connected to a computer vision system to test the performance of the proposed methodology with real information, resulting in a system capable of understanding and predicting future states of the observed fluid in real-time.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "69035385",
                        "name": "B. Moya"
                    },
                    {
                        "authorId": "51181453",
                        "name": "A. Badias"
                    },
                    {
                        "authorId": "47723344",
                        "name": "D. Gonz\u00e1lez"
                    },
                    {
                        "authorId": "2734584",
                        "name": "F. Chinesta"
                    },
                    {
                        "authorId": "3246447",
                        "name": "E. Cueto"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                ", using a prior distribution [9], a graph-network forward kinematic model [34], or a network architecture reflecting the structure of Lagrangian [22] or Hamiltonian [13] mechanical systems.",
                "Models designed with structure respecting kinematic constraints [34], symmetry [33, 39], Lagrangian mechanics [32, 22, 14, 8, 21] or Hamiltonian mechanics [13, 3, 5, 11, 43, 41] guarantee that the laws of physics are satisfied by construction, regardless of the training data.",
                "[13] model the Hamiltonian as a neural network and update its parameters by minimizing the discrepancy between its symplectic gradients and the time derivatives of the states (q, p).",
                "Hamiltonian-based methods [13, 3, 5, 11, 43, 41] use a Hamiltonian formulation [20, 15] of the system dynamics, instead, in terms of generalized coordinates q, generalized momenta p, and a Hamiltonian function, H(q, p), representing the total energy of the system.",
                "Lagrangian and Hamiltonian mechanics [20, 15] provide physical system descriptions that can be integrated into the structure of a neural network [13, 3, 5, 11, 43, 41].",
                "Recent works [22, 14, 8, 13, 5, 32] have considered a hybrid approach to this problem, where prior knowledge of the physics, governing the system dynamics, is used to assist the learning process."
            ],
            "citingPaper": {
                "paperId": "5873f8ebd98fc77c8f7861b7e10464cc4c45bc71",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-12782",
                    "ArXiv": "2106.12782",
                    "DOI": "10.15607/RSS.2021.XVII.086",
                    "CorpusId": 235623870
                },
                "corpusId": 235623870,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5873f8ebd98fc77c8f7861b7e10464cc4c45bc71",
                "title": "Hamiltonian-based Neural ODE Networks on the SE(3) Manifold For Dynamics Learning and Control",
                "abstract": "Accurate models of robot dynamics are critical for safe and stable control and generalization to novel operational conditions. Hand-designed models, however, may be insufficiently accurate, even after careful parameter tuning. This motivates the use of machine learning techniques to approximate the robot dynamics over a training set of state-control trajectories. The dynamics of many robots, including ground, aerial, and underwater vehicles, are described in terms of their SE(3) pose and generalized velocity, and satisfy conservation of energy principles. This paper proposes a Hamiltonian formulation over the SE(3) manifold of the structure of a neural ordinary differential equation (ODE) network to approximate the dynamics of a rigid body. In contrast to a black-box ODE network, our formulation guarantees total energy conservation by construction. We develop energy shaping and damping injection control for the learned, potentially under-actuated SE(3) Hamiltonian dynamics to enable a unified approach for stabilization and trajectory tracking with various platforms, including pendulum, rigid-body, and quadrotor systems.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2344318",
                        "name": "T. Duong"
                    },
                    {
                        "authorId": "50365495",
                        "name": "Nikolay A. Atanasov"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Many recent approaches have turned to structure preserving models of reversible dynamics to obtain an inductive bias that lies in between [4, 5, 6, 7, 8].",
                "Structure preserving neural networks A thorough accounting of works embedding structurepreservation into neural networks include pioneering works for Hamiltonian neural networks [4, 40], followed by development of Lagragian neural networks [41, 5] and neural networks that mimic the action of symplectic integrators [6, 7, 8]."
            ],
            "citingPaper": {
                "paperId": "2a2dd0fa4864d34874e7b1e2e39564c3147cd71e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-12619",
                    "ArXiv": "2106.12619",
                    "CorpusId": 235623998
                },
                "corpusId": 235623998,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2a2dd0fa4864d34874e7b1e2e39564c3147cd71e",
                "title": "Machine learning structure preserving brackets for forecasting irreversible processes",
                "abstract": "Forecasting of time-series data requires imposition of inductive biases to obtain predictive extrapolation, and recent works have imposed Hamiltonian/Lagrangian form to preserve structure for systems with reversible dynamics. In this work we present a novel parameterization of dissipative brackets from metriplectic dynamical systems appropriate for learning irreversible dynamics with unknown a priori model form. The process learns generalized Casimirs for energy and entropy guaranteed to be conserved and nondecreasing, respectively. Furthermore, for the case of added thermal noise, we guarantee exact preservation of a fluctuation-dissipation theorem, ensuring thermodynamic consistency. We provide benchmarks for dissipative systems demonstrating learned dynamics are more robust and generalize better than either\"black-box\"or penalty-based approaches.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3041967",
                        "name": "Kookjin Lee"
                    },
                    {
                        "authorId": "38723595",
                        "name": "Nathaniel Trask"
                    },
                    {
                        "authorId": "153799305",
                        "name": "P. Stinis"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "1 Hamiltonian Neural Networks In contrast to obtaining trajectories from a known Hamiltonian, the purpose of Hamiltonian Neural Networks (HNNs) [13] is to learn a Hamiltonian from data, composed of observed trajectories y(t) which solve Hamilton\u2019s equation (1).",
                "[13] have recently proposed a clever class of Hamiltonian Neural Networks (HNNs) whose architecture (see Figure 1) engraves the mathematical properties of Hamilton\u2019s equations (notably their symplecticity).",
                "[13], HNNs have generated much scientific interest.",
                "[13], its loss function1 for one data point (y0, y1) is LHNN = \u2225\u2225\u2225y1 \u2212 y0 h \u2212 J\u2207\u0124(y0) \u2225\u2225\u22252 L2 (5) (1)Note that Greydanus et al.",
                "The forward Euler scheme s = y0 replicates HNNs [13] trained with discretized data and represents our baseline.",
                "[13] used the analytic gradient of the true Hamiltonian as the target for most tasks, which yields a different mathematical problem, i."
            ],
            "citingPaper": {
                "paperId": "b02302d7b2e8226c9dbf161f3568b2b5a737c2f9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-11753",
                    "ArXiv": "2106.11753",
                    "DOI": "10.1016/j.jcp.2023.112495",
                    "CorpusId": 235593084
                },
                "corpusId": 235593084,
                "publicationVenue": {
                    "id": "58606051-2e63-4034-8496-9d4ed773bb49",
                    "name": "Journal of Computational Physics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Phys"
                    ],
                    "issn": "0021-9991",
                    "url": "http://www.elsevier.com/locate/jcp",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00219991",
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622866/description#description",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b02302d7b2e8226c9dbf161f3568b2b5a737c2f9",
                "title": "Symplectic Learning for Hamiltonian Neural Networks",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118633631",
                        "name": "M. David"
                    },
                    {
                        "authorId": "2074471152",
                        "name": "Florian M'ehats"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026are derived from first principles, e.g., rigid body dynamics (Wittenburg, 2013), mass action kinetics (Ingalls, 2013), or Hamiltonian dynamics (Greydanus et al., 2019), or chosen for computational convenience (e.g., linear systems (Ljung, 1998)) or parametrized to facilitate system\u2026",
                ", rigid body dynamics (Wittenburg, 2013), mass action kinetics (Ingalls, 2013), or Hamiltonian dynamics (Greydanus et al., 2019), or chosen for computational convenience (e."
            ],
            "citingPaper": {
                "paperId": "7fdc0cee050e58257bb3f8e011cecaa825fcb5e9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-11609",
                    "ArXiv": "2106.11609",
                    "CorpusId": 235593029
                },
                "corpusId": 235593029,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7fdc0cee050e58257bb3f8e011cecaa825fcb5e9",
                "title": "Distributional Gradient Matching for Learning Uncertain Neural Dynamics Models",
                "abstract": "Differential equations in general and neural ODEs in particular are an essential technique in continuous-time system identification. While many deterministic learning algorithms have been designed based on numerical integration via the adjoint method, many downstream tasks such as active learning, exploration in reinforcement learning, robust control, or filtering require accurate estimates of predictive uncertainties. In this work, we propose a novel approach towards estimating epistemically uncertain neural ODEs, avoiding the numerical integration bottleneck. Instead of modeling uncertainty in the ODE parameters, we directly model uncertainties in the state space. Our algorithm - distributional gradient matching (DGM) - jointly trains a smoother and a dynamics model and matches their gradients via minimizing a Wasserstein loss. Our experiments show that, compared to traditional approximate inference methods based on numerical integration, our approach is faster to train, faster at predicting previously unseen trajectories, and in the context of neural ODEs, significantly more accurate.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1753424594",
                        "name": "L. Treven"
                    },
                    {
                        "authorId": "41023002",
                        "name": "Philippe Wenk"
                    },
                    {
                        "authorId": "47806072",
                        "name": "Florian Dorfler"
                    },
                    {
                        "authorId": "145343838",
                        "name": "Andreas Krause"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026paradigm has successfully guided the discovery of novel deep learning models, with applications in prediction (Rubanova et al., 2019; Greydanus et al., 2019), control (Du et al., 2020), density estimation (Grathwohl et al., 2018; Lou et al., 2020; Mathieu and Nickel, 2020), time\u2026"
            ],
            "citingPaper": {
                "paperId": "460307e2b0582aae32a9b3c95579c55672c1d1e0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-11581",
                    "ArXiv": "2106.11581",
                    "CorpusId": 235593387
                },
                "corpusId": 235593387,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/460307e2b0582aae32a9b3c95579c55672c1d1e0",
                "title": "Continuous-Depth Neural Models for Dynamic Graph Prediction",
                "abstract": "We introduce the framework of continuous-depth graph neural networks (GNNs). Neural graph differential equations (Neural GDEs) are formalized as the counterpart to GNNs where the input-output relationship is determined by a continuum of GNN layers, blending discrete topological structures and differential equations. The proposed framework is shown to be compatible with static GNN models and is extended to dynamic and stochastic settings through hybrid dynamical system theory. Here, Neural GDEs improve performance by exploiting the underlying dynamics geometry, further introducing the ability to accommodate irregularly sampled data. Results prove the effectiveness of the proposed models across applications, such as traffic forecasting or prediction in genetic regulatory networks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40585370",
                        "name": "Michael Poli"
                    },
                    {
                        "authorId": "90467999",
                        "name": "Stefano Massaroli"
                    },
                    {
                        "authorId": "1387988662",
                        "name": "Clayton M. Rabideau"
                    },
                    {
                        "authorId": "1491104517",
                        "name": "Junyoung Park"
                    },
                    {
                        "authorId": "1748130",
                        "name": "A. Yamashita"
                    },
                    {
                        "authorId": "5312699",
                        "name": "H. Asama"
                    },
                    {
                        "authorId": "2085587",
                        "name": "Jinkyoo Park"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "7a004a4fcb774fb9505e683393bc2d23a2d14867",
                "externalIds": {
                    "DBLP": "journals/nn/ZhuJT22",
                    "ArXiv": "2106.10911",
                    "DOI": "10.1016/j.neunet.2021.12.007",
                    "CorpusId": 235490533,
                    "PubMed": "34995951"
                },
                "corpusId": 235490533,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7a004a4fcb774fb9505e683393bc2d23a2d14867",
                "title": "Approximation capabilities of measure-preserving neural networks",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2430811",
                        "name": "Aiqing Zhu"
                    },
                    {
                        "authorId": "121687074",
                        "name": "Pengzhan Jin"
                    },
                    {
                        "authorId": "1805110",
                        "name": "Yifa Tang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Inspired by this view, numerous ODE and PDE-based network architectures [1, 8, 9, 15, 32, 42, 48, 49], and continuous-time recurrent units [4, 29, 30, 40, 41] have been proposed."
            ],
            "citingPaper": {
                "paperId": "6ec408a30831ab535d3962224187c5873c4ab689",
                "externalIds": {
                    "ArXiv": "2106.10820",
                    "DBLP": "conf/nips/QueirugaEHM21",
                    "CorpusId": 243847840
                },
                "corpusId": 243847840,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6ec408a30831ab535d3962224187c5873c4ab689",
                "title": "Stateful ODE-Nets using Basis Function Expansions",
                "abstract": "The recently-introduced class of ordinary differential equation networks (ODE-Nets) establishes a fruitful connection between deep learning and dynamical systems. In this work, we reconsider formulations of the weights as continuous-in-depth functions using linear combinations of basis functions which enables us to leverage parameter transformations such as function projections. In turn, this view allows us to formulate a novel stateful ODE-Block that handles stateful layers. The benefits of this new ODE-Block are twofold: first, it enables incorporating meaningful continuous-in-depth batch normalization layers to achieve state-of-the-art performance; second, it enables compressing the weights through a change of basis, without retraining, while maintaining near state-of-the-art performance and reducing both inference time and memory footprint. Performance is demonstrated by applying our stateful ODE-Block to (a) image classification tasks using convolutional units and (b) sentence-tagging tasks using transformer encoder units.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40897456",
                        "name": "A. Queiruga"
                    },
                    {
                        "authorId": "2371914",
                        "name": "N. Benjamin Erichson"
                    },
                    {
                        "authorId": "2120830907",
                        "name": "Liam Hodgkinson"
                    },
                    {
                        "authorId": "1717098",
                        "name": "Michael W. Mahoney"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Besides, it has been shown 84 that neural networks often struggle to learn [25] invariant properties of physical systems and other 85 qualitative properties."
            ],
            "citingPaper": {
                "paperId": "8f50478e82411b5e059892449a4de5f1475c1018",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-10533",
                    "ArXiv": "2106.10533",
                    "CorpusId": 235489871
                },
                "corpusId": 235489871,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8f50478e82411b5e059892449a4de5f1475c1018",
                "title": "Learning to Reach, Swim, Walk and Fly in One Trial: Data-Driven Control with Scarce Data and Side Information",
                "abstract": "We develop a learning-based control algorithm for unknown dynamical systems under very severe data limitations. Specifically, the algorithm has access to streaming and noisy data only from a single and ongoing trial. It accomplishes such performance by effectively leveraging various forms of side information on the dynamics to reduce the sample complexity. Such side information typically comes from elementary laws of physics and qualitative properties of the system. More precisely, the algorithm approximately solves an optimal control problem encoding the system's desired behavior. To this end, it constructs and iteratively refines a data-driven differential inclusion that contains the unknown vector field of the dynamics. The differential inclusion, used in an interval Taylor-based method, enables to over-approximate the set of states the system may reach. Theoretically, we establish a bound on the suboptimality of the approximate solution with respect to the optimal control with known dynamics. We show that the longer the trial or the more side information is available, the tighter the bound. Empirically, experiments in a high-fidelity F-16 aircraft simulator and MuJoCo's environments illustrate that, despite the scarcity of data, the algorithm can provide performance comparable to reinforcement learning algorithms trained over millions of environment interactions. Besides, we show that the algorithm outperforms existing techniques combining system identification and model predictive control.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1475901145",
                        "name": "Franck Djeumou"
                    },
                    {
                        "authorId": "3199888",
                        "name": "U. Topcu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Another interesting direction to pursue would be to apply novel physics-informed neural network architectures[63, 64] to resolve Hamiltonians of systems with many degrees of freedom (such as molecules) using time-resolved HHG spectra, such as those obtained from solids driven by mid-IR fields [24]."
            ],
            "citingPaper": {
                "paperId": "5626e4474e53a39a6adfe24e17bb5038f7299daf",
                "externalIds": {
                    "ArXiv": "2106.08638",
                    "CorpusId": 263794401
                },
                "corpusId": 263794401,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5626e4474e53a39a6adfe24e17bb5038f7299daf",
                "title": "Deep neural networks for high harmonic spectroscopy in solids",
                "abstract": "Neural networks are a prominent tool for identifying and modeling complex patterns, which are otherwise hard to detect and analyze. While machine learning and neural networks have been finding applications across many areas of science and technology, their use in decoding ultrafast dynamics of quantum systems driven by strong laser fields has been limited so far. Here we use deep neural networks to analyze simulated noisy spectra of highly nonlinear optical response of a 2-dimensional gapped graphene crystal to intense few-cycle laser pulses. We show that a computationally simple 1-dimensional system provides a useful\"nursery school\"for our neural network, allowing it to be easily retrained to treat more complex systems, recovering the band structure and spectral phases of the incident few-cycle pulse with high accuracy, in spite of significant amplitude noise and phase jitter. Our results both offer a new tool for attosecond spectroscopy of quantum dynamics in solids and also open a route to developing all-solid-state devices for complete characterization of few-cycle pulses, including their nonlinear chirp and the carrier envelope phase.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "102617464",
                        "name": "N. Klimkin"
                    },
                    {
                        "authorId": "1388345850",
                        "name": "'Alvaro Jim'enez-Gal'an"
                    },
                    {
                        "authorId": "2252994417",
                        "name": "Rui E. F. Silva"
                    },
                    {
                        "authorId": "2256719260",
                        "name": "Misha Ivanov"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "For the rule encoder (\u03c6r), data encoder (\u03c6d), and decision block (\u03c6), we use MLPs with ReLU activation at intermediate layers, similarly to [5, 16]."
            ],
            "citingPaper": {
                "paperId": "76d5f7120eca7a2745718b474570b9201b090adb",
                "externalIds": {
                    "DBLP": "conf/nips/SeoAYZSP21",
                    "ArXiv": "2106.07804",
                    "CorpusId": 235435676
                },
                "corpusId": 235435676,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/76d5f7120eca7a2745718b474570b9201b090adb",
                "title": "Controlling Neural Networks with Rule Representations",
                "abstract": "We propose a novel training method that integrates rules into deep learning, in a way the strengths of the rules are controllable at inference. Deep Neural Networks with Controllable Rule Representations (DeepCTRL) incorporates a rule encoder into the model coupled with a rule-based objective, enabling a shared representation for decision making. DeepCTRL is agnostic to data type and model architecture. It can be applied to any kind of rule defined for inputs and outputs. The key aspect of DeepCTRL is that it does not require retraining to adapt the rule strength -- at inference, the user can adjust it based on the desired operation point on accuracy vs. rule verification ratio. In real-world domains where incorporating rules is critical -- such as Physics, Retail and Healthcare -- we show the effectiveness of DeepCTRL in teaching rules for deep learning. DeepCTRL improves the trust and reliability of the trained models by significantly increasing their rule verification ratio, while also providing accuracy gains at downstream tasks. Additionally, DeepCTRL enables novel use cases such as hypothesis testing of the rules on data samples, and unsupervised adaptation based on shared rules between datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145260557",
                        "name": "Sungyong Seo"
                    },
                    {
                        "authorId": "2676352",
                        "name": "Sercan \u00d6. Arik"
                    },
                    {
                        "authorId": "2144029",
                        "name": "Jinsung Yoon"
                    },
                    {
                        "authorId": "48505793",
                        "name": "Xiang Zhang"
                    },
                    {
                        "authorId": "1729571",
                        "name": "Kihyuk Sohn"
                    },
                    {
                        "authorId": "1945962",
                        "name": "Tomas Pfister"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "More broadly, Hamiltonian networks have been shown to improve physical characteristics, such as better conservation of energy, and to better generalize [35, 80, 103], and Lagrangian neural networks can also enforce conservation laws [63, 20]."
            ],
            "citingPaper": {
                "paperId": "0d943f17e09cdb681f84c6bf3e7ea8b491bfdccc",
                "externalIds": {
                    "ArXiv": "2106.06610",
                    "DBLP": "conf/nips/VillarHSYB21",
                    "CorpusId": 240354206
                },
                "corpusId": 240354206,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0d943f17e09cdb681f84c6bf3e7ea8b491bfdccc",
                "title": "Scalars are universal: Equivariant machine learning, structured like classical physics",
                "abstract": "There has been enormous progress in the last few years in designing neural networks that respect the fundamental symmetries and coordinate freedoms of physical law. Some of these frameworks make use of irreducible representations, some make use of high-order tensor objects, and some apply symmetry-enforcing constraints. Different physical laws obey different combinations of fundamental symmetries, but a large fraction (possibly all) of classical physics is equivariant to translation, rotation, reflection (parity), boost (relativity), and permutations. Here we show that it is simple to parameterize universally approximating polynomial functions that are equivariant under these symmetries, or under the Euclidean, Lorentz, and Poincar\\'e groups, at any dimensionality $d$. The key observation is that nonlinear O($d$)-equivariant (and related-group-equivariant) functions can be universally expressed in terms of a lightweight collection of scalars -- scalar products and scalar contractions of the scalar, vector, and tensor inputs. We complement our theory with numerical examples that show that the scalar-based method is simple, efficient, and scalable.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144790990",
                        "name": "Soledad Villar"
                    },
                    {
                        "authorId": "144735014",
                        "name": "D. Hogg"
                    },
                    {
                        "authorId": "1463037423",
                        "name": "Kate Storey-Fisher"
                    },
                    {
                        "authorId": "29892797",
                        "name": "Weichi Yao"
                    },
                    {
                        "authorId": "1401969150",
                        "name": "Ben Blum-Smith"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "b8871be8abc8035e9c3e4c29b02505182806d771",
                "externalIds": {
                    "ArXiv": "2106.09004",
                    "DOI": "10.1063/5.0113632",
                    "CorpusId": 251040995,
                    "PubMed": "36859209"
                },
                "corpusId": 251040995,
                "publicationVenue": {
                    "id": "30c0ded7-c8b4-473c-bbc0-f237234ac1a6",
                    "name": "Chaos",
                    "type": "journal",
                    "issn": "1054-1500",
                    "url": "http://chaos.aip.org/",
                    "alternate_urls": [
                        "https://aip.scitation.org/journal/cha"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b8871be8abc8035e9c3e4c29b02505182806d771",
                "title": "Learning effective stochastic differential equations from microscopic simulations: Linking stochastic numerics to deep learning.",
                "abstract": "We identify effective stochastic differential equations (SDEs) for coarse observables of fine-grained particle- or agent-based simulations; these SDEs then provide useful coarse surrogate models of the fine scale dynamics. We approximate the drift and diffusivity functions in these effective SDEs through neural networks, which can be thought of as effective stochastic ResNets. The loss function is inspired by, and embodies, the structure of established stochastic numerical integrators (here, Euler-Maruyama and Milstein); our approximations can thus benefit from backward error analysis of these underlying numerical schemes. They also lend themselves naturally to \"physics-informed\" gray-box identification when approximate coarse models, such as mean field equations, are available. Existing numerical integration schemes for Langevin-type equations and for stochastic partial differential equations can also be used for training; we demonstrate this on a stochastically forced oscillator and the stochastic wave equation. Our approach does not require long trajectories, works on scattered snapshot data, and is designed to naturally handle different time steps per snapshot. We consider both the case where the coarse collective observables are known in advance, as well as the case where they must be found in a data-driven manner.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144399320",
                        "name": "Felix Dietrich"
                    },
                    {
                        "authorId": "39110802",
                        "name": "A. Makeev"
                    },
                    {
                        "authorId": "1689633894",
                        "name": "G. Kevrekidis"
                    },
                    {
                        "authorId": "2474715",
                        "name": "N. Evangelou"
                    },
                    {
                        "authorId": "31588522",
                        "name": "Tom S. Bertalan"
                    },
                    {
                        "authorId": "39203137",
                        "name": "S. Reich"
                    },
                    {
                        "authorId": "3439407",
                        "name": "I. Kevrekidis"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "6414e36236b32f9bb981e97628de3788d1f69927",
                "externalIds": {
                    "DBLP": "journals/ijon/GongMWWCML23",
                    "ArXiv": "2106.04166",
                    "DOI": "10.1016/j.neucom.2023.01.040",
                    "CorpusId": 235367924
                },
                "corpusId": 235367924,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6414e36236b32f9bb981e97628de3788d1f69927",
                "title": "Incorporating NODE with Pre-trained Neural Differential Operator for Learning Dynamics",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2058167003",
                        "name": "Shiqi Gong"
                    },
                    {
                        "authorId": "47580728",
                        "name": "Qi Meng"
                    },
                    {
                        "authorId": null,
                        "name": "Yue Wang"
                    },
                    {
                        "authorId": "47767550",
                        "name": "Lijun Wu"
                    },
                    {
                        "authorId": "2154939268",
                        "name": "Wei Chen"
                    },
                    {
                        "authorId": "2249674",
                        "name": "Zhi-Ming Ma"
                    },
                    {
                        "authorId": "2110264337",
                        "name": "Tie-Yan Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "We build upon the Hamiltonian Neural Network (HNN) model by [18] and run experiments on a physics-inspired setting, the ideal spring, which is represented by the Hamiltonian"
            ],
            "citingPaper": {
                "paperId": "86bb2fe3facc5de24f5bb367056e791332603c1d",
                "externalIds": {
                    "ArXiv": "2106.12891",
                    "DBLP": "conf/ijcnn/BhattacharyaMP22",
                    "DOI": "10.1109/IJCNN55064.2022.9892232",
                    "CorpusId": 248405602
                },
                "corpusId": 248405602,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/86bb2fe3facc5de24f5bb367056e791332603c1d",
                "title": "Encoding Involutory Invariances in Neural Networks",
                "abstract": "In certain situations, neural networks are trained upon data that obey underlying symmetries. However, the predictions do not respect the symmetries exactly unless embedded in the network structure. In this work, we introduce architectures that embed a special kind of symmetry namely, invariance with respect to involutory linear/affine transformations up to parity p = \u00b11. We provide rigorous theorems to show that the proposed network ensures such an invariance and present qualitative arguments for a special universal approximation theorem. An adaption of our techniques to CNN tasks for datasets with inherent horizontal/vertical reflection symmetry is demonstrated. Extensive experiments indicate that the proposed model outperforms baseline feed-forward and physics-informed neural networks while identically respecting the underlying symmetry.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2028910746",
                        "name": "Anwesh Bhattacharya"
                    },
                    {
                        "authorId": "145324933",
                        "name": "M. Mattheakis"
                    },
                    {
                        "authorId": "1735677",
                        "name": "P. Protopapas"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Examples include Hamiltonian neural networks [31, 8, 94], which reflect the conservation of energy.",
                "about the underlying physical system, such as conservation of energy [8, 31, 94], independence of mechanism [59], monotonicity [55], or linearity [33]."
            ],
            "citingPaper": {
                "paperId": "7932f79a70866f8a047cce8f03a46ec633943b85",
                "externalIds": {
                    "ArXiv": "2106.02875",
                    "DBLP": "conf/nips/QianZFES21",
                    "CorpusId": 235358128
                },
                "corpusId": 235358128,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7932f79a70866f8a047cce8f03a46ec633943b85",
                "title": "Integrating Expert ODEs into Neural ODEs: Pharmacology and Disease Progression",
                "abstract": "Modeling a system's temporal behaviour in reaction to external stimuli is a fundamental problem in many areas. Pure Machine Learning (ML) approaches often fail in the small sample regime and cannot provide actionable insights beyond predictions. A promising modification has been to incorporate expert domain knowledge into ML models. The application we consider is predicting the progression of disease under medications, where a plethora of domain knowledge is available from pharmacology. Pharmacological models describe the dynamics of carefully-chosen medically meaningful variables in terms of systems of Ordinary Differential Equations (ODEs). However, these models only describe a limited collection of variables, and these variables are often not observable in clinical environments. To close this gap, we propose the latent hybridisation model (LHM) that integrates a system of expert-designed ODEs with machine-learned Neural ODEs to fully describe the dynamics of the system and to link the expert and latent variables to observable quantities. We evaluated LHM on synthetic data as well as real-world intensive care data of COVID-19 patients. LHM consistently outperforms previous works, especially when few training samples are available such as at the beginning of the pandemic.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "8797071",
                        "name": "Z. Qian"
                    },
                    {
                        "authorId": "1808276",
                        "name": "W. Zame"
                    },
                    {
                        "authorId": "26430703",
                        "name": "L. Fleuren"
                    },
                    {
                        "authorId": "2035038",
                        "name": "P. Elbers"
                    },
                    {
                        "authorId": "1729969",
                        "name": "M. Schaar"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Greydanus et al. (2019) highlights these issues and provides a method\nar X\niv :2\n10 6."
            ],
            "citingPaper": {
                "paperId": "478d4968d17d1ab556e574c32fc388f9e40b583a",
                "externalIds": {
                    "ArXiv": "2106.02973",
                    "MAG": "3174919583",
                    "DBLP": "conf/l4dc/Havens021",
                    "CorpusId": 235358317
                },
                "corpusId": 235358317,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/478d4968d17d1ab556e574c32fc388f9e40b583a",
                "title": "Forced Variational Integrator Networks for Prediction and Control of Mechanical Systems",
                "abstract": "As deep learning becomes more prevalent for prediction and control of real physical systems, it is important that these overparameterized models are consistent with physically plausible dynamics. This elicits a problem with how much inductive bias to impose on the model through known physical parameters and principles to reduce complexity of the learning problem to give us more reliable predictions. Recent work employs discrete variational integrators parameterized as a neural network architecture to learn conservative Lagrangian systems. The learned model captures and enforces global energy preserving properties of the system from very few trajectories. However, most real systems are inherently non-conservative and, in practice, we would also like to apply actuation. In this paper we extend this paradigm to account for general forcing (e.g. control input and damping) via discrete d'Alembert's principle which may ultimately be used for control applications. We show that this forced variational integrator networks (FVIN) architecture allows us to accurately account for energy dissipation and external forcing while still capturing the true underlying energy-based passive dynamics. We show that in application this can result in highly-data efficient model-based control and can predict on real non-conservative systems.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51132368",
                        "name": "Aaron J. Havens"
                    },
                    {
                        "authorId": "1733356",
                        "name": "Girish V. Chowdhary"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "bb09f46438236f6e916d2ad23eca92330a41b9fb",
                "externalIds": {
                    "ArXiv": "2106.00026",
                    "DBLP": "journals/corr/abs-2106-00026",
                    "DOI": "10.1103/PhysRevE.104.055302",
                    "CorpusId": 235266182,
                    "PubMed": "34942731"
                },
                "corpusId": 235266182,
                "publicationVenue": {
                    "id": "19842b7b-a4d1-4f9a-9714-87d878cf6e73",
                    "name": "Physical Review E",
                    "type": "journal",
                    "alternate_names": [
                        "Phys rev E",
                        "Physical review. E",
                        "Phys Rev E"
                    ],
                    "issn": "1539-3755",
                    "alternate_issns": [
                        "1550-2376",
                        "2470-0045"
                    ],
                    "url": "https://journals.aps.org/pre/",
                    "alternate_urls": [
                        "http://pre.aps.org/",
                        "http://journals.aps.org/pre/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bb09f46438236f6e916d2ad23eca92330a41b9fb",
                "title": "Machine-Learning Non-Conservative Dynamics for New-Physics Detection",
                "abstract": "Energy conservation is a basic physics principle, the breakdown of which often implies new physics. This paper presents a method for data-driven \"new physics\" discovery. Specifically, given a trajectory governed by unknown forces, our neural new-physics detector (NNPhD) aims to detect new physics by decomposing the force field into conservative and nonconservative components, which are represented by a Lagrangian neural network (LNN) and an unconstrained neural network, respectively, trained to minimize the force recovery error plus a constant \u03bb times the magnitude of the predicted nonconservative force. We show that a phase transition occurs at \u03bb=1, universally for arbitrary forces. We demonstrate that NNPhD successfully discovers new physics in toy numerical experiments, rediscovering friction (1493) from a damped double pendulum, Neptune from Uranus' orbit (1846), and gravitational waves (2017) from an inspiraling orbit. We also show how NNPhD coupled with an integrator outperforms both an LNN and an unconstrained neural network for predicting the future of a damped double pendulum.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145253202",
                        "name": "Ziming Liu"
                    },
                    {
                        "authorId": "1390816671",
                        "name": "Bohan Wang"
                    },
                    {
                        "authorId": "47580728",
                        "name": "Qi Meng"
                    },
                    {
                        "authorId": "2154939268",
                        "name": "Wei Chen"
                    },
                    {
                        "authorId": "49944508",
                        "name": "M. Tegmark"
                    },
                    {
                        "authorId": "2110264337",
                        "name": "Tie-Yan Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "The work of [16] proposes a similar approach to the previous one but using the Hamiltonian instead."
            ],
            "citingPaper": {
                "paperId": "30195d65e140ac19b4f54d72973bb3d5b94a8d67",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-14396",
                    "ArXiv": "2105.14396",
                    "CorpusId": 235253757
                },
                "corpusId": 235253757,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/30195d65e140ac19b4f54d72973bb3d5b94a8d67",
                "title": "SyReNets: Symbolic Residual Neural Networks",
                "abstract": "Despite successful seminal works on passive systems in the literature, learning free-form physical laws for controlled dynamical systems given experimental data is still an open problem. For decades, symbolic mathematical equations and system identification were the golden standards. Unfortunately, a set of assumptions about the properties of the underlying system is required, which makes the model very rigid and unable to adapt to unforeseen changes in the physical system. Neural networks, on the other hand, are known universal function approximators but are prone to over-fit, limited accuracy, and bias problems, which makes them alone unreliable candidates for such tasks. In this paper, we propose SyReNets, an approach that leverages neural networks for learning symbolic relations to accurately describe dynamic physical systems from data. It explores a sequence of symbolic layers that build, in a residual manner, mathematical relations that describes a given desired output from input variables. We apply it to learn the symbolic equation that describes the Lagrangian of a given physical system. We do this by only observing random samples of position, velocity, and acceleration as input and torque as output. Therefore, using the Lagrangian as a latent representation from which we derive torque using the Euler-Lagrange equations. The approach is evaluated using a simulated controlled double pendulum and compared with neural networks, genetic programming, and traditional system identification. The results demonstrate that, compared to neural networks and genetic programming, SyReNets converges to representations that are more accurate and precise throughout the state space. Despite having slower convergence than traditional system identification, similar to neural networks, the approach remains flexible enough to adapt to an unforeseen change in the physical system structure.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2057365220",
                        "name": "Carlos Magno Catharino Olsson Valle"
                    },
                    {
                        "authorId": "1809470",
                        "name": "S. Haddadin"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "While theoretically well-motivated learning methods for Hamiltonian [25]\u2013[28] and Lagrangian mechanics [29], [30] have recently been proposed, the practical relevance of these methods remains limited due to restrictive assumptions [29]."
            ],
            "citingPaper": {
                "paperId": "36f494c27103b5870e43495f33a76861a09f7ad1",
                "externalIds": {
                    "DBLP": "conf/icra/YangSS21",
                    "DOI": "10.1109/ICRA48506.2021.9561636",
                    "CorpusId": 239039430
                },
                "corpusId": 239039430,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/36f494c27103b5870e43495f33a76861a09f7ad1",
                "title": "Learning to Propagate Interaction Effects for Modeling Deformable Linear Objects Dynamics",
                "abstract": "Modeling dynamics of deformable linear objects (DLOs), such as cables, hoses, sutures, and catheters, is an important and challenging problem for many robotic manipulation applications. In this paper, we propose the first method to model and learn full 3D dynamics of DLOs from data. Our approach is capable of capturing the complex twisting and bending dynamics of DLOs and allows local effects to propagate globally. To this end, we adapt the interaction network (IN) dynamics learning method for capturing the interaction between neighboring segments in a DLO and augment it with a recurrent model for propagating interaction effects along the length of a DLO. For learning twisting and bending dynamics in 3D, we also introduce a new suitable representation of DLO segments and their relationships. Unlike the original IN method, our model learns to propagate the effects of local interaction between neighboring segments to each segment in the chain within a single time step, without the need for iterated propagation steps. Evaluation of our model with synthetic and newly collected real-world data shows better accuracy and generalization in short-term and long- term predictions than the current state of the art. We further integrate our learned model in a model predictive control scheme and use it to successfully control the shape of a DLO. Our implementation is available at https://gitsvn-nt.oru.se/ammlab\u2013public/in\u2013bilstm.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2144145722",
                        "name": "Yuxuan Yang"
                    },
                    {
                        "authorId": "3128110",
                        "name": "J. A. Stork"
                    },
                    {
                        "authorId": "144504646",
                        "name": "Todor Stoyanov"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In the application of neural networks to model physical systems, several authors have also constructed equivariant (or invariant) models by incorporating equations of motion - in either the Hamiltonian or Lagrangian formulation of classical mechanics - to accommodate the learning of system dynamics and conservation laws [71, 72, 73]."
            ],
            "citingPaper": {
                "paperId": "f62f8e9501302a57a5656b01b9d45e4c7463d48f",
                "externalIds": {
                    "ArXiv": "2105.13926",
                    "DBLP": "journals/corr/abs-2105-13926",
                    "DOI": "10.1007/s10462-023-10502-7",
                    "CorpusId": 235248075
                },
                "corpusId": 235248075,
                "publicationVenue": {
                    "id": "ea8553fe-2467-4367-afee-c4deb3754820",
                    "name": "Artificial Intelligence Review",
                    "type": "journal",
                    "alternate_names": [
                        "Artif Intell Rev"
                    ],
                    "issn": "0269-2821",
                    "url": "https://link.springer.com/journal/10462"
                },
                "url": "https://www.semanticscholar.org/paper/f62f8e9501302a57a5656b01b9d45e4c7463d48f",
                "title": "Geometric deep learning and equivariant neural networks",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3315379",
                        "name": "Jan E. Gerken"
                    },
                    {
                        "authorId": "104264587",
                        "name": "J. Aronsson"
                    },
                    {
                        "authorId": "2106386126",
                        "name": "Oscar Carlsson"
                    },
                    {
                        "authorId": "3311829",
                        "name": "H. Linander"
                    },
                    {
                        "authorId": "3288234",
                        "name": "F. Ohlsson"
                    },
                    {
                        "authorId": "6387775",
                        "name": "Christoffer Petersson"
                    },
                    {
                        "authorId": "2360323",
                        "name": "D. Persson"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "We also highlight that H-DNNs are fundamentally different from the neural networks proposed in [30], which have the same name but are designed to learn the Hamiltonian functions of mechanical systems."
            ],
            "citingPaper": {
                "paperId": "429f22e3ea9c091d68a4c969790e33412f03ede1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-13205",
                    "ArXiv": "2105.13205",
                    "DOI": "10.1109/TAC.2023.3239430",
                    "CorpusId": 235212129
                },
                "corpusId": 235212129,
                "publicationVenue": {
                    "id": "1283a59c-0d1f-48c3-81d7-02172f597e70",
                    "name": "IEEE Transactions on Automatic Control",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Autom Control"
                    ],
                    "issn": "0018-9286",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=9"
                },
                "url": "https://www.semanticscholar.org/paper/429f22e3ea9c091d68a4c969790e33412f03ede1",
                "title": "Hamiltonian Deep Neural Networks Guaranteeing Nonvanishing Gradients by Design",
                "abstract": "Deep neural networks (DNNs) training can be difficult due to vanishing and exploding gradients during weight optimization through backpropagation. To address this problem, we propose a general class of Hamiltonian DNNs (H-DNNs) that stem from the discretization of continuous-time Hamiltonian systems and include several existing DNN architectures based on ordinary differential equations. Our main result is that a broad set of H-DNNs ensures nonvanishing gradients by design for an arbitrary network depth. This is obtained by proving that, using a semi-implicit Euler discretization scheme, the backward sensitivity matrices involved in gradient computations are symplectic. We also provide an upper bound to the magnitude of sensitivity matrices and show that exploding gradients can be controlled through regularization. The good performance of H-DNNs is demonstrated on benchmark classification problems, including image classification with the MNIST dataset.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "73766979",
                        "name": "C. Galimberti"
                    },
                    {
                        "authorId": "33674995",
                        "name": "Luca Furieri"
                    },
                    {
                        "authorId": "151484848",
                        "name": "Liang Xu"
                    },
                    {
                        "authorId": "1404093319",
                        "name": "G. Ferrari-Trecate"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "contexts": [
                "Here we will focus our discussion around the classic HNN architecture from [11] illustrated in Figure 4(b), that receives only a system\u2019s canonical coordinates as input.",
                "As in [11] the HNN model learned to respect the conservation of energy constraint and returned physically consistent predictions.",
                "As in [11] we implemented two networks in PyTorch [22] a Baseline MLP Figure 4(a) and a HNN Figure 4(b).",
                "Some of these efforts have resulted in neural alternatives for modeling and simulation of simple dynamical systems [10, 11] and for solving ordinary and partial differential equations [12, 6, 3, 13].",
                "Hamiltonian neural networks (HNNs) are a neural architecture that learns conservation laws in an unsupervised manner [11].",
                "Hamiltonian mechanics was first utilized in neural network design nearly three decades ago [18] and in recent years this area has seen renewed interest [11, 19, 20, 21, 14].",
                "[11] generally reported results over a short time interval, t-span = [0,20] and reported results from HNNs using Tanh activation functions in their architecture.",
                "As a motivating example in embedding conservation of energy constraint through a HNN we will replicate the results [11] for a noisy ideal mass-spring system and discuss architectural choices.",
                "We replicated the results of [11] for a noisy ideal mass-spring system."
            ],
            "citingPaper": {
                "paperId": "55d91570fd0caaa7ad0c23d9d4f30d6b8d16d8b0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-09146",
                    "ArXiv": "2105.09146",
                    "CorpusId": 234777762
                },
                "corpusId": 234777762,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/55d91570fd0caaa7ad0c23d9d4f30d6b8d16d8b0",
                "title": "Physical Constraint Embedded Neural Networks for inference and noise regulation",
                "abstract": "Neural networks often require large amounts of data to generalize and can be ill-suited for modeling small and noisy experimental datasets. Standard network architectures trained on scarce and noisy data will return predictions that violate the underlying physics. In this paper, we present methods for embedding even--odd symmetries and conservation laws in neural networks and propose novel extensions and use cases for physical constraint embedded neural networks. We design an even--odd decomposition architecture for disentangling a neural network parameterized function into its even and odd components and demonstrate that it can accurately infer symmetries without prior knowledge. We highlight the noise resilient properties of physical constraint embedded neural networks and demonstrate their utility as physics-informed noise regulators. Here we employed a conservation of energy constraint embedded network as a physics-informed noise regulator for a symbolic regression task. We showed that our approach returns a symbolic representation of the neural network parameterized function that aligns well with the underlying physics while outperforming a baseline symbolic regression approach.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "153374577",
                        "name": "Gregory Barber"
                    },
                    {
                        "authorId": "39004674",
                        "name": "M. Haile"
                    },
                    {
                        "authorId": "2144891774",
                        "name": "Tzikang Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "physical equations [17, 18, 31], sequence modeling processes [6], and games [41]."
            ],
            "citingPaper": {
                "paperId": "315d61761de1440362fe85ed24e567245b79989a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-08881",
                    "ArXiv": "2105.08881",
                    "DOI": "10.1145/3447555.3464874",
                    "CorpusId": 234778128
                },
                "corpusId": 234778128,
                "publicationVenue": {
                    "id": "0b391b14-8353-4d09-be78-077ce8a257f7",
                    "name": "Energy-Efficient Computing and Networking",
                    "type": "conference",
                    "alternate_names": [
                        "Energy-efficient Comput Netw",
                        "E-Energy",
                        "e-Energy"
                    ],
                    "url": "http://www.energyware.org/"
                },
                "url": "https://www.semanticscholar.org/paper/315d61761de1440362fe85ed24e567245b79989a",
                "title": "Enforcing Policy Feasibility Constraints through Differentiable Projection for Energy Optimization",
                "abstract": "While reinforcement learning (RL) is gaining popularity in energy systems control, its real-world applications are limited due to the fact that the actions from learned policies may not satisfy functional requirements or be feasible for the underlying physical system. In this work, we propose PROjected Feasibility (PROF), a method to enforce convex operational constraints within neural policies. Specifically, we incorporate a differentiable projection layer within a neural network-based policy to enforce that all learned actions are feasible. We then update the policy end-to-end by propagating gradients through this differentiable projection layer, making the policy cognizant of the operational constraints. We demonstrate our method on two applications: energy-efficient building operation and inverter control. In the building operation setting, we show that PROF maintains thermal comfort requirements while improving energy efficiency by 4% over state-of-the-art methods. In the inverter control setting, PROF perfectly satisfies voltage constraints on the IEEE 37-bus feeder system, as it learns to curtail as little renewable energy as possible within its safety set.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145116464",
                        "name": "J. Z. Kolter"
                    },
                    {
                        "authorId": "2229064790",
                        "name": "Neural Network"
                    },
                    {
                        "authorId": null,
                        "name": "\ud835\udf3d \ud835\udf45"
                    },
                    {
                        "authorId": null,
                        "name": "\ud835\udc8c \ud835\udc99"
                    },
                    {
                        "authorId": null,
                        "name": "\ud835\udc8c \ud835\udc96"
                    },
                    {
                        "authorId": null,
                        "name": "\ud835\udc8c \ud835\udc98"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Gradient-constrained Optimization: Recent work has studied directly optimizing gradients to enforce theoretical properties while training neural nets (Greydanus, Dzamba, and Yosinski 2019)."
            ],
            "citingPaper": {
                "paperId": "384b56b7b609a3966a6c02f8259b94e6f0f3e5de",
                "externalIds": {
                    "DBLP": "conf/aaai/JenkinsFJYWL21",
                    "DOI": "10.1609/aaai.v35i9.16966",
                    "CorpusId": 235306521
                },
                "corpusId": 235306521,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/384b56b7b609a3966a6c02f8259b94e6f0f3e5de",
                "title": "Neural Utility Functions",
                "abstract": "Current neural network architectures have no mechanism for explicitly reasoning about item trade-offs. Such trade-offs are important for popular tasks such as recommendation. The main idea of this work is to give neural networks inductive biases that are inspired by economic theories. To this end, we propose Neural Utility Functions, which directly optimize the gradients of a neural network so that they are more consistent with utility theory, a mathematical framework for modeling choice among items. We demonstrate that Neural Utility Functions can recover theoretical item relationships better than vanilla neural networks, analytically show existing neural networks are not quasi-concave and do not inherently reason about trade-offs, and that augmenting existing models with a utility loss function improves recommendation results. The Neural Utility Functions we propose are theoretically motivated, and yield strong empirical results.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "83816065",
                        "name": "P. Jenkins"
                    },
                    {
                        "authorId": "143975330",
                        "name": "A. Farag"
                    },
                    {
                        "authorId": "1420081386",
                        "name": "J. S. Jenkins"
                    },
                    {
                        "authorId": "18307037",
                        "name": "Huaxiu Yao"
                    },
                    {
                        "authorId": "2116430057",
                        "name": "Suhang Wang"
                    },
                    {
                        "authorId": "2109640666",
                        "name": "Z. Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Similarly, authors in (Greydanus, Dzamba, and Yosinski 2019) draw inspiration from Hamiltonian mechanics to train models that learn exact conservation laws in an unsupervised manner by endowing the models with better inductive biases."
            ],
            "citingPaper": {
                "paperId": "7e11d26bb62115c2ef6513f784c62c2585d31f63",
                "externalIds": {
                    "DBLP": "conf/aaai/OsmaniHB21",
                    "DOI": "10.1609/aaai.v35i10.17116",
                    "CorpusId": 235349044
                },
                "corpusId": 235349044,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7e11d26bb62115c2ef6513f784c62c2585d31f63",
                "title": "Augmented Experiment in Material Engineering Using Machine Learning",
                "abstract": "The synthesis of materials using the principle of thermogravimetric analysis to discover new anticorrosive paints requires several costly experiments. This paper presents an approach combining empirical data and domain analytical models to reduce the number of real experiments required to obtain the desired synthesis. The main idea is to predict the behavior of the synthesis of two materials with well-defined mass proportions as a function of temperature. As no exact equational model exists to predict the new material, we integrate a machine learning approach circumscribed by existing domain analytical models such as heating and kinetics equations in order to derive a generative model of augmented experiments. Extensive empirical evaluation shows that using machine learning approach guided by analytic models, it is possible to substantially reduce the number of needed physical experiments without losing the approximation quality.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "33995596",
                        "name": "A. Osmani"
                    },
                    {
                        "authorId": "46253421",
                        "name": "Massinissa Hamidi"
                    },
                    {
                        "authorId": "3172145",
                        "name": "S. Bouhouche"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Another related field of HamNet is neural physics engines (Sanchez-Gonzalez et al., 2018; 2019; Greydanus et al., 2019), which learn to conduct simulations that conform to physical laws."
            ],
            "citingPaper": {
                "paperId": "69bab843432803a8f69ffa7c33779f205e843841",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-03688",
                    "ArXiv": "2105.03688",
                    "CorpusId": 234334847
                },
                "corpusId": 234334847,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/69bab843432803a8f69ffa7c33779f205e843841",
                "title": "HamNet: Conformation-Guided Molecular Representation with Hamiltonian Neural Networks",
                "abstract": "Well-designed molecular representations (fingerprints) are vital to combine medical chemistry and deep learning. Whereas incorporating 3D geometry of molecules (i.e. conformations) in their representations seems beneficial, current 3D algorithms are still in infancy. In this paper, we propose a novel molecular representation algorithm which preserves 3D conformations of molecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit positions and momentums of atoms in a molecule interact in the Hamiltonian Engine following the discretized Hamiltonian equations. These implicit coordinations are supervised with real conformations with translation-&rotation-invariant losses, and further used as inputs to the Fingerprint Generator, a message-passing neural network. Experiments show that the Hamiltonian Engine can well preserve molecular conformations, and that the fingerprints generated by HamNet achieve state-of-the-art performances on MoleculeNet, a standard molecular machine learning benchmark.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48458595",
                        "name": "Ziyao Li"
                    },
                    {
                        "authorId": "2131871168",
                        "name": "Shuwen Yang"
                    },
                    {
                        "authorId": "2090871",
                        "name": "Guojie Song"
                    },
                    {
                        "authorId": "2090465220",
                        "name": "Lingsheng Cai"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "2017; Lu et al., 2017) to physics (Greydanus et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "8eab4a92ea5f0d3545acf8ec8990f35425aaad3a",
                "externalIds": {
                    "ArXiv": "2105.03788",
                    "DBLP": "journals/corr/abs-2105-03788",
                    "CorpusId": 234339540
                },
                "corpusId": 234339540,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8eab4a92ea5f0d3545acf8ec8990f35425aaad3a",
                "title": "Dynamic Game Theoretic Neural Optimizer",
                "abstract": "The connection between training deep neural networks (DNNs) and optimal control theory (OCT) has attracted considerable attention as a principled tool of algorithmic design. Despite few attempts being made, they have been limited to architectures where the layer propagation resembles a Markovian dynamical system. This casts doubts on their flexibility to modern networks that heavily rely on non-Markovian dependencies between layers (e.g. skip connections in residual networks). In this work, we propose a novel dynamic game perspective by viewing each layer as a player in a dynamic game characterized by the DNN itself. Through this lens, different classes of optimizers can be seen as matching different types of Nash equilibria, depending on the implicit information structure of each (p)layer. The resulting method, called Dynamic Game Theoretic Neural Optimizer (DGNOpt), not only generalizes OCT-inspired optimizers to richer network class; it also motivates a new training principle by solving a multi-player cooperative game. DGNOpt shows convergence improvements over existing methods on image classification datasets with residual and inception networks. Our work marries strengths from both OCT and game theory, paving ways to new algorithmic opportunities from robust optimal control and bandit-based optimization.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46218587",
                        "name": "Guan-Horng Liu"
                    },
                    {
                        "authorId": "11126631",
                        "name": "T. Chen"
                    },
                    {
                        "authorId": "1751063",
                        "name": "Evangelos A. Theodorou"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2019), the learned representation (Lusch et al., 2018; Greydanus et al., 2019; Bau et al., 2020), hard output constraints (Mohan et al.",
                "\u2026to incorporate physics as: inputs (Reichstein et al., 2019), training loss (Raissi et al., 2019), the learned representation (Lusch et al., 2018; Greydanus et al., 2019; Bau et al., 2020), hard output constraints (Mohan et al., 2020), or evaluation function (Lu\u0308tjens* et al., 2021; Lesort et\u2026"
            ],
            "citingPaper": {
                "paperId": "d6c3097f9dcb53f600129758d9c13d1c6a0e7887",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-02939",
                    "ArXiv": "2105.02939",
                    "CorpusId": 234093152
                },
                "corpusId": 234093152,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6c3097f9dcb53f600129758d9c13d1c6a0e7887",
                "title": "PCE-PINNs: Physics-Informed Neural Networks for Uncertainty Propagation in Ocean Modeling",
                "abstract": "Climate models project an uncertainty range of possible warming scenarios from 1.5 to 5 degree Celsius global temperature increase until 2100, according to the CMIP6 model ensemble. Climate risk management and infrastructure adaptation requires the accurate quantification of the uncertainties at the local level. Ensembles of high-resolution climate models could accurately quantify the uncertainties, but most physics-based climate models are computationally too expensive to run as ensemble. Recent works in physics-informed neural networks (PINNs) have combined deep learning and the physical sciences to learn up to 15k faster copies of climate submodels. However, the application of PINNs in climate modeling has so far been mostly limited to deterministic models. We leverage a novel method that combines polynomial chaos expansion (PCE), a classic technique for uncertainty propagation, with PINNs. The PCE-PINNs learn a fast surrogate model that is demonstrated for uncertainty propagation of known parameter uncertainties. We showcase the effectiveness in ocean modeling by using the local advection-diffusion equation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "81551971",
                        "name": "Bj\u00f6rn L\u00fctjens"
                    },
                    {
                        "authorId": "40134804",
                        "name": "Catherine H. Crawford"
                    },
                    {
                        "authorId": "103156956",
                        "name": "M. Veillette"
                    },
                    {
                        "authorId": "2054846636",
                        "name": "Dava Newman"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "36439c154582cff65e8ca24da56ec232745e38a9",
                "externalIds": {
                    "DOI": "10.1038/s41524-021-00609-2",
                    "CorpusId": 237377973
                },
                "corpusId": 237377973,
                "publicationVenue": {
                    "id": "a7cc9d16-b88e-439c-a9bc-f7d032668c52",
                    "name": "npj Computational Materials",
                    "alternate_names": [
                        "npj Comput Mater"
                    ],
                    "issn": "2057-3960",
                    "url": "http://www.nature.com/npjcompumats/",
                    "alternate_urls": [
                        "http://0-search.proquest.com.pugwash.lib.warwick.ac.uk/publication/2041924",
                        "http://www.nature.com/npjcompumats/articles"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/36439c154582cff65e8ca24da56ec232745e38a9",
                "title": "Deep learning framework for material design space exploration using active transfer learning and data augmentation",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2206795053",
                        "name": "Yongtae Kim"
                    },
                    {
                        "authorId": "2108379431",
                        "name": "Youngsoo Kim"
                    },
                    {
                        "authorId": "2146344407",
                        "name": "Charles Yang"
                    },
                    {
                        "authorId": "144219183",
                        "name": "Kundo Park"
                    },
                    {
                        "authorId": "11329466",
                        "name": "Grace X. Gu"
                    },
                    {
                        "authorId": "3454274",
                        "name": "Seunghwa Ryu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "69af7c539e6957a3fb1c3402783cd2c50e563025",
                "externalIds": {
                    "ArXiv": "2105.00400",
                    "CorpusId": 233481625
                },
                "corpusId": 233481625,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/69af7c539e6957a3fb1c3402783cd2c50e563025",
                "title": "Model discovery in the sparse sampling regime",
                "abstract": "To improve the physical understanding and the predictions of complex dynamic systems, such as ocean dynamics and weather predictions, it is of paramount interest to identify interpretable models from coarsely and off-grid sampled observations. In this work, we investigate how deep learning can improve model discovery of partial differential equations when the spacing between sensors is large and the samples are not placed on a grid. We show how leveraging physics informed neural network interpolation and automatic differentiation, allow to better fit the data and its spatiotemporal derivatives, compared to more classic spline interpolation and numerical differentiation techniques. As a result, deep learning-based model discovery allows to recover the underlying equations, even when sensors are placed further apart than the data's characteristic length scale and in the presence of high noise levels. We illustrate our claims on both synthetic and experimental data sets where combinations of physical processes such as (non)-linear advection, reaction, and diffusion are correctly identified.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35672556",
                        "name": "G. Both"
                    },
                    {
                        "authorId": "98679981",
                        "name": "Georges Tod"
                    },
                    {
                        "authorId": "7464692",
                        "name": "R. Kusters"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "An example for such a bias is to learn a motion which is conserving energy as performed in the context of Hamiltonian Neural Networks (HNNs) (Greydanus et al., 2019).",
                "This is the same loss as introduced in Greydanus et al. (2019).",
                "As datasets we use the nearly circular orbits constructed by Greydanus et al. (2019), but give the whole system a boost in a random direction sampled from N (0, 0.1)2.",
                "For integrating the solutions in time from our respective symmetry control network we use a fourth order Runge-Kutta integrator as in Greydanus et al. (2019) which unlike symplectic integrators allows for a comparison with neural network approaches directly predicting the dynamics of a\u2026"
            ],
            "citingPaper": {
                "paperId": "190a96980e99b398fce18bb7fa78733134d53547",
                "externalIds": {
                    "MAG": "3159022575",
                    "ArXiv": "2104.14444",
                    "DBLP": "journals/corr/abs-2104-14444",
                    "CorpusId": 233444119
                },
                "corpusId": 233444119,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/190a96980e99b398fce18bb7fa78733134d53547",
                "title": "Improving Simulations with Symmetry Control Neural Networks",
                "abstract": "The dynamics of physical systems is often constrained to lower dimensional sub-spaces due to the presence of conserved quantities. Here we propose a method to learn and exploit such symmetry constraints building upon Hamiltonian Neural Networks. By enforcing cyclic coordinates with appropriate loss functions, we find that we can achieve improved accuracy on simple classical dynamics tasks. By fitting analytic formulae to the latent variables in our network we recover that our networks are utilizing conserved quantities such as (angular) momentum.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "147611823",
                        "name": "Marc Syvaeri"
                    },
                    {
                        "authorId": "51264706",
                        "name": "S. Krippendorf"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "engineering, such as intuitive physics [24], [25], computational fluid dynamics [26], [27], Hamiltonian mechanics [28], and simulations of Symplectic integration and Lagrangians [29],"
            ],
            "citingPaper": {
                "paperId": "acaaed5b0fbc6fe2985e90ffc3f89a5a110ce5f3",
                "externalIds": {
                    "ArXiv": "2104.13231",
                    "DOI": "10.1109/TAP.2023.3245281",
                    "CorpusId": 237519463
                },
                "corpusId": 237519463,
                "publicationVenue": {
                    "id": "64f6bc7f-a080-473b-8e71-044beeeadb13",
                    "name": "IEEE Transactions on Antennas and Propagation",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Antenna Propag"
                    ],
                    "issn": "0018-926X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=8",
                    "alternate_urls": [
                        "https://ieeeaps.org/aps_trans/index.htm",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=8234",
                        "http://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=8",
                        "https://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=8"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/acaaed5b0fbc6fe2985e90ffc3f89a5a110ce5f3",
                "title": "Physics-Informed Supervised Residual Learning for Electromagnetic Modeling",
                "abstract": "In this study, physics-informed supervised residual learning (PhiSRL) is proposed to enable an effective, robust, and general deep learning framework for 2-D electromagnetic (EM) modeling. Based on the mathematical connection between the fixed-point iteration method and the residual neural network (ResNet), PhiSRL aims to solve a system of linear matrix equations. It applies convolutional neural networks (CNNs) to learn updates of the solution with respect to the residuals. Inspired by the stationary and nonstationary iterative scheme of the fixed-point iteration method, stationary and nonstationary iterative physics-informed ResNets (SiPhiResNet and NiPhiResNet) are designed to solve the volume integral equation (VIE) of EM scattering. The effectiveness and universality of PhiSRL are validated by solving VIE of lossless and lossy scatterers with the mean squared errors (MSEs) converging to <inline-formula> <tex-math notation=\"LaTeX\">$\\sim 10^{-4}$ </tex-math></inline-formula> (SiPhiResNet) and <inline-formula> <tex-math notation=\"LaTeX\">$\\sim 10^{-7}$ </tex-math></inline-formula> (NiPhiResNet). Numerical results further verify the generalization ability of PhiSRL.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2067161406",
                        "name": "Tao Shan"
                    },
                    {
                        "authorId": "2210503224",
                        "name": "Jinhong Zeng"
                    },
                    {
                        "authorId": "50707034",
                        "name": "Xiaoqian Song"
                    },
                    {
                        "authorId": "2090408646",
                        "name": "Rui Guo"
                    },
                    {
                        "authorId": "47628788",
                        "name": "Maokun Li"
                    },
                    {
                        "authorId": "2019029201",
                        "name": "Fan Yang"
                    },
                    {
                        "authorId": "143879525",
                        "name": "Shenheng Xu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recent work has also focused on embedding specialized kinds of constraints into neural networks, such as conservation of energy (see, e.g., Greydanus et al. (2019) and Beucler et al. (2019)), and homogeneous linear inequality constraints (Frerix et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "ac879e53927cdf7c3b8b5c00bc3ff13959f11d97",
                "externalIds": {
                    "ArXiv": "2104.12225",
                    "DBLP": "journals/corr/abs-2104-12225",
                    "CorpusId": 233394003
                },
                "corpusId": 233394003,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ac879e53927cdf7c3b8b5c00bc3ff13959f11d97",
                "title": "DC3: A learning method for optimization with hard constraints",
                "abstract": "Large optimization problems with hard constraints arise in many settings, yet classical solvers are often prohibitively slow, motivating the use of deep networks as cheap\"approximate solvers.\"Unfortunately, naive deep learning approaches typically cannot enforce the hard constraints of such problems, leading to infeasible solutions. In this work, we present Deep Constraint Completion and Correction (DC3), an algorithm to address this challenge. Specifically, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. We demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power flow, where hard constraints encode the physics of the electrical grid. In both cases, DC3 achieves near-optimal objective values while preserving feasibility.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49698491",
                        "name": "P. Donti"
                    },
                    {
                        "authorId": "2381187",
                        "name": "D. Rolnick"
                    },
                    {
                        "authorId": "145116464",
                        "name": "J. Z. Kolter"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "This concern has led to the development of HNN and Lagrangian neural networks (LNN) [24, 30], with which the system energy is well conserved in the long-term evolution.",
                "In machine learning of Hamiltonian systems, a major concern is whether the system energy will be conserved in the long-term evolution [24, 30]."
            ],
            "citingPaper": {
                "paperId": "c57898ac3f987a56b1a162a668896fc917538b0a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-14474",
                    "ArXiv": "2104.14474",
                    "DOI": "10.1103/PhysRevE.104.024205",
                    "CorpusId": 233444006,
                    "PubMed": "34525517"
                },
                "corpusId": 233444006,
                "publicationVenue": {
                    "id": "19842b7b-a4d1-4f9a-9714-87d878cf6e73",
                    "name": "Physical Review E",
                    "type": "journal",
                    "alternate_names": [
                        "Phys rev E",
                        "Physical review. E",
                        "Phys Rev E"
                    ],
                    "issn": "1539-3755",
                    "alternate_issns": [
                        "1550-2376",
                        "2470-0045"
                    ],
                    "url": "https://journals.aps.org/pre/",
                    "alternate_urls": [
                        "http://pre.aps.org/",
                        "http://journals.aps.org/pre/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c57898ac3f987a56b1a162a668896fc917538b0a",
                "title": "Learning Hamiltonian dynamics by reservoir computer",
                "abstract": "Reconstructing the Kolmogorov-Arnold-Moser (KAM) dynamics diagram of Hamiltonian system from the time series of a limited number of parameters is an outstanding question in nonlinear science, especially when the Hamiltonian governing the system dynamics is unknown. Here we demonstrate that this question can be addressed by the machine learning approach knowing as reservoir computing (RC). Specifically, we show that without prior knowledge about the Hamilton equations of motion, the trained RC is able to not only predict the short-term evolution of the system state, but also replicate the long-term ergodic properties of the system dynamics. Furthermore, using the architecture of parameter-aware RC, we show that the RC trained by the time series acquired at a handful parameters is able to reconstruct the entire KAM dynamics diagram with a high precision by tuning a control parameter externally. The feasibility and efficiency of the learning techniques are demonstrated in two classical nonlinear Hamiltonian systems, namely, the double-pendulum oscillator and the standard map. Our study indicates that, as a complex dynamical system, RC is able to learn from data the Hamiltonian.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2146205052",
                        "name": "Han Zhang"
                    },
                    {
                        "authorId": "31546055",
                        "name": "Huawei Fan"
                    },
                    {
                        "authorId": "2144693659",
                        "name": "Liang Wang"
                    },
                    {
                        "authorId": "2026590208",
                        "name": "Xingang Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "3b48d1d41623e6ae24331194896034b6c7880d8f",
                "externalIds": {
                    "MAG": "3158389680",
                    "DOI": "10.3390/APP11093776",
                    "CorpusId": 235505871
                },
                "corpusId": 235505871,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3b48d1d41623e6ae24331194896034b6c7880d8f",
                "title": "Fault Diagnosis via Neural Ordinary Differential Equations",
                "abstract": "Implementation of model-based fault diagnosis systems can be a difficult task due to the complex dynamics of most systems, an appealing alternative to avoiding modeling is to use machine learning-based techniques for which the implementation is more affordable nowadays. However, the latter approach often requires extensive data processing. In this paper, a hybrid approach using recent developments in neural ordinary differential equations is proposed. This approach enables us to combine a natural deep learning technique with an estimated model of the system, making the training simpler and more efficient. For evaluation of this methodology, a nonlinear benchmark system is used by simulation of faults in actuators, sensors, and process. Simulation results show that the proposed methodology requires less processing for the training in comparison with conventional machine learning approaches since the data-set is directly taken from the measurements and inputs. Furthermore, since the model used in the essay is only a structural approximation of the plant; no advanced modeling is required. This approach can also alleviate some pitfalls of training data-series, such as complicated data augmentation methodologies and the necessity for big amounts of data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2114281575",
                        "name": "Luis Enciso-Salas"
                    },
                    {
                        "authorId": "1413600162",
                        "name": "Gustavo P\u00e9rez-Z\u00fa\u00f1iga"
                    },
                    {
                        "authorId": "1405260810",
                        "name": "J. Sotomayor-Moriano"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "As shown in Greydanus et al. (2019) with Hamiltonian Neural Networks (HNNs), one can exploit this Hamiltonian structure by parametrizing H\u0302\u03b8(z) with a neural network, and then taking derivatives to find the implied Hamiltonian dynamics."
            ],
            "citingPaper": {
                "paperId": "c12c87607371d095cb3e149fe20fef5acd9d116f",
                "externalIds": {
                    "DBLP": "conf/icml/FinziWW21",
                    "ArXiv": "2104.09459",
                    "CorpusId": 233296901
                },
                "corpusId": 233296901,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c12c87607371d095cb3e149fe20fef5acd9d116f",
                "title": "A Practical Method for Constructing Equivariant Multilayer Perceptrons for Arbitrary Matrix Groups",
                "abstract": "Symmetries and equivariance are fundamental to the generalization of neural networks on domains such as images, graphs, and point clouds. Existing work has primarily focused on a small number of groups, such as the translation, rotation, and permutation groups. In this work we provide a completely general algorithm for solving for the equivariant layers of matrix groups. In addition to recovering solutions from other works as special cases, we construct multilayer perceptrons equivariant to multiple groups that have never been tackled before, including $\\mathrm{O}(1,3)$, $\\mathrm{O}(5)$, $\\mathrm{Sp}(n)$, and the Rubik's cube group. Our approach outperforms non-equivariant baselines, with applications to particle physics and dynamical systems. We release our software library to enable researchers to construct equivariant layers for arbitrary matrix groups.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51007156",
                        "name": "Marc Finzi"
                    },
                    {
                        "authorId": "1678311",
                        "name": "M. Welling"
                    },
                    {
                        "authorId": "145771261",
                        "name": "A. Wilson"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Making use of such priors allows models in this class to exhibit desirable properties by construction, such as being strictly Hamiltonian [16, 17, 21] or globally stable [15]."
            ],
            "citingPaper": {
                "paperId": "f362c3c09405ba6eb0e4a79c72279d33c369cbaa",
                "externalIds": {
                    "ArXiv": "2104.05096",
                    "DBLP": "conf/nips/CourseEN20",
                    "MAG": "3105935177",
                    "CorpusId": 227275459
                },
                "corpusId": 227275459,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f362c3c09405ba6eb0e4a79c72279d33c369cbaa",
                "title": "Weak Form Generalized Hamiltonian Learning",
                "abstract": "We present a method for learning generalized Hamiltonian decompositions of ordinary differential equations given a set of noisy time series measurements. Our method simultaneously learns a continuous time model and a scalar energy function for a general dynamical system. Learning predictive models in this form allows one to place strong, high-level, physics inspired priors onto the form of the learnt governing equations for general dynamical systems. Moreover, having shown how our method extends and unifies some previous work in deep learning with physics inspired priors, we present a novel method for learning continuous time models from the weak form of the governing equations which is less computationally taxing than standard adjoint methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2031451281",
                        "name": "Kevin Course"
                    },
                    {
                        "authorId": "41075246",
                        "name": "T. W. Evans"
                    },
                    {
                        "authorId": "144475330",
                        "name": "P. Nair"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "To achieve physical-consistency, one could adapt the neural network architecture to incorporate physics as: inputs [31], training loss [32], the learned representation [33], [34], [22], hard output constraints [35],"
            ],
            "citingPaper": {
                "paperId": "63d701270336c3a24c5267ac16f95f57e1c8ef38",
                "externalIds": {
                    "ArXiv": "2104.04785",
                    "DBLP": "journals/corr/abs-2104-04785",
                    "CorpusId": 233210548
                },
                "corpusId": 233210548,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/63d701270336c3a24c5267ac16f95f57e1c8ef38",
                "title": "Physically-Consistent Generative Adversarial Networks for Coastal Flood Visualization",
                "abstract": "As climate change increases the intensity of natural disasters, society needs better tools for adaptation. Floods, for example, are the most frequent natural disaster, and better tools for flood risk communication could increase the support for flood-resilient infrastructure development. Our work aims to enable more visual communication of large-scale climate impacts via visualizing the output of coastal flood models as satellite imagery. We propose the first deep learning pipeline to ensure physical-consistency in synthetic visual satellite imagery. We advanced a state-of-the-art GAN called pix2pixHD, such that it produces imagery that is physically-consistent with the output of an expert-validated storm surge model (NOAA SLOSH). By evaluating the imagery relative to physics-based flood maps, we find that our proposed framework outperforms baseline models in both physical-consistency and photorealism. We envision our work to be the first step towards a global visualization of how the climate challenge will shape our landscape. Continuing on this path, we show that the proposed pipeline generalizes to visualize reforestation. We also publish a dataset of over 25k labelled image-triplets to study image-to-image translation in Earth observation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "81551971",
                        "name": "Bj\u00f6rn L\u00fctjens"
                    },
                    {
                        "authorId": "31336155",
                        "name": "B. Leshchinskiy"
                    },
                    {
                        "authorId": "1404333614",
                        "name": "C. Requena-Mesa"
                    },
                    {
                        "authorId": "2342264",
                        "name": "F. Chishtie"
                    },
                    {
                        "authorId": "2058921025",
                        "name": "Natalia D\u00edaz Rodr\u00edguez"
                    },
                    {
                        "authorId": "3200568",
                        "name": "O. Boulais"
                    },
                    {
                        "authorId": "2063992731",
                        "name": "Aruna Sankaranarayanan"
                    },
                    {
                        "authorId": "143814162",
                        "name": "A. Pi\u00f1a"
                    },
                    {
                        "authorId": "2681954",
                        "name": "Y. Gal"
                    },
                    {
                        "authorId": "1683938",
                        "name": "Chedy Ra\u00efssi"
                    },
                    {
                        "authorId": "12531802",
                        "name": "Alexander Lavin"
                    },
                    {
                        "authorId": "2054846636",
                        "name": "Dava Newman"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "3c0c7c6838fa0023252ec3d9c2f628955ab5441c",
                "externalIds": {
                    "ArXiv": "2104.04574",
                    "CorpusId": 233210408
                },
                "corpusId": 233210408,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3c0c7c6838fa0023252ec3d9c2f628955ab5441c",
                "title": "Model fusion with physics-guided machine learning",
                "abstract": "The unprecedented amount of data generated from experiments, field observations, and large-scale numerical simulations at a wide range of spatio-temporal scales have enabled the rapid advancement of data-driven and especially deep learning models in the field of fluid mechanics. Although these methods are proven successful for many applications, there is a grand challenge of improving their generalizability. This is particularly essential when data-driven models are employed within outer-loop applications like optimization. In this work, we put forth a physics-guided machine learning (PGML) framework that leverages the interpretable physics-based model with a deep learning model. The PGML framework is capable of enhancing the generalizability of data-driven models and effectively protect against or inform about the inaccurate predictions resulting from extrapolation. We apply the PGML framework as a novel model fusion approach combining the physics-based Galerkin projection model and long-short term memory (LSTM) network for parametric model order reduction of fluid flows. We demonstrate the improved generalizability of the PGML framework against a purely data-driven approach through the injection of physics features into intermediate LSTM layers. Our quantitative analysis shows that the overall model uncertainty can be reduced through the PGML approach especially for test data coming from a distribution different than the training data. Moreover, we demonstrate that our approach can be used as an inverse diagnostic tool providing a confidence score associated with models and observations. The proposed framework also allows for multi-fidelity computing by making use of low-fidelity models in the online deployment of quantified data-driven models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "11347090",
                        "name": "Suraj Pawar"
                    },
                    {
                        "authorId": "3071838",
                        "name": "O. San"
                    },
                    {
                        "authorId": "38457696",
                        "name": "Aditya G. Nair"
                    },
                    {
                        "authorId": "49219413",
                        "name": "A. Rasheed"
                    },
                    {
                        "authorId": "1834820",
                        "name": "T. Kvamsdal"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "31f787a7dadae7748b13bcb76d35df3cb0ae0671",
                "externalIds": {
                    "DOI": "10.1016/j.bbrc.2021.03.159",
                    "CorpusId": 233212349,
                    "PubMed": "33839415"
                },
                "corpusId": 233212349,
                "publicationVenue": {
                    "id": "1a9c698b-7fe4-4444-b391-208b4b2326e2",
                    "name": "Biochemical and Biophysical Research Communications - BBRC",
                    "type": "journal",
                    "alternate_names": [
                        "Biochem Biophys Res Commun",
                        "Biochemical and Biophysical Research Communications",
                        "Biochem Biophys Res Commun  BBRC"
                    ],
                    "issn": "0006-291X",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622790/description",
                    "alternate_urls": [
                        "http://www.idealibrary.com/links/toc/bbrc",
                        "https://www.journals.elsevier.com/biochemical-and-biophysical-research-communications/",
                        "http://www.sciencedirect.com/science/journal/0006291X"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/31f787a7dadae7748b13bcb76d35df3cb0ae0671",
                "title": "Learning-based event locating for single-molecule force spectroscopy.",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "31525258",
                        "name": "Zuzeng Lin"
                    },
                    {
                        "authorId": "46757846",
                        "name": "Xiaoqin Gao"
                    },
                    {
                        "authorId": "2133436136",
                        "name": "Shuai Li"
                    },
                    {
                        "authorId": "7402995",
                        "name": "Chunguang Hu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "An important component for sample-based learning of dynamical models is the choice of priors, which usually comes from our understanding of the underlying physical laws (Greydanus et al., 2019; Miles et al., 2020).",
                "This generic approach manages to achieve decent results,\nand it is often adopted as the comparative baseline when proposing other novel approaches (Greydanus et al., 2019; Miles et al., 2020; Sanchez-Gonzalez et al., 2018; Lutter et al., 2019).",
                "Long horizon prediction is a commonly used task to test the quality of learned dynamical models (Sanchez-Gonzalez et al., 2018; Lutter et al., 2019; Greydanus et al., 2019; Miles et al., 2020; Janner et al., 2019).",
                "One line of research takes advantage of the properties of the Hamiltonian and the Lagrangian function of dynamical systems (Greydanus et al., 2019; Miles et al., 2020; Lutter et al., 2019).",
                "Specifically, Greydanus et al. (2019) propose Hamiltonian Neural Networks (HNN) to explicitly use the Hamiltonian formalism and the canonical coordinates."
            ],
            "citingPaper": {
                "paperId": "a240446d138816cbb0a36d9d1e68f20c25d2923b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-02844",
                    "ArXiv": "2104.02844",
                    "CorpusId": 233168688
                },
                "corpusId": 233168688,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a240446d138816cbb0a36d9d1e68f20c25d2923b",
                "title": "GEM: Group Enhanced Model for Learning Dynamical Control Systems",
                "abstract": "Learning the dynamics of a physical system wherein an autonomous agent operates is an important task. Often these systems present apparent geometric structures. For instance, the trajectories of a robotic manipulator can be broken down into a collection of its transitional and rotational motions, fully characterized by the corresponding Lie groups and Lie algebras. In this work, we take advantage of these structures to build effective dynamical models that are amenable to sample-based learning. We hypothesize that learning the dynamics on a Lie algebra vector space is more effective than learning a direct state transition model. To verify this hypothesis, we introduce the Group Enhanced Model (GEM). GEMs significantly outperform conventional transition models on tasks of long-term prediction, planning, and model-based reinforcement learning across a diverse suite of standard continuous-control environments, including Walker, Hopper, Reacher, Half-Cheetah, Inverted Pendulums, Ant, and Humanoid. Furthermore, plugging GEM into existing state of the art systems enhances their performance, which we demonstrate on the PETS system. This work sheds light on a connection between learning of dynamics and Lie group properties, which opens doors for new research directions and practical applications along this direction. Our code is publicly available at: https://tinyurl.com/GEMMBRL.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2067291211",
                        "name": "Philippe Hansen-Estruch"
                    },
                    {
                        "authorId": "3163480",
                        "name": "Wenling Shang"
                    },
                    {
                        "authorId": "34026610",
                        "name": "Lerrel Pinto"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    },
                    {
                        "authorId": "3266876",
                        "name": "Stas Tiomkin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                ", 2016) or due to the ambiguities in unsupervised learning (Greydanus et al., 2019; Cranmer et al., 2020b).",
                "These methods require few prior assumptions about the system itself, but lack interpretability due to entangled variational factors (Chen et al., 2016) or due to the ambiguities in unsupervised learning (Greydanus et al., 2019; Cranmer et al., 2020b).",
                "Recent work on physics-based deep learning injects structure in the latent space of the dynamics using Lagrangian and Hamiltonian operators (Greydanus et al., 2019; Chen et al., 2020; Toth et al.,\n2020; Sanchez-Gonzalez et al., 2019; Cranmer et al., 2020b; Zhong et al., 2020), by explicitly\u2026",
                "\u2026in the context of differentiable physics simulation Degrave et al. (2016); de Avila Belbute-Peres et al. (2018); Cranmer et al. (2020b); Toth et al. (2020); Greydanus et al. (2019); Sanchez-Gonzalez et al. (2019), we focus on more challenging systems which have not been studied in prior art."
            ],
            "citingPaper": {
                "paperId": "92c89db048fb825d2c81b086d7bd82ed230f685b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-02646",
                    "ArXiv": "2104.02646",
                    "CorpusId": 233033848
                },
                "corpusId": 233033848,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/92c89db048fb825d2c81b086d7bd82ed230f685b",
                "title": "gradSim: Differentiable simulation for system identification and visuomotor control",
                "abstract": "We consider the problem of estimating an object's physical properties such as mass, friction, and elasticity directly from video sequences. Such a system identification problem is fundamentally ill-posed due to the loss of information during image formation. Current solutions require precise 3D labels which are labor-intensive to gather, and infeasible to create for many systems such as deformable solids or cloth. We present gradSim, a framework that overcomes the dependence on 3D supervision by leveraging differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation. This novel combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them. Moreover, our unified computation graph -- spanning from the dynamics and through the rendering process -- enables learning in challenging visuomotor control tasks, without relying on state-based (3D) supervision, while obtaining performance competitive to or better than techniques that rely on precise 3D labels.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7434636",
                        "name": "Krishna Murthy Jatavallabhula"
                    },
                    {
                        "authorId": "46637939",
                        "name": "M. Macklin"
                    },
                    {
                        "authorId": "2066832277",
                        "name": "Florian Golemo"
                    },
                    {
                        "authorId": "2961618",
                        "name": "Vikram S. Voleti"
                    },
                    {
                        "authorId": "1471743441",
                        "name": "Linda Petrini"
                    },
                    {
                        "authorId": "144069571",
                        "name": "Martin Weiss"
                    },
                    {
                        "authorId": "41226293",
                        "name": "Breandan Considine"
                    },
                    {
                        "authorId": "1585278372",
                        "name": "J\u00e9r\u00f4me Parent-L\u00e9vesque"
                    },
                    {
                        "authorId": "47966782",
                        "name": "Kevin Xie"
                    },
                    {
                        "authorId": "2253410",
                        "name": "Kenny Erleben"
                    },
                    {
                        "authorId": "3198259",
                        "name": "L. Paull"
                    },
                    {
                        "authorId": "2162768",
                        "name": "F. Shkurti"
                    },
                    {
                        "authorId": "1795014",
                        "name": "D. Nowrouzezahrai"
                    },
                    {
                        "authorId": "37895334",
                        "name": "S. Fidler"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "We used hidden layer sizes [16,16] for ICNN, [8,8] for each of the FCNN damping modules and [16,16] for ANN-PPO.",
                "Many recent methods have used the Lagrangian formulation [14], [15] and also the closely related Hamiltonian formulation [16], [17], as physics-based priors for learning dynamics models."
            ],
            "citingPaper": {
                "paperId": "d5bda58520854f7f12f1ea2fe3a3c2898bec510c",
                "externalIds": {
                    "MAG": "3144527084",
                    "DBLP": "journals/corr/abs-2103-16432",
                    "CorpusId": 232417344
                },
                "corpusId": 232417344,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d5bda58520854f7f12f1ea2fe3a3c2898bec510c",
                "title": "Learning Deep Neural Policies with Stability Guarantees",
                "abstract": "Reinforcement learning (RL) has been successfully used to solve various robotic control tasks. However, most of the existing works do not address the issue of control stability. This is in sharp contrast to the control theory community where the well-established norm is to prove stability whenever a control law is synthesized. What makes guaranteeing stability during RL difficult is threefold: non interpretable neural network policies, unknown system dynamics and random exploration. We contribute towards solving the stable RL problem in the context of robotic manipulation that may involve physical contact with the environment. Our solution is derived from physics-based prior that originates from Lagrangian mechanics and does not involve learning any dynamics model. We show how to parameterize the resulting $\\textit{energy shaping}$ policy as a deep neural network that consists of a convex potential function and a velocity dependent damping component. Our experiments, that include a real-world peg insertion task by a 7-DOF robot, validate the proposed policy structure and demonstrate the benefits of stability in RL.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2106182626",
                        "name": "S. A. Khader"
                    },
                    {
                        "authorId": "145039722",
                        "name": "Hang Yin"
                    },
                    {
                        "authorId": "144407954",
                        "name": "P. Falco"
                    },
                    {
                        "authorId": "1731490",
                        "name": "D. Kragic"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "We used hidden layer sizes [16,16] for ICNN, [8,8] for each of the FCNN damping modules and [16,16] for ANN-PPO.",
                "Many recent methods have used the Lagrangian formulation [14], [15] and also the closely related Hamiltonian formulation [16], [17], as physics-based priors for learning dynamics models."
            ],
            "citingPaper": {
                "paperId": "e8734774665fb2f7e1d4b84c4c2b4d0935a78f66",
                "externalIds": {
                    "ArXiv": "2103.16432",
                    "DBLP": "journals/ral/KhaderYFK21a",
                    "DOI": "10.1109/LRA.2021.3111962",
                    "CorpusId": 237940619
                },
                "corpusId": 237940619,
                "publicationVenue": {
                    "id": "93c335b7-edf4-45f5-8ddc-7c5835154945",
                    "name": "IEEE Robotics and Automation Letters",
                    "alternate_names": [
                        "IEEE Robot Autom Lett"
                    ],
                    "issn": "2377-3766",
                    "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=7083369"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e8734774665fb2f7e1d4b84c4c2b4d0935a78f66",
                "title": "Learning Deep Energy Shaping Policies for Stability-Guaranteed Manipulation",
                "abstract": "Deep reinforcement learning (DRL) has been successfully used to solve various robotic manipulation tasks. However, most of the existing works do not address the issue of control stability. This is in sharp contrast to the control theory community where the well-established norm is to prove stability whenever a control law is synthesized. What makes traditional stability analysis difficult for DRL are the uninterpretable nature of the neural network policies and unknown system dynamics. In this work, stability is obtained by deriving an interpretable deep policy structure based on the energy shaping control of Lagrangian systems. Then, stability during physical interaction with an unknown environment is established based on passivity. The result is a stability guaranteeing DRL in a model-free framework that is general enough for contact-rich manipulation tasks. With an experiment on a peg-in-hole task, we demonstrate, to the best of our knowledge, the first DRL with stability guarantee on a real robotic manipulator.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2106182626",
                        "name": "S. A. Khader"
                    },
                    {
                        "authorId": "145039722",
                        "name": "Hang Yin"
                    },
                    {
                        "authorId": "144407954",
                        "name": "P. Falco"
                    },
                    {
                        "authorId": "1731490",
                        "name": "D. Kragic"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "22e2d9ee20a84b8dd14f70cac1d7a97402a789be",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-03902",
                    "ArXiv": "2104.03902",
                    "CorpusId": 233181473
                },
                "corpusId": 233181473,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/22e2d9ee20a84b8dd14f70cac1d7a97402a789be",
                "title": "The Autodidactic Universe",
                "abstract": "We present an approach to cosmology in which the Universe learns its own physical laws. It does so by exploring a landscape of possible laws, which we express as a certain class of matrix models. We discover maps that put each of these matrix models in correspondence with both a gauge/gravity theory and a mathematical model of a learning machine, such as a deep recurrent, cyclic neural network. This establishes a correspondence between each solution of the physical theory and a run of a neural network. This correspondence is not an equivalence, partly because gauge theories emerge from N \u2192 \u221e limits of the matrix models, whereas the same limits of the neural networks used here are not well-defined. We discuss in detail what it means to say that learning takes place in autodidactic systems, where there is no supervision. We propose that if the neural network model can be said to learn without supervision, the same can be said for the corresponding physical theory. 1 ar X iv :2 10 4. 03 90 2v 1 [ he pth ] 2 9 M ar 2 02 1 We consider other protocols for autodidactic physical systems, such as optimization of graph variety, subset-replication using self-attention and look-ahead, geometrogenesis guided by reinforcement learning, structural learning using renormalization group techniques, and extensions. These protocols together provide a number of directions in which to explore the origin of physical laws based on putting machine learning architectures in correspondence with physical theories.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "29866115",
                        "name": "S. Alexander"
                    },
                    {
                        "authorId": "2058475857",
                        "name": "W. Cunningham"
                    },
                    {
                        "authorId": "145873883",
                        "name": "J. Lanier"
                    },
                    {
                        "authorId": "30178010",
                        "name": "L. Smolin"
                    },
                    {
                        "authorId": "51255906",
                        "name": "S. Stanojevic"
                    },
                    {
                        "authorId": "103009207",
                        "name": "M. Toomey"
                    },
                    {
                        "authorId": "1795635",
                        "name": "D. Wecker"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a301f1ab8cb4797d7a4db9e8df88066a690a56fe",
                "externalIds": {
                    "MAG": "3136841107",
                    "DOI": "10.1007/s12206-021-0342-5",
                    "CorpusId": 233709369
                },
                "corpusId": 233709369,
                "publicationVenue": {
                    "id": "737c9fb4-6dce-4072-a782-a3729b3c6b8a",
                    "name": "Journal of Mechanical Science and Technology",
                    "type": "journal",
                    "alternate_names": [
                        "J Mech Sci Technol"
                    ],
                    "issn": "1738-494X",
                    "url": "http://www.springer.com/engineering/mechanical+eng/journal/12206",
                    "alternate_urls": [
                        "http://link.springer.com/journal/12206",
                        "http://www.ksme.or.kr/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a301f1ab8cb4797d7a4db9e8df88066a690a56fe",
                "title": "Knowledge Integration into deep learning in dynamical systems: an overview and taxonomy",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2152669255",
                        "name": "S. Kim"
                    },
                    {
                        "authorId": "2122905467",
                        "name": "Iljeok Kim"
                    },
                    {
                        "authorId": "2127500852",
                        "name": "Jonghwa Lee"
                    },
                    {
                        "authorId": "2108271949",
                        "name": "Seungchul Lee"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "contexts": [
                "The ConSciNet model\u2019s performance and parameter generalization ability was assessed on two simple dynamic systems from [8]: an ideal pendulum and ideal mass-spring system.",
                "In recent years this area has seen renewed interest [8, 10, 17, 18].",
                "Here we will focus our discussion around [8]\u2019s Hamiltonian Neural Network architecture for learning exactly conserved quantities from data in an unsupervised manner.",
                "The individual results here for each value of l\u2217 are similar to [8]\u2019s results for a system with fixed parameters suggesting the ConSciNet approach successfully generalizes an HNN with regards to a system\u2019s parameters.",
                "To address this issue neural architectures have been designed to explicitly embed physical constraints and symmetries [8, 9, 10, 11, 12] in the description of classical dynamical systems.",
                "In [8] the system\u2019s physical parameters are held constant and absorbed into the canonical coordinates."
            ],
            "citingPaper": {
                "paperId": "04636c58067f8de46df28e9dd2166199096436dd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-10905",
                    "ArXiv": "2103.10905",
                    "CorpusId": 232290774
                },
                "corpusId": 232290774,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/04636c58067f8de46df28e9dd2166199096436dd",
                "title": "Joint Parameter Discovery and Generative Modeling of Dynamic Systems",
                "abstract": "Given an unknown dynamic system such as a coupled harmonic oscillator with $n$ springs and point masses. We are often interested in gaining insights into its physical parameters, i.e. stiffnesses and masses, by observing trajectories of motion. How do we achieve this from video frames or time-series data and without the knowledge of the dynamics model? We present a neural framework for estimating physical parameters in a manner consistent with the underlying physics. The neural framework uses a deep latent variable model to disentangle the system physical parameters from canonical coordinate observations. It then returns a Hamiltonian parameterization that generalizes well with respect to the discovered physical parameters. We tested our framework with simple harmonic oscillators, $n=1$, and noisy observations and show that it discovers the underlying system parameters and generalizes well with respect to these discovered parameters. Our model also extrapolates the dynamics of the system beyond the training interval and outperforms a non-physically constrained baseline model. Our source code and datasets can be found at this URL: https://github.com/gbarber94/ConSciNet.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "153374577",
                        "name": "Gregory Barber"
                    },
                    {
                        "authorId": "39004674",
                        "name": "M. Haile"
                    },
                    {
                        "authorId": "2144891774",
                        "name": "Tzikang Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "The modelling of dynamical systems with neural networks is discussed in many papers [15, 16, 17, 18]."
            ],
            "citingPaper": {
                "paperId": "2a674dc55248f990f58304887c2ed936eab55718",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-08561",
                    "ArXiv": "2103.08561",
                    "CorpusId": 232232753
                },
                "corpusId": 232232753,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2a674dc55248f990f58304887c2ed936eab55718",
                "title": "Meta-Solver for Neural Ordinary Differential Equations",
                "abstract": "A conventional approach to train neural ordinary differential equations (ODEs) is to fix an ODE solver and then learn the neural network's weights to optimize a target loss function. However, such an approach is tailored for a specific discretization method and its properties, which may not be optimal for the selected application and yield the overfitting to the given solver. In our paper, we investigate how the variability in solvers' space can improve neural ODEs performance. We consider a family of Runge-Kutta methods that are parameterized by no more than two scalar variables. Based on the solvers' properties, we propose an approach to decrease neural ODEs overfitting to the pre-defined solver, along with a criterion to evaluate such behaviour. Moreover, we show that the right choice of solver parameterization can significantly affect neural ODEs models in terms of robustness to adversarial attacks. Recently it was shown that neural ODEs demonstrate superiority over conventional CNNs in terms of robustness. Our work demonstrates that the model robustness can be further improved by optimizing solver choice for a given task. The source code to reproduce our experiments is available at https://github.com/juliagusak/neural-ode-metasolver.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3397802",
                        "name": "Julia Gusak"
                    },
                    {
                        "authorId": "9708660",
                        "name": "A. Katrutsa"
                    },
                    {
                        "authorId": "102677100",
                        "name": "Talgat Daulbaev"
                    },
                    {
                        "authorId": "145683892",
                        "name": "A. Cichocki"
                    },
                    {
                        "authorId": "1738205",
                        "name": "I. Oseledets"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "77a41b668e1d2e53c47c7317ba8ac91698d299f4",
                "externalIds": {
                    "ArXiv": "2103.08645",
                    "DBLP": "journals/corr/abs-2103-08645",
                    "DOI": "10.1103/PhysRevA.104.062404",
                    "CorpusId": 232240600
                },
                "corpusId": 232240600,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/77a41b668e1d2e53c47c7317ba8ac91698d299f4",
                "title": "Tomography of time-dependent quantum spin networks with machine learning",
                "abstract": "Interacting quantum Hamiltonians are fundamental to quantum computing. Data-based tomography of timeindependent quantum Hamiltonians has been achieved, but an open challenge is to ascertain the structures of time-dependent quantum Hamiltonians using time series measurements taken locally from a small subset of the spins. Physically, the dynamical evolution of a spin system under time-dependent driving or perturbation is described by the Heisenberg equation of motion. Motivated by this basic fact, we articulate a physics-enhanced machine-learning framework whose core is Heisenberg neural networks. In particular, we develop a deep learning algorithm according to some physics-motivated loss function based on the Heisenberg equation, which \u201cforces\u201d the neural network to follow the quantum evolution of the spin variables. We demonstrate that, from local measurements, not only can the local Hamiltonian be recovered, but the Hamiltonian reflecting the interacting structure of the whole system can also be faithfully reconstructed. We test our Heisenberg neural machine on spin systems of a variety of structures. In the extreme case in which measurements are taken from only one spin, the achieved tomography fidelity values can reach about 90%. The developed machine-learning framework is applicable to any time-dependent systems whose quantum dynamical evolution is governed by the Heisenberg equation of motion.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "93112518",
                        "name": "Chen-Di Han"
                    },
                    {
                        "authorId": "5899699",
                        "name": "Bryan Glaz"
                    },
                    {
                        "authorId": "39004674",
                        "name": "M. Haile"
                    },
                    {
                        "authorId": "144769611",
                        "name": "Y. Lai"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "018d3a5672537a9c20bbe1e518942ac72eb2ee62",
                "externalIds": {
                    "MAG": "3164520435",
                    "ArXiv": "2103.07475",
                    "DOI": "10.1002/prop.202100057",
                    "CorpusId": 232232954
                },
                "corpusId": 232232954,
                "publicationVenue": {
                    "id": "217b4bbb-b4f0-44dc-868d-1d12e766dc7f",
                    "name": "Fortschritte der Physik",
                    "alternate_names": [
                        "Fortschritte Phys"
                    ],
                    "issn": "0015-8208",
                    "url": "https://onlinelibrary.wiley.com/journal/15213978",
                    "alternate_urls": [
                        "http://www.wiley-vch.de/publish/en/journals/alphabeticindex/2244"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/018d3a5672537a9c20bbe1e518942ac72eb2ee62",
                "title": "Integrability Ex Machina",
                "abstract": "Determining whether a dynamical system is integrable is generally a difficult task which is currently done on a case by case basis requiring large human input. Here we propose and test an automated method to search for the existence of relevant structures, the Lax pair and Lax connection respectively. By formulating this search as an optimization problem, we are able to identify appropriate structures via machine learning techniques. We test our method on standard systems of classical integrability and find that we can single out some integrable deformations of a system. Due to the ambiguity in defining a Lax pair our algorithm identifies novel Lax pairs which can be easily verified analytically.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51264706",
                        "name": "S. Krippendorf"
                    },
                    {
                        "authorId": "93950278",
                        "name": "D. L\u00fcst"
                    },
                    {
                        "authorId": "147611823",
                        "name": "Marc Syvaeri"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "For example, the network may incorporate inductive bias corresponding to the physical laws of interaction and motion [25, 6, 107, 16, 92, 32]."
            ],
            "citingPaper": {
                "paperId": "390aa4fdc4072479ceaa638b1fe586d056feeb98",
                "externalIds": {
                    "ArXiv": "2103.07292",
                    "DBLP": "journals/corr/abs-2103-07292",
                    "DOI": "10.1109/CVPR46437.2021.00808",
                    "CorpusId": 232222878
                },
                "corpusId": 232222878,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/390aa4fdc4072479ceaa638b1fe586d056feeb98",
                "title": "VDSM: Unsupervised Video Disentanglement with State-Space Modeling and Deep Mixtures of Experts",
                "abstract": "Disentangled representations support a range of downstream tasks including causal reasoning, generative modeling, and fair machine learning. Unfortunately, disentanglement has been shown to be impossible without the incorporation of supervision or inductive bias. Given that supervision is often expensive or infeasible to acquire, we choose to incorporate structural inductive bias and present an unsupervised, deep State-Space-Model for Video Disentanglement (VDSM). The model disentangles latent time-varying and dynamic factors via the incorporation of hierarchical structure with a dynamic prior and a Mixture of Experts decoder. VDSM learns separate disentangled representations for the identity of the object or person in the video, and for the action being performed. We evaluate VDSM across a range of qualitative and quantitative tasks including identity and dynamics transfer, sequence generation, Fr\u00e9chet Inception Distance, and factor classification. VDSM achieves state-of-the-art performance and exceeds adversarial methods, even when the methods use additional supervision.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51468109",
                        "name": "M. Vowels"
                    },
                    {
                        "authorId": "40163061",
                        "name": "N. C. Camgoz"
                    },
                    {
                        "authorId": "145398628",
                        "name": "R. Bowden"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Among other, networks covering lagrangians [12] and hamiltonian dynamics [13] have been introduced."
            ],
            "citingPaper": {
                "paperId": "473fd59f8d4874b49794cb0ef371ab62be9fc02a",
                "externalIds": {
                    "DBLP": "conf/icit2/SchwungPN21",
                    "DOI": "10.1109/ICIT46573.2021.9453587",
                    "CorpusId": 235476206
                },
                "corpusId": 235476206,
                "publicationVenue": {
                    "id": "6c1a00cb-4b59-44d6-a887-bfd7c5b3768f",
                    "name": "International Conference on Industrial Technology",
                    "type": "conference",
                    "alternate_names": [
                        "ICIT",
                        "Int Conf Ind Technol",
                        "IEEE Int Conf Integr Technol",
                        "IEEE International Conference on Integration Technology"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/473fd59f8d4874b49794cb0ef371ab62be9fc02a",
                "title": "Rigid Body Movement Prediction Using Dual Quaternion Recurrent Neural Networks",
                "abstract": "This paper presents a novel approach for the data based prediction of rigid body movements. To this end, we combine data based learning with a physically motivated neural network architecture using the theory of dual quaternions. Particularly, we develop a novel neural network architecture based on dual quaternion algebra which is particular suitable for representing rigid body movements. To account for multi-step predictions and the inherent dynamics of rigid bodies, we particularly focus on recurrent neural networks. As such we propose both dual quaternion recurrent neural networks as well as dual quaternion long short term memories. We apply the approach to a simplified simulation environment developed using the discrete element method which allows for a very detailed simulation of the movements. The obtained results underline the applicability and potential of the approach in terms of improved prediction performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2267797",
                        "name": "Andreas Schwung"
                    },
                    {
                        "authorId": "2026685440",
                        "name": "Johannes P\u00f6ppelbaum"
                    },
                    {
                        "authorId": "2113656014",
                        "name": "Pradeep C. Nutakki"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "In terms of mechanical problems modeled by Hamiltonian systems, seminal progress include HNN (Greydanus et al., 2019) and an independent work (Bertalan et al., 2019), SRNN (Chen et al., 2020), SympNets (Jin et al., 2020), and (Lutter et al., 2019; Toth et al., 2020; Zhong et al., 2020; Wu et al.,\u2026",
                "In particular, both HNN and SRNN are based on the great idea of learning (using a neural network) the Hamiltonian that generates the vector field (VF), instead of learning the VF itself; this improves accuracy as the Hamiltonian structure of the VF will not be lost due to approximation.",
                "HNN, on the other hand, has exponentially growing error which quickly saturates to maximum values (due to boundedness of trajectories).",
                "HNN learns the Hamiltonian by matching its induced VF with the latent VF (when such information is unavailable, for example in a purely data driven context, data-based approximation such as finite-difference is needed).",
                "The time derivative data of the vector field based methods (VFNN, HNN) are generated using (1st-order) finite difference.",
                "HNN, SRNN, SympNets are trained by their provided codes.",
                "VFNN, HNN, SRNN (seq len=2), and GFNN are trained with the same data set D2, while SRNN (seq len=5) is trained with D5.",
                "One can still apply these methods regardless, for example by using finite differences to construct a fictitious vector field for VFNN and HNN to learn, or just use SRNN without realizing that no Hamiltonian will be able to produce the training data.",
                "HNN, SRNN are trained under default training setups and SympNets is trained using LA-SympNets with 30 layers and 10 sublayers (deeper than their default setups for improved performance).",
                "In terms of mechanical problems modeled by Hamiltonian systems, seminal progress include HNN (Greydanus et al., 2019) and an independent work (Bertalan et al., 2019), SRNN (Chen et al., 2020), SympNets (Jin et al., 2020), and (Lutter et al., 2019; Toth et al., 2020; Zhong et al., 2020; Wu et al., 2020; Xiong et al., 2021), all of which, except SympNets, are related to learning some quantity that produces the Hamiltonian vector field.",
                "The standard map seems to be a challenging problem; HNN and SRNN did not manage to reproduce any chaotic motion, and VFNN completely distorted the chaotic sea.",
                "Note the seminal work of HNN used RK45 which is not symplectic, however with small error tolerance (thus good precision but high computation cost).\nnonseparable3.",
                "Methods based on vector fields (e.g., VFNN) or Hamiltonian (e.g., HNN, SRNN) are not very suitable for this prediction task because there is no latent continuous (Hamiltonian) dynamics.",
                "HNN, SRNN are trained by their provided codes under default training setups.",
                "The time derivative data of the vector field based methods (VFNN, HNN) are generated using (1st-order) finite difference (with \u2206t = 1)."
            ],
            "citingPaper": {
                "paperId": "75720b44f3aedeea3e5017c58d60a1ee80dba2ec",
                "externalIds": {
                    "DBLP": "conf/icml/ChenT21",
                    "ArXiv": "2103.05632",
                    "CorpusId": 232168400
                },
                "corpusId": 232168400,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/75720b44f3aedeea3e5017c58d60a1ee80dba2ec",
                "title": "Data-driven Prediction of General Hamiltonian Dynamics via Learning Exactly-Symplectic Maps",
                "abstract": "We consider the learning and prediction of nonlinear time series generated by a latent symplectic map. A special case is (not necessarily separable) Hamiltonian systems, whose solution flows give such symplectic maps. For this special case, both generic approaches based on learning the vector field of the latent ODE and specialized approaches based on learning the Hamiltonian that generates the vector field exist. Our method, however, is different as it does not rely on the vector field nor assume its existence; instead, it directly learns the symplectic evolution map in discrete time. Moreover, we do so by representing the symplectic map via a generating function, which we approximate by a neural network (hence the name GFNN). This way, our approximation of the evolution map is always \\emph{exactly} symplectic. This additional geometric structure allows the local prediction error at each step to accumulate in a controlled fashion, and we will prove, under reasonable assumptions, that the global prediction error grows at most \\emph{linearly} with long prediction time, which significantly improves an otherwise exponential growth. In addition, as a map-based and thus purely data-driven method, GFNN avoids two additional sources of inaccuracies common in vector-field based approaches, namely the error in approximating the vector field by finite difference of the data, and the error in numerical integration of the vector field for making predictions. Numerical experiments further demonstrate our claims.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2569809",
                        "name": "Ren-Chuen Chen"
                    },
                    {
                        "authorId": "46699279",
                        "name": "Molei Tao"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Finally, we would like to point out that discrete Hamiltonian systems have already been used to design RNNs, for instance in (Greydanus et al., 2019) and also in (Chen et al.",
                "Finally, we would like to point out that discrete Hamiltonian systems have already been used to design RNNs, for instance in (Greydanus et al., 2019) and also in (Chen et al.)"
            ],
            "citingPaper": {
                "paperId": "9501128d1a84135914d48f0874d6072baa99406c",
                "externalIds": {
                    "DBLP": "conf/icml/RuschM21",
                    "ArXiv": "2103.05487",
                    "CorpusId": 232168873
                },
                "corpusId": 232168873,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9501128d1a84135914d48f0874d6072baa99406c",
                "title": "UnICORNN: A recurrent model for learning very long time dependencies",
                "abstract": "The design of recurrent neural networks (RNNs) to accurately process sequential inputs with long-time dependencies is very challenging on account of the exploding and vanishing gradient problem. To overcome this, we propose a novel RNN architecture which is based on a structure preserving discretization of a Hamiltonian system of second-order ordinary differential equations that models networks of oscillators. The resulting RNN is fast, invertible (in time), memory efficient and we derive rigorous bounds on the hidden state gradients to prove the mitigation of the exploding and vanishing gradient problem. A suite of experiments are presented to demonstrate that the proposed RNN provides state of the art performance on a variety of learning tasks with (very) long-time dependencies.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "67164720",
                        "name": "T. Konstantin Rusch"
                    },
                    {
                        "authorId": "1767117",
                        "name": "Siddhartha Mishra"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "\u2026al., 2020; Muralidhar et al., 2020; Zhang et al., 2021; BelbutePeres et al., 2020);\n\u2022 fB works as an observation function that changes signal\u2019s modality (Greydanus et al., 2019; Lutter et al., 2019; Y\u0131ld\u0131z et al., 2019; Linial et al., 2020; Toth et al., 2020; Cranmer et al., 2020; Saemundsson et\u2026",
                "Experiments on Human Locomotion\nPhysics model We modeled fP with a trainable Hamilton\u2019s equation as in Toth et al. (2020); Greydanus et al. (2019):\nfP\n([ pT qT ]T , zP ) = [ \u2212\u2202H\u2202q T \u2202H \u2202p T ]T , (26)\nwhere p \u2208 Rdy is a generalized position, q \u2208 Rdy is a generalized momentum, andH : Rdy \u00d7Rdy \u2192 R is\u2026",
                "We did not choose a specific model but let fP be a trainable Hamilton\u2019s equation as in [39, 11].",
                "We did not choose a specific model but let fP be a trainable Hamilton\u2019s equation as in Toth et al. (2020); Greydanus et al. (2019):\nfP\n([ pT qT ]T) = [ \u2212\u2202H\u2202q T \u2202H \u2202p T ]T , (24)\nwhere p \u2208 Rdy is a generalized position, q \u2208 Rdy is a generalized momentum, andH : Rdy \u00d7Rdy \u2192 R is a Hamiltonian."
            ],
            "citingPaper": {
                "paperId": "180d0ba8d544ca0f3c8646f3fcf8ecaeed308226",
                "externalIds": {
                    "DBLP": "conf/nips/TakeishiK21",
                    "ArXiv": "2102.13156",
                    "CorpusId": 232068635
                },
                "corpusId": 232068635,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/180d0ba8d544ca0f3c8646f3fcf8ecaeed308226",
                "title": "Physics-Integrated Variational Autoencoders for Robust and Interpretable Generative Modeling",
                "abstract": "Integrating physics models within machine learning models holds considerable promise toward learning robust models with improved interpretability and abilities to extrapolate. In this work, we focus on the integration of incomplete physics models into deep generative models. In particular, we introduce an architecture of variational autoencoders (VAEs) in which a part of the latent space is grounded by physics. A key technical challenge is to strike a balance between the incomplete physics and trainable components such as neural networks for ensuring that the physics part is used in a meaningful manner. To this end, we propose a regularized learning method that controls the effect of the trainable components and preserves the semantics of the physics-based latent variables as intended. We not only demonstrate generative performance improvements over a set of synthetic and real-world datasets, but we also show that we learn robust models that can consistently extrapolate beyond the training distribution in a meaningful manner. Moreover, we show that we can control the generative process in an interpretable manner.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2024905",
                        "name": "Naoya Takeishi"
                    },
                    {
                        "authorId": "1784711",
                        "name": "Alexandros Kalousis"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "0a890542adca2a1e7a2a59862e0446649443b674",
                "externalIds": {
                    "ArXiv": "2102.13235",
                    "DBLP": "journals/corr/abs-2102-13235",
                    "DOI": "10.1103/PhysRevResearch.3.023156",
                    "CorpusId": 232069001
                },
                "corpusId": 232069001,
                "publicationVenue": {
                    "id": "349f119f-f4ee-48cf-aedb-89bcb56ab8e3",
                    "name": "Physical Review Research",
                    "type": "journal",
                    "alternate_names": [
                        "Phys Rev Res"
                    ],
                    "issn": "2643-1564",
                    "url": "https://journals.aps.org/prresearch"
                },
                "url": "https://www.semanticscholar.org/paper/0a890542adca2a1e7a2a59862e0446649443b674",
                "title": "Adaptable Hamiltonian neural networks",
                "abstract": "The rapid growth of research in exploiting machine learning to predict chaotic systems has revived a recent interest in Hamiltonian Neural Networks (HNNs) with physical constraints defined by the Hamilton's equations of motion, which represent a major class of physics-enhanced neural networks. We introduce a class of HNNs capable of adaptable prediction of nonlinear physical systems: by training the neural network based on time series from a small number of bifurcation-parameter values of the target Hamiltonian system, the HNN can predict the dynamical states at other parameter values, where the network has not been exposed to any information about the system at these parameter values. The architecture of the HNN differs from the previous ones in that we incorporate an input parameter channel, rendering the HNN parameter--cognizant. We demonstrate, using paradigmatic Hamiltonian systems, that training the HNN using time series from as few as four parameter values bestows the neural machine with the ability to predict the state of the target system in an entire parameter interval. Utilizing the ensemble maximum Lyapunov exponent and the alignment index as indicators, we show that our parameter-cognizant HNN can successfully predict the route of transition to chaos. Physics-enhanced machine learning is a forefront area of research, and our adaptable HNNs provide an approach to understanding machine learning with broad applications.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "93112518",
                        "name": "Chen-Di Han"
                    },
                    {
                        "authorId": "5899699",
                        "name": "Bryan Glaz"
                    },
                    {
                        "authorId": "39004674",
                        "name": "M. Haile"
                    },
                    {
                        "authorId": "144769611",
                        "name": "Y. Lai"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "The first condition has been mitigated by explicitly incorporating symplectic structure or conservation laws on neural networks, called Hamiltonian neural networks (HNN) (Greydanus et al., 2019) for learning Hamiltonian dynamics.",
                "The most related to our work, (Greydanus et al., 2019) introduced Hamiltonian Neural Networks (HNNs) to learn the dynamics of Hamiltonian systems by parameterizing the Hamiltonian with neural networks.",
                "Following (Greydanus et al., 2019), we evaluate the MSEs of the predicted trajectories and energies from their corresponding ground truth at each time step.",
                "In (Greydanus et al., 2019), the Hamiltonian function can be approximated by neural networks, H\u03b8, called HNN.",
                "At least hundreds of states should be given as train set, and thousands of gradient steps are required for training HNN for learning a system (Greydanus et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "87174a4df7567e52771d15f949d459791145379b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-11544",
                    "ArXiv": "2102.11544",
                    "CorpusId": 232013963
                },
                "corpusId": 232013963,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/87174a4df7567e52771d15f949d459791145379b",
                "title": "Identifying Physical Law of Hamiltonian Systems via Meta-Learning",
                "abstract": "Hamiltonian mechanics is an effective tool to represent many physical processes with concise yet well-generalized mathematical expressions. A well-modeled Hamiltonian makes it easy for researchers to analyze and forecast many related phenomena that are governed by the same physical law. However, in general, identifying a functional or shared expression of the Hamiltonian is very difficult. It requires carefully designed experiments and the researcher's insight that comes from years of experience. We propose that meta-learning algorithms can be potentially powerful data-driven tools for identifying the physical law governing Hamiltonian systems without any mathematical assumptions on the representation, but with observations from a set of systems governed by the same physical law. We show that a well meta-trained learner can identify the shared representation of the Hamiltonian by evaluating our method on several types of physical systems with various experimental settings.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108154935",
                        "name": "Seungjun Lee"
                    },
                    {
                        "authorId": "11325568",
                        "name": "Haesang Yang"
                    },
                    {
                        "authorId": "2517631",
                        "name": "W. Seong"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026physical phenomena by learning the energy functionH in such equations with a neural network HNN (e.g., Chen et al. (2020); Cranmer et al. (2020); Greydanus, Dzamba, and Yosinski (2019); Matsubara, Ishikawa, and Yaguchi (2020); Zhong, Dey, and Chakraborty (2020)); however, to the best of our\u2026",
                "Among them, the most basic studies are neural ordinary differential equations (Chen et al. 2018) and HNNs (Greydanus, Dzamba, and Yosinski 2019).",
                "In this paper, we focus on theories of the properties of the most fundamental model, comprising Hamiltonian neural networks (HNNs) (Greydanus, Dzamba, and Yosinski 2019)\ndu dt = S \u2202HNN \u2202u\n(2)\nand their extensions in practical situations, where the learning error is not completely zero."
            ],
            "citingPaper": {
                "paperId": "966a110bea593cf5bd566e4a1f250f0e53cd77ed",
                "externalIds": {
                    "ArXiv": "2102.11923",
                    "DBLP": "conf/aaai/Chen0Y22",
                    "DOI": "10.1609/aaai.v36i6.20582",
                    "CorpusId": 247597094
                },
                "corpusId": 247597094,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/966a110bea593cf5bd566e4a1f250f0e53cd77ed",
                "title": "KAM Theory Meets Statistical Learning Theory: Hamiltonian Neural Networks with Non-zero Training Loss",
                "abstract": "Many physical phenomena are described by Hamiltonian mechanics using an energy function (Hamiltonian). Recently, the Hamiltonian neural network, which approximates the Hamiltonian by a neural network, and its extensions have attracted much attention. This is a very powerful method, but theoretical studies are limited. In this study, by combining the statistical learning theory and KAM theory, we provide a theoretical analysis of the behavior of Hamiltonian neural networks when the learning error is not completely zero. A Hamiltonian neural network with non-zero errors can be considered as a perturbation from the true dynamics, and the perturbation theory of the Hamilton equation is widely known as KAM theory. To apply KAM theory, we provide a generalization error bound for Hamiltonian neural networks by deriving an estimate of the covering number of the gradient of the multi-layer perceptron, which is the key ingredient of the model. This error bound gives a sup-norm bound on the Hamiltonian that is required in the application of KAM theory.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115869301",
                        "name": "Yu-Hsueh Chen"
                    },
                    {
                        "authorId": "144872058",
                        "name": "Takashi Matsubara"
                    },
                    {
                        "authorId": "2580628",
                        "name": "Takaharu Yaguchi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "A rapidlygrowing line of work (Raissi, 2018; Greydanus et al., 2019; Lutter et al., 2020; Cranmer et al., 2020; S\u00e6mundsson et al., 2020) has thus focused on how to introduce inductive biases into these networks to enable them to learn more accurate models from less data.",
                "Though neural networks are good at approximating general classes of functions, they often struggle to learn invariant properties of physical systems, such as the conservation of energy or momentum (Greydanus et al., 2019) and other qualitative properties.",
                "\u2026differential equations (E, 2017; Haber and Ruthotto, 2017; Chen et al., 2018; Ruthotto and Haber, 2018) and physicallyinspired neural networks (Raissi, 2018; Greydanus et al., 2019; Lutter et al., 2020; S\u00e6mundsson et al., 2020), we study the problem of learning unknown contact dynamics from data.",
                "L G\n] 1\n5 A\nmodels (Deisenroth et al., 2015; Greydanus et al., 2019; Lutter et al., 2020; Cranmer et al., 2020; S\u00e6mundsson et al., 2020).",
                "These inductive biases have been shown to improve data efficiency and facilitate accurate long-term forecasting (Raissi, 2018; Greydanus et al., 2019; Lutter et al., 2020; Cranmer et al., 2020; S\u00e6mundsson et al., 2020).",
                "This task has been studied previously by a number of authors, such as Greydanus et al. (2019) and S\u00e6mundsson et al. (2020), who propose to use a network constructed from a variational velocity Verlet integrator in order to learn in a data-efficient manner.",
                "Building on these ideas, a number of recent works have proposed replacing the black-box system of ODEs with other systems that are more structured (Raissi, 2018; Greydanus et al., 2019; Lutter et al., 2020; Cranmer et al., 2020; S\u00e6mundsson et al., 2020).",
                "\u2026from mechanics, such as the Euler\u2013Lagrange equations or Hamilton\u2019s equations, with structure-preserving integrators, such as the Sto\u0308rmer\u2013Verlet method, giving rise to physically structured neural networks (Greydanus et al., 2019; Lutter et al., 2020; Cranmer et al., 2020; S\u00e6mundsson et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "bd42788e01e9f8c35aa45440cd7edf484fb27528",
                "externalIds": {
                    "ArXiv": "2102.11206",
                    "DBLP": "journals/corr/abs-2102-11206",
                    "CorpusId": 232013426
                },
                "corpusId": 232013426,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bd42788e01e9f8c35aa45440cd7edf484fb27528",
                "title": "Learning Contact Dynamics using Physically Structured Neural Networks",
                "abstract": "Learning physically structured representa-tions of dynamical systems that include contact between di\ufb00erent objects is an important problem for learning-based approaches in robotics. Black-box neural networks can learn to approximately represent discontinuous dynamics, but they typically require large quantities of data and often su\ufb00er from patho-logical behaviour when forecasting for longer time horizons. In this work, we use connec-tions between deep neural networks and dif-ferential equations to design a family of deep network architectures for representing contact dynamics between objects. We show that these networks can learn discontinuous contact events in a data-e\ufb03cient manner from noisy observations in settings that are tra-ditionally di\ufb03cult for black-box approaches and recent physics inspired neural networks. Our results indicate that an idealised form of touch feedback\u2014which is heavily relied upon by biological systems\u2014is a key component of making this learning problem tractable. Together with the inductive biases introduced through the network architectures, our techniques enable accurate learning of contact dynamics from observations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2051416418",
                        "name": "Andreas Hochlehnert"
                    },
                    {
                        "authorId": "46846955",
                        "name": "Alexander Terenin"
                    },
                    {
                        "authorId": "21803598",
                        "name": "Steind\u00f3r S\u00e6mundsson"
                    },
                    {
                        "authorId": "2261881",
                        "name": "M. Deisenroth"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Neural ODE is available for modeling a continuous-time dynamics; irregularly sampled time series [21], stable dynamical systems [30, 35], and physical phenomena associated with geometric structures [2, 12, 26].",
                "Experimental Settings: We evaluated the proposed symplectic adjoint method on learning continuous-time dynamical systems [12, 26, 33]."
            ],
            "citingPaper": {
                "paperId": "3e8923eadd533eb1098f7bc89ae47547bcac1f1d",
                "externalIds": {
                    "DBLP": "conf/nips/MatsubaraMY21",
                    "ArXiv": "2102.09750",
                    "CorpusId": 231979541
                },
                "corpusId": 231979541,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3e8923eadd533eb1098f7bc89ae47547bcac1f1d",
                "title": "Symplectic Adjoint Method for Exact Gradient of Neural ODE with Minimal Memory",
                "abstract": "A neural network model of a differential equation, namely neural ODE, has enabled the learning of continuous-time dynamical systems and probabilistic distributions with high accuracy. The neural ODE uses the same network repeatedly during a numerical integration. The memory consumption of the backpropagation algorithm is proportional to the number of uses times the network size. This is true even if a checkpointing scheme divides the computation graph into sub-graphs. Otherwise, the adjoint method obtains a gradient by a numerical integration backward in time. Although this method consumes memory only for a single network use, it requires high computational cost to suppress numerical errors. This study proposes the symplectic adjoint method, which is an adjoint method solved by a symplectic integrator. The symplectic adjoint method obtains the exact gradient (up to rounding error) with memory proportional to the number of uses plus the network size. The experimental results demonstrate that the symplectic adjoint method consumes much less memory than the naive backpropagation algorithm and checkpointing schemes, performs faster than the adjoint method, and is more robust to rounding errors.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144872058",
                        "name": "Takashi Matsubara"
                    },
                    {
                        "authorId": "34838052",
                        "name": "Y. Miyatake"
                    },
                    {
                        "authorId": "2580628",
                        "name": "Takaharu Yaguchi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "One example involves the learning of invariant quantities via their Hamiltonian or Lagrangian representations [13, 14, 15, 16, 17].",
                "A particular interest was given to the automatic learning of equivariances in dynamical systems through their Hamiltonian [14, 15, 16, 17] or Lagrangian [13] formulations."
            ],
            "citingPaper": {
                "paperId": "90fb9cbdafb2c9e00c26464c9b9e25b0b739a635",
                "externalIds": {
                    "ArXiv": "2102.09589",
                    "DBLP": "journals/corr/abs-2102-09589",
                    "CorpusId": 231979467
                },
                "corpusId": 231979467,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/90fb9cbdafb2c9e00c26464c9b9e25b0b739a635",
                "title": "A Differential Geometry Perspective on Orthogonal Recurrent Models",
                "abstract": "Recently, orthogonal recurrent neural networks (RNNs) have emerged as state-of-the-art models for learning long-term dependencies. This class of models mitigates the exploding and vanishing gradients problem by design. In this work, we employ tools and insights from differential geometry to offer a novel perspective on orthogonal RNNs. We show that orthogonal RNNs may be viewed as optimizing in the space of divergence-free vector fields. Specifically, based on a well-known result in differential geometry that relates vector fields and linear operators, we prove that every divergence-free vector field is related to a skew-symmetric matrix. Motivated by this observation, we study a new recurrent model, which spans the entire space of vector fields. Our method parameterizes vector fields via the directional derivatives of scalar functions. This requires the construction of latent inner product, gradient, and divergence operators. In comparison to state-of-the-art orthogonal RNNs, our approach achieves comparable or better results on a variety of benchmark tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1927247",
                        "name": "Omri Azencot"
                    },
                    {
                        "authorId": "2371914",
                        "name": "N. Benjamin Erichson"
                    },
                    {
                        "authorId": "1398681470",
                        "name": "M. Ben-Chen"
                    },
                    {
                        "authorId": "143884206",
                        "name": "Michael W. Mahoney"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "As example applications, to solve the dynamics modeling problems, some works have introduced Hamiltonian dynamics (Greydanus et al., 2019; Toth et al., 2019; Zhong et al., 2019).",
                "In some works, in order to solve machine learning problems such as sequence prediction or reinforcement learning, neural networks attempt to learn a data symmetry of physical systems from noisy observations directly (Greydanus et al., 2019; Toth et al., 2019; Zhong et al., 2019; Sanchez-Gonzalez et al., 2019).",
                "\u2026in order to solve machine learning problems such as sequence prediction or reinforcement learning, neural networks attempt to learn a data symmetry of physical systems from noisy observations directly (Greydanus et al., 2019; Toth et al., 2019; Zhong et al., 2019; Sanchez-Gonzalez et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "bc716f31b93eac93f627e04039da7f6c4d126165",
                "externalIds": {
                    "ArXiv": "2102.08759",
                    "DBLP": "conf/iclr/KawanoKSIM21",
                    "CorpusId": 231942702
                },
                "corpusId": 231942702,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/bc716f31b93eac93f627e04039da7f6c4d126165",
                "title": "Group Equivariant Conditional Neural Processes",
                "abstract": "We present the group equivariant conditional neural process (EquivCNP), a metalearning method with permutation invariance in a data set as in conventional conditional neural processes (CNPs), and it also has transformation equivariance in data space. Incorporating group equivariance, such as rotation and scaling equivariance, provides a way to consider the symmetry of real-world data. We give a decomposition theorem for permutation-invariant and group-equivariant maps, which leads us to construct EquivCNPs with an infinite-dimensional latent space to handle group symmetries. In this paper, we build architecture using Lie group convolutional layers for practical implementation. We show that EquivCNP with translation equivariance achieves comparable performance to conventional CNPs in a 1D regression task. Moreover, we demonstrate that incorporating an appropriate Lie group equivariance, EquivCNP is capable of zero-shot generalization for an image-completion task by selecting an appropriate Lie group equivariance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2105938934",
                        "name": "M. Kawano"
                    },
                    {
                        "authorId": "1896666",
                        "name": "Wataru Kumagai"
                    },
                    {
                        "authorId": "41152282",
                        "name": "Akiyoshi Sannai"
                    },
                    {
                        "authorId": "1715282",
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "authorId": "49484314",
                        "name": "Y. Matsuo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "99540a2b2d5135766eb8f2e4197d1772369de9a7",
                "externalIds": {
                    "ArXiv": "2102.11023",
                    "DBLP": "journals/corr/abs-2102-11023",
                    "CorpusId": 231985825
                },
                "corpusId": 231985825,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/99540a2b2d5135766eb8f2e4197d1772369de9a7",
                "title": "Predicting Material Properties Using a 3D Graph Neural Network with Invariant Local Descriptors",
                "abstract": "Accurate prediction of physical properties is critical for discovering and designing novel materials. Machine learning technologies have attracted significant attention in the materials science community for their potential for large-scale screening. Graph Convolution Neural Network (GCNN) is one of the most successful machine learning methods because of its flexibility and effectiveness in describing 3D structural data. Most existing GCNN models focus on the topological structure but overly simplify the three-dimensional geometric structure. However, in materials science, the 3D-spatial distribution of atoms is crucial for determining the atomic states and interatomic forces. This paper proposes an adaptive GCNN with a novel convolution mechanism that simultaneously models atomic interactions among all neighbor atoms in three-dimensional space. We apply the proposed model to two distinctly challenging problems on predicting material properties. The first is Henry's constant for gas adsorption in Metal-Organic Frameworks (MOFs), which is notoriously difficult because of its high sensitivity to atomic configurations. The second is the ion conductivity in solid-state crystal materials, which is difficult because of few labeled data available for training. The new model outperforms existing graph-based models on both data sets, suggesting that the critical three-dimensional geometric information is indeed captured.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1390499062",
                        "name": "Boyu Zhang"
                    },
                    {
                        "authorId": "2051329266",
                        "name": "Mushen Zhou"
                    },
                    {
                        "authorId": "2157570954",
                        "name": "Jianzhong Wu"
                    },
                    {
                        "authorId": "40088807",
                        "name": "Fuchang Gao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Energy: The prior of Lagrangian/Hamiltonian dynamics conserve energy along each collision-free trajectory, which is one of the reason that Lagrangian/Hamiltonian-based neural network models perform better in prediction and generalization [7, 13, 14, 10]."
            ],
            "citingPaper": {
                "paperId": "7e1438c3b39e4a533cd2369fc181c65800979b5a",
                "externalIds": {
                    "ArXiv": "2102.06794",
                    "DBLP": "conf/nips/ZhongDC21",
                    "CorpusId": 235765903
                },
                "corpusId": 235765903,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7e1438c3b39e4a533cd2369fc181c65800979b5a",
                "title": "Extending Lagrangian and Hamiltonian Neural Networks with Differentiable Contact Models",
                "abstract": "The incorporation of appropriate inductive bias plays a critical role in learning dynamics from data. A growing body of work has been exploring ways to enforce energy conservation in the learned dynamics by encoding Lagrangian or Hamiltonian dynamics into the neural network architecture. These existing approaches are based on differential equations, which do not allow discontinuity in the states and thereby limit the class of systems one can learn. However, in reality, most physical systems, such as legged robots and robotic manipulators, involve contacts and collisions, which introduce discontinuities in the states. In this paper, we introduce a differentiable contact model, which can capture contact mechanics: frictionless/frictional, as well as elastic/inelastic. This model can also accommodate inequality constraints, such as limits on the joint angles. The proposed contact model extends the scope of Lagrangian and Hamiltonian neural networks by allowing simultaneous learning of contact and system properties. We demonstrate this framework on a series of challenging 2D and 3D physical systems with different coefficients of restitution and friction. The learned dynamics can be used as a differentiable physics simulator for downstream gradient-based optimization tasks, such as planning and control.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35367989",
                        "name": "Yaofeng Desmond Zhong"
                    },
                    {
                        "authorId": "32553531",
                        "name": "Biswadip Dey"
                    },
                    {
                        "authorId": "2019491887",
                        "name": "Amit Chakraborty"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "A large body of work (Cohen et al., 2020; 2018; Esteves et al., 2018; Greydanus et al., 2019; Romero et al., 2020; Finzi et al., 2020; Tai et al., 2019) proposes to hard-code equivariances in the neural architecture, which requires a priori knowledge of the transformations present in the data."
            ],
            "citingPaper": {
                "paperId": "84c39ca4b3790881f7511ad590682642f9f739df",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-05623",
                    "ArXiv": "2102.05623",
                    "CorpusId": 231861500
                },
                "corpusId": 231861500,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/84c39ca4b3790881f7511ad590682642f9f739df",
                "title": "Addressing the Topological Defects of Disentanglement via Distributed Operators",
                "abstract": "A core challenge in Machine Learning is to learn to disentangle natural factors of variation in data (e.g. object shape vs. pose). A popular approach to disentanglement consists in learning to map each of these factors to distinct subspaces of a model's latent representation. However, this approach has shown limited empirical success to date. Here, we show that, for a broad family of transformations acting on images--encompassing simple affine transformations such as rotations and translations--this approach to disentanglement introduces topological defects (i.e. discontinuities in the encoder). Motivated by classical results from group representation theory, we study an alternative, more flexible approach to disentanglement which relies on distributed latent operators, potentially acting on the entire latent space. We theoretically and empirically demonstrate the effectiveness of this approach to disentangle affine transformations. Our work lays a theoretical foundation for the recent success of a new generation of models using distributed operators for disentanglement.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3365029",
                        "name": "Diane Bouchacourt"
                    },
                    {
                        "authorId": "3407874",
                        "name": "Mark Ibrahim"
                    },
                    {
                        "authorId": "4789914",
                        "name": "St\u00e9phane Deny"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "NODE framework also allows encoding prior knowledge about the observed phenomena on the network topology, which leads to Hamiltonian and Lagrangian neural networks that are capable of long-term extrapolations, even when trained from images (Greydanus et al., 2019; Cranmer et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "2a51f798cc61947eefb0f754f409dbd78cb69cda",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-04764",
                    "ArXiv": "2102.04764",
                    "CorpusId": 231855323
                },
                "corpusId": 231855323,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2a51f798cc61947eefb0f754f409dbd78cb69cda",
                "title": "Continuous-Time Model-Based Reinforcement Learning",
                "abstract": "Model-based reinforcement learning (MBRL) approaches rely on discrete-time state transition models whereas physical systems and the vast majority of control tasks operate in continuous-time. To avoid time-discretization approximation of the underlying process, we propose a continuous-time MBRL framework based on a novel actor-critic method. Our approach also infers the unknown state evolution differentials with Bayesian neural ordinary differential equations (ODE) to account for epistemic uncertainty. We implement and test our method on a new ODE-RL suite that explicitly solves continuous-time control systems. Our experiments illustrate that the model is robust against irregular and noisy data, is sample-efficient, and can solve control problems which pose challenges to discrete-time MBRL methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9112269",
                        "name": "\u00c7agatay Yildiz"
                    },
                    {
                        "authorId": "34066178",
                        "name": "Markus Heinonen"
                    },
                    {
                        "authorId": "49121467",
                        "name": "H. L\u00e4hdesm\u00e4ki"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Other examples of dynamical systems inspired models include the learning of invariant quantities via their Hamiltonian or Lagrangian representations [37, 22, 10, 71, 58]."
            ],
            "citingPaper": {
                "paperId": "a747905cc60cde18a8c1fc1e365e319cea0ff49b",
                "externalIds": {
                    "ArXiv": "2102.04877",
                    "DBLP": "journals/corr/abs-2102-04877",
                    "CorpusId": 231855389
                },
                "corpusId": 231855389,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a747905cc60cde18a8c1fc1e365e319cea0ff49b",
                "title": "Noisy Recurrent Neural Networks",
                "abstract": "We provide a general framework for studying recurrent neural networks (RNNs) trained by injecting noise into hidden states. Specifically, we consider RNNs that can be viewed as discretizations of stochastic differential equations driven by input data. This framework allows us to study the implicit regularization effect of general noise injection schemes by deriving an approximate explicit regularizer in the small noise regime. We find that, under reasonable assumptions, this implicit regularization promotes flatter minima; it biases towards models with more stable dynamics; and, in classification tasks, it favors models with larger classification margin. Sufficient conditions for global stability are obtained, highlighting the phenomenon of stochastic stabilization, where noise injection can improve stability during training. Our theory is supported by empirical results which demonstrate that the RNNs have improved robustness with respect to various input perturbations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "98291940",
                        "name": "S. H. Lim"
                    },
                    {
                        "authorId": "2371914",
                        "name": "N. Benjamin Erichson"
                    },
                    {
                        "authorId": "90068212",
                        "name": "Liam Hodgkinson"
                    },
                    {
                        "authorId": "1717098",
                        "name": "Michael W. Mahoney"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "In contrast to our method the HNN requires additional derivative information, either analytical or as finite differences.",
                "The dynamics f\u0302 is again trained with independent sparse GPs, where the third\nStructure-preserving Gaussian Process Dynamics\n0 5 10 15 20 25 30 35 40 time t\n0\n1\n2\n3\n4\n5\nL 2 -e\nrr or\nSGPD Euler HNN\n0 5 10 15 20 25 30 time t\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\nL 2 -e\nrr or\nSGPD Euler HNN\n0 5 10 15 20 25 30 35 40 time t\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nL 2 -e\nrr or\nSGPD Euler HNN\nFigure 3: L2-errors of averaged state trajectories for all three methods.",
                "We compare with the following methods: Hamiltonian neural network (HNN) [11]: Deep learning approach that is tailored to respect Hamiltonian structure.",
                "SGPD 9 \u00b7 10\u22124 \u00b710\u22123 Euler 10\u22123 4 \u00b7 10\u22123 HNN 10\u22123 2 \u00b7 10\u22123",
                "Since the HNN is designed for non-constrained Hamiltonians it requires pairs of p and q and is, thus, not applicable.",
                "Inspired by [11], we average the approximated energy along 5 independent trajectories Hn = \u22115 i=1 H n 5 and compute the average total energy \u0124 = 1 n \u2211 nHn.",
                "This was addressed by applying a NN [11].",
                "Table 1: Shown are the total L2-errors in 1a and an analysis of the total energy for the non-separable system 1b.\n(a) total L2-errors (mean (std) over 5 indep. runs)\ntask SGPD Euler HNN (i) 0.421 (0.1) 0.459 (0.12) 4.69 (0.02) (ii) 0.056 (0.01) 0.057 (0.009) 0.12 (0.009) (iii) 0.033 (0.01) 0.034 (0.021) 0.035 (0.007) (iv) 0.046 (0.014) 0.073 (0.02) -\n(b) Energy for non-separable Hamiltonian\nmethod energy err. std. dev.",
                "We compare to the following state-of-theart approaches:\nHamiltonian neural network (HNN) [14]: Deep learning approach that is tailored to respect Hamiltonian structure.",
                "Separable Hamiltonians: the systems i) and ii) are both separable Hamiltonians that are also considered as baseline problems in [11]."
            ],
            "citingPaper": {
                "paperId": "51b54166813b0f1850e4d0ee9bb22a0fb66eb3bf",
                "externalIds": {
                    "DBLP": "conf/pkdd/EnsingerSZTT22",
                    "ArXiv": "2102.01606",
                    "DOI": "10.1007/978-3-031-26419-1_9",
                    "CorpusId": 245007197
                },
                "corpusId": 245007197,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/51b54166813b0f1850e4d0ee9bb22a0fb66eb3bf",
                "title": "Structure-Preserving Gaussian Process Dynamics",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2047583803",
                        "name": "K. Ensinger"
                    },
                    {
                        "authorId": "40897801",
                        "name": "Friedrich Solowjow"
                    },
                    {
                        "authorId": "3001733",
                        "name": "Sebastian Ziesche"
                    },
                    {
                        "authorId": "2144071766",
                        "name": "Michael Tiemann"
                    },
                    {
                        "authorId": "2715093",
                        "name": "Sebastian Trimpe"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4d3336d75c192e52591d4cd4c61b7aa73b33e32b",
                "externalIds": {
                    "DBLP": "journals/nature/RaayoniGMPHMHHK21",
                    "DOI": "10.1038/s41586-021-03229-4",
                    "CorpusId": 231805819,
                    "PubMed": "33536657"
                },
                "corpusId": 231805819,
                "publicationVenue": {
                    "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                    "name": "Nature",
                    "type": "journal",
                    "issn": "0028-0836",
                    "url": "https://www.nature.com/",
                    "alternate_urls": [
                        "http://www.nature.com/nature/",
                        "https://www.nature.com/nature/",
                        "http://www.nature.com/nature/archive/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4d3336d75c192e52591d4cd4c61b7aa73b33e32b",
                "title": "Generating conjectures on fundamental constants with the Ramanujan Machine",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1387998950",
                        "name": "Gal Raayoni"
                    },
                    {
                        "authorId": "2047860505",
                        "name": "Shahar Gottlieb"
                    },
                    {
                        "authorId": "153885479",
                        "name": "Yahel Manor"
                    },
                    {
                        "authorId": "1387998946",
                        "name": "George Pisha"
                    },
                    {
                        "authorId": "2167267826",
                        "name": "Yoav Harris"
                    },
                    {
                        "authorId": "2329488",
                        "name": "Uri Mendlovic"
                    },
                    {
                        "authorId": "49386124",
                        "name": "D. Haviv"
                    },
                    {
                        "authorId": "152474058",
                        "name": "Y. Hadad"
                    },
                    {
                        "authorId": "34806142",
                        "name": "I. Kaminer"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "f8353413b8cf5d9f5eafc03b30bae31a28a20297",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-08486",
                    "ArXiv": "2101.08486",
                    "CorpusId": 231662538
                },
                "corpusId": 231662538,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f8353413b8cf5d9f5eafc03b30bae31a28a20297",
                "title": "Differential Euler: Designing a Neural Network approximator to solve the Chaotic Three Body Problem",
                "abstract": "The three body problem is a special case of the n-body problem where one takes the initial positions and velocities of three point masses and attempts to predict their motion over time according to Newton\u2019s laws of motion and universal gravitation. Though analytical solutions have been found for special cases, the general problem remains unsolved (the solutions that do exist are impractical [1]). Fortunately, for many applications, we may not need to solve the problem completely [6] \u2014 i.e., predicting with reasonable accuracy for some time steps, may be sufficient. Recently, Breen et al [4] attempted to approximately solve the three body problem using (what appears to be) a simple neural network (NN). Although their methods appear to achieve some success in reducing the computational overhead, their model is extremely restricted, applying to a specialised two-dimensional case [13]. The authors do not provide explanations for critical decisions taken in their experimental design, no details on their model or architecture, and nor do they publish their code. Moreover, the model does not generalize well to unseen cases (where this new method might be an alternative to an expensive numerical integrator). In this paper, we propose a detailed experimental setup to determine the feasibility of using neural networks to solve the three body problem up to a certain number of time steps. We establish a benchmark on the dataset size and set an accuracy threshold to measure the viability of our results for practical applications. Then, we build our models according to the listed class of NNs using a dataset generated from standard numerical integrators. We gradually increase the complexity of our data set to determine whether NNs can learn a representation of the chaotic three body problem well enough to replace numerical integrators in real-life scenarios.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118920938",
                        "name": "Pratyush Kumar"
                    },
                    {
                        "authorId": "2113241135",
                        "name": "Aishwarya Das"
                    },
                    {
                        "authorId": "2409602",
                        "name": "Debayan Gupta"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5f15ca431987bf7af7541773ac4ddba287432f2a",
                "externalIds": {
                    "DOI": "10.1109/ICCECE51280.2021.9342165",
                    "CorpusId": 231850611
                },
                "corpusId": 231850611,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5f15ca431987bf7af7541773ac4ddba287432f2a",
                "title": "Self-supervised hamiltonian mechanics neural networks",
                "abstract": "We developed a method to derive the hamiltonian of an unknown system by machine learning the motions of the system. We modified the training process of Greydanus et al.\u2019s hamiltonian neural network to make it be capable of learning from a dataset without the change-of-state ground truth. In other word, the learning process is self-supervised. This improvement can extend the application of the hamiltonian neural network because it is sometimes difficult to accurately measure the change of state of the system. Our model can now be able to learn the free particle system and the harmonic oscillator system.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2048015824",
                        "name": "Youqiu Zhan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "This paradigm, including one of its core model classes, the Neural Ordinary Differential Equation (Neural ODE) [17], [18] has been successfully employed in combination to energy based models [19], [20], [21], [22], offering a grounded way to introduce first\u2013principles into neural networks."
            ],
            "citingPaper": {
                "paperId": "8dadcf997604056411875bd3efef61caf12f3eeb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-05537",
                    "ArXiv": "2101.05537",
                    "DOI": "10.1137/21m1414279",
                    "CorpusId": 231603395
                },
                "corpusId": 231603395,
                "publicationVenue": {
                    "id": "d35d93ad-8e2c-4482-a896-897ce5c326fa",
                    "name": "SIAM Journal on Applied Dynamical Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Siam Journal on Applied Dynamical Systems",
                        "Siam J Appl Dyn Syst",
                        "SIAM J Appl Dyn Syst"
                    ],
                    "issn": "1536-0040",
                    "url": "https://epubs.siam.org/journal/sjaday"
                },
                "url": "https://www.semanticscholar.org/paper/8dadcf997604056411875bd3efef61caf12f3eeb",
                "title": "Optimal Energy Shaping via Neural Approximators",
                "abstract": "We introduce optimal energy shaping as an enhancement of classical passivity-based control methods. A promising feature of passivity theory, alongside stability, has traditionally been claimed to be intuitive performance tuning along the execution of a given task. However, a systematic approach to adjust performance within a passive control framework has yet to be developed, as each method relies on few and problem-specific practical insights. Here, we cast the classic energy-shaping control design process in an optimal control framework; once a task-dependent performance metric is defined, an optimal solution is systematically obtained through an iterative procedure relying on neural networks and gradient-based optimization. The proposed method is validated on state-regulation tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "90467999",
                        "name": "Stefano Massaroli"
                    },
                    {
                        "authorId": "40585370",
                        "name": "Michael Poli"
                    },
                    {
                        "authorId": "21857982",
                        "name": "Federico Califano"
                    },
                    {
                        "authorId": "2085587",
                        "name": "Jinkyoo Park"
                    },
                    {
                        "authorId": "1748130",
                        "name": "A. Yamashita"
                    },
                    {
                        "authorId": "5312699",
                        "name": "H. Asama"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "More general formulations such as Hamiltonian (Greydanus et al., 2019; Toth et al., 2019) and Lagrangian",
                "More general formulations such as Hamiltonian (Greydanus et al., 2019; Toth et al., 2019) and Lagrangian\nDynamics (Lutter et al., 2019) are currently restricted in terms of the dimension of the dynamical system."
            ],
            "citingPaper": {
                "paperId": "7869928bc7c5809b05760367acc2eb5fd2a0e8c7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-05834",
                    "ArXiv": "2101.05834",
                    "CorpusId": 231627780
                },
                "corpusId": 231627780,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7869928bc7c5809b05760367acc2eb5fd2a0e8c7",
                "title": "Physics-aware, probabilistic model order reduction with guaranteed stability",
                "abstract": "Given (small amounts of) time-series' data from a high-dimensional, fine-grained, multiscale dynamical system, we propose a generative framework for learning an effective, lower-dimensional, coarse-grained dynamical model that is predictive of the fine-grained system's long-term evolution but also of its behavior under different initial conditions. We target fine-grained models as they arise in physical applications (e.g. molecular dynamics, agent-based models), the dynamics of which are strongly non-stationary but their transition to equilibrium is governed by unknown slow processes which are largely inaccessible by brute-force simulations. Approaches based on domain knowledge heavily rely on physical insight in identifying temporally slow features and fail to enforce the long-term stability of the learned dynamics. On the other hand, purely statistical frameworks lack interpretability and rely on large amounts of expensive simulation data (long and multiple trajectories) as they cannot infuse domain knowledge. The generative framework proposed achieves the aforementioned desiderata by employing a flexible prior on the complex plane for the latent, slow processes, and an intermediate layer of physics-motivated latent variables that reduces reliance on data and imbues inductive bias. In contrast to existing schemes, it does not require the a priori definition of projection operators from the fine-grained description and addresses simultaneously the tasks of dimensionality reduction and model estimation. We demonstrate its efficacy and accuracy in multiscale physical systems of particle dynamics where probabilistic, long-term predictions of phenomena not contained in the training data are produced.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "113831649",
                        "name": "Sebastian Kaltenbach"
                    },
                    {
                        "authorId": "2415520",
                        "name": "P. Koutsourelakis"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "This type of behavior presents a difficulty for machine learning and is impossible for methods that assume the pendulum to be a closed system, such as HNNs (Greydanus et al., 2019) (see Appendix B.",
                "Hamiltonian Neural Networks (HNNs) (Greydanus et al., 2019) and Symplectic Recurrent Neural Networks (Chen et al., 2019) make energy conserving predictions by using the Hamiltonian, a function that maps the inputs to the quantity that needs to be conserved.",
                "However, standard deep learning approaches struggle at conserving quantities across layers or timesteps (Beucler et al., 2019b; Greydanus et al., 2019; Song & Hopke, 1996; Yitian & Gu, 2003), and often solve a task by exploiting spurious correlations (Szegedy et al.",
                "To this end, we use the data generation process by (Greydanus et al., 2019).",
                "This type of behavior presents a difficulty for machine learning and is impossible for methods that assume the pendulum to be a closed system, such as HNNs (Greydanus et al., 2019) (see Appendix B.3.2).",
                "This type of behavior presents a difficulty for machine learning and is impossible for methods that assume the pendulum to be closed systems, such as HNNs (Greydanus et al., 2019).",
                "However, standard deep learning approaches struggle at conserving quantities across layers or timesteps (Beucler et al., 2019b; Greydanus et al., 2019; Song & Hopke, 1996; Yitian & Gu, 2003), and often solve a task by exploiting spurious correlations (Szegedy et al., 2014; Lapuschkin et al., 2019).",
                "Hamiltonian Neural Networks (HNNs) (Greydanus et al., 2019) and Symplectic Recurrent Neural Networks (Chen et al."
            ],
            "citingPaper": {
                "paperId": "30fcba6760b5fc5235ef8a0f2a61ccff63a7c3a0",
                "externalIds": {
                    "DBLP": "conf/icml/HoedtKKHHNHK21",
                    "ArXiv": "2101.05186",
                    "CorpusId": 231592444
                },
                "corpusId": 231592444,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/30fcba6760b5fc5235ef8a0f2a61ccff63a7c3a0",
                "title": "MC-LSTM: Mass-Conserving LSTM",
                "abstract": "The success of Convolutional Neural Networks (CNNs) in computer vision is mainly driven by their strong inductive bias, which is strong enough to allow CNNs to solve vision-related tasks with random weights, meaning without learning. Similarly, Long Short-Term Memory (LSTM) has a strong inductive bias towards storing information over time. However, many real-world systems are governed by conservation laws, which lead to the redistribution of particular quantities -- e.g. in physical and economical systems. Our novel Mass-Conserving LSTM (MC-LSTM) adheres to these conservation laws by extending the inductive bias of LSTM to model the redistribution of those stored quantities. MC-LSTMs set a new state-of-the-art for neural arithmetic units at learning arithmetic operations, such as addition tasks, which have a strong conservation law, as the sum is constant over time. Further, MC-LSTM is applied to traffic forecasting, modelling a pendulum, and a large benchmark dataset in hydrology, where it sets a new state-of-the-art for predicting peak flows. In the hydrology example, we show that MC-LSTM states correlate with real-world processes and are therefore interpretable.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1400417383",
                        "name": "Pieter-Jan Hoedt"
                    },
                    {
                        "authorId": "67249817",
                        "name": "Frederik Kratzert"
                    },
                    {
                        "authorId": "40270530",
                        "name": "D. Klotz"
                    },
                    {
                        "authorId": "2046836716",
                        "name": "Christina Halmich"
                    },
                    {
                        "authorId": "103040573",
                        "name": "Markus Holzleitner"
                    },
                    {
                        "authorId": "8451065",
                        "name": "G. Nearing"
                    },
                    {
                        "authorId": "3308557",
                        "name": "S. Hochreiter"
                    },
                    {
                        "authorId": "1994964",
                        "name": "G. Klambauer"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "0b3aceaeb2bd8e07085d9ce924ee91d9d1866d56",
                "externalIds": {
                    "MAG": "3118392103",
                    "DOI": "10.1029/2021JB022320",
                    "CorpusId": 231832828
                },
                "corpusId": 231832828,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0b3aceaeb2bd8e07085d9ce924ee91d9d1866d56",
                "title": "Bayesian Geophysical Inversion Using Invertible Neural Networks",
                "abstract": "Constraining geophysical models with observed data usually involves solving nonlinear and nonunique inverse problems. Neural mixture density networks (MDNs) provide an efficient way to estimate Bayesian posterior marginal probability density functions (pdf's) that represent the nonunique solution. However, it is difficult to infer correlations between parameters using MDNs, and in turn to draw samples from the posterior pdf. We introduce an alternative to resolve these issues: invertible neural networks (INNs). These are simultaneously trained to represent uncertain forward functions and to solve Bayesian inverse problems. In its usual form, the method does not account for uncertainty caused by data noise and becomes less effective in high dimensionality. To overcome these issues, in this study, we include data uncertainties as additional model parameters, and train the network by maximizing the likelihood of the data used for training. We apply the method to two types of imaging problems: One\u2010dimensional surface wave dispersion inversion and two\u2010dimensional travel time tomography, and we compare the results to those obtained using Monte Carlo and MDNs. Results show that INNs provide comparable posterior pdfs to those obtained using Monte Carlo, including correlations between parameters, and provide more accurate marginal distributions than MDNs. After training, INNs estimate posterior pdfs in seconds on a typical desktop computer. Hence they can be used to provide efficient solutions for repeated inverse problems using different data sets. Also even accounting for training time, our results show that INNs can be more efficient than Monte Carlo methods for solving inverse problems only once.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2149172220",
                        "name": "Xin Zhang"
                    },
                    {
                        "authorId": "145749164",
                        "name": "A. Curtis"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "d2ab37ecfe0cfd8fe1124eec66ad63ba0dff1fea",
                "externalIds": {
                    "MAG": "3138927827",
                    "DOI": "10.1029/2020GL092133",
                    "CorpusId": 236612840
                },
                "corpusId": 236612840,
                "publicationVenue": {
                    "id": "8d7867f3-aa72-437e-9ed9-c11efe95151b",
                    "name": "Geophysical Research Letters",
                    "type": "journal",
                    "alternate_names": [
                        "Geophys Res Lett"
                    ],
                    "issn": "0094-8276",
                    "url": "http://onlinelibrary.wiley.com/journal/10.1002/(ISSN)1944-8007",
                    "alternate_urls": [
                        "http://agupubs.onlinelibrary.wiley.com/agu/journal/10.1002/(ISSN)1944-8007/",
                        "http://onlinelibrary.wiley.com/journal/10.1002/(ISSN)1944-8007/issues",
                        "http://www.agu.org/journals/gl/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d2ab37ecfe0cfd8fe1124eec66ad63ba0dff1fea",
                "title": "A Toy Model to Investigate Stability of AI\u2010Based Dynamical Systems",
                "abstract": "The development of atmospheric parameterizations based on neural networks is often hampered by numerical instability issues. Previous attempts to replicate these issues in a toy model have proven ineffective. We introduce a new toy model for atmospheric dynamics, which consists in an extension of the Lorenz\u201963 model to a higher dimension. While feedforward neural networks trained on a single orbit can easily reproduce the dynamics of the Lorenz\u201963 model, they fail to reproduce the dynamics of the new toy model, leading to unstable trajectories. Instabilities become more frequent as the dimension of the new model increases, but are found to occur even in very low dimension. Training the feedforward neural network on a different learning sample, based on Latin Hypercube Sampling, solves the instability issue. Our results suggest that the design of the learning sample can significantly influence the stability of dynamical systems driven by neural networks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "10369986",
                        "name": "B. Balogh"
                    },
                    {
                        "authorId": "1403737779",
                        "name": "D. Saint\u2010Martin"
                    },
                    {
                        "authorId": "39228561",
                        "name": "A. Ribes"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In addition, deep learning tools are used to solve various practical problems related to PDE recently [5, 14, 26]."
            ],
            "citingPaper": {
                "paperId": "f7c8b50b9ffb7c51fd715c5f5aab9a4708b85074",
                "externalIds": {
                    "ArXiv": "2012.13870",
                    "DBLP": "journals/corr/abs-2012-13870",
                    "CorpusId": 229678038
                },
                "corpusId": 229678038,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f7c8b50b9ffb7c51fd715c5f5aab9a4708b85074",
                "title": "A Neural Network with Plane Wave Activation for Helmholtz Equation",
                "abstract": "This paper proposes a plane wave activation based neural network (PWNN) for solving Helmholtz equation, the basic partial differential equation to represent wave propagation, e.g. acoustic wave, electromagnetic wave, and seismic wave. Unlike using traditional activation based neural network (TANN) or $sin$ activation based neural network (SIREN) for solving general partial differential equations, we instead introduce a complex activation function $e^{\\mathbf{i}{x}}$, the plane wave which is the basic component of the solution of Helmholtz equation. By a simple derivation, we further find that PWNN is actually a generalization of the plane wave partition of unity method (PWPUM) by additionally imposing a learned basis with both amplitude and direction to better characterize the potential solution. We firstly investigate our performance on a problem with the solution is an integral of the plane waves with all known directions. The experiments demonstrate that: PWNN works much better than TANN and SIREN on varying architectures or the number of training samples, that means the plane wave activation indeed helps to enhance the representation ability of neural network toward the solution of Helmholtz equation; PWNN has competitive performance than PWPUM, e.g. the same convergence order but less relative error. Furthermore, we focus a more practical problem, the solution of which only integrate the plane waves with some unknown directions. We find that PWNN works much better than PWPUM at this case. Unlike using the plane wave basis with fixed directions in PWPUM, PWNN can learn a group of optimized plane wave basis which can better predict the unknown directions of the solution. The proposed approach may provide some new insights in the aspect of applying deep learning in Helmholtz equation.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2137317162",
                        "name": "Ziming Wang"
                    },
                    {
                        "authorId": "49446670",
                        "name": "Tao Cui"
                    },
                    {
                        "authorId": "2451845",
                        "name": "Xueshuang Xiang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ee9584090efc437d9cf14df5cf3935cf174689c2",
                "externalIds": {
                    "MAG": "3124909210",
                    "DOI": "10.26754/JJII3A.4863",
                    "CorpusId": 234415467
                },
                "corpusId": 234415467,
                "publicationVenue": {
                    "id": "6ce9c46a-d412-48bd-9a2b-767c12d41cbe",
                    "name": "Jornada de J\u00f3venes Investigadores del I3A",
                    "type": "journal",
                    "alternate_names": [
                        "Jorn J\u00f3venes Investig I3A"
                    ],
                    "issn": "2341-4790"
                },
                "url": "https://www.semanticscholar.org/paper/ee9584090efc437d9cf14df5cf3935cf174689c2",
                "title": "Hacia un cient\u00edfico digital",
                "abstract": "En este trabajo se pretende aplicar inteligencia artificial para predecir la evoluci\u00f3n temporal de un sistema f\u00edsico arbitrario s\u00f3lo mediante el uso de datos, es decir, sin conocer las ecuaciones que lo rigen. Este m\u00e9todo se ha validadado con dos sistemas din\u00e1micos disipativos: uno continuo y otro discreto.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2091667166",
                        "name": "Quercus Hern\u00e1ndez"
                    },
                    {
                        "authorId": "2091679012",
                        "name": "Alberto Bad\u00edas"
                    },
                    {
                        "authorId": "40444391",
                        "name": "David Gonz\u00e1lez"
                    },
                    {
                        "authorId": "2064857600",
                        "name": "Francisco Chinesta"
                    },
                    {
                        "authorId": "2065605582",
                        "name": "El\u00edas Cueto"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4ae59c8ec126f57decc00cfc8474cc9e3c875b7f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-10885",
                    "MAG": "3169316583",
                    "ArXiv": "2012.10885",
                    "CorpusId": 229340145
                },
                "corpusId": 229340145,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4ae59c8ec126f57decc00cfc8474cc9e3c875b7f",
                "title": "LieTransformer: Equivariant self-attention for Lie Groups",
                "abstract": "Group equivariant neural networks are used as building blocks of group invariant neural networks, which have been shown to improve generalisation performance and data efficiency through principled parameter sharing. Such works have mostly focused on group equivariant convolutions, building on the result that group equivariant linear maps are necessarily convolutions. In this work, we extend the scope of the literature to self-attention, that is emerging as a prominent building block of deep learning models. We propose the LieTransformer, an architecture composed of LieSelfAttention layers that are equivariant to arbitrary Lie groups and their discrete subgroups. We demonstrate the generality of our approach by showing experimental results that are competitive to baseline methods on a wide range of tasks: shape counting on point clouds, molecular property regression and modelling particle trajectories under Hamiltonian dynamics.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2031631712",
                        "name": "M. Hutchinson"
                    },
                    {
                        "authorId": "153892869",
                        "name": "Charline Le Lan"
                    },
                    {
                        "authorId": "1749331444",
                        "name": "Sheheryar Zaidi"
                    },
                    {
                        "authorId": "39200547",
                        "name": "Emilien Dupont"
                    },
                    {
                        "authorId": "1725303",
                        "name": "Y. Teh"
                    },
                    {
                        "authorId": "3407176",
                        "name": "Hyunjik Kim"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "2705d12ae06372f996ec76771c687bb08367efd0",
                "externalIds": {
                    "PubMedCentral": "8211802",
                    "MAG": "3112491006",
                    "DOI": "10.1038/s41598-021-92278-w",
                    "CorpusId": 229303379,
                    "PubMed": "34140609"
                },
                "corpusId": 229303379,
                "publicationVenue": {
                    "id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                    "name": "Scientific Reports",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Rep"
                    ],
                    "issn": "2045-2322",
                    "url": "http://www.nature.com/srep/",
                    "alternate_urls": [
                        "http://www.nature.com/srep/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2705d12ae06372f996ec76771c687bb08367efd0",
                "title": "Parsimonious neural networks learn interpretable physical laws",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "21765207",
                        "name": "Saaketh Desai"
                    },
                    {
                        "authorId": "145823562",
                        "name": "A. Strachan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "posed neural network is inspired by Hamiltonian mechanics [20, 35]."
            ],
            "citingPaper": {
                "paperId": "ea1185eab63fcdfb717f81c352b07eea48ab972a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-14334",
                    "ArXiv": "2012.14334",
                    "MAG": "3144695563",
                    "DOI": "10.1016/J.NEUCOM.2021.03.074",
                    "CorpusId": 229679765
                },
                "corpusId": 229679765,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ea1185eab63fcdfb717f81c352b07eea48ab972a",
                "title": "Time-Continuous Energy-Conservation Neural Network for Structural Dynamics Analysis",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2150671186",
                        "name": "Yuan Feng"
                    },
                    {
                        "authorId": "65981655",
                        "name": "Hexiang Wang"
                    },
                    {
                        "authorId": "2109713482",
                        "name": "Han Yang"
                    },
                    {
                        "authorId": "100605058",
                        "name": "Fangbo Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "isInfluential": true,
            "contexts": [
                "This directly contrasts with similar approaches [21], [6], [7] that only focus on learning a Hamiltonian but do not actually account for data generated by a conservative process.",
                "A wide variety of recent work has focused on the same problem where a physics-informed identification of dynamical systems is sought by prioritizing adherence to physical laws and/or phenomena such as stability [1], [2], [3], [4], conservation of energy [5], [6], [7], the principle of least action [8], [9], [10], [11], and symmetries in the dynamics [12], [10], [11].",
                "For example in [6], the authors learn a Hamiltonian system (denoted a Hamiltonian neural network) by minimizing the squared difference between the derivative of the parameterized Hamiltonian and the numerical derivatives of the states",
                "This approach is akin to the derivative reconstruction approaches used by, for example, Hamiltonian neural networks [6]."
            ],
            "citingPaper": {
                "paperId": "932314b95a12478582b276b967a880d63a7a099f",
                "externalIds": {
                    "DBLP": "conf/cdc/GaliotoG20",
                    "DOI": "10.1109/CDC42340.2020.9303852",
                    "CorpusId": 220524175
                },
                "corpusId": 220524175,
                "publicationVenue": {
                    "id": "ab066af1-bfee-42da-84bb-42f7e199d0d0",
                    "name": "IEEE Conference on Decision and Control",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Decision and Control",
                        "IEEE Conf Decis Control",
                        "Conf Decis Control",
                        "CDC"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=403"
                },
                "url": "https://www.semanticscholar.org/paper/932314b95a12478582b276b967a880d63a7a099f",
                "title": "Bayesian Identification of Hamiltonian Dynamics from Symplectic Data",
                "abstract": "We propose a Bayesian probabilistic formulation for system identification of Hamiltonian systems. This approach uses an approximate marginal Markov Chain Monte Carlo algorithm to directly discover a system Hamiltonian from data. Our approach improves upon existing methods in two ways: first we encode the fact that the data generating process is symplectic directly into our learning objective, and second we utilize a learning objective that simultaneously accounts for unknown parameters, model form, and measurement noise. This objective is the log marginal posterior of a probabilistic model that embeds a symplectic and reversible integrator within an uncertain dynamical system. We demonstrate that the resulting learning problem yields dynamical systems that have improved accuracy and reduced predictive uncertainty compared to existing state-of-the-art approaches. Simulation results are shown on the H\u00e9non-Heiles Hamiltonian system.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2093676645",
                        "name": "Nicholas Galioto"
                    },
                    {
                        "authorId": "2439226",
                        "name": "A. Gorodetsky"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Additional details on ARMs with linear transformed NNs is found in [22, 68, 23, 72, 67, 71] (cf.",
                "[67] 2019 DD NN \ue234 PyTorch (Python) Lutter et al."
            ],
            "citingPaper": {
                "paperId": "bf079ddc422ddbdfd0511baf5e48649389f422cc",
                "externalIds": {
                    "MAG": "3169805247",
                    "DBLP": "journals/corr/abs-2012-06250",
                    "ArXiv": "2012.06250",
                    "DOI": "10.1002/gamm.202100009",
                    "CorpusId": 228376142
                },
                "corpusId": 228376142,
                "publicationVenue": {
                    "id": "b388508d-7838-440c-a44d-bb2a1bfbe204",
                    "name": "GAMM-Mitteilungen",
                    "type": "journal",
                    "alternate_names": [
                        "Gamm-mitteilungen"
                    ],
                    "issn": "0936-7195",
                    "url": "https://onlinelibrary.wiley.com/journal/15222608"
                },
                "url": "https://www.semanticscholar.org/paper/bf079ddc422ddbdfd0511baf5e48649389f422cc",
                "title": "Structured learning of rigid\u2010body dynamics: A survey and unified view from a robotics perspective",
                "abstract": "Accurate models of mechanical system dynamics are often critical for model\u2010based control and reinforcement learning. Fully data\u2010driven dynamics models promise to ease the process of modeling and analysis, but require considerable amounts of data for training and often do not generalize well to unseen parts of the state space. Combining data\u2010driven modeling with prior analytical knowledge is an attractive alternative as the inclusion of structural knowledge into a regression model improves the model's data efficiency and physical integrity. In this article, we survey supervised regression models that combine rigid\u2010body mechanics with data\u2010driven modeling techniques. We analyze the different latent functions (such as kinetic energy or dissipative forces) and operators (such as differential operators and projection matrices) underlying common descriptions of rigid\u2010body mechanics. Based on this analysis, we provide a unified view on the combination of data\u2010driven regression models, such as neural networks and Gaussian processes, with analytical model priors. Furthermore, we review and discuss key techniques for designing structured models such as automatic differentiation.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2064785715",
                        "name": "A. R. Geist"
                    },
                    {
                        "authorId": "2715093",
                        "name": "Sebastian Trimpe"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "In [25] the models of HNNs and LNNs were generalized to Constrained Hamiltonian Neural Networks (CHNNs) and Constrained Lagrangian Neural Networks (CLNNs), enabling them to learn constrained mechanical systems written in Cartesian coordinates.",
                "Moreover, INN-based PNNs are able to learn multiple trajectories of a Poisson system on the whole data space simultaneously, while AE-HNNs, HGNs, CHNNs and CLNNs are only designed to work on low-dimensional submanifolds of Rn.",
                "For example, HNNs [15] use a neural network H\u0303 to approximate the Hamiltonian H in (1), then learn H\u0303 by reformulating the loss function.",
                "In other developments, autoencoder-based HNNs (AE-HNNs) [15] and Hamiltonian Generative Networks (HGNs) [19] were proposed to learn and predict the images of mechanical systems, which can be seen as Hamiltonian systems on manifolds embedded in high-dimensional spaces.",
                "Based on HNNs, other models were proposed to tackle problems in generative modeling [16], [19] and continuous control [20]."
            ],
            "citingPaper": {
                "paperId": "a521b4f034fc94a6ecbe6807c2183eed40320ca1",
                "externalIds": {
                    "ArXiv": "2012.03133",
                    "DBLP": "journals/corr/abs-2012-03133",
                    "MAG": "3112050848",
                    "DOI": "10.1109/TNNLS.2022.3148734",
                    "CorpusId": 227333962,
                    "PubMed": "35180089"
                },
                "corpusId": 227333962,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a521b4f034fc94a6ecbe6807c2183eed40320ca1",
                "title": "Learning Poisson systems and trajectories of autonomous systems via Poisson neural networks",
                "abstract": "We propose the Poisson neural networks (PNNs) to learn Poisson systems and trajectories of autonomous systems from data. Based on the Darboux-Lie theorem, the phase flow of a Poisson system can be written as the composition of: 1) a coordinate transformation; 2) an extended symplectic map; and 3) the inverse of the transformation. In this work, we extend this result to the unknotted trajectories of autonomous systems. We employ structured neural networks with physical priors to approximate the three aforementioned maps. We demonstrate through several simulations that PNNs are capable of handling very accurately several challenging tasks, including the motion of a particle in the electromagnetic potential, the nonlinear Schr\u00f6dinger equation, and pixel observations of the two-body problem.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "121687074",
                        "name": "Pengzhan Jin"
                    },
                    {
                        "authorId": "2116702527",
                        "name": "Zhen Zhang"
                    },
                    {
                        "authorId": "3439407",
                        "name": "I. Kevrekidis"
                    },
                    {
                        "authorId": "1720124",
                        "name": "G. Karniadakis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026shared network weights can be learned via forming a loss function consisting\n1We also note that there are other studies (e.g., (Cranmer et al. 2020; Greydanus, Dzamba, and Yosinski 2019)) using the idea of parameterizing the governing equations, where derivatives are also computed using\u2026"
            ],
            "citingPaper": {
                "paperId": "bba8eaa382df676e5b6bc68e5e634c3252cc8420",
                "externalIds": {
                    "DBLP": "conf/aaai/KimL0JP21",
                    "MAG": "3111557793",
                    "ArXiv": "2012.02681",
                    "DOI": "10.1609/aaai.v35i9.16992",
                    "CorpusId": 227306031
                },
                "corpusId": 227306031,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/bba8eaa382df676e5b6bc68e5e634c3252cc8420",
                "title": "DPM: A Novel Training Method for Physics-Informed Neural Networks in Extrapolation",
                "abstract": "We present a method for learning dynamics of complex physical processes described by time-dependent nonlinear partial differential equations (PDEs). Our particular interest lies in extrapolating solutions in time beyond the range of temporal domain used in training. Our choice for a baseline method is physics-informed neural network (PINN) because the method parameterizes not only the solutions, but also the equations that describe the dynamics of physical processes. We demonstrate that PINN performs poorly on extrapolation tasks in many benchmark problems. To address this, we propose a novel method for better training PINN and demonstrate that our newly enhanced PINNs can accurately extrapolate solutions in time. Our method shows up to 72% smaller errors than state-of-the-art methods in terms of the standard L2-norm metric.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2109169657",
                        "name": "Jungeun Kim"
                    },
                    {
                        "authorId": "3041967",
                        "name": "Kookjin Lee"
                    },
                    {
                        "authorId": "2148665454",
                        "name": "Dongeun Lee"
                    },
                    {
                        "authorId": "2031913711",
                        "name": "Sheo Yon Jin"
                    },
                    {
                        "authorId": "5166698",
                        "name": "Noseong Park"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Works such as [12, 18] propose networks constrained through physical properties such as Hamiltonian co-ordinates or Lagrangian Dynamics."
            ],
            "citingPaper": {
                "paperId": "5fa8b76256a2125c7a72db372b6e0d6be90d3a54",
                "externalIds": {
                    "MAG": "3104485462",
                    "ArXiv": "2012.02788",
                    "DBLP": "journals/corr/abs-2012-02788",
                    "CorpusId": 221093728
                },
                "corpusId": 221093728,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5fa8b76256a2125c7a72db372b6e0d6be90d3a54",
                "title": "Neural Dynamic Policies for End-to-End Sensorimotor Learning",
                "abstract": "The current dominant paradigm in sensorimotor control, whether imitation or reinforcement learning, is to train policies directly in raw action spaces such as torque, joint angle, or end-effector position. This forces the agent to make decisions individually at each timestep in training, and hence, limits the scalability to continuous, high-dimensional, and long-horizon tasks. In contrast, research in classical robotics has, for a long time, exploited dynamical systems as a policy representation to learn robot behaviors via demonstrations. These techniques, however, lack the flexibility and generalizability provided by deep learning or reinforcement learning and have remained under-explored in such settings. In this work, we begin to close this gap and embed the structure of a dynamical system into deep neural network-based policies by reparameterizing action spaces via second-order differential equations. We propose Neural Dynamic Policies (NDPs) that make predictions in trajectory distribution space as opposed to prior policy learning methods where actions represent the raw control space. The embedded structure allows end-to-end policy learning for both reinforcement and imitation learning setups. We show that NDPs outperform the prior state-of-the-art in terms of either efficiency or performance across several robotic control tasks for both imitation and reinforcement learning setups. Project video and code are available at this https URL",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "8527563",
                        "name": "Shikhar Bahl"
                    },
                    {
                        "authorId": "2874057",
                        "name": "Mustafa Mukadam"
                    },
                    {
                        "authorId": "1726095131",
                        "name": "A. Gupta"
                    },
                    {
                        "authorId": "2004879394",
                        "name": "Deepak Pathak"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In recent years, a growing number of neural network models have been proposed to solve the problem of learning dynamics, expressed via a set of differential equations, from data (Lutter et al., 2019b; Greydanus et al., 2019; Zhong et al., 2020a; Chen et al., 2020; Roehrl et al., 2020; Cranmer et al., 2020; Finzi et al., 2020).",
                "\u2026of neural network models have been proposed to solve the problem of learning dynamics, expressed via a set of differential equations, from data (Lutter et al., 2019b; Greydanus et al., 2019; Zhong et al., 2020a; Chen et al., 2020; Roehrl et al., 2020; Cranmer et al., 2020; Finzi et al., 2020).",
                "HNN: Greydanus et al. (2019) introduces Hamiltonian Neural Networks (HNN) to learn Hamiltonian dynamics (6) from data by parametrizing the Hamiltonian H using a neural network."
            ],
            "citingPaper": {
                "paperId": "616da79290f81e21e1834a84604c809f8cc0e768",
                "externalIds": {
                    "ArXiv": "2012.02334",
                    "DBLP": "journals/corr/abs-2012-02334",
                    "MAG": "3110785006",
                    "CorpusId": 227306084
                },
                "corpusId": 227306084,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/616da79290f81e21e1834a84604c809f8cc0e768",
                "title": "Benchmarking Energy-Conserving Neural Networks for Learning Dynamics from Data",
                "abstract": "The last few years have witnessed an increased interest in incorporating physics-informed inductive bias in deep learning frameworks. In particular, a growing volume of literature has been exploring ways to enforce energy conservation while using neural networks for learning dynamics from observed time-series data. In this work, we present a comparative analysis of the energy-conserving neural networks - for example, deep Lagrangian network, Hamiltonian neural network, etc. - wherein the underlying physics is encoded in their computation graph. We focus on ten neural network models and explain the similarities and differences between the models. We compare their performance in 4 different physical systems. Our result highlights that using a high-dimensional coordinate system and then imposing restrictions via explicit constraints can lead to higher accuracy in the learned dynamics. We also point out the possibility of leveraging some of these energy-conserving models to design energy-based controllers.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35367989",
                        "name": "Yaofeng Desmond Zhong"
                    },
                    {
                        "authorId": "32553531",
                        "name": "Biswadip Dey"
                    },
                    {
                        "authorId": "2019491887",
                        "name": "Amit Chakraborty"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "3878019d52d2f4f163d1749ed41ec72177c8e86d",
                "externalIds": {
                    "ArXiv": "2012.00698",
                    "MAG": "3108993200",
                    "DOI": "10.3934/CPAA.2021093",
                    "CorpusId": 227239368
                },
                "corpusId": 227239368,
                "publicationVenue": {
                    "id": "11467b5a-e3d0-4343-a18a-2e656c0b2760",
                    "name": "Communications on Pure and Applied Analysis",
                    "type": "journal",
                    "alternate_names": [
                        "Commun Pure Appl Anal"
                    ],
                    "issn": "1534-0392"
                },
                "url": "https://www.semanticscholar.org/paper/3878019d52d2f4f163d1749ed41ec72177c8e86d",
                "title": "Data-driven optimal control of a seir model for COVID-19",
                "abstract": "Since the first case of COVID-19 in December 2019, a total of $11,357,322$ confirmed cases in the US and $55,624,562$ confirmed cases worldwide have been reported, up to November 17, 2020, evoking fear locally and internationally. In particular, the coronavirus is surging nationwide at this time. In this paper, we investigate a basic Susceptible-Exposed-Infectious-Recovered (SEIR) model and calibrate the model parameters to the reported data. We also attempt to forecast the evolution of the outbreak over a relatively short time period and provide scheduled optimal controls of the COVID-19 epidemic. We provide efficient numerical algorithms based on a generalized Pontryagin Maximum Principle associated with the optimal control theory. Numerical experiments demonstrate the effective performance of the proposed model and its numerical approximations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2109527698",
                        "name": "Hailiang Liu"
                    },
                    {
                        "authorId": "2117127998",
                        "name": "Xuping Tian"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "such as Hamiltonian flows [7, 21] and Augmented Normalizing Flows (ANFs) [8]."
            ],
            "citingPaper": {
                "paperId": "537c9772e3e8e5cab843873a9cbaf761632ee439",
                "externalIds": {
                    "ArXiv": "2012.00429",
                    "MAG": "3106628198",
                    "CorpusId": 227239483
                },
                "corpusId": 227239483,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/537c9772e3e8e5cab843873a9cbaf761632ee439",
                "title": "Temperature-steerable flows",
                "abstract": "Boltzmann generators approach the sampling problem in many-body physics by combining a normalizing flow and a statistical reweighting method to generate samples of a physical system's equilibrium density. The equilibrium distribution is usually defined by an energy function and a thermodynamic state, such as a given temperature. Here we propose temperature-steerable flows (TSF) which are able to generate a family of probability densities parametrized by a choosable temperature parameter. TSFs can be embedded in a generalized ensemble sampling framework such as parallel tempering in order to sample a physical system across thermodynamic states, such as multiple temperatures.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "116366008",
                        "name": "Manuel Dibak"
                    },
                    {
                        "authorId": "1380082499",
                        "name": "Leon Klein"
                    },
                    {
                        "authorId": "2064855776",
                        "name": "Frank No'e"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ee9a9364e98bbaedffbe137adbaa4da8d9ee77b5",
                "externalIds": {
                    "MAG": "3109817511",
                    "DOI": "10.21203/rs.3.rs-94390/v1",
                    "CorpusId": 250536799
                },
                "corpusId": 250536799,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ee9a9364e98bbaedffbe137adbaa4da8d9ee77b5",
                "title": "Deep neural networks for high harmonic spectroscopy in solids",
                "abstract": "\n Neural networks are a prominent tool for identifying and modeling complex patterns, which are otherwise hard to detect and analyse. While machine learning and neural networks have \nbeen finding applications across many areas of science and technology, their use in decoding ultrafast \ndynamics of quantum systems driven by strong laser fields has been limited so far. Here we use deep neural networks to analyze spectra of highly nonlinear optical response \nof a crystal to intense few-cycle laser pulses. We construct a deep neural network that can efficiently utilize such spectra to resolve both the complex spectral phase of ultrashort laser pulses and simultaneously reconstruct the band structure of the crystal. Our results offer a new tool for attosecond spectroscopy of quantum dynamics and also open a route to developing all-solid-state devices for complete characterization of few-cycle pulses, including their nonlinear chirp and the carrier envelope phase.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "102617464",
                        "name": "N. Klimkin"
                    },
                    {
                        "authorId": "32106860",
                        "name": "M. Ivanov"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "A host of works have begun to view neural networks from a dynamical systems perspective leading to new regularizations [23, 24], architectures [25, 26, 27, 28, 29], analysis methods [30, 31, 20, 32], and stability guarantees for some architectures [33]."
            ],
            "citingPaper": {
                "paperId": "fcac3f8490f03266b6d8a69fe2e9d4a8eecb811d",
                "externalIds": {
                    "ArXiv": "2011.13492",
                    "DOI": "10.1109/ojcsys.2022.3186838",
                    "CorpusId": 249494446
                },
                "corpusId": 249494446,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fcac3f8490f03266b6d8a69fe2e9d4a8eecb811d",
                "title": "Dissipative Deep Neural Dynamical Systems",
                "abstract": "In this paper, we provide sufficient conditions for dissipativity and local asymptotic stability of discrete-time dynamical systems parametrized by deep neural networks. We leverage the representation of neural networks as pointwise affine maps, thus exposing their local linear operators and making them accessible to classical system analytic and design methods. This allows us to \u201ccrack open the black box\u201d of the neural dynamical system\u2019s behavior by evaluating their dissipativity, and estimating their stationary points and state-space partitioning. We relate the norms of these local linear operators to the energy stored in the dissipative system with supply rates represented by their aggregate bias terms. Empirically, we analyze the variance in dynamical behavior and eigenvalue spectra of these local linear operators with varying weight factorizations, activation functions, bias terms, and depths.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2403282",
                        "name": "J\u00e1n Drgo\u0148a"
                    },
                    {
                        "authorId": "145655596",
                        "name": "Aaron Tuor"
                    },
                    {
                        "authorId": "71573360",
                        "name": "Soumya Vasisht"
                    },
                    {
                        "authorId": "1885215",
                        "name": "D. Vrabie"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Most of them are done in a supervised way [1, 2, 3, 4]."
            ],
            "citingPaper": {
                "paperId": "d57a9e482b7b6fca0aea228e43d529281171a9b5",
                "externalIds": {
                    "ArXiv": "2011.11891",
                    "DBLP": "journals/corr/abs-2011-11891",
                    "MAG": "3108925605",
                    "CorpusId": 227151304
                },
                "corpusId": 227151304,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d57a9e482b7b6fca0aea228e43d529281171a9b5",
                "title": "Learning Principle of Least Action with Reinforcement Learning",
                "abstract": "Nature provides a way to understand physics with reinforcement learning since nature favors the economical way for an object to propagate. In the case of classical mechanics, nature favors the object to move along the path according to the integral of the Lagrangian, called the action $\\mathcal{S}$. We consider setting the reward/penalty as a function of $\\mathcal{S}$, so the agent could learn the physical trajectory of particles in various kinds of environments with reinforcement learning. In this work, we verified the idea by using a Q-Learning based algorithm on learning how light propagates in materials with different refraction indices, and show that the agent could recover the minimal-time path equivalent to the solution obtained by Snell's law or Fermat's Principle. We also discuss the similarity of our reinforcement learning approach to the path integral formalism.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2110880315",
                        "name": "Zehao Jin"
                    },
                    {
                        "authorId": "50993009",
                        "name": "J. Lin"
                    },
                    {
                        "authorId": "2118154707",
                        "name": "Siao-Fong Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "such as polynomial systems, Sums-of-Squares (SOS) method leads to a bilinear optimization that is solved by some form of alternation (Jarvis-Wloszek et al., 2003; Majumdar et al., 2013). As a dual to Lyapunov-based methods, Majumdar et al. (2014) uses the notion of occupation measure to optimize a feedback controller for a polynomial system, but it has scalability issues due to its reliance on SDP optimization toolbox."
            ],
            "citingPaper": {
                "paperId": "70187fa8938b86eb7f318d2f35dd9469114c914d",
                "externalIds": {
                    "DBLP": "conf/l4dc/MehrjouGS21",
                    "MAG": "3109681870",
                    "CorpusId": 227143118
                },
                "corpusId": 227143118,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/70187fa8938b86eb7f318d2f35dd9469114c914d",
                "title": "Neural Lyapunov Redesign",
                "abstract": "Learning controllers merely based on a performance metric has been proven effective in many physical and non-physical tasks in both control theory and reinforcement learning. However, in practice, the controller must guarantee some notion of safety to ensure that it does not harm either the agent or the environment. Stability is a crucial notion of safety, whose violation can certainly cause unsafe behaviors. Lyapunov functions are effective tools to assess stability in nonlinear dynamical systems. In this paper, we combine an improving Lyapunov function with automatic controller synthesis in an iterative fashion to obtain control policies with large safe regions. We propose a two-player collaborative algorithm that alternates between estimating a Lyapunov function and deriving a controller that gradually enlarges the stability region of the closed-loop system. We provide theoretical results on the class of systems that can be treated with the proposed algorithm and empirically evaluate the effectiveness of our method using an exemplary dynamical system.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2406046",
                        "name": "Arash Mehrjou"
                    },
                    {
                        "authorId": "1678622",
                        "name": "M. Ghavamzadeh"
                    },
                    {
                        "authorId": "1707625",
                        "name": "B. Sch\u00f6lkopf"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Greydanus et al. (2019); Zhong et al. (2020) presented a dynamics model that conformed to the Hamiltonian equation and demonstrated that their model conserves a quantity analogous to the total energy."
            ],
            "citingPaper": {
                "paperId": "979a3bb316337fada93645d2c8edd6e41443b58a",
                "externalIds": {
                    "DBLP": "conf/l4dc/AhnS21",
                    "ArXiv": "2011.10605",
                    "MAG": "3108681217",
                    "CorpusId": 227126924
                },
                "corpusId": 227126924,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/979a3bb316337fada93645d2c8edd6e41443b58a",
                "title": "Nested Mixture of Experts: Cooperative and Competitive Learning of Hybrid Dynamical System",
                "abstract": "Model-based reinforcement learning (MBRL) algorithms can attain significant sample efficiency but require an appropriate network structure to represent system dynamics. Current approaches include white-box modeling using analytic parameterizations and black-box modeling using deep neural networks. However, both can suffer from a bias-variance trade-off in the learning process, and neither provides a structured method for injecting domain knowledge into the network. As an alternative, gray-box modeling leverages prior knowledge in neural network training but only for simple systems. In this paper, we devise a nested mixture of experts (NMOE) for representing and learning hybrid dynamical systems. An NMOE combines both white-box and black-box models while optimizing bias-variance trade-off. Moreover, an NMOE provides a structured method for incorporating various types of prior knowledge by training the associative experts cooperatively or competitively. The prior knowledge includes information on robots' physical contacts with the environments as well as their kinematic and dynamic properties. In this paper, we demonstrate how to incorporate prior knowledge into our NMOE in various continuous control domains, including hybrid dynamical systems. We also show the effectiveness of our method in terms of data-efficiency, generalization to unseen data, and bias-variance trade-off. Finally, we evaluate our NMOE using an MBRL setup, where the model is integrated with a model-based controller and trained online.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "8813907",
                        "name": "Junhyeok Ahn"
                    },
                    {
                        "authorId": "143603756",
                        "name": "L. Sentis"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "The same technique has been used to analyze the training of neural networks in other contexts (Saxe et al., 2014; Greydanus et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "4d5c9a2a55b64bcf2cd01c5aa954776575c98500",
                "externalIds": {
                    "ArXiv": "2011.10036",
                    "DBLP": "journals/corr/abs-2011-10036",
                    "MAG": "3099471123",
                    "CorpusId": 227054213
                },
                "corpusId": 227054213,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4d5c9a2a55b64bcf2cd01c5aa954776575c98500",
                "title": "On the Dynamics of Training Attention Models",
                "abstract": "The attention mechanism has been widely used in deep neural networks as a model component. By now, it has become a critical building block in many state-of-the-art natural language models. Despite its great success established empirically, the working mechanism of attention has not been investigated at a sufficient theoretical depth to date. In this paper, we set up a simple text classification task and study the dynamics of training a simple attention-based classification model using gradient descent. In this setting, we show that, for the discriminative words that the model should attend to, a persisting identity exists relating its embedding and the inner product of its key and the query. This allows us to prove that training must converge to attending to the discriminative words when the attention output is classified by a linear classifier. Experiments are performed, which validates our theoretical analysis and provides further insights.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35434866",
                        "name": "Haoye Lu"
                    },
                    {
                        "authorId": "2047889",
                        "name": "Yongyi Mao"
                    },
                    {
                        "authorId": "144297190",
                        "name": "A. Nayak"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Prior work has investigated this kind of temporal extrapolation in recurrent networks, but solutions usually required baking in a conservation law of some sort [26, 27]."
            ],
            "citingPaper": {
                "paperId": "ac5ed37f35375ea423c8474a944f27d3a0ab8774",
                "externalIds": {
                    "ArXiv": "2011.08822",
                    "MAG": "3101298048",
                    "DBLP": "journals/corr/abs-2011-08822",
                    "CorpusId": 226975684
                },
                "corpusId": 226975684,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ac5ed37f35375ea423c8474a944f27d3a0ab8774",
                "title": "Learning Canonical Transformations",
                "abstract": "Humans understand a set of canonical geometric transformations (such as translation and rotation) that support generalization by being untethered to any specific object. We explore inductive biases that help a neural network model learn these transformations in pixel space in a way that can generalize out-of-domain. Specifically, we find that high training set diversity is sufficient for the extrapolation of translation to unseen shapes and scales, and that an iterative training scheme achieves significant extrapolation of rotation in time.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "12188966",
                        "name": "Zachary Dulberg"
                    },
                    {
                        "authorId": "153564781",
                        "name": "J. Cohen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "More generally, QDF can be viewed as one of the approaches such as the physics-informed, Hamiltonian, Fermionic, and Pauli neural networks [32, 33, 34, 35, 36, 37, 38]; these solve the physical problems and equations using physically meaningful modeling."
            ],
            "citingPaper": {
                "paperId": "e67f528c88de4e6dfc95d096f95108520c60355e",
                "externalIds": {
                    "ArXiv": "2011.07929",
                    "MAG": "3104978323",
                    "DBLP": "conf/nips/TsubakiM20",
                    "CorpusId": 226964621
                },
                "corpusId": 226964621,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e67f528c88de4e6dfc95d096f95108520c60355e",
                "title": "On the equivalence of molecular graph convolution and molecular wave function with poor basis set",
                "abstract": "In this study, we demonstrate that the linear combination of atomic orbitals (LCAO), an approximation of quantum physics introduced by Pauling and Lennard-Jones in the 1920s, corresponds to graph convolutional networks (GCNs) for molecules. However, GCNs involve unnecessary nonlinearity and deep architecture. We also verify that molecular GCNs are based on a poor basis function set compared with the standard one used in theoretical calculations or quantum chemical simulations. From these observations, we describe the quantum deep field (QDF), a machine learning (ML) model based on an underlying quantum physics, in particular the density functional theory (DFT). We believe that the QDF model can be easily understood because it can be regarded as a single linear layer GCN. Moreover, it uses two vanilla feedforward neural networks to learn an energy functional and a Hohenberg--Kohn map that have nonlinearities inherent in quantum physics and the DFT. For molecular energy prediction tasks, we demonstrated the viability of an ``extrapolation,'' in which we trained a QDF model with small molecules, tested it with large molecules, and achieved high extrapolation performance. This will lead to reliable and practical applications for discovering effective materials. The implementation is available at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "33150940",
                        "name": "Masashi Tsubaki"
                    },
                    {
                        "authorId": "9425225",
                        "name": "T. Mizoguchi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "rporating strong inductive biases inspired by physics, for instance, architectures respecting energy conservation laws. Examples of such architecture are linear operator constraints [52], Hamiltonian [53] or Lagrangian [54] neural networks. From an architecture perspective, the work we present here is inspired by a family of neural state-space models (SSM) [55, 56, 57, 58, 59], representing structural"
            ],
            "citingPaper": {
                "paperId": "f9bc65fcda3179b7f14edefb23916a5aaad20edf",
                "externalIds": {
                    "MAG": "3100539944",
                    "ArXiv": "2011.05987",
                    "DBLP": "journals/corr/abs-2011-05987",
                    "DOI": "10.1016/J.ENBUILD.2021.110992",
                    "CorpusId": 226306920
                },
                "corpusId": 226306920,
                "publicationVenue": {
                    "id": "255da382-a06e-458d-90ed-b6cd9bfb6053",
                    "name": "Energy and Buildings",
                    "type": "journal",
                    "alternate_names": [
                        "Energy Build"
                    ],
                    "issn": "0378-7788",
                    "url": "https://www.journals.elsevier.com/energy-and-buildings",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/03787788"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f9bc65fcda3179b7f14edefb23916a5aaad20edf",
                "title": "Physics-constrained Deep Learning of Multi-zone Building Thermal Dynamics",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2403282",
                        "name": "J\u00e1n Drgo\u0148a"
                    },
                    {
                        "authorId": "9583852",
                        "name": "Aaron Tuor"
                    },
                    {
                        "authorId": "38530907",
                        "name": "V. Chandan"
                    },
                    {
                        "authorId": "1885215",
                        "name": "D. Vrabie"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Other work linking conservation law and machine learning [9, 19\u201321] focus on embedding physical inductive biases into machine learning, but not the other way around to automate physical discoveries with machine leaning.",
                "[20] S."
            ],
            "citingPaper": {
                "paperId": "5f824383c7b906cf0fc87f1f9eb19fef74a4a4e9",
                "externalIds": {
                    "MAG": "3100050954",
                    "DBLP": "journals/corr/abs-2011-04698",
                    "ArXiv": "2011.04698",
                    "DOI": "10.1103/PhysRevLett.126.180604",
                    "CorpusId": 226289626,
                    "PubMed": "34018805"
                },
                "corpusId": 226289626,
                "publicationVenue": {
                    "id": "16c9f9d4-bee1-435d-8c85-22a3deba109d",
                    "name": "Physical Review Letters",
                    "type": "journal",
                    "alternate_names": [
                        "Phys Rev Lett"
                    ],
                    "issn": "0031-9007",
                    "url": "https://journals.aps.org/prl/",
                    "alternate_urls": [
                        "http://journals.aps.org/prl/",
                        "http://prl.aps.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5f824383c7b906cf0fc87f1f9eb19fef74a4a4e9",
                "title": "AI Poincar\u00e9: Machine Learning Conservation Laws from Trajectories",
                "abstract": "We present AI Poincar\u00e9, a machine learning algorithm for autodiscovering conserved quantities using trajectory data from unknown dynamical systems. We test it on five Hamiltonian systems, including the gravitational three-body problem, and find that it discovers not only all exactly conserved quantities, but also periodic orbits, phase transitions, and breakdown timescales for approximate conservation laws.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2145253202",
                        "name": "Ziming Liu"
                    },
                    {
                        "authorId": "2011933",
                        "name": "Max Tegmark"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "A Hamiltonian network achieves better conservation of an energy-like quantity without damping [24]."
            ],
            "citingPaper": {
                "paperId": "b8a60650658f0ec6f50bfe0d00baf4ce615d1c2d",
                "externalIds": {
                    "DBLP": "conf/iros/WangAB21",
                    "ArXiv": "2011.04929",
                    "DOI": "10.1109/IROS51168.2021.9636783",
                    "CorpusId": 236428526
                },
                "corpusId": 236428526,
                "publicationVenue": {
                    "id": "37275deb-3fcf-4d16-ae77-95db9899b1f3",
                    "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "IROS",
                        "Intelligent Robots and Systems",
                        "Intell Robot Syst",
                        "IEEE/RJS Int Conf Intell Robot Syst"
                    ],
                    "url": "http://www.iros.org/"
                },
                "url": "https://www.semanticscholar.org/paper/b8a60650658f0ec6f50bfe0d00baf4ce615d1c2d",
                "title": "Sim2Sim Evaluation of a Novel Data-Efficient Differentiable Physics Engine for Tensegrity Robots",
                "abstract": "Learning policies in simulation is promising for reducing human effort when training robot controllers. This is especially true for soft robots that are more adaptive and safe but also more difficult to accurately model and control. The sim2real gap is the main barrier to successfully transfer policies from simulation to a real robot. System identification can be applied to reduce this gap but traditional identification methods require a lot of manual tuning. Data-driven alternatives can tune dynamical models directly from data but are often data hungry, which also incorporates human effort in collecting data. This work proposes a data-driven, end-to-end differentiable simulator focused on the exciting but challenging domain of tensegrity robots. To the best of the authors\u2019 knowledge, this is the first differentiable physics engine for tensegrity robots that supports cable, contact, and actuation modeling. The aim is to develop a reasonably simplified, data-driven simulation, which can learn approximate dynamics with limited ground truth data. The dynamics must be accurate enough to generate policies that can be transferred back to the ground-truth system. As a first step in this direction, the current work demonstrates sim2sim transfer, where the unknown physical model of MuJoCo acts as a ground truth system. Two different tensegrity robots are used for evaluation and learning of locomotion policies, a 6-bar and a 3-bar tensegrity. The results indicate that only 0.25% of ground truth data are needed to train a policy that works on the ground truth system when the differentiable engine is used for training against training the policy directly on the ground truth system.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2152878792",
                        "name": "Kun Wang"
                    },
                    {
                        "authorId": "2297496",
                        "name": "Mridul Aanjaneya"
                    },
                    {
                        "authorId": "1739036",
                        "name": "Kostas E. Bekris"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "A Hamiltonian network achieves better conservation of an energy-like quantity without damping [24]."
            ],
            "citingPaper": {
                "paperId": "3d303735e68113faa9cc8947f1e421a2b7dcbd97",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-04929",
                    "MAG": "3098996264",
                    "CorpusId": 226290066
                },
                "corpusId": 226290066,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3d303735e68113faa9cc8947f1e421a2b7dcbd97",
                "title": "An End-to-End Differentiable but Explainable Physics Engine for Tensegrity Robots: Modeling and Control",
                "abstract": "This work proposes an end-to-end differentiable physics engine for tensegrity robots, which introduces a data-efficient linear contact model for accurately predicting collision responses that arise due to contacting surfaces, and a linear actuator model that can drive these robots by expanding and contracting their flexible cables. To the best of the authors' knowledge, this is the \\emph{first} differentiable physics engine for tensegrity robots that supports cable modeling, contact, and actuation. This engine can be used inside an off-the-shelf, RL-based locomotion controller in order to provide training examples. This paper proposes a progressive training pipeline for the differentiable physics engine that helps avoid local optima during the training phase and reduces data requirements. It demonstrates the data-efficiency benefits of using the differentiable engine for learning locomotion policies for NASA's icosahedron SUPERballBot. In particular, after the engine has been trained with few trajectories to match a ground truth simulated model, then a policy learned on the differentiable engine is shown to be transferable back to the ground-truth model. Training the controller requires orders of magnitude more data than training the differential engine.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2152878792",
                        "name": "Kun Wang"
                    },
                    {
                        "authorId": "2297496",
                        "name": "Mridul Aanjaneya"
                    },
                    {
                        "authorId": "1739036",
                        "name": "Kostas E. Bekris"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "This would allow us to use Hamiltonian neural network approaches to learn the gravitational potential [6]."
            ],
            "citingPaper": {
                "paperId": "85dd30a9960a7465b8783ab810bbcf40e0d63239",
                "externalIds": {
                    "ArXiv": "2205.02244",
                    "MAG": "3101144826",
                    "DOI": "10.3847/1538-4357/aca3a7",
                    "CorpusId": 226289989
                },
                "corpusId": 226289989,
                "publicationVenue": {
                    "id": "7652ece8-fbd3-4050-b927-cf29d7f0eaa8",
                    "name": "Astrophysical Journal",
                    "type": "journal",
                    "alternate_names": [
                        "Astrophys J",
                        "The Astrophysical Journal"
                    ],
                    "issn": "0004-637X",
                    "url": "https://iopscience.iop.org/journal/0004-637X",
                    "alternate_urls": [
                        "http://iopscience.org/apj"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/85dd30a9960a7465b8783ab810bbcf40e0d63239",
                "title": "Deep Potential: Recovering the Gravitational Potential from a Snapshot of Phase Space",
                "abstract": "One of the major goals of the field of Milky Way dynamics is to recover the gravitational potential field. Mapping the potential would allow us to determine the spatial distribution of matter\u2014both baryonic and dark\u2014throughout the galaxy. We present a novel method for determining the gravitational field from a snapshot of the phase-space positions of stars, based only on minimal physical assumptions, which makes use of recently developed tools from the field of deep learning. We first train a normalizing flow on a sample of observed six-dimensional phase-space coordinates of stars, obtaining a smooth, differentiable approximation of the distribution function. Using the collisionless Boltzmann equation, we then find the gravitational potential\u2014represented by a feed-forward neural network\u2014that renders this distribution function stationary. This method, which we term \u201cDeep Potential,\u201d is more flexible than previous parametric methods, which fit restricted classes of analytic models of the distribution function and potential to the data. We demonstrate Deep Potential on mock data sets and demonstrate its robustness under various nonideal conditions. Deep Potential is a promising approach to mapping the density of the Milky Way and other stellar systems, using rich data sets of stellar positions and kinematics now being provided by Gaia and ground-based spectroscopic surveys.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "47426507",
                        "name": "G. Green"
                    },
                    {
                        "authorId": "2079223725",
                        "name": "Y. Ting \u4e01"
                    },
                    {
                        "authorId": "93661003",
                        "name": "Harshil Kamdar"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "93742e37b100a8f53e8689e65c57b8a39fa5943c",
                "externalIds": {
                    "DBLP": "conf/icra/HeidenMCSS21",
                    "ArXiv": "2011.04217",
                    "MAG": "3098074920",
                    "DOI": "10.1109/ICRA48506.2021.9560935",
                    "CorpusId": 226281782
                },
                "corpusId": 226281782,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/93742e37b100a8f53e8689e65c57b8a39fa5943c",
                "title": "NeuralSim: Augmenting Differentiable Simulators with Neural Networks",
                "abstract": "Differentiable simulators provide an avenue for closing the sim-to-real gap by enabling the use of efficient, gradient-based optimization algorithms to find the simulation parameters that best fit the observed sensor readings. Nonetheless, these analytical models can only predict the dynamical behavior of systems for which they have been designed. In this work, we study the augmentation of a novel differentiable rigid-body physics engine via neural networks that is able to learn nonlinear relationships between dynamic quantities and can thus model effects not accounted for in traditional simulators. Such augmentations require less data to train and generalize better compared to entirely data-driven models. Through extensive experiments, we demonstrate the ability of our hybrid simulator to learn complex dynamics involving frictional contacts from real data, as well as match known models of viscous friction, and present an approach for automatically discovering useful augmentations. We show that, besides benefiting dynamics modeling, inserting neural networks can accelerate model-based control architectures. We observe a ten-fold speedup when replacing the QP solver inside a model-predictive gait controller for quadruped robots with a neural network, allowing us to significantly improve control delays as we demonstrate in real-hardware experiments. We publish code, additional results and videos from our experiments on our project webpage at https://sites.google.com/usc.edu/neuralsim.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "6014852",
                        "name": "Eric Heiden"
                    },
                    {
                        "authorId": "2064138376",
                        "name": "David Millard"
                    },
                    {
                        "authorId": "1716551",
                        "name": "Erwin Coumans"
                    },
                    {
                        "authorId": "2007772617",
                        "name": "Yizhou Sheng"
                    },
                    {
                        "authorId": "1732493",
                        "name": "G. Sukhatme"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "(Greydanus, Dzamba, and Yosinski 2019) and (Cranmer et al. 2020) directly encode symmetries in neural networks using respectively the Hamiltonian and Lagrangian framework."
            ],
            "citingPaper": {
                "paperId": "bfc4244690eb5b982efbf5230d0b46dc7088b67d",
                "externalIds": {
                    "ArXiv": "2011.04336",
                    "DBLP": "conf/aaaiss/BothVK21",
                    "MAG": "3100566689",
                    "CorpusId": 226281776
                },
                "corpusId": 226281776,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bfc4244690eb5b982efbf5230d0b46dc7088b67d",
                "title": "Sparsely Constrained Neural Networks for Model Discovery of PDEs",
                "abstract": "Sparse regression on a library of candidate features has developed as the prime method to discover the PDE underlying a spatio-temporal dataset. As these features consist of higher order derivatives, model discovery is typically limited to low-noise and dense datasets due to the erros inherent to numerical differentiation. Neural network-based approaches circumvent this limit, but to date have ignored advances in sparse regression algorithms. In this paper we present a modular framework that combines deep-learning based approaches with an arbitrary sparse regression technique. We demonstrate with several examples that this combination facilitates and enhances model discovery tasks. We release our framework as a package at this https URL",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35672556",
                        "name": "G. Both"
                    },
                    {
                        "authorId": "7464692",
                        "name": "R. Kusters"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "173ef889ce13d1691530248368c383152152190f",
                "externalIds": {
                    "ArXiv": "2011.03902",
                    "DBLP": "journals/corr/abs-2011-03902",
                    "MAG": "3102864525",
                    "CorpusId": 226282371
                },
                "corpusId": 226282371,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/173ef889ce13d1691530248368c383152152190f",
                "title": "Learning Neural Event Functions for Ordinary Differential Equations",
                "abstract": "The existing Neural ODE formulation relies on an explicit knowledge of the termination time. We extend Neural ODEs to implicitly defined termination criteria modeled by neural event functions, which can be chained together and differentiated through. Neural Event ODEs are capable of modeling discrete (instantaneous) changes in a continuous-time system, without prior knowledge of when these changes should occur or how many such changes should exist. We test our approach in modeling hybrid discrete- and continuous- systems such as switching dynamical systems and collision in multi-body systems, and we propose simulation-based training of point processes with applications in discrete control.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51466615",
                        "name": "Ricky T. Q. Chen"
                    },
                    {
                        "authorId": "1773498",
                        "name": "Brandon Amos"
                    },
                    {
                        "authorId": "1729762",
                        "name": "Maximilian Nickel"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "An alternative to penalty and barrier methods are neural network architectures imposing hard constraints, such as linear operator constraints [59], or architectures with Hamiltonian [60] and Lagrangian [61] structural priors for enforcing energy conservation laws."
            ],
            "citingPaper": {
                "paperId": "40215b4f0629a221da357a630402609cb8329c68",
                "externalIds": {
                    "ArXiv": "2011.03699",
                    "DOI": "10.1016/j.jprocont.2022.06.001",
                    "CorpusId": 236428787
                },
                "corpusId": 236428787,
                "publicationVenue": {
                    "id": "40d78211-eba2-4757-9427-598f366f3fa9",
                    "name": "Journal of Process Control",
                    "type": "journal",
                    "alternate_names": [
                        "J Process Control"
                    ],
                    "issn": "0959-1524",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/30445/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09591524"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/40215b4f0629a221da357a630402609cb8329c68",
                "title": "Differentiable predictive control: Deep learning alternative to explicit model predictive control for unknown nonlinear systems",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2403282",
                        "name": "J\u00e1n Drgo\u0148a"
                    },
                    {
                        "authorId": "116843454",
                        "name": "Karol Ki\u0161"
                    },
                    {
                        "authorId": "145655596",
                        "name": "Aaron Tuor"
                    },
                    {
                        "authorId": "1885215",
                        "name": "D. Vrabie"
                    },
                    {
                        "authorId": "1947333",
                        "name": "Martin Klauco"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "ods are neural network architectures imposing hard constraints, such as linear operator constraints [45], IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 3 or architectures with Hamiltonian [46] and Lagrangian [47] structural priors for introducing physics-based constraints such as energy conservation laws. II. PRELIMINARIES A. System Dynamics We assume unknown partially observable nonlinear"
            ],
            "citingPaper": {
                "paperId": "8333774ea18bb3f0de7360f9a7600cedaf6d6d07",
                "externalIds": {
                    "MAG": "3098187358",
                    "DBLP": "journals/corr/abs-2011-03699",
                    "CorpusId": 226282213
                },
                "corpusId": 226282213,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8333774ea18bb3f0de7360f9a7600cedaf6d6d07",
                "title": "Differentiable Predictive Control: An MPC Alternative for Unknown Nonlinear Systems using Constrained Deep Learning",
                "abstract": "We present an alternative to model predictive control (MPC) for unknown nonlinear systems in low-resource embedded device settings. The structure of the presented data-driven control policy learning method, Differentiable Predictive Control (DPC), echos the structure of classical MPC, by i) using a prediction model capturing controlled system dynamics, ii) receding horizon optimal control action predictions, and iii) enforcing inequality constraints via penalty methods. However, contrary to MPC, the presented control architecture does not require the system dynamics model to synthesize the control policy. Instead, a dynamics model is learned end-to-end from time-series measurements of the system dynamics in the off-policy setup. The control policy is then optimized via gradient descent by differentiating the closed-loop system dynamics model. The proposed architecture allows to train the control policy to track the distribution of reference signals and handle time-varying inequality constraints. We experimentally demonstrate that it is possible to train generalizing constrained optimal control policies purely based on the observations of the dynamics of the unknown nonlinear system. The proposed control method is applied to a laboratory device in embedded implementation using a Raspberry Pi micro-controller. We demonstrate superior reference tracking control performance compared to classical explicit MPC and a baseline PI controller, and pivotal efficiency gains in online computational demands, memory requirements, policy complexity, and construction. Beyond improved control performance, the DPC method scales linearly compared to exponential scalability of the explicit MPC solved via multiparametric programming, hence, opening doors for applications in nonlinear systems with a large number of variables and fast sampling rates which are beyond the reach of classical explicit MPC.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2403282",
                        "name": "J\u00e1n Drgo\u0148a"
                    },
                    {
                        "authorId": "116843454",
                        "name": "Karol Ki\u0161"
                    },
                    {
                        "authorId": "145655596",
                        "name": "Aaron Tuor"
                    },
                    {
                        "authorId": "1885215",
                        "name": "D. Vrabie"
                    },
                    {
                        "authorId": "1947333",
                        "name": "Martin Klauco"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "81f348ead58c15c4cc9b444d27869283c299ba85",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-01734",
                    "MAG": "3095435181",
                    "ArXiv": "2011.01734",
                    "DOI": "10.1109/ICRA48506.2021.9561805",
                    "CorpusId": 226237495
                },
                "corpusId": 226237495,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/81f348ead58c15c4cc9b444d27869283c299ba85",
                "title": "Differentiable Physics Models for Real-world Offline Model-based Reinforcement Learning",
                "abstract": "A limitation of model-based reinforcement learning (MBRL) is the exploitation of errors in the learned models. Blackbox models can fit complex dynamics with high fidelity, but their behavior is undefined outside of the data distribution. Physics-based models are better at extrapolating, due to the general validity of their informed structure, but underfit in the real world due to the presence of unmodeled phenomena. In this work, we demonstrate experimentally that for the offline model-based reinforcement learning setting, physics-based models can be beneficial compared to high-capacity function approximators if the mechanical structure is known. Physics-based models can learn to perform the ball in a cup (BiC) task on a physical manipulator using only 4 minutes of sampled data using offline MBRL. We find that black-box models consistently produce unviable policies for BiC as all predicted trajectories diverge to physically impossible state, despite having access to more data than the physics-based model. In addition, we generalize the approach of physics parameter identification from modeling holonomic multi-body systems to systems with nonholonomic dynamics using end-to-end automatic differentiation.Videos: https://sites.google.com/view/ball-in-a-cup-in-4-minutes/",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "49199170",
                        "name": "M. Lutter"
                    },
                    {
                        "authorId": "17160667",
                        "name": "Johannes Silberbauer"
                    },
                    {
                        "authorId": "31349388",
                        "name": "Joe Watson"
                    },
                    {
                        "authorId": "145197867",
                        "name": "Jan Peters"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[11] introduced Hamiltonian neural networks (HHN), they did apply HNN to noncanonical coordinates of the simple pendulum, but their loss function assumed the conjugate momentum equalled the velocity, which is not generally true.",
                "[11] in their \u201cHamiltonian neural networks\u201d introduced a new physics-aware neural network that outputs an energylike scalar and then optimises its gradient to respect theHamiltonianflow.",
                "[11] Introduces Hamiltonian neural network (HNN) [12] Improves to Hamiltonian generative network [13] Symplectic neural network [15] Symmetries neural network [14] Nonlinear pendulum application [16] HNN learns order-to-chaos transition [17] Quantifies HNN advantage with training [18] HNN for both libration and rotation"
            ],
            "citingPaper": {
                "paperId": "71079261ff66497aa75abb5b3e220c627c4d6dd5",
                "externalIds": {
                    "ArXiv": "2010.15201",
                    "MAG": "3097385874",
                    "DBLP": "journals/corr/abs-2010-15201",
                    "DOI": "10.1007/s11071-020-06185-2",
                    "CorpusId": 225103333
                },
                "corpusId": 225103333,
                "publicationVenue": {
                    "id": "10925c1c-0929-4ec5-8268-a8a52bd84631",
                    "name": "Nonlinear dynamics",
                    "type": "journal",
                    "alternate_names": [
                        "Nonlinear Dyn",
                        "Nonlinear Dynamics",
                        "Nonlinear dyn"
                    ],
                    "issn": "0924-090X",
                    "url": "http://www.springer.com/11071",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11071"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/71079261ff66497aa75abb5b3e220c627c4d6dd5",
                "title": "Forecasting Hamiltonian dynamics without canonical coordinates",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145638518",
                        "name": "A. Choudhary"
                    },
                    {
                        "authorId": "2468955",
                        "name": "J. Lindner"
                    },
                    {
                        "authorId": "1443437285",
                        "name": "Elliott G. Holliday"
                    },
                    {
                        "authorId": "50783104",
                        "name": "Scott T. Miller"
                    },
                    {
                        "authorId": "2053142",
                        "name": "S. Sinha"
                    },
                    {
                        "authorId": "3067672",
                        "name": "W. Ditto"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "(2) We show how to learn Hamiltonians and Lagrangians in Cartesian coordinates via explicit constraints using networks that we term Constrained Hamiltonian Neural Networks (CHNNs) and Constrained Lagrangian Neural Networks (CLNNs).",
                "We compare our Constrained Hamiltonian and Lagrangian Neural Networks (CHNNs, CLNNs) against NeuralODEs [1], Hamiltonian Neural Networks (HNNs) [9], and Deep Lagrangian Networks (DeLaNs) [14] on the systems described above.",
                "Zhong et al. [21] showed how to extend HNNs to dissapative systems, and Cranmer et al. [3] with LNNs showed how DeLaNs could be generalized to systems without quadratic kinetic energies.",
                "Previous work has considered relatively simple systems such as the 1 and 2-pendulum [3, 9], Cartpole, and Acrobot [2].",
                "[9] proposed Hamiltonian Neural Networks (HNNs) which parametrize H with a neural network.",
                "Greydanus et al. [9] proposed Hamiltonian Neural Networks (HNNs) which parametrize H with a neural network.",
                "On these systems, our explicitly-constrained CHNNs and CLNNs are 10 to 100 times more accurate than HNNs [9] and DeLaNs [14], which are implicitlyconstrained models.",
                "Recent work has shown that we can model physical systems by learning their Hamiltonians and Lagrangians from data [9, 14, 20]."
            ],
            "citingPaper": {
                "paperId": "b639d2c614219f3de1e6a21091a1ad8d443916e8",
                "externalIds": {
                    "MAG": "3093884544",
                    "DBLP": "conf/nips/FinziWW20",
                    "ArXiv": "2010.13581",
                    "CorpusId": 225067856
                },
                "corpusId": 225067856,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b639d2c614219f3de1e6a21091a1ad8d443916e8",
                "title": "Simplifying Hamiltonian and Lagrangian Neural Networks via Explicit Constraints",
                "abstract": "Reasoning about the physical world requires models that are endowed with the right inductive biases to learn the underlying dynamics. Recent works improve generalization for predicting trajectories by learning the Hamiltonian or Lagrangian of a system rather than the differential equations directly. While these methods encode the constraints of the systems using generalized coordinates, we show that embedding the system into Cartesian coordinates and enforcing the constraints explicitly with Lagrange multipliers dramatically simplifies the learning problem. We introduce a series of challenging chaotic and extended-body systems, including systems with N-pendulums, spring coupling, magnetic fields, rigid rotors, and gyroscopes, to push the limits of current approaches. Our experiments show that Cartesian coordinates with explicit constraints lead to a 100x improvement in accuracy and data efficiency.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51007156",
                        "name": "Marc Finzi"
                    },
                    {
                        "authorId": "2133843034",
                        "name": "Ke Alexander Wang"
                    },
                    {
                        "authorId": "145771261",
                        "name": "A. Wilson"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "d53fbc2bd3adba8fd9d0df1aa7d852e0e20d53ac",
                "externalIds": {
                    "ArXiv": "2010.12932",
                    "MAG": "3094142577",
                    "DBLP": "journals/corr/abs-2010-12932",
                    "CorpusId": 225070180
                },
                "corpusId": 225070180,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d53fbc2bd3adba8fd9d0df1aa7d852e0e20d53ac",
                "title": "LagNetViP: A Lagrangian Neural Network for Video Prediction",
                "abstract": "The dominant paradigms for video prediction rely on opaque transition models where neither the equations of motion nor the underlying physical quantities of the system are easily inferred. The equations of motion, as defined by Newton's second law, describe the time evolution of a physical system state and can therefore be applied toward the determination of future system states. In this paper, we introduce a video prediction model where the equations of motion are explicitly constructed from learned representations of the underlying physical quantities. To achieve this, we simultaneously learn a low-dimensional state representation and system Lagrangian. The kinetic and potential energy terms of the Lagrangian are distinctly modelled and the low-dimensional equations of motion are explicitly constructed using the Euler-Lagrange equations. We demonstrate the efficacy of this approach for video prediction on image sequences rendered in modified OpenAI gym Pendulum-v0 and Acrobot environments.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1405669277",
                        "name": "Christine Allen-Blanchette"
                    },
                    {
                        "authorId": "1491176908",
                        "name": "Sushant Veer"
                    },
                    {
                        "authorId": "1780468",
                        "name": "Anirudha Majumdar"
                    },
                    {
                        "authorId": "3301461",
                        "name": "Naomi Ehrich Leonard"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "in [14], the Hamiltonian conservation is encoded in the loss function.",
                "We show a motivational example in Figure 1 by comparing our approach with a traditional HNN method [14] regarding their structural designs and predicting abilities.",
                ", by enforcing incompressibility [24, 2], the Galilean invariance [22], quasistatic equilibrium [12], and the invariant quantities in Lagrangian systems [10, 23] and Hamiltonian systems [16, 14, 18, 37].",
                "[14] introduced Hamiltonian neural networks (HNNs) to conserve the Hamiltonian energy of the system by reformulating the loss function."
            ],
            "citingPaper": {
                "paperId": "1ab8bae9fa2df31c81aac4226e12094c616f8a22",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-12636",
                    "ArXiv": "2010.12636",
                    "MAG": "3094020114",
                    "CorpusId": 225068405
                },
                "corpusId": 225068405,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1ab8bae9fa2df31c81aac4226e12094c616f8a22",
                "title": "Nonseparable Symplectic Neural Networks",
                "abstract": "Predicting the behaviors of Hamiltonian systems has been drawing increasing attention in scientific machine learning. However, the vast majority of the literature was focused on predicting separable Hamiltonian systems with their kinematic and potential energy terms being explicitly decoupled, while building data-driven paradigms to predict nonseparable Hamiltonian systems that are ubiquitous in fluid dynamics and quantum mechanics were rarely explored. The main computational challenge lies in the effective embedding of symplectic priors to describe the inherently coupled evolution of position and momentum, which typically exhibits intricate dynamics with many degrees of freedom. To solve the problem, we propose a novel neural network architecture, Nonseparable Symplectic Neural Networks (NSSNNs), to uncover and embed the symplectic structure of a nonseparable Hamiltonian system from limited observation data. The enabling mechanics of our approach is an augmented symplectic time integrator to decouple the position and momentum energy terms and facilitate their evolution. We demonstrated the efficacy and versatility of our method by predicting a wide range of Hamiltonian systems, both separable and nonseparable, including vortical flow and quantum system. We showed the unique computational merits of our approach to yield long-term, accurate, and robust predictions for large-scale Hamiltonian systems by rigorously enforcing symplectomorphism.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "47722204",
                        "name": "S. Xiong"
                    },
                    {
                        "authorId": "40495066",
                        "name": "Yunjin Tong"
                    },
                    {
                        "authorId": "1491320619",
                        "name": "Xingzhe He"
                    },
                    {
                        "authorId": "3443627",
                        "name": "Cheng Yang"
                    },
                    {
                        "authorId": "14386295",
                        "name": "Shuqi Yang"
                    },
                    {
                        "authorId": "92035057",
                        "name": "Bo Zhu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "50887d7b6af2b1856fc6262ad5af7c05f59bbfb6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-10626",
                    "MAG": "3093585472",
                    "ArXiv": "2010.10626",
                    "DOI": "10.1016/j.cma.2021.113831",
                    "CorpusId": 224814103
                },
                "corpusId": 224814103,
                "publicationVenue": {
                    "id": "3bfaa538-a67d-47d7-bfda-6f82748e9a29",
                    "name": "Computer Methods in Applied Mechanics and Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Method Appl Mech Eng"
                    ],
                    "issn": "0045-7825",
                    "url": "https://www.journals.elsevier.com/computer-methods-in-applied-mechanics-and-engineering",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00457825"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/50887d7b6af2b1856fc6262ad5af7c05f59bbfb6",
                "title": "Data-driven Identification of 2D Partial Differential Equations using extracted physical features",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1999900316",
                        "name": "Kazem Meidani"
                    },
                    {
                        "authorId": "3614493",
                        "name": "A. Farimani"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Models that learn to respect conservative laws, through the use of Hamiltonian dynamics have been recently developed [26].",
                "3 [36, 8, 56, 7, 26] O(2)",
                "Without attempting to be exhaustive, recent domains of application include; the development of Hamiltonian Monte Carlo techniques [28], applications of symplectic integration to optimization [32], inference of symbolic models from data [29], and the development of Hamiltonian Neural Networks [26, 57]."
            ],
            "citingPaper": {
                "paperId": "fc674c6ea9087b654f9a37ec2396d05eceb49b73",
                "externalIds": {
                    "MAG": "3161839595",
                    "ArXiv": "2010.09785",
                    "DBLP": "journals/corr/abs-2010-09785",
                    "DOI": "10.3934/jcd.2021012",
                    "CorpusId": 224803624
                },
                "corpusId": 224803624,
                "publicationVenue": {
                    "id": "5f58c92f-0128-4372-963e-172d82dbf89a",
                    "name": "Journal of Computational Dynamics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Dyn"
                    ],
                    "issn": "2158-2491",
                    "url": "http://aimsciences.org/journal/2158-2491"
                },
                "url": "https://www.semanticscholar.org/paper/fc674c6ea9087b654f9a37ec2396d05eceb49b73",
                "title": "On Computational Poisson Geometry II: Numerical Methods",
                "abstract": "<p style='text-indent:20px;'>We present twelve numerical methods for evaluation of objects and concepts from Poisson geometry. We describe how each method works with examples, and explain how it is executed in code. These include methods that evaluate Hamiltonian and modular vector fields, compute the image under the coboundary and trace operators, the Lie bracket of differential 1\u2013forms, gauge transformations, and normal forms of Lie\u2013Poisson structures on <inline-formula><tex-math id=\"M1\">\\begin{document}$ {\\mathbf{R}^{{3}}} $\\end{document}</tex-math></inline-formula>. The complexity of each of our methods is calculated, and we include experimental verifications on examples in dimensions two and three.</p>",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1429802563",
                        "name": "M. Evangelista-Alvarado"
                    },
                    {
                        "authorId": "1412516577",
                        "name": "J. C. Ru'iz-Pantale'on"
                    },
                    {
                        "authorId": "2066256143",
                        "name": "P. Su'arez-Serrato"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "While alternative methods for modelling, such as Lagrangian and Hamiltonian Mechanics, may be appreciated for their mathematical elegance and convenience; the Newton Euler approach is based on global and interpretable physical quantities enabling robust out-of-sample generalization.",
                "To overcome this shortcoming, \u2018grey-box\u2019 models that combine deep networks with physical insights have been recently proposed, e.g., incorporating Lagrangian (Lutter et al., 2019; Lutter & Peters, 2019; Gupta et al., 2019) and Hamiltonian Mechanics (Greydanus et al., 2019) for energy-conserving models.",
                ", 2019) and Hamiltonian Mechanics (Greydanus et al., 2019) for energy-conserving models.",
                "\u2026this shortcoming, \u2018grey-box\u2019 models that combine deep networks with physical insights have been recently proposed, e.g., incorporating Lagrangian (Lutter et al., 2019; Lutter & Peters, 2019; Gupta et al., 2019) and Hamiltonian Mechanics (Greydanus et al., 2019) for energy-conserving models."
            ],
            "citingPaper": {
                "paperId": "05f8b75aca7a0e2a0ae6db23878e47abde8e682b",
                "externalIds": {
                    "ArXiv": "2010.09802",
                    "MAG": "3094631394",
                    "DBLP": "journals/corr/abs-2010-09802",
                    "CorpusId": 224803831
                },
                "corpusId": 224803831,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/05f8b75aca7a0e2a0ae6db23878e47abde8e682b",
                "title": "A Differentiable Newton Euler Algorithm for Multi-body Model Learning",
                "abstract": "In this work, we examine a spectrum of hybrid model for the domain of multi-body robot dynamics. We motivate a computation graph architecture that embodies the Newton Euler equations, emphasizing the utility of the Lie Algebra form in translating the dynamical geometry into an efficient computational structure for learning. We describe the used virtual parameters that enable unconstrained physical plausible dynamics and the used actuator models. In the experiments, we define a family of 26 grey-box models and evaluate them for system identification of the simulated and physical Furuta Pendulum and Cartpole. The comparison shows that the kinematic parameters, required by previous white-box system identification methods, can be accurately inferred from data. Furthermore, we highlight that models with guaranteed bounded energy of the uncontrolled system generate non-divergent trajectories, while more general models have no such guarantee, so their performance strongly depends on the data distribution. Therefore, the main contributions of this work is the introduction of a white-box model that jointly learns dynamic and kinematics parameters and can be combined with black-box components. We then provide extensive empirical evaluation on challenging systems and different datasets that elucidates the comparative performance of our grey-box architecture with comparable white- and black-box models.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "49199170",
                        "name": "M. Lutter"
                    },
                    {
                        "authorId": "17160667",
                        "name": "Johannes Silberbauer"
                    },
                    {
                        "authorId": "31349388",
                        "name": "Joe Watson"
                    },
                    {
                        "authorId": "145197867",
                        "name": "Jan Peters"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "There have been several recent articles that focus on dynamical systems, only subject to conservative forces, in the context of machine learning [1, 2, 3, 4, 5]."
            ],
            "citingPaper": {
                "paperId": "67daa96e83d87d4c7fa06ea7ba72a73612276560",
                "externalIds": {
                    "MAG": "3094464170",
                    "ArXiv": "2010.11270",
                    "DBLP": "journals/corr/abs-2010-11270",
                    "CorpusId": 225040607
                },
                "corpusId": 225040607,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/67daa96e83d87d4c7fa06ea7ba72a73612276560",
                "title": "Learning second order coupled differential equations that are subject to non-conservative forces",
                "abstract": "In this article we address the question whether it is possible to learn the differential equations describing the physical properties of a dynamical system, subject to non-conservative forces, from observations of its realspace trajectory(ies) only. We introduce a network that incorporates a difference approximation for the second order derivative in terms of residual connections between convolutional blocks, whose shared weights represent the coefficients of a second order ordinary differential equation. We further combine this solver-like architecture with a convolutional network, capable of learning the relation between trajectories of coupled oscillators and therefore allows us to make a stable forecast even if the system is only partially observed. We optimize this map together with the solver network, while sharing their weights, to form a powerful framework capable of learning the complex physical properties of a dissipative dynamical system.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2114053189",
                        "name": "R. M\u00fcller"
                    },
                    {
                        "authorId": "32575506",
                        "name": "J. L. Janssen"
                    },
                    {
                        "authorId": "51345025",
                        "name": "J. Camacaro"
                    },
                    {
                        "authorId": "51418404",
                        "name": "C. Bessega"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026design of novel architectures (Chang et al. 2017; Zhu, Chang, and Fu 2018; Demeester 2019; Chang et al. 2019; Cranmer et al. 2020; Massaroli et al. 2020a) as well as guiding the injection of physics\u2013inspired inductive biases (Greydanus, Dzamba, and Yosinski 2019; K\u00f6hler, Klein, and No\u00e9 2019)."
            ],
            "citingPaper": {
                "paperId": "6477203435e30444693012506dbb1a0cca7c8b28",
                "externalIds": {
                    "ArXiv": "2010.08304",
                    "DBLP": "journals/corr/abs-2010-08304",
                    "MAG": "3093456297",
                    "CorpusId": 223953495
                },
                "corpusId": 223953495,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6477203435e30444693012506dbb1a0cca7c8b28",
                "title": "Neural Ordinary Differential Equations for Intervention Modeling",
                "abstract": "By interpreting the forward dynamics of the latent representation of neural networks as an ordinary differential equation, Neural Ordinary Differential Equation (Neural ODE) emerged as an effective framework for modeling a system dynamics in the continuous time domain. However, real-world systems often involves external interventions that cause changes in the system dynamics such as a moving ball coming in contact with another ball, or such as a patient being administered with particular drug. Neural ODE and a number of its recent variants, however, are not suitable for modeling such interventions as they do not properly model the observations and the interventions separately. In this paper, we propose a novel neural ODE-based approach (IMODE) that properly model the effect of external interventions by employing two ODE functions to separately handle the observations and the interventions. Using both synthetic and real-world time-series datasets involving interventions, our experimental results consistently demonstrate the superiority of IMODE compared to existing approaches.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1998963567",
                        "name": "Daehoon Gwak"
                    },
                    {
                        "authorId": "1739457722",
                        "name": "Gyuhyeon Sim"
                    },
                    {
                        "authorId": "40585370",
                        "name": "Michael Poli"
                    },
                    {
                        "authorId": "90467999",
                        "name": "Stefano Massaroli"
                    },
                    {
                        "authorId": "1795455",
                        "name": "J. Choo"
                    },
                    {
                        "authorId": "3242613",
                        "name": "E. Choi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "This complexity can be seen in the original work on Hamiltonian Neural Networks (Greydanus, Dzamba, and Yosinski 2019) where the performance on the 2-body problem was good, but significantly deteriorated on the 3-body problem.",
                "Most of the existing work focuses on introducing better physical biases such as a relational model (Battaglia et al. 2016), conservation law bias (Greydanus, Dzamba, and Yosinski 2019), combining\nthese with an ODE bias (Sanchez-Gonzalez et al. 2019) and various similar refinements (Chen et al.\u2026",
                "Recent studies show that neural networks can successfully learn to simulate complex physical processes (Battaglia et al. 2016; Sanchez-Gonzalez et al. 2018; Mrowca et al. 2018; Li et al. 2018; Greydanus, Dzamba, and Yosinski 2019; Sanchez-Gonzalez et al. 2019, 2020).",
                "To address this, various datadriven methods for learning system dynamics have been investigated (Battaglia et al. 2016; Mrowca et al. 2018; Li et al. 2018; Greydanus, Dzamba, and Yosinski 2019; SanchezGonzalez et al. 2019, 2020; Finzi et al. 2020)."
            ],
            "citingPaper": {
                "paperId": "56f6708f22cbedcc1c600d78408bfbb46669d483",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-06948",
                    "MAG": "3093141853",
                    "ArXiv": "2010.06948",
                    "DOI": "10.1609/aaai.v35i10.17078",
                    "CorpusId": 222341680
                },
                "corpusId": 222341680,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/56f6708f22cbedcc1c600d78408bfbb46669d483",
                "title": "Scalable Graph Networks for Particle Simulations",
                "abstract": "Learning system dynamics directly from observations is a promising direction in machine learning due to its potential to significantly enhance our ability to understand physical systems. However, the dynamics of many real-world systems are challenging to learn due to the presence of nonlinear potentials and a number of interactions that scales quadratically with the number of particles N, as in the case of the N-body problem. In this work we introduce an approach that transforms a fully-connected interaction graph into a hierarchical one which reduces the number of edges to O(N). This results in a linear time and space complexity while the pre-computation of the hierarchical graph requires O(N log (N)) time and O(N) space. Using our approach, we are able to train models on much larger particle counts, even on a single GPU. We evaluate how the phase space position accuracy and energy conservation depend on the number of simulated particles. Our approach retains high accuracy and efficiency even on large-scale gravitational N-body simulations which are impossible to run on a single machine if a fully-connected graph is used. Similar results are also observed when simulating Coulomb interactions. Furthermore, we make several important observations regarding the performance of this new hierarchical model, including: i) its accuracy tends to improve with the number of particles in the simulation and ii) its generalisation to unseen particle counts is also much better than for models that use all O(N^2) interactions.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1995092493",
                        "name": "Karolis Martinkus"
                    },
                    {
                        "authorId": "40401747",
                        "name": "Aur\u00e9lien Lucchi"
                    },
                    {
                        "authorId": "2210804",
                        "name": "Nathanael Perraudin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "The Hamiltonian (Greydanus et al., 2019; Toth et al., 2020) is implemented by a MLP that takes the state Xt and outputs a scalar estimation of the Hamiltonian H of the system: the derivative is then computed by an in-graph gradient of H with respect to the input: F (Xt) = ( \u2202H \u2202(d\u03b8/ dt) ,\u2212 \u2202H d\u03b8 ) .",
                "For the pendulum, we compare to Hamiltonian neural networks (Greydanus et al., 2019; Toth et al., 2020) and to the the deep Galerkin method (DGM, Sirignano & Spiliopoulos, 2018).",
                "Our physical models are: \u2022 Hamiltonian (Greydanus et al., 2019), a conservative approximation, with Fp = {FH p : (u, v) 7\u2192 (\u2202yH(u, v),\u2212\u2202xH(u, v)) | H \u2208 H(1)(R(2))}, H(1)(R(2)) is the first order Sobolev space.",
                "Incomplete physics Hamiltonian (Greydanus et al., 2019) -1."
            ],
            "citingPaper": {
                "paperId": "757a3c0e2a43f3d15b91c52fae32b081a6a66e3a",
                "externalIds": {
                    "ArXiv": "2010.04456",
                    "DBLP": "journals/corr/abs-2010-04456",
                    "MAG": "3092352028",
                    "DOI": "10.1088/1742-5468/ac3ae5",
                    "CorpusId": 222272443
                },
                "corpusId": 222272443,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/757a3c0e2a43f3d15b91c52fae32b081a6a66e3a",
                "title": "Augmenting physical models with deep networks for complex dynamics forecasting",
                "abstract": "Forecasting complex dynamical phenomena in settings where only partial knowledge of their dynamics is available is a prevalent problem across various scientific fields. While purely data-driven approaches are arguably insufficient in this context, standard physical modeling-based approaches tend to be over-simplistic, inducing non-negligible errors. In this work, we introduce the APHYNITY framework, a principled approach for augmenting incomplete physical dynamics described by differential equations with deep data-driven models. It consists of decomposing the dynamics into two components: a physical component accounting for the dynamics for which we have some prior knowledge, and a data-driven component accounting for errors of the physical model. The learning problem is carefully formulated such that the physical model explains as much of the data as possible, while the data-driven component only describes information that cannot be captured by the physical model; no more, no less. This not only provides the existence and uniqueness for this decomposition, but also ensures interpretability and benefit generalization. Experiments made on three important use cases, each representative of a different family of phenomena, i.e. reaction\u2013diffusion equations, wave equations and the non-linear damped pendulum, show that APHYNITY can efficiently leverage approximate physical models to accurately forecast the evolution of the system and correctly identify relevant physical parameters. The code is available at https://github.com/yuan-yin/APHYNITY.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3965182",
                        "name": "V. Guen"
                    },
                    {
                        "authorId": "2109472874",
                        "name": "Yuan Yin"
                    },
                    {
                        "authorId": "1853488882",
                        "name": "J\u00e9r\u00e9mie Don\u00e0"
                    },
                    {
                        "authorId": "10771473",
                        "name": "Ibrahim Ayed"
                    },
                    {
                        "authorId": "2065044561",
                        "name": "Emmanuel de B'ezenac"
                    },
                    {
                        "authorId": "1728523",
                        "name": "Nicolas Thome"
                    },
                    {
                        "authorId": "1741426",
                        "name": "P. Gallinari"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "We also mention hybrid methods, which use a discretization of an ODE (in particular a Hamiltonian system) in order to learn the continuous representation of the data, see for instance [15, 9]."
            ],
            "citingPaper": {
                "paperId": "ae17836bbdb6e2d5abc78d088ecff14d28415ff2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-00951",
                    "MAG": "3091744265",
                    "ArXiv": "2010.00951",
                    "CorpusId": 222125116
                },
                "corpusId": 222125116,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ae17836bbdb6e2d5abc78d088ecff14d28415ff2",
                "title": "Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies",
                "abstract": "Circuits of biological neurons, such as in the functional parts of the brain can be modeled as networks of coupled oscillators. Inspired by the ability of these systems to express a rich set of outputs while keeping (gradients of) state variables bounded, we propose a novel architecture for recurrent neural networks. Our proposed RNN is based on a time-discretization of a system of second-order ordinary differential equations, modeling networks of controlled nonlinear oscillators. We prove precise bounds on the gradients of the hidden states, leading to the mitigation of the exploding and vanishing gradient problem for this RNN. Experiments show that the proposed RNN is comparable in performance to the state of the art on a variety of benchmarks, demonstrating the potential of this architecture to provide stable and accurate RNNs for processing complex sequential data.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "67164720",
                        "name": "T. Konstantin Rusch"
                    },
                    {
                        "authorId": "1767117",
                        "name": "Siddhartha Mishra"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", enforcing conservation laws in subdomains [40], hyperbolic conservation laws [57], Hamiltonian mechanics [25, 67], symplectic structures [11, 32], Lagrangian mechanics [14] and metriplectic structure [29]) and we believe that adapting/extending ideas of these approaches potentially mitigates the limitation of data-driven ROM approaches."
            ],
            "citingPaper": {
                "paperId": "812f9a18c9768d24536bdd784d08f82336a8f28a",
                "externalIds": {
                    "MAG": "3095118158",
                    "ArXiv": "2010.14685",
                    "DBLP": "journals/corr/abs-2010-14685",
                    "DOI": "10.1098/rspa.2021.0162",
                    "CorpusId": 225094147
                },
                "corpusId": 225094147,
                "publicationVenue": {
                    "id": "b61ce141-a434-431b-a154-68fc26e348f3",
                    "name": "Proceedings of the Royal Society A",
                    "type": "journal",
                    "alternate_names": [
                        "Proc R Soc A",
                        "Proc R Soc Math Phys Eng Sci",
                        "Proceedings of The Royal Society A: Mathematical, Physical and Engineering Sciences"
                    ],
                    "issn": "1364-5021",
                    "url": "https://www.jstor.org/journal/procmathphysengi",
                    "alternate_urls": [
                        "http://rspa.royalsocietypublishing.org/content/by/year",
                        "http://rspa.royalsocietypublishing.org/about",
                        "http://rspa.royalsocietypublishing.org/",
                        "https://royalsocietypublishing.org/journal/rspa"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/812f9a18c9768d24536bdd784d08f82336a8f28a",
                "title": "Parameterized neural ordinary differential equations: applications to computational physics problems",
                "abstract": "This work proposes an extension of neural ordinary differential equations (NODEs) by introducing an additional set of ODE input parameters to NODEs. This extension allows NODEs to learn multiple dynamics specified by the input parameter instances. Our extension is inspired by the concept of parameterized ODEs, which are widely investigated in computational science and engineering contexts, where characteristics of the governing equations vary over the input parameters. We apply the proposed parameterized NODEs (PNODEs) for learning latent dynamics of complex dynamical processes that arise in computational physics, which is an essential component for enabling rapid numerical simulations for time-critical physics applications. For this, we propose an encoder\u2013decoder-type framework, which models latent dynamics as PNODEs. We demonstrate the effectiveness of PNODEs on benchmark problems from computational physics.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3041967",
                        "name": "Kookjin Lee"
                    },
                    {
                        "authorId": "35221323",
                        "name": "E. Parish"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "db97b286dbd0c1f1e69bbcd8c92f9341a5d4037c",
                "externalIds": {
                    "MAG": "3034789343",
                    "DOI": "10.1016/j.physd.2020.132620",
                    "CorpusId": 224862665
                },
                "corpusId": 224862665,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/db97b286dbd0c1f1e69bbcd8c92f9341a5d4037c",
                "title": "An overview on recent machine learning techniques for Port Hamiltonian systems",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "67214498",
                        "name": "Karim Cherifi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "More works on learning Hamiltonian systems can be found in [11, 34, 43] and references cited therein."
            ],
            "citingPaper": {
                "paperId": "7ef234aa60401c0fb32506979d461e85c22d6199",
                "externalIds": {
                    "MAG": "3088508210",
                    "DBLP": "journals/corr/abs-2009-13415",
                    "CorpusId": 221971369
                },
                "corpusId": 221971369,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7ef234aa60401c0fb32506979d461e85c22d6199",
                "title": "Learning Interpretable and Thermodynamically Stable Partial Differential Equations",
                "abstract": "In this work, we develop a method for learning interpretable and thermodynamically stable partial differential equations (PDEs) based on the Conservation-dissipation Formalism of irreversible thermodynamics. As governing equations for non-equilibrium flows in one dimension, the learned PDEs are parameterized by fully-connected neural networks and satisfy the conservation-dissipation principle automatically. In particular, they are hyperbolic balance laws. The training data are generated from a kinetic model with smooth initial data. Numerical results indicate that the learned PDEs can achieve good accuracy in a wide range of Knudsen numbers. Remarkably, the learned dynamics can give satisfactory results with randomly sampled discontinuous initial data although it is trained only with smooth initial data.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2391774",
                        "name": "Juntao Huang"
                    },
                    {
                        "authorId": "2116417193",
                        "name": "Zhiting Ma"
                    },
                    {
                        "authorId": "49455479",
                        "name": "Y. Zhou"
                    },
                    {
                        "authorId": "34070717",
                        "name": "W. Yong"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "More works on learning Hamiltonian systems can be found in [13, 37, 46] and references cited therein."
            ],
            "citingPaper": {
                "paperId": "c2672d933faa17de2f2caf9942f386464709287f",
                "externalIds": {
                    "ArXiv": "2009.13415",
                    "MAG": "3109390220",
                    "DOI": "10.1515/jnet-2021-0008",
                    "CorpusId": 227225654
                },
                "corpusId": 227225654,
                "publicationVenue": {
                    "id": "bd314ae9-81ae-45be-9cf7-860255f1c0d0",
                    "name": "Journal of Non-Equilibrium Thermodynamics",
                    "type": "journal",
                    "alternate_names": [
                        "J Non-equilibrium Thermodyn"
                    ],
                    "issn": "0340-0204",
                    "alternate_issns": [
                        "1437-4358"
                    ],
                    "url": "https://www.degruyter.com/view/j/jnet",
                    "alternate_urls": [
                        "http://www.degruyter.com/view/j/jnet"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c2672d933faa17de2f2caf9942f386464709287f",
                "title": "Learning Thermodynamically Stable and Galilean Invariant Partial Differential Equations for Non-Equilibrium Flows",
                "abstract": "Abstract In this work, we develop a method for learning interpretable, thermodynamically stable and Galilean invariant partial differential equations (PDEs) based on the conservation-dissipation formalism of irreversible thermodynamics. As governing equations for non-equilibrium flows in one dimension, the learned PDEs are parameterized by fully connected neural networks and satisfy the conservation-dissipation principle automatically. In particular, they are hyperbolic balance laws and Galilean invariant. The training data are generated from a kinetic model with smooth initial data. Numerical results indicate that the learned PDEs can achieve good accuracy in a wide range of Knudsen numbers. Remarkably, the learned dynamics can give satisfactory results with randomly sampled discontinuous initial data and Sod\u2019s shock tube problem although it is trained only with smooth initial data.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2391774",
                        "name": "Juntao Huang"
                    },
                    {
                        "authorId": "2116417193",
                        "name": "Zhiting Ma"
                    },
                    {
                        "authorId": "49455479",
                        "name": "Y. Zhou"
                    },
                    {
                        "authorId": "34070717",
                        "name": "W. Yong"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "A popular way of encoding inductive biases is with clever network design to make predictions translation equivariant (CNNs), permutation equivariant (GNNs), or conserve energy [23].",
                "We showcase this in the real pendulum experiment used by Hamiltonian Neural Networks (HNNs) [23]."
            ],
            "citingPaper": {
                "paperId": "077f685947f977c946226d1b86c0b031f835486f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2009-10623",
                    "MAG": "3088170100",
                    "ArXiv": "2009.10623",
                    "CorpusId": 221836440
                },
                "corpusId": 221836440,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/077f685947f977c946226d1b86c0b031f835486f",
                "title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time",
                "abstract": "From CNNs to attention mechanisms, encoding inductive biases into neural networks has been a fruitful source of improvement in machine learning. Auxiliary losses are a general way of encoding biases in order to help networks learn better representations by adding extra terms to the loss function. However, since they are minimized on the training data, they suffer from the same generalization gap as regular task losses. Moreover, by changing the loss function, the network is optimizing a different objective than the one we care about. In this work we solve both problems: first, we take inspiration from \\textit{transductive learning} and note that, after receiving an input but before making a prediction, we can fine-tune our models on any unsupervised objective. We call this process tailoring, because we customize the model to each input. Second, we formulate a nested optimization (similar to those in meta-learning) and train our models to perform well on the task loss after adapting to the tailoring loss. The advantages of tailoring and meta-tailoring are discussed theoretically and demonstrated empirically on several diverse examples: encoding inductive conservation laws from physics to improve predictions, improving local smoothness to increase robustness to adversarial examples, and using contrastive losses on the query image to improve generalization.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "27587911",
                        "name": "Ferran Alet"
                    },
                    {
                        "authorId": "1392876047",
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "authorId": "1388700951",
                        "name": "Tomas Lozano-Perez"
                    },
                    {
                        "authorId": "1709512",
                        "name": "L. Kaelbling"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5cdf93a4a06b18eae113c506d6c1b7d20ce2f59d",
                "externalIds": {
                    "MAG": "3089156014",
                    "DOI": "10.1016/J.CSFX.2020.100046",
                    "CorpusId": 224949981
                },
                "corpusId": 224949981,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5cdf93a4a06b18eae113c506d6c1b7d20ce2f59d",
                "title": "The scaling of physics-informed machine learning with data and dimensions",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50783104",
                        "name": "Scott T. Miller"
                    },
                    {
                        "authorId": "2468955",
                        "name": "J. Lindner"
                    },
                    {
                        "authorId": "145638518",
                        "name": "A. Choudhary"
                    },
                    {
                        "authorId": "2053142",
                        "name": "S. Sinha"
                    },
                    {
                        "authorId": "3067672",
                        "name": "W. Ditto"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Through TorchDyn neural differential equations and derivative models, e.g (Greydanus et al., 2019; Toth et al., 2019; Massaroli et al., 2020b; Lutter et al., 2019; Cranmer et al., 2020; Massaroli et al., 2020a; Li et al., 2020), including yet\u2013to\u2013be\u2013published combinations, can effortlessly be\u2026",
                "\u2026(Massaroli et al., 2020b)\nGale\u0308rkin Neural ODEs (Massaroli et al., 2020b) Stacked Neural ODEs (Massaroli et al., 2020b)\nHamiltonian (Greydanus et al., 2019)\nLagrangian (Lutter et al., 2019; Cranmer et al., 2020)\nStable Neural Flows (Massaroli et al., 2020a)\nGraph Neural ODEs (Poli\u2026",
                "g (Greydanus et al., 2019; Toth et al., 2019; Massaroli et al., 2020b; Lutter et al., 2019; Cranmer et al., 2020; Massaroli et al., 2020a; Li et al., 2020), including yet\u2013to\u2013be\u2013published combinations, can effortlessly be obtained by ad hoc primitives in combination with the rich PyTorch (Paszke et al.",
                "Energy models There exists a whole line of work of physics\u2013inspired Neural ODE variants such as Hamiltonian Neural Networks (Greydanus et al., 2019), Lagrangian Neural Networks (Lutter et al.",
                "Some examples include a complete cookbook for neural ordinary differential equation variants, and tutorials for Hamiltonian Neural Networks (Greydanus et al., 2019), FFJORD (Grathwohl et al.",
                "Energy models There exists a whole line of work of physics\u2013inspired Neural ODE variants such as Hamiltonian Neural Networks (Greydanus et al., 2019), Lagrangian Neural Networks (Lutter et al., 2019; Cranmer et al., 2020) or general energy\u2013based models (Massaroli et al., 2020a).",
                "Some examples include a complete cookbook for neural ordinary differential equation variants, and tutorials for Hamiltonian Neural Networks (Greydanus et al., 2019), FFJORD (Grathwohl et al., 2018), Neural Graph Ordinary Differential Equations (Poli et al., 2019) and more.",
                ", 2020b) Hamiltonian (Greydanus et al., 2019) Lagrangian (Lutter et al."
            ],
            "citingPaper": {
                "paperId": "7fb63e2479bc76ae72bbf67289f212a4c16f886f",
                "externalIds": {
                    "ArXiv": "2009.09346",
                    "DBLP": "journals/corr/abs-2009-09346",
                    "MAG": "3087644732",
                    "CorpusId": 221818712
                },
                "corpusId": 221818712,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7fb63e2479bc76ae72bbf67289f212a4c16f886f",
                "title": "TorchDyn: A Neural Differential Equations Library",
                "abstract": "Continuous-depth learning has recently emerged as a novel perspective on deep learning, improving performance in tasks related to dynamical systems and density estimation. Core to these approaches is the neural differential equation, whose forward passes are the solutions of an initial value problem parametrized by a neural network. Unlocking the full potential of continuous-depth models requires a different set of software tools, due to peculiar differences compared to standard discrete neural networks, e.g inference must be carried out via numerical solvers. We introduce TorchDyn, a PyTorch library dedicated to continuous-depth learning, designed to elevate neural differential equations to be as accessible as regular plug-and-play deep learning primitives. This objective is achieved by identifying and subdividing different variants into common essential components, which can be combined and freely repurposed to obtain complex compositional architectures. TorchDyn further offers step-by-step tutorials and benchmarks designed to guide researchers and contributors.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "40585370",
                        "name": "Michael Poli"
                    },
                    {
                        "authorId": "90467999",
                        "name": "Stefano Massaroli"
                    },
                    {
                        "authorId": "1748130",
                        "name": "A. Yamashita"
                    },
                    {
                        "authorId": "5312699",
                        "name": "H. Asama"
                    },
                    {
                        "authorId": "2085587",
                        "name": "Jinkyoo Park"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "A different way of formulating rigid body dynamics has been investigated in [11] using energy conservation laws."
            ],
            "citingPaper": {
                "paperId": "70e896f999c6372cd92b32a97b7d821f20f19c95",
                "externalIds": {
                    "PubMedCentral": "7972712",
                    "ArXiv": "2009.08292",
                    "DBLP": "journals/corr/abs-2009-08292",
                    "MAG": "3085647390",
                    "DOI": "10.1007/978-3-030-71278-5_4",
                    "CorpusId": 221761331
                },
                "corpusId": 221761331,
                "publicationVenue": {
                    "id": "4bdc459e-68eb-4596-8e24-85dd8a047952",
                    "name": "German Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Ger Conf Pattern Recognit",
                        "GCPR"
                    ],
                    "url": "http://www.dagm.de/"
                },
                "url": "https://www.semanticscholar.org/paper/70e896f999c6372cd92b32a97b7d821f20f19c95",
                "title": "Learning to Identify Physical Parameters from Video Using Differentiable Physics",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1946865681",
                        "name": "Rama Krishna Kandukuri"
                    },
                    {
                        "authorId": "31039722",
                        "name": "Jan Achterhold"
                    },
                    {
                        "authorId": "143672328",
                        "name": "Michael M\u00f6ller"
                    },
                    {
                        "authorId": "1683956",
                        "name": "J. St\u00fcckler"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "58792475c912ca69de98c0e8a9e5d7f9f1eea355",
                "externalIds": {
                    "MAG": "3086666652",
                    "CorpusId": 263525979
                },
                "corpusId": 263525979,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/58792475c912ca69de98c0e8a9e5d7f9f1eea355",
                "title": "Symplectic Gaussian Process Regression of Hamiltonian Flow Maps",
                "abstract": "We present an approach to construct appropriate and efficient emulators for Hamiltonian flow maps. Intended future applications are long-term tracing of fast charged particles in accelerators and magnetic plasma confinement configurations. The method is based on multi-output Gaussian process regression on scattered training data. To obtain long-term stability the symplectic property is enforced via the choice of the matrix-valued covariance function. Based on earlier work on spline interpolation we observe derivatives of the generating function of a canonical transformation. A product kernel produces an accurate implicit method, whereas a sum kernel results in a fast explicit method from this approach. Both correspond to a symplectic Euler method in terms of numerical integration. These methods are applied to the pendulum and the Henon-Heiles system and results compared to an symmetric regression with orthogonal polynomials. In the limit of small mapping times, the Hamiltonian function can be identified with a part of the generating function and thereby learned from observed time-series data of the system's evolution. Besides comparable performance of implicit kernel and spectral regression for symplectic maps, we demonstrate a substantial increase in performance for learning the Hamiltonian function compared to existing approaches.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "21574289",
                        "name": "K. Rath"
                    },
                    {
                        "authorId": "2250978448",
                        "name": "Christopher G. Albert"
                    },
                    {
                        "authorId": "2252240267",
                        "name": "Bernd Bischl"
                    },
                    {
                        "authorId": "2425907",
                        "name": "U. Toussaint"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In this paper, we will investigate Hamiltonian neural networks (HNN) [6], [22], in which the unknown Hamiltonian function H instead of the total vector field f is parameterized.",
                "This statement was discussed in [22], while IMDE reveal this problem theoretically."
            ],
            "citingPaper": {
                "paperId": "39e44851686f9561f0af4e5b5af5bfae5342c3cf",
                "externalIds": {
                    "ArXiv": "2009.01058",
                    "DBLP": "journals/corr/abs-2009-01058",
                    "MAG": "3081818614",
                    "CorpusId": 221446365
                },
                "corpusId": 221446365,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/39e44851686f9561f0af4e5b5af5bfae5342c3cf",
                "title": "Inverse modified differential equations for discovery of dynamics",
                "abstract": "We introduce \\textbf{inverse modified differential equations} (IMDEs) to contribute to the fundamental theory of discovery of dynamics. In particular, we investigate the IMDEs for the neural ordinary differential equations (neural ODEs). Training such a learning model actually returns an approximation of an IMDE, rather than the original system. Thus, the convergence analysis for data-driven discovery is illuminated. The discrepancy of discovery depends on the order of the integrator used. Furthermore, IMDEs make clear the behavior of parameterizing some blocks in neural ODEs. We also perform several experiments to numerically substantiate our theoretical results.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2430811",
                        "name": "Aiqing Zhu"
                    },
                    {
                        "authorId": "121687074",
                        "name": "Pengzhan Jin"
                    },
                    {
                        "authorId": "1805110",
                        "name": "Yifa Tang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "8019fb16bbb35f60d5d9c4052b91739e24879be3",
                "externalIds": {
                    "MAG": "3080588469",
                    "PubMedCentral": "8966633",
                    "ArXiv": "2008.12249",
                    "DBLP": "journals/corr/abs-2008-12249",
                    "DOI": "10.1039/d1sc06946b",
                    "CorpusId": 221340823,
                    "PubMed": "35432900"
                },
                "corpusId": 221340823,
                "publicationVenue": {
                    "id": "2a0713ba-a4af-4a0a-ae49-81b8edeca660",
                    "name": "Chemical Science",
                    "type": "journal",
                    "alternate_names": [
                        "Chem Sci",
                        "Chem sci",
                        "Chemical science"
                    ],
                    "issn": "2041-6520",
                    "alternate_issns": [
                        "1478-6524"
                    ],
                    "url": "http://www.rsc.org/chemicalscience",
                    "alternate_urls": [
                        "http://xlink.rsc.org/?genre=journal&journal_code=SC",
                        "http://www.rsc.org/is/journals/current/chemscience/chemscience.htm",
                        "https://pubs.rsc.org/en/Content/ArticleLanding/2012/SC/c1sc00582k"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8019fb16bbb35f60d5d9c4052b91739e24879be3",
                "title": "PIGNet: a physics-informed deep learning model toward generalized drug\u2013target interaction predictions",
                "abstract": "Recently, deep neural network (DNN)-based drug\u2013target interaction (DTI) models were highlighted for their high accuracy with affordable computational costs. Yet, the models' insufficient generalization remains a challenging problem in the practice of in silico drug discovery. We propose two key strategies to enhance generalization in the DTI model. The first is to predict the atom\u2013atom pairwise interactions via physics-informed equations parameterized with neural networks and provides the total binding affinity of a protein\u2013ligand complex as their sum. We further improved the model generalization by augmenting a broader range of binding poses and ligands to training data. We validated our model, PIGNet, in the comparative assessment of scoring functions (CASF) 2016, demonstrating the outperforming docking and screening powers than previous methods. Our physics-informing strategy also enables the interpretation of predicted affinities by visualizing the contribution of ligand substructures, providing insights for further ligand optimization.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "134739554",
                        "name": "Seokhyun Moon"
                    },
                    {
                        "authorId": "1910509933",
                        "name": "Wonho Zhung"
                    },
                    {
                        "authorId": "2109001200",
                        "name": "Soojung Yang"
                    },
                    {
                        "authorId": "11662590",
                        "name": "Jaechang Lim"
                    },
                    {
                        "authorId": "3446579",
                        "name": "W. Kim"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "DO IDEAS HAVE SHAPE? 33 learn the laws of physics, and instead of crafting the Hamiltonian by hand, [39] proposes parameterizing it with a neural network and then learning it directly from data.",
                "[39] draws inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner: the purpose is to"
            ],
            "citingPaper": {
                "paperId": "471ca03559337321e6a7ea006c5fc0f8e9b0e24e",
                "externalIds": {
                    "ArXiv": "2008.03920",
                    "DOI": "10.1016/j.physd.2022.133592",
                    "CorpusId": 253224423
                },
                "corpusId": 253224423,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/471ca03559337321e6a7ea006c5fc0f8e9b0e24e",
                "title": "Do ideas have shape? Idea registration as the continuous limit of artificial neural networks",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1760230",
                        "name": "H. Owhadi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "6ccda521047aeca1da2d5899699173302cd49b77",
                "externalIds": {
                    "ArXiv": "2008.02491",
                    "DBLP": "journals/corr/abs-2008-02491",
                    "MAG": "3047477702",
                    "CorpusId": 221006044
                },
                "corpusId": 221006044,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6ccda521047aeca1da2d5899699173302cd49b77",
                "title": "Large-time asymptotics in deep learning",
                "abstract": "It is by now well-known that practical deep supervised learning may roughly be cast as an optimal control problem for a specific discrete-time, nonlinear dynamical system called an artificial neural network. In this work, we consider the continuous-time formulation of the deep supervised learning problem, and study the latter's behavior when the final time horizon increases, a fact that can be interpreted as increasing the number of layers in the neural network setting. When considering the classical regularized empirical risk minimization problem , we show that, in long time, the optimal states converge to zero training error, namely approach the zero training error regime, whilst the optimal control parameters approach, on an appropriate scale, minimal norm parameters with corresponding states precisely in the zero training error regime. This result provides an alternative theoretical underpinning to the notion that neural networks learn best in the overparametrized regime, when seen from the large layer perspective. We also propose a learning problem consisting of minimizing a cost with a state tracking term, and establish the well-known turnpike property, which indicates that the solutions of the learning problem in long time intervals consist of three pieces, the first and the last of which being transient short-time arcs, and the middle piece being a long-time arc staying exponentially close to the optimal solution of an associated static learning problem. This property in fact stipulates a quantitative estimate for the number of layers required to reach the zero training error regime. Both of the aforementioned asymptotic regimes are addressed in the context of continuous-time and continuous space-time neural networks, the latter taking the form of nonlinear, integro-differential equations, hence covering residual neural networks with both fixed and possibly variable depths.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "102956362",
                        "name": "Carlos Esteve"
                    },
                    {
                        "authorId": "1658854547",
                        "name": "Borjan Geshkovski"
                    },
                    {
                        "authorId": "47151248",
                        "name": "Dario Pighin"
                    },
                    {
                        "authorId": "2120765",
                        "name": "E. Zuazua"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Due to their popularity, there is a rich body of work that aims to explain \u201cwhy\u201d shortcut connections between layers enables the training of very deep NNs [73, 4, 29, 48, 51, 44].",
                "They enable the training of very deep neural networks (NNs), and they have emerged as the state-of-the-art architecture for a large number of tasks in computer vision, natural language processing, and related areas.",
                "Others have rediscovered integration schemes such as the implicit trapezoidal rule, which coincides with the scaled Cayley transform in the context of recurrent neural networks (RNNs) [33].",
                "Of course, specific architectures (i.e., specific designs of NNs for a given problem) often correspond to a family of graphs with some hyperparameters that generate a particular computational graph.",
                "\u2022 Is there a more appropriate interpretation for this class of NNs that stems from dynamical systems and numerical\nintegration theory?",
                "Before we develop the foundations for continuous-in-depth NNs (in Sec.",
                "Importantly, this approach\nenables us to decouple the computational graph from the model parameters, a property which does not hold for traditional discrete NNs but which is crucial for being a numerical integrator in a meaningful sense.",
                "These tools are also useful for analyzing ML and optimization algorithms [75, 70, 58, 59], as well as for improving our understanding of NNs [35, 55, 46, 45, 7, 50, 68].",
                "Along these lines, several physics-based models [66, 53, 63, 54, 17, 2] and continuous analogues to NNs [23, 14, 3] have recently been proposed.",
                "The idea of Neural ODEs is to use NNs to parameterize a differential equation that governs the hidden states x(t) \u2208 Rd with respect to time t,\nx\u0307 = F (x(t), t; \u03b8\u0302) = \u03c3(W\u0302 . . . \u03c3(W\u0302x+ A\u0302t+ b\u0302) \u00b7 \u00b7 \u00b7+ b\u0302), (4)\nwhere F : Rd \u2192 Rd denotes the network that is parameterized by the learnable parameters \u03b8\u0302 = {A\u0302, W\u0302 , b\u0302, . . . }, and where x\u0307 = dx/dt is the time derivative, and \u03c3 is any nonlinearity.",
                "ResNets and other traditional discrete NNs can be described by a discrete computational graph that represents the stages of computation and data flow during a forward evaluation.",
                "Continuous-in-depth NNs are independent of any particular choice of discrete computational graph or parameter representation, yielding the property of manifestation invariance.",
                "Other theoretical research fronts include interfacing Neural ODEs with normalizing flows [22, 79, 40, 56], graph NNs [61, 67], stochastic differential equations [52, 37, 24, 47] and RNNs [10, 65, 27, 18, 49]."
            ],
            "citingPaper": {
                "paperId": "f612193a890844c5ecb4605e6a87000957b6487c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2008-02389",
                    "MAG": "3046996896",
                    "ArXiv": "2008.02389",
                    "CorpusId": 221006097
                },
                "corpusId": 221006097,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f612193a890844c5ecb4605e6a87000957b6487c",
                "title": "Continuous-in-Depth Neural Networks",
                "abstract": "Recent work has attempted to interpret residual networks (ResNets) as one step of a forward Euler discretization of an ordinary differential equation, focusing mainly on syntactic algebraic similarities between the two systems. Discrete dynamical integrators of continuous dynamical systems, however, have a much richer structure. We first show that ResNets fail to be meaningful dynamical integrators in this richer sense. We then demonstrate that neural network models can learn to represent continuous dynamical systems, with this richer structure and properties, by embedding them into higher-order numerical integration schemes, such as the Runge Kutta schemes. Based on these insights, we introduce ContinuousNet as a continuous-in-depth generalization of ResNet architectures. ContinuousNets exhibit an invariance to the particular computational graph manifestation. That is, the continuous-in-depth model can be evaluated with different discrete time step sizes, which changes the number of layers, and different numerical integration schemes, which changes the graph connectivity. We show that this can be used to develop an incremental-in-depth training scheme that improves model quality, while significantly decreasing training time. We also show that, once trained, the number of units in the computational graph can even be decreased, for faster inference with little-to-no accuracy drop.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "40897456",
                        "name": "A. Queiruga"
                    },
                    {
                        "authorId": "2371914",
                        "name": "N. Benjamin Erichson"
                    },
                    {
                        "authorId": "2152707745",
                        "name": "D. Taylor"
                    },
                    {
                        "authorId": "1717098",
                        "name": "Michael W. Mahoney"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "The interest of the machine learning community in physical phenomena has substantially grown for the last few years (Shi et al., 2015; Long et al., 2018; Greydanus et al., 2019).",
                "Greydanus et al. (2019), Chen et al. (2020) and Toth et al. (2020) introduce non-regression losses by taking advantage of Hamiltonian mechanics (Hamilton, 1835), while Tompson et al. (2017) and Raissi et al. (2020) combine physically inspired constraints and structural priors for fluid dynamic\u2026"
            ],
            "citingPaper": {
                "paperId": "9130668c2927ee5989854e09ad565a2dd1bd6391",
                "externalIds": {
                    "MAG": "3047211079",
                    "DBLP": "journals/corr/abs-2008-01352",
                    "ArXiv": "2008.01352",
                    "CorpusId": 220961494
                },
                "corpusId": 220961494,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9130668c2927ee5989854e09ad565a2dd1bd6391",
                "title": "PDE-Driven Spatiotemporal Disentanglement",
                "abstract": "A recent line of work in the machine learning community addresses the problem of predicting high-dimensional spatiotemporal phenomena by leveraging specific tools from the differential equations theory. Following this direction, we propose in this article a novel and general paradigm for this task based on a resolution method for partial differential equations: the separation of variables. This inspiration allows us to introduce a dynamical interpretation of spatiotemporal disentanglement. It induces a principled model based on learning disentangled spatial and temporal representations of a phenomenon to accurately predict future observations. We experimentally demonstrate the performance and broad applicability of our method against prior state-of-the-art models on physical and synthetic video datasets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1853488882",
                        "name": "J\u00e9r\u00e9mie Don\u00e0"
                    },
                    {
                        "authorId": "35622441",
                        "name": "Jean-Yves Franceschi"
                    },
                    {
                        "authorId": "1782552",
                        "name": "S. Lamprier"
                    },
                    {
                        "authorId": "1741426",
                        "name": "P. Gallinari"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Neural ODE and its applications [6, 5, 15, 42, 32, 7, 40], alias ODE networks (ODENs), tackle these issues by learning the governing equations, rather than the state transitions directly.",
                "Furthermore, they can lean the underlying law of conservation of energy automatically, because they fully exploit the nature of the Hamiltonian mechanics [15].",
                "Recent works [15, 42, 32, 7, 40] apply the Hamiltonian mechanics to ODE networks, and succeed in enforcing the energy conservation as well as the accurate time evolution of classical conservative systems.",
                "Moreover, some of them use special ODE functions such as Hamilton\u2019s equations to incorporate physical properties to neural network structurally [15, 42, 32, 7, 40].",
                ", only for suitable explicitly conservative systems [15].",
                "However, these Hamiltonian ODE networks have inherent limitations that they cannot be applied to non-conservative systems, since the Hamiltonian structures require to strictly conserve the total energy [15].",
                ", evolutionary algorithms [35, 28], sparse optimizations [33, 4], Gaussian process regressions [38, 8], and neural networks [18, 1, 15, 42, 32].",
                "They are shown to be able to represent the vast majority of dynamical systems with higher precision over vanilla recurrent neural networks and their variants [6, 5], but are still unable to learn underlying physics such as the law of conservation [15].",
                "Consequently, they often overfit to short-term training trajectories and fail to predict the long-term behaviors of complex dynamical systems [15, 42].",
                "It is also not straightforward to predict the continuous-time dynamics, because neural network models typically assume the discrete time-step between states [15]."
            ],
            "citingPaper": {
                "paperId": "b2c078e7416de65bbc431c24762edcf33ac9f59b",
                "externalIds": {
                    "MAG": "3044111002",
                    "DBLP": "journals/corr/abs-2007-11362",
                    "ArXiv": "2007.11362",
                    "CorpusId": 220686334
                },
                "corpusId": 220686334,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b2c078e7416de65bbc431c24762edcf33ac9f59b",
                "title": "Time-Reversal Symmetric ODE Network",
                "abstract": "Time-reversal symmetry, which requires that the dynamics of a system should not change with the reversal of time axis, is a fundamental property that frequently holds in classical and quantum mechanics. In this paper, we propose a novel loss function that measures how well our ordinary differential equation (ODE) networks comply with this time-reversal symmetry; it is formally defined by the discrepancy in the time evolutions of ODE networks between forward and backward dynamics. Then, we design a new framework, which we name as Time-Reversal Symmetric ODE Networks (TRS-ODENs), that can learn the dynamics of physical systems more sample-efficiently by learning with the proposed loss function. We evaluate TRS-ODENs on several classical dynamics, and find they can learn the desired time evolution from observed noisy and complex trajectories. We also show that, even for systems that do not possess the full time-reversal symmetry, TRS-ODENs can achieve better predictive performances over baselines.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2064530354",
                        "name": "In Huh"
                    },
                    {
                        "authorId": "1720494",
                        "name": "Eunho Yang"
                    },
                    {
                        "authorId": "35788904",
                        "name": "Sung Ju Hwang"
                    },
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2014), offering new system\u2013theoretic perspectives on neural network architecture design (Greydanus et al., 2019; Bai et al., 2019; Poli et al., 2019; Cranmer et al., 2020) and generative modeling (Grathwohl et al.",
                "\u2026research in continuous deep learning (Zhang et al., 2014), offering new system\u2013theoretic perspectives on neural network architecture design (Greydanus et al., 2019; Bai et al., 2019; Poli et al., 2019; Cranmer et al., 2020) and generative modeling (Grathwohl et al., 2018; Yang et al.,\u2026"
            ],
            "citingPaper": {
                "paperId": "9af2b207750268981d20b4c491c7eb96a80a170c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-09601",
                    "ArXiv": "2007.09601",
                    "MAG": "3043741336",
                    "CorpusId": 220646787
                },
                "corpusId": 220646787,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9af2b207750268981d20b4c491c7eb96a80a170c",
                "title": "Hypersolvers: Toward Fast Continuous-Depth Models",
                "abstract": "The infinite-depth paradigm pioneered by Neural ODEs has launched a renaissance in the search for novel dynamical system-inspired deep learning primitives; however, their utilization in problems of non-trivial size has often proved impossible due to poor computational scalability. This work paves the way for scalable Neural ODEs with time-to-prediction comparable to traditional discrete networks. We introduce hypersolvers, neural networks designed to solve ODEs with low overhead and theoretical guarantees on accuracy. The synergistic combination of hypersolvers and Neural ODEs allows for cheap inference and unlocks a new frontier for practical application of continuous-depth models. Experimental evaluations on standard benchmarks, such as sampling for continuous normalizing flows, reveal consistent pareto efficiency over classical numerical methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "40585370",
                        "name": "Michael Poli"
                    },
                    {
                        "authorId": "90467999",
                        "name": "Stefano Massaroli"
                    },
                    {
                        "authorId": "1748130",
                        "name": "A. Yamashita"
                    },
                    {
                        "authorId": "5312699",
                        "name": "H. Asama"
                    },
                    {
                        "authorId": "2085587",
                        "name": "Jinkyoo Park"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Among those, interpretation of DNNs as discrete-time nonlinear dynamical systems, by viewing each layer as a distinct time step, has received tremendous focus as it enables rich analysis ranging from numerical equations [2], mean-field theory [3], to physics [4, 5, 6]."
            ],
            "citingPaper": {
                "paperId": "6b8f965431aeb16aeb3f9cc387041bcb6edcec45",
                "externalIds": {
                    "MAG": "3043357527",
                    "DBLP": "journals/corr/abs-2007-08880",
                    "ArXiv": "2007.08880",
                    "CorpusId": 220633440
                },
                "corpusId": 220633440,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6b8f965431aeb16aeb3f9cc387041bcb6edcec45",
                "title": "A Differential Game Theoretic Neural Optimizer for Training Residual Networks",
                "abstract": "Connections between Deep Neural Networks (DNNs) training and optimal control theory has attracted considerable attention as a principled tool of algorithmic design. Differential Dynamic Programming (DDP) neural optimizer is a recently proposed method along this line. Despite its empirical success, the applicability has been limited to feedforward networks and whether such a trajectory-optimization inspired framework can be extended to modern architectures remains unclear. In this work, we derive a generalized DDP optimizer that accepts both residual connections and convolution layers. The resulting optimal control representation admits a game theoretic perspective, in which training residual networks can be interpreted as cooperative trajectory optimization on state-augmented dynamical systems. This Game Theoretic DDP (GT-DDP) optimizer enjoys the same theoretic connection in previous work, yet generates a much complex update rule that better leverages available information during network propagation. Evaluation on image classification datasets (e.g. MNIST and CIFAR100) shows an improvement in training convergence and variance reduction over existing methods. Our approach highlights the benefit gained from architecture-aware optimization.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "46218587",
                        "name": "Guan-Horng Liu"
                    },
                    {
                        "authorId": "11126631",
                        "name": "T. Chen"
                    },
                    {
                        "authorId": "1751063",
                        "name": "Evangelos A. Theodorou"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a08c2b0f7ac943f0cb05e064a477204db4f3fee2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-08383",
                    "ArXiv": "2007.08383",
                    "PubMedCentral": "7733882",
                    "MAG": "3042780272",
                    "DOI": "10.1016/j.patter.2020.100142",
                    "CorpusId": 220546254,
                    "PubMed": "33336200"
                },
                "corpusId": 220546254,
                "publicationVenue": {
                    "id": "17bac89e-3dba-467a-b9d4-71e3baefb08b",
                    "name": "Patterns",
                    "type": "journal",
                    "issn": "2666-3899",
                    "url": "https://www.cell.com/patterns",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/patterns"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a08c2b0f7ac943f0cb05e064a477204db4f3fee2",
                "title": "Deep Learning in Protein Structural Modeling and Design",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2153575781",
                        "name": "Wenhao Gao"
                    },
                    {
                        "authorId": "11404542",
                        "name": "S. Mahajan"
                    },
                    {
                        "authorId": "2714145",
                        "name": "Jeremias Sulam"
                    },
                    {
                        "authorId": "144438064",
                        "name": "Jeffrey J. Gray"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[7] S.",
                "By physics-inspired neural networks [4] authors generally mean either incorporating domain knowledge in the traditional NN or providing additional loss functions to ensure physically consistent predictions [5\u20137]."
            ],
            "citingPaper": {
                "paperId": "26cdfd23267e0eab09c60231d15aec5c38dae496",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-03555",
                    "MAG": "3039383667",
                    "ArXiv": "2007.03555",
                    "DOI": "10.1103/PhysRevAccelBeams.23.074601",
                    "CorpusId": 220381002
                },
                "corpusId": 220381002,
                "publicationVenue": {
                    "id": "f0b25a48-865d-41bf-8487-1394ac41a63f",
                    "name": "Physical Review Accelerators and Beams",
                    "type": "journal",
                    "alternate_names": [
                        "Phys rev accel beam",
                        "Phys Rev Accel Beam",
                        "Physical review accelerators and beams"
                    ],
                    "issn": "2469-9888",
                    "url": "https://journals.aps.org/prab/"
                },
                "url": "https://www.semanticscholar.org/paper/26cdfd23267e0eab09c60231d15aec5c38dae496",
                "title": "Physics-Based Deep Neural Networks for Beam Dynamics in Charged Particle Accelerators",
                "abstract": "This paper presents a novel approach for constructing neural networks which model charged particle beam dynamics. In our approach, the Taylor maps arising in the representation of dynamics are mapped onto the weights of a polynomial neural network. The resulting network approximates the dynamical system with perfect accuracy prior to training and provides a possibility to tune the network weights on additional experimental data. We propose a symplectic regularization approach for such polynomial neural networks that always restricts the trained model to Hamiltonian systems and significantly improves the training procedure. The proposed networks can be used for beam dynamics simulations or for fine-tuning of beam optics models with experimental data. The structure of the network allows for the modeling of large accelerators with a large number of magnets. We demonstrate our approach on the examples of the existing PETRA III and the planned PETRA IV storage rings at DESY.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145368241",
                        "name": "A. Ivanov"
                    },
                    {
                        "authorId": "31640490",
                        "name": "I. Agapov"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Prior work on automatically learning symmetries is more sparse, and includes works that focus on Gaussian processes [49] and symmetries of physical systems [20, 8]."
            ],
            "citingPaper": {
                "paperId": "2e072998dd7b40e9e514b2bb43c8074bd5aa43d2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-02933",
                    "MAG": "3039743463",
                    "ArXiv": "2007.02933",
                    "CorpusId": 220363897
                },
                "corpusId": 220363897,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2e072998dd7b40e9e514b2bb43c8074bd5aa43d2",
                "title": "Meta-Learning Symmetries by Reparameterization",
                "abstract": "Many successful deep learning architectures are equivariant to certain transformations in order to conserve parameters and improve generalization: most famously, convolution layers are equivariant to shifts of the input. This approach only works when practitioners know a-priori symmetries of the task and can manually construct an architecture with the corresponding equivariances. Our goal is a general approach for learning equivariances from data, without needing prior knowledge of a task's symmetries or custom task-specific architectures. We present a method for learning and encoding equivariances into networks by learning corresponding parameter sharing patterns from data. Our method can provably encode equivariance-inducing parameter sharing for any finite group of symmetry transformations, and we find experimentally that it can automatically learn a variety of equivariances from symmetries in data. We provide our experiment code and pre-trained models at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2064472884",
                        "name": "Allan Zhou"
                    },
                    {
                        "authorId": "2066655204",
                        "name": "Tom Knowles"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Some methods bypass this problem by learning energetic invariants of the system [16, 17, 18] or exploiting the symplectic structure of the problem [19, 20], reporting promising and interpretable results for Hamiltonian dynamics."
            ],
            "citingPaper": {
                "paperId": "26f6e9238fdf7a196e07dc41a83cc171b4c0feaa",
                "externalIds": {
                    "MAG": "3135975277",
                    "DBLP": "journals/corr/abs-2007-03758",
                    "ArXiv": "2007.03758",
                    "DOI": "10.1016/J.CMA.2021.113763",
                    "CorpusId": 220403576
                },
                "corpusId": 220403576,
                "publicationVenue": {
                    "id": "3bfaa538-a67d-47d7-bfda-6f82748e9a29",
                    "name": "Computer Methods in Applied Mechanics and Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Method Appl Mech Eng"
                    ],
                    "issn": "0045-7825",
                    "url": "https://www.journals.elsevier.com/computer-methods-in-applied-mechanics-and-engineering",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00457825"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/26f6e9238fdf7a196e07dc41a83cc171b4c0feaa",
                "title": "Deep learning of thermodynamics-aware reduced-order models from data",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145556993",
                        "name": "Quercus Hernandez"
                    },
                    {
                        "authorId": "51181453",
                        "name": "A. Badias"
                    },
                    {
                        "authorId": "47723344",
                        "name": "D. Gonz\u00e1lez"
                    },
                    {
                        "authorId": "2734584",
                        "name": "F. Chinesta"
                    },
                    {
                        "authorId": "3246447",
                        "name": "E. Cueto"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Hamiltonian Neural Network (HNN) [3] provides a single experiment with image observations, which requires a modification in the model.",
                "All of these works (except one particular experiment in HNN [3]) require direct observation of low dimensional position and velocity data.",
                "Baselines We set up two baseline models: HGN [6] and PixelHNN [3].",
                "We implemented HGN based on the architecture described in the paper and used the official code for PixelHNN.",
                "The control input to the simulator is u(q, q\u0307) = \u03b2(q) + v(q\u0307) which is designed as in Section 2.2 with the learned potential energy, input matrix, coordinates encoded from the output images, and q?.\nBaselines We set up two baseline models: HGN [6] and PixelHNN [3].",
                "Moreover, both HNN and HGN focus on prediction and have no design of control.",
                "In the original implementation of PixelHNN [3], the angle of the pendulum is constrained to be from \u2212\u03c0/6 to \u03c0/6, where a linear approximation of the nonlinear dynamics is learned, which makes the learned coordinates easy to interpret.",
                "PixelHNN does not use an integrator and requires a special term in the loss function.",
                "Hamiltonian Neural Networks [3] learn Hamiltonian dynamics from position, velocity and acceleration data.",
                "PixelHNN does not account for the rotational nature of coordinate q, so the reconstruction images around the unstable equilibrium point are blurry and the learned coordinates are not easy to interpret (see Supplementary Material).",
                "Recently, an increasing number of works [1, 2, 3, 4, 5] have incorporated Lagrangian/Hamiltonian dynamics into learning dynamical systems from coordinate data, to improve prediction and generalization."
            ],
            "citingPaper": {
                "paperId": "45dde4ce5dc531ded482d1fa6048445141a41905",
                "externalIds": {
                    "DBLP": "conf/nips/ZhongL20",
                    "ArXiv": "2007.01926",
                    "MAG": "3038141699",
                    "CorpusId": 220363329
                },
                "corpusId": 220363329,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/45dde4ce5dc531ded482d1fa6048445141a41905",
                "title": "Unsupervised Learning of Lagrangian Dynamics from Images for Prediction and Control",
                "abstract": "Recent approaches for modelling dynamics of physical systems with neural networks enforce Lagrangian or Hamiltonian structure to improve prediction and generalization. However, these approaches fail to handle the case when coordinates are embedded in high-dimensional data such as images. We introduce a new unsupervised neural network model that learns Lagrangian dynamics from images, with interpretability that benefits prediction and control. The model infers Lagrangian dynamics on generalized coordinates that are simultaneously learned with a coordinate-aware variational autoencoder (VAE). The VAE is designed to account for the geometry of physical systems composed of multiple rigid bodies in the plane. By inferring interpretable Lagrangian dynamics, the model learns physical system properties, such as kinetic and potential energy, which enables long-term prediction of dynamics in the image space and synthesis of energy-based controllers.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35367989",
                        "name": "Yaofeng Desmond Zhong"
                    },
                    {
                        "authorId": "3301461",
                        "name": "Naomi Ehrich Leonard"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5927d03234e4bed5d758b2e416123daed81cd3d9",
                "externalIds": {
                    "MAG": "3099760025",
                    "ArXiv": "2007.04496",
                    "DOI": "10.1088/1361-6587/abcbaa",
                    "CorpusId": 220425414
                },
                "corpusId": 220425414,
                "publicationVenue": {
                    "id": "163d7606-729c-45a5-8c99-48d4d965b55d",
                    "name": "Plasma Physics and Controlled Fusion",
                    "type": "journal",
                    "alternate_names": [
                        "Plasma Phys Control Fusion"
                    ],
                    "issn": "0741-3335",
                    "url": "http://www.iop.org/EJ/journal/0741-3335",
                    "alternate_urls": [
                        "https://iopscience.iop.org/0741-3335"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5927d03234e4bed5d758b2e416123daed81cd3d9",
                "title": "Fast neural Poincar\u00e9 maps for toroidal magnetic fields",
                "abstract": "Poincar\u00e9 maps for toroidal magnetic fields are routinely employed to study gross confinement properties in devices built to contain hot plasmas. In most practical applications, evaluating a Poincar\u00e9 map requires numerical integration of a magnetic field line, a process that can be slow and that cannot be easily accelerated using parallel computations. We propose a novel neural network architecture, the H\u00e9nonNet, and show that it is capable of accurately learning realistic Poincar\u00e9 maps from observations of a conventional field-line-following algorithm. After training, such learned Poincar\u00e9 maps evaluate much faster than the field-line integration method. Moreover, the H\u00e9nonNet architecture exactly reproduces the primary physics constraint imposed on field-line Poincar\u00e9 maps: flux preservation. This structure-preserving property is the consequence of each layer in a H\u00e9nonNet being a symplectic map. We demonstrate empirically that a H\u00e9nonNet can learn to mock the confinement properties of a large magnetic island by using coiled hyperbolic invariant manifolds to produce a sticky chaotic region at the desired island location. This suggests a novel approach to designing magnetic fields with good confinement properties that may be more flexible than ensuring confinement using KAM tori.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2264263",
                        "name": "J. Burby"
                    },
                    {
                        "authorId": "1429560602",
                        "name": "Q. Tang"
                    },
                    {
                        "authorId": "20693875",
                        "name": "R. Maulik"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Other studies have investigated the similarities of dynamical systems and deep learning methods [65] and employed conservation laws to learn systems described by Hamiltonian mechanics [18, 12].",
                "In this context, deep learning methods are receiving strongly growing attention [40, 4, 18] and show promise to account for those components of the solutions that are difficult to resolve or are not well captured by our physical models."
            ],
            "citingPaper": {
                "paperId": "8995944de6f298c6a439675e31068ab8258722ba",
                "externalIds": {
                    "ArXiv": "2007.00016",
                    "MAG": "3098435014",
                    "DBLP": "journals/corr/abs-2007-00016",
                    "CorpusId": 220280657
                },
                "corpusId": 220280657,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8995944de6f298c6a439675e31068ab8258722ba",
                "title": "Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers",
                "abstract": "Finding accurate solutions to partial differential equations (PDEs) is a crucial task in all scientific and engineering disciplines. It has recently been shown that machine learning methods can improve the solution accuracy by correcting for effects not captured by the discretized PDE. We target the problem of reducing numerical errors of iterative PDE solvers and compare different learning approaches for finding complex correction functions. We find that previously used learning approaches are significantly outperformed by methods that integrate the solver into the training loop and thereby allow the model to interact with the PDE during training. This provides the model with realistic input distributions that take previous corrections into account, yielding improvements in accuracy with stable rollouts of several hundred recurrent evaluation steps and surpassing even tailored supervised variants. We highlight the performance of the differentiable physics networks for a wide variety of PDEs, from non-linear advection-diffusion systems to three-dimensional Navier-Stokes flows.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2679476",
                        "name": "Kiwon Um"
                    },
                    {
                        "authorId": "144179670",
                        "name": "Yun Fei"
                    },
                    {
                        "authorId": "13094542",
                        "name": "P. Holl"
                    },
                    {
                        "authorId": "117920737",
                        "name": "R. Brand"
                    },
                    {
                        "authorId": "1786445",
                        "name": "N. Th\u00fcrey"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recent articles have tried to address these limitations [26] by constructing physics-aware networks that learn conservation laws, but their extension to complicated systems with incomplete observations remains unclear."
            ],
            "citingPaper": {
                "paperId": "cb33ac00fac9ac92a38083a75759016dc3fe61ce",
                "externalIds": {
                    "MAG": "3037369969",
                    "ArXiv": "2006.14725",
                    "DOI": "10.1063/5.0019884",
                    "CorpusId": 220127885
                },
                "corpusId": 220127885,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cb33ac00fac9ac92a38083a75759016dc3fe61ce",
                "title": "Non-autoregressive time-series methods for stable parametric reduced-order models",
                "abstract": "Advection-dominated dynamical systems, characterized by partial differential equations, are found in applications ranging from weather forecasting to engineering design where accuracy and robustness are crucial. There has been significant interest in the use of techniques borrowed from machine learning to reduce the computational expense and/or improve the accuracy of predictions for these systems. These rely on the identification of a basis that reduces the dimensionality of the problem and the subsequent use of time series and sequential learning methods to forecast the evolution of the reduced state. Often, however, machine-learned predictions after reduced-basis projection are plagued by issues of stability stemming from incomplete capture of multiscale processes as well as due to error growth for long forecast durations. To address these issues, we have developed a non-autoregressive time series approach for predicting linear reduced-basis time histories of forward models. In particular, we demonstrate that non-autoregressive counterparts of sequential learning methods such as long short-term memory (LSTM) considerably improve the stability of machine-learned reduced-order models. We evaluate our approach on the inviscid shallow water equations and show that a non-autoregressive variant of the standard LSTM approach that is bidirectional in the principal component directions obtains the best accuracy for recreating the nonlinear dynamics of partial observations. Moreover\u2014and critical for many applications of these surrogates\u2014inference times are reduced by three orders of magnitude using our approach, compared with both the equation-based Galerkin projection method and the standard LSTM approach.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "20693875",
                        "name": "R. Maulik"
                    },
                    {
                        "authorId": "2377271",
                        "name": "Bethany Lusch"
                    },
                    {
                        "authorId": "69372472",
                        "name": "Prasanna Balaprakash"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "\u2026there is a considerable amount of literature where authors have endowed neural networks with classical Hamiltonian mechanics (Toth et al. 2019; Greydanus, Dzamba, and Yosinski 2019; Tong et al. 2020; Iten et al. 2020; Bondesan and Lamacraft 2019; Zhong, Dey, and Chakraborty 2019; Chmiela et\u2026",
                "In particular, the amount of literature where authors have endowed neural networks with classical Hamiltonian mechanics has increased considerably [7, 8, 10, 30, 31, 32, 33].",
                "In the case of natural sciences, applying ML to physics is not new, several works have been reported (Toth et al. 2019; Greydanus, Dzamba, and Yosinski 2019; Cranmer et al. 2020; Tong et al. 2020) where different authors have combined",
                "In this respect, several works have been reported [7, 8, 9, 10] where different authors have used Hamilton\u2019s equations of motion to generate trajectories that obey the energy conservation and the laws of classical physics."
            ],
            "citingPaper": {
                "paperId": "890b3b42b4f988a204d990f46c67b34d6c2c8aa0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-13297",
                    "MAG": "3037780680",
                    "ArXiv": "2006.13297",
                    "CorpusId": 220041548
                },
                "corpusId": 220041548,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/890b3b42b4f988a204d990f46c67b34d6c2c8aa0",
                "title": "Learning Potentials of Quantum Systems using Deep Neural Networks",
                "abstract": "Machine Learning has wide applications in a broad range of subjects, including physics. Recent works have shown that neural networks can learn classical Hamiltonian mechanics. The results of these works motivate the following question: Can we endow neural networks with inductive biases coming from quantum mechanics and provide insights for quantum phenomena? In this work, we try answering these questions by investigating possible approximations for reconstructing the Hamiltonian of a quantum system given one of its wave--functions. Instead of handcrafting the Hamiltonian and a solution of the Schrodinger equation, we design neural networks that aim to learn it directly from our observations. We show that our method, termed Quantum Potential Neural Networks (QPNN), can learn potentials in an unsupervised manner with remarkable accuracy for a wide range of quantum systems, such as the quantum harmonic oscillator, particle in a box perturbed by an external potential, hydrogen atom, Poschl--Teller potential, and a solitary wave system. Furthermore, in the case of a particle perturbed by an external force, we also learn the perturbed wave function in a joint end-to-end manner.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2131857530",
                        "name": "Arijit Sehanobish"
                    },
                    {
                        "authorId": "2400638",
                        "name": "H. Corzo"
                    },
                    {
                        "authorId": "103697788",
                        "name": "Onur Kara"
                    },
                    {
                        "authorId": "7385683",
                        "name": "D. V. Dijk"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "The link between dynamical systems and models for forecasting sequential data also provides the opportunity to incorporate physical knowledge into the learning process which improves the generalization performance, robustness, and ability to learn with limited data [36, 37, 38, 39, 40, 41, 42, 43]."
            ],
            "citingPaper": {
                "paperId": "bbc89fa342c06cf2216884238c531b1f6434e61d",
                "externalIds": {
                    "DBLP": "conf/iclr/ErichsonAQHM21",
                    "MAG": "3035985817",
                    "ArXiv": "2006.12070",
                    "CorpusId": 219965819
                },
                "corpusId": 219965819,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/bbc89fa342c06cf2216884238c531b1f6434e61d",
                "title": "Lipschitz Recurrent Neural Networks",
                "abstract": "Viewing recurrent neural networks (RNNs) as continuous-time dynamical systems, we propose a recurrent unit that describes the hidden state's evolution with two parts: a well-understood linear component plus a Lipschitz nonlinearity. This particular functional form facilitates stability analysis of the long-term behavior of the recurrent unit using tools from nonlinear systems theory. In turn, this enables architectural design decisions before experimentation. Sufficient conditions for global stability of the recurrent unit are obtained, motivating a novel scheme for constructing hidden-to-hidden matrices. Our experiments demonstrate that the Lipschitz RNN can outperform existing recurrent units on a range of benchmark tasks, including computer vision, language modeling and speech prediction tasks. Finally, through Hessian-based analysis we demonstrate that our Lipschitz recurrent unit is more robust with respect to input and parameter perturbations as compared to other continuous-time RNNs.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2371914",
                        "name": "N. Benjamin Erichson"
                    },
                    {
                        "authorId": "1927247",
                        "name": "Omri Azencot"
                    },
                    {
                        "authorId": "40897456",
                        "name": "A. Queiruga"
                    },
                    {
                        "authorId": "1717098",
                        "name": "Michael W. Mahoney"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "This model is a combination of a Hamiltonian Neural Network [45, 46] and GN."
            ],
            "citingPaper": {
                "paperId": "643ac3ef063c77eb02a3d52637c11fe028bfae28",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-11287",
                    "MAG": "3102090196",
                    "ArXiv": "2006.11287",
                    "CorpusId": 219966125
                },
                "corpusId": 219966125,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/643ac3ef063c77eb02a3d52637c11fe028bfae28",
                "title": "Discovering Symbolic Models from Deep Learning with Inductive Biases",
                "abstract": "We develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example-a detailed dark matter simulation-and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "32122523",
                        "name": "M. Cranmer"
                    },
                    {
                        "authorId": "1398105826",
                        "name": "Alvaro Sanchez-Gonzalez"
                    },
                    {
                        "authorId": "2019153",
                        "name": "P. Battaglia"
                    },
                    {
                        "authorId": "1606128526",
                        "name": "Rui Xu"
                    },
                    {
                        "authorId": "11638962",
                        "name": "K. Cranmer"
                    },
                    {
                        "authorId": "49071192",
                        "name": "D. Spergel"
                    },
                    {
                        "authorId": "21220036",
                        "name": "S. Ho"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Another class of object-based models explicitly represents the Hamiltonian or Lagrangian of the physical system [7, 9, 11].",
                "Another class of object-based models explicitly represents the Hamiltonian or Lagrangian of the physical system (Chen et al., 2019; Cranmer et al., 2020; Greydanus et al., 2019).",
                "Some forward-prediction models may exhibit better generalizing properties by making additional assumptions: for example, conservation of energy or point mass objects [7, 9, 11]."
            ],
            "citingPaper": {
                "paperId": "127e77fdd3af775ed96443eb38f94242d9eaf9cf",
                "externalIds": {
                    "ArXiv": "2006.10734",
                    "MAG": "3036779904",
                    "DBLP": "journals/corr/abs-2006-10734",
                    "CorpusId": 219792186
                },
                "corpusId": 219792186,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/127e77fdd3af775ed96443eb38f94242d9eaf9cf",
                "title": "Forward Prediction for Physical Reasoning",
                "abstract": "Physical reasoning requires forward prediction: the ability to forecast what will happen next given some initial world state. We study the performance of state-of-the-art forward-prediction models in complex physical-reasoning tasks. We do so by incorporating models that operate on object or pixel-based representations of the world, into simple physical-reasoning agents. We find that forward-prediction models improve the performance of physical-reasoning agents, particularly on complex tasks that involve many objects. However, we also find that these improvements are contingent on the training tasks being similar to the test tasks, and that generalization to different tasks is more challenging. Surprisingly, we observe that forward predictors with better pixel accuracy do not necessarily lead to better physical-reasoning performance. Nevertheless, our best models set a new state-of-the-art on the PHYRE benchmark for physical reasoning.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3102850",
                        "name": "Rohit Girdhar"
                    },
                    {
                        "authorId": "47029037",
                        "name": "Laura Gustafson"
                    },
                    {
                        "authorId": "3289587",
                        "name": "Aaron B. Adcock"
                    },
                    {
                        "authorId": "1803520",
                        "name": "L. Maaten"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Learning Physically Meaningful Systems Another related thread of studies is to learn physical systems, such as Lagrangian (Lutter, Ritter, and Peters 2019; Cranmer et al. 2020) and Hamiltonian (Greydanus, Dzamba, and Yosinski 2019; Matsubara, Ishikawa, and Yaguchi 2020) mechanics using neural networks.",
                "\u2026Physically Meaningful Systems Another related thread of studies is to learn physical systems, such as Lagrangian (Lutter, Ritter, and Peters 2019; Cranmer et al. 2020) and Hamiltonian (Greydanus, Dzamba, and Yosinski 2019; Matsubara, Ishikawa, and Yaguchi 2020) mechanics using neural networks.",
                "\u2026in dynamics models includes the Gaussian process dynamics models (Wang, Fleet, and Hertzmann 2006) and models based on deep neural networks (e.g., Takeishi, Kawahara, and Yairi 2017; Lusch, Kutz, and Brunton 2018; Chen et al. 2018; Manek and Kolter 2019; Greydanus, Dzamba, and Yosinski 2019).",
                "Learning physically meaningful systems Another related thread of studies is to learn physical models, such as Lagrangian [25, 9] and Hamiltonian [14] neural networks.",
                "Extension to port-Hamiltonian systems (Zhong, Dey, and Chakraborty 2020) is also considered."
            ],
            "citingPaper": {
                "paperId": "3e917b66419016b75c624abc426d2a537598fae4",
                "externalIds": {
                    "ArXiv": "2006.08935",
                    "DBLP": "conf/aaai/TakeishiK21",
                    "MAG": "3035101501",
                    "DOI": "10.1609/aaai.v35i11.17176",
                    "CorpusId": 219708450
                },
                "corpusId": 219708450,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/3e917b66419016b75c624abc426d2a537598fae4",
                "title": "Learning Dynamics Models with Stable Invariant Sets",
                "abstract": "Invariance and stability are essential notions in dynamical systems study, and thus it is of great interest to learn a dynamics model with a stable invariant set. However, existing methods can only handle the stability of an equilibrium. In this paper, we propose a method to ensure that a dynamics model has a stable invariant set of general classes such as limit cycles and line attractors. We start with the approach by Manek and Kolter (2019), where they use a learnable Lyapunov function to make a model stable with regard to an equilibrium. We generalize it for general sets by introducing projection onto them. To resolve the difficulty of specifying a to-be stable invariant set analytically, we propose defining such a set as a primitive shape (e.g., sphere) in a latent space and learning the transformation between the original and latent spaces. It enables us to compute the projection easily, and at the same time, we can maintain the model's flexibility using various invertible neural networks for the transformation. We present experimental results that show the validity of the proposed method and the usefulness for long-term prediction.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2024905",
                        "name": "Naoya Takeishi"
                    },
                    {
                        "authorId": "1704932",
                        "name": "Y. Kawahara"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "result"
            ],
            "isInfluential": false,
            "contexts": [
                "[Lutter et al., 2019; Greydanus et al., 2019] show that Lagrangian/Hamiltonian mechanics can be imposed to learn the equations of motion of a mechanical system and [Seo and Liu, 2019] regularize a graph neural network with a specific physics equation.",
                "Although there have been some works [de Bezenac et al., 2018; Greydanus et al., 2019] improving data efficiency via explicitly incorporating PDEs as neural network layers when modeling spatiotemporal dynamics, it is hard to generalize for modeling different or unknown dynamics, which is ubiquitous in real-world scenario.",
                "Although there have been some works [de Bezenac et al., 2018; Greydanus et al., 2019] improving data efficiency via explicitly incorporating PDEs as neural network layers when modeling spatiotemporal dynamics, it is hard to generalize for modeling different or unknown dynamics, which is ubiquitous\u2026"
            ],
            "citingPaper": {
                "paperId": "22014da2f9626592217b46ffd208bea4e93058ee",
                "externalIds": {
                    "ArXiv": "2006.08831",
                    "DBLP": "conf/ijcai/SeoMRL21",
                    "MAG": "3035123541",
                    "DOI": "10.24963/ijcai.2021/405",
                    "CorpusId": 219708802
                },
                "corpusId": 219708802,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/22014da2f9626592217b46ffd208bea4e93058ee",
                "title": "Physics-aware Spatiotemporal Modules with Auxiliary Tasks for Meta-Learning",
                "abstract": "Modeling the dynamics of real-world physical systems is critical for spatiotemporal prediction tasks, but challenging when data is limited. The scarcity of real-world data and the difficulty in reproducing the data distribution hinder directly applying meta-learning techniques. Although the knowledge of governing partial differential equations (PDE) of the data can be helpful for the fast adaptation to few observations, it is mostly infeasible to exactly find the equation for observations in real-world physical systems. In this work, we propose a framework, physics-aware meta-learning with auxiliary tasks, whose spatial modules incorporate PDE-independent knowledge and temporal modules utilize the generalized features from the spatial modules to be adapted to the limited data, respectively. The framework is inspired by a local conservation law expressed mathematically as a continuity equation and does not require the exact form of governing equation to model the spatiotemporal observations. The proposed method mitigates the need for a large number of real-world tasks for meta-learning by leveraging spatial information in simulated data to meta-initialize the spatial modules. We apply the proposed framework to both synthetic and real-world spatiotemporal prediction tasks and demonstrate its superior performance with limited observations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145260557",
                        "name": "Sungyong Seo"
                    },
                    {
                        "authorId": "27737939",
                        "name": "Chuizheng Meng"
                    },
                    {
                        "authorId": "2267664",
                        "name": "Sirisha Rambhatla"
                    },
                    {
                        "authorId": "47909587",
                        "name": "Yan Liu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "d2b66d4629abc7f689f5bf7b5dd2b88910ad828e",
                "externalIds": {
                    "MAG": "3099254792",
                    "ArXiv": "2006.07220",
                    "DBLP": "conf/nips/NorcliffeBDSL20",
                    "CorpusId": 219636354
                },
                "corpusId": 219636354,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d2b66d4629abc7f689f5bf7b5dd2b88910ad828e",
                "title": "On Second Order Behaviour in Augmented Neural ODEs",
                "abstract": "Neural Ordinary Differential Equations (NODEs) are a new class of models that transform data continuously through infinite-depth architectures. The continuous nature of NODEs has made them particularly suitable for learning the dynamics of complex physical systems. While previous work has mostly been focused on first order ODEs, the dynamics of many systems, especially in classical physics, are governed by second order laws. In this work, we consider Second Order Neural ODEs (SONODEs). We show how the adjoint sensitivity method can be extended to SONODEs and prove that the optimisation of a first order coupled ODE is equivalent and computationally more efficient. Furthermore, we extend the theoretical understanding of the broader class of Augmented NODEs (ANODEs) by showing they can also learn higher order dynamics with a minimal number of augmented dimensions, but at the cost of interpretability. This indicates that the advantages of ANODEs go beyond the extra space offered by the augmented dimensions, as originally thought. Finally, we compare SONODEs and ANODEs on synthetic and real dynamical systems and demonstrate that the inductive biases of the former generally result in faster training and better performance.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1748965720",
                        "name": "Alexander Norcliffe"
                    },
                    {
                        "authorId": "46195895",
                        "name": "Cristian Bodnar"
                    },
                    {
                        "authorId": "80740711",
                        "name": "Ben Day"
                    },
                    {
                        "authorId": "2050787",
                        "name": "N. Simidjievski"
                    },
                    {
                        "authorId": "144269589",
                        "name": "P. Lio\u2019"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "introduced Hamiltonian Neural Networks (HNNs) to endow machine learning models for dynamical systems with better physical inductive biases [10].",
                "There is a growing body of research in this domain, but much of it relies upon black-box machine learning techniques that solve the prediction problem while neglecting the governing equations problem [10, 5].",
                "To this end, there has been significant research interest in applying machine learning to physical dynamical systems [10, 5, 21, 22, 29, 3, 32, 4]."
            ],
            "citingPaper": {
                "paperId": "767e50c134f136784aeee75b1448c947e19bf6ad",
                "externalIds": {
                    "MAG": "3102017980",
                    "ArXiv": "2006.12972",
                    "DBLP": "conf/nips/DiPietroXZ20",
                    "CorpusId": 219980537
                },
                "corpusId": 219980537,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/767e50c134f136784aeee75b1448c947e19bf6ad",
                "title": "Sparse Symplectically Integrated Neural Networks",
                "abstract": "We introduce Sparse Symplectically Integrated Neural Networks (SSINNs), a novel model for learning Hamiltonian dynamical systems from data. SSINNs combine fourth-order symplectic integration with a learned parameterization of the Hamiltonian obtained using sparse regression through a mathematically elegant function space. This allows for interpretable models that incorporate symplectic inductive biases and have low memory requirements. We evaluate SSINNs on four classical Hamiltonian dynamical problems: the Henon-Heiles system, nonlinearly coupled oscillators, a multi-particle mass-spring system, and a pendulum system. Our results demonstrate promise in both system prediction and conservation of energy, outperforming the current state-of-the-art black-box prediction techniques by an order of magnitude. Further, SSINNs successfully converge to true governing equations from highly limited and noisy data, demonstrating potential applicability in the discovery of new physical governing equations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1396992658",
                        "name": "Daniel M. DiPietro"
                    },
                    {
                        "authorId": "47722204",
                        "name": "S. Xiong"
                    },
                    {
                        "authorId": "92035057",
                        "name": "Bo Zhu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "For related work on learning Hamiltonian systems, see [3, 16]."
            ],
            "citingPaper": {
                "paperId": "49c773fac8b8af8a3a6182772e7d6dc16a937788",
                "externalIds": {
                    "DBLP": "conf/l4dc/AhmadiK20",
                    "ArXiv": "2008.10135",
                    "MAG": "3095947501",
                    "DOI": "10.1137/20m1388644",
                    "CorpusId": 221265950
                },
                "corpusId": 221265950,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/49c773fac8b8af8a3a6182772e7d6dc16a937788",
                "title": "Learning Dynamical Systems with Side Information",
                "abstract": "We present a mathematical and computational framework for the problem of learning a dynamical system from noisy observations of a few trajectories and subject to side information. Side information is any knowledge we might have about the dynamical system we would like to learn besides trajectory data. It is typically inferred from domain-specific knowledge or basic principles of a scientific discipline. We are interested in explicitly integrating side information into the learning process in order to compensate for scarcity of trajectory observations. We identify six types of side information that arise naturally in many applications and lead to convex constraints in the learning problem. First, we show that when our model for the unknown dynamical system is parameterized as a polynomial, one can impose our side information constraints computationally via semidefinite programming. We then demonstrate the added value of side information for learning the dynamics of basic models in physics and cell biology, as well as for learning and controlling the dynamics of a model in epidemiology. Finally, we study how well polynomial dynamical systems can approximate continuously-differentiable ones while satisfying side information (either exactly or approximately). Our overall learning methodology combines ideas from convex optimization, real algebra, dynamical systems, and functional approximation theory, and can potentially lead to new synergies between these areas.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1792465",
                        "name": "Amir Ali Ahmadi"
                    },
                    {
                        "authorId": "40899758",
                        "name": "Bachir El Khadir"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", also tried to combine an autoencoder with an Hamiltonian Neural Networks (HNN) to model the dynamics of pixel observations of a pendulum [10].",
                "Machine learning in fluid systems The rapid advent of machine learning techniques is opening up new possibilities to solve the physical system\u2019s identification problems by statistically exploring the underlying structure of a variety of physical systems, encompassing applications in quantum physics [35], thermodynamics [14], material science [36], rigid body control [9], Lagrangian systems [8], and Hamiltonian systems [10, 19, 37]."
            ],
            "citingPaper": {
                "paperId": "43aa01607b2327b186f68cde4186f77c7b3f1e61",
                "externalIds": {
                    "ArXiv": "2006.04178",
                    "DBLP": "journals/corr/abs-2006-04178",
                    "MAG": "3033230180",
                    "CorpusId": 219530702
                },
                "corpusId": 219530702,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/43aa01607b2327b186f68cde4186f77c7b3f1e61",
                "title": "Neural Vortex Method: from Finite Lagrangian Particles to Infinite Dimensional Eulerian Dynamics",
                "abstract": "In the field of fluid numerical analysis, there has been a long-standing problem: lacking of a rigorous mathematical tool to map from a continuous flow field to discrete vortex particles, hurdling the Lagrangian particles from inheriting the high resolution of a large-scale Eulerian solver. To tackle this challenge, we propose a novel learning-based framework, the Neural Vortex Method (NVM), which builds a neural-network description of the Lagrangian vortex structures and their interaction dynamics to reconstruct the high-resolution Eulerian flow field in a physically-precise manner. The key components of our infrastructure consist of two networks: a vortex representation network to identify the Lagrangian vortices from a grid-based velocity field and a vortex interaction network to learn the underlying governing dynamics of these finite structures. By embedding these two networks with a vorticity-to-velocity Poisson solver and training its parameters using the high-fidelity data obtained from high-resolution direct numerical simulation, we can predict the accurate fluid dynamics on a precision level that was infeasible for all the previous conventional vortex methods (CVMs). To the best of our knowledge, our method is the first approach that can utilize motions of finite particles to learn infinite dimensional dynamic systems. We demonstrate the efficacy of our method in generating highly accurate prediction results, with low computational cost, of the leapfrogging vortex rings system, the turbulence system, and the systems governed by Euler equations with different external forces.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "47722204",
                        "name": "S. Xiong"
                    },
                    {
                        "authorId": "1491320619",
                        "name": "Xingzhe He"
                    },
                    {
                        "authorId": "40495066",
                        "name": "Yunjin Tong"
                    },
                    {
                        "authorId": "92035057",
                        "name": "Bo Zhu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "The development of automatic differentiation packages for machine learning has opened up a new approach to stability analysis and estimation of functions of interest in control theory such as Hamiltonian [38] or Lyapunov function [39]."
            ],
            "citingPaper": {
                "paperId": "fe25a473bc3123205ee232c0509b11e7a61cfa79",
                "externalIds": {
                    "ArXiv": "2006.03947",
                    "DBLP": "journals/corr/abs-2006-03947",
                    "MAG": "3033364212",
                    "CorpusId": 219531380
                },
                "corpusId": 219531380,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fe25a473bc3123205ee232c0509b11e7a61cfa79",
                "title": "Automatic Policy Synthesis to Improve the Safety of Nonlinear Dynamical Systems",
                "abstract": "Learning controllers merely based on a performance metric has been proven effective in many physical and non-physical tasks in both control theory and reinforcement learning. However, in practice, the controller must guarantee some notion of safety to ensure that it does not harm either the agent or the environment. Stability is a crucial notion of safety, whose violation can certainly cause unsafe behaviors. Lyapunov functions are effective tools to assess stability in nonlinear dynamical systems. In this paper, we combine an improving Lyapunov function with automatic controller synthesis to obtain control policies with large safe regions. We propose a two-player collaborative algorithm that alternates between estimating a Lyapunov function and deriving a controller that gradually enlarges the stability region of the closed-loop system. We provide theoretical results on the class of systems that can be treated with the proposed algorithm and empirically evaluate the effectiveness of our method using an exemplary dynamical system.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2406046",
                        "name": "Arash Mehrjou"
                    },
                    {
                        "authorId": "1678622",
                        "name": "M. Ghavamzadeh"
                    },
                    {
                        "authorId": "1707625",
                        "name": "B. Sch\u00f6lkopf"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "It is interesting to note the similarity between this computational graph and the one of the residual network structure He et al. (2016). Note that number of repeated blocks in the computational graph (thus, the depth of the architecture) corresponds to the length of the training sequence.",
                "In these cases, a physics-based neural network may be used to learn the system\u2019s Hamiltonian or Lagrangian function, instead of the individual components its ODE representation as independent terms (Greydanus et al., 2019; Lutter et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "10a2c2be37afebe949f8b3a89bfbef7084aa3214",
                "externalIds": {
                    "MAG": "3132425763",
                    "DBLP": "journals/ejcon/ForgioneP21",
                    "ArXiv": "2006.02915",
                    "DOI": "10.1016/j.ejcon.2021.01.008",
                    "CorpusId": 219304851
                },
                "corpusId": 219304851,
                "publicationVenue": {
                    "id": "cd47c826-7767-476f-b755-eb7d728c10df",
                    "name": "European Journal of Control",
                    "type": "journal",
                    "alternate_names": [
                        "Eur J Control"
                    ],
                    "issn": "0947-3580",
                    "url": "http://ejc.revuesonline.com/"
                },
                "url": "https://www.semanticscholar.org/paper/10a2c2be37afebe949f8b3a89bfbef7084aa3214",
                "title": "Continuous-time system identification with neural networks: model structures and fitting criteria",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3022415",
                        "name": "Marco Forgione"
                    },
                    {
                        "authorId": "2106384",
                        "name": "D. Piga"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In addition to the works discussed above, a research area in the unsupervised learning literature of particular interest is that of learning physically plausible representations (from video) by enforcing temporal evolution according to explicit or implicit physical dynamics [4, 19, 25, 45]."
            ],
            "citingPaper": {
                "paperId": "2e0bc6cc4153025cd37d957cb896abe237502cd5",
                "externalIds": {
                    "DBLP": "conf/cvpr/JaquesBH21",
                    "ArXiv": "2006.01959",
                    "MAG": "3033735750",
                    "DOI": "10.1109/CVPR46437.2021.00443",
                    "CorpusId": 219259810
                },
                "corpusId": 219259810,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2e0bc6cc4153025cd37d957cb896abe237502cd5",
                "title": "NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces",
                "abstract": "Learning low-dimensional latent state space dynamics models has proven powerful for enabling vision-based planning and learning for control. We introduce a latent dynamics learning framework that is uniquely designed to induce proportional controlability in the latent space, thus enabling the use of simple and well-known PID controllers. We show that our learned dynamics model enables proportional control from pixels, dramatically simplifies and accelerates behavioural cloning of vision-based controllers, and provides interpretable goal discovery when applied to imitation learning of switching controllers from demonstration. Notably, such proportional controlability also allows for robust path following from visual demonstrations using Dynamic Movement Primitives in the learned latent space.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50425877",
                        "name": "Miguel Jaques"
                    },
                    {
                        "authorId": "145841847",
                        "name": "Michael Burke"
                    },
                    {
                        "authorId": "1697755",
                        "name": "Timothy M. Hospedales"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "09ef364eacfa362d1e9bd8d827fa7e587a028610",
                "externalIds": {
                    "MAG": "2993987425",
                    "DOI": "10.1103/PHYSREVE.101.062207",
                    "CorpusId": 208617695,
                    "PubMed": "32688545"
                },
                "corpusId": 208617695,
                "publicationVenue": {
                    "id": "19842b7b-a4d1-4f9a-9714-87d878cf6e73",
                    "name": "Physical Review E",
                    "type": "journal",
                    "alternate_names": [
                        "Phys rev E",
                        "Physical review. E",
                        "Phys Rev E"
                    ],
                    "issn": "1539-3755",
                    "alternate_issns": [
                        "1550-2376",
                        "2470-0045"
                    ],
                    "url": "https://journals.aps.org/pre/",
                    "alternate_urls": [
                        "http://pre.aps.org/",
                        "http://journals.aps.org/pre/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/09ef364eacfa362d1e9bd8d827fa7e587a028610",
                "title": "Physics-enhanced neural networks learn order and chaos.",
                "abstract": "Artificial neural networks are universal function approximators. They can forecast dynamics, but they may need impractically many neurons to do so, especially if the dynamics is chaotic. We use neural networks that incorporate Hamiltonian dynamics to efficiently learn phase space orbits even as nonlinear systems transition from order to chaos. We demonstrate Hamiltonian neural networks on a widely used dynamics benchmark, the H\u00e9non-Heiles potential, and on nonperturbative dynamical billiards. We introspect to elucidate the Hamiltonian neural network forecasting.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145638518",
                        "name": "A. Choudhary"
                    },
                    {
                        "authorId": "2468955",
                        "name": "J. Lindner"
                    },
                    {
                        "authorId": "1443437285",
                        "name": "Elliott G. Holliday"
                    },
                    {
                        "authorId": "50783104",
                        "name": "Scott T. Miller"
                    },
                    {
                        "authorId": "2053142",
                        "name": "S. Sinha"
                    },
                    {
                        "authorId": "3067672",
                        "name": "W. Ditto"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", see [6], [31], [32], [33], [34], and [35].",
                ", the sum of potential and kinematic energies of a pendulum [6])."
            ],
            "citingPaper": {
                "paperId": "2a27fd522de62b66017d1161d8982a596eaa1fc2",
                "externalIds": {
                    "ArXiv": "2006.12745",
                    "DBLP": "journals/corr/abs-2006-12745",
                    "MAG": "3036013675",
                    "CorpusId": 219980655
                },
                "corpusId": 219980655,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2a27fd522de62b66017d1161d8982a596eaa1fc2",
                "title": "Learning Physical Constraints with Neural Projections",
                "abstract": "We propose a new family of neural networks to predict the behaviors of physical systems by learning their underpinning constraints. A neural projection operator liesat the heart of our approach, composed of a lightweight network with an embedded recursive architecture that interactively enforces learned underpinning constraints and predicts the various governed behaviors of different physical systems. Our neural projection operator is motivated by the position-based dynamics model that has been used widely in game and visual effects industries to unify the various fast physics simulators. Our method can automatically and effectively uncover a broad range of constraints from observation point data, such as length, angle, bending, collision, boundary effects, and their arbitrary combinations, without any connectivity priors. We provide a multi-group point representation in conjunction with a configurable network connection mechanism to incorporate prior inputs for processing complex physical systems. We demonstrated the efficacy of our approach by learning a set of challenging physical systems all in a unified and simple fashion including: rigid bodies with complex geometries, ropes with varying length and bending, articulated soft and rigid bodies, and multi-object collisions with complex boundaries.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "14386295",
                        "name": "Shuqi Yang"
                    },
                    {
                        "authorId": "1491320619",
                        "name": "Xingzhe He"
                    },
                    {
                        "authorId": "92035057",
                        "name": "Bo Zhu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Two recent works aim to uncover physical laws from data in a general manner (Iten et al., 2020; Greydanus et al., 2019).",
                "g an ODE solver into the network structure. Chen et al. (2018) propose a general method to parameterize the derivative of the hidden state and then apply an arbitrary ODE solver. Gupta et al. (2019), Greydanus et al. (2019) and Zhong et al. (2020) take such a perspective for mechanical systems and model the derivative of the desired state. An earlier example for this idea, applied in another domain, given by Al Seyab an",
                "Gupta et al. (2019), Greydanus et al. (2019) and Zhong et al. (2020) take such a perspective for mechanical systems and model the derivative of the desired state.",
                "One recent development is physicsinformed neural networks (Raissi et al., 2019; Lutter et al., 2019; Greydanus et al., 2019; Zhong et al., 2020; Gupta et al., 2019; Rackauckas et al., 2020), which use mechanistic equations to endow neural networks with better prior.",
                "nt subsystems or multi-delity modeling (Fernandez-Godino et al., 2016; von Stosch et al., 2014). One recent development is physicsinformed neural networks (Raissi et al., 2019; Lutter et al., 2019; Greydanus et al., 2019; Zhong et al., 2020; Gupta et al., 2019; Rackauckas et al., 2020), which use mechanistic equations to endow neural networks with better prior. We follow this line and propose physics-informed neural ",
                "ilar motivation to ours and also achieves improved data and computational eciency in diverse examples. Two recent works aim to uncover physical laws from data in a general manner (Iten et al., 2020; Greydanus et al., 2019). More specic for mechanical systems, physics-informed neural networks were demonstrated with simulated time series data for the forward model (1) of a pendulum, double pendulum, and a cart pole syst"
            ],
            "citingPaper": {
                "paperId": "2add51575a7b1a8aef84ad99f8c17e13d3d5e518",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2005-14617",
                    "ArXiv": "2005.14617",
                    "MAG": "3029817982",
                    "DOI": "10.1016/J.IFACOL.2020.12.2182",
                    "CorpusId": 219124414
                },
                "corpusId": 219124414,
                "publicationVenue": {
                    "id": "af98f1eb-affb-4b55-b8ff-1964b29cf894",
                    "name": "IFAC-PapersOnLine",
                    "type": "journal",
                    "issn": "2405-8963",
                    "url": "https://www.journals.elsevier.com/ifac-papersonline/",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/24058963",
                        "https://www.journals.elsevier.com/ifac-papersonline"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2add51575a7b1a8aef84ad99f8c17e13d3d5e518",
                "title": "Modeling System Dynamics with Physics-Informed Neural Networks Based on Lagrangian Mechanics",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "114207785",
                        "name": "Manuel A. Roehrl"
                    },
                    {
                        "authorId": "1727058",
                        "name": "T. Runkler"
                    },
                    {
                        "authorId": "1729522737",
                        "name": "Veronika Brandtstetter"
                    },
                    {
                        "authorId": "2157402",
                        "name": "Michel Tokic"
                    },
                    {
                        "authorId": "1729514867",
                        "name": "Stefan Obermayer"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "The alternative tabula rasa approach assumes no physics whatsoever and attempts to learn physical object properties [60], object positions [61, 62], object relations [63] and time evolution [64\u201366] by learning a lowdimensional representation or latent space which is unfortunately too complex or inscrutable to allow discovery of exact equations of motion.",
                "[66] Samuel Greydanus, Misko Dzamba, and Jason Yosinski."
            ],
            "citingPaper": {
                "paperId": "3b92bdfdf5d8d65fb5691fee954e4c7cb54909a0",
                "externalIds": {
                    "ArXiv": "2005.11212",
                    "DBLP": "journals/corr/abs-2005-11212",
                    "MAG": "3028434239",
                    "DOI": "10.1103/PhysRevE.103.043307",
                    "CorpusId": 218863035,
                    "PubMed": "34005960"
                },
                "corpusId": 218863035,
                "publicationVenue": {
                    "id": "19842b7b-a4d1-4f9a-9714-87d878cf6e73",
                    "name": "Physical Review E",
                    "type": "journal",
                    "alternate_names": [
                        "Phys rev E",
                        "Physical review. E",
                        "Phys Rev E"
                    ],
                    "issn": "1539-3755",
                    "alternate_issns": [
                        "1550-2376",
                        "2470-0045"
                    ],
                    "url": "https://journals.aps.org/pre/",
                    "alternate_urls": [
                        "http://pre.aps.org/",
                        "http://journals.aps.org/pre/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3b92bdfdf5d8d65fb5691fee954e4c7cb54909a0",
                "title": "Symbolic Pregression: Discovering Physical Laws from Raw Distorted Video",
                "abstract": "We present a method for unsupervised learning of equations of motion for objects in raw and optionally distorted unlabeled synthetic video (or, more generally, for discovering and modeling predictable features in time-series data). We first train an autoencoder that maps each video frame into a low-dimensional latent space where the laws of motion are as simple as possible, by minimizing a combination of nonlinearity, acceleration, and prediction error. Differential equations describing the motion are then discovered using Pareto-optimal symbolic regression. We find that our pre-regression (\"pregression\") step is able to rediscover Cartesian coordinates of unlabeled moving objects even when the video is distorted by a generalized lens. Using intuition from multidimensional knot theory, we find that the pregression step is facilitated by first adding extra latent space dimensions to avoid topological problems during training and then removing these extra dimensions via principal component analysis. An inertial frame is autodiscovered by minimizing the combined equation complexity for multiple experiments.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "122123672",
                        "name": "S. Udrescu"
                    },
                    {
                        "authorId": "2011933",
                        "name": "Max Tegmark"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                ", embedding the notion of an incompressible fluid [29, 48], the Galilean invariance [28], a quasistatic physics simulation [13], and the invariant quantities in Lagrangian systems [7] and Hamiltonian systems [21, 14, 24, 51, 8, 49].",
                "In particular, we demonstrate that the training period of our model can be around 6000 times shorter than its predicting period (other methods have the training period 1\u201325 times shorter than the predicting period [5, 14, 24]), and the number of training samples is around 5 times smaller (meaning we use 5 times fewer time-sequences as in the training process) than that used by other methods.",
                "ODE-net [5] and HNN [14], have to rely on intermediate data in their training data to train the model.",
                "first tried to enforce conservative features of the Hamiltonian system by reformulating the loss function using Hamilton\u2019s equations, known as Hamiltonian neural networks (HNNs) [14]."
            ],
            "citingPaper": {
                "paperId": "7fbd3fe24323781eded5ca428ec649370bf1d03f",
                "externalIds": {
                    "DBLP": "journals/jcphy/TongXHPZ21",
                    "MAG": "3022424628",
                    "ArXiv": "2005.04986",
                    "DOI": "10.1016/j.jcp.2021.110325",
                    "CorpusId": 218580916
                },
                "corpusId": 218580916,
                "publicationVenue": {
                    "id": "58606051-2e63-4034-8496-9d4ed773bb49",
                    "name": "Journal of Computational Physics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Phys"
                    ],
                    "issn": "0021-9991",
                    "url": "http://www.elsevier.com/locate/jcp",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00219991",
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622866/description#description",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7fbd3fe24323781eded5ca428ec649370bf1d03f",
                "title": "Symplectic Neural Networks in Taylor Series Form for Hamiltonian Systems",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "40495066",
                        "name": "Yunjin Tong"
                    },
                    {
                        "authorId": "47722204",
                        "name": "S. Xiong"
                    },
                    {
                        "authorId": "1491320619",
                        "name": "Xingzhe He"
                    },
                    {
                        "authorId": "1689207911",
                        "name": "Guanghan Pan"
                    },
                    {
                        "authorId": "92035057",
                        "name": "Bo Zhu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4281b3e4c3a5be1f231a9c035bdec16477bbd672",
                "externalIds": {
                    "MAG": "3045274967",
                    "CorpusId": 220793756
                },
                "corpusId": 220793756,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4281b3e4c3a5be1f231a9c035bdec16477bbd672",
                "title": "Parsimonious neural networks learn classical mechanics, its underlying symmetries, and an accurate time integrator",
                "abstract": "Machine learning is playing an increasing role in the physical sciences and significant progress has been made towards embedding physics into domain-agnostic models. Less explored is the potential of machine learning in the discovery of physical laws from observational data through interpretable models. We combine neural networks with evolutionary optimization to find the simplest models that describe the time evolution of a point particle under a highly nonlinear potential. The resulting parsimonious neural networks are easily interpretable as Newton's second law expressed as a non-trivial time integrator that exhibits time-reversibility and conserves energy. By extracting the underlying physics, the model significantly outperforms a generic feed-forward neural network. Furthermore, the models discovered belong to the widely used family of Verlet algorithms which are not only reversible but symplectic.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "21765207",
                        "name": "Saaketh Desai"
                    },
                    {
                        "authorId": "145823562",
                        "name": "A. Strachan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Numerous approaches exist to model time dynamics but the most straightforward methods involve learning functions which map inputs directly to their time derivatives [6] or to the next state [1, 3].",
                "Another key attribute of the Hamiltonian is that the vector field S is a symplectic gradient meaningH remains constant as long as state vectors are integrated along S [6].",
                "Extensive research has shown that enriching models with well-chosen inductive biases such as Hamiltonian Neural Networks (HNNs) [6], ordinary differential equations (ODEs) [4, 5] and more recently, graphs [1, 14, 13] can significantly improve learning.",
                "Recently, [6] demonstrated that dynamic predictions through time can be improved using Hamiltonian Neural Networks (HNNs) which endow models with a Hamiltonian constraint.",
                "Inspired by this work, [6] and [15] show that one can take a physical system (position and momentum values, or e.",
                "To build the noisy data, we follow the approach of [6] and [11] by sampling from a Gaussian N (0, 0.",
                "dq dt = \u2202H \u2202p , dp dt = \u2212\u2202H \u2202q (1) As a consequence, it is noted in [6] that by accurately learning a Hamiltonian, the system\u2019s dynamics can be naturally extracted through backpropagation.",
                "However, [6] show that using this simple approach is not enough to accurately predict energy conserving phase space trajectories."
            ],
            "citingPaper": {
                "paperId": "2468c730aa140f1910260aff685d6310b0ad6404",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2004-13688",
                    "MAG": "3023762998",
                    "CorpusId": 216562707
                },
                "corpusId": 216562707,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2468c730aa140f1910260aff685d6310b0ad6404",
                "title": "VIGN: Variational Integrator Graph Networks",
                "abstract": "Rich, physically-informed inductive biases play an imperative role in accurately modelling the time dynamics of physical systems. In this paper, we introduce Variational Integrator Graph Networks (VIGNs), the first approach to combine a Variational Integrator (VI) inductive bias with a Graph Network (GN) and demonstrate an order of magnitude improvement in performance, both in terms of data-efficient learning and predictive accuracy, over existing methods. We show that this improvement arises because VIs induce coupled learning of generalized position and momentum updates which can be formulated as a Partitioned Runge-Kutta (PRK) method. We empirically establish that VIGN outperforms numerous methods in learning from existing datasets with noise.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144446378",
                        "name": "Shaan Desai"
                    },
                    {
                        "authorId": "143841496",
                        "name": "Stephen J. Roberts"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "(1997); Hansen and Ostermeier (2001) for system identification is to collect ground truth data and subsequently optimize the dynamics model over certain parameters of a physics simulation engine, so as to minimize the difference between the predicted trajectory and the ground truth.",
                "Recently, Li et al. (2019b) proposed the multi-step propagation network and Greydanus et al. (2019) applied a Hamiltonian network to conserve an\n1. https://rutgers.box.com/shared/static/i9vvxpc8152i5e0zg897fj47sanen7nj.mp4\nenergy-like quantity without damping."
            ],
            "citingPaper": {
                "paperId": "3a1f1b5e1bfe8026df79f772abfaefbf1c6a85b5",
                "externalIds": {
                    "MAG": "3022109602",
                    "ArXiv": "2004.13859",
                    "DBLP": "conf/l4dc/WangAB20",
                    "CorpusId": 216641775
                },
                "corpusId": 216641775,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3a1f1b5e1bfe8026df79f772abfaefbf1c6a85b5",
                "title": "A First Principles Approach for Data-Efficient System Identification of Spring-Rod Systems via Differentiable Physics Engines",
                "abstract": "We propose a novel differentiable physics engine for system identification of complex spring-rod assemblies. Unlike black-box data-driven methods for learning the evolution of a dynamical system and its parameters, we modularize the design of our engine using a discrete form of the governing equations of motion, similar to a traditional physics engine. We further reduce the dimension from 3D to 1D for each module, which allows efficient learning of system parameters using linear regression. As a side benefit, the regression parameters correspond to physical quantities, such as spring stiffness or the mass of the rod, making the pipeline explainable. The approach significantly reduces the amount of training data required, and also avoids iterative identification of data sampling and model training. We compare the performance of the proposed engine with previous solutions, and demonstrate its efficacy on tensegrity systems, such as NASA's icosahedron.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2152878792",
                        "name": "Kun Wang"
                    },
                    {
                        "authorId": "2297496",
                        "name": "Mridul Aanjaneya"
                    },
                    {
                        "authorId": "1739036",
                        "name": "Kostas E. Bekris"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Further, (Greydanus et al., 2019) combined Hamiltonian mechanics with neural networks to predict the forward dynamics of conservative mechanical systems."
            ],
            "citingPaper": {
                "paperId": "7ee3ed755a13f024bef0157f91a58749a139f091",
                "externalIds": {
                    "DBLP": "conf/l4dc/GeistT20",
                    "MAG": "3109137980",
                    "ArXiv": "2004.11238",
                    "CorpusId": 216080489
                },
                "corpusId": 216080489,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7ee3ed755a13f024bef0157f91a58749a139f091",
                "title": "Learning Constrained Dynamics with Gauss Principle adhering Gaussian Processes",
                "abstract": "The identification of the constrained dynamics of mechanical systems is often challenging. Learning methods promise to ease an analytical analysis, but require considerable amounts of data for training. We propose to combine insights from analytical mechanics with Gaussian process regression to improve the model's data efficiency and constraint integrity. The result is a Gaussian process model that incorporates a priori constraint knowledge such that its predictions adhere to Gauss' principle of least constraint. In return, predictions of the system's acceleration naturally respect potentially non-ideal (non-)holonomic equality constraints. As corollary results, our model enables to infer the acceleration of the unconstrained system from data of the constrained system and enables knowledge transfer between differing constraint configurations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2064785715",
                        "name": "A. R. Geist"
                    },
                    {
                        "authorId": "2715093",
                        "name": "Sebastian Trimpe"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Multistep neural network (MNN) [33], and Hamiltonian neural network (HNet) [17], are two examples of non-zero target errors.",
                "In order to learn Hamiltonian systems, [17] proposes the HNet to learn a parametric function for H(y).",
                "This often causes them to drift away from the true dynamics of the system as errors accumulate [17]."
            ],
            "citingPaper": {
                "paperId": "47e50929eb9358913fcbc1f4cf50952cbe3ab799",
                "externalIds": {
                    "ArXiv": "2004.13830",
                    "DBLP": "journals/corr/abs-2004-13830",
                    "MAG": "3023117547",
                    "CorpusId": 216641670
                },
                "corpusId": 216641670,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/47e50929eb9358913fcbc1f4cf50952cbe3ab799",
                "title": "Deep Hamiltonian networks based on symplectic integrators",
                "abstract": "HNets is a class of neural networks on grounds of physical prior for learning Hamiltonian systems. This paper explains the influences of different integrators as hyper-parameters on the HNets through error analysis. If we define the network target as the map with zero empirical loss on arbitrary training data, then the non-symplectic integrators cannot guarantee the existence of the network targets of HNets. We introduce the inverse modified equations for HNets and prove that the HNets based on symplectic integrators possess network targets and the differences between the network targets and the original Hamiltonians depend on the accuracy orders of the integrators. Our numerical experiments show that the phase flows of the Hamiltonian systems obtained by symplectic HNets do not exactly preserve the original Hamiltonians, but preserve the network targets calculated; the loss of the network target for the training data and the test data is much less than the loss of the original Hamiltonian; the symplectic HNets have more powerful generalization ability and higher accuracy than the non-symplectic HNets in addressing predicting issues. Thus, the symplectic integrators are of critical importance for HNets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2430811",
                        "name": "Aiqing Zhu"
                    },
                    {
                        "authorId": "121687074",
                        "name": "Pengzhan Jin"
                    },
                    {
                        "authorId": "1805110",
                        "name": "Yifa Tang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "osed method are inspired by interpretations of deep neural networks through the optics of differential equations [33, 17, 49, 30], and neural architectures based on physics priors such as Hamiltonian [27] and Lagrangian [50] networks. System identi\ufb01cation with neural networks: The natural idea of using RNN-based state-space models is motivated by previous works [45, 44, 57, 31]. A related idea of usin"
            ],
            "citingPaper": {
                "paperId": "279848b1d2e6ae3e1c0ac70a198e06c7b343cb5e",
                "externalIds": {
                    "MAG": "3020255553",
                    "DBLP": "journals/corr/abs-2004-11184",
                    "CorpusId": 216080566
                },
                "corpusId": 216080566,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/279848b1d2e6ae3e1c0ac70a198e06c7b343cb5e",
                "title": "Constrained Physics-Informed Deep Learning for Stable System Identification and Control of Unknown Linear Systems",
                "abstract": "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. \nThis paper presents a novel data-driven method for learning deep constrained continuous control policies and dynamical models of linear systems. By leveraging partial knowledge of system dynamics and constraint enforcing multi-objective loss functions, the method can learn from small and static datasets, handle time-varying state and input constraints and enforce the stability properties of the controlled system. We use a continuous control design example to demonstrate the performance of the method on three distinct tasks: system identification, control policy learning, and simultaneous system identification and policy learning. We assess the system identification performance by comparing open-loop simulations of the true system and the learned models. We demonstrate the performance of the policy learning methodology in closed-loop simulations using the system model affected by varying levels of parametric and additive uncertainties. We report superior performance in terms of reference tracking, robustness, and online computational and memory footprints compared with classical control approaches, namely LQR and LQI controllers, and with three variants of model predictive control (MPC) formulations and two traditional MPC solution approaches. We then evaluate the potential of simultaneously learning the system model and control policy. Our empirical results demonstrate the effectiveness of our unifying framework for constrained optimal control of linear systems to provide stability guarantees of the learned dynamics, robustness to uncertainty, and high sampling efficiency.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2403282",
                        "name": "J\u00e1n Drgo\u0148a"
                    },
                    {
                        "authorId": "9583852",
                        "name": "Aaron Tuor"
                    },
                    {
                        "authorId": "1885215",
                        "name": "D. Vrabie"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5c87481aa0a8c33d8f3cc729f08c8c6b2a3cccf3",
                "externalIds": {
                    "DBLP": "journals/mlst/KadupitiyaFJ22",
                    "ArXiv": "2004.06493",
                    "DOI": "10.1088/2632-2153/ac5f60",
                    "CorpusId": 245130901
                },
                "corpusId": 245130901,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5c87481aa0a8c33d8f3cc729f08c8c6b2a3cccf3",
                "title": "Solving Newton\u2019s equations of motion with large timesteps using recurrent neural networks based operators",
                "abstract": "Classical molecular dynamics simulations are based on solving Newton\u2019s equations of motion. Using a small timestep, numerical integrators such as Verlet generate trajectories of particles as solutions to Newton\u2019s equations. We introduce operators derived using recurrent neural networks that accurately solve Newton\u2019s equations utilizing sequences of past trajectory data, and produce energy-conserving dynamics of particles using timesteps up to 4000 times larger compared to the Verlet timestep. We demonstrate significant speedup in many example problems including 3D systems of up to 16 particles.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50575955",
                        "name": "J. Kadupitiya"
                    },
                    {
                        "authorId": "145943296",
                        "name": "Geoffrey Fox"
                    },
                    {
                        "authorId": "143756794",
                        "name": "V. Jadhao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Hamiltonian neural networks [18], Lagrangian neural networks [19], and symplectic RNNs [15] are examples of these DL approaches that have been shown to learn the physics of",
                "[18] Sam Greydanus, Misko Dzamba, and Jason Yosinski, \u201cHamiltonian neural networks,\u201d arXiv preprint arXiv:1906.",
                "put, in some cases leveraging the information that the partial derivatives of the output with respect to inputs are the time derivatives of the inputs [18]."
            ],
            "citingPaper": {
                "paperId": "66537438253d94d25b261d719778a7accaf544fc",
                "externalIds": {
                    "MAG": "3017231357",
                    "DBLP": "journals/corr/abs-2004-06493",
                    "CorpusId": 215754261
                },
                "corpusId": 215754261,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/66537438253d94d25b261d719778a7accaf544fc",
                "title": "Simulating Molecular Dynamics with Large Timesteps using Recurrent Neural Networks",
                "abstract": "Molecular dynamics simulations rely on numerical integrators such as Verlet to solve the Newton's equations of motion. Using a sufficiently small timestep to avoid discretization errors, Verlet integrators generate a trajectory of particle positions as solutions to the equations of motions. We introduce an integrator based on recurrent neural networks that is trained on trajectories generated using Verlet integrator and learns to propagate the dynamics of particles with timestep up to 4000 times larger compared to the Verlet timestep. We demonstrate significant net speedup of up to 32000 for few-particle (1 - 16) 3D systems and over a variety of force fields.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50575955",
                        "name": "J. Kadupitiya"
                    },
                    {
                        "authorId": "145943292",
                        "name": "G. Fox"
                    },
                    {
                        "authorId": "143756794",
                        "name": "V. Jadhao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Also, we plan to explore the \u201crecurrent\u201d versions of physics-informed neural network architectures [33, 34] where in addition to the use of sequence-to-sequence mapping, the infusion of conservation laws in the design of loss functions of",
                "[33] Sam Greydanus, Misko Dzamba, and Jason Yosinski, \u201cHamiltonian neural networks,\u201d arXiv preprint arXiv:1906.",
                "Here, instead of performing explicit time evolution, neural networks observe the positions and momenta and produce the Hamiltonian as output [33]."
            ],
            "citingPaper": {
                "paperId": "680d9bec8c5b3ed4bd5181e514fd45fc3b210ca8",
                "externalIds": {
                    "MAG": "3024238419",
                    "CorpusId": 218674117
                },
                "corpusId": 218674117,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/680d9bec8c5b3ed4bd5181e514fd45fc3b210ca8",
                "title": "Deep Learning Based Integrators for Solving Newton's Equations with Large Timesteps",
                "abstract": "Classical molecular dynamics simulations are based on Newton's equations of motion and rely on numerical integrators to solve them. Using a small timestep to avoid discretization errors, Verlet integrators generate a trajectory of particle positions as solutions to Newton's equations. We introduce an integrator based on deep neural networks that is trained on trajectories generated using the Verlet integrator and learns to propagate the dynamics of particles with timestep up to 4000$\\times$ larger compared to the Verlet timestep. We demonstrate significant net speedup of up to 32000 for 1 - 16 particle 3D systems and over a variety of force fields.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50575955",
                        "name": "J. Kadupitiya"
                    },
                    {
                        "authorId": "145943292",
                        "name": "G. Fox"
                    },
                    {
                        "authorId": "143756794",
                        "name": "V. Jadhao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "3 Hamiltonian neural networks We present here results on the pendulum problem also considered in [22, 24].",
                "Furthermore, building on the recently introduced Hamiltonian neural networks [22, 23], Matsubara et al."
            ],
            "citingPaper": {
                "paperId": "d586e7b86830ac82d2bd36311b60e05b039f5845",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2003-08267",
                    "ArXiv": "2003.08267",
                    "MAG": "3011165391",
                    "DOI": "10.1007/s10543-022-00909-z",
                    "CorpusId": 212747573
                },
                "corpusId": 212747573,
                "publicationVenue": {
                    "id": "434c487e-8dbd-42b7-b803-08b70a454903",
                    "name": "BIT Numerical Mathematics",
                    "type": "journal",
                    "alternate_names": [
                        "Bit Numerical Mathematics",
                        "BIT Numer Math",
                        "Bit Numer Math"
                    ],
                    "issn": "0006-3835",
                    "url": "https://link.springer.com/journal/10543",
                    "alternate_urls": [
                        "http://www.csc.kth.se/BIT/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d586e7b86830ac82d2bd36311b60e05b039f5845",
                "title": "Order theory for discrete gradient methods",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "102675722",
                        "name": "S\u00f8lve Eidnes"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Neural ODEs have also been shown to improve upon previous learning methods in the approximation of physical systems [10], [11]."
            ],
            "citingPaper": {
                "paperId": "6c67ac2a6a79c29620981dba521ead930e562fd8",
                "externalIds": {
                    "ArXiv": "2003.08063",
                    "DBLP": "journals/corr/abs-2003-08063",
                    "MAG": "3011162163",
                    "CorpusId": 212747670
                },
                "corpusId": 212747670,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6c67ac2a6a79c29620981dba521ead930e562fd8",
                "title": "Stable Neural Flows",
                "abstract": "We introduce a provably stable variant of neural ordinary differential equations (neural ODEs) whose trajectories evolve on an energy functional parametrised by a neural network. Stable neural flows provide an implicit guarantee on asymptotic stability of the depth-flows, leading to robustness against input perturbations and low computational burden for the numerical solver. The learning procedure is cast as an optimal control problem, and an approximate solution is proposed based on adjoint sensivity analysis. We further introduce novel regularizers designed to ease the optimization process and speed up convergence. The proposed model class is evaluated on non-linear classification and function approximation tasks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "90467999",
                        "name": "Stefano Massaroli"
                    },
                    {
                        "authorId": "40585370",
                        "name": "Michael Poli"
                    },
                    {
                        "authorId": "8420701",
                        "name": "Michelangelo Bin"
                    },
                    {
                        "authorId": "2085587",
                        "name": "Jinkyoo Park"
                    },
                    {
                        "authorId": "1748130",
                        "name": "A. Yamashita"
                    },
                    {
                        "authorId": "5312699",
                        "name": "H. Asama"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "The Hamiltonian operator in physics is the primary tool for modeling the time evolution of systems with conserved quantities, but until recently the formalism had not been integrated with NNs. Greydanus et al. [112] design a NN architecture that naturally learns and respects energy conservation and other invariance laws in simple mass-spring or pendulum systems.",
                "For example, in NNs, weights are often initialized according to a random distribution prior to training.",
                "For example, RNNs encode temporal invariance and CNNs can implicitly encode spatial translation, rotation, and scale invariance.",
                "This is taken a step further in Toth et al. [268], where they show that not only can NNs learn the Hamiltonian, but also the abstract phase space (assumed to be known in Greydanus et al. [112]) to more effectively model expressive densities in similar physical systems and also extend more generally to other problems in physics.",
                "There is a vast amount of other work using physics-guided architecture towards solving PDEs and other PDE-related applications as well which are not included in this survey (e.g. see ICLR workshop on deep learning for differential equations ([5]))\nA recent direction also relating to conserved or invariant quantities is the incorporation of the Hamiltonian operator into NNs [64, 112, 268, 317].",
                "A similar transfer and adapt approach is seen in Lu et al. [180], but for an ensemble of NNs transferred from related tasks.",
                "More specifically, they demonstrated the encoding of translational symmetries, rotational symmetries, scale invariances, and uniform motion into NNs using customized convolutional layers in CNNs that enforce desired invariance properties.",
                "A recent approach is seen in geophysics where researchers use NNs for the waveform inversion modeling to find subsurface parameters from seismic wave data.",
                "Another common technique in inverse modeling of images (e.g. medical imaging, particle physics imaging), is the use of CNNs as deep image priors [271].",
                "Recently, the Hamiltonian-parameterized NNs above have also been expanded into NN architectures that perform additional differential equation-based integration steps based on the derivatives approximated by the Hamiltonian network [61].",
                "This can take place in many ways, including using domain-informed convolutions for CNNs, additional domain-informed discriminators in GANs, or structures informed by the physical characteristics of the problem.",
                "In a general setting, Wang et al. [281] show how spatiotemporal models can be made more generalizable by incorporating symmetries into deep NNs.",
                "They define a parabolic CNN inspired by anisotropic filtering, a hyperbolic CNN based on Hamiltonian systems, and a second order hyperbolic CNN. Hyperbolic CNNs were found to preserve the energy in the system as intended, which set them apart from parabolic CNNs that\n, Vol. 1, No. 1, Article .",
                ", see ICLR workshop on deep learning for differential equations ([5])) A recent direction also relating to conserved or invariant quantities is the incorporation of the Hamiltonian operator into NNs [64, 112, 268, 317].",
                "Their idea is to use NNs to discover hidden signs of \"simplicity\", such as symmetry or separability in the training data, which enables breaking the massive search space into smaller ones with fewer variables to be determined.",
                "In a pioneering work by Ruthotto et al [236], three variations of CNNs are proposed to improve classifiers for images.",
                "[112]) to more effectively model expressive densities in similar physical systems and also extend more generally to other problems in physics.",
                "To do this, they substitute NN layers into an unrolled version of an existing solution framework which drastically reduced the overall computational cost due to the fast forward evaluation property of NNs, but kept information of the underlying physical models of power grids and of physical constraints.",
                "Schutt et al. [245] proposes continuous-filter convolutional (cfconv) layers for CNNs to allow for modeling objects with arbitrary positions such as atoms in molecules, in contrast to objects described by Cartesian-gridded data such as images.",
                "The modular and flexible nature of NNs in particular makes them prime candidates for architecture modification.",
                "In particular, NN solvers can reduce the high computational demands of traditional numerical methods into a single forward-pass of a NN. Notably, solutions obtained via NNs are also naturally differentiable and have a closed analytic form that can be transferred to any subsequent calculations, a feature not found in more traditional solving methods [159].",
                "This is similar to the common application of pre-training in computer vision, where CNNs are often pre-trained with very large image datasets before being fine-tuned on images from the task at hand [259].",
                "Though these loss functions are mostly seen in common variants of NNs, they are also be seen in architectures such as echo state networks.",
                "[112] designed a NN architecture that naturally learns and respects energy conservation and other invariance laws in simple mass-spring or pendulum systems.",
                "Lagergren et al. [160] expand on this by using ANNs to construct the dictionary of functions."
            ],
            "citingPaper": {
                "paperId": "18b0ca3448bd9703bff86c375362b6ad34f797f7",
                "externalIds": {
                    "ArXiv": "2003.04919",
                    "DBLP": "journals/csur/WillardJXSK23",
                    "DOI": "10.1145/3514228",
                    "CorpusId": 236318599
                },
                "corpusId": 236318599,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/18b0ca3448bd9703bff86c375362b6ad34f797f7",
                "title": "Integrating Scientific Knowledge with Machine Learning for Engineering and Environmental Systems",
                "abstract": "There is a growing consensus that solutions to complex science and engineering problems require novel methodologies that are able to integrate traditional physics-based modeling approaches with state-of-the-art machine learning (ML) techniques. This article provides a structured overview of such techniques. Application-centric objective areas for which these approaches have been applied are summarized, and then classes of methodologies used to construct physics-guided ML models and hybrid physics-ML frameworks are described. We then provide a taxonomy of these existing techniques, which uncovers knowledge gaps and potential crossovers of methods between disciplines that can serve as ideas for future research.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51439518",
                        "name": "J. Willard"
                    },
                    {
                        "authorId": "38139853",
                        "name": "X. Jia"
                    },
                    {
                        "authorId": "4632515",
                        "name": "Shaoming Xu"
                    },
                    {
                        "authorId": "1707756",
                        "name": "M. Steinbach"
                    },
                    {
                        "authorId": "2107978833",
                        "name": "Vipin Kumar"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "In this paper, we follow the same approach as Greydanus et al. (2019), but with the objective of learning a Lagrangian rather than a Hamiltonian so not to restrict the learned kinetic energy.",
                "In the same manner as Greydanus et al. (2019), we can also write a loss function in terms of the discrepancy between x\u0308Lt and x\u0308truet .",
                "Recent work by Greydanus et al. (2019), Toth et al. (2019), and Chen et al. (2019) built on previous approaches of endowing neural networks with physical priors by demonstrating how to learn invariant quantities by approximating a Hamiltonian with a neural network.",
                "In the same manner as Greydanus et al. (2019), we can also write a loss function in terms of the discrepancy between \u1e8dt and \u1e8dtrue t .",
                "This was the core motivation behind Hamiltonian Neural Networks by Greydanus et al. (2019) and Hamiltonian Generative Networks by Toth et al. (2019)."
            ],
            "citingPaper": {
                "paperId": "1926103a9a5c9099b42652c0192b5fcda571d36f",
                "externalIds": {
                    "MAG": "3021457401",
                    "DBLP": "journals/corr/abs-2003-04630",
                    "ArXiv": "2003.04630",
                    "CorpusId": 212644628
                },
                "corpusId": 212644628,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1926103a9a5c9099b42652c0192b5fcda571d36f",
                "title": "Lagrangian Neural Networks",
                "abstract": "Accurate models of the world are built upon notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. Yet even though neural network models see increasing use in the physical sciences, they struggle to learn these symmetries. In this paper, we propose Lagrangian Neural Networks (LNNs), which can parameterize arbitrary Lagrangians using neural networks. In contrast to models that learn Hamiltonians, LNNs do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute. Unlike previous approaches, our method does not restrict the functional form of learned energies and will produce energy-conserving models for a variety of tasks. We test our approach on a double pendulum and a relativistic particle, demonstrating energy conservation where a baseline approach incurs dissipation and modeling relativity without canonical coordinates where a Hamiltonian approach fails. Finally, we show how this model can be applied to graphs and continuous systems using a Lagrangian Graph Network, and demonstrate it on the 1D wave equation.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "32122523",
                        "name": "M. Cranmer"
                    },
                    {
                        "authorId": "14851288",
                        "name": "S. Greydanus"
                    },
                    {
                        "authorId": "7018631",
                        "name": "Stephan Hoyer"
                    },
                    {
                        "authorId": "2019153",
                        "name": "P. Battaglia"
                    },
                    {
                        "authorId": "49071192",
                        "name": "D. Spergel"
                    },
                    {
                        "authorId": "21220036",
                        "name": "S. Ho"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ". Integrating Physics-Based Modeling With Machine Learning: A Survey 15 A recent direction also relating to conserved or invariant quantities is the incorporation of the Hamiltonian operator into NNs [54, 98, 249, 299]. The Hamiltonian operator in physics is the primarytoolformodelingthetimeevolutionofsystemswithconservedquantities,butuntilrecently the formalism had not been integrated with NNs. Greydanus et al. [9",
                " themselves. This is taken a step further in Toth et al. [249], where they show that not only can NNs learn the Hamiltonian, but also the abstract phase space (assumed to be known in Greydanus et al. [98]) to more effectively model expressive densities in similar physical systems and also extend more generally to other problems in physics. Recently, the Hamiltonian-parameterized NNs above have also be"
            ],
            "citingPaper": {
                "paperId": "09e85ad84c4ed40461340ac1bd5fadbd2a5b2340",
                "externalIds": {
                    "MAG": "3010993481",
                    "DBLP": "journals/corr/abs-2003-04919",
                    "CorpusId": 212657607
                },
                "corpusId": 212657607,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/09e85ad84c4ed40461340ac1bd5fadbd2a5b2340",
                "title": "Integrating Physics-Based Modeling with Machine Learning: A Survey",
                "abstract": "In this manuscript, we provide a structured and comprehensive overview of techniques to integrate machine learning with physics-based modeling. First, we provide a summary of application areas for which these approaches have been applied. Then, we describe classes of methodologies used to construct physics-guided machine learning models and hybrid physics-machine learning frameworks from a machine learning standpoint. With this foundation, we then provide a systematic organization of these existing techniques and discuss ideas for future research.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51439518",
                        "name": "J. Willard"
                    },
                    {
                        "authorId": "38139853",
                        "name": "X. Jia"
                    },
                    {
                        "authorId": "4632515",
                        "name": "Shaoming Xu"
                    },
                    {
                        "authorId": "1707756",
                        "name": "M. Steinbach"
                    },
                    {
                        "authorId": "2107978833",
                        "name": "Vipin Kumar"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "8c46959d4ce047b857c351b89c88b08f6833208e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2003-02989",
                    "ArXiv": "2003.02989",
                    "MAG": "3010243192",
                    "CorpusId": 212628244
                },
                "corpusId": 212628244,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8c46959d4ce047b857c351b89c88b08f6833208e",
                "title": "TensorFlow Quantum: A Software Framework for Quantum Machine Learning",
                "abstract": "We introduce TensorFlow Quantum (TFQ), an open source library for the rapid prototyping of hybrid quantum-classical models for classical or quantum data. This framework offers high-level abstractions for the design and training of both discriminative and generative quantum models under TensorFlow and supports high-performance quantum circuit simulators. We provide an overview of the software architecture and building blocks through several examples and review the theory of hybrid quantum-classical neural networks. We illustrate TFQ functionalities via several basic applications including supervised learning for quantum classification, quantum control, and quantum approximate optimization. Moreover, we demonstrate how one can apply TFQ to tackle advanced quantum learning tasks including meta-learning, Hamiltonian learning, and sampling thermal states. We hope this framework provides the necessary tools for the quantum computing and machine learning research communities to explore models of both natural and artificial quantum systems, and ultimately discover new quantum algorithms which could potentially yield a quantum advantage.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "96014865",
                        "name": "M. Broughton"
                    },
                    {
                        "authorId": "102620709",
                        "name": "Guillaume Verdon"
                    },
                    {
                        "authorId": "1387997379",
                        "name": "T. Mccourt"
                    },
                    {
                        "authorId": "2111088987",
                        "name": "Antonio J. Martinez"
                    },
                    {
                        "authorId": "2115662711",
                        "name": "J. Yoo"
                    },
                    {
                        "authorId": "46826180",
                        "name": "S. Isakov"
                    },
                    {
                        "authorId": "2055156577",
                        "name": "Philip Massey"
                    },
                    {
                        "authorId": "10777296",
                        "name": "M. Niu"
                    },
                    {
                        "authorId": "2700608",
                        "name": "R. Halavati"
                    },
                    {
                        "authorId": "70332752",
                        "name": "E. Peters"
                    },
                    {
                        "authorId": "50464368",
                        "name": "M. Leib"
                    },
                    {
                        "authorId": "88908378",
                        "name": "Andrea Skolik"
                    },
                    {
                        "authorId": "82911375",
                        "name": "Michael Streif"
                    },
                    {
                        "authorId": "10721789",
                        "name": "David Von Dollen"
                    },
                    {
                        "authorId": "1933508",
                        "name": "J. McClean"
                    },
                    {
                        "authorId": "48703443",
                        "name": "S. Boixo"
                    },
                    {
                        "authorId": "36577444",
                        "name": "D. Bacon"
                    },
                    {
                        "authorId": "145106485",
                        "name": "A. Ho"
                    },
                    {
                        "authorId": "2665814",
                        "name": "H. Neven"
                    },
                    {
                        "authorId": "145233982",
                        "name": "M. Mohseni"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "This point of view allows one to construct models which enjoy highlevel properties such as time invertibility via Hamiltonian (Greydanus et al., 2019) or Symplectic (Chen et al.",
                "The nonlinear (undamped) pendulum (Hirsch et al., 1974) is a classic textbook example for dynamical systems, which is also used for benchmarking deep models (e.g., Greydanus et al., 2019; Bertalan et al., 2019; Chen et al., 2020).",
                "To this end, a few approaches were recently proposed (Greydanus et al., 2019; Chen et al., 2020) where the obtained dynamics are reversible by construction due to the leapfrog integration.",
                "Other methods attempt to learn conservation laws from data and their associated Hamiltonian representation, leading to exact preservation of energy (Greydanus et al., 2019) and better handling of stiff problems (Chen et al.",
                "This point of view allows one to construct models which enjoy highlevel properties such as time invertibility via Hamiltonian (Greydanus et al., 2019) or Symplectic (Chen et al., 2020; Zhong et al., 2020) networks.",
                "We also outperform the Hamiltonian NN (Greydanus et al., 2019) in all settings.",
                "Other methods attempt to learn conservation laws from data and their associated Hamiltonian representation, leading to exact preservation of energy (Greydanus et al., 2019) and better handling of stiff problems (Chen et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "0872dfacadb282200f909e167e34dd9a7d9f27cd",
                "externalIds": {
                    "ArXiv": "2003.02236",
                    "MAG": "3034565332",
                    "DBLP": "journals/corr/abs-2003-02236",
                    "CorpusId": 211989200
                },
                "corpusId": 211989200,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0872dfacadb282200f909e167e34dd9a7d9f27cd",
                "title": "Forecasting Sequential Data using Consistent Koopman Autoencoders",
                "abstract": "Recurrent neural networks are widely used on time series data, yet such models often ignore the underlying physical structures in such sequences. A new class of physics-based methods related to Koopman theory has been introduced, offering an alternative for processing nonlinear dynamical systems. In this work, we propose a novel Consistent Koopman Autoencoder model which, unlike the majority of existing work, leverages the forward and backward dynamics. Key to our approach is a new analysis which explores the interplay between consistent dynamics and their associated Koopman operators. Our network is directly related to the derived analysis, and its computational requirements are comparable to other baselines. We evaluate our method on a wide range of high-dimensional and short-term dependent problems, and it achieves accurate estimates for significant prediction horizons, while also being robust to noise.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1927247",
                        "name": "Omri Azencot"
                    },
                    {
                        "authorId": "2371914",
                        "name": "N. Benjamin Erichson"
                    },
                    {
                        "authorId": "2060138350",
                        "name": "Vanessa Lin"
                    },
                    {
                        "authorId": "143884206",
                        "name": "Michael W. Mahoney"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2018] as well as graph neural networks coupled with differentiable ODE solvers which have been used to learn the Hamiltonian dynamics of physical systems given their interactions modelled as a dynamic graph [Greydanus et al., 2019].",
                "GNNs coupled with differentiable ODE solvers have been used to learn the Hamiltonian dynamics of physical systems given their interactions modelled as a dynamic graph [Greydanus et al., 2019]."
            ],
            "citingPaper": {
                "paperId": "ad366df3a6641b7144d6af21d7b7d75629641c52",
                "externalIds": {
                    "MAG": "3037471945",
                    "DBLP": "journals/corr/abs-2003-00330",
                    "ArXiv": "2003.00330",
                    "DOI": "10.24963/ijcai.2020/679",
                    "CorpusId": 211677661
                },
                "corpusId": 211677661,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/ad366df3a6641b7144d6af21d7b7d75629641c52",
                "title": "Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective",
                "abstract": "Neural-symbolic computing has now become the subject of interest of both academic and industry research laboratories. \n\nGraph Neural Networks (GNNs) have been widely used in relational and symbolic domains, with widespread application of GNNs in combinatorial optimization, constraint satisfaction, relational reasoning and other scientific domains.\n\nThe need for improved explainability, interpretability and trust of AI systems in general demands principled methodologies, as suggested by neural-symbolic computing. \n\nIn this paper, we review the state-of-the-art on the use of GNNs as a model of neural-symbolic computing. This includes the application of GNNs in several domains as well as their relationship to current developments in neural-symbolic computing.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2335532",
                        "name": "L. Lamb"
                    },
                    {
                        "authorId": "2925941",
                        "name": "A. Garcez"
                    },
                    {
                        "authorId": "145467467",
                        "name": "M. Gori"
                    },
                    {
                        "authorId": "144677268",
                        "name": "Marcelo O. R. Prates"
                    },
                    {
                        "authorId": "144862483",
                        "name": "Pedro H. C. Avelar"
                    },
                    {
                        "authorId": "9083969",
                        "name": "Moshe Y. Vardi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "In this section we analyze how GOKU-net can be used for observed signal extrapolation and ODE parameter identification in three domains: the classic Lotka-Volterra system (Lotka, 1910) with added non-linear emission function; an OpenAI Gym video simulator of a pendulum (Brockman et al., 2016; Greydanus et al., 2019); and a model of the cardiovascular system based on Zenker et al.",
                "We generated the data as in Greydanus et al. (2019), with one important change: the ODE parameter l was uniformly sampled, l \u223c U [1, 2] instead of being constant, making the task much harder.",
                "Machine learning with mechanistic components Closer in spirit to our work is the work by Greydanus et al. (2019) on Hamiltonian neural networks.",
                "As in Greydanus et al. (2019), we pre-processed the observed data such that each frame is of size 28 \u00d7 28.",
                "\u2026parameter identification in three domains: the classic Lotka-Volterra system (Lotka, 1910) with added non-linear emission function; an OpenAI Gym video simulator of a pendulum (Brockman et al., 2016; Greydanus et al., 2019); and a model of the cardiovascular system based on Zenker et al. (2007).",
                ", 2018) not required learned 7 3 HNN (Greydanus et al., 2019) can be used learned 7 3 DSSM (Miladinovi\u0107 et al.",
                "We followed Greydanus et al. (2019) and used the Pendulum-v0 environment from OpenAI Gym (Brockman et al., 2016).",
                "The function f\u03b8f is an ODE model for these physiological variables, such as those presented by Guyton et al. (1972); Smith et al.",
                "HNN by (Greydanus et al., 2019).",
                "We followed Greydanus et al. (2019) and used the Pendulum-v0 environment from OpenAI Gym (Brockman et al.",
                "This includes HNN (Greydanus et al., 2019) which has difficulty with the fact that the ODE parameter is not constant.",
                ", 2016; Greydanus et al., 2019); and a model of the cardiovascular system based on Zenker et al. (2007). In each case we train the model on a set of sequences with varying ODE parameters (\u03b8f ) and initial conditions (z0), and test on unseen sequences with parameters and initial conditions sampled from the same distribution as the train."
            ],
            "citingPaper": {
                "paperId": "158a2f8a168c48005e2277b16dfd2d1d9d248843",
                "externalIds": {
                    "MAG": "3013638491",
                    "DBLP": "conf/chil/LinialRES21",
                    "ArXiv": "2003.10775",
                    "DOI": "10.1145/3450439.3451866",
                    "CorpusId": 214623129
                },
                "corpusId": 214623129,
                "publicationVenue": {
                    "id": "67d171e0-fd12-4512-a35d-c4d7af1bd5b3",
                    "name": "ACM Conference on Health, Inference, and Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CHIL",
                        "ACM Conf Health Inference Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/158a2f8a168c48005e2277b16dfd2d1d9d248843",
                "title": "Generative ODE modeling with known unknowns",
                "abstract": "In several crucial applications, domain knowledge is encoded by a system of ordinary differential equations (ODE), often stemming from underlying physical and biological processes. A motivating example is intensive care unit patients: the dynamics of vital physiological functions, such as the cardiovascular system with its associated variables (heart rate, cardiac contractility and output and vascular resistance) can be approximately described by a known system of ODEs. Typically, some of the ODE variables are directly observed (heart rate and blood pressure for example) while some are unobserved (cardiac contractility, output and vascular resistance), and in addition many other variables are observed but not modeled by the ODE, for example body temperature. Importantly, the unobserved ODE variables are \"known-unknowns\": We know they exist and their functional dynamics, but cannot measure them directly, nor do we know the function tying them to all observed measurements. As is often the case in medicine, and specifically the cardiovascular system, estimating these known-unknowns is highly valuable and they serve as targets for therapeutic manipulations. Under this scenario we wish to learn the parameters of the ODE generating each observed time-series, and extrapolate the future of the ODE variables and the observations. We address this task with a variational autoencoder incorporating the known ODE function, called GOKU-net1 for Generative ODE modeling with Known Unknowns. We first validate our method on videos of single and double pendulums with unknown length or mass; we then apply it to a model of the cardiovascular system. We show that modeling the known-unknowns allows us to successfully discover clinically meaningful unobserved system parameters, leads to much better extrapolation, and enables learning using much smaller training sets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1587757233",
                        "name": "Ori Linial"
                    },
                    {
                        "authorId": "1954568",
                        "name": "D. Eytan"
                    },
                    {
                        "authorId": "2304764",
                        "name": "Uri Shalit"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "As shown in Greydanus et al. (2019), a neural network parametrizing H\u0302\u03b8(z, t) can be learned directly from trajectory data, providing substantial benefits in generalization over directly modeling F (z, t)."
            ],
            "citingPaper": {
                "paperId": "563ca4cda06665f4b90f8fce9bcb28c02e6872b9",
                "externalIds": {
                    "ArXiv": "2002.12880",
                    "DBLP": "journals/corr/abs-2002-12880",
                    "MAG": "3008803199",
                    "CorpusId": 211572990
                },
                "corpusId": 211572990,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/563ca4cda06665f4b90f8fce9bcb28c02e6872b9",
                "title": "Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data",
                "abstract": "The translation equivariance of convolutional layers enables convolutional neural networks to generalize well on image problems. While translation equivariance provides a powerful inductive bias for images, we often additionally desire equivariance to other transformations, such as rotations, especially for non-image data. We propose a general method to construct a convolutional layer that is equivariant to transformations from any specified Lie group with a surjective exponential map. Incorporating equivariance to a new group requires implementing only the group exponential and logarithm maps, enabling rapid prototyping. Showcasing the simplicity and generality of our method, we apply the same model architecture to images, ball-and-stick molecular data, and Hamiltonian dynamical systems. For Hamiltonian systems, the equivariance of our models is especially impactful, leading to exact conservation of linear and angular momentum.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51007156",
                        "name": "Marc Finzi"
                    },
                    {
                        "authorId": "2067201658",
                        "name": "S. Stanton"
                    },
                    {
                        "authorId": "7991830",
                        "name": "Pavel Izmailov"
                    },
                    {
                        "authorId": "145771261",
                        "name": "A. Wilson"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2017) and discrete mechanism (Greydanus et al., 2019; Zhong et al., 2019) when learning problems inherit ar X iv :2 00 2.",
                "It also enables principled architecture design by bringing rich analysis from numerical differential equations (Lu et al., 2017; Long et al., 2017) and discrete mechanism (Greydanus et al., 2019; Zhong et al., 2019) when learning problems inherit\nar X\niv :2\n00 2."
            ],
            "citingPaper": {
                "paperId": "925e6e1e9dec14569c3fe06d238f961443b3b48e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2002-08809",
                    "ArXiv": "2002.08809",
                    "MAG": "3007468665",
                    "CorpusId": 231812484
                },
                "corpusId": 231812484,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/925e6e1e9dec14569c3fe06d238f961443b3b48e",
                "title": "Differential Dynamic Programming Neural Optimizer",
                "abstract": "Interpretation of Deep Neural Networks (DNNs) training as an optimal control problem with nonlinear dynamical systems has received considerable attention recently, yet the algorithmic development remains relatively limited. In this work, we make an attempt along this line by reformulating the training procedure from the trajectory optimization perspective. We first show that most widely-used algorithms for training DNNs can be linked to the Differential Dynamic Programming (DDP), a celebrated second-order trajectory optimization algorithm rooted in the Approximate Dynamic Programming. In this vein, we propose a new variant of DDP that can accept batch optimization for training feedforward networks, while integrating naturally with the recent progress in curvature approximation. The resulting algorithm features layer-wise feedback policies which improve convergence rate and reduce sensitivity to hyper-parameter over existing methods. We show that the algorithm is competitive against state-ofthe-art first and second order methods. Our work opens up new avenues for principled algorithmic design built upon the optimal control theory.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "46218587",
                        "name": "Guan-Horng Liu"
                    },
                    {
                        "authorId": "11126631",
                        "name": "T. Chen"
                    },
                    {
                        "authorId": "1751063",
                        "name": "Evangelos A. Theodorou"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "On the other hand, [16] and [17] have utilized Hamiltonian mechanics for learning dynamics from data."
            ],
            "citingPaper": {
                "paperId": "4afac5dc8d009e831e2718f98e9dff77053f2ce7",
                "externalIds": {
                    "ArXiv": "2002.08860",
                    "DBLP": "journals/corr/abs-2002-08860",
                    "MAG": "3007175346",
                    "CorpusId": 211205165
                },
                "corpusId": 211205165,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4afac5dc8d009e831e2718f98e9dff77053f2ce7",
                "title": "Dissipative SymODEN: Encoding Hamiltonian Dynamics with Dissipation and Control into Deep Learning",
                "abstract": "In this work, we introduce Dissipative SymODEN, a deep learning architecture which can infer the dynamics of a physical system with dissipation from observed state trajectories. To improve prediction accuracy while reducing network size, Dissipative SymODEN encodes the port-Hamiltonian dynamics with energy dissipation and external input into the design of its computation graph and learns the dynamics in a structured way. The learned model, by revealing key aspects of the system, such as the inertia, dissipation, and potential energy, paves the way for energy-based controllers.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35367989",
                        "name": "Yaofeng Desmond Zhong"
                    },
                    {
                        "authorId": "32553531",
                        "name": "Biswadip Dey"
                    },
                    {
                        "authorId": "2019491887",
                        "name": "Amit Chakraborty"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "We show how this can be achieved without introducing additional inductive biases such as (Greydanus et al., 2019) through a synergistic combination of a two\u2013layer Gal\u00ebrkin Neural ODEs and the generalized adjoint with integral loss l(s) := \u2016\u03b2(s)\u2212 z(s)\u20162."
            ],
            "citingPaper": {
                "paperId": "b8db0d2a39ca356abe63a8eabbc5ed9c868f5907",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2002-08071",
                    "MAG": "3007175350",
                    "ArXiv": "2002.08071",
                    "CorpusId": 211171558
                },
                "corpusId": 211171558,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b8db0d2a39ca356abe63a8eabbc5ed9c868f5907",
                "title": "Dissecting Neural ODEs",
                "abstract": "Continuous deep learning architectures have recently re-emerged as variants of Neural Ordinary Differential Equations (Neural ODEs). The infinite-depth approach offered by these models theoretically bridges the gap between deep learning and dynamical systems; however, deciphering their inner working is still an open challenge and most of their applications are currently limited to the inclusion as generic black-box modules. In this work, we \"open the box\" and offer a system-theoretic perspective, including state augmentation strategies and robustness, with the aim of clarifying the influence of several design choices on the underlying dynamics. We also introduce novel architectures: among them, a Galerkin-inspired depth-varying parameter model and neural ODEs with data-controlled vector fields.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "90467999",
                        "name": "Stefano Massaroli"
                    },
                    {
                        "authorId": "40585370",
                        "name": "Michael Poli"
                    },
                    {
                        "authorId": "2085587",
                        "name": "Jinkyoo Park"
                    },
                    {
                        "authorId": "1748130",
                        "name": "A. Yamashita"
                    },
                    {
                        "authorId": "5312699",
                        "name": "H. Asama"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "result"
            ],
            "isInfluential": false,
            "contexts": [
                "We consider a simple environment similar to that studied in Higgins et al. (2018) and Caselles-Dupr\u00e9 et al.",
                "Another requirement we wish to make concerning the representation to be learned is that it is disentangled in the sense of Higgins et al. (2018). Formally, if there exists a subgroup decomposition of G such that G = G1 \u00d7 G2.",
                "\u2026parallel, physics and machine learning have been forging strong ties mostly based upon the Hamiltonian theory and the integration of ordinary differential equations in the latent space to describe the evolution of dynamical systems (Chen et al. (2018), Toth et al. (2019), Greydanus et al. (2019))."
            ],
            "citingPaper": {
                "paperId": "081d2ae593cb23ef0100cda759a5a4e297211382",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2002-06991",
                    "MAG": "3006126551",
                    "ArXiv": "2002.06991",
                    "CorpusId": 211133037
                },
                "corpusId": 211133037,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/081d2ae593cb23ef0100cda759a5a4e297211382",
                "title": "Learning Group Structure and Disentangled Representations of Dynamical Environments",
                "abstract": "Discovering the underlying structure of a dynamical environment involves learning representations that are interpretable and disentangled, which is a challenging task. In physics, interpretable representations of our universe and its underlying dynamics are formulated in terms of representations of groups of symmetry transformations. We propose a physics-inspired method, built upon the theory of group representation, that learns a representation of an environment structured around the transformations that generate its evolution. Experimentally, we learn the structure of explicitly symmetric environments without supervision while ensuring the interpretability of the representations. We show that the learned representations allow for accurate long-horizon predictions and further demonstrate a correlation between the quality of predictions and disentanglement in the latent space.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1402910228",
                        "name": "Robin Quessard"
                    },
                    {
                        "authorId": "2067064746",
                        "name": "T. Barrett"
                    },
                    {
                        "authorId": "37289174",
                        "name": "W. Clements"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Typical ones include Hamiltonian neural networks (Greydanus et al., 2019), Hidden Markov Model (HMM) (Alshamaa et al., 2019; Eddy, 1996), Kalman Filter (KF) (Farahi & Yazdi, 2020; Harvey, 1990; Kalman, 1960) and Particle Filter (PF) (Santos et al., 2019; Djuric et al., 2003), as well as the models\u2026"
            ],
            "citingPaper": {
                "paperId": "902c918934e04a9fc50dbc3b1a8a65c26565399b",
                "externalIds": {
                    "ArXiv": "2002.03513",
                    "DBLP": "conf/icml/MaLZ021",
                    "CorpusId": 211069402
                },
                "corpusId": 211069402,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/902c918934e04a9fc50dbc3b1a8a65c26565399b",
                "title": "Learning Stochastic Behaviour from Aggregate Data",
                "abstract": "Learning nonlinear dynamics from aggregate data is a challenging problem because the full trajectory of each individual is not available, namely, the individual observed at one time may not be observed at the next time point, or the identity of individual is unavailable. This is in sharp contrast to learning dynamics with full trajectory data, on which the majority of existing methods are based. We propose a novel method using the weak form of Fokker Planck Equation (FPE) -- a partial differential equation -- to describe the density evolution of data in a sampled form, which is then combined with Wasserstein generative adversarial network (WGAN) in the training process. In such a sample-based framework we are able to learn the nonlinear dynamics from aggregate data without explicitly solving the partial differential equation (PDE) FPE. We demonstrate our approach in the context of a series of synthetic and real-world data sets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2048487381",
                        "name": "Shaojun Ma"
                    },
                    {
                        "authorId": "2108422879",
                        "name": "Shu Liu"
                    },
                    {
                        "authorId": "145203884",
                        "name": "H. Zha"
                    },
                    {
                        "authorId": "48053919",
                        "name": "Haomin Zhou"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "There are a number of examples of using neural networks to learn the Hamiltonian or Lagrangian of a dynamic system and training this model using the derivatives of the neural network (Greydanus et al., 2019; Lutter et al., 2019; Zhong et al., 2019; Gupta et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "28b14000bc8c91179a02870b65aec3b31d78927d",
                "externalIds": {
                    "MAG": "3004920033",
                    "ArXiv": "2002.01600",
                    "DBLP": "journals/corr/abs-2002-01600",
                    "CorpusId": 211031993
                },
                "corpusId": 211031993,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/28b14000bc8c91179a02870b65aec3b31d78927d",
                "title": "Linearly Constrained Neural Networks",
                "abstract": "We present an approach to designing neural network based models that will explicitly satisfy known linear operator constraints. To achieve this, the target function is modelled as a linear transformation of an underlying function. This transformation is chosen such that any prediction of the target function is guaranteed to satisfy the constraints. The approach is demonstrated on both simulated and real-data examples.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "37792275",
                        "name": "J. Hendriks"
                    },
                    {
                        "authorId": "29911780",
                        "name": "Carl Jidling"
                    },
                    {
                        "authorId": "47846099",
                        "name": "A. Wills"
                    },
                    {
                        "authorId": "1802623",
                        "name": "Thomas Bo Sch\u00f6n"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5c8e124247df991bce0c192a506a18e37709c0c7",
                "externalIds": {
                    "ArXiv": "2001.11107",
                    "DOI": "10.1103/PhysRevE.105.065305",
                    "CorpusId": 237485607,
                    "PubMed": "35854562"
                },
                "corpusId": 237485607,
                "publicationVenue": {
                    "id": "19842b7b-a4d1-4f9a-9714-87d878cf6e73",
                    "name": "Physical Review E",
                    "type": "journal",
                    "alternate_names": [
                        "Phys rev E",
                        "Physical review. E",
                        "Phys Rev E"
                    ],
                    "issn": "1539-3755",
                    "alternate_issns": [
                        "1550-2376",
                        "2470-0045"
                    ],
                    "url": "https://journals.aps.org/pre/",
                    "alternate_urls": [
                        "http://pre.aps.org/",
                        "http://journals.aps.org/pre/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5c8e124247df991bce0c192a506a18e37709c0c7",
                "title": "Hamiltonian neural networks for solving equations\u00a0of motion.",
                "abstract": "There has been a wave of interest in applying machine learning to study dynamical systems. We present a Hamiltonian neural network that solves the differential equations\u00a0that govern dynamical systems. This is an equation-driven machine learning method where the optimization process of the network depends solely on the predicted functions without using any ground truth data. The model learns solutions that satisfy, up to an arbitrarily small error, Hamilton's equations\u00a0and, therefore, conserve the Hamiltonian invariants. The choice of an appropriate activation function drastically improves the predictability of the network. Moreover, an error analysis is derived and states that the numerical errors depend on the overall network performance. The Hamiltonian network is then employed to solve the equations\u00a0for the nonlinear oscillator and the chaotic H\u00e9non-Heiles dynamical system. In both systems, a symplectic Euler integrator requires two orders more evaluation points than the Hamiltonian network to achieve the same order of the numerical error in the predicted phase space trajectories.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145324933",
                        "name": "M. Mattheakis"
                    },
                    {
                        "authorId": "144633639",
                        "name": "David Sondak"
                    },
                    {
                        "authorId": "1735677",
                        "name": "P. Protopapas"
                    }
                ]
            }
        },
        {
            "intents": [
                "result"
            ],
            "isInfluential": false,
            "contexts": [
                "Similar to DeLaN, Hamiltonian Neural Networks (HNN) (Greydanus et al., 2019) predict the Hamiltonian (instead of Lagrangian) of a dynamical system."
            ],
            "citingPaper": {
                "paperId": "54ba51bda108f2a4462e7f6966088360782f79a2",
                "externalIds": {
                    "ArXiv": "2001.08861",
                    "DBLP": "conf/l4dc/SutantoWLMSRM20",
                    "MAG": "3002324011",
                    "CorpusId": 210911768
                },
                "corpusId": 210911768,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/54ba51bda108f2a4462e7f6966088360782f79a2",
                "title": "Encoding Physical Constraints in Differentiable Newton-Euler Algorithm",
                "abstract": "The recursive Newton-Euler Algorithm (RNEA) is a popular technique for computing the dynamics of robots. RNEA can be framed as a differentiable computational graph, enabling the dynamics parameters of the robot to be learned from data via modern auto-differentiation toolboxes. However, the dynamics parameters learned in this manner can be physically implausible. In this work, we incorporate physical constraints in the learning by adding structure to the learned parameters. This results in a framework that can learn physically plausible dynamics via gradient descent, improving the training speed as well as generalization of the learned dynamics models. We evaluate our method on real-time inverse dynamics control tasks on a 7 degree of freedom robot arm, both in simulation and on the real robot. Our experiments study a spectrum of structure added to the parameters of the differentiable RNEA algorithm, and compare their performance and generalization.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "8202756",
                        "name": "Giovanni Sutanto"
                    },
                    {
                        "authorId": "32451493",
                        "name": "Austin S. Wang"
                    },
                    {
                        "authorId": "1491144944",
                        "name": "Yixin Lin"
                    },
                    {
                        "authorId": "2874057",
                        "name": "Mustafa Mukadam"
                    },
                    {
                        "authorId": "1732493",
                        "name": "G. Sukhatme"
                    },
                    {
                        "authorId": "2762463",
                        "name": "Akshara Rai"
                    },
                    {
                        "authorId": "153145615",
                        "name": "Franziska Meier"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Deep Lagrangian Networks [31] and Hamiltonian Networks [32] represent functions in the respective classical mechanics frameworks using deep neural networks."
            ],
            "citingPaper": {
                "paperId": "a532f44a806db3a73b1078130c37bafb801b6328",
                "externalIds": {
                    "ArXiv": "2001.08539",
                    "MAG": "3003152535",
                    "DBLP": "journals/corr/abs-2001-08539",
                    "CorpusId": 208839966
                },
                "corpusId": 208839966,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a532f44a806db3a73b1078130c37bafb801b6328",
                "title": "Automatic Differentiation and Continuous Sensitivity Analysis of Rigid Body Dynamics",
                "abstract": "A key ingredient to achieving intelligent behavior is physical understanding that equips robots with the ability to reason about the effects of their actions in a dynamic environment. Several methods have been proposed to learn dynamics models from data that inform model-based control algorithms. While such learning-based approaches can model locally observed behaviors, they fail to generalize to more complex dynamics and under long time horizons. \nIn this work, we introduce a differentiable physics simulator for rigid body dynamics. Leveraging various techniques for differential equation integration and gradient calculation, we compare different methods for parameter estimation that allow us to infer the simulation parameters that are relevant to estimation and control of physical systems. In the context of trajectory optimization, we introduce a closed-loop model-predictive control algorithm that infers the simulation parameters through experience while achieving cost-minimizing performance.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2064138376",
                        "name": "David Millard"
                    },
                    {
                        "authorId": "6014852",
                        "name": "Eric Heiden"
                    },
                    {
                        "authorId": "2615954",
                        "name": "Shubham Agrawal"
                    },
                    {
                        "authorId": "1732493",
                        "name": "G. Sukhatme"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[14] Sam Greydanus, Misko Dzamba, and Jason Yosinski.",
                "The data efficiency of PINNs has been shown to further improve when encoding structural assumptions like energy conservation by directly modeling the Hamiltonian [14, 15]."
            ],
            "citingPaper": {
                "paperId": "696b388ee6221c6dbcfd647a06883b2bfee773d9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2001-04385",
                    "MAG": "2999026783",
                    "ArXiv": "2001.04385",
                    "DOI": "10.21203/RS.3.RS-55125/V1",
                    "CorpusId": 210164697
                },
                "corpusId": 210164697,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/696b388ee6221c6dbcfd647a06883b2bfee773d9",
                "title": "Universal Differential Equations for Scientific Machine Learning",
                "abstract": "\n In the context of science, the well-known adage \u201ca picture is worth a thousand words\u201d might well be \u201ca model is worth a thousand datasets.\u201d Scientific models, such as Newtonian physics or biological gene regulatory networks, are human-driven simplifications of complex phenomena that serve as surrogates for the countless experiments that validated the models. Recently, machine learning has been able to overcome the inaccuracies of approximate modeling by directly learning the entire set of nonlinear interactions from data. However, without any predetermined structure from the scientific basis behind the problem, machine learning approaches are flexible but data-expensive, requiring large databases of homogeneous labeled training data. A central challenge is reconciling data that is at odds with simplified models without requiring \"big data\". In this work demonstrate how a mathematical object, which we denote universal differential equations (UDEs), can be utilized as a theoretical underpinning to a diverse array of problems in scientific machine learning to yield efficient algorithms and generalized approaches. The UDE model augments scientific models with machine-learnable structures for scientifically-based learning. We show how UDEs can be utilized to discover previously unknown governing equations, accurately extrapolate beyond the original data, and accelerate model simulation, all in a time and data-efficient manner. This advance is coupled with open-source software that allows for training UDEs which incorporate physical constraints, delayed interactions, implicitly-defined events, and intrinsic stochasticity in the model. Our examples show how a diverse set of computationally-difficult modeling issues across scientific disciplines, from automatically discovering biological mechanisms to accelerating the training of physics-informed neural networks and large-eddy simulations, can all be transformed into UDE training problems that are efficiently solved by a single software methodology.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "5365695",
                        "name": "Christopher Rackauckas"
                    },
                    {
                        "authorId": "20859037",
                        "name": "Yingbo Ma"
                    },
                    {
                        "authorId": "118225834",
                        "name": "Julius Martensen"
                    },
                    {
                        "authorId": "1482544386",
                        "name": "Collin Warner"
                    },
                    {
                        "authorId": "123251938",
                        "name": "K. Zubov"
                    },
                    {
                        "authorId": "93421340",
                        "name": "R. Supekar"
                    },
                    {
                        "authorId": "2056678977",
                        "name": "Dominic J. Skinner"
                    },
                    {
                        "authorId": "37288593",
                        "name": "A. Ramadhan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "31e0726182237d47a5eecce749bf9a1f3713e2e9",
                "externalIds": {
                    "DBLP": "journals/nn/JinZZTK20",
                    "MAG": "3081814969",
                    "DOI": "10.1016/j.neunet.2020.08.017",
                    "CorpusId": 219766063,
                    "PubMed": "32890788"
                },
                "corpusId": 219766063,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/31e0726182237d47a5eecce749bf9a1f3713e2e9",
                "title": "SympNets: Intrinsic structure-preserving symplectic networks for identifying Hamiltonian systems",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "121687074",
                        "name": "Pengzhan Jin"
                    },
                    {
                        "authorId": "2116702527",
                        "name": "Zhen Zhang"
                    },
                    {
                        "authorId": "2430811",
                        "name": "Aiqing Zhu"
                    },
                    {
                        "authorId": "1805110",
                        "name": "Yifa Tang"
                    },
                    {
                        "authorId": "1720124",
                        "name": "G. Karniadakis"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Learning physically relevant concepts of the two body problem was recently solved both by Hamiltonian Neural Networks [12, 13] and by very problem specific VAE architectures [14]."
            ],
            "citingPaper": {
                "paperId": "0ca2387bb31b743b6dfe3fc537390c34a226252f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1912-00656",
                    "MAG": "2989686409",
                    "CorpusId": 208527676
                },
                "corpusId": 208527676,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0ca2387bb31b743b6dfe3fc537390c34a226252f",
                "title": "Rodent: Relevance determination in ODE",
                "abstract": "From a set of observed trajectories of a partially observed system, we aim to learn its underlying (physical) process without having to make too many assumptions about the generating model. We start with a very general, over-parameterized ordinary differential equation (ODE) of order N and learn the minimal complexity of the model, by which we mean both the order of the ODE as well as the minimum number of non-zero parameters that are needed to solve the problem. The minimal complexity is found by combining the Variational Auto-Encoder (VAE) with Automatic Relevance Determination (ARD) to the problem of learning the parameters of an ODE which we call Rodent. We show that it is possible to learn not only one specific model for a single process, but a manifold of models representing harmonic signals in general.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "153546490",
                        "name": "Niklas Heim"
                    },
                    {
                        "authorId": "10192900",
                        "name": "V. \u0160m\u00eddl"
                    },
                    {
                        "authorId": "1802358",
                        "name": "T. Pevn\u00fd"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ac31f7c237105862029f498840395a5ca357437b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1911-08856",
                    "ArXiv": "1911.08856",
                    "MAG": "2989999667",
                    "CorpusId": 208175986
                },
                "corpusId": 208175986,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ac31f7c237105862029f498840395a5ca357437b",
                "title": "Learning Generalized Quasi-Geostrophic Models Using Deep Neural Numerical Models",
                "abstract": "We introduce a new strategy designed to help physicists discover hidden laws governing dynamical systems. We propose to use machine learning automatic differentiation libraries to develop hybrid numerical models that combine components based on prior physical knowledge with components based on neural networks. In these architectures, named Deep Neural Numerical Models (DNNMs), the neural network components are used as building-blocks then deployed for learning hidden variables of underlying physical laws governing dynamical systems. In this paper, we illustrate an application of DNNMs to upper ocean dynamics, more precisely the dynamics of a sea surface tracer, the Sea Surface Height (SSH). We develop an advection-based fully differentiable numerical scheme, where parts of the computations can be replaced with learnable ConvNets, and make connections with the single-layer Quasi-Geostrophic (QG) model, a baseline theory in physical oceanography developed decades ago.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1879174",
                        "name": "Redouane Lguensat"
                    },
                    {
                        "authorId": "50520856",
                        "name": "J. Sommer"
                    },
                    {
                        "authorId": "91004451",
                        "name": "Sammy Metref"
                    },
                    {
                        "authorId": "3130195",
                        "name": "E. Cosme"
                    },
                    {
                        "authorId": "1731935",
                        "name": "Ronan Fablet"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5dc913f410bcf84f4656dca6216b4ab047c0dffc",
                "externalIds": {
                    "PubMedCentral": "7652943",
                    "MAG": "2982172145",
                    "DOI": "10.1038/s41598-020-76301-0",
                    "CorpusId": 204837958,
                    "PubMed": "33168886"
                },
                "corpusId": 204837958,
                "publicationVenue": {
                    "id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                    "name": "Scientific Reports",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Rep"
                    ],
                    "issn": "2045-2322",
                    "url": "http://www.nature.com/srep/",
                    "alternate_urls": [
                        "http://www.nature.com/srep/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5dc913f410bcf84f4656dca6216b4ab047c0dffc",
                "title": "Machine learning and serving of discrete field theories",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "145199627",
                        "name": "H. Qin"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4cdda86d5bc2396d0dc766abc955b4bd4de22a24",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1910-10147",
                    "ArXiv": "1910.10147",
                    "MAG": "2993196139",
                    "CorpusId": 210077753
                },
                "corpusId": 210077753,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4cdda86d5bc2396d0dc766abc955b4bd4de22a24",
                "title": "Machine learning and serving of discrete field theories - when artificial intelligence meets the discrete universe",
                "abstract": "A method for machine learning and serving of discrete field theories in physics is developed. The learning algorithm trains a discrete field theory from a set of observational data on a spacetime lattice, and the serving algorithm uses the learned discrete field theory to predict new observations of the field for new boundary and initial conditions. The approach to learn discrete field theories overcomes the difficulties associated with learning continuous theories by artificial intelligence. The serving algorithm of discrete field theories belongs to the family of structure-preserving geometric algorithms, which have been proven to be superior to the conventional algorithms based on discretization of differential equations. The effectiveness of the method and algorithms developed is demonstrated using the examples of nonlinear oscillations and the Kepler problem. In particular, the learning algorithm learns a discrete field theory from a set of data of planetary orbits similar to what Kepler inherited from Tycho Brahe in 1601, and the serving algorithm correctly predicts other planetary orbits, including parabolic and hyperbolic escaping orbits, of the solar system without learning or knowing Newton's laws of motion and universal gravitation. The proposed algorithms are also applicable when effects of special relativity and general relativity are important. The illustrated advantages of discrete field theories relative to continuous theories in terms of machine learning compatibility are consistent with Bostrom's simulation hypothesis.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "145199627",
                        "name": "H. Qin"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": true,
            "contexts": [
                "We replicate the setup from Greydanus et al. (2019) with one key difference: we introduce noise in all observations, rather than only introducing it in (qt,pt) and observing (q\u0307t, p\u0307t) noise free.",
                "For predictions with the baseline NN and HNN, we use the procedure of Greydanus et al. (2019), which uses fourth order Runga-Kutta with an error tolerance of 10\u22129, implemented in scipy.integrate.solve ivp.",
                "We compare our proposed VINs with Hamiltonian neural networks (HNNs) (Greydanus et al., 2019) and standard feed-forward neural networks (NNs) without additional structure that would explicitly incorporate physical or mechanical constraints.",
                "For the mass-spring system, we set the spring constant and mass to k = m = 1, as was done by Greydanus et al. (2019).",
                "As a workaround, Greydanus et al. (2019) proposed to parameterize the dynamical system\u2019s Hamiltonian using a neural network, and to learn it directly from data.",
                "Owing to the precise form of their equations, such networks generally do not conserve these quantities exactly (Greydanus et al., 2019).",
                "Greydanus et al. (2019) demonstrated that this flaw harms the networks\u2019 capacity for accurate long-term prediction.",
                "C.1 Noisy System Observations\nThe setup resembles the one of Greydanus et al. (2019) closely."
            ],
            "citingPaper": {
                "paperId": "2195f61f4c40d724cfeb47498bbb26f176c87390",
                "externalIds": {
                    "DBLP": "conf/aistats/SaemundssonTHD20",
                    "MAG": "3037817062",
                    "ArXiv": "1910.09349",
                    "CorpusId": 213277159
                },
                "corpusId": 213277159,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2195f61f4c40d724cfeb47498bbb26f176c87390",
                "title": "Variational Integrator Networks for Physically Structured Embeddings",
                "abstract": "Learning workable representations of dynamical systems is becoming an increasingly important problem in a number of application areas. By leveraging recent work connecting deep neural networks to systems of differential equations, we propose \\emph{variational integrator networks}, a class of neural network architectures designed to preserve the geometric structure of physical systems. This class of network architectures facilitates accurate long-term prediction, interpretability, and data-efficient learning, while still remaining highly flexible and capable of modeling complex behavior. We demonstrate that they can accurately learn dynamical systems from both noisy observations in phase space and from image pixels within which the unknown dynamics are embedded.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "21803598",
                        "name": "Steind\u00f3r S\u00e6mundsson"
                    },
                    {
                        "authorId": "46846955",
                        "name": "Alexander Terenin"
                    },
                    {
                        "authorId": "1380228856",
                        "name": "Katja Hofmann"
                    },
                    {
                        "authorId": "2261881",
                        "name": "M. Deisenroth"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "We replicate the setup from Greydanus et al. (2019) with one key difference: we introduce noise in all observations, rather than only introducing it in (qt,pt) and observing (q\u0307t, p\u0307t) noise free.",
                "Following Greydanus et al. (2019), we examine two scenarios: (a) a moderate-data regime, where models are trained using 25 training trajectories with a total of 750 data points, (b) a low-data regime using 5 training trajectories with a total of 150 data points.",
                "We represent an embedding in the network by qt = q\u03b8(q1, q2, h, t), (10) which denotes layer t in the VIN, see Figure 1 for an illustration of a VIN.",
                "For predictions with the baseline NN and HNN, we use the procedure of Greydanus et al. (2019), which uses fourth order Runga-Kutta with an error tolerance of 10\u22129, implemented in scipy.integrate.solve ivp.",
                "We\ncompare our proposed VINs with Hamiltonian neural networks (HNNs) (Greydanus et al. 2019) and standard feed-forward neural networks (NNs) without additional structure that would explicitly incorporate physical or mechanical constraints.",
                "For the mass-spring system, we set the spring constant and mass to k = m = 1, as was done by Greydanus et al. (2019).",
                "As a workaround, Greydanus et al. (2019) proposed to parameterize the dynamical system\u2019s Hamiltonian using a neural network, and to learn it directly from data.",
                "D.1 Noisy system observations\nThe setup resembles the one of Greydanus et al. (2019) closely.",
                "Owing to the precise form of their equations, such networks generally cannot learn to conserve these quantities exactly (Greydanus et al. 2019).",
                "It has been recently demonstrated by Greydanus et al. (2019) that this flaw harms the networks\u2019 capacity for accurate long-term prediction."
            ],
            "citingPaper": {
                "paperId": "2472ef334b730fec9a7a83572cf463f5acae47ba",
                "externalIds": {
                    "MAG": "2981069286",
                    "DBLP": "journals/corr/abs-1910-09349",
                    "CorpusId": 204800780
                },
                "corpusId": 204800780,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2472ef334b730fec9a7a83572cf463f5acae47ba",
                "title": "Variational Integrator Networks for Physically Meaningful Embeddings",
                "abstract": "Learning workable representations of dynamical systems is becoming an increasingly important problem in a number of application areas. By leveraging recent work connecting deep neural networks to systems of differential equations, we propose variational integrator networks, a class of neural network architectures designed to ensure faithful representations of the dynamics under study. This class of network architectures facilitates accurate long-term prediction, interpretability, and data-efficient learning, while still remaining highly flexible and capable of modeling complex behavior. We demonstrate that they can accurately learn dynamical systems from both noisy observations in phase space and from image pixels within which the unknown dynamics are embedded.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "21803598",
                        "name": "Steind\u00f3r S\u00e6mundsson"
                    },
                    {
                        "authorId": "46846955",
                        "name": "Alexander Terenin"
                    },
                    {
                        "authorId": "1380228856",
                        "name": "Katja Hofmann"
                    },
                    {
                        "authorId": "2261881",
                        "name": "M. Deisenroth"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4628933c1c35d5c4d849a2f0a73278e49b0f6a2c",
                "externalIds": {
                    "MAG": "2981925162",
                    "DOI": "10.3390/app9204443",
                    "CorpusId": 207969999
                },
                "corpusId": 207969999,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4628933c1c35d5c4d849a2f0a73278e49b0f6a2c",
                "title": "Modal-Energy-Based Neuro-Controller for Seismic Response Reduction of a Nonlinear Building Structure",
                "abstract": "This study presents a neuro-control algorithm based on structural modal energy that outputs an optimal control signal to reduce vibration during earthquakes. The modal energy of a structure is used in the objective function during the training process of a neural network. The modal energy and control signal are then minimized by the proposed neuro-control technique. A three-story nonlinear building was installed with an active mass damper, which was used to verify the applicability of the proposed control algorithm. The El Centro earthquake was adopted to train the modal-energy-based neuro-controller. The six recorded earthquakes were employed to consider unknown earthquake effects after training. The results obtained from the proposed control algorithm were compared with those obtained from a non-controlled response and a multilayer perceptron. The numerical results show that the proposed control algorithm is quite effective in reducing the structural response and modal energy. While nonlinear hysteretic behaviors appear in the non-controlled responses, these nonlinear behaviors almost entirely disappear with control.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "72197915",
                        "name": "Seongkyu Chang"
                    },
                    {
                        "authorId": "146554504",
                        "name": "Deokyong Sung"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "03547cf81db895a448c3d0283bdfa20695ed26ab",
                "externalIds": {
                    "MAG": "3137474564",
                    "DBLP": "journals/corr/abs-1910-03193",
                    "ArXiv": "1910.03193",
                    "DOI": "10.1038/s42256-021-00302-5",
                    "CorpusId": 233822586
                },
                "corpusId": 233822586,
                "publicationVenue": {
                    "id": "6457124b-39bf-4d02-bff4-73752ff21562",
                    "name": "Nature Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "Nat Mach Intell"
                    ],
                    "issn": "2522-5839",
                    "url": "https://www.nature.com/natmachintell/"
                },
                "url": "https://www.semanticscholar.org/paper/03547cf81db895a448c3d0283bdfa20695ed26ab",
                "title": "Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2149373363",
                        "name": "Lu Lu"
                    },
                    {
                        "authorId": "121687074",
                        "name": "Pengzhan Jin"
                    },
                    {
                        "authorId": "2867058",
                        "name": "G. Pang"
                    },
                    {
                        "authorId": "150358909",
                        "name": "Zhongqiang Zhang"
                    },
                    {
                        "authorId": "1720124",
                        "name": "G. Karniadakis"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": true,
            "contexts": [
                " 3:55 9:8 3:72 Table1: AveragepixelMSEovera30stepunrollonthetrainandtestdataonfourphysicalsystems. All values are multiplied by 1e+4. We evaluate two versions of the Hamiltonian Neural Network (HNN) (Greydanus et al., 2019): the original architecture and a convolutional version closely matched to the architecture of HGN. We also compare four versions of our proposed Hamiltonian Generative Network (HGN): the full version",
                "In the experiments presented here, we reimplemented the PixelHNN architecture as described in Greydanus et al. (2019) and trained it using the full loss (15).",
                "energy of the system denoted as a radius rin the phase space,beforesamplingtheinitialstate(q;p)uniformlyonthecircleofradiusr. Notethatourpendulum dataset is more challenging than the one described in Greydanus et al. (2019), where the pendulum had a \ufb01xed radius and was initialized at a maximum angle of 30from the central axis. Mass-spring. The dynamics of a frictionless mass-spring system are modeled by the Hamiltonian",
                " in classical mechanics but which can still affect their behavior or function, like the colour or shape of an object. The \ufb01rst question was recently addressed by the Hamiltonian Neural Network (HNN) (Greydanus et al., 2019) approach, which was able to learn the Hamiltonian of three simple physical systems from noisy phase space observations. However, to address the second question, HNN makes assumptions that restrict it",
                "One of the most comparable approaches to ours is the Hamiltonian Neural Network (HNN) (Greydanus et al., 2019).",
                "To ensure that our re-implementation of HNN was correct, we replicated all the results presented in the original paper (Greydanus et al., 2019) by verifying that it could learn the dynamics of the mass-spring, pendulum and two-body systems well from the ground truth state, and the dynamics of a\u2026",
                "A.4 DATASETS\nThe datasets for the experiments described in 4 were generated in a similar manner to Greydanus et al. (2019) for comparative purposes.",
                "The first question was recently addressed by the Hamiltonian Neural Network (HNN) (Greydanus et al., 2019) approach, which was able to learn the Hamiltonian of three simple physical systems from noisy phase space observations.",
                "\u2026phase space, before sampling the initial state (q,p) uniformly on the circle of radius r. Note that our pendulum dataset is more challenging than the one described in Greydanus et al. (2019), where the pendulum had a fixed radius and was initialized at a maximum angle of 30\u25e6 from the central axis.",
                "A.3 HAMILTONIAN NEURAL NETWORK\nThe Hamiltonian Neural Network (HNN) (Greydanus et al., 2019) learns a differentiable function H(q,p) that maps a system\u2019s state in phase space (its position q and momentum p) to a scalar quantity interpreted as the system\u2019s Hamiltonian.",
                "In order to directly compare the performance of HGN to that of its closest baseline, HNN, we generated four datasets analogous to the data used in Greydanus et al. (2019)."
            ],
            "citingPaper": {
                "paperId": "80beec251b5d9f4f78fca2ea2016cf9d763b844c",
                "externalIds": {
                    "MAG": "2976349204",
                    "ArXiv": "1909.13789",
                    "DBLP": "journals/corr/abs-1909-13789",
                    "CorpusId": 203593936
                },
                "corpusId": 203593936,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/80beec251b5d9f4f78fca2ea2016cf9d763b844c",
                "title": "Hamiltonian Generative Networks",
                "abstract": "The Hamiltonian formalism plays a central role in classical and quantum physics. Hamiltonians are the main tool for modelling the continuous time evolution of systems with conserved quantities, and they come equipped with many useful properties, like time reversibility and smooth interpolation in time. These properties are important for many machine learning problems - from sequence prediction to reinforcement learning and density modelling - but are not typically provided out of the box by standard tools such as recurrent neural networks. In this paper, we introduce the Hamiltonian Generative Network (HGN), the first approach capable of consistently learning Hamiltonian dynamics from high-dimensional observations (such as images) without restrictive domain assumptions. Once trained, we can use HGN to sample new trajectories, perform rollouts both forward and backward in time and even speed up or slow down the learned dynamics. We demonstrate how a simple modification of the network architecture turns HGN into a powerful normalising flow model, called Neural Hamiltonian Flow (NHF), that uses Hamiltonian dynamics to model expressive densities. We hope that our work serves as a first practical demonstration of the value that the Hamiltonian formalism can bring to deep learning.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "50399980",
                        "name": "Peter Toth"
                    },
                    {
                        "authorId": "1748523",
                        "name": "Danilo Jimenez Rezende"
                    },
                    {
                        "authorId": "2689633",
                        "name": "Andrew Jaegle"
                    },
                    {
                        "authorId": "2026995",
                        "name": "S. Racani\u00e8re"
                    },
                    {
                        "authorId": "3436640",
                        "name": "Aleksandar Botev"
                    },
                    {
                        "authorId": "39051054",
                        "name": "I. Higgins"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "00c6a6bd4b5d8a66b088fa1363a865624037781a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1910-00024",
                    "ArXiv": "1910.00024",
                    "MAG": "2979026759",
                    "DOI": "10.1103/PhysRevX.10.021020",
                    "CorpusId": 203610605
                },
                "corpusId": 203610605,
                "publicationVenue": {
                    "id": "98eedf55-1e67-4c3d-a25d-79861b87ae04",
                    "name": "Physical Review X",
                    "type": "journal",
                    "alternate_names": [
                        "Phys Rev X"
                    ],
                    "issn": "2160-3308",
                    "url": "https://journals.aps.org/prx/",
                    "alternate_urls": [
                        "http://journals.aps.org/prx/",
                        "http://prx.aps.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/00c6a6bd4b5d8a66b088fa1363a865624037781a",
                "title": "Neural Canonical Transformation with Symplectic Flows",
                "abstract": "Canonical transformation plays a fundamental role in simplifying and solving classical Hamiltonian systems. We construct flexible and powerful canonical transformations as generative models using symplectic neural networks. The model transforms physical variables towards a latent representation with an independent harmonic oscillator Hamiltonian. Correspondingly, the phase space density of the physical system flows towards a factorized Gaussian distribution in the latent space. Since the canonical transformation preserves the Hamiltonian evolution, the model captures nonlinear collective modes in the learned latent representation. We present an efficient implementation of symplectic neural coordinate transformations and two ways to train the model. The variational free energy calculation is based on the analytical form of physical Hamiltonian. While the phase space density estimation only requires samples in the coordinate space for separable Hamiltonians. We demonstrate appealing features of neural canonical transformation using toy problems including two-dimensional ring potential and harmonic chain. Finally, we apply the approach to real-world problems such as identifying slow collective modes in alanine dipeptide and conceptual compression of the MNIST dataset.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2109135534",
                        "name": "Shuo-Hui Li"
                    },
                    {
                        "authorId": "2113541017",
                        "name": "Chen Dong"
                    },
                    {
                        "authorId": "2125538501",
                        "name": "Linfeng Zhang"
                    },
                    {
                        "authorId": "48169641",
                        "name": "Lei Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Most closely related to ours, Greydanus et al. (2019) use a neural network to learn the Hamiltonian of the dynamical system in such a way that its partial derivatives match the time derivatives of the position and momentum variables, which are both assumed to be observed.",
                "A recent paper (Greydanus et al., 2019), Hamiltonian Neural Networks (HNN), proposes to do so representing the Hamiltonian function H(q, p) as a multilayer neural network.",
                "For instance, with our terminology, the HNN model of Greydanus et al. (2019) is a \u201csingle-step E-E H-NET\u201d with the additional subtlety that they supervise the training with actual derivatives instead of relying on finite differences between successive steps of the observed trajectories.",
                "1 Introduction Can machines learn physical laws from data? A recent paper (Greydanus et al., 2019), Hamiltonian Neural Networks (HNN), proposes to do so representing the Hamiltonian function H(q, p) as a multilayer neural network.",
                "Moreover, Greydanus et al. (2019) mentions that HNN does not outperform a baseline method using O-NET in learning the three-body system\u2019s evolution.",
                "Here, we test our SRNN together with other baselines on the noiseless three-body system with the same configurations as Greydanus et al. (2019).",
                "For instance, Greydanus et al. (2019) trains H-NET in a fully supervised manner using the observed tuples (p, q, p\u0307, q\u0307)."
            ],
            "citingPaper": {
                "paperId": "ffb1b305dfd84b81999da356cd8f9790636414df",
                "externalIds": {
                    "ArXiv": "1909.13334",
                    "MAG": "2995774847",
                    "DBLP": "journals/corr/abs-1909-13334",
                    "CorpusId": 203593991
                },
                "corpusId": 203593991,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ffb1b305dfd84b81999da356cd8f9790636414df",
                "title": "Symplectic Recurrent Neural Networks",
                "abstract": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by a neural network and furthermore leverages symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show that SRNNs succeed reliably on complex and noisy Hamiltonian systems. We also show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "8157979",
                        "name": "Zhengdao Chen"
                    },
                    {
                        "authorId": "2108049998",
                        "name": "Jianyu Zhang"
                    },
                    {
                        "authorId": "2877311",
                        "name": "Mart\u00edn Arjovsky"
                    },
                    {
                        "authorId": "52184096",
                        "name": "L. Bottou"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "57784e5db3504d549d16382de8f7f4ad222b3d71",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1909-12790",
                    "MAG": "2975784920",
                    "ArXiv": "1909.12790",
                    "CorpusId": 203591407
                },
                "corpusId": 203591407,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/57784e5db3504d549d16382de8f7f4ad222b3d71",
                "title": "Hamiltonian Graph Networks with ODE Integrators",
                "abstract": "We introduce an approach for imposing physically informed inductive biases in learned simulation models. We combine graph networks with a differentiable ordinary differential equation integrator as a mechanism for predicting future states, and a Hamiltonian as an internal representation. We find that our approach outperforms baselines without these biases in terms of predictive accuracy, energy accuracy, and zero-shot generalization to time-step sizes and integrator orders not experienced during training. This advances the state-of-the-art of learned simulation, and in principle is applicable beyond physical domains.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1398105826",
                        "name": "Alvaro Sanchez-Gonzalez"
                    },
                    {
                        "authorId": "2603033",
                        "name": "V. Bapst"
                    },
                    {
                        "authorId": "11638962",
                        "name": "K. Cranmer"
                    },
                    {
                        "authorId": "2019153",
                        "name": "P. Battaglia"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "A more recent (concurrent) work by Greydanus et al. (2019) uses Hamiltonian mechanics to learn the dynamics of autonomous, energy-conserved mechanical systems from time-series data of position, momentum, and their derivatives.",
                "In the HNN paper Greydanus et al. (2019), the initial conditions of the trajectories are generated randomly in an annulus, whereas in this paper, we generate the initial state conditions uniformly in a reasonable range in each state dimension.",
                "In Hamiltonian Neural Networks (HNN), Greydanus et al. (2019) incorporate the Hamiltonian structure into learning by minimizing the difference between the symplectic gradients and the true gradients.",
                "Previous approaches (Lutter et al., 2019; Greydanus et al., 2019) require data in the form of generalized coordinates and their derivatives up to the second order."
            ],
            "citingPaper": {
                "paperId": "29edf1f3bf744aa1a1b7bea3c1fb455bad8fc898",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1909-12077",
                    "MAG": "2975293371",
                    "ArXiv": "1909.12077",
                    "CorpusId": 202889233
                },
                "corpusId": 202889233,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/29edf1f3bf744aa1a1b7bea3c1fb455bad8fc898",
                "title": "Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control",
                "abstract": "In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system, given by an ordinary differential equation (ODE), from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consistent models for physical systems, opens up new possibilities for synthesizing model-based control strategies.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "35367989",
                        "name": "Yaofeng Desmond Zhong"
                    },
                    {
                        "authorId": "32553531",
                        "name": "Biswadip Dey"
                    },
                    {
                        "authorId": "2019491887",
                        "name": "Amit Chakraborty"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "12b7f49fa2265aee96d6725de39a6f238d7ca8b6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1909-02702",
                    "ArXiv": "1909.02702",
                    "MAG": "3010839561",
                    "DOI": "10.1109/CDC40024.2019.9030017",
                    "CorpusId": 202538857
                },
                "corpusId": 202538857,
                "publicationVenue": {
                    "id": "ab066af1-bfee-42da-84bb-42f7e199d0d0",
                    "name": "IEEE Conference on Decision and Control",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Decision and Control",
                        "IEEE Conf Decis Control",
                        "Conf Decis Control",
                        "CDC"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=403"
                },
                "url": "https://www.semanticscholar.org/paper/12b7f49fa2265aee96d6725de39a6f238d7ca8b6",
                "title": "Port\u2013Hamiltonian Approach to Neural Network Training",
                "abstract": "Neural networks are discrete entities: subdivided into discrete layers and parametrized by weights which are iteratively optimized via difference equations. Recent work proposes networks with layer outputs which are no longer quantized but are solutions of an ordinary differential equation (ODE); however, these networks are still optimized via discrete methods (e.g. gradient descent). In this paper, we explore a different direction: namely, we propose a novel framework for learning in which the parameters themselves are solutions of ODEs. By viewing the optimization process as the evolution of a port-Hamiltonian system, we can ensure convergence to a minimum of the objective function. Numerical experiments have been performed to show the validity and effectiveness of the proposed methods.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "90467999",
                        "name": "Stefano Massaroli"
                    },
                    {
                        "authorId": "40585370",
                        "name": "Michael Poli"
                    },
                    {
                        "authorId": "21857982",
                        "name": "Federico Califano"
                    },
                    {
                        "authorId": "1806339",
                        "name": "Angela Faragasso"
                    },
                    {
                        "authorId": "2085587",
                        "name": "Jinkyoo Park"
                    },
                    {
                        "authorId": "1748130",
                        "name": "A. Yamashita"
                    },
                    {
                        "authorId": "5312699",
                        "name": "H. Asama"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026networks that can learn arbitrary conservation laws (hyperbolic conservation laws (Raissi, Perdikaris, and Karniadakis 2019), Hamiltonian dynamics (Greydanus, Dzamba, and Yosinski 2019; Toth et al. 2019; Chen et al. 2019), Lagrangian dynamics (Cranmer et al. 2020)), or (2) designing a loss\u2026"
            ],
            "citingPaper": {
                "paperId": "02327b889e75a9d86d4351cfea7c17e730d8efa4",
                "externalIds": {
                    "ArXiv": "1909.09754",
                    "DBLP": "conf/aaai/LeeC21",
                    "MAG": "2974114687",
                    "DOI": "10.2172/1569346",
                    "CorpusId": 202718772
                },
                "corpusId": 202718772,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/02327b889e75a9d86d4351cfea7c17e730d8efa4",
                "title": "Deep Conservation: A Latent-Dynamics Model for Exact Satisfaction of Physical Conservation Laws",
                "abstract": "This work proposes an approach for latent-dynamics learning that exactly enforces physical conservation laws. The method comprises two steps. First, the method computes a low-dimensional embedding of the high-dimensional dynamical-system state using deep convolutional autoencoders. This defines a low-dimensional nonlinear manifold on which the state is subsequently enforced to evolve. Second, the method defines a latent-dynamics model that associates with the solution to a constrained optimization problem. Here, the objective function is defined as the sum of squares of conservation-law violations over control volumes within a finite-volume discretization of the problem; nonlinear equality constraints explicitly enforce conservation over prescribed subdomains of the problem. Under modest conditions, the resulting dynamics model guarantees that the time-evolution of the latent state exactly satisfies conservation laws over the prescribed subdomains.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "3041967",
                        "name": "Kookjin Lee"
                    },
                    {
                        "authorId": "33892852",
                        "name": "K. Carlberg"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "8cc693a47f717ed9f680f560c6fb71514d9a5a7b",
                "externalIds": {
                    "MAG": "2966473221",
                    "ArXiv": "1907.12715",
                    "DOI": "10.1063/1.5128231",
                    "CorpusId": 198985969,
                    "PubMed": "31893645"
                },
                "corpusId": 198985969,
                "publicationVenue": {
                    "id": "30c0ded7-c8b4-473c-bbc0-f237234ac1a6",
                    "name": "Chaos",
                    "type": "journal",
                    "issn": "1054-1500",
                    "url": "http://chaos.aip.org/",
                    "alternate_urls": [
                        "https://aip.scitation.org/journal/cha"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8cc693a47f717ed9f680f560c6fb71514d9a5a7b",
                "title": "On learning Hamiltonian systems from data.",
                "abstract": "Concise, accurate descriptions of physical systems through their conserved quantities abound in the natural sciences. In data science, however, current research often focuses on regression problems, without routinely incorporating additional assumptions about the system that generated the data. Here, we propose to explore a particular type of underlying structure in the data: Hamiltonian systems, where an \"energy\" is conserved. Given a collection of observations of such a Hamiltonian system over time, we extract phase space coordinates and a Hamiltonian function of them that acts as the generator of the system dynamics. The approach employs an autoencoder neural network component to estimate the transformation from observations to the phase space of a Hamiltonian system. An additional neural network component is used to approximate the Hamiltonian function on this constructed space, and the two components are trained jointly. As an alternative approach, we also demonstrate the use of Gaussian processes for the estimation of such a Hamiltonian. After two illustrative examples, we extract an underlying phase space as well as the generating Hamiltonian from a collection of movies of a pendulum. The approach is fully data-driven and does not assume a particular form of the Hamiltonian function.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "31588522",
                        "name": "Tom S. Bertalan"
                    },
                    {
                        "authorId": "144399320",
                        "name": "Felix Dietrich"
                    },
                    {
                        "authorId": "31263739",
                        "name": "Igor Mezi'c"
                    },
                    {
                        "authorId": "3439407",
                        "name": "I. Kevrekidis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Note added: Following submission of this work, the closely related preprint (Greydanus et al., 2019) appeared."
            ],
            "citingPaper": {
                "paperId": "7630e18d5fe4489747421d3ce9a44129eaff7d5e",
                "externalIds": {
                    "MAG": "2951333726",
                    "DBLP": "journals/corr/abs-1906-04645",
                    "ArXiv": "1906.04645",
                    "CorpusId": 184488308
                },
                "corpusId": 184488308,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7630e18d5fe4489747421d3ce9a44129eaff7d5e",
                "title": "Learning Symmetries of Classical Integrable Systems",
                "abstract": "The solution of problems in physics is often facilitated by a change of variables. In this work we present neural transformations to learn symmetries of Hamiltonian mechanical systems. Maintaining the Hamiltonian structure requires novel network architectures that parametrize symplectic transformations. We demonstrate the utility of these architectures by learning the structure of integrable models. Our work exemplifies the adaptation of neural transformations to a family constrained by more than the condition of invertibility, which we expect to be a common feature of applications of these methods.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "6715047",
                        "name": "R. Bondesan"
                    },
                    {
                        "authorId": "7702763",
                        "name": "A. Lamacraft"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Most previous studies focused on these two types of systems [8, 19, 38, 44, 45].",
                "The algorithm is applicable to any computational graph such as convolutional neural network [19] and graph neural network [12], and thereby one can handle extended tasks or further improve the modeling accuracy.",
                "Meanwhile, most previous studies aimed to model continuous-time differential equations and employed numerical integrators (typically, an explicit Runge\u2013Kutta method) to integrate the neural network models for learning and computing the dynamics [7, 8, 19, 45].",
                "The HNN is a neural network where the output represents the system energy H , and its gradient with respect to the input state ~u is used for the time-derivative [19].",
                "Most studies have focused on one of the first two systems [19, 44, 45] under special conditions [8, 38, 41], or they are too general to model the conservation and dissipation laws [7, 34].",
                "Previous models interpolate the discrete-time data using numerical integrators for learning and computing [8, 19, 38, 41, 44, 45].",
                "The Hamiltonian neural network (HNN) [19] implements the Hamiltonian structure on a neural network and thereby produces the energy conservation law in physics.",
                "The HNN approximates an energy function H from the data using a neural network, and thereby, builds a Hamiltonian system [19].",
                "Continuous Calculus Discrete Calculus approximation by numerical integrators [8,19,38,41,44,45] discrete approximation [5,6,15-18,24,32,33] physical phenomena discrete-time modeling",
                "HNN SymODEN Dissipative SRNN/VIN DGNet [19] [45] [44] [8, 38, 41] (this paper) Hamiltonian system yes yes yes yes yes Dissipative ODE yes yes Hamiltonian PDE yes Dissipative PDE yes Learning from finite difference approx."
            ],
            "citingPaper": {
                "paperId": "963973dfb5187e14c5d26bc9e72fb6eaf292b459",
                "externalIds": {
                    "DBLP": "conf/nips/MatsubaraIY20",
                    "MAG": "3036215402",
                    "CorpusId": 219793044
                },
                "corpusId": 219793044,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/963973dfb5187e14c5d26bc9e72fb6eaf292b459",
                "title": "Deep Energy-based Modeling of Discrete-Time Physics",
                "abstract": "Physical phenomena in the real world are often described by energy-based modeling theories, such as Hamiltonian mechanics or the Landau theory, which yield various physical laws. Recent developments in neural networks have enabled the mimicking of the energy conservation law by learning the underlying continuous-time differential equations. However, this may not be possible in discrete time, which is often the case in practical learning and computation. Moreover, other physical laws have been overlooked in the previous neural network models. In this study, we propose a deep energy-based physical model that admits a specific differential geometric structure. From this structure, the conservation or dissipation law of energy and the mass conservation law follow naturally. To ensure the energetic behavior in discrete time, we also propose an automatic discrete differential algorithm that enables neural networks to employ the discrete gradient method.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "144872058",
                        "name": "Takashi Matsubara"
                    },
                    {
                        "authorId": "49346000",
                        "name": "Ai Ishikawa"
                    },
                    {
                        "authorId": "2580628",
                        "name": "Takaharu Yaguchi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "The use of ordinary differential equation (ODE) solvers within deep learning frameworks has allowed end-to-end training of Neural ODEs (Chen et al., 2018) in a variety of settings.",
                "We consider parametrizing event functions with neural networks in the context of solving ODEs, extending Neural ODEs to implicitly defined termination times.",
                "\u20262020), generative modeling (Grathwohl et al., 2018; Zhang et al., 2018; Chen & Duvenaud, 2019; Onken et al., 2020), time series modeling (Rubanova et al., 2019; De Brouwer et al., 2019; Jia & Benson, 2019; Kidger et al., 2020), and physics-based models (Zhong et al., 2019; Greydanus et al., 2019).",
                "Though there exist physics-motivated parameterizations of Neural ODEs (e.g. Zhong et al., 2019; Greydanus et al., 2019; Finzi et al., 2020), these cannot model collision effects.",
                ", 2020), and physics-based models (Zhong et al., 2019; Greydanus et al., 2019).",
                "By introducing differentiable termination criteria in Neural ODEs, our approach allows the model to efficiently and automatically handle state discontinuities.",
                "To further expand the applications of Neural ODEs, we investigate the parameterization and learning of a termination criteria, such that the termination time is only implicitly defined and will depend on changes in the continuous-time state."
            ],
            "citingPaper": {
                "paperId": "7dc3b121a03c6a97ce72ae12d53e24d461df2b81",
                "externalIds": {
                    "DOI": "10.1017/9781316338636.008",
                    "CorpusId": 241274223
                },
                "corpusId": 241274223,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7dc3b121a03c6a97ce72ae12d53e24d461df2b81",
                "title": "Ordinary Differential Equations",
                "abstract": "The existing Neural ODE formulation relies on an explicit knowledge of the termination time. We extend Neural ODEs to implicitly defined termination criteria modeled by neural event functions, which can be chained together and differentiated through. Neural Event ODEs are capable of modeling discrete and instantaneous changes in a continuous-time system, without prior knowledge of when these changes should occur or how many such changes should exist. We test our approach in modeling hybrid discreteand continuoussystems such as switching dynamical systems and collision in multi-body systems, and we propose simulation-based training of point processes with applications in discrete control.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "51466615",
                        "name": "Ricky T. Q. Chen"
                    },
                    {
                        "authorId": "1773498",
                        "name": "Brandon Amos"
                    },
                    {
                        "authorId": "1729762",
                        "name": "Maximilian Nickel"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                ", 2019), Hamiltonian neural networks (HNN) (Sanchez-Gonzalez et al., 2019; Greydanus et al., 2019; Zhong et al., 2020; 2021), and neural ODE (NODE) (Chen et al.",
                "To this extent, three broad approaches have been proposed, namely, Lagrangian neural networks (LNN) (Cranmer et al., 2020a; Finzi et al., 2020; Lutter et al., 2019), Hamiltonian neural networks (HNN) (Sanchez-Gonzalez et al., 2019; Greydanus et al., 2019; Zhong et al., 2020; 2021), and neural ODE (NODE) (Chen et al., 2018; Gruver et al., 2021).",
                "Thus, a NODE with a second-order bias can give similar performances to\n\u2217SB: School of Interdisciplinary Research, SR: Department of Computer Science, NMAK and RB: Department of Civil Engineering, J: Department of Electrical Engineering, SR and NMAK: Yardi School of Artificial Intelligence (joint appointment).\nthat of an HNN (Gruver et al., 2021).",
                "\u2026proposed, namely, Lagrangian neural networks (LNN) (Cranmer et al., 2020a; Finzi et al., 2020; Lutter et al., 2019), Hamiltonian neural networks (HNN) (Sanchez-Gonzalez et al., 2019; Greydanus et al., 2019; Zhong et al., 2020; 2021), and neural ODE (NODE) (Chen et al., 2018; Gruver et al., 2021).",
                "The learning efficiency of LNNs and HNNs is shown to enhance significantly by employing explicit constraints (Finzi et al., 2020) and their inherent structure Zhong et al. (2019).",
                "Although the idea of graph-based modeling has been suggested for physical systems (Cranmer et al., 2020b; Greydanus et al., 2019), the inductive biases induced due to different graph structures and their consequences on the dynamics remain poorly explored.",
                "More specifically, an HNN with separable potential (V (q)) and kinetic (T (q, q\u0307)) energies is equivalent to a second order NODE of the form q\u0308 = F\u0302 (q, q\u0307).",
                "We also show that the final model with the appropriate inductive biases significantly outperforms the simple version of GNODE with no additional inductive biases and even the graph versions of LNN and HNN.",
                "In addition, it has been shown that the superior performance of HNNs and LNNs is mainly due to their second-order bias, and not due to their symplectic or energy conserving bias (Gruver et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "f5524163f20b389c6e707f0ded92f1f51bc394de",
                "externalIds": {
                    "CorpusId": 259854848
                },
                "corpusId": 259854848,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f5524163f20b389c6e707f0ded92f1f51bc394de",
                "title": "E NHANCING THE I NDUCTIVE B IASES OF G RAPH N EU - RAL ODE FOR M ODELING P HYSICAL S YSTEMS",
                "abstract": ".",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "15675757",
                        "name": "S. Bishnoi"
                    },
                    {
                        "authorId": "2131107456",
                        "name": "Ravinder Bhattoo"
                    },
                    {
                        "authorId": "1699732",
                        "name": "Sayan Ranu"
                    },
                    {
                        "authorId": "2144896197",
                        "name": "N. Krishnan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "c5c120bd24b9f5993201e0f6c5b1408aafe67b13",
                "externalIds": {
                    "DOI": "10.17163/ings.n30.2023.07",
                    "CorpusId": 259579896
                },
                "corpusId": 259579896,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c5c120bd24b9f5993201e0f6c5b1408aafe67b13",
                "title": "Evolutionary artificial neural network for temperature control in a batch polymerization reactor",
                "abstract": "The integration of artificial intelligence techniques introduces fresh perspectives in the implementation of these methods. This paper presents the combination of neural networks and evolutionary strategies to create what is known as evolutionary artificial neural networks (EANNs). In the process, the excitation function of neurons was modified to allow asexual reproduction. As a result, neurons evolved and developed significantly. The technique of a batch polymerization reactor temperature controller to produce polymethylmethacrylate (PMMA) by free radicals was compared with two different controls, such as PID and GMC, demonstrating that artificial intelligence-based controllers can be applied. These controllers provide better results than conventional controllers without creating transfer functions to the control process represented.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "82400569",
                        "name": "F. S\u00e1nchez-Ruiz"
                    },
                    {
                        "authorId": "2222160591",
                        "name": "Elizabeth Arg\u00fcelles Hernandez"
                    },
                    {
                        "authorId": "2141032607",
                        "name": "Jos\u00e9 Terrones-Salgado"
                    },
                    {
                        "authorId": "2222157962",
                        "name": "Luz Judith Fern\u00e1ndez Quiroz"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In addition to physical inductive biases encodable via differentiable equations, there have also been recently developed methods to effectively impose energy conservation on learnt representations (Greydanus et al., 2019; Cranmer et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "66d74179040947bfebce9ce31008bbc0374cf007",
                "externalIds": {
                    "DBLP": "conf/icml/GhoshGDLKCCKKE23",
                    "CorpusId": 260927387
                },
                "corpusId": 260927387,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/66d74179040947bfebce9ce31008bbc0374cf007",
                "title": "Harmonic Neural Networks",
                "abstract": "Harmonic functions are abundant in nature, appearing in limiting cases of Maxwell\u2019s, Navier-Stokes equations, the heat and the wave equation. Consequently, there are many applications of harmonic functions from industrial process optimisation to robotic path planning and the calculation of first exit times of random walks. Despite their ubiquity and relevance, there have been few attempts to incorporate inductive biases towards harmonic functions in machine learning contexts. In this work, we demonstrate effective means of representing harmonic functions in neural networks and extend such results also to quantum neural networks to demonstrate the generality of our approach. We benchmark our approaches against (quantum) physics-informed neural networks, where we show favourable performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "123402741",
                        "name": "Atiyo Ghosh"
                    },
                    {
                        "authorId": "35636440",
                        "name": "A. Gentile"
                    },
                    {
                        "authorId": "35056184",
                        "name": "M. Dagrada"
                    },
                    {
                        "authorId": "2195974609",
                        "name": "Chul Lee"
                    },
                    {
                        "authorId": "2195883178",
                        "name": "S. Kim"
                    },
                    {
                        "authorId": "2196074496",
                        "name": "Hyukgeun Cha"
                    },
                    {
                        "authorId": "2196249687",
                        "name": "Yunjun Choi"
                    },
                    {
                        "authorId": "122204604",
                        "name": "Dongho Kim"
                    },
                    {
                        "authorId": "144969941",
                        "name": "J. Kye"
                    },
                    {
                        "authorId": "13620411",
                        "name": "V. Elfving"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Furthermore, we use hyperbolic tangent (tanh) and Rectified Linear Unit (ReLU) as activation function for the first and second hidden layer, respectively, while [16, 7, 24] use tanh for both.",
                ", is a hybrid machine learning framework imposing hard constraints on a data-driven model [16].",
                "In contrast, works like [16, 7, 11] either assume derivatives are known or perform one or more integration steps at each training step.",
                "Following [16, 7, 24], we use fully connected neural networks with two hidden layers of 100 neurons each.",
                "[16], via the SymODEN [36, 35] and portHamiltonian neural network (PHNN) [11] frameworks, and is defined for systems on any manifold.",
                "The HNNs of [16] use a neural network \u0124\u03b8 with weights \u03b8 to approximate the Hamiltonian H(q, p) of a system.",
                "3 Implementation and hyperparameters Following [16, 7, 24], we use fully connected neural networks with two hidden layers of 100 neurons each.",
                "suggest that the derivatives can be approximated by finite differences when unknown [16]."
            ],
            "citingPaper": {
                "paperId": "8a41dbf6d7cccde4c8e8dd5ad7de70277ef247cd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-02660",
                    "DOI": "10.48550/arXiv.2206.02660",
                    "CorpusId": 249395498
                },
                "corpusId": 249395498,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8a41dbf6d7cccde4c8e8dd5ad7de70277ef247cd",
                "title": "Port-Hamiltonian Neural Networks with State Dependent Ports",
                "abstract": "Hybrid machine learning based on Hamiltonian formulations has recently been successfully demonstrated for simple mechanical systems. In this work, we stress-test the method on both simple mass-spring systems and more complex and realistic systems with several internal and external forces, including a system with multiple connected tanks. We quantify performance under various conditions and show that imposing di\ufb00erent assumptions greatly a\ufb00ect the performance during training presenting advantages and limitations of the method. We demonstrate that port-Hamiltonian neural networks can be extended to larger dimensions with state-dependent ports. We consider learning on systems with known and unknown external forces and show how it can be used to detect deviations in a system and still provide a valid model when the deviations are removed. Finally, we propose a symmetric high-order integrator for improved training on sparse and noisy data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "102675722",
                        "name": "S\u00f8lve Eidnes"
                    },
                    {
                        "authorId": "1560989357",
                        "name": "Alexander J. Stasik"
                    },
                    {
                        "authorId": "1491380459",
                        "name": "Camilla Sterud"
                    },
                    {
                        "authorId": "1404060050",
                        "name": "Eivind B\u00f8hn"
                    },
                    {
                        "authorId": "1388390391",
                        "name": "S. Riemer-S\u00f8rensen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Physics Learning There have been a significant research effort in learning physics models from data such as Hamiltonian neural networks (Greydanus et al. [2019]), Lagrangian neural networks (Cranmer et al. [2020]), Deep lagrangian neural networks (Lutter et al. [2018]).",
                "Physics Learning There have been a significant research effort in learning physics models from data such as Hamiltonian neural networks (Greydanus et al. [2019]), Lagrangian neural networks (Cranmer et al. [2020]), Deep lagrangian neural networks (Lutter et al.",
                "Physics Learning There have been a significant research effort in learning physics models from data such as Hamiltonian neural networks (Greydanus et al. [2019]), Lagrangian neural networks (Cranmer et al.",
                "Physics Learning There have been a significant research effort in learning physics models from data such as Hamiltonian neural networks (Greydanus et al. [2019]), Lagrangian neural networks (Cranmer et al. [2020]), Deep lagrangian neural networks (Lutter et al. [2018]). Neural networks have been used to analyze in-situ experiment data for materials under extreme heat and irradiation (Niu et al. [2020]).",
                "Physics Learning There have been a significant research effort in learning physics models from data such as Hamiltonian neural networks (Greydanus et al. [2019]), Lagrangian neural networks (Cranmer et al. [2020]), Deep lagrangian neural networks (Lutter et al. [2018]). Neural networks have been used to analyze in-situ experiment data for materials under extreme heat and irradiation (Niu et al. [2020]). Similar to our work, Xue et al. [2021] proposes to make physics learning more efficient by combining 2 step PDE trajectory extraction and model learning into a single learning method, and Sima and Xue [2021] proposes to use locality sensitive hashing to avoid redundant computations for similar data points make forward simulation more efficient."
            ],
            "citingPaper": {
                "paperId": "b795fce06e514a791ca83323868747eb3384d95c",
                "externalIds": {
                    "DBLP": "conf/uai/NasimZEX22",
                    "CorpusId": 252899004
                },
                "corpusId": 252899004,
                "publicationVenue": {
                    "id": "f9af8000-42f8-410d-a622-e8811e41660a",
                    "name": "Conference on Uncertainty in Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Uncertainty in Artificial Intelligence",
                        "UAI",
                        "Conf Uncertain Artif Intell",
                        "Uncertain Artif Intell"
                    ],
                    "url": "http://www.auai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/b795fce06e514a791ca83323868747eb3384d95c",
                "title": "Efficient learning of sparse and decomposable PDEs using random projection",
                "abstract": "Learning physics models in the form of Partial Differential Equations (PDEs) is carried out through back-propagation to match the simulations of the physics model with experimental observations. Nevertheless, such matching involves computation over billions of elements, presenting a significant computational overhead. We notice many PDEs in real world problems are sparse and decomposable, where the temporal updates and the spatial features are sparsely concentrated on small interface regions. We propose R APID -PDE, an algorithm to expedite the learning of sparse and decomposable PDEs. Our R APID -PDE first uses random projection to compress the high dimensional sparse updates and features into low dimensional representations and then use these compressed signals during learning. Crucially, such a conversion is only carried out once prior to learning and the entire learning process is conducted in the compressed space. Theoretically, we derive a constant factor approximation between the projected loss function and the original one with poly-logarithmic number of projected dimensions. Empirically, we demonstrate R APID -PDE with data compressed to 0.05% of its original size learns similar models, compared with uncompressed algorithms in learning a set of phase-field models, which govern the spatial-temporal dynamics of nano-scale structures in metallic materials.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "104331438",
                        "name": "M. Nasim"
                    },
                    {
                        "authorId": "2153648873",
                        "name": "Xinghang Zhang"
                    },
                    {
                        "authorId": "1390185113",
                        "name": "A. El-Azab"
                    },
                    {
                        "authorId": "2937530",
                        "name": "Yexiang Xue"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "For HNN, we use 4 \u00d7 100 \u00d7 100 \u00d7 1 FNN with Tanh activation and without the last layer bias as proposed by [7].",
                "The neural networks embedded with the induced biases have remarkable abilities in learning and generalizing the intrinsic kinetics of the underlying systems from the noisy data, such as the Hamiltonian neural networks [7, 8], the Lagrangian neural networks [5], and the physics-informed neural networks [13, 9, 6].",
                "We compare our HNKO with the currently mainstream methods, the HNN [7] and the EDMD [19], in terms of the trajectory prediction and the energy conservation.",
                "Indeed, this framework does not require an injection of the regularization terms into the loss function as those usually required in the literature [7, 5]."
            ],
            "citingPaper": {
                "paperId": "e6b1a3381a758ac5105444d2a6a80e61023d3c3d",
                "externalIds": {
                    "CorpusId": 259841301
                },
                "corpusId": 259841301,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e6b1a3381a758ac5105444d2a6a80e61023d3c3d",
                "title": "Hamiltonian Neural Koopman Operator",
                "abstract": "Recently, physics-informed learning, a class of deep learning framework that in-corporates the physics priors and the observational noise-perturbed data into the neural network models, has shown outstanding performances in learning physical principles with higher accuracy, faster training speed, and better generalization ability. Here, for the Hamiltonian mechanics and using the Koopman operator theory, we propose a typical physics-informed learning framework, named as H amiltonian N eural K oopman O perator (HNKO) to learn the corresponding Koopman operator automatically satisfying the conservation laws. We analytically investigate the dimension of the manifold induced by the orthogonal transformation, and use a modified auto-encoder to identify the nonlinear coordinate transformation that is required for approximating the Koopman operator. Taking the Kepler problem as an example, we demonstrate that the proposed HNKO in robustly learning the Hamiltonian dynamics outperforms the representative methods developed in the literature. Our results suggest that feeding the prior knowledge of the underlying system and the mathematical theory appropriately to the learning framework can reinforce the capability of the deep learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2223153066",
                        "name": "Jingdong Zhang"
                    },
                    {
                        "authorId": "27716042",
                        "name": "Qunxi Zhu"
                    },
                    {
                        "authorId": "2113563142",
                        "name": "Wei Lin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "HNN D-HNN NODE SymODEN D-SymODEN SympGPR SSGP [14] [9, 39] [4] [48] [47] [35] (a) Energy conservation law X X X X X X (b) Energy dissipation law X X X (c) Learning with ODE solver X X X X (d) Uncertainty X X",
                "The Hamiltonian neural network (HNN) [14] introduced prior knowledge of Hamiltonian mechanics as an inductive bias for training DNNs; the core concept is to parameterize the Hamiltonian (i.",
                "We can easily extend our proposed approach to learn Hamiltonians from high-dimensional data (such as images) by combining an autoencoder with an SSGP, as in [14, 42].",
                "We compared the SSGP with the existing models (see Table 1): Hamiltonian neural network (HNN) [14], dissipative HNN (D-HNN) [39], neural ordinary differential equation (NODE) [4], symplectic ODE-Net (SymODEN) [48], dissipative SymODEN (D-SymODEN) [47] and symplectic Gaussian process regression (SympGPR) [35].",
                "The Hamiltonian neural network (HNN) [14] and variants (e."
            ],
            "citingPaper": {
                "paperId": "17f9fb7be352d5849a97e3e8c39829e6b33830d6",
                "externalIds": {
                    "DBLP": "conf/nips/0002IU22",
                    "CorpusId": 258509659
                },
                "corpusId": 258509659,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/17f9fb7be352d5849a97e3e8c39829e6b33830d6",
                "title": "Symplectic Spectrum Gaussian Processes: Learning Hamiltonians from Noisy and Sparse Data",
                "abstract": "Hamiltonian mechanics is a well-established theory for modeling the time evolution of systems with conserved quantities (called Hamiltonian ), such as the total energy of the system. Recent works have parameterized the Hamiltonian by machine learning models (e.g., neural networks), allowing Hamiltonian dynamics to be obtained from state trajectories without explicit mathematical modeling. However, the performance of existing models is limited as we can observe only noisy and sparse trajectories in practice. This paper proposes a probabilistic model that can learn the dynamics of conservative or dissipative systems from noisy and sparse data. We introduce a Gaussian process that incorporates the symplectic geometric structure of Hamiltonian systems, which is used as a prior distribution for estimating Hamiltonian systems with additive dissipation. We then present its spectral representation, Symplectic Spectrum Gaussian Processes (SSGPs) , for which we newly derive random Fourier features with symplectic structures. This allows us to construct an ef\ufb01cient variational inference algorithm for training the models while simulating the dynamics via ordinary differential equation solvers. Experiments on several physical systems show that SSGP offers excellent performance in predicting dynamics that follow the energy conservation or dissipation law from noisy and sparse data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143668549",
                        "name": "Yusuke Tanaka"
                    },
                    {
                        "authorId": "2664600",
                        "name": "Tomoharu Iwata"
                    },
                    {
                        "authorId": "1735221",
                        "name": "N. Ueda"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "At the same time, the deep learning community has developed powerful tools by adapting physical modeling concepts to deep learning, for instance the Neural ODE approach was proposed for the modeling of continuous transformations ([40]), stable neural architectures were also developed by embedding neural networks with invariant structures ([34, 48, 46])."
            ],
            "citingPaper": {
                "paperId": "417a317aed59278b918bf045e5098a49eeef106e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-10746",
                    "CorpusId": 247025648
                },
                "corpusId": 247025648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/417a317aed59278b918bf045e5098a49eeef106e",
                "title": "CD-ROM: Complementary Deep-Reduced Order Model",
                "abstract": "Model order reduction through the POD-Galerkin method can lead to dramatic gains in terms of computational efficiency in solving physical problems. However, the applicability of the method to non linear high-dimensional dynamical systems such as the Navier-Stokes equations has been shown to be limited, producing inaccurate and sometimes unstable models. This paper proposes a closure modeling approach for classical POD-Galerkin reduced order models (ROM). We use multi layer perceptrons (MLP) to learn a continuous in time closure model through the recently proposed Neural ODE method. Inspired by Taken\u2019s theorem as well as the Mori-Zwanzig formalism, we augment ROMs with a delay differential equation architecture to model non-Markovian effects in reduced models. The proposed model, called CD-ROM (Complementary Deep Reduced Order Model) is able to retain information from past states of the system and use it to correct the imperfect reduced dynamics. The model can be integrated in time as a system of ordinary differential equations using any classical time marching scheme. We demonstrate the ability of our CD-ROM approach to improve the accuracy of POD-Galerkin models on two CFD examples, even in configurations unseen during training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155649051",
                        "name": "Emmanuel Menier"
                    },
                    {
                        "authorId": "89936237",
                        "name": "M. Bucci"
                    },
                    {
                        "authorId": "26631868",
                        "name": "Mouadh Yagoubi"
                    },
                    {
                        "authorId": "3083383",
                        "name": "L. Mathelin"
                    },
                    {
                        "authorId": "2066691430",
                        "name": "M. Schoenauer"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "A better approach would be to express dynamics as a set of differential equations since time is actually continuous [23]."
            ],
            "citingPaper": {
                "paperId": "b8afcdf7a74fcf6ac434ea0809e672171518b648",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-08672",
                    "DOI": "10.48550/arXiv.2204.08672",
                    "CorpusId": 248239622
                },
                "corpusId": 248239622,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b8afcdf7a74fcf6ac434ea0809e672171518b648",
                "title": "A Score-based Geometric Model for Molecular Dynamics Simulations",
                "abstract": "Molecular dynamics (MD) has long been the de facto choice for modeling complex atomistic systems from \ufb01rst principles, and recently deep learning become a popular way to accelerate it. Notwithstanding, preceding approaches depend on intermediate variables such as the potential energy or force \ufb01elds to update atomic positions, which requires additional computations to perform back-propagation. To waive this requirement, we propose a novel model called ScoreMD by directly estimating the gradient of the log density of molecular conformations. Moreover, we analyze that diffusion processes highly accord with the principle of enhanced sampling in MD simulations, and is therefore a perfect match to our sequential conformation generation task. That is, ScoreMD perturbs the molecular structure with a conditional noise depending on atomic accelerations and employs conformations at previous timeframes as the prior distribution for sampling. Another challenge of modeling such a conformation generation process is that the molecule is kinetic instead of static, which no prior studies strictly consider. To solve this challenge, we introduce a equivariant geometric Transformer as a score function in the diffusion process to calculate the corresponding gradient. It incorporates the directions and velocities of atomic motions via 3D spherical Fourier-Bessel representations. With multiple architectural improvements, we outperforms state-of-the-art baselines on MD17 and isomers of C7O2H10. This research provides new insights into the acceleration of new material and drug discovery.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152167768",
                        "name": "Fang Wu"
                    },
                    {
                        "authorId": "1737486",
                        "name": "Qian Zhang"
                    },
                    {
                        "authorId": "2153674069",
                        "name": "Xurui Jin"
                    },
                    {
                        "authorId": "2146420304",
                        "name": "Yinghui Jiang"
                    },
                    {
                        "authorId": "1390908654",
                        "name": "Stan Z. Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "An adjacent research direction is the method/models which consider CT dynamics and directly use the state derivatives and often the noiseless states to formulate structured and interpretable models such as Hamiltonian Neural Networks (HNN) [16], Lagrangian Neural Networks (LNN) [10] and Sparse Identification of Nonlinear Dynamics (SINDy) [7].",
                "[16,10]) on the state-space obtaining a widely applicable method."
            ],
            "citingPaper": {
                "paperId": "52e47a603fa797dcc1d6433511ccc19fea9dabf8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-09405",
                    "DOI": "10.48550/arXiv.2204.09405",
                    "CorpusId": 248266694
                },
                "corpusId": 248266694,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/52e47a603fa797dcc1d6433511ccc19fea9dabf8",
                "title": "Deep subspace encoders for continuous-time state-space identification",
                "abstract": ". Continuous-time (CT) models have shown an improved sample e\ufb03ciency during learning and enable ODE analysis methods for enhanced interpretability compared to discrete-time (DT) models. Even with numerous recent developments, the multifaceted CT state-space model identi\ufb01cation problem remains to be solved in full, considering common experimental aspects such as the presence of external inputs, measurement noise, and latent states. This paper presents a novel estimation method that includes these aspects and that is able to obtain state-of-the-art results on multiple benchmarks where a small fully connected neural network describes the CT dynamics. The novel estimation method called the subspace encoder approach ascertains these results by altering the well-known simulation loss to include short subsections instead, by using an encoder function and a state-derivative normalization term to obtain a computationally feasible and stable optimization problem. This encoder function estimates the initial states of each considered subsection. We prove that the existence of the encoder function has the necessary condition of a Lipschitz continuous state-derivative utilizing established properties of ODEs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1602277127",
                        "name": "G. Beintema"
                    },
                    {
                        "authorId": "1767103",
                        "name": "M. Schoukens"
                    },
                    {
                        "authorId": "152809968",
                        "name": "R. T'oth"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Hamiltonian dynamics [31] describes a system\u2019s total energyH(q,p) as a function of its canonical coordinates q and momenta p, e."
            ],
            "citingPaper": {
                "paperId": "dddad45df8d22f7d3cbe83e5ec33636736c726ca",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-07266",
                    "DOI": "10.48550/arXiv.2205.07266",
                    "CorpusId": 248811221
                },
                "corpusId": 248811221,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dddad45df8d22f7d3cbe83e5ec33636736c726ca",
                "title": "Discovering the Representation Bottleneck of Graph Neural Networks from Multi-order Interactions",
                "abstract": "Most graph neural networks (GNNs) rely on the message passing paradigm to propagate node features and build interactions. Recent works point out that different graph learning tasks require different ranges of interactions between nodes. To investigate its underlying mechanism, we explore the capacity of GNNs to capture pairwise interactions between nodes under contexts with different complexities, especially for their graph-level and node-level applications in scienti\ufb01c domains like biochemistry and physics. When formulating pairwise interactions, we study two common graph construction methods in scienti\ufb01c domains, i.e., K-nearest neighbor (KNN) graphs and fully-connected (FC) graphs. Furthermore, we demonstrate that the inductive bias introduced by KNN-graphs and FC-graphs hinders GNNs to learn the most informative order of interactions. Such a phenomenon is broadly shared by several GNNs for different graph learning tasks and forbids GNNs to achieve the global minimum loss, so we name it a representation bottleneck . To overcome that, we propose a novel graph rewiring approach based on the pairwise interaction strengths to dynamically adjust the reception \ufb01elds of each node. Extensive experiments in molecular property prediction and dynamic system forecast prove the superiority of our method over state-of-the-art GNN baselines. Besides, this paper provides a reasonable explanation of why subgraphs play an important role in the determination of graph properties.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152167768",
                        "name": "Fang Wu"
                    },
                    {
                        "authorId": "2118155623",
                        "name": "Siyuan Li"
                    },
                    {
                        "authorId": "47767812",
                        "name": "Lirong Wu"
                    },
                    {
                        "authorId": "1390908654",
                        "name": "Stan Z. Li"
                    },
                    {
                        "authorId": "9215251",
                        "name": "Dragomir R. Radev"
                    },
                    {
                        "authorId": "1737486",
                        "name": "Qian Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "This line of work originates from Greydanus et al. (2019); Cranmer et al. (2020) and has been extended to NODEs for Hamiltonian and port-Hamiltonian systems (Zhong et al., 2020; Massaroli et al., 2020a).",
                "It also does not conserve energy, which is not surprising when no structure is imposed, as discussed e.g., in (Greydanus et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "197f0beed22a8b013f32e1f29cb583147d2fa159",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-12550",
                    "DOI": "10.48550/arXiv.2205.12550",
                    "CorpusId": 249062941
                },
                "corpusId": 249062941,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/197f0beed22a8b013f32e1f29cb583147d2fa159",
                "title": "Learning dynamics from partial observations with structured neural ODEs",
                "abstract": "Identifying dynamical systems from experimental data is a notably dif\ufb01cult task. Prior knowledge generally helps, but the extent of this knowledge varies with the application, and customized models are often needed. We propose a \ufb02exible framework to incorporate a broad spectrum of physical insight into neural ODE-based system identi\ufb01cation, giving physical interpretability to the resulting latent space. This insight is either enforced through hard constraints in the optimization problem or added in its cost function. In order to link the partial and possibly noisy observations to the latent state, we rely on tools from nonlinear observer theory to build a recognition model. We demonstrate the performance of the proposed approach on numerical simulations and on an experimental dataset from a robotic exoskeleton.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1410600781",
                        "name": "Mona Buisson-Fenet"
                    },
                    {
                        "authorId": "84320271",
                        "name": "V. Morgenthaler"
                    },
                    {
                        "authorId": "2715093",
                        "name": "Sebastian Trimpe"
                    },
                    {
                        "authorId": "2086729",
                        "name": "F. D. Meglio"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2020), reasoning in visual question answering (Kim & Lee, 2019), and physical system identification (Lutter et al., 2019; Greydanus et al., 2019; Finzi et al., 2020) etc.",
                "\u2026due to high training complexity, applied works have so far been limited to few domains like Reinforcement Learning (Du et al., 2020), reasoning in visual question answering (Kim & Lee, 2019), and physical system identification (Lutter et al., 2019; Greydanus et al., 2019; Finzi et al., 2020) etc."
            ],
            "citingPaper": {
                "paperId": "726e069bfa66dac36c5abc12b6237a88ca777f60",
                "externalIds": {
                    "DBLP": "conf/iclr/0003YHXS22",
                    "CorpusId": 249456000
                },
                "corpusId": 249456000,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/726e069bfa66dac36c5abc12b6237a88ca777f60",
                "title": "SketchODE: Learning neural sketch representation in continuous time",
                "abstract": "Learning meaningful representations for chirographic drawing data such as sketches, handwriting, and flowcharts is a gateway for understanding and emulating human creative expression. Despite being inherently continuous-time data, existing works have treated these as discrete-time sequences, disregarding their true nature. In this work, we model such data as continuous-time functions and learn compact representations by virtue of Neural Ordinary Differential Equations. To this end, we introduce the first continuous-time Seq2Seq model and demonstrate some remarkable properties that set it apart from traditional discrete-time analogues. We also provide solutions for some practical challenges for such models, including introducing a family of parameterized ODE dynamics & continuoustime data augmentation particularly suitable for the task. Our models are validated on several datasets including VectorMNIST, DiDi and Quick, Draw!.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2113239037",
                        "name": "Ayan Das"
                    },
                    {
                        "authorId": "2653152",
                        "name": "Yongxin Yang"
                    },
                    {
                        "authorId": "1697755",
                        "name": "Timothy M. Hospedales"
                    },
                    {
                        "authorId": "2149786193",
                        "name": "Tao Xiang"
                    },
                    {
                        "authorId": "2115712433",
                        "name": "Yi-Zhe Song"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "f6868760667f9b2a829fb7480ba881580e58ede1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-03680",
                    "DOI": "10.48550/arXiv.2208.03680",
                    "CorpusId": 251402588
                },
                "corpusId": 251402588,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f6868760667f9b2a829fb7480ba881580e58ede1",
                "title": "Accelerating Numerical Solvers for Large-Scale Simulation of Dynamical System via NeurVec",
                "abstract": "Ensemble-based large-scale simulation of dynamical systems is essential to a wide range of science and engineering problems. Conventional numerical solvers used in the simulation are signi\ufb01cantly limited by the step size for time integration, which hampers e\ufb03ciency and feasibility especially when high accuracy is desired. To overcome this limitation, we propose a data-driven corrector method that allows using large step sizes while compensating for the integration error for high accuracy. This corrector is represented in the form of a vector-valued function and is modeled by a neural network to regress the error in the phase space. Hence we name the corrector neural vector (NeurVec). We show that NeurVec can achieve the same accuracy as traditional solvers with much larger step sizes. We empirically demonstrate that NeurVec can accelerate a variety of numerical solvers signi\ufb01cantly and overcome the stability restriction of these solvers. Our results on benchmark problems, ranging from high-dimensional problems to chaotic systems, suggest that NeurVec is capable of capturing the leading error term and maintaining the statistics of ensemble forecasts.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109670338",
                        "name": "Zhongzhan Huang"
                    },
                    {
                        "authorId": "116746634",
                        "name": "Senwei Liang"
                    },
                    {
                        "authorId": "104693914",
                        "name": "Hong Zhang"
                    },
                    {
                        "authorId": "2906086",
                        "name": "Haizhao Yang"
                    },
                    {
                        "authorId": "2181395240",
                        "name": "Liang Lin"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[25]) and Lagrangian Neural Networks (Cranmer et al."
            ],
            "citingPaper": {
                "paperId": "4b50dee2077d037dde76fedc0b739922251e5ab8",
                "externalIds": {
                    "CorpusId": 251665392
                },
                "corpusId": 251665392,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4b50dee2077d037dde76fedc0b739922251e5ab8",
                "title": "Machine learning for closure models",
                "abstract": "Many real-world physical processes, such as fluid flows and molecular dynamics, are understood well enough that their behaviour can be accurately translated into mathematical systems of equations, which can then be solved by a computer algorithm. This process forms the basis of the research field of Scientific Computing and although it is very successful, performing accurate simulations can be computationally expensive. As a result, techniques have been developed for Model Order Reduction (MOR), which aim to drastically reduce the complexity of such systems of equations while sacrificing only little accuracy compared to the original Full-Order Model (FOM). The resulting Reduced-Order Models (ROMs) often need to include a correction (or \u2018closure\u2019) term to account for the error that was introduced when performing the reduction. In recent years, Machine Learning (ML) has become a popular way to obtain such closure terms. However, the research on ML for closure terms differs from more general ML research in that relevant domain knowledge (i.e. applicable laws of physics or statistical observations) can be used to design the ML model. Moreover, ML closure models do not need to learn the entire dynamics of the underlying system but only the error between the real dynamics of the FOM and the approximate dynamics of the ROM. In this thesis, several sets of experiments are performed that aim to assess the efficacy of simple ML closure models for a number of problems in the form of ordinary or partial differential equations, and to inform future uses of ML in ROM by comparing several ML architectures and training procedures. Even simple ML closure models are found to perform drastically better than models without closure term, while also outperforming \u2018pure\u2019 ML models that do not use prior knowledge as an approximation. Furthermore, models that are formulated to be continuous in time (as the underlying processes are) outperform models that are discrete in time, and models with domain knowledge embedded in their designs outperform models without such properties. As for training procedures, several methods are compared and although one method clearly outperforms the others, the specific problem considered determines which ODE solvers are applicable, which in turn influences the suitability of different training procedures. Finally, some models are compared that allow for a memory effect, in which future states depend not only on the current state but also on past states. While models with memory effects have been found to perform well in other works, they do not outperform simpler memoryless models on the problem considered in this thesis. Nevertheless, the value of ML for closure terms is clear since the accuracy of a numerical method can be improved significantly by supplementing the method with a relatively small neural network. However, more research should be done to examine the performance of such closure terms compared to purely numerical methods, preferably using more complex problems for testing. Finally, ML closure models with memory should be examined more critically to see how models can be obtained that do not suffer from overfitting.",
                "year": 2022,
                "authors": []
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "df0dc45dbfdc6525cb210b16d83b7a4ef873b1ca",
                "externalIds": {
                    "DBLP": "conf/iclr/LiangHZ22",
                    "CorpusId": 251648970
                },
                "corpusId": 251648970,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/df0dc45dbfdc6525cb210b16d83b7a4ef873b1ca",
                "title": "Stiffness-aware neural network for learning Hamiltonian systems",
                "abstract": "We propose stiffness-aware neural network (SANN), a new method for learning Hamiltonian dynamical systems from data. SANN identi\ufb01es and splits the training data into stiff and nonstiff portions based on a stiffness-aware index, a simple, yet effective metric we introduce to quantify the stiffness of the dynamical system. This classi\ufb01cation along with a resampling technique allows us to apply different time integration strategies such as step size adaptation to better capture the dynamical characteristics of the Hamiltonian vector \ufb01elds. We evaluate SANN on complex physical systems including a three-body problem and billiard model. We show that SANN is more stable and can better preserve energy when compared with the state-of-the-art methods, leading to signi\ufb01cant improvement in accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "116746634",
                        "name": "Senwei Liang"
                    },
                    {
                        "authorId": "2109670338",
                        "name": "Zhongzhan Huang"
                    },
                    {
                        "authorId": "2146244223",
                        "name": "Hong Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Among these, Lagrangian neural networks (LNNs) and Hamiltonian neural networks (HNNs) are two physics-informed neural networks with strong inductive biases that outperform other learning paradigms of dynamical systems (Greydanus et al., 2019; Zhong et al., 2021; Sanchez-Gonzalez et al., 2019; Yang et al., 2020; Cranmer et al., 2020a; Finzi et al., 2020; Lutter et al., 2019; Duong and Atanasov, 2021).",
                "\u2026networks (HNNs) are two physics-informed neural networks with strong inductive biases that outperform other learning paradigms of dynamical systems (Greydanus et al., 2019; Zhong et al., 2021; Sanchez-Gonzalez et al., 2019; Yang et al., 2020; Cranmer et al., 2020a; Finzi et al., 2020; Lutter et\u2026",
                "Most of the works on LNN has focused on relatively simpler particle-based systems such as springs and pendulums (Cranmer et al., 2020a; Finzi et al., 2020; Lutter et al., 2019; Greydanus et al., 2019; Gruver et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "7e7a90859c1dacf3f283339a968c6b4d43ee869f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-11588",
                    "DOI": "10.48550/arXiv.2209.11588",
                    "CorpusId": 252519333
                },
                "corpusId": 252519333,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7e7a90859c1dacf3f283339a968c6b4d43ee869f",
                "title": "Learning Rigid Body Dynamics with Lagrangian Graph Neural Network",
                "abstract": "Lagrangian and Hamiltonian neural networks (L NN and H NN respectively) encode strong inductive biases that allow them to outperform other models of physical systems signi\ufb01cantly. However, these models have, thus far, mostly been limited to simple systems such as pendulums and springs or a single rigid body such as a gyroscope or a rigid rotor. Here, we present a Lagrangian graph neural network (L GNN ) that can learn the dynamics of rigid bodies by exploiting their topology. We demonstrate the performance of L GNN by learning the dynamics of ropes, chains, and trusses with the bars modeled as rigid bodies. L GNN also exhibits generalizability\u2014L GNN trained on chains with a few segments exhibits generalizability to simulate a chain with large number of links and arbitrary link length. We also show that the L GNN can simulate unseen hybrid systems including bars and chains, on which they have not been trained on. Speci\ufb01cally, we show that the L GNN can be used to model the dynamics of complex real-world structures such as the stability of tensegrity structures. Finally, we discuss the non-diagonal nature of the mass matrix and it\u2019s ability to generalize in complex systems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2131107456",
                        "name": "Ravinder Bhattoo"
                    },
                    {
                        "authorId": "1699732",
                        "name": "Sayan Ranu"
                    },
                    {
                        "authorId": "2144896197",
                        "name": "N. Krishnan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "The model would be more effective if it were true to reality [14]."
            ],
            "citingPaper": {
                "paperId": "ca5549c3ff2179afd185950524cb9a266468ac6b",
                "externalIds": {
                    "DBLP": "journals/access/ZhangZWG22",
                    "DOI": "10.1109/ACCESS.2022.3181750",
                    "CorpusId": 249560955
                },
                "corpusId": 249560955,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ca5549c3ff2179afd185950524cb9a266468ac6b",
                "title": "Predicting the Materials Properties Using a 3D Graph Neural Network with Invariant Representation",
                "abstract": "Accurate prediction of physical properties is critical for discovering and designing novel materials. Machine learning technologies have attracted significant attention in the materials science community for their potential for large-scale screening. Graph Convolution Neural Network (GCNN) is one of the most successful machine learning methods because of its flexibility and effectiveness in describing 3D structural data. Most existing GCNN models focus on the topological structure but overly simplify the three-dimensional geometric structure. However, in materials science, the 3D-spatial distribution of atoms is crucial for determining the atomic states and interatomic forces. This paper proposes an adaptive GCNN with a novel convolution mechanism that simultaneously models atomic interactions among all neighboring atoms in three-dimensional space. We apply the proposed model to two distinctly challenging materials properties prediction problems. The first is Henry\u2019s constant for gas adsorption in Metal-Organic Frameworks (MOFs), which is notoriously difficult because of its high sensitivity to atomic configurations. The second is the ion conductivity in solid-state crystal materials, which is difficult because of the few labeled data available for training. The new model outperforms existing graph-based models on both data sets, suggesting that the critical three-dimensional geometric information is indeed captured.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Boyu Zhang"
                    },
                    {
                        "authorId": "2051329266",
                        "name": "Mushen Zhou"
                    },
                    {
                        "authorId": "2157570954",
                        "name": "Jianzhong Wu"
                    },
                    {
                        "authorId": "40088807",
                        "name": "Fuchang Gao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Figure 3: A Hamiltonian neural network as introduced in [3].",
                "However, the number of specialized neural networks for Hamiltonian systems has been growing significantly during the last years [3, 5]."
            ],
            "citingPaper": {
                "paperId": "7a9b54b3cddfb74e47993f1d35e262d69d0e6211",
                "externalIds": {
                    "DOI": "10.23967/eccomas.2022.262",
                    "CorpusId": 254010657
                },
                "corpusId": 254010657,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7a9b54b3cddfb74e47993f1d35e262d69d0e6211",
                "title": "Structure-Preserving Neural Networks for the N-body Problem",
                "abstract": ". In order to understand when it is useful to build physics constraints into neural networks, we investigate di\ufb00erent neural network topologies to solve the N -body problem. Solving the chaotic N -body problem with high accuracy is a challenging task, requiring special numerical integrators that are able to approximate the trajectories with extreme precision. In [1] it is shown that a neural network can be a viable alternative, o\ufb00ering solutions many orders of magnitude faster. Specialized neural network topologies for applications in scienti\ufb01c computing are still rare compared to specialized neural networks for more classical machine learning applications. However, the number of specialized neural networks for Hamiltonian systems has been growing signi\ufb01cantly during the last years [3, 5]. We analyze the performance of SympNets introduced in [5], preserving the symplectic structure of the phase space \ufb02ow map, for the prediction of trajectories in N -body systems. In particular, we compare the accuracy of SympNets against standard multilayer perceptrons, both inside and outside the range of training data. We analyze our \ufb01ndings using a novel view on the topology of SympNets. Additionally, we also compare SympNets against classical symplectic numerical integrators. While the bene\ufb01ts of symplectic integrators for Hamiltonian systems are well",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "153466324",
                        "name": "P. Horn"
                    },
                    {
                        "authorId": "2773446",
                        "name": "B. Koren"
                    },
                    {
                        "authorId": "10773336",
                        "name": "S. P. Portegies Zwart"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e686256a071d6c64913d49c7ad2de0b838d32580",
                "externalIds": {
                    "DOI": "10.1109/tai.2022.3230632",
                    "CorpusId": 254070169
                },
                "corpusId": 254070169,
                "publicationVenue": {
                    "id": "3c27e831-750f-45bc-9914-2148a5259eba",
                    "name": "IEEE Transactions on Artificial Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Artif Intell"
                    ],
                    "issn": "2691-4581",
                    "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9078688"
                },
                "url": "https://www.semanticscholar.org/paper/e686256a071d6c64913d49c7ad2de0b838d32580",
                "title": "A Memory-Efficient Neural Ordinary Differential Equation Framework Based on High-Level Adjoint Differentiation",
                "abstract": ",",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146244559",
                        "name": "Hong Zhang"
                    },
                    {
                        "authorId": "2118226242",
                        "name": "Wenjun Zhao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026by the level of inductive bias incorporated, spanning methods which use black-box architectures composed of multilayer perceptrons (Thuruthel et al. (2018)) to grey-box architectures that aim to preserve physical invariants (Thuruthel et al. (2018); Greydanus et al. (2019); Cranmer et al. (2020)).",
                "The spectrum \u039bE , has no low-order resonance relationship with any eigenvalue in the outer spectrum \u039bout (see Haller and Ponsioen (2016); Cenedese et al."
            ],
            "citingPaper": {
                "paperId": "ebb3bdc6989aefbd19195643bb9bc7002256cfef",
                "externalIds": {
                    "CorpusId": 254076561
                },
                "corpusId": 254076561,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ebb3bdc6989aefbd19195643bb9bc7002256cfef",
                "title": "Practical Deployment of Spectral Submanifold Reduction for Optimal Control of High-Dimensional Systems",
                "abstract": ": Real-time optimal control of high-dimensional, nonlinear systems remains a challenging task due to the computational intractability of their models. While several model-reduction and learning-based approaches for constructing low-dimensional surrogates of the original system have been proposed in the literature, these approaches suffer from fundamental issues which limit their application in real-world scenarios. Namely, they typically lack generalizability to different control tasks, ability to trade dimensionality for accuracy, and ability to preserve the structure of the dynamics. Recently, we proposed to extract low-dimensional dynamics on Spectral Submanifolds (SSMs) to overcome these issues and validated our approach in a highly accurate simulation environment. In this manuscript, we extend our framework to a real-world setting by employing time-delay embeddings to embed SSMs in an observable space of appropriate dimension. This allows us to learn highly accurate, low-dimensional dynamics purely from observational data. We show that these innovations extend Spectral Submanifold Reduction (SSMR) to real-world applications and showcase the effectiveness of SSMR on a soft robotic system.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8383806",
                        "name": "J. I. Alora"
                    },
                    {
                        "authorId": "101002420",
                        "name": "Mattia Cenedese"
                    },
                    {
                        "authorId": "1868195",
                        "name": "E. Schmerling"
                    },
                    {
                        "authorId": "6314087",
                        "name": "G. Haller"
                    },
                    {
                        "authorId": "1696085",
                        "name": "M. Pavone"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "For example, [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40] exploit Lagrangian or Hamiltonian mechanics to learn an energy-conserving system based on position, momentum, and the derivatives thereof along trajectories.",
                "This problem setup is distinct from that of HNN [33], HGN [36], or Symplectic ODE-Net [35]."
            ],
            "citingPaper": {
                "paperId": "510b5f11bf1ea270f8cc33f6b16e15102bb2ee3d",
                "externalIds": {
                    "DBLP": "conf/nips/YangRNR22",
                    "CorpusId": 254994240
                },
                "corpusId": 254994240,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/510b5f11bf1ea270f8cc33f6b16e15102bb2ee3d",
                "title": "Learning Physics Constrained Dynamics Using Autoencoders",
                "abstract": "We consider the problem of estimating states ( e.g., position and velocity) and physical parameters ( e.g., friction, elasticity) from a sequence of observations when provided a dynamic equation that describes the behavior of the system. The dynamic equation can arise from first principles ( e.g., Newton\u2019s laws) and provide useful cues for learning, but its physical parameters are unknown. To address this problem, we propose a model that estimates states and physical parameters of the system using two main components. First, an autoencoder compresses a sequence of observations ( e.g., sensor measurements, pixel images) into a sequence for the state representation that is consistent with physics by including a simulation of the dynamic equation. Second, an estimator is coupled with the autoencoder to predict the values of the physical parameters. We also theoretically and empirically show that using Fourier feature mappings improves the generalization of the estimator in predicting physical parameters compared to raw state sequences when learning from high-frequency data. In our experiments on three visual and one sensor measurement tasks, our model imposes interpretability on latent states and achieves improved generalization performance for long-term prediction of system dynamics over state-of-the-art baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "11844404",
                        "name": "Tsung-Yen Yang"
                    },
                    {
                        "authorId": "2085202",
                        "name": "J. Rosca"
                    },
                    {
                        "authorId": "144958935",
                        "name": "Karthik Narasimhan"
                    },
                    {
                        "authorId": "2133868318",
                        "name": "Peter J. Ramadge"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "The propagator objective is used within algorithms such as dynamic mode decomposition (DMD) [61] and sparse identification of nonlinear dynamics (SINDy) [10] and for training certain neural networks such as Hamiltonian neural networks [29]."
            ],
            "citingPaper": {
                "paperId": "57212c1f8e007afdf5eafcb8c5989ed8fbcd6c0a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-13902",
                    "DOI": "10.48550/arXiv.2212.13902",
                    "CorpusId": 255186331
                },
                "corpusId": 255186331,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/57212c1f8e007afdf5eafcb8c5989ed8fbcd6c0a",
                "title": "Robust identification of non-autonomous dynamical systems using stochastic dynamics models",
                "abstract": "This paper considers the problem of system identi\ufb01cation (ID) of linear and nonlinear non-autonomous systems from noisy and sparse data. We propose and analyze an objective function derived from a Bayesian formulation for learning a hidden Markov model with stochastic dynamics. We then analyze this objective function in the context of several state-of-the-art approaches for both linear and nonlinear system ID. In the former, we analyze least squares approaches for Markov parameter estimation, and in the latter, we analyze the multiple shooting approach. We demonstrate the limitations of the optimization problems posed by these existing methods by showing that they can be seen as special cases of the proposed optimization objective under certain simplifying assumptions: conditional independence of data and zero model error. Furthermore, we observe that our proposed approach has improved smoothness and inherent regularization that make it well-suited for system ID and provide mathematical explanations for these characteristics\u2019 origins. Finally, numerical simulations demonstrate a mean squared error over 8.7 times lower compared to multiple shooting when data are noisy and/or sparse. Moreover, the proposed approach can identify accurate and generalizable models even when there are more parameters than data or when the underlying system exhibits chaotic behavior.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2093676645",
                        "name": "Nicholas Galioto"
                    },
                    {
                        "authorId": "2439226",
                        "name": "A. Gorodetsky"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Similar considerations hold for works that further extended the Neural ODE model, such as Hamiltonian Neural Networks (Greydanus et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "c2e9b9fb86aee4c1dd2db4dad2bef53056808da2",
                "externalIds": {
                    "CorpusId": 259124948
                },
                "corpusId": 259124948,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c2e9b9fb86aee4c1dd2db4dad2bef53056808da2",
                "title": "C ONTINUAL L EARNING THROUGH H AMILTON E QUATIONS",
                "abstract": "Learning in a continual manner is one of the main challenges that the machine learning community is currently facing. The importance of the problem can be readily understood as soon as we consider settings where an agent is supposed to learn through an online interaction with a data stream, rather than operating offline on previously prepared data collections. In the last few years many efforts have been spent in proposing both models and algorithms to let machines learn in a continual manner, and the problem still remains extremely challenging. Many of the existing works rely on re-adapting the usual learning framework inherited from classic statistical approaches, that are typical of noncontinual-learning oriented problems. In this paper we consider a fully new perspective, rethinking the methodologies to be used to tackle continual learning, instead of re-adapting offline-oriented optimization. In particular, we propose a novel method to frame continual and online learning within the framework of optimal control. The proposed formulation leads to a novel interpretation of learning dynamics in terms of Hamilton equations. As a case study for the theory, we consider the problem of unsupervised optical flow estimation from a video stream. An experimental proof of concept for this learning task is discussed with the purpose of illustrating the soundness of the proposed approach, and opening to further research in this direction.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3409251",
                        "name": "Alessandro Betti"
                    },
                    {
                        "authorId": "1753459288",
                        "name": "Lapo Faggi"
                    },
                    {
                        "authorId": "145467467",
                        "name": "M. Gori"
                    },
                    {
                        "authorId": "51231820",
                        "name": "Matteo Tiezzi"
                    },
                    {
                        "authorId": "1711985422",
                        "name": "Simone Marullo"
                    },
                    {
                        "authorId": "133908117",
                        "name": "Enrico Meloni"
                    },
                    {
                        "authorId": "1760309",
                        "name": "S. Melacci"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026been growing interest in developing better deep neural network based dynamics models for physical systems through better inductive biases (Lutter et al., 2019a; Greydanus et al., 2019; Lutter et al., 2019b; Zhong et al., 2019, 2020; Cranmer et al., 2020; Finzi et al., 2020; Zhong et al., 2021a).",
                "Recently there has been growing interest in developing better deep neural network based dynamics models for physical systems through better inductive biases (Lutter et al., 2019a; Greydanus et al., 2019; Lutter et al., 2019b; Zhong et al., 2019, 2020; Cranmer et al., 2020; Finzi et al., 2020; Zhong et al., 2021a)."
            ],
            "citingPaper": {
                "paperId": "7fecadaa8deb107fbf20dfd3290d3917cc24891b",
                "externalIds": {
                    "CorpusId": 259305520
                },
                "corpusId": 259305520,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7fecadaa8deb107fbf20dfd3290d3917cc24891b",
                "title": "Lagrangian Model Based Reinforcement Learning",
                "abstract": "We are interested in reinforcement learning (RL) for physical systems. One of the drawbacks of traditional RL algorithms has been their poor sample efficiency. One approach to improve it is model-based RL. In our algorithm, we learn a model of the environment, essentially its transition dynamics and reward function, use it to generate imaginary trajectories and backpropagate through them to update the policy, exploiting the differentiability of the model. Intuitively, learning more accurate models should lead to better performance. Recently, there has been growing interest in developing better deep neural network based dynamics models for physical systems, through better inductive biases. We focus on systems undergoing rigid body motion. We compare two versions of our model-based RL algorithm, one which uses a standard deep neural network based dynamics model and the other which uses a Lagrangian Neural Network based dynamics model, which utilizes the structure of the underlying physics. We find that, in environments that are not sensitive to initial conditions, both versions achieve similar average-return, while the physics-informed version achieves better sample efficiency. Whereas, in environments that are sensitive to initial conditions, the physics-informed version achieves significantly better average-return and sample efficiency. In these latter environments, our physics-informed model-based RL approach achieves better average-return than Soft Actor-Critic, a state-of-the-art model-free RL algorithm.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "153519852",
                        "name": "Adithya Ramesh"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "The Hamiltonian Neural Network [16] is among the first methods that attempt to incorporate the structure of dynamical systems into a machine learning framework."
            ],
            "citingPaper": {
                "paperId": "3e25cddbd651f97eb65717b96a5466379f49478b",
                "externalIds": {
                    "DOI": "10.18122/td.1997.boisestate",
                    "CorpusId": 254391113
                },
                "corpusId": 254391113,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3e25cddbd651f97eb65717b96a5466379f49478b",
                "title": "Data-Driven Passivity-Based Control of Underactuated Robotic Systems",
                "abstract": "Classical control strategies for robotic systems are based on the idea that feedback control can be used to override the natural dynamics of the \n machines. Passivity-based control (Pbc) is a branch of nonlinear control theory that follows a similar approach, where the natural dynamics is \n modified based on the overall energy of the system. This method involves transforming a nonlinear control system, through a suitable control input, \n into another fictitious system that has desirable stability characteristics. The majority of Pbc techniques require the discovery of a \n reasonable storage function, which acts as a Lyapunov function candidate that can be used to certify stability.\n There are several challenges in the design of a suitable storage function, including: 1) what a reasonable choice for the function is for a given control system, and 2) the control synthesis requires a closed-form solution to a set of nonlinear partial differential equations. The latter is in general difficult to overcome, especially for systems with high degrees of freedom, limiting the applicability of Pbc techniques.\n A machine learning framework that automatically determines the storage function for underactuated robotic systems is introduced in this dissertation. This framework combines the expressive power of neural networks with the systematic methods of the Pbc paradigm, bridging the gap between controllers derived from learning algorithms and nonlinear control theory. A series of experiments demonstrates the efficacy and applicability of this framework for a family of underactuated robots.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "89668696",
                        "name": "Wankun Sirichotiyakul"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Damped Pendulum (DPL) Now a standard benchmark for hybrid models, we consider the motion of a pendulum of length L damped due to viscous friction (Greydanus et al., 2019; Yin et al., 2021).",
                ", 2018) or Hamiltonian priors (Greydanus et al., 2019; Lee et al., 2021), increases generalization power w."
            ],
            "citingPaper": {
                "paperId": "c8651d5306c1e30cf0636b05fa346dae0ae3f0dd",
                "externalIds": {
                    "DBLP": "conf/iclr/DonaDGL22",
                    "CorpusId": 251647934
                },
                "corpusId": 251647934,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c8651d5306c1e30cf0636b05fa346dae0ae3f0dd",
                "title": "Constrained Physical-Statistics Models for Dynamical System Identification and Prediction",
                "abstract": "Modeling dynamical systems combining prior physical knowledge and machine learning (ML) is promising in scientific problems when the underlying processes are not fully understood, e.g. when the dynamics is partially known. A common practice to identify the respective parameters of the physical and ML components is to formulate the problem as supervised learning on observed trajectories. However, this formulation leads to an infinite number of possible decompositions. To solve this ill-posedness, we reformulate the learning problem by introducing an upper bound on the prediction error of a physical-statistical model. This allows us to control the contribution of both the physical and statistical components to the overall prediction. This framework generalizes several existing hybrid schemes proposed in the literature. We provide theoretical guarantees on the wellposedness of our formulation along with a proof of convergence in a simple affine setting. For more complex dynamics, we validate our framework experimentally.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1853488882",
                        "name": "J\u00e9r\u00e9mie Don\u00e0"
                    },
                    {
                        "authorId": "2181917156",
                        "name": "Marie D\u00e9chelle"
                    },
                    {
                        "authorId": "1741426",
                        "name": "P. Gallinari"
                    },
                    {
                        "authorId": "2181918106",
                        "name": "Marina L\u00e9vy"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "The symplectic Gaussian process dynamic model (SGPD) is compared to the explicit Euler method, which corresponds to the standard state-space approach as explained in Section 1 and to the Hamiltonian neural network (Greydanus et al., 2019) with their code.",
                "We took two examples that were also considered in Greydanus et al. (2019).",
                "Neural networks are able to learn a separable Hamiltonian from data (Greydanus et al., 2019).",
                "\u2026model from a continuous pointof-view, treating the vector field f\u2014and not the discretization \u03c6\u2014as the sole object of interest (Heinonen and d\u2019Alch\u00e9 Buc, 2014; Greydanus et al., 2019), it remains an open question how to automatically choose\nthe most appropriate discretization for the learned model."
            ],
            "citingPaper": {
                "paperId": "9133533b03d908cee2b023b32a9b2e181c5eb617",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-01606",
                    "CorpusId": 231749717
                },
                "corpusId": 231749717,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9133533b03d908cee2b023b32a9b2e181c5eb617",
                "title": "Symplectic Gaussian Process Dynamics",
                "abstract": "Dynamics model learning is challenging and at the same time an active field of research. Due to potential safety critical downstream applications, such as control tasks, there is a need for theoretical guarantees. While GPs induce rich theoretical guarantees as function approximators in space, they do not explicitly cope with the time aspect of dynamical systems. However, propagating system properties through time is exactly what classical numerical integrators were designed for. We introduce a recurrent sparse Gaussian process based variational inference scheme that is able to discretize the underlying system with any explicit or implicit single or multistep integrator, thus leveraging properties of numerical integrators. In particular we discuss Hamiltonian problems coupled with symplectic integrators producing volume preserving predictions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2047583803",
                        "name": "K. Ensinger"
                    },
                    {
                        "authorId": "40897801",
                        "name": "Friedrich Solowjow"
                    },
                    {
                        "authorId": "48800465",
                        "name": "M. Tiemann"
                    },
                    {
                        "authorId": "2715093",
                        "name": "Sebastian Trimpe"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Among those studies, the most basic ones would be neural differential equations (Chen et al., 2018) and Hamiltonian neural networks (Greydanus et al., 2019).",
                ", 2018) and Hamiltonian neural networks (Greydanus et al., 2019).",
                "Thirdly, we consider the behavior of one of the most important energy-based models, Hamiltonian neural networks (Greydanus et al., 2019), especially when the loss function does not completely vanish, under the assumption that the learning target is an integrable system in the sense of Liouville.",
                "\u2026of research on predicting the corresponding physical phenomena by learning the energy function H in such equations with a neural network HNN (e.g., Greydanus et al. (2019); Cranmer et al. (2020); Chen et al. (2020); Zhong et al. (2020); Matsubara et al. (2020)); however, to the best of our\u2026",
                "In such a situation, Hamiltonian neural networks (Greydanus et al., 2019)\nd\ndt q1q2p1 p2  = ( O I\u2212I O ) \u2207HNN(q1, q2, p1, p2)\nare not appropriate because this model can be used only when p1 and p2 are generalized momenta; in the case considered here, they are supposed to be unknown."
            ],
            "citingPaper": {
                "paperId": "3b3ba44dfc42037a53315b6e6bf830cb28a4b5d6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-11923",
                    "CorpusId": 232035460
                },
                "corpusId": 232035460,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3b3ba44dfc42037a53315b6e6bf830cb28a4b5d6",
                "title": "Universal Approximation Properties of Neural Networks for Energy-Based Physical Systems",
                "abstract": "In Hamiltonian mechanics and the Landau theory, many physical phenomena are modeled using energy. In this paper, we prove the universal approximation property of neural network models for such physical phenomena. We also discuss behaviors of the models for integrable Hamiltonian systems when the loss function does not vanish completely by applying the KAM theory.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "123332055",
                        "name": "Y. Chen"
                    },
                    {
                        "authorId": "144872058",
                        "name": "Takashi Matsubara"
                    },
                    {
                        "authorId": "2580628",
                        "name": "Takaharu Yaguchi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "The performance difference between CLNN and CHNN is minor since their\narchitectures are similar.",
                "Neural ODE is leveraged by Symplectic ODE-Net (Zhong et al., 2020a) and Constrained Lagrangian/Hamiltonian Neural Network (CLNN/CHNN) (Finzi et al., 2020) to learn unknown sys-\ntem properties in rigid body dynamics without contacts.",
                "Deep Lagrangian Networks (Lutter et al., 2019), Hamiltonian Neural Networks (HNN) (Greydanus et al., 2019), Symplectic ODE-Net (SymODEN) (Zhong et al., 2020a), Symplectic Recurrent Neural Networks (SRNN) (Chen et al., 2020) and Lagrangian Neural Networks (LNN) (Cranmer et al., 2020) incorporate physics prior in the form of Lagrangian/Hamiltonian dynamics to learn physical motions with limited data and improved generalization.",
                "These two versions are combined with CLNN and CHNN to set up the following four neural network models: (i) CM-CD-CLNN, (ii) CM-CD-CHNN, (iii) CMr-CD-CLNN, and (iv) CMrCD-CHNN.",
                "The system properties are parametrized as in CLNN and CHNN (Finzi et al., 2020).",
                "Deep Lagrangian Networks (Lutter et al., 2019), Hamiltonian Neural Networks (HNN) (Greydanus et al., 2019), Symplectic ODE-Net (SymODEN) (Zhong et al., 2020a), Symplectic Recurrent Neural Networks (SRNN) (Chen et al., 2020) and Lagrangian Neural Networks (LNN) (Cranmer et al., 2020) incorporate\u2026",
                ", 2019), Hamiltonian Neural Networks (HNN) (Greydanus et al., 2019), Symplectic ODE-Net (SymODEN) (Zhong et al.",
                "In each task, we jointly learn system and contact properties from trajectory data by extending CLNN/CHNN with the proposed contact model.",
                "Our contact model extends CHNN/CLNN to enable learning of hybrid dynamics in rigid body systems and offer interpretability about system and contact properties."
            ],
            "citingPaper": {
                "paperId": "13913349c7a62a8204d831fc535b743ee715fe73",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-06794",
                    "CorpusId": 231925258
                },
                "corpusId": 231925258,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/13913349c7a62a8204d831fc535b743ee715fe73",
                "title": "A Differentiable Contact Model to Extend Lagrangian and Hamiltonian Neural Networks for Modeling Hybrid Dynamics",
                "abstract": "The incorporation of appropriate inductive bias plays a critical role in learning dynamics from data. A growing body of work has been exploring ways to enforce energy conservation in the learned dynamics by incorporating Lagrangian or Hamiltonian dynamics into the design of the neural network architecture. However, these existing approaches are based on differential equations, which does not allow discontinuity in the states, and thereby limits the class of systems one can learn. Real systems, such as legged robots and robotic manipulators, involve contacts and collisions, which introduce discontinuities in the states. In this paper, we introduce a differentiable contact model, which can capture contact mechanics, both frictionless and frictional, as well as both elastic and inelastic. This model can also accommodate inequality constraints, such as limits on the joint angles. The proposed contact model extends the scope of Lagrangian and Hamiltonian neural networks by allowing simultaneous learning of contact properties and system properties. We demonstrate this framework on a series of challenging 2D and 3D physical systems with different coefficients of restitution and friction.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35367989",
                        "name": "Yaofeng Desmond Zhong"
                    },
                    {
                        "authorId": "32553531",
                        "name": "Biswadip Dey"
                    },
                    {
                        "authorId": "2019491887",
                        "name": "Amit Chakraborty"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "da82c01808a478c70217603b722bb49d1473435d",
                "externalIds": {
                    "CorpusId": 231887396
                },
                "corpusId": 231887396,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/da82c01808a478c70217603b722bb49d1473435d",
                "title": "[Re] Hamiltonian Generative Networks",
                "abstract": "Hamilton\u02bcs equations are widely used in classical and quantum physics. The Hamiltonian Generative Network (HGN) is the first approach that aims to \u201dlearn the Hamiltonian dynamics of simple physical systems from high-dimensional observations without restrictive domain assumptions\u201d. To do so, a variational model is trained to reconstruct the evolution of physical systems directly from images by integrating the learned Hamiltonian. New trajectories can be sampled and rollouts can be performed forward and backward in time. In this work, we re-implement the HGN architecture and the physical environments (pendulum, body-spring system, and 2,3-bodies). We reproduce the paper experiments and we further expand them by testing on two new environments and one new integrator. Overall, we find that obtaining both good reconstruction and generative capabilities is hard and sensitive to the variational parameters. deviation \u03c3 = 0 . 1 to each phase-space coordinate at each step and render 32x32 image observations. Objects in the systems are represented as circles and we use different colors to represent different objects. We generate 50000 train samples and 10000 test samples for each physical system. To sample the initial conditions ( q 0 , p 0 ) , we first sample the total energy denoted as a radius r in phase space and then ( q 0 , p 0 ) are sampled uniformly on the circle of radius r . Note that here q and p represent the actual positions and momenta vectors of the bodies in the system. These are only used to generate the sequence of images and are not made available to the HGN architecture. The trajectories for each environment are computed using the ground-truth Hamiltonian dynamics and SciPy ODE solver [6].",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2180192363",
                        "name": "Balsells Rodas"
                    },
                    {
                        "authorId": "2180190625",
                        "name": "Federico Taschin"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Hamiltonian neural networks [17] embeds Hamiltonian mechanics in the architecture of the neural network to ensure that the model follows energy conservation laws."
            ],
            "citingPaper": {
                "paperId": "5a55453da0c2595525e53017d695c851c2b25f51",
                "externalIds": {
                    "DOI": "10.1109/OJIES.2021.3064820",
                    "CorpusId": 221376893
                },
                "corpusId": 221376893,
                "publicationVenue": {
                    "id": "de0a2547-5263-46d4-9830-07b59dd6f118",
                    "name": "IEEE Open Journal of the Industrial Electronics Society",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Open J Ind Electron Soc"
                    ],
                    "issn": "2644-1284",
                    "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8782706"
                },
                "url": "https://www.semanticscholar.org/paper/5a55453da0c2595525e53017d695c851c2b25f51",
                "title": "Physics Enhanced Data-Driven Models With Variational Gaussian Processes",
                "abstract": "Centuries of development in natural sciences and mathematical modeling provide valuable domain expert knowledge that has yet to be explored for the development of machine learning models. When modeling complex physical systems, both domain knowledge and data provide necessary information about the system. In this paper, we present a data-driven model that takes advantage of partial domain knowledge in order to improve generalization and interpretability. The presented approach, which we call EVGP (Explicit Variational Gaussian Process), has the following advantages: 1) using available domain knowledge to improve the assumptions (inductive bias) of the model, 2) scalability to large datasets, 3) improved interpretability. We show how the EVGP model can be used to learn system dynamics using basic Newtonian mechanics as prior knowledge. We demonstrate how the addition of prior domain-knowledge to data-driven models outperforms purely data-driven models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "27344798",
                        "name": "Daniel L. Marino"
                    },
                    {
                        "authorId": "2670374",
                        "name": "M. Manic"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In order to improve the generalization ability, DL techniques aim to learn the exact physical laws instead of approximating inner law from the data set [35], such as Hamiltonian neural network motivated by Hamiltonian mechanics [35], Symplectic recurrent neural network with Symplectic integration [36] and Lagrangian neural network for simulating Lagrangians [37]."
            ],
            "citingPaper": {
                "paperId": "6b89a44b62c2000df76cbe26c5949502742d36d4",
                "externalIds": {
                    "CorpusId": 233407841
                },
                "corpusId": 233407841,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6b89a44b62c2000df76cbe26c5949502742d36d4",
                "title": "Physics-informed Supervised Residual Learning for 2D Electromagnetic Forward Modeling",
                "abstract": "\u2014In this paper, we propose the physics-informed supervised residual learning (PISRL) which is a general framework for electromagnetic forward modeling based on deep learning. PISRL is designed to solve a system of linear matrix equations and not limited to a speci\ufb01c electromagnetic problem. Stationary and non-stationary iterative PISRL neural networks (SIPISRLNN and NSIPISRLNN) are designed based on the mathematical connection between residual neural network (ResNet) [1] and stationary or non-stationary iterative meth-ods. With convolutional neural network mapping residuals and modi\ufb01cations, SIPISRLNN and NSIPISRLNN can improve the approximated solutions via iterative process, which mimics the procedure of solving linear equations by stationary and non-stationary iterative methods. The generalities of SIPISRLNN and NSIPISRLNN are validated by solving different types of volume integral equations (VIEs) with non-lossy and lossy scatterers. In numerical results of non-lossy scatterers, the mean squared errors (MSEs) of SIPISRLNN and NSIPISRLNN \ufb01nally converge below 3 . 152 \u00d7 10 \u2212 4 and 4 . 8925 \u00d7 10 \u2212 7 ; in results of lossy scatterers, the MSEs converge below 1 . 2775 \u00d7 10 \u2212 4 and 1 . 567 \u00d7 10 \u2212 7 . The generalization abilities of SIPISRLNN and NSIPISRLNN are veri\ufb01ed by testing them on the data sets of contrast shapes and different incident frequencies that are unseen during the training process.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2067161406",
                        "name": "Tao Shan"
                    },
                    {
                        "authorId": "50707034",
                        "name": "Xiaoqian Song"
                    },
                    {
                        "authorId": "2090408646",
                        "name": "Rui Guo"
                    },
                    {
                        "authorId": "2247326966",
                        "name": "Fellow Ieee Fan Yang"
                    },
                    {
                        "authorId": "2247322349",
                        "name": "Member Ieee Shenheng Xu"
                    },
                    {
                        "authorId": "47628788",
                        "name": "Maokun Li"
                    },
                    {
                        "authorId": "143879525",
                        "name": "Shenheng Xu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "There is also an increasing interest in designing hybrid methods, which use a discretization of an ODE (in particular a Hamiltonian system) in order to learn the continuous representation of the data, see for instance Greydanus et al. (2019); Chen et al. (2020)."
            ],
            "citingPaper": {
                "paperId": "b329176277a4b5023f27682e0194422ef4663d81",
                "externalIds": {
                    "CorpusId": 233442634
                },
                "corpusId": 233442634,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b329176277a4b5023f27682e0194422ef4663d81",
                "title": "STABLE ARCHITECTURE FOR LEARNING LONG TIME DEPENDENCIES",
                "abstract": "Circuits of biological neurons, such as in the functional parts of the brain can be modeled as networks of coupled oscillators. Inspired by the ability of these systems to express a rich set of outputs while keeping (gradients of) state variables bounded, we propose a novel architecture for recurrent neural networks. Our proposed RNN is based on a time-discretization of a system of second-order ordinary differential equations, modeling networks of controlled nonlinear oscillators. We prove precise bounds on the gradients of the hidden states, leading to the mitigation of the exploding and vanishing gradient problem for this RNN. Experiments show that the proposed RNN is comparable in performance to the state of the art on a variety of benchmarks, demonstrating the potential of this architecture to provide stable and accurate RNNs for processing complex sequential data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "67164720",
                        "name": "T. Konstantin Rusch"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "b571afb72d61dcf20ac99a687fd48fab77482682",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-06610",
                    "CorpusId": 235422198
                },
                "corpusId": 235422198,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b571afb72d61dcf20ac99a687fd48fab77482682",
                "title": "Scalars are universal: Gauge-equivariant machine learning, structured like classical physics",
                "abstract": "There has been enormous progress in the last few years in designing conceivable (though not always practical) neural networks that respect the gauge symmetries\u2014or coordinate freedom\u2014of physical law. Some of these frameworks make use of irreducible representations, some make use of higher order tensor objects, and some apply symmetry-enforcing constraints. Different physical laws obey different combinations of fundamental symmetries, but a large fraction (possibly all) of classical physics is equivariant to translation, rotation, reflection (parity), boost (relativity), and permutations. Here we show that it is simple to parameterize universally approximating polynomial functions that are equivariant under these symmetries, or under the Euclidean, Lorentz, and Poincar\u00e9 groups, at any dimensionality d. The key observation is that nonlinear O(d)-equivariant (and related-group-equivariant) functions can be expressed in terms of a lightweight collection of scalars\u2014scalar products and scalar contractions of the scalar, vector, and tensor inputs. These results demonstrate theoretically that gauge-invariant deep learning models for classical physics with good scaling for large problems are feasible right now.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144790990",
                        "name": "Soledad Villar"
                    },
                    {
                        "authorId": "144735014",
                        "name": "D. Hogg"
                    },
                    {
                        "authorId": "1463037423",
                        "name": "Kate Storey-Fisher"
                    },
                    {
                        "authorId": "29892797",
                        "name": "Weichi Yao"
                    },
                    {
                        "authorId": "1401969150",
                        "name": "Ben Blum-Smith"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "This has inspired followup work to include other integrator schemes for deterministic, ordinary differential equations (ODE) such as symplectic integrators [6, 21, 53].",
                "Recently, these ideas have been rediscovered and used in deep architectures such as ResNets [22], with symplectic schemes for Hamiltonian [6, 21, 53] and Poisson networks [25]."
            ],
            "citingPaper": {
                "paperId": "037f82fa06421cf3a054dac5f97fb002dc0b7f4f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-09004",
                    "CorpusId": 235446859
                },
                "corpusId": 235446859,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/037f82fa06421cf3a054dac5f97fb002dc0b7f4f",
                "title": "Learning effective stochastic differential equations from microscopic simulations: combining stochastic numerics and deep learning",
                "abstract": "We identify effective stochastic differential equations (SDE) for coarse observables of \ufb01ne-grained particle- or agent-based simulations; these SDE then provide coarse surrogate models of the \ufb01ne scale dynamics. We approximate the drift and diffusivity functions in these effective SDE through neural networks, which can be thought of as effective stochastic ResNets. The loss function is inspired by, and embodies, the structure of established stochastic numerical integrators (here, Euler-Maruyama and Milstein); our approximations can thus bene\ufb01t from error analysis of these underlying numerical schemes. They also lend themselves naturally to \u201cphysics-informed\u201d gray-box identi\ufb01cation when approximate coarse models, such as mean \ufb01eld equations, are available. Our approach does not require long trajectories, works on scattered snapshot data, and is designed to naturally handle different time steps per snapshot. We consider both the case where the coarse collective observables are known in advance, as well as the case where they must be found in a data-driven manner.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144399320",
                        "name": "Felix Dietrich"
                    },
                    {
                        "authorId": "39110802",
                        "name": "A. Makeev"
                    },
                    {
                        "authorId": "1689633894",
                        "name": "G. Kevrekidis"
                    },
                    {
                        "authorId": "2474715",
                        "name": "N. Evangelou"
                    },
                    {
                        "authorId": "31588522",
                        "name": "Tom S. Bertalan"
                    },
                    {
                        "authorId": "39203137",
                        "name": "S. Reich"
                    },
                    {
                        "authorId": "3439407",
                        "name": "I. Kevrekidis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recently, neural networks have been proposed [15, 16] that not only learn the dynamics of the system but also",
                "Recent research [15, 16] features artificial neural networks that incorporate Hamiltonian structure to learn"
            ],
            "citingPaper": {
                "paperId": "a14cf952777549efe62ba0c9e7fea75de84bc2bc",
                "externalIds": {
                    "CorpusId": 235484512
                },
                "corpusId": 235484512,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a14cf952777549efe62ba0c9e7fea75de84bc2bc",
                "title": "Physics enhanced neural networks predict order and chaos",
                "abstract": "Anshul Choudhary, John F. Lindner*, 2 Elliott G. Holliday, Scott T. Miller, Sudeshna Sinha, 3 and William L. Ditto Nonlinear Artificial Intelligence Laboratory, Physics Department, North Carolina State University, Raleigh, NC 27607, USA Physics Department, The College of Wooster, Wooster, OH 44691, USA Indian Institute of Science Education and Research Mohali, Knowledge City, SAS Nagar, Sector 81, Manauli PO 140 306, Punjab, India (Dated: May 29, 2021)",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145638518",
                        "name": "A. Choudhary"
                    },
                    {
                        "authorId": "2468955",
                        "name": "J. Lindner"
                    },
                    {
                        "authorId": "1443437285",
                        "name": "Elliott G. Holliday"
                    },
                    {
                        "authorId": "50783104",
                        "name": "Scott T. Miller"
                    },
                    {
                        "authorId": "2053142",
                        "name": "S. Sinha"
                    },
                    {
                        "authorId": "3067672",
                        "name": "W. Ditto"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "9fd68ad790e4afd1eaa86f7f23e3446f52162bb2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-10820",
                    "CorpusId": 235490369
                },
                "corpusId": 235490369,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9fd68ad790e4afd1eaa86f7f23e3446f52162bb2",
                "title": "Compressing Deep ODE-Nets using Basis Function Expansions",
                "abstract": "The recently-introduced class of ordinary differential equation networks (ODE-Nets) establishes a fruitful connection between deep learning and dynamical systems. In this work, we reconsider formulations of the weights as continuous-depth functions using linear combinations of basis functions. This perspective allows us to compress the weights through a change of basis, without retraining, while maintaining near state-of-the-art performance. In turn, both inference time and the memory footprint are reduced, enabling quick and rigorous adaptation between computational environments. Furthermore, our framework enables meaningful continuous-in-time batch normalization layers using function projections. The performance of basis function compression is demonstrated by applying continuous-depth models to (a) image classification tasks using convolutional units and (b) sentence-tagging tasks using transformer encoder units.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40897456",
                        "name": "A. Queiruga"
                    },
                    {
                        "authorId": "2371914",
                        "name": "N. Benjamin Erichson"
                    },
                    {
                        "authorId": "90068212",
                        "name": "Liam Hodgkinson"
                    },
                    {
                        "authorId": "1717098",
                        "name": "Michael W. Mahoney"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ea74d10b1657e8e97a571a95db6ba24d372d6c14",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-12996",
                    "CorpusId": 236447490
                },
                "corpusId": 236447490,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ea74d10b1657e8e97a571a95db6ba24d372d6c14",
                "title": "Hamiltonian Operator Inference: Physics-preserving Learning of Reduced-order Models for Hamiltonian Systems",
                "abstract": "This work presents a nonintrusive physics-preserving method to learn reduced-order models (ROMs) of Hamiltonian systems. Traditional intrusive projection-based model reduction approaches utilize symplectic Galerkin projection to construct Hamiltonian reduced models by projecting Hamilton\u2019s equations of the full model onto a symplectic subspace. This symplectic projection requires complete knowledge about the full model operators and full access to manipulate the computer code. In contrast, the proposed Hamiltonian operator inference approach embeds the physics into the operator inference framework to develop a data-driven model reduction method that preserves the underlying symplectic structure. Our method exploits knowledge of the Hamiltonian functional to define and parametrize a Hamiltonian ROM form which can then be learned from data projected via symplectic projectors. The proposed method is \u2018gray-box\u2019 in that it utilizes knowledge of the Hamiltonian structure at the partial differential equation level, as well as knowledge of spatially local components in the system. However, it does not require access to computer code, only data to learn the models. Our numerical results demonstrate Hamiltonian operator inference on a linear wave equation, the cubic nonlinear Schr\u00f6dinger equation, and a nonpolynomial sine-Gordon equation. Accurate long-time predictions far outside the training time interval for nonlinear examples illustrate the generalizability of our learned models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47557147",
                        "name": "Harsh Sharma"
                    },
                    {
                        "authorId": "2108367753",
                        "name": "Zhu Wang"
                    },
                    {
                        "authorId": "2053218326",
                        "name": "B. Kramer"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Hamiltonian neural network (HNN) as proposed in [16] uses partial derivatives of the final output instead of the actual output value, to approximate an energy function and build a Hamiltonian system with a neural network.",
                "Recently, a series of research [16, 29, 40, 9, 39, 36] aim at learning partial differential equations from"
            ],
            "citingPaper": {
                "paperId": "a685eb8ad83cfcc251df8ad95c366ddac5f1609c",
                "externalIds": {
                    "DBLP": "conf/pkdd/XueNZFZE21",
                    "DOI": "10.1007/978-3-030-86517-7_8",
                    "CorpusId": 236492023
                },
                "corpusId": 236492023,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a685eb8ad83cfcc251df8ad95c366ddac5f1609c",
                "title": "Physics Knowledge Discovery via Neural Differential Equation Embedding",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2937530",
                        "name": "Yexiang Xue"
                    },
                    {
                        "authorId": "104331438",
                        "name": "M. Nasim"
                    },
                    {
                        "authorId": "2951389",
                        "name": "Maosen Zhang"
                    },
                    {
                        "authorId": "104354065",
                        "name": "C. Fan"
                    },
                    {
                        "authorId": "2153648873",
                        "name": "Xinghang Zhang"
                    },
                    {
                        "authorId": "1390185113",
                        "name": "A. El-Azab"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "To this end, we use the data generation process by (Greydanus et al., 2019).",
                "This type of behavior presents a difficulty for machine learning and is impossible for methods that assume the pendulum to be closed systems, such as Hamiltonian Neural Networks (HNNs) (Greydanus et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "9448d3fa25451a7678351f1f224f2305d689857d",
                "externalIds": {
                    "CorpusId": 236771933
                },
                "corpusId": 236771933,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9448d3fa25451a7678351f1f224f2305d689857d",
                "title": "MC-LSTM: Appendix",
                "abstract": "Neural networks that learn arithmetic operations have recently come into focus (Trask et al., 2018; Madsen & Johansen, 2020). Specialized neural modules for arithmetic operations could play a role for complex AI systems since cognitive studies indicate that there is a part of the brain that enables animals and humans to perform basic arithmetic operations (Nieder, 2016; Gallistel, 2018). Although this primitive number processor can only perform approximate arithmetic, it is a fundamental part of our ability to understand and interpret numbers (Dehaene, 2011).",
                "year": 2021,
                "authors": []
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Another related field of HamNet is neural physics engines (Sanchez-Gonzalez et al., 2018; 2019; Greydanus et al., 2019), which learn to conduct simulations that conform to physical laws."
            ],
            "citingPaper": {
                "paperId": "39607afb9953a6c63a97cc0250b644e0a1b7e13a",
                "externalIds": {
                    "CorpusId": 236923656
                },
                "corpusId": 236923656,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/39607afb9953a6c63a97cc0250b644e0a1b7e13a",
                "title": "TION WITH HAMILTONIAN NEURAL NETWORKS",
                "abstract": "Well-designed molecular representations (fingerprints) are vital to combine medical chemistry and deep learning. Whereas incorporating 3D geometry of molecules (i.e. conformations) in their representations seems beneficial, current 3D algorithms are still in infancy. In this paper, we propose a novel molecular representation algorithm which preserves 3D conformations of molecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit positions and momentums of atoms in a molecule interact in the Hamiltonian Engine following the discretized Hamiltonian equations. These implicit coordinations are supervised with real conformations with translation& rotation-invariant losses, and further used as inputs to the Fingerprint Generator, a message-passing neural network. Experiments show that the Hamiltonian Engine can well preserve molecular conformations, and that the fingerprints generated by HamNet achieve stateof-the-art performances on MoleculeNet, a standard molecular machine learning benchmark.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Ziyao Li"
                    },
                    {
                        "authorId": "2131871168",
                        "name": "Shuwen Yang"
                    },
                    {
                        "authorId": "2090871",
                        "name": "Guojie Song"
                    },
                    {
                        "authorId": "2090465220",
                        "name": "Lingsheng Cai"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "db170fcac0ec6abecc6279ff355de4ca64f334a7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-11684",
                    "CorpusId": 237303861
                },
                "corpusId": 237303861,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/db170fcac0ec6abecc6279ff355de4ca64f334a7",
                "title": "Disentangling ODE parameters from dynamics in VAEs",
                "abstract": "Deep networks have become increasingly of interest in dynamical system prediction, but generalization remains elusive. In this work, we consider the physical parameters of ODEs as factors of variation of the data generating process. By leveraging ideas from supervised disentanglement in VAEs, we aim to separate the ODE parameters from the dynamics in the latent space. Experiments show that supervised disentanglement allows VAEs to capture the variability in the dynamics and extrapolate better to ODE parameter spaces that were not present in the training data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "123727301",
                        "name": "Stathi Fotiadis"
                    },
                    {
                        "authorId": "2109420189",
                        "name": "Mario Lino"
                    },
                    {
                        "authorId": "12717675",
                        "name": "C. Cantwell"
                    },
                    {
                        "authorId": "2815535",
                        "name": "A. Bharath"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5674daa4bb2baefe57f1891c490117b36e08e5c1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-09974",
                    "CorpusId": 237581561
                },
                "corpusId": 237581561,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5674daa4bb2baefe57f1891c490117b36e08e5c1",
                "title": "Learning Adaptive Control for SE(3) Hamiltonian Dynamics",
                "abstract": "Fast adaptive control is a critical component for reliable robot autonomy in rapidly changing operational conditions. While a robot dynamics model may be obtained from first principles or learned from data, updating its parameters is often too slow for online adaptation to environment changes. This motivates the use of machine learning techniques to learn disturbance descriptors from trajectory data offline as well as the design of adaptive control to estimate and compensate the disturbances online. This paper develops adaptive geometric control for rigid-body systems, such as ground, aerial, and underwater vehicles, that satisfy Hamilton\u2019s equations of motion over the SE(3) manifold. Our design consists of an offline system identification stage, followed by an online adaptive control stage. In the first stage, we learn a Hamiltonian model of the system dynamics using a neural ordinary differential equation (ODE) network trained from state-control trajectory data with different disturbance realizations. The disturbances are modeled as a linear combination of nonlinear descriptors. In the second stage, we design a trajectory tracking controller with disturbance compensation from an energy-based perspective. An adaptive control law is employed to adjust the disturbance model online proportional to the geometric tracking errors on the SE(3) manifold. We verify our adaptive geometric controller for trajectory tracking on a fully-actuated pendulum and an under-actuated quadrotor.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2344318",
                        "name": "T. Duong"
                    },
                    {
                        "authorId": "50365495",
                        "name": "Nikolay A. Atanasov"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "2ba5c138940e54af8d34e5a64a5a92ff11929b13",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-12690",
                    "CorpusId": 239768202
                },
                "corpusId": 239768202,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2ba5c138940e54af8d34e5a64a5a92ff11929b13",
                "title": "Scalable Lipschitz Residual Networks with Convex Potential Flows",
                "abstract": "The Lipschitz constant of neural networks has been established as a key property to enforce the robustness of neural networks to adversarial examples. However, recent attempts to build 1-Lipschitz Neural Networks have all shown limitations and robustness have to be traded for accuracy and scalability or vice versa. In this work, we first show that using convex potentials in a residual network gradient flow provides a built-in 1-Lipschitz transformation. From this insight, we leverage the work on Input Convex Neural Networks to parametrize efficient layers with this property. A comprehensive set of experiments on CIFAR-10 demonstrates the scalability of our architecture and the benefit of our approach for `2 provable defenses. Indeed, we train very deep and wide neural networks (up to 1000 layers) and reach state-of-the-art results in terms of standard and certified accuracy, along with empirical robustness, in comparison with other 1-Lipschitz architectures.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47516106",
                        "name": "Laurent Meunier"
                    },
                    {
                        "authorId": "2134988135",
                        "name": "Blaise Delattre"
                    },
                    {
                        "authorId": "145533439",
                        "name": "Alexandre Araujo"
                    },
                    {
                        "authorId": "2311059",
                        "name": "A. Allauzen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Gradient fields modeling Gradient fields modeling is one of the popular tools for modeling manybody systems from predicting motion trajectories of physical systems (Greydanus et al., 2019; Norcliffe et al., 2020; Li et al., 2020) to estimating probabilistic densities of complex systems (Song & Ermon, 2019; Cai et al.",
                "\u2026modeling Gradient fields modeling is one of the popular tools for modeling manybody systems from predicting motion trajectories of physical systems (Greydanus et al., 2019; Norcliffe et al., 2020; Li et al., 2020) to estimating probabilistic densities of complex systems (Song & Ermon, 2019; Cai et\u2026"
            ],
            "citingPaper": {
                "paperId": "8e3ca5951f705406d5a89bf4cbd4d4c436cdd8f0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-14811",
                    "CorpusId": 240070366
                },
                "corpusId": 240070366,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8e3ca5951f705406d5a89bf4cbd4d4c436cdd8f0",
                "title": "Equivariant vector field network for many-body system modeling",
                "abstract": "Modeling many-body systems has been a long-standing challenge in science, from classical and quantum physics to computational biology. Equivariance is a critical physical symmetry for many-body dynamic systems, which enables robust and accurate prediction under arbitrary reference transformations. In light of this, great efforts have been put on encoding this symmetry into deep neural networks, which significantly boosts the prediction performance of down-streaming tasks. Some general equivariant models which are computationally efficient have been proposed, however, these models have no guarantee on the approximation power and may have information loss. In this paper, we leverage insights from the scalarization technique in differential geometry to model many-body systems by learning the gradient vector fields, which are SE(3) and permutation equivariant. Specifically, we propose the Equivariant Vector Field Network (EVFN), which is built on a novel tuple of equivariant basis and the associated scalarization and vectorization layers. Since our tuple equivariant basis forms a complete basis, learning the dynamics with our EVFN has no information loss and no tensor operations are involved before the final vectorization, which reduces the complex optimization on tensors to a minimum. We evaluate our method on predicting trajectories of simulated Newton mechanics systems with both full and partially observed data, as well as the equilibrium state of small molecules (molecular conformation) evolving as a statistical mechanics system. Experimental results across multiple tasks demonstrate that our model achieves best or competitive performance on baseline models in various types of datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "134861298",
                        "name": "Weitao Du"
                    },
                    {
                        "authorId": "2153527929",
                        "name": "He Zhang"
                    },
                    {
                        "authorId": "93584228",
                        "name": "Yuanqi Du"
                    },
                    {
                        "authorId": "47580728",
                        "name": "Qi Meng"
                    },
                    {
                        "authorId": "2154939268",
                        "name": "Wei Chen"
                    },
                    {
                        "authorId": "2064567675",
                        "name": "Bin Shao"
                    },
                    {
                        "authorId": "2110264337",
                        "name": "Tie-Yan Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Feed-forward models, with necessary inductive biases, have been used for sequence modelling both in language (Bai et al., 2018) and also in dynamical systems (Greydanus et al., 2019; Fotiadis et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "612fa87e3e380c207d6e5a0e4227054f63b0a54c",
                "externalIds": {
                    "CorpusId": 240070284
                },
                "corpusId": 240070284,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/612fa87e3e380c207d6e5a0e4227054f63b0a54c",
                "title": "DISENTANGLED GENERATIVE MODELS FOR ROBUST DYNAMICAL SYSTEM PREDICTION",
                "abstract": "Deep neural networks have become increasingly of interest in dynamical system prediction, but out-of-distribution generalization and long-term stability still remains challenging. In this work, we treat the domain parameters of dynamical systems as factors of variation of the data generating process. By leveraging ideas from supervised disentanglement and causal factorization, we aim to separate the domain parameters from the dynamics in the latent space of generative models. In our experiments we model dynamics both in phase space and in video sequences and conduct rigorous OOD evaluations 1. Results indicate that disentangled VAEs adapt better to domain parameters spaces that were not present in the training data. At the same time, disentanglement can improve the long-term and out-of-distribution predictions of state-of-the-art models in video sequences.2",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "123727301",
                        "name": "Stathi Fotiadis"
                    },
                    {
                        "authorId": "2109420189",
                        "name": "Mario Lino"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", DNNs) have been proven effective as solutions to a variety of scientific computing problems, including the approximation of solutions to partial differential equations [13, 14, 15, 16, 17]."
            ],
            "citingPaper": {
                "paperId": "3cf4056363924ba7009858fce00574da09869338",
                "externalIds": {
                    "DBLP": "conf/nips/DengDRKSTPM21",
                    "CorpusId": 244906490
                },
                "corpusId": 244906490,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3cf4056363924ba7009858fce00574da09869338",
                "title": "Benchmarking Data-driven Surrogate Simulators for Artificial Electromagnetic Materials",
                "abstract": "Artificial electromagnetic materials (AEMs), including metamaterials, derive their electromagnetic properties from geometry rather than chemistry. With the appropriate geometric design, AEMs have achieved exotic properties not realizable with conventional materials (e.g., cloaking or negative refractive index). However, understanding the relationship between the AEM structure and its properties is often poorly understood. While computational electromagnetic simulation (CEMS) may help design new AEMs, its use is limited due to its long computational time. Recently, it has been shown that deep learning can be an alternative solution to infer the relationship between an AEM geometry and its properties using a (relatively) small pool of CEMS data. However, the limited publicly released datasets and models and no widely-used benchmark for comparison have made using deep learning approaches even more difficult. Furthermore, configuring CEMS for a specific problem requires substantial expertise and time, making reproducibility challenging. Here, we develop a collection of three classes of AEM problems: metamaterials, nanophotonics, and color filter designs. We also publicly release software, allowing other researchers to conduct additional simulations for each system easily. Finally, we conduct experiments on our benchmark datasets with three recent neural network architectures: the multilayer perceptron (MLP), MLP-mixer, and transformer. We identify the methods and models that generalize best over the three problems to establish the best practice and baseline results upon which future research can build.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2111213163",
                        "name": "Yang Deng"
                    },
                    {
                        "authorId": "27320445",
                        "name": "Juncheng Dong"
                    },
                    {
                        "authorId": "1972460143",
                        "name": "Simiao Ren"
                    },
                    {
                        "authorId": "2065925874",
                        "name": "Omar Khatib"
                    },
                    {
                        "authorId": "36050308",
                        "name": "Mohammadreza Soltani"
                    },
                    {
                        "authorId": "1780864",
                        "name": "V. Tarokh"
                    },
                    {
                        "authorId": "2132419281",
                        "name": "Willie J. Padilla"
                    },
                    {
                        "authorId": "1787626",
                        "name": "Jordan M. Malof"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Recent work of Greydanus et al. [2019] demonstrates such process can be represented using a neural network similar to ResNet. Backward learning is to discover (the parameters of) the physics model automatically from experimental data, which also attracts recent attention in Niu et al. [2020] and Xue et al. [2021]. Backward learning can be achieved by embedding a neural network modeling the forward simulation into the overall architecture and minimizing a loss function which penalizes the difference between the simulated result and the observed data via back-propagation.",
                "Recent work of Greydanus et al. [2019] demonstrates such process can be represented using a neural network similar to ResNet."
            ],
            "citingPaper": {
                "paperId": "34bfd887b0fa70d9560c3a9442318568659d87b1",
                "externalIds": {
                    "DBLP": "conf/nips/SimaX21",
                    "CorpusId": 245022453
                },
                "corpusId": 245022453,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/34bfd887b0fa70d9560c3a9442318568659d87b1",
                "title": "LSH-SMILE: Locality Sensitive Hashing Accelerated Simulation and Learning",
                "abstract": "The advancement of deep neural networks over the last decade has enabled progress in scienti\ufb01c knowledge discovery in the form of learning Partial Differential Equations (PDEs) directly from experiment data. Nevertheless, forward simulation and backward learning of large-scale dynamic systems requires handling billions mutually interacting elements, the scale of which overwhelms current computing architectures. We propose Locality Sensitive Hashing Accelerated Simulation and Learning (LSH-S MI L E ), a uni\ufb01ed framework to scale up both forward simulation and backward learning of physics systems. LSH-S MI L E takes advantages of (i) the locality of PDE updates, (ii) similar temporal dynamics shared by multiple elements. LSH-S MI L E hashes elements with similar dynamics into a single hash bucket and handles their updates at once. This allows LSH-S MI L E to scale with respect to the number of non-empty hash buckets, a drastic improvement over conventional approaches. Theoretically, we prove a novel bound on the errors introduced by LSH-S MI L E . Experimentally, we demonstrate that LSH-S MI L E simulates physics systems at comparable quality with exact approaches, but with way less time and space complexity. Such savings also translate to better learning performance due to LSH-S MI L E \u2019s ability to propagate gradients over a long duration.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2144553163",
                        "name": "Chonghao Sima"
                    },
                    {
                        "authorId": "2937530",
                        "name": "Yexiang Xue"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Greydanus et al. (2019) use NNs to predict Hamiltonian from phase-space coordinates s = (p,q) and their derivatives."
            ],
            "citingPaper": {
                "paperId": "47046680631ce273867b51f58ff0c6a97105e982",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-01641",
                    "CorpusId": 246411653
                },
                "corpusId": 246411653,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/47046680631ce273867b51f58ff0c6a97105e982",
                "title": "Hamiltonian Operator Disentanglement of Content and Motion in Image Sequences",
                "abstract": "We introduce a deep generative model for image sequences that reliably factorise the latent space into content and motion variables. To model the diverse dynamics, we split the motion space into subspaces and introduce a unique Hamiltonian operator for each subspace. The Hamiltonian formulation provides reversible dynamics that constrain the evolution of the motion path along the low-dimensional manifold and conserves learnt invariant properties. The explicit split of the motion space decomposes the Hamiltonian into symmetry groups and gives long-term separability of the dynamics. This split also means we can learn content representations that are easy to interpret and control. We demonstrate the utility of our model by swapping the motion of two videos, generating long term sequences of various actions from a given image, unconditional sequence generation and image rotations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109256663",
                        "name": "M. A. Khan"
                    },
                    {
                        "authorId": "1728216",
                        "name": "A. Storkey"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Hamiltonian neural network (HNN) is a neural network that models the HamiltonianH and defines the dynamics following the Hamiltonian mechanics, thereby ensuring the energy conservation law [11].",
                "HNN [11] LNN [7] Skew Matrix Learning (Sec.",
                "One way to achieve this is to extract features from the images by using an autoencoder and to learn the equation of motion that the features satisfy [11]."
            ],
            "citingPaper": {
                "paperId": "f6ff2f5a64a4a6c47d00db328cd712d497f44d56",
                "externalIds": {
                    "DBLP": "conf/nips/ChenMY21",
                    "CorpusId": 247517286
                },
                "corpusId": 247517286,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f6ff2f5a64a4a6c47d00db328cd712d497f44d56",
                "title": "Neural Symplectic Form: Learning Hamiltonian Equations on General Coordinate Systems",
                "abstract": "In recent years, substantial research on the methods for learning Hamiltonian equations has been conducted. Although these approaches are very promising, the commonly used representation of the Hamilton equation uses the generalized momenta, which are generally unknown. Therefore, the training data must be represented in this unknown coordinate system, and this causes difficulty in applying the model to real data. Meanwhile, Hamiltonian equations also have a coordinate-free expression that is expressed by using the symplectic 2-form. In this paper, we propose a model that learns the symplectic form from data using neural networks, thereby providing a method for learning Hamiltonian equations from data represented in general coordinate systems, which are not limited to the generalized coordinates and the generalized momenta. Consequently, the proposed method is capable not only of modeling the target equations of both Hamiltonian and Lagrangian formalisms but also of extracting unknown Hamiltonian structures hidden in the data. For example, many polynomial ordinary differential equations such as the Lotka\u2013Volterra equation are known to admit non-trivial Hamiltonian structures, and our numerical experiments show that such structures can certainly be learned from data. Technically, each symplectic 2-form is associated with a skew-symmetric matrix, but not all skew-symmetric matrices define a symplectic 2-form. In the proposed method, using the fact that symplectic 2-forms are derived as the exterior derivative of certain differential 1-forms, we model the differential 1-form by neural networks, thereby improving the efficiency of learning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115869301",
                        "name": "Yu-Hsueh Chen"
                    },
                    {
                        "authorId": "144872058",
                        "name": "Takashi Matsubara"
                    },
                    {
                        "authorId": "2580628",
                        "name": "Takaharu Yaguchi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "state [12, 13, 14, 15], analytic constraints [16, 17], or evaluation function [18, 19]."
            ],
            "citingPaper": {
                "paperId": "4402f4a834f5dd8bcc6cf27093e4c53bc3f3ecbd",
                "externalIds": {
                    "CorpusId": 249037923
                },
                "corpusId": 249037923,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4402f4a834f5dd8bcc6cf27093e4c53bc3f3ecbd",
                "title": "Spectral PINNs: Fast Uncertainty Propagation with Physics-Informed Neural Networks",
                "abstract": "Physics-informed neural networks (PINNs) promise to signi\ufb01cantly speed up partial differential equation (PDE) solvers. However, most PINNs can only solve deterministic PDEs. Here, we consider stochastic PDEs that contain partially unknown parameters. We aim to quickly quantify the impact of uncertain parameters onto the solution of a PDE - that is - we want to perform fast uncertainty propagation. Classical uncertainty propagation methods such as Monte Carlo sampling, stochastic Galerkin, collocation, or discrete projection methods become computationally too expensive with an increasing number of stochastic parameters. For example, the well-known spectral or polynomial chaos expansions achieve to separate the spatiotemporal and probabilistic domains and offer theoretical guarantees and fast computation of stochastic summaries (e.g., mean), but can be computationally expensive to form. Our Spectral PINNs approximate the underly-ing spectral coef\ufb01cients with a neural network and reduce the computational cost of the spectral expansion while maintaining guarantees. We derive the method for partial differential equations, discuss runtime, demonstrate initial results on the convection-diffusion equation, and provide steps towards convergence guarantees.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "31336155",
                        "name": "B. Leshchinskiy"
                    },
                    {
                        "authorId": "1404333614",
                        "name": "C. Requena-Mesa"
                    },
                    {
                        "authorId": "2342264",
                        "name": "F. Chishtie"
                    },
                    {
                        "authorId": "1423321194",
                        "name": "N. D\u00edaz-Rodr\u00edguez"
                    },
                    {
                        "authorId": "3200568",
                        "name": "O. Boulais"
                    },
                    {
                        "authorId": "1388130057",
                        "name": "A. Sankaranarayanan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "87621506aefb6023a814141744bab386cee52d96",
                "externalIds": {
                    "MAG": "3140504026",
                    "DOI": "10.1587/NOLTA.12.134",
                    "CorpusId": 234201303
                },
                "corpusId": 234201303,
                "publicationVenue": {
                    "id": "8326ad98-e9ee-4518-81dd-5ccaa5331c05",
                    "name": "Nonlinear Theory and Its Applications IEICE",
                    "type": "journal",
                    "alternate_names": [
                        "Nonlinear Theory and Its Applications, IEICE",
                        "Nonlinear Theory It Appl IEICE"
                    ],
                    "issn": "2185-4106",
                    "url": "https://www.jstage.jst.go.jp/browse/nolta",
                    "alternate_urls": [
                        "https://www.jstage.jst.go.jp/browse/nolta/list/-char/en",
                        "http://www.nolta.ieice.org/data/archives/archives.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/87621506aefb6023a814141744bab386cee52d96",
                "title": "Negotiating the separatrix with machine learning",
                "abstract": ": Physics-informed machine learning has recently been shown to e\ufb03ciently learn complex trajectories of nonlinear dynamical systems, even when order and chaos coexist. However, care must be taken when one or more variables are unbounded, such as in rotations. Here we use the framework of Hamiltonian Neural Networks (HNN) to learn the complex dynamics of nonlinear single and double pendulums, which can both librate and rotate, by mapping the unbounded phase space onto a compact cylinder. We clearly demonstrate that our approach can successfully forecast the motion of these challenging systems, capable of both bounded and unbounded motion. It is also evident that HNN can yield an energy surface that closely matches the surface generated by the true Hamiltonian function. Further we observe that the relative energy error for HNN decreases as a power law with number of training pairs, with HNN clearly outperforming conventional neural networks quantitatively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50783104",
                        "name": "Scott T. Miller"
                    },
                    {
                        "authorId": "2468955",
                        "name": "J. Lindner"
                    },
                    {
                        "authorId": "145638518",
                        "name": "A. Choudhary"
                    },
                    {
                        "authorId": "2053142",
                        "name": "S. Sinha"
                    },
                    {
                        "authorId": "3067672",
                        "name": "W. Ditto"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "(3) HNN approximates the Hamiltonian H from the data using a neural network (Greydanus et al., 2019).",
                "Hamiltonian neural networks (HNN) and Lagrangian neural networks (LNN) learn Hamiltonian and Lagrangian mechanics, ensuring the energy conservation in continuous time (Greydanus et al., 2019; Cranmer et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "c6b7b8530c2c76a582c863f12b66f675761b489b",
                "externalIds": {
                    "CorpusId": 251244924
                },
                "corpusId": 251244924,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c6b7b8530c2c76a582c863f12b66f675761b489b",
                "title": "D EEP D ISCRETE -T IME L AGRANGIAN M ECHANICS",
                "abstract": "Ensuring physical laws such as the conservation law of energy is important for physics simulations. Recent studies demonstrated that neural networks successfully learn physical dynamics and conserve the system energy using symplectic integrators or a discrete gradient method. While their approaches depend on the canonical momentum or velocity, measuring an accurate velocity is troublesome because it is often measured by a linear interpolation of two points. Without an accurate velocity, a learned dynamics may be greatly different from the teacher system. In this paper, we propose a neural network based on discrete-time Lagrangian mechanics. The proposed approach learns the physical dynamics only from the position data and conserves the energy strictly in discrete-time by using a discrete gradient method. Experimental results on simulated physical systems demonstrated that our approach learns the energy surface in the state space accurately and conserves the modeled system energy strictly.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144872058",
                        "name": "Takashi Matsubara"
                    },
                    {
                        "authorId": "2580628",
                        "name": "Takaharu Yaguchi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Artificial neural network models [45], [46] have been used to deal with the Hamiltonian and Lagrangian mechanics, but these approaches consider an ideal situation."
            ],
            "citingPaper": {
                "paperId": "a1846cdee2aaae92fc8cf8c22f8d65583456e125",
                "externalIds": {
                    "DBLP": "journals/access/SeoL21",
                    "DOI": "10.1109/ACCESS.2021.3103876",
                    "CorpusId": 237246998
                },
                "corpusId": 237246998,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a1846cdee2aaae92fc8cf8c22f8d65583456e125",
                "title": "Neural Network-Based Intuitive Physics for Non-Inertial Reference Frames",
                "abstract": "Classical mechanics offers us reliable means to predict various physical quantities, but it is difficult to derive the precise dynamic equations underlying most phenomena and obtain physical quantities in real-world situations. Intuitive physics, the ability to intuitively understand and predict physical phenomena, prevents this complication. However, its applications are confined to the inertial frame of reference. Here, we explored the potentials of neural network-based intuitive physics for solving non-inertial reference frames. We designed three experiments, each of which represents different types of real-world challenges. The task required predicting the speed of an object while the observer accelerates. We demonstrated that multilayer perceptron, invariant methods, and long-term memory networks successfully learn underlying dynamics from observations. This implies that neural network-based intuitive physics provides alternative means to predict various quantities in real-world applications that are unsolvable by classical physics methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2148247097",
                        "name": "Jongwoo Seo"
                    },
                    {
                        "authorId": "2152574490",
                        "name": "Sang Wan Lee"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "8138d2e2d01870d9a2f6eda5dee0c53d84567249",
                "externalIds": {
                    "CorpusId": 253253106
                },
                "corpusId": 253253106,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8138d2e2d01870d9a2f6eda5dee0c53d84567249",
                "title": "L EARNED S IMULATORS THAT SATISFY THE L AWS OF T HERMODYNAMICS",
                "abstract": "of in systems positive of tion scheme Energy-entropy-momentum integration schemes for general discrete non-smooth dissipative problems in thermomechanics. on",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "69035385",
                        "name": "B. Moya"
                    },
                    {
                        "authorId": "51181453",
                        "name": "A. Badias"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "c542d490329acd85e21a9f1846cd083c68a805c7",
                "externalIds": {
                    "CorpusId": 260442745
                },
                "corpusId": 260442745,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c542d490329acd85e21a9f1846cd083c68a805c7",
                "title": "On Second Order Behaviour in Augmented Neural ODEs: A Short Summary",
                "abstract": "In Norcliffe et al. [13], we discussed and systematically analysed how Neural ODEs (NODEs) can learn higher-order order dynamics. In particular, we focused on second-order dynamic behaviour and analysed Augmented NODEs (ANODEs), showing that they can learn second-order dynamics with only a few augmented dimensions, but are unable to correctly model the velocity (\ufb01rst derivative). In response, we proposed Second Order NODEs (SONODEs), that build on top of ANODEs, but explicitly take into account the second-order physics-based inductive biases. These biases, besides making them more ef\ufb01cient and noise-robust when modelling second-order dynamics, make them more interpretable than ANODEs, therefore more suitable in many real-world scienti\ufb01c modelling applications.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1748965720",
                        "name": "Alexander Norcliffe"
                    },
                    {
                        "authorId": "46195895",
                        "name": "Cristian Bodnar"
                    },
                    {
                        "authorId": "80740711",
                        "name": "Ben Day"
                    },
                    {
                        "authorId": "2050787",
                        "name": "N. Simidjievski"
                    },
                    {
                        "authorId": "2075355155",
                        "name": "Pietro Lio'"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "One line of work aims to understand how neural networks can be structured and trained to reproduce known physical system behavior, with the goal of designing general methods applicable in a variety of settings (Greydanus et al., 2019; Sanchez-Gonzalez et al., 2019; Chen et al., 2020; Sanchez-Gonzalez et al., 2018; Raissi et al., 2017; 2019; Lu et al., 2019; Haghighat et al., 2020; Tartakovsky et al., 2018).",
                "\u2026structured and trained to reproduce known physical system behavior, with the goal of designing general methods applicable in a variety of settings (Greydanus et al., 2019; Sanchez-Gonzalez et al., 2019; Chen et al., 2020; Sanchez-Gonzalez et al., 2018; Raissi et al., 2017; 2019; Lu et al., 2019;\u2026"
            ],
            "citingPaper": {
                "paperId": "52c8a102b41e519140c896c0d79a078c69d8135e",
                "externalIds": {
                    "CorpusId": 253250024
                },
                "corpusId": 253250024,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/52c8a102b41e519140c896c0d79a078c69d8135e",
                "title": "A N E XTENSIBLE B ENCHMARK S UITE FOR L EARNING TO S IMULATE P HYSICAL S YSTEMS",
                "abstract": "Time integration of models of physical systems is a core task of scienti\ufb01c computing. Recently, there has been a of in data-driven methods that learn from data a model of the physical system and then integrate it in time to make predictions. This work introduces benchmarks for evaluating data-driven methods on a variety of physical systems and proposes evaluation scenarios. The proposed benchmarks comprise three representative physical systems (spring, spring mesh, wave) and a collection of classical time integrators as baselines. For demonstration purposes, we apply several data-driven methods to the benchmarks and report accuracy and computational ef\ufb01ciency.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1589786778",
                        "name": "Joan Bruna"
                    },
                    {
                        "authorId": "3241132",
                        "name": "Daniele Panozzo"
                    },
                    {
                        "authorId": "2863679",
                        "name": "B. Peherstorfer"
                    },
                    {
                        "authorId": "145516498",
                        "name": "D. Zorin"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "result"
            ],
            "isInfluential": true,
            "contexts": [
                "In parallel, physics and machine learning have been forging strong ties based for example on Hamiltonian theory and the integration of ordinary differential equations in the latent space to describe the evolution of dynamical systems (Chen et al. (2018); Toth et al. (2019); Greydanus et al. (2019)).",
                "We consider a simple environment similar to that studied in Higgins et al. (2018) and Caselles-Dupr\u00e9 et al.",
                "Another requirement we wish to make concerning the representation to be learned is that it is disentangled in the sense of Higgins et al. (2018). Formally, if there exists a subgroup decomposition of G such that G = G1 \u00d7 G2.",
                "Another requirement we wish to make concerning the representation to be learned is that it is disentangled in the sense of Higgins et al. (2018). Formally, if there exists a subgroup decomposition of G such that G = G1 \u00d7 G2... \u00d7 Gn, we would like to decompose the representation (\u03c1, V ) in subrepresentations V = V1 \u2295 V2... \u2295 Vn such that the restricted subrepresentations (\u03c1|Gi , Vi)i are non-trivial and the restricted subrepresentations (\u03c1|Gi , Vj)j 6=i are trivial (we recall that a trivial representation of G is equal to the identity for every element of the group G). This definition of disentangled representations has several advantages. First, it maps onto an intuitive notion of disentangled representation as one that separates the data generative factors into different subspaces. It also provides a principled resolution to several points of contention concerning what should be considered a data generative factor, which ones can be disentangled and which ones cannot, and what dimensionality the representation of each factor should have. However, despite theoretical analysis by Higgins et al. (2018) and Caselles-Dupr\u00e9 et al."
            ],
            "citingPaper": {
                "paperId": "5800c6078ba37382b220ccfac252caf1b60e4e38",
                "externalIds": {
                    "DBLP": "conf/nips/QuessardBC20",
                    "MAG": "3105142225",
                    "CorpusId": 227275384
                },
                "corpusId": 227275384,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5800c6078ba37382b220ccfac252caf1b60e4e38",
                "title": "Learning Disentangled Representations and Group Structure of Dynamical Environments",
                "abstract": "Learning disentangled representations is a key step towards effectively discovering and modelling the underlying structure of environments. In the natural sciences, physics has found great success by describing the universe in terms of symmetry preserving transformations. Inspired by this formalism, we propose a framework, built upon the theory of group representation, for learning representations of a dynamical environment structured around the transformations that generate its evolution. Experimentally, we learn the structure of explicitly symmetric environments without supervision from observational data generated by sequential interactions. We further introduce an intuitive disentanglement regularisation to ensure the interpretability of the learnt representations. We show that our method enables accurate long-horizon predictions, and demonstrate a correlation between the quality of predictions and disentanglement in the latent space.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1402910228",
                        "name": "Robin Quessard"
                    },
                    {
                        "authorId": "2067064746",
                        "name": "T. Barrett"
                    },
                    {
                        "authorId": "37289174",
                        "name": "W. Clements"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "In the rest of the sections, we will talk a little about the theory of Hamiltonian Neural Networks and then we will move on to our implementations and results.",
                "In this work, we reproduce the paper1 Hamiltonian Neural Network [1].",
                "After training, we use these models to predict (\u2202q/\u2202t, \u2202p/\u2202t) given (q, p), and use this to approximate the trajectory of the system using our differential equation given by equation [1].",
                "This equation is found using the domain-specific knowledge, but in the paper, the equation is learned by the Hamiltonian Neural Network on its own using the data.",
                "The approach presented in the paper takes inspiration from the Hamiltonian mechanics, a branch of physics that deals with conservation laws and invariances, and defines the concept of Hamiltonian Neural Networks (HNNs)."
            ],
            "citingPaper": {
                "paperId": "032a2e15bcf833f07110ff4571020625adbb65b3",
                "externalIds": {
                    "CorpusId": 251925912
                },
                "corpusId": 251925912,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/032a2e15bcf833f07110ff4571020625adbb65b3",
                "title": "Replication / NeurIPS 2019 Reproducibility Challenge [Re] Hamiltonian Neural Networks",
                "abstract": "In today\u2019s world, neural networks are being in almost every discipline resulting in signi\ufb01cant improvement in all the tools and applications. But in the \ufb01eld of Physics, they struggle to attain the basic laws like conservation of momentum. The paper Hamiltonian Neural Networks addresses this issue by using Hamiltonian mechanics to train the neural network in an unsupervised method. The following report is an explanation of the paper and the code to reproduce the claimed results.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1410310337",
                        "name": "Ayush Garg"
                    },
                    {
                        "authorId": "1739332228",
                        "name": "S. S. Kagi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                ", 2016) or due to the ambiguities in unsupervised learning (Greydanus et al., 2019; Cranmer et al., 2020b).",
                "Recent work on physics-based deep learning injects structure in the latent space of the dynamics using Lagrangian and Hamiltonian operators (Greydanus et al., 2019; Chen et al., 2020; Toth et al., 2020; Sanchez-Gonzalez et al., 2019; Cranmer et al., 2020b; Zhong et al., 2020), by explicitly conserving physical quantities, or with ground truth supervision (Asenov et al.",
                "\u2026in the context of differentiable physics simulation Degrave et al. (2016); de Avila Belbute-Peres et al. (2018); Cranmer et al. (2020b); Toth et al. (2020); Greydanus et al. (2019); Sanchez-Gonzalez et al. (2019), we focus on more challenging systems which have not been studied in prior art.",
                "Recent work on physics-based deep learning injects structure in the latent space of the dynamics using Lagrangian and Hamiltonian operators (Greydanus et al., 2019; Chen et al., 2020; Toth et al., 2020; Sanchez-Gonzalez et al., 2019; Cranmer et al., 2020b; Zhong et al., 2020), by explicitly\u2026",
                "These methods require few prior assumptions about the system itself, but lack interpretability due to entangled variational factors (Chen et al., 2016) or due to the ambiguities in unsupervised learning (Greydanus et al., 2019; Cranmer et al., 2020b)."
            ],
            "citingPaper": {
                "paperId": "36d2903f2251c14bcddceb285c75c5c10b4c1256",
                "externalIds": {
                    "CorpusId": 232084662
                },
                "corpusId": 232084662,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/36d2903f2251c14bcddceb285c75c5c10b4c1256",
                "title": "Differentiable simulation for system identification and visuomotor control \u2207 Sim : D IFFERENTIABLE SIMULATION FOR SYSTEM IDENTIFICATION AND VISUOMOTOR CONTROL",
                "abstract": "We consider the problem of estimating an object\u2019s physical properties such as mass, friction, and elasticity directly from video sequences. Such a system identification problem is fundamentally ill-posed due to the loss of information during image formation. Current solutions require precise 3D labels which are labor-intensive to gather, and infeasible to create for many systems such as deformable solids or cloth. We present \u2207Sim, a framework that overcomes the dependence on 3D supervision by leveraging differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation. This novel combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them. Moreover, our unified computation graph \u2013 spanning from the dynamics and through the rendering process \u2013 enables learning in challenging visuomotor control tasks, without relying on state-based (3D) supervision, while obtaining performance competitive to or better than techniques that rely on precise 3D labels.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "7434636",
                        "name": "Krishna Murthy Jatavallabhula"
                    },
                    {
                        "authorId": "46637939",
                        "name": "M. Macklin"
                    },
                    {
                        "authorId": "2970150",
                        "name": "Florian Golemo"
                    },
                    {
                        "authorId": "2961618",
                        "name": "Vikram S. Voleti"
                    },
                    {
                        "authorId": "2060687770",
                        "name": "Linda"
                    },
                    {
                        "authorId": "2096957486",
                        "name": "Petrini"
                    },
                    {
                        "authorId": "144069571",
                        "name": "Martin Weiss"
                    },
                    {
                        "authorId": "41226293",
                        "name": "Breandan Considine"
                    },
                    {
                        "authorId": "1585278372",
                        "name": "J\u00e9r\u00f4me Parent-L\u00e9vesque"
                    },
                    {
                        "authorId": "47966782",
                        "name": "Kevin Xie"
                    },
                    {
                        "authorId": "2102299108",
                        "name": "Kenny"
                    },
                    {
                        "authorId": "2083161852",
                        "name": "Erleben"
                    },
                    {
                        "authorId": "3198259",
                        "name": "L. Paull"
                    },
                    {
                        "authorId": "2162768",
                        "name": "F. Shkurti"
                    },
                    {
                        "authorId": "1795014",
                        "name": "D. Nowrouzezahrai"
                    },
                    {
                        "authorId": "37895334",
                        "name": "S. Fidler"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "The use of ordinary differential equation (ODE) solvers within deep learning frameworks has allowed end-to-end training of Neural ODEs (Chen et al., 2018) in a variety of settings.",
                "We consider parametrizing event functions with neural networks in the context of solving ODEs, extending Neural ODEs to implicitly defined termination times.",
                "\u20262020), generative modeling (Grathwohl et al., 2018; Zhang et al., 2018; Chen & Duvenaud, 2019; Onken et al., 2020), time series modeling (Rubanova et al., 2019; De Brouwer et al., 2019; Jia & Benson, 2019; Kidger et al., 2020), and physics-based models (Zhong et al., 2019; Greydanus et al., 2019).",
                "Though there exist physics-motivated parameterizations of Neural ODEs (e.g. Zhong et al., 2019; Greydanus et al., 2019; Finzi et al., 2020), these cannot model collision effects.",
                ", 2020), and physics-based models (Zhong et al., 2019; Greydanus et al., 2019).",
                "By introducing differentiable termination criteria in Neural ODEs, our approach allows the model to efficiently and automatically handle state discontinuities.",
                "To further expand the applications of Neural ODEs, we investigate the parameterization and learning of a termination criteria, such that the termination time is only implicitly defined and will depend on changes in the continuous-time state."
            ],
            "citingPaper": {
                "paperId": "16336c5a2c5121822850fd2c2d13454003afc671",
                "externalIds": {
                    "DOI": "10.1002/9781119626879.ch6",
                    "CorpusId": 236921786
                },
                "corpusId": 236921786,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/16336c5a2c5121822850fd2c2d13454003afc671",
                "title": "ORDINARY DIFFERENTIAL EQUATIONS",
                "abstract": "The existing Neural ODE formulation relies on an explicit knowledge of the termination time. We extend Neural ODEs to implicitly defined termination criteria modeled by neural event functions, which can be chained together and differentiated through. Neural Event ODEs are capable of modeling discrete and instantaneous changes in a continuous-time system, without prior knowledge of when these changes should occur or how many such changes should exist. We test our approach in modeling hybrid discreteand continuoussystems such as switching dynamical systems and collision in multi-body systems, and we propose simulation-based training of point processes with applications in discrete control.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51466615",
                        "name": "Ricky T. Q. Chen"
                    },
                    {
                        "authorId": "1773498",
                        "name": "Brandon Amos"
                    },
                    {
                        "authorId": "1729762",
                        "name": "Maximilian Nickel"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "On the other hand, [16] and [17] have utilized Hamiltonian mechanics for learning dynamics from data."
            ],
            "citingPaper": {
                "paperId": "faa015865914d26d6400d978251337e712139a80",
                "externalIds": {
                    "CorpusId": 211734081
                },
                "corpusId": 211734081,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/faa015865914d26d6400d978251337e712139a80",
                "title": "ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations (DeepDiffEq) DISSIPATIVE SYMODEN: ENCODING HAMILTONIAN DYNAMICS",
                "abstract": "In this work, we introduce Dissipative SymODEN, a deep learning architecture which can infer the dynamics of a physical system with dissipation from observed state trajectories. To improve prediction accuracy while reducing network size, Dissipative SymODEN encodes the port-Hamiltonian dynamics with energy dissipation and external input into the design of its computation graph and learns the dynamics in a structured way. The learned model, by revealing key aspects of the system, such as the inertia, dissipation, and potential energy, paves the way for energy-based controllers.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2112888509",
                        "name": "Yao Zhong"
                    },
                    {
                        "authorId": "32553531",
                        "name": "Biswadip Dey"
                    },
                    {
                        "authorId": "2019491887",
                        "name": "Amit Chakraborty"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "On the other hand, [16] and [17] have utilized Hamiltonian mechanics for learning dynamics from data."
            ],
            "citingPaper": {
                "paperId": "3ca790fb9bcd65f5c44fb085b8cf1a724fd4f5fe",
                "externalIds": {
                    "CorpusId": 211835536
                },
                "corpusId": 211835536,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3ca790fb9bcd65f5c44fb085b8cf1a724fd4f5fe",
                "title": "DISSIPATIVE SYMODEN: ENCODING HAMILTONIAN DYNAMICS",
                "abstract": "In this work, we introduce Dissipative SymODEN, a deep learning architecture which can infer the dynamics of a physical system with dissipation from observed state trajectories. To improve prediction accuracy while reducing network size, Dissipative SymODEN encodes the port-Hamiltonian dynamics with energy dissipation and external input into the design of its computation graph and learns the dynamics in a structured way. The learned model, by revealing key aspects of the system, such as the inertia, dissipation, and potential energy, paves the way for energy-based controllers.",
                "year": 2020,
                "authors": []
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "587cf6001c8265dea75e42a204446fb51895e78e",
                "externalIds": {
                    "MAG": "3072670751",
                    "DOI": "10.5075/EPFL-THESIS-10257",
                    "CorpusId": 226558017
                },
                "corpusId": 226558017,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/587cf6001c8265dea75e42a204446fb51895e78e",
                "title": "Deep Generative Models and Applications",
                "abstract": "Over the past few years, there have been fundamental breakthroughs in core problems in machine learning, largely driven by advances in deep neural networks. The amount of annotated data drastically increased and supervised deep discriminative models exceeded human-level performances in certain object detection tasks [Russakovsky et al., 2015, He et al., 2015]. The increasing availability in quantity and complexity of unlabelled data also opens up exciting possibilities for the development of unsupervised learning methods. Among the family of unsupervised methods, deep generative models find numerous applications. Moreover, as real-world applications include high dimensional data, the ability of generative models to automatically learn semantically meaningful subspaces makes their advancement an essential step toward developing more efficient algorithms. Generative Adversarial Networks (GANs) are a family of unsupervised generative algorithms that have demonstrated impressive performance for data synthesis and are now used in a wide range of computer vision tasks. Despite this success, they gained a reputation for being difficult to train, which results in a time-consuming and human-involved development process to use them. In the first part of this thesis, we focus on improving the stability and the performances of GANs. Foremost, we consider an alternative training process to the standard one, named SGAN, in which several adversarial \u201clocal\u201d pairs of networks are trained independently so that a \u201cglobal\u201d supervising pair of networks can be trained against them. The goal is to train the global pair with the corresponding ensemble opponent for improved performances in terms of mode coverage. Experimental results on both toy and real-world problems demonstrate that this approach outperforms standard training in terms of better mitigating mode collapse, stability while converging and that it surprisingly, increases the convergence speed as well. Next, to further reduce the computational footprint while maintaining the stability and performance advantages of SGAN, we focus on training single pair of adversarial networks using variance reduced gradient. More precisely, we study the effect of the stochastic gradient noise on the training of generative adversarial networks (GANs) and show that it can prevent the convergence of standard game optimization methods, while the batch version converges. We address this issue with two stochastic variance-reduced gradient and extragradient optimization algorithms for GANs, named SVRG-GAN and SVRE, respectively. As batch extragradient is the only method that converges for simple examples of games, our analyses focus on SVRE, which method for a large class of games improves upon the previous convergence rates proposed in the literature. We observe empirically that SVRE performs similarly to a batch method",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2458542",
                        "name": "Tatjana Chavdarova"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "da800c3aedecb05cad3c11803d298eedaef594c7",
                "externalIds": {
                    "CorpusId": 231718122
                },
                "corpusId": 231718122,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/da800c3aedecb05cad3c11803d298eedaef594c7",
                "title": "The Error Analysis of Numerical Integrators for Deep Neural Network Modeling of Differential Equations",
                "abstract": "Recently, learning equations of motion to describe dynamics from data using neural networks has been attracting attention. During such training, numerical integration is used to compare the data with the solution of the neural network model; however, discretization errors due to numerical integration prevent the model from being trained correctly. In this study, we propose a theoretical framework for investigating the effect of numerical integration on modeling errors and perform the error analysis of the Runge-Kutta methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1572323664",
                        "name": "Shunpei Terakawa"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "While incorporating physical models into deep learning approaches has been studied in various forms [10, 5, 3, 11], differentiable simulations allow a trained model to autonomously explore and experience the physical environment and receive directed feedback regarding its interactions throughout the solver iterations."
            ],
            "citingPaper": {
                "paperId": "91cef9a5bab296f96491287a1cb0ebdf0d9afc67",
                "externalIds": {
                    "CorpusId": 231739749
                },
                "corpusId": 231739749,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/91cef9a5bab296f96491287a1cb0ebdf0d9afc67",
                "title": "Differentiable Physics for Improving the Accuracy of Iterative PDE-Solvers with Neural Networks",
                "abstract": "Finding accurate solutions to partial differential equations (PDEs) is a crucial task for a wide range of fields in physics and engineering. We target the problem of reducing numerical errors of iterative PDE solvers and compare different learning approaches for finding complex correction functions. In particular, we highlight the performance of differentiable physics networks for a wide variety of PDEs, from non-linear advection-diffusion systems to three-dimensional Navier-Stokes flows.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "13094542",
                        "name": "P. Holl"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In [3] and its variants[11, 1, 12, 8], the Hamiltonian function can be approximated by neural networks, H\u03b8, called Hamiltonian Neural Networks (HNN)."
            ],
            "citingPaper": {
                "paperId": "ca0bf46f11fd29d5626e49a5d17b7c4240d31c92",
                "externalIds": {
                    "CorpusId": 231773239
                },
                "corpusId": 231773239,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ca0bf46f11fd29d5626e49a5d17b7c4240d31c92",
                "title": "Meta-Learned Hamiltonian",
                "abstract": "Many physical systems governed by the same physical laws can be expressed by a well-established Hamiltonian with adapting different physical parameters. However, in general, establishing an appropriate Hamiltonian of the unknown process is a central challenge for a wide area of science and engineering problems. We suggest that meta-learning algorithms can be one of the powerful data-driven tools for identifying the shared representation of Hamiltonian of an unknown process. In our demonstration, we show that a meta-learned model, which is considered implicitly to learn the shared representation of Hamiltonian, predicts the dynamics of new systems from observing a few point dynamics of the systems.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2108154935",
                        "name": "Seungjun Lee"
                    },
                    {
                        "authorId": "2517631",
                        "name": "W. Seong"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Machine learning in fluid systems The rapid advent of machine learning techniques is opening up new possibilities to solve the physical system\u2019s identification problems by statistically exploring the underlying structure of a variety of physical systems, encompassing applications in quantum physics (35), thermodynamics (17), material science (36), rigid body control (12), Lagrangian systems (9), and Hamiltonian systems (14; 19; 37).",
                ", also tried to combine an autoencoder with an Hamiltonian Neural Networks (HNN) to model the dynamics of pixel observations of a pendulum (14)."
            ],
            "citingPaper": {
                "paperId": "62d815df1e886808d7d5aa34f7a69d37cee80bbb",
                "externalIds": {
                    "CorpusId": 236795744
                },
                "corpusId": 236795744,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/62d815df1e886808d7d5aa34f7a69d37cee80bbb",
                "title": "VORTEXNET: LEARNING COMPLEX DYNAMIC SYS-",
                "abstract": "In this paper, we present a novel physics-rooted network structure that dramatically facilitates the learning of complex dynamic systems. Our method is inspired by the Vortex Method in fluid dynamics, whose key idea lies in that, given the observed flow field, instead of describing it with a function of space and time, one can equivalently understand the observation as being caused by a number of Lagrangian particles \u2014\u2013 vortices, flowing with the field. Since the number of such vortices are much smaller than that of the Eulerian, grid discretization, this Lagrangian discretization in essence encodes the system dynamics on a compact physics-based latent space. Our method enforces such Lagrangian discretization with a Encoder\u2014Dynamics\u2014Decode network structure, and trains it with a novel three-stage curriculum learning algorithm. With data generated from the high precision Eulerian DNS method, our alorithm takes advantage of the simplifying power of the Lagrangian method while persisting the physical integrity. This method fundamentally differs from the current approaches in the field of physicsinformed learning, and provides superior results for being more versatile, yielding more physical-correctness with less data sample, and faster to compute at high precision. Beyond providing a viable way of simulating complex fluid at highprecision, our method opens up a brand new horizon for embedding knowledge prior via constructing physically-valid latent spaces, which can be applied to further research areas beyond physical simulation.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2122423450",
                        "name": "Tems With"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recently, some works leveraged data specific knowledge to shape the prediction function, for example imposing specific fluid dynamic [26] or Hamiltonian constraints [11, 31]."
            ],
            "citingPaper": {
                "paperId": "6bd206e93f25492c345b590f4abe51db04bd0512",
                "externalIds": {
                    "CorpusId": 262898505
                },
                "corpusId": 262898505,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6bd206e93f25492c345b590f4abe51db04bd0512",
                "title": "Bridging Dynamical Models and Deep Networks to Solve Forward and Inverse Problems",
                "abstract": "Modeling the dynamics of physical systems recently gained attention in the machine learning community. Most recent works rely on complete observations of the physical state, whereas only partial observations are available in practice. Estimating the full state dynamics is important for the understanding of the underlying phenomenon and for model based prediction. Largely unexplored from an ML viewpoint, we address in this work the estimation and forecast of a partially observed spatio-temporal system, leveraging prior dynamical knowledge. To solve both forward (forecasting) and inverse (identi\ufb01cation) problems, we bridge numerical models of partial differential equations and deep learning and introduce a dynamical regularization on the unobserved states. This constrains our estimation and improves estimation performances. The approach is validated on two simulated datasets where the dynamics is controlled and fully known.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2181917156",
                        "name": "Marie D\u00e9chelle"
                    },
                    {
                        "authorId": "1853488882",
                        "name": "J\u00e9r\u00e9mie Don\u00e0"
                    },
                    {
                        "authorId": "2249230996",
                        "name": "K\u00e9vin Plessis-Fraissard"
                    },
                    {
                        "authorId": "2237639535",
                        "name": "Patrick Gallinari"
                    },
                    {
                        "authorId": "2181918106",
                        "name": "Marina L\u00e9vy"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Most closely related to ours, Greydanus et al. (2019) use a neural network to learn the Hamiltonian of the dynamical system in such a way that its partial derivatives match the time derivatives of the position and momentum variables, which are both assumed to be observed.",
                "Moreover, (Greydanus et al., 2019) mentions that HNN does not outperform a baseline method using O-NET in learning the three-body system\u2019s evolution.",
                "A recent paper (Greydanus et al., 2019), Hamiltonian Neural Networks (HNN), proposes to do so representing the Hamiltonian function H(q, p) as a multilayer neural network.",
                "For instance, with our terminology, the HNN model of Greydanus et al. (2019) is a \u201csingle-step E-E H-NET\u201d with the additional subtlety that they supervise the training with actual derivatives instead of relying on finite differences between successive steps of the observed trajectories.",
                "1 Introduction Can machines learn physical laws from data? A recent paper (Greydanus et al., 2019), Hamiltonian Neural Networks (HNN), proposes to do so representing the Hamiltonian function H(q, p) as a multilayer neural network.",
                "Here, we test our SRNN together with other baselines on the noiseless three-body system with the same configurations as (Greydanus et al., 2019).",
                "For instance, Greydanus et al. (2019) trains H-NET in a fully supervised manner using the observed tuples (p, q, p\u0307, q\u0307)."
            ],
            "citingPaper": {
                "paperId": "69f1b6e5398a1993d3dc99d8e4fa18def14bb6b4",
                "externalIds": {
                    "CorpusId": 209486273
                },
                "corpusId": 209486273,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/69f1b6e5398a1993d3dc99d8e4fa18def14bb6b4",
                "title": "Symplectic Recurrent Neural Network",
                "abstract": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. SRNNs model the Hamiltonian function of the system by a neural networks, and leverage symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show SRNNs succeed reliably on complex and noisy Hamiltonian systems. Finally, we show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards.",
                "year": 2019,
                "authors": []
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Among the possible choices are reproducing kernel Hilbert spaces [18; 19; 5], trigonometric functions, and functions parameterized by neural networks [4; 7].",
                "For related work on learning Hamiltonian systems, see [7; 2]."
            ],
            "citingPaper": {
                "paperId": "328b6ca079da89d976428cc8d833c6379e10e3c5",
                "externalIds": {
                    "CorpusId": 211018635
                },
                "corpusId": 211018635,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/328b6ca079da89d976428cc8d833c6379e10e3c5",
                "title": "Learning Dynamical Systems with Side Information (short version)",
                "abstract": "We present a mathematical formalism and a computational framework for the problem of learning a dynamical system from noisy observations of a few trajectories and subject to side information (e.g., physical laws or contextual knowledge). We identify six classes of side information which can be imposed by semidefinite programming and that arise naturally in many applications. We demonstrate their value on two examples from epidemiology and physics. Some density results on polynomial dynamical systems that either exactly or approximately satisfy side information are also presented.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1792465",
                        "name": "Amir Ali Ahmadi"
                    },
                    {
                        "authorId": "40899758",
                        "name": "Bachir El Khadir"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Many works in recent years have proposed a black-box parameterization for the dynamics of a mechanical system [4, 5, 7, 2, 3]."
            ],
            "citingPaper": {
                "paperId": null,
                "externalIds": null,
                "corpusId": "233740223",
                "publicationVenue": null,
                "url": null,
                "title": "MINIMIZING DISCRETE EULER-LAGRANGE RESIDUALS",
                "abstract": null,
                "year": 2021,
                "authors": []
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "In recent years, the use of physical loss functions has proven beneficial for the training procedure, yielding substantial improvements over purely supervised training approaches (Tompson et al., 2017; Wu & Tegmark, 2019; Greydanus et al., 2019).",
                "Additional works have shown the advantages of physical loss formulations (Greydanus et al., 2019; Cranmer et al., 2020)."
            ],
            "citingPaper": {
                "paperId": null,
                "externalIds": null,
                "corpusId": "247634131",
                "publicationVenue": null,
                "url": null,
                "title": "PHYSICAL DEEP LEARNING",
                "abstract": null,
                "year": 2022,
                "authors": []
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a085ea2a3a346e4e7e700f5ee4905c334ecc8f5b",
                "externalIds": {
                    "CorpusId": 252040973
                },
                "corpusId": 252040973,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a085ea2a3a346e4e7e700f5ee4905c334ecc8f5b",
                "title": "Latent Deep Space: Generative Adversarial Networks (GANs) in the Sciences",
                "abstract": "The recent spectacular success of machine learning in the sciences points to the emergence of a new artificial intelligence trading zone. The epistemological implications of this trading zone, however, have so far not been studied in depth. Critical research on machine learning systems, in media studies, visual studies, and \u201ccritical AI studies,\u201d in the past five years, has focused almost exclusively on the social use of machine learning, producing an almost insurmountable backlog of deeply flawed technical reality. Among this backlog, one machine learning technique warrants particular attention from the perspective of media studies and visual studies: the generative adversarial network (GAN), a type of deep convolutional neural network that operates primarily on image data. In this paper, I argue that GANs are not only technically but also epistemically opaque systems: where GANs seem to enhance our view of an object under investigation, they actually present us with a technically and historically predetermined space of visual possibilities. I discuss this hypothesis in relation to established theories of images in the sciences and recent applications of GANs to problems in astronomy and medicine. I conclude by proposing that contemporary artistic uses of GANs point to their true potential as engines of scientific speculation.",
                "year": null,
                "authors": [
                    {
                        "authorId": "3405588",
                        "name": "Fabian Offert"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": null,
                "externalIds": null,
                "corpusId": "259953342",
                "publicationVenue": null,
                "url": null,
                "title": "Symplectically Integrated Symbolic Regression of Hamiltonian Symplectically Integrated Symbolic Regression of Hamiltonian Dynamical Systems Dynamical Systems",
                "abstract": null,
                "year": null,
                "authors": []
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "bdd331d2e4f3254730b88689f8bb1a47f23ab269",
                "externalIds": {
                    "CorpusId": 257926813
                },
                "corpusId": 257926813,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bdd331d2e4f3254730b88689f8bb1a47f23ab269",
                "title": "D IFFERENTIABLE P HYSICS",
                "abstract": "Differentiable physics provides a new approach for modeling and understanding the physical systems by pairing the new technology of differentiable programming with classical numerical methods for physical simulation. We survey the rapidly growing literature of differentiable physics techniques and highlight methods for parameter estimation, learning representations, solving differential equations, and developing what we call scienti\ufb01c foundation models using data and inductive priors. We argue that differentiable physics offers a new paradigm for modeling physical phenomena by combining classical analytic solutions with numerical methodology using the bridge of differentiable programming.",
                "year": null,
                "authors": [
                    {
                        "authorId": "2378027",
                        "name": "Bharath Ramsundar"
                    },
                    {
                        "authorId": "32415194",
                        "name": "Dilip Krishnamurthy"
                    },
                    {
                        "authorId": "2631003",
                        "name": "V. Viswanathan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "6338e211bedbc02ddd329ec25b8e11063ddb9b58",
                "externalIds": {
                    "CorpusId": 260420896
                },
                "corpusId": 260420896,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6338e211bedbc02ddd329ec25b8e11063ddb9b58",
                "title": "Structured learning of rigid-body dynamics: A survey and uni\ufb01ed view",
                "abstract": "Accurate models of mechanical system dynamics are often critical for model-based control and reinforcement learning. Fully data-driven dynamics models promise to ease the process of modeling and analysis, but require considerable amounts of data for training and often do not generalize well to unseen parts of the state space. Combining data-driven modelling with prior analytical knowledge is an attractive alternative as the inclusion of structural knowledge into a regression model improves the model\u2019s data e\ufb03ciency and physical integrity. In this article, we survey supervised regression models that combine rigid-body mechanics with data-driven modelling techniques. We analyze the di\ufb00erent latent functions (such as kinetic energy or dissipative forces) and operators (such as di\ufb00erential operators and projection matrices) underlying common descriptions of rigid-body mechanics. Based on this analysis, we provide a uni\ufb01ed view on the combination of data-driven regression models, such as neural networks and Gaussian processes, with analytical model priors. Further, we review and discuss key techniques for designing structured models such as automatic di\ufb00erentiation.",
                "year": null,
                "authors": [
                    {
                        "authorId": "2064785715",
                        "name": "A. R. Geist"
                    },
                    {
                        "authorId": "2715093",
                        "name": "Sebastian Trimpe"
                    }
                ]
            }
        }
    ]
}