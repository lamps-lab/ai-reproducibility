{
    "offset": 0,
    "data": [
        {
            "contexts": [
                "Some methods of this type require multiple pruning and retraining cycles [20] and thus prolong the required training time."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a81472d08552953bb0de4be9c93bb8c51dcc32ff",
                "externalIds": {
                    "ArXiv": "2310.02448",
                    "CorpusId": 263620422
                },
                "corpusId": 263620422,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a81472d08552953bb0de4be9c93bb8c51dcc32ff",
                "title": "Feather: An Elegant Solution to Effective DNN Sparsification",
                "abstract": "Neural Network pruning is an increasingly popular way for producing compact and efficient models, suitable for resource-limited environments, while preserving high performance. While the pruning can be performed using a multi-cycle training and fine-tuning process, the recent trend is to encompass the sparsification process during the standard course of training. To this end, we introduce Feather, an efficient sparse training module utilizing the powerful Straight-Through Estimator as its core, coupled with a new thresholding operator and a gradient scaling technique, enabling robust, out-of-the-box sparsification performance. Feather's effectiveness and adaptability is demonstrated using various architectures on the CIFAR dataset, while on ImageNet it achieves state-of-the-art Top-1 validation accuracy using the ResNet-50 architecture, surpassing existing methods, including more complex and computationally heavy ones, by a considerable margin. Code is publicly available at https://github.com/athglentis/feather .",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2253668206",
                        "name": "Athanasios Glentis Georgoulakis"
                    },
                    {
                        "authorId": "1730770",
                        "name": "George Retsinas"
                    },
                    {
                        "authorId": "1750686",
                        "name": "P. Maragos"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We apply an incremental fine-tuning algorithm based on learning rate rewinding (Renda et al., 2020) to the baseline model to obtain the accuracy values corresonding to the following sparsity levels: 50%, 75%, 87.",
                "A number of algorithms have been proposed in the literature for accuracy recovery of pruned models (Deng et al., 2021; Renda et al., 2020; Hoefler et al., 2021)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "48b78b23c3b9be76aef2bec0baeaa68a1ef89e91",
                "externalIds": {
                    "ArXiv": "2310.00496",
                    "CorpusId": 263334160
                },
                "corpusId": 263334160,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/48b78b23c3b9be76aef2bec0baeaa68a1ef89e91",
                "title": "The Sparsity Roofline: Understanding the Hardware Limits of Sparse Neural Networks",
                "abstract": "We introduce the Sparsity Roofline, a visual performance model for evaluating sparsity in neural networks. The Sparsity Roofline jointly models network accuracy, sparsity, and predicted inference speedup. Our approach does not require implementing and benchmarking optimized kernels, and the predicted speedup is equal to what would be measured when the corresponding dense and sparse kernels are equally well-optimized. We achieve this through a novel analytical model for predicting sparse network performance, and validate the predicted speedup using several real-world computer vision architectures pruned across a range of sparsity patterns and degrees. We demonstrate the utility and ease-of-use of our model through two case studies: (1) we show how machine learning researchers can predict the performance of unimplemented or unoptimized block-structured sparsity patterns, and (2) we show how hardware designers can predict the performance implications of new sparsity patterns and sparse data formats in hardware. In both scenarios, the Sparsity Roofline helps performance experts identify sparsity regimes with the highest performance potential.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2249759423",
                        "name": "Cameron Shinn"
                    },
                    {
                        "authorId": "2249759700",
                        "name": "Collin McCarthy"
                    },
                    {
                        "authorId": "31225166",
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "authorId": "2249761562",
                        "name": "Muhammad Osama"
                    },
                    {
                        "authorId": "2249760131",
                        "name": "John D. Owens"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2022), learning rate rewinding (Renda et al., 2020), and knowledge distillation (Hinton et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6bfd1c8cc501a78fdb88c00a6e25da7a78de925a",
                "externalIds": {
                    "ArXiv": "2310.02277",
                    "CorpusId": 263620664
                },
                "corpusId": 263620664,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6bfd1c8cc501a78fdb88c00a6e25da7a78de925a",
                "title": "Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity",
                "abstract": "The traditional notion of\"Junk DNA\"has long been linked to non-coding segments within the human genome, constituting roughly 98% of its composition. However, recent research has unveiled the critical roles some of these seemingly non-functional DNA sequences play in cellular processes. Intriguingly, the weights within deep neural networks exhibit a remarkable similarity to the redundancy observed in human genes. It was believed that weights in gigantic models contained excessive redundancy, and could be removed without compromising performance. This paper challenges this conventional wisdom by presenting a compelling counter-argument. We employ sparsity as a tool to isolate and quantify the nuanced significance of low-magnitude weights in pre-trained large language models (LLMs). Our study demonstrates a strong correlation between these weight magnitudes and the knowledge they encapsulate, from a downstream task-centric angle. we raise the\"Junk DNA Hypothesis\"backed by our in-depth investigation: while small-magnitude weights may appear\"useless\"for simple tasks and suitable for pruning, they actually encode crucial knowledge necessary for solving more difficult downstream tasks. Removing these seemingly insignificant weights can lead to irreversible knowledge forgetting and performance damage in difficult tasks. These findings offer fresh insights into how LLMs encode knowledge in a task-sensitive manner, pave future research direction in model pruning, and open avenues for task-aware conditional computation during inference.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2254142682",
                        "name": "Lu Yin"
                    },
                    {
                        "authorId": "2255081092",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2253397454",
                        "name": "Ajay Jaiswal"
                    },
                    {
                        "authorId": "2253458053",
                        "name": "Souvik Kundu"
                    },
                    {
                        "authorId": "2254949434",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Both the IMP and LTH language-specific pruning meth-ods achieve matching performance to the original dense multilingual model and surpass the dense monolingual models.",
                "The iterative magnitude pruning (IMP) method [6] involves fine-tuning a dense model for a specified number of steps denoted as T while making pruning decisions based on the magnitude of weights.",
                "The multilingual pathway model undergoes training for 200K and 100K steps for IMP and LTH methods, respectively.",
                "The ASR Pathways [12] provides a method to fine-tune a multi-lingual ASR model using the language-specific sub-networks (or pathways) identified through IMP, LTH, or other pruning methods.",
                "To initiate the IMP procedure, we initialize model parameters \u03b8 with pre-trained dense weights \u03b8 0 and set the binary pruning mask m to all ones 1 , where m \u2208 { 0 , 1 } | \u03b8 | .",
                "For the pruning step, we simply raise the sparsity level and prune from all weights in \u03b8 n as opposed to pruning from weights in m \u2299 \u03b8 T in the IMP procedure.",
                "When monolingual data is used in IMP, this procedure yields a language-specific pruning mask m l for a language l .",
                "Within the framework of the IMP procedure, we introduce a mask adaptation step denoted as n (where n   T ).",
                "Our proposed Stage (2) modified the IMP and the LTH language-specific pruning methods in Stage (2) and achieved a consistent 5.3% relative WER reduction averaged across languages.",
                "However, the pruning process, such as Iterative Magnitude Pruning (IMP) [6, 11] and Lottery Ticket Hypothesis (LTH) [8], involves multiple iterations of pruning and re-training to achieve the best performance.",
                "The IMP procedure is illustrated as follows: Repeat 2.",
                "The lottery ticket hypothesis (LTH) method [8] modifies the Step 3 of the IMP procedure by assigning the pre-trained dense weights \u03b8 0 to \u03b8 instead of \u03b8 T , referred to as a re-winding step."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "3ec0392c250e183f58d148a472b82b34a92da350",
                "externalIds": {
                    "ArXiv": "2309.13018",
                    "CorpusId": 262217386
                },
                "corpusId": 262217386,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3ec0392c250e183f58d148a472b82b34a92da350",
                "title": "Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model",
                "abstract": "Neural network pruning offers an effective method for compressing a multilingual automatic speech recognition (ASR) model with minimal performance loss. However, it entails several rounds of pruning and re-training needed to be run for each language. In this work, we propose the use of an adaptive masking approach in two scenarios for pruning a multilingual ASR model efficiently, each resulting in sparse monolingual models or a sparse multilingual model (named as Dynamic ASR Pathways). Our approach dynamically adapts the sub-network, avoiding premature decisions about a fixed sub-network structure. We show that our approach outperforms existing pruning methods when targeting sparse monolingual models. Further, we illustrate that Dynamic ASR Pathways jointly discovers and trains better sub-networks (pathways) of a single multilingual model by adapting from different sub-network initializations, thereby reducing the need for language-specific pruning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2245070813",
                        "name": "Jiamin Xie"
                    },
                    {
                        "authorId": "49243436",
                        "name": "Ke Li"
                    },
                    {
                        "authorId": "2245316402",
                        "name": "Jinxi Guo"
                    },
                    {
                        "authorId": "2894428",
                        "name": "Andros Tjandra"
                    },
                    {
                        "authorId": "2229513388",
                        "name": "Shangguan Yuan"
                    },
                    {
                        "authorId": "2769735",
                        "name": "Leda Sari"
                    },
                    {
                        "authorId": "2238043209",
                        "name": "Chunyang Wu"
                    },
                    {
                        "authorId": "49222521",
                        "name": "J. Jia"
                    },
                    {
                        "authorId": "3222225",
                        "name": "Jay Mahadeokar"
                    },
                    {
                        "authorId": "1729960",
                        "name": "Ozlem Kalinli"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, unstructured pruning [14] is further used to remove unimportant weights and reduce computational costs."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7ff623a93a13e7ab306368e184c4afb507963d3d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-10510",
                    "ArXiv": "2309.10510",
                    "DOI": "10.48550/arXiv.2309.10510",
                    "CorpusId": 262054951
                },
                "corpusId": 262054951,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7ff623a93a13e7ab306368e184c4afb507963d3d",
                "title": "Logic Design of Neural Networks for High-Throughput and Low-Power Applications",
                "abstract": "Neural networks (NNs) have been successfully deployed in various fields. In NNs, a large number of multiplyaccumulate (MAC) operations need to be performed. Most existing digital hardware platforms rely on parallel MAC units to accelerate these MAC operations. However, under a given area constraint, the number of MAC units in such platforms is limited, so MAC units have to be reused to perform MAC operations in a neural network. Accordingly, the throughput in generating classification results is not high, which prevents the application of traditional hardware platforms in extreme-throughput scenarios. Besides, the power consumption of such platforms is also high, mainly due to data movement. To overcome this challenge, in this paper, we propose to flatten and implement all the operations at neurons, e.g., MAC and ReLU, in a neural network with their corresponding logic circuits. To improve the throughput and reduce the power consumption of such logic designs, the weight values are embedded into the MAC units to simplify the logic, which can reduce the delay of the MAC units and the power consumption incurred by weight movement. The retiming technique is further used to improve the throughput of the logic circuits for neural networks. In addition, we propose a hardware-aware training method to reduce the area of logic designs of neural networks. Experimental results demonstrate that the proposed logic designs can achieve high throughput and low power consumption for several high-throughput applications.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2243950298",
                        "name": "Kangwei Xu"
                    },
                    {
                        "authorId": "31602842",
                        "name": "Grace Li Zhang"
                    },
                    {
                        "authorId": "1732787",
                        "name": "Ulf Schlichtmann"
                    },
                    {
                        "authorId": "48218808",
                        "name": "Bing Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2949b03a5e1bc3cddaa0784e2a2ac4d0d0c996f7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-01786",
                    "ArXiv": "2309.01786",
                    "DOI": "10.48550/arXiv.2309.01786",
                    "CorpusId": 261530377
                },
                "corpusId": 261530377,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2949b03a5e1bc3cddaa0784e2a2ac4d0d0c996f7",
                "title": "Safe and Robust Watermark Injection with a Single OoD Image",
                "abstract": "Training a high-performance deep neural network requires large amounts of data and computational resources. Protecting the intellectual property (IP) and commercial ownership of a deep model is challenging yet increasingly crucial. A major stream of watermarking strategies implants verifiable backdoor triggers by poisoning training samples, but these are often unrealistic due to data privacy and safety concerns and are vulnerable to minor model changes such as fine-tuning. To overcome these challenges, we propose a safe and robust backdoor-based watermark injection technique that leverages the diverse knowledge from a single out-of-distribution (OoD) image, which serves as a secret key for IP verification. The independence of training data makes it agnostic to third-party promises of IP security. We induce robustness via random perturbation of model parameters during watermark injection to defend against common watermark removal attacks, including fine-tuning, pruning, and model extraction. Our experimental results demonstrate that the proposed watermarking approach is not only time- and sample-efficient without training data, but also robust against the watermark removal attacks above.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2237953183",
                        "name": "Shuyang Yu"
                    },
                    {
                        "authorId": "2110805917",
                        "name": "Junyuan Hong"
                    },
                    {
                        "authorId": "2237954046",
                        "name": "Haobo Zhang"
                    },
                    {
                        "authorId": "113727681",
                        "name": "Haotao Wang"
                    },
                    {
                        "authorId": "2237946662",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "2143462929",
                        "name": "Jiayu Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In 2019, lottery ticket hypothesis (LTH) [15] was proposed by combining existing pruning and training modes.",
                "It leverages upon both channel-level and LTH pruning.",
                "*corresponding author was originally a one-shot pattern but has since been improved to an iterative pattern, which is further guided by the learning rate rewinding strategy [18] in the case of CPLR.",
                "Researchers have compared various strategies based on LTH [16, 17, 18], and one effective scheme is to use learning rate rewinding [18]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "9ffffa0547892da041c60c6bac37073b271ef3e4",
                "externalIds": {
                    "DOI": "10.21437/interspeech.2023-1717",
                    "CorpusId": 260921429
                },
                "corpusId": 260921429,
                "publicationVenue": {
                    "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
                    "name": "Interspeech",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Int Speech Commun Assoc",
                        "INTERSPEECH",
                        "Conference of the International Speech Communication Association"
                    ],
                    "issn": "2308-457X",
                    "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
                    "alternate_urls": [
                        "http://www.isca-speech.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9ffffa0547892da041c60c6bac37073b271ef3e4",
                "title": "A Multiple-Teacher Pruning Based Self-Distillation (MT-PSD) Approach to Model Compression for Audio-Visual Wake Word Spotting",
                "abstract": "We propose a novel model compression approach using multiple-teacher pruning based self-distillation for audio-visual wake word spotting, facilitating compact neural network implementations without sacrificing system performances. In each stage of the proposed framework, we prune a teacher model obtained in the previous stage to generate a student model, then fine-tune it with teacher-student learning and use it as a new teacher model for following stages. A normalized intra-class loss is designed to optimize this pruning based self-distillation (PSD) process. Both single-teacher PSD (ST-PSD) and multi-teacher PSD (MT-PSD) are adopted in the fine-tuning process each stage. When tested on audio-visual wake word spotting in MISP2021 Challenge, the two proposed techniques outperform state-of-the-art methods in both system performances and model efficiencies. Moreover, MT-PSD that leverages upon the complementarity of multiple teachers obtained in different stages also outperforms ST-PSD.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143388884",
                        "name": "Haotian Wang"
                    },
                    {
                        "authorId": "145419855",
                        "name": "Jun Du"
                    },
                    {
                        "authorId": "48054592",
                        "name": "Hengshun Zhou"
                    },
                    {
                        "authorId": "9391905",
                        "name": "Chin-Hui Lee"
                    },
                    {
                        "authorId": "1806461",
                        "name": "Yuling Ren"
                    },
                    {
                        "authorId": "2130418656",
                        "name": "Jiangjiang Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2022 Learning Rate Rewinding: Learning rate rewinding, proposed in [40], trains the remained weights from the final values using the learning rate schedule for the specified number of epochs.",
                "The results in [40, 53] show that weight rewinding can achieve higher accuracy than fine-tuning.",
                "\u2022 Compression Ratio: Compression ratio in [39, 40] is defined as the ratio of the original weight numbers to the preserved weight numbers, but in [41] it is defined as the ratio of the preserved weight numbers to the original weight numbers.",
                "recover the performance [40], where w \u2032\u2032 T and m \u2032\u2032 are the final results of weights and masks after the overall pruning process, respectively.",
                "For example, if 10% of weights are preserved, then the compression ratio in [40] is 10, but it is 10% in [41]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "81b268e98042864c025b401eb6a54dcb566486d5",
                "externalIds": {
                    "ArXiv": "2308.06767",
                    "DBLP": "journals/corr/abs-2308-06767",
                    "DOI": "10.48550/arXiv.2308.06767",
                    "CorpusId": 260887757
                },
                "corpusId": 260887757,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/81b268e98042864c025b401eb6a54dcb566486d5",
                "title": "A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations",
                "abstract": "Modern deep neural networks, particularly recent large language models, come with massive model sizes that require significant computational and storage resources. To enable the deployment of modern models on resource-constrained environments and accelerate inference time, researchers have increasingly explored pruning techniques as a popular research direction in neural network compression. However, there is a dearth of up-to-date comprehensive review papers on pruning. To address this issue, in this survey, we provide a comprehensive review of existing research works on deep neural network pruning in a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to prune, and 4) fusion of pruning and other compression techniques. We then provide a thorough comparative analysis of seven pairs of contrast settings for pruning (e.g., unstructured/structured) and explore emerging topics, including post-training pruning, different levels of supervision for pruning, and broader applications (e.g., adversarial robustness) to shed light on the commonalities and differences of existing methods and lay the foundation for further method development. To facilitate future research, we build a curated collection of datasets, networks, and evaluations on different applications. Finally, we provide some valuable recommendations on selecting pruning methods and prospect promising research directions. We build a repository at https://github.com/hrcheng1066/awesome-pruning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2230189096",
                        "name": "Hongrong Cheng"
                    },
                    {
                        "authorId": "2211872272",
                        "name": "Miao Zhang"
                    },
                    {
                        "authorId": "3177281",
                        "name": "Javen Qinfeng Shi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, the work [24] extends the training of the subnetwork from initialization to the early stage of pretraining, which improves the accuracy of the subnetwork in more challenging tasks.",
                "[24] Alex Renda, Jonathan Frankle, and Michael Carbin, \u2018Comparing rewinding and fine-tuning in neural network pruning\u2019, in International Conference on Learning Representations, (2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "27b93c9534fa3d71cba4eda7d82a51f57de39b17",
                "externalIds": {
                    "ArXiv": "2308.02916",
                    "DBLP": "journals/corr/abs-2308-02916",
                    "DOI": "10.48550/arXiv.2308.02916",
                    "CorpusId": 260680389
                },
                "corpusId": 260680389,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/27b93c9534fa3d71cba4eda7d82a51f57de39b17",
                "title": "Adversarial Erasing with Pruned Elements: Towards Better Graph Lottery Ticket",
                "abstract": "Graph Lottery Ticket (GLT), a combination of core subgraph and sparse subnetwork, has been proposed to mitigate the computational cost of deep Graph Neural Networks (GNNs) on large input graphs while preserving original performance. However, the winning GLTs in exisiting studies are obtained by applying iterative magnitude-based pruning (IMP) without re-evaluating and re-considering the pruned information, which disregards the dynamic changes in the significance of edges/weights during graph/model structure pruning, and thus limits the appeal of the winning tickets. In this paper, we formulate a conjecture, i.e., existing overlooked valuable information in the pruned graph connections and model parameters which can be re-grouped into GLT to enhance the final performance. Specifically, we propose an adversarial complementary erasing (ACE) framework to explore the valuable information from the pruned components, thereby developing a more powerful GLT, referred to as the ACE-GLT. The main idea is to mine valuable information from pruned edges/weights after each round of IMP, and employ the ACE technique to refine the GLT processing. Finally, experimental results demonstrate that our ACE-GLT outperforms existing methods for searching GLT in diverse tasks. Our code will be made publicly available.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2199542719",
                        "name": "Yuwen Wang"
                    },
                    {
                        "authorId": "2128786021",
                        "name": "Shunyu Liu"
                    },
                    {
                        "authorId": "38644044",
                        "name": "Kai Chen"
                    },
                    {
                        "authorId": "2173757802",
                        "name": "Tongtian Zhu"
                    },
                    {
                        "authorId": "2159566162",
                        "name": "Jilin Qiao"
                    },
                    {
                        "authorId": "2153859223",
                        "name": "Mengjie Shi"
                    },
                    {
                        "authorId": "50826651",
                        "name": "Yuanyu Wan"
                    },
                    {
                        "authorId": "2152127912",
                        "name": "Mingli Song"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "531ae1f7fcb29911a6d519c454e7035a79a3abf9",
                "externalIds": {
                    "ArXiv": "2307.04552",
                    "DBLP": "journals/corr/abs-2307-04552",
                    "DOI": "10.48550/arXiv.2307.04552",
                    "CorpusId": 259501735
                },
                "corpusId": 259501735,
                "publicationVenue": {
                    "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
                    "name": "Interspeech",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Int Speech Commun Assoc",
                        "INTERSPEECH",
                        "Conference of the International Speech Communication Association"
                    ],
                    "issn": "2308-457X",
                    "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
                    "alternate_urls": [
                        "http://www.isca-speech.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/531ae1f7fcb29911a6d519c454e7035a79a3abf9",
                "title": "SparseVSR: Lightweight and Noise Robust Visual Speech Recognition",
                "abstract": "Recent advances in deep neural networks have achieved unprecedented success in visual speech recognition. However, there remains substantial disparity between current methods and their deployment in resource-constrained devices. In this work, we explore different magnitude-based pruning techniques to generate a lightweight model that achieves higher performance than its dense model equivalent, especially under the presence of visual noise. Our sparse models achieve state-of-the-art results at 10% sparsity on the LRS3 dataset and outperform the dense equivalent up to 70% sparsity. We evaluate our 50% sparse model on 7 different visual noise types and achieve an overall absolute improvement of more than 2% WER compared to the dense equivalent. Our results confirm that sparse networks are more resistant to noise than dense networks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1410464604",
                        "name": "Adriana Fernandez-Lopez"
                    },
                    {
                        "authorId": "1390823178",
                        "name": "Honglie Chen"
                    },
                    {
                        "authorId": "12558026",
                        "name": "Pingchuan Ma"
                    },
                    {
                        "authorId": "1490942098",
                        "name": "A. Haliassos"
                    },
                    {
                        "authorId": "2403354",
                        "name": "Stavros Petridis"
                    },
                    {
                        "authorId": "145387780",
                        "name": "M. Pantic"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026magnitude pruning (IMP) framework [Han et al., 2015], a simple and successful approach for pruning deep neural networks [Blalock et al., 2020, Renda et al., 2020] which is pivotal to finding \u201clottery tickets\u201d [Frankle and Carbin, 2019, Frankle et al., 2019, 2020], i.e. sparse subnetworks\u2026",
                "\u2026(lottery ticket rewinding was used when required, see appendix B) was used for our experiments as it provides an effective procedure to find subnetworks with nontrivial sparsities that have low test error [Frankle and Carbin, 2019, Frankle et al., 2020, Blalock et al., 2020, Renda et al., 2020]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "27ceb7eb6090d4087a35d36325adf3b4cdb4ec5d",
                "externalIds": {
                    "DBLP": "conf/uai/AroraIGS23",
                    "ArXiv": "2306.12190",
                    "DOI": "10.48550/arXiv.2306.12190",
                    "CorpusId": 259211888
                },
                "corpusId": 259211888,
                "publicationVenue": {
                    "id": "f9af8000-42f8-410d-a622-e8811e41660a",
                    "name": "Conference on Uncertainty in Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Uncertainty in Artificial Intelligence",
                        "UAI",
                        "Conf Uncertain Artif Intell",
                        "Uncertain Artif Intell"
                    ],
                    "url": "http://www.auai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/27ceb7eb6090d4087a35d36325adf3b4cdb4ec5d",
                "title": "Quantifying lottery tickets under label noise: accuracy, calibration, and complexity",
                "abstract": "Pruning deep neural networks is a widely used strategy to alleviate the computational burden in machine learning. Overwhelming empirical evidence suggests that pruned models retain very high accuracy even with a tiny fraction of parameters. However, relatively little work has gone into characterising the small pruned networks obtained, beyond a measure of their accuracy. In this paper, we use the sparse double descent approach to identify univocally and characterise pruned models associated with classification tasks. We observe empirically that, for a given task, iterative magnitude pruning (IMP) tends to converge to networks of comparable sizes even when starting from full networks with sizes ranging over orders of magnitude. We analyse the best pruned models in a controlled experimental setup and show that their number of parameters reflects task difficulty and that they are much better than full networks at capturing the true conditional probability distribution of the labels. On real data, we similarly observe that pruned models are less prone to overconfident predictions. Our results suggest that pruned models obtained via IMP not only have advantageous computational properties but also provide a better representation of uncertainty in learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3439844",
                        "name": "V. Arora"
                    },
                    {
                        "authorId": "2220345264",
                        "name": "Daniele Irto"
                    },
                    {
                        "authorId": "48671001",
                        "name": "Sebastian Goldt"
                    },
                    {
                        "authorId": "1693720",
                        "name": "G. Sanguinetti"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A quick review of existing pruning methods reveals a possible reason: they typically require retraining [3, 25], training from scratch [8, 32, 44, 54] or even an extensive iterative process [23, 37, 77].",
                "Existing pruning methods usually require either modifications to the training procedure [23, 33, 44, 56], retraining the pruned networks to regain accuracy [25, 26, 41], or an even more computationally intensive iterative retraining process [18, 32, 54, 77]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "24c4cbd0fa2d214e8ff914d92c9b37e45c4a6bbf",
                "externalIds": {
                    "ArXiv": "2306.11695",
                    "DBLP": "journals/corr/abs-2306-11695",
                    "DOI": "10.48550/arXiv.2306.11695",
                    "CorpusId": 259203115
                },
                "corpusId": 259203115,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/24c4cbd0fa2d214e8ff914d92c9b37e45c4a6bbf",
                "title": "A Simple and Effective Pruning Approach for Large Language Models",
                "abstract": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prune weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method on LLaMA across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and competes favorably against recent methods involving intensive weight update. Code is available at https://github.com/locuslab/wanda.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2984183",
                        "name": "Mingjie Sun"
                    },
                    {
                        "authorId": "2109168016",
                        "name": "Zhuang Liu"
                    },
                    {
                        "authorId": "25901845",
                        "name": "Anna Bair"
                    },
                    {
                        "authorId": "145116464",
                        "name": "J. Z. Kolter"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3034d8cafbbe6a3486053b35bfa4cdd8c597ae66",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-09707",
                    "ArXiv": "2306.09707",
                    "DOI": "10.48550/arXiv.2306.09707",
                    "CorpusId": 259187883
                },
                "corpusId": 259187883,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3034d8cafbbe6a3486053b35bfa4cdd8c597ae66",
                "title": "Representation and decomposition of functions in DAG-DNNs and structural network pruning",
                "abstract": "The conclusions provided by deep neural networks (DNNs) must be carefully scrutinized to determine whether they are universal or architecture dependent. The term DAG-DNN refers to a graphical representation of a DNN in which the architecture is expressed as a direct-acyclic graph (DAG), on which arcs are associated with functions. The level of a node denotes the maximum number of hops between the input node and the node of interest. In the current study, we demonstrate that DAG-DNNs can be used to derive all functions defined on various sub-architectures of the DNN. We also demonstrate that the functions defined in a DAG-DNN can be derived via a sequence of lower-triangular matrices, each of which provides the transition of functions defined in sub-graphs up to nodes at a specified level. The lifting structure associated with lower-triangular matrices makes it possible to perform the structural pruning of a network in a systematic manner. The fact that decomposition is universally applicable to all DNNs means that network pruning could theoretically be applied to any DNN, regardless of the underlying architecture. We demonstrate that it is possible to obtain the winning ticket (sub-network and initialization) for a weak version of the lottery ticket hypothesis, based on the fact that the sub-network with initialization can achieve training performance on par with that of the original network using the same number of iterations or fewer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "34600044",
                        "name": "Wonjun Hwang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6c4d61e7a1ecea4a3af8321ceef0933708a6ff0c",
                "externalIds": {
                    "ArXiv": "2306.08960",
                    "DBLP": "journals/corr/abs-2306-08960",
                    "DOI": "10.48550/arXiv.2306.08960",
                    "CorpusId": 259165552
                },
                "corpusId": 259165552,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6c4d61e7a1ecea4a3af8321ceef0933708a6ff0c",
                "title": "Neural Network Compression using Binarization and Few Full-Precision Weights",
                "abstract": "Quantization and pruning are two effective Deep Neural Networks model compression methods. In this paper, we propose Automatic Prune Binarization (APB), a novel compression technique combining quantization with pruning. APB enhances the representational capability of binary networks using a few full-precision weights. Our technique jointly maximizes the accuracy of the network while minimizing its memory impact by deciding whether each weight should be binarized or kept in full precision. We show how to efficiently perform a forward pass through layers compressed using APB by decomposing it into a binary and a sparse-dense matrix multiplication. Moreover, we design two novel efficient algorithms for extremely quantized matrix multiplication on CPU, leveraging highly efficient bitwise operations. The proposed algorithms are 6.9x and 1.5x faster than available state-of-the-art solutions. We extensively evaluate APB on two widely adopted model compression datasets, namely CIFAR10 and ImageNet. APB delivers better accuracy/memory trade-off compared to state-of-the-art methods based on i) quantization, ii) pruning, and iii) combination of pruning and quantization. APB outperforms quantization in the accuracy/efficiency trade-off, being up to 2x faster than the 2-bit quantized model with no loss in accuracy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2763561",
                        "name": "F. M. Nardini"
                    },
                    {
                        "authorId": "2140042293",
                        "name": "Cosimo Rulli"
                    },
                    {
                        "authorId": "2471674",
                        "name": "Salvatore Trani"
                    },
                    {
                        "authorId": "1797933",
                        "name": "Rossano Venturini"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "765668202c757c908da5ab5072c7540987ebcea5",
                "externalIds": {
                    "DBLP": "conf/medcomnet/KondratevaDLS23",
                    "DOI": "10.1109/MedComNet58619.2023.10168858",
                    "CorpusId": 259365271
                },
                "corpusId": 259365271,
                "publicationVenue": {
                    "id": "b093d4d3-f396-43ee-ab10-059b1b2cc604",
                    "name": "Mediterranean Communication and Computer Networking Conference",
                    "type": "conference",
                    "alternate_names": [
                        "MedComNet",
                        "Mediterr Commun Comput Netw Conf"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/765668202c757c908da5ab5072c7540987ebcea5",
                "title": "Parameter Prioritization for Efficient Transmission of Neural Networks in Small Satellite Applications",
                "abstract": "Low-earth-orbit (LEO) satellites can be used to launch cost-effective Earth observation missions. Onboard processing, in particular using machine learning (ML) approaches, is often discussed as a way to reduce the amount of data transmitted back to Earth. However, the combination of LEO satellites and ML brings unique communication challenges, as requirements - and therefore ML models - often change throughout the lifetime of a satellite mission. In this paper, we propose a novel communication protocol that deals with model updates efficiently by providing incremental updates with low communication overhead and quick improvements in onboard classification accuracy. Our initial evaluation shows that the proposed, priority-based approach significantly outperforms the baseline of transmitting updated models without considering prioritization.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2036116672",
                        "name": "Olga Kondrateva"
                    },
                    {
                        "authorId": "1767520",
                        "name": "S. Dietzel"
                    },
                    {
                        "authorId": "2186187800",
                        "name": "Ansgar L\u00f6\u00dfer"
                    },
                    {
                        "authorId": "1776439",
                        "name": "B. Scheuermann"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent studies [30, 21, 39] have highlighted the importance of the learning rate schedule in the retraining phase and have proposed specific guidelines for selecting an appropriate schedule for iterative magnitude pruning (IMP) [13] and provide valuable insights into the impact of learning rate schedules."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "286667b49b7008b47ad1aa738351e0882ea303d2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-05056",
                    "ArXiv": "2306.05056",
                    "DOI": "10.48550/arXiv.2306.05056",
                    "CorpusId": 259108455
                },
                "corpusId": 259108455,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/286667b49b7008b47ad1aa738351e0882ea303d2",
                "title": "Magnitude Attention-based Dynamic Pruning",
                "abstract": "Existing pruning methods utilize the importance of each weight based on specified criteria only when searching for a sparse structure but do not utilize it during training. In this work, we propose a novel approach - \\textbf{M}agnitude \\textbf{A}ttention-based Dynamic \\textbf{P}runing (MAP) method, which applies the importance of weights throughout both the forward and backward paths to explore sparse model structures dynamically. Magnitude attention is defined based on the magnitude of weights as continuous real-valued numbers enabling a seamless transition from a redundant to an effective sparse network by promoting efficient exploration. Additionally, the attention mechanism ensures more effective updates for important layers within the sparse network. In later stages of training, our approach shifts from exploration to exploitation, exclusively updating the sparse model composed of crucial weights based on the explored structure, resulting in pruned models that not only achieve performance comparable to dense models but also outperform previous pruning methods on CIFAR-10/100 and ImageNet.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2114716973",
                        "name": "Jihye Back"
                    },
                    {
                        "authorId": "40894534",
                        "name": "Namhyuk Ahn"
                    },
                    {
                        "authorId": "2116315847",
                        "name": "Jang-Hyun Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Additionally, weight values located around 0 have minimal impact on the final accuracy outcome, which is consistent with the principles of weight magnitude pruning [22]."
            ],
            "intents": [
                "result"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9bcc50af6ffe9d5f76e4abb9725346de1afd50f3",
                "externalIds": {
                    "DBLP": "conf/hpsr/SpantidiA23",
                    "DOI": "10.1109/HPSR57248.2023.10147918",
                    "CorpusId": 259158419
                },
                "corpusId": 259158419,
                "publicationVenue": {
                    "id": "9440cc3d-d905-402e-a274-8e60da64789f",
                    "name": "International Conference on High Performance Switching and Routing",
                    "type": "conference",
                    "alternate_names": [
                        "High Performance Switching and Routing",
                        "Int Conf High Perform Switch Routing",
                        "HPSR",
                        "High Perform Switch Routing"
                    ],
                    "url": "http://www.ieee-hpsr.org/"
                },
                "url": "https://www.semanticscholar.org/paper/9bcc50af6ffe9d5f76e4abb9725346de1afd50f3",
                "title": "The Perfect Match: Selecting Approximate Multipliers for Energy-Efficient Neural Network Inference",
                "abstract": "Reconfigurable approximate multipliers have been proposed as a way to improve the energy efficiency of neural network inference. However, selecting the optimal combination of approximate modes is a challenging problem due to the tradeoff between energy savings and accuracy loss. In this paper, we propose a methodology for selecting the best triad of approximate multipliers to form a reconfigurable approximate multiplier that can satisfy a maximum accuracy drop threshold and achieve the highest possible energy savings. We use formal methods to produce a Pareto-front of solutions that satisfy the accuracy constraint and maximize energy savings. Experimental results show that our methodology can achieve significant gains in energy with negligible drops in accuracy when compared to the baseline.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51021108",
                        "name": "Ourania Spantidi"
                    },
                    {
                        "authorId": "145614289",
                        "name": "Iraklis Anagnostopoulos"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Usually, extra fine-tuning is conducted for the pruned network to help maintain the performance [20, 28, 37]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "59f87a2464ab1d3c0376ca30d09c9204c89653dd",
                "externalIds": {
                    "DBLP": "conf/cvpr/ChenSDCZLG23",
                    "DOI": "10.1109/CVPR52729.2023.01138",
                    "CorpusId": 254685665
                },
                "corpusId": 254685665,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/59f87a2464ab1d3c0376ca30d09c9204c89653dd",
                "title": "Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners",
                "abstract": "Optimization in multi-task learning (MTL) is more challenging than single-task learning (STL), as the gradient from different tasks can be contradictory. When tasks are related, it can be beneficial to share some parameters among them (cooperation). However, some tasks require additional parameters with expertise in a specific type of data or discrimination (specialization). To address the MTL challenge, we propose Mod-Squad, a new model that is Modularized into groups of experts (a \u2018Squad\u2019). This structure allows us to formalize cooperation and specialization as the process of matching experts and tasks. We optimize this matching process during the training of a single model. Specifically, we incorporate mixture of experts (MoE) layers into a transformer model, with a new loss that incorporates the mutual dependence between tasks and experts. As a result, only a small set of experts are activated for each task. This prevents the sharing of the entire backbone model between all tasks, which strengthens the model, especially when the training set size and the number of tasks scale up. More interestingly, for each task, we can extract the small set of experts as a standalone model that maintains the same performance as the large model. Extensive experiments on the Taskonomy dataset with 13 vision tasks and the PASCAL-Context dataset with 5 vision tasks show the superiority of our approach. The project page can be accessed at https://vis-www.cs.umass.edu/Mod-Squad.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "13556061",
                        "name": "Z. Chen"
                    },
                    {
                        "authorId": "2714199",
                        "name": "Yikang Shen"
                    },
                    {
                        "authorId": "2055624181",
                        "name": "Mingyu Ding"
                    },
                    {
                        "authorId": "2111329651",
                        "name": "Zhenfang Chen"
                    },
                    {
                        "authorId": "3459894",
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "authorId": "1389846455",
                        "name": "E. Learned-Miller"
                    },
                    {
                        "authorId": "2056157586",
                        "name": "Chuang Gan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Fine-tuning [41] is adopted to adjust the potential performance of the pruned model to the optimal level."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "44c39a87bc2ac2ded75dd36005971931ca019206",
                "externalIds": {
                    "PubMedCentral": "10305692",
                    "DBLP": "journals/sensors/LiuL23",
                    "DOI": "10.3390/s23125539",
                    "CorpusId": 259283381,
                    "PubMed": "37420706"
                },
                "corpusId": 259283381,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/44c39a87bc2ac2ded75dd36005971931ca019206",
                "title": "YOLO-GW: Quickly and Accurately Detecting Pedestrians in a Foggy Traffic Environment",
                "abstract": "In practice, the object detection algorithm is limited by a complex detection environment, hardware costs, computing power, and chip running memory. The performance of the detector will be greatly reduced during operation. Determining how to realize real-time, fast, and high-precision pedestrian recognition in a foggy traffic environment is a very challenging problem. To solve this problem, the dark channel de-fogging algorithm is added to the basis of the YOLOv7 algorithm, which effectively improves the de-fogging efficiency of the dark channel through the methods of down-sampling and up-sampling. In order to further improve the accuracy of the YOLOv7 object detection algorithm, the ECA module and a detection head are added to the network to improve object classification and regression. Moreover, an 864 \u00d7 864 network input size is used for model training to improve the accuracy of the object detection algorithm for pedestrian recognition. Then the combined pruning strategy was used to improve the optimized YOLOv7 detection model, and finally, the optimization algorithm YOLO-GW was obtained. Compared with YOLOv7 object detection, YOLO-GW increased Frames Per Second (FPS) by 63.08%, mean Average Precision (mAP) increased by 9.06%, parameters decreased by 97.66%, and volume decreased by 96.36%. Smaller training parameters and model space make it possible for the YOLO-GW target detection algorithm to be deployed on the chip. Through analysis and comparison of experimental data, it is concluded that YOLO-GW is more suitable for pedestrian detection in a fog environment than YOLOv7.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2140077910",
                        "name": "Xinchao Liu"
                    },
                    {
                        "authorId": "2108433306",
                        "name": "Yier Lin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the follow-up work [22, 55], the authors introduce LTH with rewinding to enable LTH for deeper models and larger datasets."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d36cd8385c969608282d874b1623564959a71b42",
                "externalIds": {
                    "DBLP": "conf/cvpr/TangYLL23",
                    "DOI": "10.1109/CVPR52729.2023.02338",
                    "CorpusId": 260068477
                },
                "corpusId": 260068477,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d36cd8385c969608282d874b1623564959a71b42",
                "title": "Fair Scratch Tickets: Finding Fair Sparse Networks without Weight Training",
                "abstract": "Recent studies suggest that computer vision models come at the risk of compromising fairness. There are exten-sive works to alleviate unfairness in computer vision using pre-processing, in-processing, and post-processing meth-ods. In this paper, we lead a novel fairness-aware learning paradigm for in-processing methods through the lens of the lottery ticket hypothesis (LTH) in the context of computer vision fairness. We randomly initialize a dense neural net-work and find appropriate binary masks for the weights to obtain fair sparse subnetworks without any weight training. Interestingly, to the best of our knowledge, we are the first to discover that such sparse subnetworks with inborn fair-ness exist in randomly initialized networks, achieving an accuracy-fairness trade-off comparable to that of dense neural networks trained with existing fairness-aware in-processing approaches. We term these fair subnetworks as Fair Scratch Tickets (FSTs). We also theoretically pro-vide fairness and accuracy guarantees for them. In our experiments, we investigate the existence of FSTs on var-ious datasets, target attributes, random initialization meth-ods, sparsity patterns, and fairness surrogates. We also find that FSTs can transfer across datasets and investigate other properties of FSTs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2212886261",
                        "name": "Pengwei Tang"
                    },
                    {
                        "authorId": "145889956",
                        "name": "W. Yao"
                    },
                    {
                        "authorId": "1934400475",
                        "name": "Zhicong Li"
                    },
                    {
                        "authorId": "2156610887",
                        "name": "Yong Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some literatures discuss the effectiveness of pruning, fine-tuning [24], and rewinding weights [29] for LTH.",
                "LTH has been widely applied in image classification [9\u2013 12, 30, 32, 33], natural language processing [5, 28, 29, 34], and so on."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "960734a6a2f56c6f405cb8d7c675bfaa310cddfc",
                "externalIds": {
                    "DBLP": "conf/cvpr/LinLHQ0W23",
                    "DOI": "10.1109/CVPR52729.2023.01384",
                    "CorpusId": 260935676
                },
                "corpusId": 260935676,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/960734a6a2f56c6f405cb8d7c675bfaa310cddfc",
                "title": "Memory-Friendly Scalable Super-Resolution via Rewinding Lottery Ticket Hypothesis",
                "abstract": "Scalable deep Super-Resolution (SR) models are increasingly in demand, whose memory can be customized and tuned to the computational recourse of the platform. The existing dynamic scalable SR methods are not memory-friendly enough because multi-scale models have to be saved with a fixed size for each model. Inspired by the success of Lottery Tickets Hypothesis (LTH) on image classification, we explore the existence of unstructured scalable SR deep models, that is, we find gradual shrinkage subnetworks of extreme sparsity named winning tickets. In this paper, we propose a Memory-friendly Scalable SR framework (MSSR). The advantage is that only a single scalable model covers multiple SR models with different sizes, instead of reloading SR models of different sizes. Concretely, MSSR consists of the forward and backward stages, the former for model compression and the latter for model expansion. In the forward stage, we take advantage of LTH with rewinding weights to progressively shrink the SR model and the pruning-out masks that form nested sets. Moreover, stochastic self-distillation (SSD) is conducted to boost the performance of sub-networks. By stochastically selecting multiple depths, the current model inputs the selected features into the corresponding parts in the larger model and improves the performance of the current model based on the feedback results of the larger model. In the backward stage, the smaller SR model could be expanded by recovering and fine-tuning the pruned parameters according to the pruning-out masks obtained in the forward. Extensive experiments show the effectiveness of MMSR. The smallest-scale sub-network could achieve the sparsity of 94% and outperforms the compared lightweight SR methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118747647",
                        "name": "Jin Lin"
                    },
                    {
                        "authorId": "2047166",
                        "name": "Xiaotong Luo"
                    },
                    {
                        "authorId": "143662631",
                        "name": "Ming Hong"
                    },
                    {
                        "authorId": "1696146",
                        "name": "Yanyun Qu"
                    },
                    {
                        "authorId": "2118596087",
                        "name": "Yuan Xie"
                    },
                    {
                        "authorId": "34815981",
                        "name": "Zongze Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While [53], [54] scaled up LTH by rewinding [55], [56]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c7a0c19007fe97db0f12895b3ef2ab1ca78fffb8",
                "externalIds": {
                    "DBLP": "conf/cvpr/Saxena0XK23",
                    "DOI": "10.1109/CVPR52729.2023.01557",
                    "CorpusId": 261080892
                },
                "corpusId": 261080892,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c7a0c19007fe97db0f12895b3ef2ab1ca78fffb8",
                "title": "Re-GAN: Data-Efficient GANs Training via Architectural Reconfiguration",
                "abstract": "Training Generative Adversarial Networks (GANs) on high-fidelity images usually requires a vast number of training images. Recent research on GAN tickets reveals that dense GANs models contain sparse sub-networks or \u201clottery tickets\u201d that, when trained separately, yield better results under limited data. However, finding GANs tickets requires an expensive process of train-prune-retrain. In this paper, we propose Re-GAN, a data-efficient GANs training that dynamically reconfigures GANs architecture during training to explore different sub-network structures in training time. Our method repeatedly prunes unimportant connections to regularize GANs network and regrows them to reduce the risk of prematurely pruning important connections. Re-GAN stabilizes the GANs models with less data and offers an alternative to the existing GANs tickets and progressive growing methods. We demonstrate that Re-GAN is a generic training methodology which achieves stability on datasets of varying sizes, domains, and resolutions (CIFAR-10, Tiny-ImageNet, and multiple few-shot generation datasets) as well as different GANs architectures (SNGAN, ProGAN, StyleGAN2 and AutoGAN). Re-GAN also improves performance when combined with the recent augmentation approaches. Moreover, Re-GAN requires fewer floating-point operations (FLOPs) and less training time by removing the unimportant connections during GANs training while maintaining comparable or even generating higher-quality samples. When compared to state-of-the-art StyleGAN2, our method outperforms without requiring any additional fine-tuning step. Code can be found at this link: https://github.com/IntellicentAI-Lab/Re-GAN",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144109422",
                        "name": "Divya Saxena"
                    },
                    {
                        "authorId": "144115026",
                        "name": "Jiannong Cao"
                    },
                    {
                        "authorId": "81946114",
                        "name": "Jiahao Xu"
                    },
                    {
                        "authorId": "6674523",
                        "name": "Tarun Kulshrestha"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A considerable amount of work has focused on improving the sub-network performance in the (second) pruning stage (Blalock et al., 2020) and the (third) retraining stage (Renda et al., 2020; Le & Hua, 2021).",
                ", 2020) and the (third) retraining stage (Renda et al., 2020; Le & Hua, 2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ae3fe0f334a1dd969047be1a725cd85bfe20d3d0",
                "externalIds": {
                    "ArXiv": "2305.18383",
                    "DBLP": "conf/icml/ZhouYCM23",
                    "DOI": "10.48550/arXiv.2305.18383",
                    "CorpusId": 258967212
                },
                "corpusId": 258967212,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ae3fe0f334a1dd969047be1a725cd85bfe20d3d0",
                "title": "A Three-regime Model of Network Pruning",
                "abstract": "Recent work has highlighted the complex influence training hyperparameters, e.g., the number of training epochs, can have on the prunability of machine learning models. Perhaps surprisingly, a systematic approach to predict precisely how adjusting a specific hyperparameter will affect prunability remains elusive. To address this gap, we introduce a phenomenological model grounded in the statistical mechanics of learning. Our approach uses temperature-like and load-like parameters to model the impact of neural network (NN) training hyperparameters on pruning performance. A key empirical result we identify is a sharp transition phenomenon: depending on the value of a load-like parameter in the pruned model, increasing the value of a temperature-like parameter in the pre-pruned model may either enhance or impair subsequent pruning performance. Based on this transition, we build a three-regime model by taxonomizing the global structure of the pruned NN loss landscape. Our model reveals that the dichotomous effect of high temperature is associated with transitions between distinct types of global structures in the post-pruned model. Based on our results, we present three case-studies: 1) determining whether to increase or decrease a hyperparameter for improved pruning; 2) selecting the best model to prune from a family of models; and 3) tuning the hyperparameter of the Sharpness Aware Minimization method for better pruning performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2111405998",
                        "name": "Yefan Zhou"
                    },
                    {
                        "authorId": "49576470",
                        "name": "Yaoqing Yang"
                    },
                    {
                        "authorId": "2218932579",
                        "name": "Arin Chang"
                    },
                    {
                        "authorId": "1717098",
                        "name": "Michael W. Mahoney"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[26] Alex Renda, Jonathan Frankle, and Michael Carbin.",
                "Recently, iterative magnitude pruning [25, 26] has emerged as a natural solution to this problem."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7244716a1141a3cf02aedaf2c22cfc2526eaac56",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-18402",
                    "ArXiv": "2305.18402",
                    "DOI": "10.48550/arXiv.2305.18402",
                    "CorpusId": 258967367
                },
                "corpusId": 258967367,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7244716a1141a3cf02aedaf2c22cfc2526eaac56",
                "title": "Neural Sculpting: Uncovering hierarchically modular task structure through pruning and network analysis",
                "abstract": "Natural target functions and tasks typically exhibit hierarchical modularity - they can be broken down into simpler sub-functions that are organized in a hierarchy. Such sub-functions have two important features: they have a distinct set of inputs (input-separability) and they are reused as inputs higher in the hierarchy (reusability). Previous studies have established that hierarchically modular neural networks, which are inherently sparse, offer benefits such as learning efficiency, generalization, multi-task learning, and transferability. However, identifying the underlying sub-functions and their hierarchical structure for a given task can be challenging. The high-level question in this work is: if we learn a task using a sufficiently deep neural network, how can we uncover the underlying hierarchy of sub-functions in that task? As a starting point, we examine the domain of Boolean functions, where it is easier to determine whether a task is hierarchically modular. We propose an approach based on iterative unit and edge pruning (during training), combined with network analysis for module detection and hierarchy inference. Finally, we demonstrate that this method can uncover the hierarchical modularity of a wide range of Boolean functions and two vision tasks based on the MNIST digits dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "21819184",
                        "name": "S. M. Patil"
                    },
                    {
                        "authorId": "144691372",
                        "name": "Loizos Michael"
                    },
                    {
                        "authorId": "144734756",
                        "name": "C. Dovrolis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To address this issue, the concept of rewinding is introduced [Frankle et al., 2020a, Renda et al., 2020].",
                "Empirical studies have demonstrated that iterative magnitude pruning (IMP) - applying magnitude-based pruning multiple times - can effectively remove a large proportion of weights in neural networks [Frankle and Carbin, 2019, Renda et al., 2020, Frankle et al., 2020a].",
                "Although simple to implement, IMP shows state-of-the-art results for network pruning, especially for extreme sparsity regimes [Renda et al., 2020]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "feeb1fc53c3d52083041697ccaeb800bbe42162b",
                "externalIds": {
                    "ArXiv": "2305.14852",
                    "DBLP": "journals/corr/abs-2305-14852",
                    "DOI": "10.48550/arXiv.2305.14852",
                    "CorpusId": 258865885
                },
                "corpusId": 258865885,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/feeb1fc53c3d52083041697ccaeb800bbe42162b",
                "title": "SWAMP: Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning",
                "abstract": "Given the ever-increasing size of modern neural networks, the significance of sparse architectures has surged due to their accelerated inference speeds and minimal memory demands. When it comes to global pruning techniques, Iterative Magnitude Pruning (IMP) still stands as a state-of-the-art algorithm despite its simple nature, particularly in extremely sparse regimes. In light of the recent finding that the two successive matching IMP solutions are linearly connected without a loss barrier, we propose Sparse Weight Averaging with Multiple Particles (SWAMP), a straightforward modification of IMP that achieves performance comparable to an ensemble of two IMP solutions. For every iteration, we concurrently train multiple sparse models, referred to as particles, using different batch orders yet the same matching ticket, and then weight average such models to produce a single mask. We demonstrate that our method consistently outperforms existing baselines across different sparsities through extensive experiments on various data and neural network structures.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2187600505",
                        "name": "Moonseok Choi"
                    },
                    {
                        "authorId": "2110211216",
                        "name": "Hyungi Lee"
                    },
                    {
                        "authorId": "2065197138",
                        "name": "G. Nam"
                    },
                    {
                        "authorId": "2124954802",
                        "name": "Juho Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Prevalent importance criteria are based on the parameter\u2019s magnitude (Zhu and Gupta, 2017; Renda et al., 2020) or sensitivity (Louizos et al.",
                "Prevalent importance criteria are based on the parameter\u2019s magnitude (Zhu and Gupta, 2017; Renda et al., 2020) or sensitivity (Louizos et al., 2018; Sanh et al., 2020; Liang et al., 2021; Zhang et al., 2022)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "734d6a6dc2382a673b29e93ce68837f1eb95bd47",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-12394",
                    "ArXiv": "2305.12394",
                    "DOI": "10.48550/arXiv.2305.12394",
                    "CorpusId": 258833212
                },
                "corpusId": 258833212,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/734d6a6dc2382a673b29e93ce68837f1eb95bd47",
                "title": "Pruning Pre-trained Language Models with Principled Importance and Self-regularization",
                "abstract": "Iterative pruning is one of the most effective compression methods for pre-trained language models. We discovered that finding the optimal pruning decision is an equality-constrained 0-1 Integer Linear Programming problem. The solution to this optimization problem leads to a principled importance criterion which we use to rank parameters during iterative model pruning. To mitigate the poor generalization at high sparsity levels, we propose a self-regularization scheme where model prediction is regularized by the latest checkpoint with increasing sparsity throughout pruning. Our experiments on natural language understanding, question-answering, named entity recognition, and data-to-text generation with various Transformer-based PLMs show the effectiveness of the approach at various sparsity levels.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1642328639",
                        "name": "Siyu Ren"
                    },
                    {
                        "authorId": "1796651",
                        "name": "Kenny Q. Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another alternative is weight pruning, which reduces the number of parameters in neural networks (Han et al., 2015a;b; Frankle & Carbin, 2018; Renda et al., 2020; Sreenivasan et al., 2022b).",
                "Other efforts have sought to eliminate the need for model weight rewinding (Renda et al., 2020) and to identify winning tickets at initialization (Wang et al.",
                "Other efforts have sought to eliminate the need for model weight rewinding (Renda et al., 2020) and to identify winning tickets at initialization (Wang et al., 2020a; Sreenivasan et al., 2022b)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a06a4a38668c4737ab2ce80badc177ea3f520456",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-02538",
                    "ArXiv": "2305.02538",
                    "DOI": "10.48550/arXiv.2305.02538",
                    "CorpusId": 258480187
                },
                "corpusId": 258480187,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a06a4a38668c4737ab2ce80badc177ea3f520456",
                "title": "Cuttlefish: Low-Rank Model Training without All the Tuning",
                "abstract": "Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacrificing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing Cuttlefish, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. Cuttlefish leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. Cuttlefish switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that Cuttlefish generates models up to 5.6 times smaller than full-rank models, and attains up to a 1.2 times faster end-to-end training process while preserving comparable accuracy. Moreover, Cuttlefish outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109798334",
                        "name": "Hongyi Wang"
                    },
                    {
                        "authorId": "2216074086",
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "authorId": "150266516",
                        "name": "Pongsakorn U-chupala"
                    },
                    {
                        "authorId": "2112517704",
                        "name": "Yoshiki Tanaka"
                    },
                    {
                        "authorId": "2064963077",
                        "name": "Eric P. Xing"
                    },
                    {
                        "authorId": "1740595",
                        "name": "Dimitris Papailiopoulos"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, some pruning techniques may result in more training efforts during model building stage, such as methods that are based on the procedures of training, pruning, and fine-tuning/re-training [36], [37]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1073f81c20ff46608908e493c12c45ca1d4e6e7c",
                "externalIds": {
                    "DBLP": "journals/cim/ChenWCLO23",
                    "DOI": "10.1109/MCI.2023.3245733",
                    "CorpusId": 258136838
                },
                "corpusId": 258136838,
                "publicationVenue": {
                    "id": "ee372de7-efda-4907-a03f-359292ea27f6",
                    "name": "IEEE Computational Intelligence Magazine",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Comput Intell Mag"
                    ],
                    "issn": "1556-603X",
                    "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=10207",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10207"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1073f81c20ff46608908e493c12c45ca1d4e6e7c",
                "title": "Survey on AI Sustainability: Emerging Trends on Learning Algorithms and Research Challenges [Review Article]",
                "abstract": "Artificial Intelligence (AI) is a fast-growing research and development (R&D) discipline which is attracting increasing attention because it promises to bring vast benefits for consumers and businesses, with considerable benefits promised in productivity growth and innovation. To date, significant accomplishments have been reported in many areas that have been deemed challenging for machines, ranging from computer vision, natural language processing, audio analysis to smart sensing and many others. The technology trend in realizing success has developed towards increasingly complex and large-size AI models to solve more complex problems at superior performance and robustness. This rapid progress, however, has taken place at the expense of substantial environmental costs and resources. In addition, debates on the societal impacts of AI, such as fairness, safety, and privacy, have continued to grow in intensity. These issues have reflected major concerns pertaining to the sustainable development of AI. In this work, major trends in machine learning approaches that can address the sustainability problem of AI have been reviewed. Specifically, the emerging AI methodologies and algorithms are examined for addressing the sustainability issue of AI in two major aspects, i.e., environmental sustainability and social sustainability of AI. Then, the major limitations of the existing studies are highlighted, and potential research challenges and directions are proposed for the development of the next generation of sustainable AI techniques. It is believed that this technical review can help promote a sustainable development of AI R&D activities for the research community.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48354147",
                        "name": "Zhenghua Chen"
                    },
                    {
                        "authorId": "1390606776",
                        "name": "Min Wu"
                    },
                    {
                        "authorId": "143962358",
                        "name": "Alvin Chan"
                    },
                    {
                        "authorId": "2108674591",
                        "name": "Xiaoli Li"
                    },
                    {
                        "authorId": "8748397",
                        "name": "Y. Ong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The strategies in [12], [13], [14], [15] try to accelerate com-"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9c26766b8c34d537655fabcf18698c0f765fed99",
                "externalIds": {
                    "DBLP": "journals/tsc/LuZSFWTZ23",
                    "DOI": "10.1109/TSC.2022.3187118",
                    "CorpusId": 250160539
                },
                "corpusId": 250160539,
                "publicationVenue": {
                    "id": "e2742a49-1b55-4a31-b319-7455fa5e12d4",
                    "name": "IEEE Transactions on Services Computing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Serv Comput"
                    ],
                    "issn": "1939-1374",
                    "url": "http://www.computer.org/tsc",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4629386"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9c26766b8c34d537655fabcf18698c0f765fed99",
                "title": "TreeNet Based Fast Task Decomposition for Resource-Constrained Edge Intelligence",
                "abstract": "Edge intelligence is an emerging technology that integrates edge computing and deep learning to bring AI to the network's edge. It has gained wide attention for its lower network latency and better privacy preservation abilities. However, the inference of deep neural networks is computationally demanding and results in poor real-time performance, making it challenging for resource-constrained edge devices. In this article, we propose a hierarchical deep learning model based on TreeNet to reduce the computational cost for edge devices. Based on the similarity of the classification categories, we decompose a given task into disjoint sub-tasks to reduce the complexity of the required model. Then a lightweight binary classifier is proposed for evaluating the sub-task inference result. If the inference result of a sub-task is unreliable, our system will forward the input samples to the cloud server for further processing. We also proposed a new strategy for finding and sharing common features across sub-tasks to improve training speed and accuracy. The experimental results on several popular datasets demonstrate the effectiveness of our approach in speeding up inferences while processing most of the input data with a low error rate.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2213844237",
                        "name": "Dong Lu"
                    },
                    {
                        "authorId": "2779198",
                        "name": "Yanlong Zhai"
                    },
                    {
                        "authorId": "2109764600",
                        "name": "Jun Shen"
                    },
                    {
                        "authorId": "2440461",
                        "name": "M. Fahmideh"
                    },
                    {
                        "authorId": "2144207383",
                        "name": "Jianqing Wu"
                    },
                    {
                        "authorId": "1411295963",
                        "name": "Jude Tchaye-Kondi"
                    },
                    {
                        "authorId": "1692039",
                        "name": "Liehuang Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, it is demonstrated that in the case of CVNN the rewinding process of both weights and learning rate has a positive effect, and as a consequence WR is a more promising pruning technique than LR and FT, which is not the case in real-valued NN [4].",
                "In LR only the learning rate is reset to its pre-trained value, leaving the unpruned weights to be re-trained from their values at the end of the initial training phase [4]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b5cb394da38401f135235b2f9976ff45f6fb705d",
                "externalIds": {
                    "DOI": "10.1364/cleo_si.2023.sm3i.6",
                    "CorpusId": 260266869
                },
                "corpusId": 260266869,
                "publicationVenue": {
                    "id": "bc96f9bd-89da-4c9a-ab24-27749617f6ff",
                    "name": "Conference on Lasers and Electro-Optics",
                    "type": "conference",
                    "alternate_names": [
                        "CLEO",
                        "Conf Laser Electro-optics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b5cb394da38401f135235b2f9976ff45f6fb705d",
                "title": "Combination of Optical Phase Conjugation and Advanced Pruning Techniques to reduce the Computational Complexity of Neural Network-Based Equalisers",
                "abstract": "Combining midlink optical phase conjugation and different pruning techniques is proposed to develop low complexity complex-valued neural networks based equalisers in a PDM 28 Gbaud 64 QAM transmission over 400km of SSMF.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1580690649",
                        "name": "D. A. Ron"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020); and (ii) recovering weights to previous values is compatible with the finding that tuning pruned networks with earlier weight values or learning rate schedule is beneficial (Renda et al., 2020).",
                "\u2026allowing the growth of connections has been shown to yield better sparse networks (Evci et al., 2020); and (ii) recovering weights to previous values is compatible with the finding that tuning pruned networks with earlier weight values or learning rate schedule is beneficial (Renda et al., 2020)."
            ],
            "intents": [
                "result"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8c82901a48ce2e1a3ed39a2fad85fa98dd81ade2",
                "externalIds": {
                    "ArXiv": "2304.11237",
                    "DBLP": "journals/corr/abs-2304-11237",
                    "DOI": "10.48550/arXiv.2304.11237",
                    "CorpusId": 258298360
                },
                "corpusId": 258298360,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8c82901a48ce2e1a3ed39a2fad85fa98dd81ade2",
                "title": "Effective Neural Network L0 Regularization With BinMask",
                "abstract": "$L_0$ regularization of neural networks is a fundamental problem. In addition to regularizing models for better generalizability, $L_0$ regularization also applies to selecting input features and training sparse neural networks. There is a large body of research on related topics, some with quite complicated methods. In this paper, we show that a straightforward formulation, BinMask, which multiplies weights with deterministic binary masks and uses the identity straight-through estimator for backpropagation, is an effective $L_0$ regularizer. We evaluate BinMask on three tasks: feature selection, network sparsification, and model regularization. Despite its simplicity, BinMask achieves competitive performance on all the benchmarks without task-specific tuning compared to methods designed for each task. Our results suggest that decoupling weights from mask optimization, which has been widely adopted by previous work, is a key component for effective $L_0$ regularization.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "49104216",
                        "name": "Kai Jia"
                    },
                    {
                        "authorId": "1720971",
                        "name": "M. Rinard"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "acbb22741f0a0a016cfd3b6504372f203f49577c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-09453",
                    "ArXiv": "2304.09453",
                    "DOI": "10.48550/arXiv.2304.09453",
                    "CorpusId": 258212551
                },
                "corpusId": 258212551,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/acbb22741f0a0a016cfd3b6504372f203f49577c",
                "title": "Network Pruning Spaces",
                "abstract": "Network pruning techniques, including weight pruning and filter pruning, reveal that most state-of-the-art neural networks can be accelerated without a significant performance drop. This work focuses on filter pruning which enables accelerated inference with any off-the-shelf deep learning library and hardware. We propose the concept of \\emph{network pruning spaces} that parametrize populations of subnetwork architectures. Based on this concept, we explore the structure aspect of subnetworks that result in minimal loss of accuracy in different pruning regimes and arrive at a series of observations by comparing subnetwork distributions. We conjecture through empirical studies that there exists an optimal FLOPs-to-parameter-bucket ratio related to the design of original network in a pruning regime. Statistically, the structure of a winning subnetwork guarantees an approximately optimal ratio in this regime. Upon our conjectures, we further refine the initial pruning space to reduce the cost of searching a good subnetwork architecture. Our experimental results on ImageNet show that the subnetwork we found is superior to those from the state-of-the-art pruning methods under comparable FLOPs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2167423417",
                        "name": "Xuanyu He"
                    },
                    {
                        "authorId": "2214748577",
                        "name": "Yu-I Yang"
                    },
                    {
                        "authorId": "1961291450",
                        "name": "Ran Song"
                    },
                    {
                        "authorId": "65774691",
                        "name": "Jiachen Pu"
                    },
                    {
                        "authorId": "2202599958",
                        "name": "Conggang Hu"
                    },
                    {
                        "authorId": "2106723043",
                        "name": "Feijun Jiang"
                    },
                    {
                        "authorId": "143715293",
                        "name": "W. Zhang"
                    },
                    {
                        "authorId": "1720753309",
                        "name": "Huanghao Ding"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This rejuvenated the field of sparse deep learning Renda et al. (2020); Chen et al. (2020) and more recently the interest spilled over into sparse reinforcement learning (RL) as well Arnob et al. (2021); Sokar et al. (2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "af96cf1737a167bfd442d4d23cb94cf6b7281453",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-07488",
                    "ArXiv": "2304.07488",
                    "DOI": "10.48550/arXiv.2304.07488",
                    "CorpusId": 258179154
                },
                "corpusId": 258179154,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/af96cf1737a167bfd442d4d23cb94cf6b7281453",
                "title": "SalientGrads: Sparse Models for Communication Efficient and Data Aware Distributed Federated Training",
                "abstract": "Federated learning (FL) enables the training of a model leveraging decentralized data in client sites while preserving privacy by not collecting data. However, one of the significant challenges of FL is limited computation and low communication bandwidth in resource limited edge client nodes. To address this, several solutions have been proposed in recent times including transmitting sparse models and learning dynamic masks iteratively, among others. However, many of these methods rely on transmitting the model weights throughout the entire training process as they are based on ad-hoc or random pruning criteria. In this work, we propose Salient Grads, which simplifies the process of sparse training by choosing a data aware subnetwork before training, based on the model-parameter's saliency scores, which is calculated from the local client data. Moreover only highly sparse gradients are transmitted between the server and client models during the training process unlike most methods that rely on sharing the entire dense model in each round. We also demonstrate the efficacy of our method in a real world federated learning application and report improvement in wall-clock communication time.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "31048326",
                        "name": "Riyasat Ohib"
                    },
                    {
                        "authorId": "150941167",
                        "name": "Bishal Thapaliya"
                    },
                    {
                        "authorId": "2214582716",
                        "name": "Pratyush Gaggenapalli"
                    },
                    {
                        "authorId": "46701354",
                        "name": "J. Liu"
                    },
                    {
                        "authorId": "2133976600",
                        "name": "Vince D. Calhoun"
                    },
                    {
                        "authorId": "32611384",
                        "name": "S. Plis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There are mainly two types of pruning methods: post-hoc pruning (Han et al., 2015; Renda et al., 2020; Molchanov et al., 2019; LeCun et al., 1989; Hassibi & Stork, 1992) and foresight pruning (Lee et al.",
                "Most post-hoc pruning methods intend to reduce the inference time, with the exception of lottery ticket pruning (Frankle & Carbin, 2018; Renda et al., 2020) which prunes the networks for the purpose of finding a trainable sub-network.",
                "There are mainly two types of pruning methods: post-hoc pruning (Han et al., 2015; Renda et al., 2020; Molchanov et al., 2019; LeCun et al., 1989; Hassibi & Stork, 1992) and foresight pruning (Lee et al., 2018; Wang et al., 2020; Alizadeh et al., 2022; Tanaka et al., 2020; de Jorge et al., 2020b;\u2026",
                "Lottery ticket experiments (Frankle & Carbin, 2018; Renda et al., 2020; Frankle et al., 2020a) use the same architectures."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "b6da4e11e24da4e863bbc1c5c7bd6080d0906b98",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-02840",
                    "ArXiv": "2304.02840",
                    "DOI": "10.48550/arXiv.2304.02840",
                    "CorpusId": 257985378
                },
                "corpusId": 257985378,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b6da4e11e24da4e863bbc1c5c7bd6080d0906b98",
                "title": "NTK-SAP: Improving neural network pruning by aligning training dynamics",
                "abstract": "Pruning neural networks before training has received increasing interest due to its potential to reduce training time and memory. One popular method is to prune the connections based on a certain metric, but it is not entirely clear what metric is the best choice. Recent advances in neural tangent kernel (NTK) theory suggest that the training dynamics of large enough neural networks is closely related to the spectrum of the NTK. Motivated by this finding, we propose to prune the connections that have the least influence on the spectrum of the NTK. This method can help maintain the NTK spectrum, which may help align the training dynamics to that of its dense counterpart. However, one possible issue is that the fixed-weight-NTK corresponding to a given initial point can be very different from the NTK corresponding to later iterates during the training phase. We further propose to sample multiple realizations of random weights to estimate the NTK spectrum. Note that our approach is weight-agnostic, which is different from most existing methods that are weight-dependent. In addition, we use random inputs to compute the fixed-weight-NTK, making our method data-agnostic as well. We name our foresight pruning algorithm Neural Tangent Kernel Spectrum-Aware Pruning (NTK-SAP). Empirically, our method achieves better performance than all baselines on multiple datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2159213654",
                        "name": "Yite Wang"
                    },
                    {
                        "authorId": "49620929",
                        "name": "Dawei Li"
                    },
                    {
                        "authorId": "2068169846",
                        "name": "Ruoyu Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[17] found that weight rewinding and learning rate rewinding outperformed fine-tuning.",
                "[17] proposed a third retraining technique, learning rate rewinding, where the unpruned weights are trained from their final values with the learning rate schedule from weight rewinding.",
                "One such retraining technique is fine-tuning where a small fixed learning rate is used used to train the unpruned weights from their final trained values [17]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "4e99f9984237be42d64efb9dcee1930dd041a971",
                "externalIds": {
                    "ArXiv": "2303.15479",
                    "DBLP": "journals/corr/abs-2303-15479",
                    "DOI": "10.48550/arXiv.2303.15479",
                    "CorpusId": 257771556
                },
                "corpusId": 257771556,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4e99f9984237be42d64efb9dcee1930dd041a971",
                "title": "Exploring the Performance of Pruning Methods in Neural Networks: An Empirical Study of the Lottery Ticket Hypothesis",
                "abstract": "In this paper, we explore the performance of different pruning methods in the context of the lottery ticket hypothesis. We compare the performance of L1 unstructured pruning, Fisher pruning, and random pruning on different network architectures and pruning scenarios. The experiments include an evaluation of one-shot and iterative pruning, an examination of weight movement in the network during pruning, a comparison of the pruning methods on networks of varying widths, and an analysis of the performance of the methods when the network becomes very sparse. Additionally, we propose and evaluate a new method for efficient computation of Fisher pruning, known as batched Fisher pruning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2212796281",
                        "name": "Eirik Fladmark"
                    },
                    {
                        "authorId": "2212386124",
                        "name": "Muhammad Hamza Sajjad"
                    },
                    {
                        "authorId": "2212805271",
                        "name": "Laura Brinkholm Justesen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "965715ac9ab12f146cc4984d8e680d56f0d14c8d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-08595",
                    "ArXiv": "2303.08595",
                    "DOI": "10.48550/arXiv.2303.08595",
                    "CorpusId": 257233142
                },
                "corpusId": 257233142,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/965715ac9ab12f146cc4984d8e680d56f0d14c8d",
                "title": "Automatic Attention Pruning: Improving and Automating Model Pruning using Attentions",
                "abstract": "Pruning is a promising approach to compress deep learning models in order to deploy them on resource-constrained edge devices. However, many existing pruning solutions are based on unstructured pruning, which yields models that cannot efficiently run on commodity hardware; and they often require users to manually explore and tune the pruning process, which is time-consuming and often leads to sub-optimal results. To address these limitations, this paper presents Automatic Attention Pruning (AAP), an adaptive, attention-based, structured pruning approach to automatically generate small, accurate, and hardware-efficient models that meet user objectives. First, it proposes iterative structured pruning using activation-based attention maps to effectively identify and prune unimportant filters. Then, it proposes adaptive pruning policies for automatically meeting the pruning objectives of accuracy-critical, memory-constrained, and latency-sensitive tasks. A comprehensive evaluation shows that AAP substantially outperforms the state-of-the-art structured pruning works for a variety of model architectures. Our code is at: https://github.com/kaiqi123/Automatic-Attention-Pruning.git.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1995855",
                        "name": "Kaiqi Zhao"
                    },
                    {
                        "authorId": "101682296",
                        "name": "Animesh Jain"
                    },
                    {
                        "authorId": "2152527896",
                        "name": "Ming Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026et al., 2018a; Li et al., 2019; Zhang et al., 2018), reinforcement learning (He et al., 2018b; Chen et al., 2019), lottery ticket (Frankle & Carbin, 2018; Frankle et al., 2019; Renda et al., 2020), etc.; (iii) (iteratively) retrain the pruned model to regain the accuracy regression during pruning."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e3faac97f35bd02ffa534a672bb25c1ac0d57330",
                "externalIds": {
                    "ArXiv": "2303.06862",
                    "DBLP": "conf/iclr/ChenLDZZ23",
                    "DOI": "10.48550/arXiv.2303.06862",
                    "CorpusId": 257495957
                },
                "corpusId": 257495957,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e3faac97f35bd02ffa534a672bb25c1ac0d57330",
                "title": "OTOV2: Automatic, Generic, User-Friendly",
                "abstract": "The existing model compression methods via structured pruning typically require complicated multi-stage procedures. Each individual stage necessitates numerous engineering efforts and domain-knowledge from the end-users which prevent their wider applications onto broader scenarios. We propose the second generation of Only-Train-Once (OTOv2), which first automatically trains and compresses a general DNN only once from scratch to produce a more compact model with competitive performance without fine-tuning. OTOv2 is automatic and pluggable into various deep learning applications, and requires almost minimal engineering efforts from the users. Methodologically, OTOv2 proposes two major improvements: (i) Autonomy: automatically exploits the dependency of general DNNs, partitions the trainable variables into Zero-Invariant Groups (ZIGs), and constructs the compressed model; and (ii) Dual Half-Space Projected Gradient (DHSPG): a novel optimizer to more reliably solve structured-sparsity problems. Numerically, we demonstrate the generality and autonomy of OTOv2 on a variety of model architectures such as VGG, ResNet, CARN, ConvNeXt, DenseNet and StackedUnets, the majority of which cannot be handled by other methods without extensive handcrafting efforts. Together with benchmark datasets including CIFAR10/100, DIV2K, Fashion-MNIST, SVNH and ImageNet, its effectiveness is validated by performing competitively or even better than the state-of-the-arts. The source code is available at https://github.com/tianyic/only_train_once.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153166785",
                        "name": "Tianyi Chen"
                    },
                    {
                        "authorId": "46225943",
                        "name": "Luming Liang"
                    },
                    {
                        "authorId": "46873622",
                        "name": "Tian Ding"
                    },
                    {
                        "authorId": "2188797022",
                        "name": "Zhihui Zhu"
                    },
                    {
                        "authorId": "15623770",
                        "name": "Ilya Zharkov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is in stark contrast with the behavior of SNNs on the image classification task, where LTH can gracefully preserve the matching performance even at very extreme sparsities (>95% on CIFAR-10/100 (Yin et al., 2022) and >80% on ImageNet (Renda et al., 2020)).",
                "For instance, even though ImageNet has been considered a rather challenging task over years, very high accuracy (>90",
                "Following a mainstream convention in many sparse training papers (Frankle & Carbin, 2019; Gale et al., 2019; Evci et al., 2020; Lee et al., 2019; Liu et al., 2021c), we sparsify most layers in the model including embedding layers and classifier heads, and we do not apply advanced techniques such as Iterative Learning Rate Rewinding (Renda et al., 2020) and Knowledge Distillation (Hinton et al., 2015) in our main evaluations, even if we observe that they help to alleviate accuracy drops as in Appendix C.\n\u2022 Lottery Ticket Hypothesis (LTH) (Frankle & Carbin, 2019) is a strong post-training pruning baseline that iteratively adopts magnitude pruning after training to produce binary masks and re-train together with weights from step t.",
                "Nevertheless, such a well-tuned pruning recipe is both time- and resource-intensive (9\u00d7more fine-tuning time, besides Hessian matrix\napproximation); and even so, the strongest SNNs still fall short of their dense counterpart by around 10% accuracy at sparsities between 60%\u2212 80%, in contrast to \u201cnormal\u201d SNNs that easily match their dense models on CIFAR, ImageNet, or GLUE.",
                "Secondly, people are obsessed with evaluating SNNs on well-understood datasets, including but not limited to MNIST (LeCun, 1998) (26 papers), CIFAR-10/100 (Krizhevsky et al., 2009) (59 and 37 papers, respectively), ImageNet (Deng et al., 2009) (62 papers), and GLUE (Wang et al., 2018) (9 papers), where deep neural networks have already exceeded the human-equivalent performance (refer to Appendix D for more details).",
                ", 2021c), we sparsify most layers in the model including embedding layers and classifier heads, and we do not apply advanced techniques such as Iterative Learning Rate Rewinding (Renda et al., 2020) and Knowledge Distillation (Hinton et al.",
                "\u2026in the model including embedding layers and classifier heads, and we do not apply advanced techniques such as Iterative Learning Rate Rewinding (Renda et al., 2020) and Knowledge Distillation (Hinton et al., 2015) in our main evaluations, even if we observe that they help to alleviate accuracy\u2026",
                "Specifically, we follow Kurtic et al. (2022) and replace the magnitude pruning criterion of the LTH framework with the second-order oBERT criterion; adopt Learning Rate Rewinding (LRR) (Renda et al., 2020) and Knowledge Distillation (KD) (Hinton et al., 2015) during each pruning iteration; and keep the embeddings and classification heads dense.",
                "Renda et al. (2020) further found that instead of re-training with the initial weights, re-training with the final weights achieves better performance.",
                "\u2026(2022) and replace the magnitude pruning criterion of the LTH framework with the second-order oBERT criterion; adopt Learning Rate Rewinding (LRR) (Renda et al., 2020) and Knowledge Distillation (KD) (Hinton et al., 2015) during each pruning iteration; and keep the embeddings and classification\u2026",
                "(2022) and replace the magnitude pruning criterion of the LTH framework with the second-order oBERT criterion; adopt Learning Rate Rewinding (LRR) (Renda et al., 2020) and Knowledge Distillation (KD) (Hinton et al."
            ],
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "fdacdbc6a00eeb42efe7f81848b0bc09be5ca997",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-02141",
                    "ArXiv": "2303.02141",
                    "DOI": "10.48550/arXiv.2303.02141",
                    "CorpusId": 257353428
                },
                "corpusId": 257353428,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/fdacdbc6a00eeb42efe7f81848b0bc09be5ca997",
                "title": "Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!",
                "abstract": "Sparse Neural Networks (SNNs) have received voluminous attention predominantly due to growing computational and memory footprints of consistently exploding parameter count in large-scale models. Similar to their dense counterparts, recent SNNs generalize just as well and are equipped with numerous favorable benefits (e.g., low complexity, high scalability, and robustness), sometimes even better than the original dense networks. As research effort is focused on developing increasingly sophisticated sparse algorithms, it is startling that a comprehensive benchmark to evaluate the effectiveness of these algorithms has been highly overlooked. In absence of a carefully crafted evaluation benchmark, most if not all, sparse algorithms are evaluated against fairly simple and naive tasks (eg. CIFAR, ImageNet, GLUE, etc.), which can potentially camouflage many advantages as well unexpected predicaments of SNNs. In pursuit of a more general evaluation and unveiling the true potential of sparse algorithms, we introduce\"Sparsity May Cry\"Benchmark (SMC-Bench), a collection of carefully-curated 4 diverse tasks with 10 datasets, that accounts for capturing a wide range of domain-specific and sophisticated knowledge. Our systemic evaluation of the most representative sparse algorithms reveals an important obscured observation: the state-of-the-art magnitude- and/or gradient-based sparse algorithms seemingly fail to perform on SMC-Bench when applied out-of-the-box, sometimes at significantly trivial sparsity as low as 5%. By incorporating these well-thought and diverse tasks, SMC-Bench is designed to favor and encourage the development of more scalable and generalizable sparse algorithms.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2109338656",
                        "name": "Zhenyu (Allen) Zhang"
                    },
                    {
                        "authorId": "1753619492",
                        "name": "Xuxi Chen"
                    },
                    {
                        "authorId": "8242939",
                        "name": "Tianjin Huang"
                    },
                    {
                        "authorId": "145018564",
                        "name": "Ajay Jaiswal"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[272] propose to rewind the learning rate schedule but not the weight value (learning rate rewinding)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "79dcd3843796fd552844cea6a0f244d5852fb9fb",
                "externalIds": {
                    "ArXiv": "2303.00566",
                    "DBLP": "journals/corr/abs-2303-00566",
                    "DOI": "10.48550/arXiv.2303.00566",
                    "CorpusId": 257255597
                },
                "corpusId": 257255597,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/79dcd3843796fd552844cea6a0f244d5852fb9fb",
                "title": "Structured Pruning for Deep Convolutional Neural Networks: A survey",
                "abstract": "The remarkable performance of deep Convolutional neural networks (CNNs) is generally attributed to their deeper and wider architectures, which can come with significant computational costs. Pruning neural networks has thus gained interest since it effectively lowers storage and computational costs. In contrast to weight pruning, which results in unstructured models, structured pruning provides the benefit of realistic acceleration by producing models that are friendly to hardware implementation. The special requirements of structured pruning have led to the discovery of numerous new challenges and the development of innovative solutions. This article surveys the recent progress towards structured pruning of deep CNNs. We summarize and compare the state-of-the-art structured pruning techniques with respect to filter ranking methods, regularization methods, dynamic execution, neural architecture search, the lottery ticket hypothesis, and the applications of pruning. While discussing structured pruning algorithms, we briefly introduce the unstructured pruning counterpart to emphasize their differences. Furthermore, we provide insights into potential research opportunities in the field of structured pruning. A curated list of neural network pruning papers can be found at https://github.com/he-y/Awesome-Pruning",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1743616",
                        "name": "Yang He"
                    },
                    {
                        "authorId": "2210247831",
                        "name": "Lingao Xiao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[30] and the L-norm based filter pruning approach proposed by Li et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fbf3ed026c4c3025c371606bfab9b97a37fd89e3",
                "externalIds": {
                    "DBLP": "conf/vr/SinghHLPSZA23",
                    "DOI": "10.1109/VR55154.2023.00036",
                    "CorpusId": 257368112
                },
                "corpusId": 257368112,
                "publicationVenue": {
                    "id": "8b5a4b01-a494-4d68-b69a-e32afb408419",
                    "name": "IEEE Conference on Virtual Reality and 3D User Interfaces",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Conf Virtual Real 3D User Interface",
                        "IEEE Virtual Real Conf",
                        "VR",
                        "IEEE Virtual Reality Conference"
                    ],
                    "url": "http://ieeevr.org/"
                },
                "url": "https://www.semanticscholar.org/paper/fbf3ed026c4c3025c371606bfab9b97a37fd89e3",
                "title": "Power, Performance, and Image Quality Tradeoffs in Foveated Rendering",
                "abstract": "Extended reality (XR) devices, including augmented, virtual, and mixed reality, provide a deeply immersive experience. However, practical limitations like weight, heat, and comfort put extreme constraints on the performance, power consumption, and image quality of such systems. In this paper, we study how these constraints form the tradeoff between Fixed Foveated Rendering (FFR), Gaze-Tracked Foveated Rendering (TFR), and conventional, non-foveated rendering. While existing papers have often studied these methods, we provide the first comprehensive study of their relative feasibility in practical systems with limited battery life and computational budget. We show that TFR with the added cost of the gaze-tracker can often be more expensive than FFR. Thus, we co-design a gaze-tracked foveated renderer considering its benefits in computation, power efficiency, and tradeoffs in image quality. We describe principled approximations for eye tracking which provide up to a 9x speedup in runtime performance with approximately a 20x improvement in energy efficiency when run on a mobile GPU. In isolation, these approximations appear to significantly degrade the gaze quality, but appropriate compensation in the visual pipeline can mitigate the loss. Overall, we show that with a highly optimized gaze-tracker, TFR is feasible compared to FFR, resulting in up to 1.25x faster frame times while also reducing total energy consumption by over 40%.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2210831123",
                        "name": "Rahul Singh"
                    },
                    {
                        "authorId": "1975945",
                        "name": "Muhammad Huzaifa"
                    },
                    {
                        "authorId": "2108345828",
                        "name": "Jeffrey Liu"
                    },
                    {
                        "authorId": "2514626",
                        "name": "Anjul Patney"
                    },
                    {
                        "authorId": "36829056",
                        "name": "Hashim Sharif"
                    },
                    {
                        "authorId": "2124210780",
                        "name": "Yifan Zhao"
                    },
                    {
                        "authorId": "3196444",
                        "name": "S. Adve"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5fd3110dbb4828e177d26f263ed17aa88da0a3ff",
                "externalIds": {
                    "DBLP": "journals/pr/ChenWC23",
                    "DOI": "10.1016/j.patcog.2023.109527",
                    "CorpusId": 257575672
                },
                "corpusId": 257575672,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5fd3110dbb4828e177d26f263ed17aa88da0a3ff",
                "title": "Towards Automatic Model Compression via a Unified Two-Stage Framework",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109007495",
                        "name": "Weihan Chen"
                    },
                    {
                        "authorId": "1656803942",
                        "name": "Peisong Wang"
                    },
                    {
                        "authorId": "2149023580",
                        "name": "Jian Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Post-hoc pruning prunes weights of a fully-trained neural network, and they usually have high computation cost due to the multiple rounds of train-prune-retrain procedure (Han et al., 2015; Renda et al., 2020).",
                "Post-hoc pruning was initially proposed to reduce the inference time, while later work on lottery ticket works (Frankle & Carbin, 2018; Renda et al., 2020) aimed to mine trainable sub-networks."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "df1ad8f44fc73282581a389f079df7adc446ac3e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-14670",
                    "ArXiv": "2302.14670",
                    "DOI": "10.48550/arXiv.2302.14670",
                    "CorpusId": 257232904
                },
                "corpusId": 257232904,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/df1ad8f44fc73282581a389f079df7adc446ac3e",
                "title": "Double Dynamic Sparse Training for GANs",
                "abstract": "The past decade has witnessed a drastic increase in modern deep neural networks (DNNs) size, especially for generative adversarial networks (GANs). Since GANs usually suffer from high computational complexity, researchers have shown an increased interest in applying pruning methods to reduce the training and inference costs of GANs. Among different pruning methods invented for supervised learning, dynamic sparse training (DST) has gained increasing attention recently as it enjoys excellent training efficiency with comparable performance to post-hoc pruning. Hence, applying DST on GANs, where we train a sparse GAN with a fixed parameter count throughout training, seems to be a good candidate for reducing GAN training costs. However, a few challenges, including the degrading training instability, emerge due to the adversarial nature of GANs. Hence, we introduce a quantity called balance ratio (BR) to quantify the balance of the generator and the discriminator. We conduct a series of experiments to show the importance of BR in understanding sparse GAN training. Building upon single dynamic sparse training (SDST), where only the generator is adjusted during training, we propose double dynamic sparse training (DDST) to control the BR during GAN training. Empirically, DDST automatically determines the density of the discriminator and greatly boosts the performance of sparse GANs on multiple datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2159213654",
                        "name": "Yite Wang"
                    },
                    {
                        "authorId": "2149267217",
                        "name": "Jing Wu"
                    },
                    {
                        "authorId": "2130392",
                        "name": "N. Hovakimyan"
                    },
                    {
                        "authorId": "153899948",
                        "name": "Ruoyu Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[35] compares the impact of finetuning and rewinding and introduces the aspect of learning rate rewinding."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "de643458575961f99923953f6727809f74451b1b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-11244",
                    "ArXiv": "2302.11244",
                    "DOI": "10.48550/arXiv.2302.11244",
                    "CorpusId": 257079070
                },
                "corpusId": 257079070,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/de643458575961f99923953f6727809f74451b1b",
                "title": "Considering Layerwise Importance in the Lottery Ticket Hypothesis",
                "abstract": "The Lottery Ticket Hypothesis (LTH) showed that by iteratively training a model, removing connections with the lowest global weight magnitude and rewinding the remaining connections, sparse networks can be extracted. This global comparison removes context information between connections within a layer. Here we study means for recovering some of this layer distributional context and generalise the LTH to consider weight importance values rather than global weight magnitudes. We find that given a repeatable training procedure, applying different importance metrics leads to distinct performant lottery tickets with little overlapping connections. This strongly suggests that lottery tickets are not unique",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2064051623",
                        "name": "Benjamin Vandersmissen"
                    },
                    {
                        "authorId": "108666950",
                        "name": "Jos\u00e9 Oramas"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Pruning is a powerful compression approach which removes redundant parameters without significantly deteriorating the full model performance Han et al. (2015b;a); Paganini & Forde (2020); Zhu & Gupta (2017); Renda et al. (2020); Zafrir et al. (2021); Liang et al. (2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b835345c6168d7b179516700aa4460912a8857e9",
                "externalIds": {
                    "ArXiv": "2302.09632",
                    "DBLP": "journals/corr/abs-2302-09632",
                    "DOI": "10.48550/arXiv.2302.09632",
                    "CorpusId": 257038997
                },
                "corpusId": 257038997,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b835345c6168d7b179516700aa4460912a8857e9",
                "title": "HomoDistil: Homotopic Task-Agnostic Distillation of Pre-trained Transformers",
                "abstract": "Knowledge distillation has been shown to be a powerful model compression approach to facilitate the deployment of pre-trained language models in practice. This paper focuses on task-agnostic distillation. It produces a compact pre-trained model that can be easily fine-tuned on various tasks with small computational costs and memory footprints. Despite the practical benefits, task-agnostic distillation is challenging. Since the teacher model has a significantly larger capacity and stronger representation power than the student model, it is very difficult for the student to produce predictions that match the teacher's over a massive amount of open-domain training data. Such a large prediction discrepancy often diminishes the benefits of knowledge distillation. To address this challenge, we propose Homotopic Distillation (HomoDistil), a novel task-agnostic distillation approach equipped with iterative pruning. Specifically, we initialize the student model from the teacher model, and iteratively prune the student's neurons until the target width is reached. Such an approach maintains a small discrepancy between the teacher's and student's predictions throughout the distillation process, which ensures the effectiveness of knowledge transfer. Extensive experiments demonstrate that HomoDistil achieves significant improvements on existing baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "98703980",
                        "name": "Chen Liang"
                    },
                    {
                        "authorId": "5795999",
                        "name": "Haoming Jiang"
                    },
                    {
                        "authorId": "2146249169",
                        "name": "Zheng Li"
                    },
                    {
                        "authorId": "48784944",
                        "name": "Xianfeng Tang"
                    },
                    {
                        "authorId": "2208693304",
                        "name": "Bin Yin"
                    },
                    {
                        "authorId": "2153707398",
                        "name": "Tuo Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Unfortunately, coarse-grained sparsity suffers severe performance drops at sparsity levels higher than 50% due to the flexibility constraint on network sparsity (Renda et al., 2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4477998ffdb78df97eeaf8463179e6c240147fa7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-06058",
                    "ArXiv": "2302.06058",
                    "DOI": "10.48550/arXiv.2302.06058",
                    "CorpusId": 256827025
                },
                "corpusId": 256827025,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4477998ffdb78df97eeaf8463179e6c240147fa7",
                "title": "Bi-directional Masks for Efficient N: M Sparse Training",
                "abstract": "We focus on addressing the dense backward propagation issue for training efficiency of N:M fine-grained sparsity that preserves at most N out of M consecutive weights and achieves practical speedups supported by the N:M sparse tensor core. Therefore, we present a novel method of Bi-directional Masks (Bi-Mask) with its two central innovations in: 1) Separate sparse masks in the two directions of forward and backward propagation to obtain training acceleration. It disentangles the forward and backward weight sparsity and overcomes the very dense gradient computation. 2) An efficient weight row permutation method to maintain performance. It picks up the permutation candidate with the most eligible N:M weight blocks in the backward to minimize the gradient gap between traditional uni-directional masks and our bi-directional masks. Compared with existing uni-directional scenario that applies a transposable mask and enables backward acceleration, our Bi-Mask is experimentally demonstrated to be more superior in performance. Also, our Bi-Mask performs on par with or even better than methods that fail to achieve backward acceleration. Project of this paper is available at \\url{https://github.com/zyxxmu/Bi-Mask}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108078624",
                        "name": "Yu-xin Zhang"
                    },
                    {
                        "authorId": "1753623782",
                        "name": "Yiting Luo"
                    },
                    {
                        "authorId": "49352079",
                        "name": "Mingbao Lin"
                    },
                    {
                        "authorId": "2112888098",
                        "name": "Yunshan Zhong"
                    },
                    {
                        "authorId": "2205729484",
                        "name": "Jingjing Xie"
                    },
                    {
                        "authorId": "2070266415",
                        "name": "Fei Chao"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4117f8b1aee5907cb8c0907f3cffbb11b27f28e0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-05601",
                    "ArXiv": "2302.05601",
                    "DOI": "10.48550/arXiv.2302.05601",
                    "CorpusId": 256827141
                },
                "corpusId": 256827141,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4117f8b1aee5907cb8c0907f3cffbb11b27f28e0",
                "title": "Pruning Deep Neural Networks from a Sparsity Perspective",
                "abstract": "In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may under-prune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a limit that appears to correspond to the beginning of underfitting. Subsequently, PQI decreases again when the model collapse and significant deterioration in the performance of the model start to occur. Additionally, our experiments demonstrate that the proposed adaptive pruning algorithm with proper choice of hyper-parameters is superior to the iterative pruning algorithms such as the lottery ticket-based pruning methods, in terms of both compression efficiency and robustness.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51298945",
                        "name": "Enmao Diao"
                    },
                    {
                        "authorId": "2096527",
                        "name": "G. Wang"
                    },
                    {
                        "authorId": "14895949",
                        "name": "Jiawei Zhan"
                    },
                    {
                        "authorId": "2168770330",
                        "name": "Yuhong Yang"
                    },
                    {
                        "authorId": "143798670",
                        "name": "Jie Ding"
                    },
                    {
                        "authorId": "1780864",
                        "name": "V. Tarokh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "LTH has since been empowered by weight/learning rate rewinding (Frankle et al. 2020; Renda et al. 2020), and the existence of LTH has been verified in various applications, showing the almost universal intrinsic sparsity in overparameterized networks (Chen et al. 2020, 2021c,b).",
                "LTH has since been empowered by weight/learning rate rewinding (Frankle et al. 2020; Renda et al. 2020), and the existence of LTH has been verified in various applications, showing the almost universal intrinsic sparsity in overparameterized networks (Chen et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "712573cc74633ec2283724e868328fd2d319c091",
                "externalIds": {
                    "ArXiv": "2302.02596",
                    "DBLP": "journals/corr/abs-2302-02596",
                    "DOI": "10.48550/arXiv.2302.02596",
                    "CorpusId": 256615905
                },
                "corpusId": 256615905,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/712573cc74633ec2283724e868328fd2d319c091",
                "title": "Ten Lessons We Have Learned in the New \"Sparseland\": A Short Handbook for Sparse Neural Network Researchers",
                "abstract": "This article does not propose any novel algorithm or new hardware for sparsity. Instead, it aims to serve the\"common good\"for the increasingly prosperous Sparse Neural Network (SNN) research community. We attempt to summarize some most common confusions in SNNs, that one may come across in various scenarios such as paper review/rebuttal and talks - many drawn from the authors' own bittersweet experiences! We feel that doing so is meaningful and timely, since the focus of SNN research is notably shifting from traditional pruning to more diverse and profound forms of sparsity before, during, and after training. The intricate relationships between their scopes, assumptions, and approaches lead to misunderstandings, for non-experts or even experts in SNNs. In response, we summarize ten Q\\&As of SNNs from many key aspects, including dense vs. sparse, unstructured sparse vs. structured sparse, pruning vs. sparse training, dense-to-sparse training vs. sparse-to-sparse training, static sparsity vs. dynamic sparsity, before-training/during-training vs. post-training sparsity, and many more. We strive to provide proper and generically applicable answers to clarify those confusions to the best extent possible. We hope our summary provides useful general knowledge for people who want to enter and engage with this exciting community; and also provides some\"mind of ease\"convenience for SNN researchers to explain their work in the right contexts. At the very least (and perhaps as this article's most insignificant target functionality), if you are writing/planning to write a paper or rebuttal in the field of SNNs, we hope some of our answers could help you!",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the literature, The design space of pruning algorithms encompasses a range of aspects, including pruning schemes [21, 39], parameter selection [20, 43, 44], layer sparsity [27, 49] and training techniques [47, 58]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "da075ad0ec2c88335af85602a76a33e034536896",
                "externalIds": {
                    "DBLP": "conf/cvpr/FangMSMW23",
                    "ArXiv": "2301.12900",
                    "DOI": "10.1109/CVPR52729.2023.01544",
                    "CorpusId": 256390345
                },
                "corpusId": 256390345,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/da075ad0ec2c88335af85602a76a33e034536896",
                "title": "DepGraph: Towards Any Structural Pruning",
                "abstract": "Structural pruning enables model acceleration by removing structurally-grouped parameters from neural networks. However, the parameter-grouping patterns vary widely across different models, making architecture-specific pruners, which rely on manually-designed grouping schemes, non-generalizable to new architectures. In this work, we study a highly-challenging yet barely-explored task, any structural pruning, to tackle general structural pruning of arbitrary architecture like CNNs, RNNs, GNNs and Transformers. The most prominent obstacle towards this goal lies in the structural coupling, which not only forces different layers to be pruned simultaneously, but also expects all removed parameters to be consistently unimportant, thereby avoiding structural issues and significant performance degradation after pruning. To address this problem, we propose a general and fully automatic method, Dependency Graph (DepGraph), to explicitly model the dependency between layers and comprehensively group coupled parameters for pruning. In this work, we extensively evaluate our method on several architectures and tasks, including ResNe(X)t, DenseNet, MobileNet and Vision transformer for images, GAT for graph, DGCNN for 3D point cloud, alongside LSTM for language, and demonstrate that, even with a simple norm-based criterion, the proposed method consistently yields gratifying performances.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "150110431",
                        "name": "Gongfan Fang"
                    },
                    {
                        "authorId": "15532066",
                        "name": "Xinyin Ma"
                    },
                    {
                        "authorId": "2152127912",
                        "name": "Mingli Song"
                    },
                    {
                        "authorId": "153122790",
                        "name": "M. B. Mi"
                    },
                    {
                        "authorId": "48631088",
                        "name": "Xinchao Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c714f39c410eb934565b398a829edac0b6058728",
                "externalIds": {
                    "ArXiv": "2301.10835",
                    "DBLP": "journals/corr/abs-2301-10835",
                    "DOI": "10.48550/arXiv.2301.10835",
                    "CorpusId": 256274507
                },
                "corpusId": 256274507,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c714f39c410eb934565b398a829edac0b6058728",
                "title": "When Layers Play the Lottery, all Tickets Win at Initialization",
                "abstract": "Pruning is a standard technique for reducing the computational cost of deep networks. Many advances in pruning leverage concepts from the Lottery Ticket Hypothesis (LTH). LTH reveals that inside a trained dense network exists sparse subnetworks (tickets) able to achieve similar accuracy (i.e., win the lottery - winning tickets). Pruning at initialization focuses on finding winning tickets without training a dense network. Studies on these concepts share the trend that subnetworks come from weight or filter pruning. In this work, we investigate LTH and pruning at initialization from the lens of layer pruning. First, we confirm the existence of winning tickets when the pruning process removes layers. Leveraged by this observation, we propose to discover these winning tickets at initialization, eliminating the requirement of heavy computational resources for training the initial (over-parameterized) dense network. Extensive experiments show that our winning tickets notably speed up the training phase and reduce up to 51% of carbon emission, an important step towards democratization and green Artificial Intelligence. Beyond computational benefits, our winning tickets exhibit robustness against adversarial and out-of-distribution examples. Finally, we show that our subnetworks easily win the lottery at initialization while tickets from filter removal (the standard structured LTH) hardly become winning tickets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145311611",
                        "name": "Artur Jord\u00e3o"
                    },
                    {
                        "authorId": "2202701409",
                        "name": "George Correa de Araujo"
                    },
                    {
                        "authorId": "8125221",
                        "name": "H. Maia"
                    },
                    {
                        "authorId": "2059195394",
                        "name": "H\u00e9lio Pedrini"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1825493cc6a12c1a509b03593991653ff3c76c49",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-05219",
                    "ArXiv": "2301.05219",
                    "DOI": "10.48550/arXiv.2301.05219",
                    "CorpusId": 255749315
                },
                "corpusId": 255749315,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1825493cc6a12c1a509b03593991653ff3c76c49",
                "title": "Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning",
                "abstract": "The state of neural network pruning has been noticed to be unclear and even confusing for a while, largely due to\"a lack of standardized benchmarks and metrics\"[3]. To standardize benchmarks, first, we need to answer: what kind of comparison setup is considered fair? This basic yet crucial question has barely been clarified in the community, unfortunately. Meanwhile, we observe several papers have used (severely) sub-optimal hyper-parameters in pruning experiments, while the reason behind them is also elusive. These sub-optimal hyper-parameters further exacerbate the distorted benchmarks, rendering the state of neural network pruning even more obscure. Two mysteries in pruning represent such a confusing status: the performance-boosting effect of a larger finetuning learning rate, and the no-value argument of inheriting pretrained weights in filter pruning. In this work, we attempt to explain the confusing state of network pruning by demystifying the two mysteries. Specifically, (1) we first clarify the fairness principle in pruning experiments and summarize the widely-used comparison setups; (2) then we unveil the two pruning mysteries and point out the central role of network trainability, which has not been well recognized so far; (3) finally, we conclude the paper and give some concrete suggestions regarding how to calibrate the pruning benchmarks in the future. Code: https://github.com/mingsun-tse/why-the-state-of-pruning-so-confusing.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2113269100",
                        "name": "Huan Wang"
                    },
                    {
                        "authorId": "12282768",
                        "name": "Can Qin"
                    },
                    {
                        "authorId": "153802755",
                        "name": "Yue Bai"
                    },
                    {
                        "authorId": "2156255943",
                        "name": "Yun Fu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the literature, there have also been several studies, for example investigating whether rewinding (training from scratch with a fixed mask) can perform just as well as the fine-tuning on top of the original unpruned network (Renda et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4f4991f93ed86b777c8b0f192dac034a3144b165",
                "externalIds": {
                    "ArXiv": "2301.04502",
                    "DBLP": "journals/corr/abs-2301-04502",
                    "DOI": "10.48550/arXiv.2301.04502",
                    "CorpusId": 255595834
                },
                "corpusId": 255595834,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4f4991f93ed86b777c8b0f192dac034a3144b165",
                "title": "Pruning Compact ConvNets for Efficient Inference",
                "abstract": "Neural network pruning is frequently used to compress over-parameterized networks by large amounts, while incurring only marginal drops in generalization performance. However, the impact of pruning on networks that have been highly optimized for efficient inference has not received the same level of attention. In this paper, we analyze the effect of pruning for computer vision, and study state-of-the-art ConvNets, such as the FBNetV3 family of models. We show that model pruning approaches can be used to further optimize networks trained through NAS (Neural Architecture Search). The resulting family of pruned models can consistently obtain better performance than existing FBNetV3 models at the same level of computation, and thus provide state-of-the-art results when trading off between computational complexity and generalization performance on the ImageNet benchmark. In addition to better generalization performance, we also demonstrate that when limited computation resources are available, pruning FBNetV3 models incur only a fraction of GPU-hours involved in running a full-scale NAS.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143032877",
                        "name": "Sayan Ghosh"
                    },
                    {
                        "authorId": "2107060033",
                        "name": "Karthik Prasad"
                    },
                    {
                        "authorId": "4527324",
                        "name": "Xiaoliang Dai"
                    },
                    {
                        "authorId": "2918780",
                        "name": "Peizhao Zhang"
                    },
                    {
                        "authorId": "3130257",
                        "name": "Bichen Wu"
                    },
                    {
                        "authorId": "1709589",
                        "name": "Graham Cormode"
                    },
                    {
                        "authorId": "48682997",
                        "name": "P\u00e9ter Vajda"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, empirical studies have shown that unstructured pruning often yields much better results than structured [35].",
                "In structured pruning, weights are pruned in groups by removing whole neurons or entire channels [35, 23]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0092b76176dbd19527f595c2cf1cea8e1126be9c",
                "externalIds": {
                    "ArXiv": "2308.06780",
                    "DBLP": "conf/comad/IqbalM23",
                    "DOI": "10.1145/3570991.3570997",
                    "CorpusId": 255416733
                },
                "corpusId": 255416733,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0092b76176dbd19527f595c2cf1cea8e1126be9c",
                "title": "Neural Networks at a Fraction with Pruned Quaternions",
                "abstract": "Contemporary state-of-the-art neural networks have increasingly large numbers of parameters, which prevents their deployment on devices with limited computational power. Pruning is one technique to remove unnecessary weights and reduce resource requirements for training and inference. In addition, for ML tasks where the input data is multi-dimensional, using higher-dimensional data embeddings such as complex numbers or quaternions has been shown to reduce the parameter count while maintaining accuracy. In this work, we conduct pruning on real and quaternion-valued implementations of different architectures on classification tasks. We find that for some architectures, at very high sparsity levels, quaternion models provide higher accuracies than their real counterparts. For example, at the task of image classification on CIFAR-10 using Conv-4, at of the number of parameters as the original model, the pruned quaternion version outperforms the pruned real by more than . Experiments on various network architectures and datasets show that for deployment in extremely resource-constrained environments, a sparse quaternion network might be a better candidate than a real sparse model of similar architecture.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2199253514",
                        "name": "Sahel Mohammad Iqbal"
                    },
                    {
                        "authorId": "2199255897",
                        "name": "Subhankar Mishra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The LTH has been well applied in different domains [12]\u2013 [15]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fd529dc803b31987f543a5d6de389e52315abaeb",
                "externalIds": {
                    "DBLP": "conf/bigdataconf/HarnYHZSSK22",
                    "DOI": "10.1109/BigData55660.2022.10020964",
                    "CorpusId": 256322331
                },
                "corpusId": 256322331,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fd529dc803b31987f543a5d6de389e52315abaeb",
                "title": "IGRP: Iterative Gradient Rank Pruning for Finding Graph Lottery Ticket",
                "abstract": "Graph Neural Networks (GNNs) have shown promising performance in many applications, yet remain extremely difficult to train over large-scale graph datasets. Existing weight pruning techniques can prune out the layer weights; however, they cannot fully address the high computation complexity of GNN inference, caused by large graph size and complicated node connections. In this paper, we propose an Iterative Gradient Rank Pruning (IGRP) algorithm to find graph lottery tickets (GLT) of GNNs where each GLT includes a pruned adjacency matrix and a sub-network. Our IGRP can avoid layer collapse and the winning ticket achieves Maximal critical compression. We evaluate the proposed method on small-scale (Cora and Citeseer), medium-scale (PubMed and Wiki-CS), and large-scale (Ogbn-ArXiv and Ogbn-Products) graph datasets. We demonstrate that both Single-shot and Multi-shot of IGRP outperform the state-of-the-art unified GNN sparsification (UGS) framework on node classification. The source code can be found in https://github.com/poweiharn/IGRP_GNN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7444800",
                        "name": "P. Harn"
                    },
                    {
                        "authorId": "2203118892",
                        "name": "Sai Deepthi Yeddula"
                    },
                    {
                        "authorId": "2064059011",
                        "name": "Bo Hui"
                    },
                    {
                        "authorId": "40539618",
                        "name": "J. Zhang"
                    },
                    {
                        "authorId": "2203154091",
                        "name": "Libo Sun"
                    },
                    {
                        "authorId": "1717206",
                        "name": "Min-Te Sun"
                    },
                    {
                        "authorId": "1758909",
                        "name": "Wei-Shinn Ku"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5b993855e5452e3a70fd7ff0790d8fb96f7cdc01",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-08066",
                    "ArXiv": "2212.08066",
                    "DOI": "10.48550/arXiv.2212.08066",
                    "CorpusId": 263793067
                },
                "corpusId": 263793067,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5b993855e5452e3a70fd7ff0790d8fb96f7cdc01",
                "title": "Mod-Squad: Designing Mixture of Experts As Modular Multi-Task Learners",
                "abstract": "Optimization in multi-task learning (MTL) is more challenging than single-task learning (STL), as the gradient from different tasks can be contradictory. When tasks are related, it can be beneficial to share some parameters among them (cooperation). However, some tasks require additional parameters with expertise in a specific type of data or discrimination (specialization). To address the MTL challenge, we propose Mod-Squad, a new model that is Modularized into groups of experts (a 'Squad'). This structure allows us to formalize cooperation and specialization as the process of matching experts and tasks. We optimize this matching process during the training of a single model. Specifically, we incorporate mixture of experts (MoE) layers into a transformer model, with a new loss that incorporates the mutual dependence between tasks and experts. As a result, only a small set of experts are activated for each task. This prevents the sharing of the entire backbone model between all tasks, which strengthens the model, especially when the training set size and the number of tasks scale up. More interestingly, for each task, we can extract the small set of experts as a standalone model that maintains the same performance as the large model. Extensive experiments on the Taskonomy dataset with 13 vision tasks and the PASCAL-Context dataset with 5 vision tasks show the superiority of our approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2256748673",
                        "name": "Zitian Chen"
                    },
                    {
                        "authorId": "2714199",
                        "name": "Yikang Shen"
                    },
                    {
                        "authorId": "2256679206",
                        "name": "Mingyu Ding"
                    },
                    {
                        "authorId": "2111329651",
                        "name": "Zhenfang Chen"
                    },
                    {
                        "authorId": "2256791148",
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "authorId": "2256702025",
                        "name": "Erik G. Learned-Miller"
                    },
                    {
                        "authorId": "2248474897",
                        "name": "Chuang Gan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several state-of-the-art pruning methods (Renda et al., 2019; Frankle & Carbin, 2019) have demonstrated that a significant quantity of parameters can be removed without sacrificing accuracy.",
                "reasons: (i) Iterative pruning tends to provide better pruning performance than one-shot pruning as reported in the literature (Frankle & Carbin, 2019; Renda et al., 2019).",
                "Lastly, we highlight that all LR schedules used, including SILO, are rewound to the initial state at the beginning of each pruning cycle, which is the same as the LR rewinding in (Renda et al., 2019).",
                "Several recent works (Renda et al., 2019; Frankle & Carbin, 2019) have noticed the important role of LR in network pruning.",
                "Some follow-on works (Zhou et al., 2019; Renda et al., 2019; Malach et al., 2020) investigated this phenomenon more precisely and applied this method in other fields (e.",
                "9 and a weight decay of 1e-4 (same as (Renda et al., 2019; Frankle & Carbin, 2019))."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2e09a43f15dfe8068283207f37795f2e918bbe15",
                "externalIds": {
                    "ArXiv": "2212.06144",
                    "DBLP": "journals/corr/abs-2212-06144",
                    "DOI": "10.48550/arXiv.2212.06144",
                    "CorpusId": 254591563
                },
                "corpusId": 254591563,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2e09a43f15dfe8068283207f37795f2e918bbe15",
                "title": "Optimizing Learning Rate Schedules for Iterative Pruning of Deep Neural Networks",
                "abstract": "The importance of learning rate (LR) schedules on network pruning has been observed in a few recent works. As an example, Frankle and Carbin (2019) highlighted that winning tickets (i.e., accuracy preserving subnetworks) can not be found without applying a LR warmup schedule and Renda, Frankle and Carbin (2020) demonstrated that rewinding the LR to its initial state at the end of each pruning cycle improves performance. In this paper, we go one step further by first providing a theoretical justification for the surprising effect of LR schedules. Next, we propose a LR schedule for network pruning called SILO, which stands for S-shaped Improved Learning rate Optimization. The advantages of SILO over existing state-of-the-art (SOTA) LR schedules are two-fold: (i) SILO has a strong theoretical motivation and dynamically adjusts the LR during pruning to improve generalization. Specifically, SILO increases the LR upper bound (max_lr) in an S-shape. This leads to an improvement of 2% - 4% in extensive experiments with various types of networks (e.g., Vision Transformers, ResNet) on popular datasets such as ImageNet, CIFAR-10/100. (ii) In addition to the strong theoretical motivation, SILO is empirically optimal in the sense of matching an Oracle, which exhaustively searches for the optimal value of max_lr via grid search. We find that SILO is able to precisely adjust the value of max_lr to be within the Oracle optimized interval, resulting in performance competitive with the Oracle with significantly lower complexity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50151902",
                        "name": "Shiyu Liu"
                    },
                    {
                        "authorId": "2978590",
                        "name": "Rohan Ghosh"
                    },
                    {
                        "authorId": "2176001260",
                        "name": "John Tan Chong Min"
                    },
                    {
                        "authorId": "1770486",
                        "name": "M. Motani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some follow-on works (Zhou et al. 2019; Renda, Frankle, and Carbin 2019; Malach et al. 2020) investigated this phenomenon more precisely and applied this method in other fields (e.g., transfer learning (Mehta 2019), reinforcement learning and natural language processing (Yu et al. 2020)).",
                "We refer to the standard implementation reported in (Renda, Frankle, and Carbin 2019; Frankle and Carbin 2019) (i.e., SGD optimizer (Ruder 2016), 100 training epochs and batch size of 128, learning rate warmup to 0.03 and drop by a factor of 10 at 55 and 70 epochs) and compute the static DNR and\u2026",
                "A recent work (Renda, Frankle, and Carbin 2019) proposed learning rate rewinding which used the same learning rate schedule to retrain the pruned network, leading to a better\npruning performance.",
                "21: end if\nWe train the network using SGD with momentum = 0.9 and a weight decay of 1e-4 (same as (Renda, Frankle, and Carbin 2019; Frankle and Carbin 2019)).",
                "Specifically, the implementations for Tables 1 - 6 are from (Frankle and Carbin 2019), (Zhao et al. 2019), (Chin et al. 2020), (Renda, Frankle, and Carbin 2019) and (Dosovitskiy et al. 2020).",
                "The implementation for Table 10 - 13 are from (Zhao et al. 2019), (Chin et al. 2020), (Renda, Frankle, and Carbin 2019) and (Dosovitskiy et al. 2020), respectively."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "39b233ee818e9e5ae72b1bfcb2f38e4b92decdc4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-06145",
                    "ArXiv": "2212.06145",
                    "DOI": "10.48550/arXiv.2212.06145",
                    "CorpusId": 254591743
                },
                "corpusId": 254591743,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/39b233ee818e9e5ae72b1bfcb2f38e4b92decdc4",
                "title": "AP: Selective Activation for De-sparsifying Pruned Neural Networks",
                "abstract": "The rectified linear unit (ReLU) is a highly successful activation function in neural networks as it allows networks to easily obtain sparse representations, which reduces overfitting in overparameterized networks. However, in network pruning, we find that the sparsity introduced by ReLU, which we quantify by a term called dynamic dead neuron rate (DNR), is not beneficial for the pruned network. Interestingly, the more the network is pruned, the smaller the dynamic DNR becomes during optimization. This motivates us to propose a method to explicitly reduce the dynamic DNR for the pruned network, i.e., de-sparsify the network. We refer to our method as Activating-while-Pruning (AP). We note that AP does not function as a stand-alone method, as it does not evaluate the importance of weights. Instead, it works in tandem with existing pruning methods and aims to improve their performance by selective activation of nodes to reduce the dynamic DNR. We conduct extensive experiments using popular networks (e.g., ResNet, VGG) via two classical and three state-of-the-art pruning methods. The experimental results on public datasets (e.g., CIFAR-10/100) suggest that AP works well with existing pruning methods and improves the performance by 3% - 4%. For larger scale datasets (e.g., ImageNet) and state-of-the-art networks (e.g., vision transformer), we observe an improvement of 2% - 3% with AP as opposed to without. Lastly, we conduct an ablation study to examine the effectiveness of the components comprising AP.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50151902",
                        "name": "Shiyu Liu"
                    },
                    {
                        "authorId": "2978590",
                        "name": "Rohan Ghosh"
                    },
                    {
                        "authorId": "2195339195",
                        "name": "Dylan Tan"
                    },
                    {
                        "authorId": "1770486",
                        "name": "M. Motani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Most authors [6, 5, 32, 18] agree on the fact that: (i) the pruning should not start at the beginning of training, due to the heavy changes a model undergoes in the early stage of training; and (ii) the increase of sparsity should be progressive, to limit the interference between weight update and weight zeroing.",
                "To limit these computations, sparse networks have been thoroughly investigated in the past few years [43, 4, 27, 41, 21, 38, 3, 6, 32, 22, 42, 25], and significant efforts have been devoted to their efficient hardware implementation [8, 29].",
                "This is in contrast with the iterative post-training pruning solutions [6, 32], which achieve SoA accuracy/sparsity trade-off, but suffer from an extremely large computation cost due to the multiple rounds of training required to progressively increase the pruning ratio.",
                "As a recognized SoA upper bound in terms of accuracy/sparsity trade-off, the Learning-Rate-Rewind (LRR) recursive pruning method is considered [32].",
                "Doing so, SoA accuracy are obtained, but the training cost largely grows with the sparsity ratio [6, 32].",
                "Another convincing experimental argument in favor of our method lies in the fact that, when combined with iterative and thus quite complex (due to multiple rounds of training cycles) post-training pruning solutions proposed in [6, 32], our ST-3 defines a novel SoA accuracy/sparsity trade-off.",
                "This aspect is generally overlooked in previous work [6, 4, 32, 20], where only the model compression is considered, disregarding the number of operations affected by the zeroed weights."
            ],
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "e2a06471cd317ff426c1cc79c0139cd12f699f00",
                "externalIds": {
                    "DBLP": "conf/wacv/VanderschuerenV23",
                    "ArXiv": "2212.01076",
                    "DOI": "10.1109/WACV56688.2023.00380",
                    "CorpusId": 254221223
                },
                "corpusId": 254221223,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/e2a06471cd317ff426c1cc79c0139cd12f699f00",
                "title": "Are Straight-Through gradients and Soft-Thresholding all you need for Sparse Training?",
                "abstract": "Turning the weights to zero when training a neural network helps in reducing the computational complexity at inference. To progressively increase the sparsity ratio in the network without causing sharp weight discontinuities during training, our work combines soft-thresholding and straight-through gradient estimation to update the raw, i.e. non-thresholded, version of zeroed weights. Our method, named ST-3 for straight-through/soft-thresholding/sparse-training2, obtains SoA results, both in terms of accuracy/sparsity and accuracy/FLOPS trade-offs, when progressively increasing the sparsity ratio in a single training cycle. In particular, despite its simplicity, ST-3 favorably compares to the most recent methods, adopting differentiable formulations [42] or bio-inspired neuroregeneration principles [25]. This suggests that the key ingredients for effective sparsification primarily lie in the ability to give the weights the freedom to evolve smoothly across the zero state while progressively increasing the sparsity ratio.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "98683669",
                        "name": "A. Vanderschueren"
                    },
                    {
                        "authorId": "1719942",
                        "name": "C. Vleeschouwer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026may reduce memory footprint and improve computational efficiency of model training or inference (e.g., [Han et al., 2015, Frankle and Carbin, 2019, Renda et al., 2020, Blalock et al., 2020, Lebedev and Lempitsky, 2016, Molchanov et al., 2017, Dong et al., 2017, Yu et al., 2018, Baykal et al.,\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "af7ced1c8e0713c320029444d174fde4bd682a68",
                "externalIds": {
                    "ArXiv": "2212.00291",
                    "DBLP": "journals/corr/abs-2212-00291",
                    "DOI": "10.48550/arXiv.2212.00291",
                    "CorpusId": 254125049
                },
                "corpusId": 254125049,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/af7ced1c8e0713c320029444d174fde4bd682a68",
                "title": "The Effect of Data Dimensionality on Neural Network Prunability",
                "abstract": "Practitioners prune neural networks for efficiency gains and generalization improvements, but few scrutinize the factors determining the prunability of a neural network the maximum fraction of weights that pruning can remove without compromising the model's test accuracy. In this work, we study the properties of input data that may contribute to the prunability of a neural network. For high dimensional input data such as images, text, and audio, the manifold hypothesis suggests that these high dimensional inputs approximately lie on or near a significantly lower dimensional manifold. Prior work demonstrates that the underlying low dimensional structure of the input data may affect the sample efficiency of learning. In this paper, we investigate whether the low dimensional structure of the input data affects the prunability of a neural network.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2172356226",
                        "name": "Zachary Ankner"
                    },
                    {
                        "authorId": "26433838",
                        "name": "Alex Renda"
                    },
                    {
                        "authorId": "2533850",
                        "name": "G. Dziugaite"
                    },
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "2188780740",
                        "name": "Tian Jin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "94d8e29580b6fc29c9ffbee88d49401e0e2da1b5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-16667",
                    "ArXiv": "2211.16667",
                    "DOI": "10.1109/DAC56929.2023.10247716",
                    "CorpusId": 254096500
                },
                "corpusId": 254096500,
                "publicationVenue": {
                    "id": "021b37d3-cef1-4c12-a442-257f7900c23d",
                    "name": "Design Automation Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Des Autom Conf",
                        "DAC"
                    ],
                    "url": "http://www.dac.com/"
                },
                "url": "https://www.semanticscholar.org/paper/94d8e29580b6fc29c9ffbee88d49401e0e2da1b5",
                "title": "Dynamic Sparse Training via Balancing the Exploration-Exploitation Trade-off",
                "abstract": "Over-parameterization of deep neural networks (DNNs) has shown high prediction accuracy for many applications. Although effective, the large number of parameters hinders its popularity on resource-limited devices and has an outsize environmental impact. Sparse training (using a fixed number of nonzero weights in each iteration) could significantly mitigate the training costs by reducing the model size. However, existing sparse training methods mainly use either random-based or greedy-based drop-and-grow strategies, resulting in local minimal and low accuracy. In this work, to assist explainable sparse training, we propose important weights Exploitation and coverage Exploration to characterize Dynamic Sparse Training (DST-EE), and provide quantitative analysis of these two metrics. We further design an acquisition function and provide the theoretical guarantees for the proposed method and clarify its convergence property. Experimental results show that sparse models (up to 98% sparsity) obtained by our proposed method outperform the SOTA sparse training methods on a wide variety of deep learning tasks. On VGG-19 / CIFAR-100, ResNet-50 / CIFAR-10, ResNet-50 / CIFAR-100, our method has even higher accuracy than dense models. On ResNet-50 / ImageNet, the proposed method has up to 8.2% accuracy improvement compared to SOTA sparse training methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2122804649",
                        "name": "Shaoyi Huang"
                    },
                    {
                        "authorId": "2144399315",
                        "name": "Bowen Lei"
                    },
                    {
                        "authorId": "2116459424",
                        "name": "Dongkuan Xu"
                    },
                    {
                        "authorId": "144490597",
                        "name": "Hongwu Peng"
                    },
                    {
                        "authorId": "2116969722",
                        "name": "Yue Sun"
                    },
                    {
                        "authorId": "3197711",
                        "name": "Mimi Xie"
                    },
                    {
                        "authorId": "2881873",
                        "name": "Caiwen Ding"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Second, the NN-based learning permits the application of complexity reduction through network pruning [41], which is more fine-grained than discarding triplets."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "20cbba99a7578f14647c55413372564bc0844ce1",
                "externalIds": {
                    "ArXiv": "2211.10789",
                    "DOI": "10.1109/JLT.2023.3279449",
                    "CorpusId": 253735111
                },
                "corpusId": 253735111,
                "publicationVenue": {
                    "id": "a1f606c0-a59f-4771-bdb9-aac3a2ca72eb",
                    "name": "Journal of Lightwave Technology",
                    "type": "journal",
                    "alternate_names": [
                        "J Light Technol"
                    ],
                    "issn": "0733-8724",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=50",
                    "alternate_urls": [
                        "http://jlt.osa.org/journal/JLT/about.cfm",
                        "http://www.opticsinfobase.org/jlt/journal/JLT/about.cfm",
                        "http://jlt.osa.org/issue.cfm"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/20cbba99a7578f14647c55413372564bc0844ce1",
                "title": "Deep Learning-Aided Perturbation Model-Based Fiber Nonlinearity Compensation",
                "abstract": "Fiber nonlinearity effects cap achievable rates and ranges in long-haul optical fiber communication links. Conventional nonlinearity compensation methods, such as perturbation theory-based nonlinearity compensation (PB-NLC), attempt to compensate for the nonlinearity by approximating analytical solutions to the signal propagation over optical fibers. However, their practical usability is limited by model mismatch and the immense computational complexity associated with the analytical computation of perturbation triplets and the nonlinearity distortion field. Recently, machine learning techniques have been used to optimise parameters of PB-based approaches, which traditionally have been determined analytically from physical models. It has been claimed in the literature that the learned PB-NLC approaches have improved performance and/or reduced computational complexity over their non-learned counterparts. In this paper, we first revisit the acclaimed benefits of the learned PB-NLC approaches by carefully carrying out a comprehensive performance-complexity analysis utilizing state-of-the-art complexity reduction methods. Interestingly, our results show that least squares-based PB-NLC with clustering quantization has the best performance-complexity trade-off among the learned PB-NLC approaches. Second, we advance the state-of-the-art of learned PB-NLC by proposing and designing a fully learned structure by adopting the noiseless Manakov equation as the channel propagation model. We apply a bi-directional recurrent neural network for learning perturbation triplets that are alike those obtained from the analytical computation and are used as input features for the neural network to estimate the nonlinearity distortion field. Finally, we demonstrate through numerical simulations that our proposed fully learned approach achieves an improved performance-complexity trade-off compared to the existing learned and non-learned PB-NLC techniques.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2098204832",
                        "name": "S. Luo"
                    },
                    {
                        "authorId": "2000986360",
                        "name": "Sunish Kumar Orappanpara Soman"
                    },
                    {
                        "authorId": "35699680",
                        "name": "L. Lampe"
                    },
                    {
                        "authorId": "3252345",
                        "name": "J. Mitra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Pre-trained language models (PLMs) have achieved great success in NLP (Devlin et al., 2019a), but they are vulnerable to adversarial examples crafted by performing subtle perturbations on normal examples (Ren et al., 2019; Garg and Ramakrishnan, 2020).",
                "In NLP, previous work has found that matching subnetworks exist in Transformers, LSTMs, and PLMs (Yu et al., 2020; Renda et al., 2020; Chen et al., 2020).",
                "RobusT (Zheng et al., 2022b): An approach that identifies robust tickets from the original PLMs through learning binary masks.",
                "In previous work, robust tickets with unstructured sparsity in PLMs are extracted after a tedious training process."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "3f91227c2a5b08815569b4c420883c56dd4833ab",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-07263",
                    "ACL": "2022.emnlp-main.569",
                    "ArXiv": "2211.07263",
                    "DOI": "10.48550/arXiv.2211.07263",
                    "CorpusId": 253510521
                },
                "corpusId": 253510521,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/3f91227c2a5b08815569b4c420883c56dd4833ab",
                "title": "Efficient Adversarial Training with Robust Early-Bird Tickets",
                "abstract": "Adversarial training is one of the most powerful methods to improve the robustness of pre-trained language models (PLMs). However, this approach is typically more expensive than traditional fine-tuning because of the necessity to generate adversarial examples via gradient descent. Delving into the optimization process of adversarial training, we find that robust connectivity patterns emerge in the early training phase (typically 0.15~0.3 epochs), far before parameters converge. Inspired by this finding, we dig out robust early-bird tickets (i.e., subnetworks) to develop an efficient adversarial training method: (1) searching for robust tickets with structured sparsity in the early stage; (2) fine-tuning robust tickets in the remaining time. To extract the robust tickets as early as possible, we design a ticket convergence metric to automatically terminate the searching process. Experiments show that the proposed efficient adversarial training method can achieve up to 7\\times \\sim 13 \\times training speedups while maintaining comparable or even better robustness compared to the most competitive state-of-the-art adversarial training methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2190751523",
                        "name": "Zhiheng Xi"
                    },
                    {
                        "authorId": "2058585152",
                        "name": "Rui Zheng"
                    },
                    {
                        "authorId": "2067331064",
                        "name": "Tao Gui"
                    },
                    {
                        "authorId": "47835189",
                        "name": "Qi Zhang"
                    },
                    {
                        "authorId": "1790227",
                        "name": "Xuanjing Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "When the preset pruning-rate is too small, the weights of the network change slightly, and the similar structure may be utilized by using the same learning rate to retrain the network, makes the network converge faster to the early-stop point with the similar performance as the original network [26].",
                "Some researchers have compared various strategies based on LTH and one effective scheme is based on learning rate rewinding [26]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8d5e06c5d074ab0c2ad42db5a0e7c0db77a78459",
                "externalIds": {
                    "DOI": "10.23919/APSIPAASC55919.2022.9980299",
                    "CorpusId": 254931374
                },
                "corpusId": 254931374,
                "publicationVenue": {
                    "id": "5b924e1a-30f3-4275-bdb8-5a15517c0fde",
                    "name": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Asia-pacific Signal Inf Process Assoc Annu Summit Conf",
                        "APSIPA"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8d5e06c5d074ab0c2ad42db5a0e7c0db77a78459",
                "title": "A Novel Approach to Structured Pruning of Neural Network for Designing Compact Audio-Visual Wake Word Spotting System",
                "abstract": "In this paper, we propose a novel approach to structured pruning of neural network. Firstly, we extend the original channel-level pruning from one-shot manner to iterative manner. Then we further employ the learning rate rewinding strategy in the lottery ticket hypothesis (LTH) to guide the channel-level pruning, yielding a new algorithm named channel-level pruning with learning rate rewinding (CPLR). Finally, we apply CPLR to prune the audio and video networks for designing compact audio-visual wake word spotting (AVWWS) system. Tested on MISP-2021 AVWWS database, the results show that the proposed CPLR approach performs better than either the channel-level pruning approach or LTH approach in term of both system performance and model efficiency. More interestingly, we observe that while the network parameters are greatly reduced by CPLR, the network generalization capability can be even better.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2143388884",
                        "name": "Haotian Wang"
                    },
                    {
                        "authorId": "145419855",
                        "name": "Jun Du"
                    },
                    {
                        "authorId": "48054592",
                        "name": "Hengshun Zhou"
                    },
                    {
                        "authorId": "2197711657",
                        "name": "Heng Lu"
                    },
                    {
                        "authorId": "50206929",
                        "name": "Yuhang Cao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b2322a59bd5b49d920d41128934192f77f5def0d",
                "externalIds": {
                    "ArXiv": "2211.03013",
                    "DBLP": "journals/corr/abs-2211-03013",
                    "ACL": "2022.acl-long.157",
                    "DOI": "10.18653/v1/2022.acl-long.157",
                    "CorpusId": 248780046
                },
                "corpusId": 248780046,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/b2322a59bd5b49d920d41128934192f77f5def0d",
                "title": "Robust Lottery Tickets for Pre-trained Language Models",
                "abstract": "Recent works on Lottery Ticket Hypothesis have shown that pre-trained language models (PLMs) contain smaller matching subnetworks(winning tickets) which are capable of reaching accuracy comparable to the original models. However, these tickets are proved to be notrobust to adversarial examples, and even worse than their PLM counterparts. To address this problem, we propose a novel method based on learning binary weight masks to identify robust tickets hidden in the original PLMs. Since the loss is not differentiable for the binary mask, we assign the hard concrete distribution to the masks and encourage their sparsity using a smoothing approximation of L0 regularization.Furthermore, we design an adversarial loss objective to guide the search for robust tickets and ensure that the tickets perform well bothin accuracy and robustness. Experimental results show the significant improvement of the proposed method over previous work on adversarial robustness evaluation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2058585152",
                        "name": "Rui Zheng"
                    },
                    {
                        "authorId": "2165225344",
                        "name": "Rong Bao"
                    },
                    {
                        "authorId": "2110347739",
                        "name": "Yuhao Zhou"
                    },
                    {
                        "authorId": "2113983954",
                        "name": "Di Liang"
                    },
                    {
                        "authorId": "2592528",
                        "name": "Sirui Wang"
                    },
                    {
                        "authorId": "39533001",
                        "name": "Wei Wu"
                    },
                    {
                        "authorId": "2067331064",
                        "name": "Tao Gui"
                    },
                    {
                        "authorId": "47835189",
                        "name": "Qi Zhang"
                    },
                    {
                        "authorId": "1790227",
                        "name": "Xuanjing Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Apart from the image classification tasks, the LTH is also imported in many other research areas [Chen et al., 2020; Mallya et al., 2018; Gale et al., 2019; Yu et al., 2019; Renda et al., 2020; Chen et al., 2020; Prasanna et al., 2020; Girish et al., 2021].",
                "The works [Gale et al., 2019; Yu et al., 2019; Renda et al., 2020] show that the subnetworks exist early in the training instead of initialization on Transformers.",
                "The following work [Renda et al., 2020] extends (rewinds) the training of the subnetwork from initialization to the early stage of pretraining, which improves the accuracy of the subnetwork in more challenging tasks."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "58bb83f1b13ab6296733ec4c832c458831894fc0",
                "externalIds": {
                    "DBLP": "conf/ijcai/ShenKQDYM0MW23",
                    "ArXiv": "2211.01484",
                    "DOI": "10.24963/ijcai.2023/153",
                    "CorpusId": 258309380
                },
                "corpusId": 258309380,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/58bb83f1b13ab6296733ec4c832c458831894fc0",
                "title": "Data Level Lottery Ticket Hypothesis for Vision Transformers",
                "abstract": "The conventional lottery ticket hypothesis (LTH) claims that there exists a sparse subnetwork within a dense neural network and a proper random initialization method, called the winning ticket, such that it can be trained from scratch to almost as good as the dense counterpart. Meanwhile, the research of LTH in vision transformers (ViTs) is scarcely evaluated. In this paper, we first show that the conventional winning ticket is hard to find at weight level of ViTs by existing methods. Then, we generalize the LTH for ViTs to input data consisting of image patches inspired by the input dependence of ViTs. That is, there exists a subset of input image patches such that a ViT can be trained from scratch by using only this subset of patches and achieve similar accuracy to the ViTs trained by using all image patches. We call this subset of input patches the winning tickets, which represent a significant amount of information in the input data. We use a ticket selector to generate the winning tickets based on the informativeness of patches for various types of ViT, including DeiT, LV-ViT, and Swin Transformers. The experiments show that there is a clear difference between the performance of models trained with winning tickets and randomly selected subsets, which verifies our proposed theory. We elaborate the analogical similarity between our proposed Data-LTH-ViTs and the conventional LTH for further verifying the integrity of our theory. The Source codes are available at https://github.com/shawnricecake/vit-lottery-ticket-input.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2007668856",
                        "name": "Xuan Shen"
                    },
                    {
                        "authorId": "32409528",
                        "name": "Zhenglun Kong"
                    },
                    {
                        "authorId": "39449475",
                        "name": "Minghai Qin"
                    },
                    {
                        "authorId": "2052289825",
                        "name": "Peiyan Dong"
                    },
                    {
                        "authorId": "9347641",
                        "name": "Geng Yuan"
                    },
                    {
                        "authorId": "2148338860",
                        "name": "Xin Meng"
                    },
                    {
                        "authorId": "2168478659",
                        "name": "Hao Tang"
                    },
                    {
                        "authorId": "151480882",
                        "name": "Xiaolong Ma"
                    },
                    {
                        "authorId": "2136922252",
                        "name": "Yanzhi Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0696c125606bcbeefde756f7a8a66055e298e9a2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-13738",
                    "ArXiv": "2210.13738",
                    "DOI": "10.48550/arXiv.2210.13738",
                    "CorpusId": 253107616
                },
                "corpusId": 253107616,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0696c125606bcbeefde756f7a8a66055e298e9a2",
                "title": "Pruning's Effect on Generalization Through the Lens of Training and Regularization",
                "abstract": "Practitioners frequently observe that pruning improves model generalization. A long-standing hypothesis based on bias-variance trade-off attributes this generalization improvement to model size reduction. However, recent studies on over-parameterization characterize a new model size regime, in which larger models achieve better generalization. Pruning models in this over-parameterized regime leads to a contradiction -- while theory predicts that reducing model size harms generalization, pruning to a range of sparsities nonetheless improves it. Motivated by this contradiction, we re-examine pruning's effect on generalization empirically. We show that size reduction cannot fully account for the generalization-improving effect of standard pruning algorithms. Instead, we find that pruning leads to better training at specific sparsities, improving the training loss over the dense model. We find that pruning also leads to additional regularization at other sparsities, reducing the accuracy degradation due to noisy examples over the dense model. Pruning extends model training time and reduces model size. These two factors improve training and add regularization respectively. We empirically demonstrate that both factors are essential to fully explaining pruning's impact on generalization.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2188780740",
                        "name": "Tian Jin"
                    },
                    {
                        "authorId": "1701041",
                        "name": "Michael Carbin"
                    },
                    {
                        "authorId": "39331522",
                        "name": "Daniel M. Roy"
                    },
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "2533850",
                        "name": "G. Dziugaite"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A recent, very promising line of research [10, 31] suggests that only a subnetwork of the topology is responsible for carrying out accurately a particular task, and thus if we can detect which is this subnetwork and then train only this, pruning the rest of the network, then we can gain significant speedups in training."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "488bd32e1fd11703297ca04ae217bbaf090de17f",
                "externalIds": {
                    "DBLP": "journals/apin/FragkouSK23",
                    "DOI": "10.1007/s10489-022-04195-8",
                    "CorpusId": 253075745
                },
                "corpusId": 253075745,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/488bd32e1fd11703297ca04ae217bbaf090de17f",
                "title": "Model reduction of feed forward neural networks for resource-constrained devices",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8451893",
                        "name": "E. Fragkou"
                    },
                    {
                        "authorId": "48385268",
                        "name": "Marianna K. Sayeg"
                    },
                    {
                        "authorId": "39597140",
                        "name": "Dimitrios Katsaros"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We note that this learning rate schedule is different from prior work on pruning, which typically uses a single decay cycle [Kusupati et al., 2020, Singh and Alistarh, 2020, Peste et al., 2021], or dynamic learning rate rewinding, e.g. [Frankle et al., 2019, Renda et al., 2020]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3137c3ae4d21cf19d1bec8648f5f2935b5a3378e",
                "externalIds": {
                    "ArXiv": "2210.09223",
                    "CorpusId": 258987321
                },
                "corpusId": 258987321,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3137c3ae4d21cf19d1bec8648f5f2935b5a3378e",
                "title": "CAP: Correlation-Aware Pruning for Highly-Accurate Sparse Vision Models",
                "abstract": "Driven by significant improvements in architectural design and training pipelines, computer vision has recently experienced dramatic progress in terms of accuracy on classic benchmarks such as ImageNet. These highly-accurate models are challenging to deploy, as they appear harder to compress using standard techniques such as pruning. We address this issue by introducing the Correlation Aware Pruner (CAP), a new unstructured pruning framework which significantly pushes the compressibility limits for state-of-the-art architectures. Our method is based on two technical advancements: a new theoretically-justified pruner, which can handle complex weight correlations accurately and efficiently during the pruning process itself, and an efficient finetuning procedure for post-compression recovery. We validate our approach via extensive experiments on several modern vision models such as Vision Transformers (ViT), modern CNNs, and ViT-CNN hybrids, showing for the first time that these can be pruned to high sparsity levels (e.g. $\\geq 75$%) with low impact on accuracy ($\\leq 1$% relative drop). Our approach is also compatible with structured pruning and quantization, and can lead to practical speedups of 1.5 to 2.4x without accuracy loss. To further showcase CAP's accuracy and scalability, we use it to show for the first time that extremely-accurate large vision models, trained via self-supervised techniques, can also be pruned to moderate sparsities, with negligible accuracy loss.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2006108901",
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "authorId": "40992614",
                        "name": "Eldar Kurtic"
                    },
                    {
                        "authorId": "1502248377",
                        "name": "Elias Frantar"
                    },
                    {
                        "authorId": "3311387",
                        "name": "Dan Alistarh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In line with LTH, weight rewinding (Renda et al., 2020) is adopted to retrain the identified positive soft prompts.",
                "Following the idea in LTH, we adopt the weight rewinding technique (Renda et al., 2020) to re-train the soft prompts after the two-level hierarchical structured pruning."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "40e49ba41eca31f9c2661cc65f2c13dc4f2c7859",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-04457",
                    "ACL": "2022.emnlp-main.758",
                    "ArXiv": "2210.04457",
                    "DOI": "10.48550/arXiv.2210.04457",
                    "CorpusId": 252780166
                },
                "corpusId": 252780166,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/40e49ba41eca31f9c2661cc65f2c13dc4f2c7859",
                "title": "XPrompt: Exploring the Extreme of Prompt Tuning",
                "abstract": "Prompt tuning learns soft prompts to condition the frozen Pre-trained Language Models (PLMs) for performing downstream tasks in a parameter-efficient manner. While prompt tuning has gradually reached the performance level of fine-tuning as the model scale increases, there is still a large performance gap between prompt tuning and fine-tuning for models of moderate and small scales (typically less than 11B parameters). In this paper, we empirically show that the trained prompt tokens can have a negative impact on a downstream task and thus degrade its performance. To bridge the gap, we propose a novel Prompt tuning model with an eXtremely small scale (XPrompt) under the regime of lottery tickets hypothesis. Specifically, XPrompt eliminates the negative prompt tokens at different granularity levels through a hierarchical structured pruning, yielding a more parameter-efficient prompt yet with a competitive performance. Comprehensive experiments are carried out on the SuperGLUE tasks, and the results indicate that XPrompt is able to close the performance gap at smaller model scales.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2163587600",
                        "name": "Fang Ma"
                    },
                    {
                        "authorId": "145107889",
                        "name": "Chen Zhang"
                    },
                    {
                        "authorId": "2152318755",
                        "name": "Lei Ren"
                    },
                    {
                        "authorId": "2109593338",
                        "name": "Jingang Wang"
                    },
                    {
                        "authorId": "2145778781",
                        "name": "Qifan Wang"
                    },
                    {
                        "authorId": "50224935",
                        "name": "Wei Yu Wu"
                    },
                    {
                        "authorId": "38472218",
                        "name": "Xiaojun Quan"
                    },
                    {
                        "authorId": "2151679983",
                        "name": "Dawei Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Among the iterative schemes, the IMP (Iterative Magnitude Pruning scheme) [17, 20, 56\u2013 65, 36] has played a significant role in identifying high-quality \u2018winning tickets\u2019, as postulated by LTH (Lottery Ticket Hypothesis) [18, 19].",
                ", the use of early-epoch rewinding for model re-initialization [18] and the no-rewinding (i.",
                "In this work, if a matching subnetwork is found better than the winning ticket obtained by the same method that follows the original LTH setup [18, 19], we will also call such a matching subnetwork a winning ticket throughout the paper.",
                "[18] Alex Renda, Jonathan Frankle, and Michael Carbin, \u201cComparing rewinding and fine-tuning in neural network pruning,\u201d in 8th International Conference on Learning Representations, 2020.",
                "For (ii), the unpruned weights in each pruning iteration are re-set to the weights at initialization or at an early-training epoch [18], and re-trained till convergence.",
                "As illustrated by the Lottery Ticket Hypothesis (LTH), pruning also has the potential of improving their generalization ability.",
                "L G\n] 2\n1 A\nAmong various proposed model pruning algorithms [5, 9, 11, 14\u201327], the heuristics-based Iterative Magnitude Pruning (IMP) is the current dominant approach to achieving model sparsity without suffering performance loss, as suggested and empirically justified by the Lottery Ticket Hypothesis (LTH) [17].",
                ", specified by different learning rates and model initialization or \u2018rewinding\u2019 strategies [18]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "13cf75c5fb62db04e2485997b03be33e2125ace5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-04092",
                    "ArXiv": "2210.04092",
                    "DOI": "10.48550/arXiv.2210.04092",
                    "CorpusId": 252780187
                },
                "corpusId": 252780187,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/13cf75c5fb62db04e2485997b03be33e2125ace5",
                "title": "Advancing Model Pruning via Bi-level Optimization",
                "abstract": "The deployment constraints in practical applications necessitate the pruning of large-scale deep learning models, i.e., promoting their weight sparsity. As illustrated by the Lottery Ticket Hypothesis (LTH), pruning also has the potential of improving their generalization ability. At the core of LTH, iterative magnitude pruning (IMP) is the predominant pruning method to successfully find 'winning tickets'. Yet, the computation cost of IMP grows prohibitively as the targeted pruning ratio increases. To reduce the computation overhead, various efficient 'one-shot' pruning methods have been developed, but these schemes are usually unable to find winning tickets as good as IMP. This raises the question of how to close the gap between pruning accuracy and pruning efficiency? To tackle it, we pursue the algorithmic advancement of model pruning. Specifically, we formulate the pruning problem from a fresh and novel viewpoint, bi-level optimization (BLO). We show that the BLO interpretation provides a technically-grounded optimization base for an efficient implementation of the pruning-retraining learning paradigm used in IMP. We also show that the proposed bi-level optimization-oriented pruning method (termed BiP) is a special class of BLO problems with a bi-linear problem structure. By leveraging such bi-linearity, we theoretically show that BiP can be solved as easily as first-order optimization, thus inheriting the computation efficiency. Through extensive experiments on both structured and unstructured pruning with 5 model architectures and 4 data sets, we demonstrate that BiP can find better winning tickets than IMP in most cases, and is computationally as efficient as the one-shot pruning schemes, demonstrating 2-7 times speedup over IMP for the same level of model accuracy and sparsity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155369380",
                        "name": "Yihua Zhang"
                    },
                    {
                        "authorId": "100630765",
                        "name": "Yuguang Yao"
                    },
                    {
                        "authorId": "2944292",
                        "name": "P. Ram"
                    },
                    {
                        "authorId": "31643513",
                        "name": "Pu Zhao"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "66603575",
                        "name": "Min-Fong Hong"
                    },
                    {
                        "authorId": "2136922252",
                        "name": "Yanzhi Wang"
                    },
                    {
                        "authorId": "2118464654",
                        "name": "Sijia Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Let \u03c4 denote a training step from which IMP-WR works (i.e., the onset of linear mode connectivity).",
                "Renda et al. (2020) find that both IMP-WR and IMP-LRR find matching subnetworks while standard finetuning (with fixed small learning rates) does not.",
                "As shown in (Renda et al., 2020), this produces networks that perform equivalently to IMP-WR.",
                "However, IMP-WR has a special property: it can be retrained from an iteration early in training.",
                "IMP with Weight Rewinding (IMP-WR) is described in Algorithm 1 (Frankle et al., 2020).",
                "D IMP-LRR SUBNETWORKS CAN BE RETRAINED FROM AN EARLY REWIND POINT\nIMP with learning rate rewinding (IMP-LRR) has been shown to exceed the performance of standard fine tuning, and match the performance of IMP-WR (Renda et al., 2020).",
                "Both IMP-LRR and IMP-WR can be used to find matching subnetworks.",
                "We also study two variants of IMP with different retraining strategies in Section 3.4: IMP with LR rewinding (IMP-LRR) and IMP with finetuning (IMP-FT) (Renda et al., 2020).",
                "IMP-FT is similar to IMP-LRR but instead of repeating the entire LR schedule, we continue training at the final low LR for the same number of steps: w(L) = AT (m(L) w(L\u22121), T ).",
                "7 demonstrates that both IMP-WR and IMP-LRR reequilibriate the weight distribution after pruning, i.e. a retrained network once again contains a substantial fraction of small magnitude weights that are amenable to pruning.",
                "Renda et al. (2020) show that this finetuning underperforms IMP-WR in final test accuracy.",
                "Renda et al. (2020) show that fine tuning underperforms IMP-WR in final test accuracy.",
                "Alternative methods that attain matching performance at the sparsity levels as IMP also feature iterative pruning and retraining (Renda et al., 2020; Savarese et al., 2020).",
                "To alleviate this, Renda et al. (2020) propose a middle ground between IMP-WR and FINE TUNING called IMP with LEARNING RATE REWINDING (IMP-LRR).",
                "4: IMP with LR rewinding (IMP-LRR) and IMP with finetuning (IMP-FT) (Renda et al., 2020).",
                "In the IMP-WR framework proposed by Frankle et al. (2020) after each pruning step the network is rewound to an early rewind point w\u03c4 , and from that point on the network is retrained with the new sparsity pattern.",
                "Why does pruning a larger fraction in one iteration destroy the actionable information in the mask? Fourth, why does retraining allow us to prune more weights? A variant of IMP that uses a different retraining strategy (learning rate rewinding) also successfully identifies matching subnetworks while another variant (finetuning) fails (Renda et al., 2020).",
                "The axial subspace associated with mask m(L) is a colored subspace, the pruned rewind point m(L) w\u03c4 is the circle in this subspace, the level L solution w(L) obtained from\nAlgorithm 1: Iterative Magnitude Pruning-Weight Rewinding (IMP-WR) (Frankle et al., 2020)\n1: Initialize a dense network w0 \u2208 Rd and a pruning mask m(0) = 1d. 2: Train w0 for \u03c4 steps to w\u03c4 .",
                "As shown in Renda et al. (2020), this produces networks that perform equivalently to IMP-WR.",
                "To alleviate this, Renda et al. (2020) propose a middle ground between IMP-WR and IMP-FT called learning rate rewinding (IMP-LRR).",
                "Successful retraining strategies such as weight and learning rate (LR) rewinding (Renda et al., 2020) both do this while finetuning (FT), an unsuccessful retraining strategy, does not.",
                "A variant of IMP that uses a different retraining strategy (learning rate rewinding) also successfully identifies matching subnetworks while another variant (finetuning) fails (Renda et al., 2020)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "3de645f0c1993cd3f3374ad747640a1aa6658a82",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-03044",
                    "ArXiv": "2210.03044",
                    "DOI": "10.48550/arXiv.2210.03044",
                    "CorpusId": 252735281
                },
                "corpusId": 252735281,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3de645f0c1993cd3f3374ad747640a1aa6658a82",
                "title": "Unmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask?",
                "abstract": "Modern deep learning involves training costly, highly overparameterized networks, thus motivating the search for sparser networks that can still be trained to the same accuracy as the full network (i.e. matching). Iterative magnitude pruning (IMP) is a state of the art algorithm that can find such highly sparse matching subnetworks, known as winning tickets. IMP operates by iterative cycles of training, masking smallest magnitude weights, rewinding back to an early training point, and repeating. Despite its simplicity, the underlying principles for when and how IMP finds winning tickets remain elusive. In particular, what useful information does an IMP mask found at the end of training convey to a rewound network near the beginning of training? How does SGD allow the network to extract this information? And why is iterative pruning needed? We develop answers in terms of the geometry of the error landscape. First, we find that$\\unicode{x2014}$at higher sparsities$\\unicode{x2014}$pairs of pruned networks at successive pruning iterations are connected by a linear path with zero error barrier if and only if they are matching. This indicates that masks found at the end of training convey the identity of an axial subspace that intersects a desired linearly connected mode of a matching sublevel set. Second, we show SGD can exploit this information due to a strong form of robustness: it can return to this mode despite strong perturbations early in training. Third, we show how the flatness of the error landscape at the end of training determines a limit on the fraction of weights that can be pruned at each iteration of IMP. Finally, we show that the role of retraining in IMP is to find a network with new small weights to prune. Overall, these results make progress toward demystifying the existence of winning tickets by revealing the fundamental role of error landscape geometry.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1690452",
                        "name": "Mansheej Paul"
                    },
                    {
                        "authorId": "2650184",
                        "name": "F. Chen"
                    },
                    {
                        "authorId": "152574768",
                        "name": "Brett W. Larsen"
                    },
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "25769960",
                        "name": "S. Ganguli"
                    },
                    {
                        "authorId": "2533850",
                        "name": "G. Dziugaite"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020) or follow a computationally expensive cycle of pruning and retraining for multiple iterations (Gale et al., 2019; Savarese et al., 2020; You et al., 2019; Frankle & Carbin, 2019; Renda et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "11422fff4d42b70c31af69381ff32d35031c939d",
                "externalIds": {
                    "DBLP": "conf/icml/GadhikarMB23",
                    "ArXiv": "2210.02412",
                    "CorpusId": 258987596
                },
                "corpusId": 258987596,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/11422fff4d42b70c31af69381ff32d35031c939d",
                "title": "Why Random Pruning Is All We Need to Start Sparse",
                "abstract": "Random masks define surprisingly effective sparse neural network models, as has been shown empirically. The resulting sparse networks can often compete with dense architectures and state-of-the-art lottery ticket pruning algorithms, even though they do not rely on computationally expensive prune-train iterations and can be drawn initially without significant computational overhead. We offer a theoretical explanation of how random masks can approximate arbitrary target networks if they are wider by a logarithmic factor in the inverse sparsity $1 / \\log(1/\\text{sparsity})$. This overparameterization factor is necessary at least for 3-layer random networks, which elucidates the observed degrading performance of random networks at higher sparsity. At moderate to high sparsity levels, however, our results imply that sparser networks are contained within random source networks so that any dense-to-sparse training scheme can be turned into a computationally more efficient sparse-to-sparse one by constraining the search to a fixed random mask. We demonstrate the feasibility of this approach in experiments for different pruning methods and propose particularly effective choices of initial layer-wise sparsity ratios of the random source network. As a special case, we show theoretically and experimentally that random source networks also contain strong lottery tickets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2048027747",
                        "name": "Advait Gadhikar"
                    },
                    {
                        "authorId": "79760097",
                        "name": "Sohom Mukherjee"
                    },
                    {
                        "authorId": "8813467",
                        "name": "R. Burkholz"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b6408c7dc8ce8c386197990d90ccb528419db25b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-14624",
                    "ArXiv": "2209.14624",
                    "DOI": "10.48550/arXiv.2209.14624",
                    "CorpusId": 252595918
                },
                "corpusId": 252595918,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b6408c7dc8ce8c386197990d90ccb528419db25b",
                "title": "Is Complexity Required for Neural Network Pruning? A Case Study on Global Magnitude Pruning",
                "abstract": "Pruning neural networks has become popular in the last decade when it was shown that a large number of weights can be safely removed from modern neural networks without compromising accuracy. Numerous pruning methods have been proposed since then, each claiming to be better than the previous. Many state-of-the-art (SOTA) techniques today rely on complex pruning methodologies utilizing importance scores, getting feedback through back-propagation or having heuristics-based pruning rules amongst others. In this work, we question whether this pattern of introducing complexity is really necessary to achieve better pruning results. We benchmark these SOTA techniques against a naive pruning baseline, namely, Global Magnitude Pruning (Global MP). Global MP ranks weights in order of their magnitudes and prunes the smallest ones. Hence, in its vanilla form, it is one of the simplest pruning techniques. Surprisingly, we find that vanilla Global MP outperforms all the other SOTA techniques and achieves a new SOTA result. It also achieves promising performance on FLOPs sparsification, which we find is enhanced, when pruning is conducted in a gradual fashion. We also find that Global MP is generalizable across tasks, datasets, and models with superior performance. Moreover, a common issue that many pruning algorithms run into at high sparsity rates, namely, layer-collapse, can be easily fixed in Global MP by setting a minimum threshold of weights to be retained in each layer. Lastly, unlike many other SOTA techniques, Global MP does not require any additional algorithm specific hyper-parameters and is very straightforward to tune and implement. We showcase our findings on various models (WRN-28-8, ResNet-32, ResNet-50, MobileNet-V1 and FastGRNN) and multiple datasets (CIFAR-10, ImageNet and HAR-2). Code is available at https://github.com/manasgupta-1/GlobalMP.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109835363",
                        "name": "Manas Gupta"
                    },
                    {
                        "authorId": "7197691",
                        "name": "Efe Camci"
                    },
                    {
                        "authorId": "2186404613",
                        "name": "Vishandi Rudy Keneta"
                    },
                    {
                        "authorId": "2186403073",
                        "name": "Abhishek Vaidyanathan"
                    },
                    {
                        "authorId": "2186404868",
                        "name": "Ritwik Kanodia"
                    },
                    {
                        "authorId": "2121484",
                        "name": "Chuan-Sheng Foo"
                    },
                    {
                        "authorId": "2186403431",
                        "name": "Wu Min"
                    },
                    {
                        "authorId": "2056595772",
                        "name": "Lin Jie"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1316893f8352e4c2555adbac40b25eb0ae6f39ba",
                "externalIds": {
                    "ArXiv": "2209.10085",
                    "DOI": "10.1109/JLT.2023.3276373",
                    "CorpusId": 252407732
                },
                "corpusId": 252407732,
                "publicationVenue": {
                    "id": "a1f606c0-a59f-4771-bdb9-aac3a2ca72eb",
                    "name": "Journal of Lightwave Technology",
                    "type": "journal",
                    "alternate_names": [
                        "J Light Technol"
                    ],
                    "issn": "0733-8724",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=50",
                    "alternate_urls": [
                        "http://jlt.osa.org/journal/JLT/about.cfm",
                        "http://www.opticsinfobase.org/jlt/journal/JLT/about.cfm",
                        "http://jlt.osa.org/issue.cfm"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1316893f8352e4c2555adbac40b25eb0ae6f39ba",
                "title": "Joint PMD Tracking and Nonlinearity Compensation With Deep Neural Networks",
                "abstract": "Overcoming fiber nonlinearity is one of the core challenges limiting the capacity of optical fiber communication systems. Machine learning based solutions such as learned digital backpropagation (LDBP) and the recently proposed deep convolutional recurrent neural network (DCRNN) have been shown to be effective for fiber nonlinearity compensation (NLC). Incorporating distributed compensation of polarization mode dispersion (PMD) within the learned models can improve their performance even further but at the same time, it also couples the compensation of nonlinearity and PMD. Consequently, it is important to consider the time variation of PMD for such a joint compensation scheme. In this paper, we investigate the impact of PMD drift on the DCRNN model with distributed compensation of PMD. We propose a transfer learning based selective training scheme to adapt the learned neural network model to changes in PMD. We demonstrate that fine-tuning only a small subset of weights as per the proposed method is sufficient for adapting the model to PMD drift. Using decision directed feedback for online learning, we track continuous PMD drift resulting from a time-varying rotation of the state of polarization (SOP). We show that transferring knowledge from a pre-trained base model using the proposed scheme significantly reduces the re-training efforts for different PMD realizations. Applying the hinge model for SOP rotation, our simulation results show that the learned models maintain their performance gains while tracking the PMD.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "73170567",
                        "name": "Prasham Jain"
                    },
                    {
                        "authorId": "35699680",
                        "name": "L. Lampe"
                    },
                    {
                        "authorId": "3252345",
                        "name": "J. Mitra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "LTH based pruning is able to further reduce DCRNN-PMD complexity by almost 50% with negligible performance loss.",
                "have varying potential for complexity reduction, and we apply an iterative pruning and fine-tuning approach based on the lottery ticket hypothesis (LTH)[8] to operate the learned NLC methods at their optimal complexity.",
                "LTH-based weight pruning is then applied to reduce complexity of each model and obtain performance results at each complexity level.",
                "For this, we account for the fact that different NN models have varying potential for complexity reduction, and we apply an iterative pruning and fine-tuning approach based on the lottery ticket hypothesis (LTH)[8] to operate the learned NLC methods at their optimal complexity.",
                "Then, to effectively retrain the pruned model to compensate for the removed weights, we apply LTH[8], wherein we rewind the learning rate schedule before fine-tuning."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6d51162b9e183302cc044ea1d6c13fd465f93357",
                "externalIds": {
                    "CorpusId": 254930442
                },
                "corpusId": 254930442,
                "publicationVenue": {
                    "id": "c99ed5c4-aafc-4f8b-85e7-cb9fd73f8ac1",
                    "name": "European Conference on Optical Communication",
                    "type": "conference",
                    "alternate_names": [
                        "ECOC",
                        "Eur Conf Opt Commun"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6d51162b9e183302cc044ea1d6c13fd465f93357",
                "title": "Deep Convolutional Recurrent Neural Network For Fiber Nonlinearity Compensation",
                "abstract": "An iterative deep convolutional recurrent neural network is proposed to mitigate fiber non-linearity with distributed compensation of polarization mode dispersion, demonstrating 1.3 dB Q-factor gain over previous neural network based techniques for dual-polarized 960 km 32 Gbaud 64QAM transmission.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "73170567",
                        "name": "Prasham Jain"
                    },
                    {
                        "authorId": "2059767631",
                        "name": "Lutz Lampe"
                    },
                    {
                        "authorId": "3252345",
                        "name": "J. Mitra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use a pruning approach similar to[5], but rather than applying an absolute threshold, we gradually prune the relatively small weights and rewind the learning rate schedule before fine tuning[17]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c6051b9f85b4272eb4a6ee970360ae45eb657226",
                "externalIds": {
                    "ArXiv": "2210.03440",
                    "CorpusId": 252762327
                },
                "corpusId": 252762327,
                "publicationVenue": {
                    "id": "c99ed5c4-aafc-4f8b-85e7-cb9fd73f8ac1",
                    "name": "European Conference on Optical Communication",
                    "type": "conference",
                    "alternate_names": [
                        "ECOC",
                        "Eur Conf Opt Commun"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c6051b9f85b4272eb4a6ee970360ae45eb657226",
                "title": "Learning for Perturbation-Based Fiber Nonlinearity Compensation",
                "abstract": "Several machine learning inspired methods for perturbation-based fiber n onlinearity (PB-NLC) compensation have been presented in recent literature. We critically revisit acclaimed benefits of those over non-learned methods. Numerical results suggest that learned linear processing of perturbation triplets of PB-NLC is preferable over feedforward neural-network solutions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2098204832",
                        "name": "S. Luo"
                    },
                    {
                        "authorId": "2126994598",
                        "name": "Sunish Kumar Orappanpara Soman"
                    },
                    {
                        "authorId": "35699680",
                        "name": "L. Lampe"
                    },
                    {
                        "authorId": "3252345",
                        "name": "J. Mitra"
                    },
                    {
                        "authorId": "2118671349",
                        "name": "Chuandong Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Unstructured Sparsity prunes the model without any sparsity pattern constraint [43, 17, 28, 12, 14, 60, 18, 31, 51, 6, 14, 26, 9, 4, 34].",
                "Magnitude pruning, which selects the pruning elements by their absolute values, is the most widely used method [43, 17, 28, 12, 14, 60, 18, 31, 40, 33].",
                "The sparsified models often ends up with similar or worse performance (because of the extra complexity to compress and decompress the parameters) than their dense counterparts [2, 32, 43, 21, 30, 15, 59, 50, 10]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a10dd5e0f74a14ca0e7c37a121bf77970f932e32",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-07617",
                    "ArXiv": "2209.07617",
                    "DOI": "10.48550/arXiv.2209.07617",
                    "CorpusId": 252355271
                },
                "corpusId": 252355271,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a10dd5e0f74a14ca0e7c37a121bf77970f932e32",
                "title": "Training Recipe for N: M Structured Sparsity with Decaying Pruning Mask",
                "abstract": "Sparsity has become one of the promising methods to compress and accelerate Deep Neural Networks (DNNs). Among different categories of sparsity, structured sparsity has gained more attention due to its efficient execution on modern accelerators. Particularly, N:M sparsity is attractive because there are already hardware accelerator architectures that can leverage certain forms of N:M structured sparsity to yield higher compute-efficiency. In this work, we focus on N:M sparsity and extensively study and evaluate various training recipes for N:M sparsity in terms of the trade-off between model accuracy and compute cost (FLOPs). Building upon this study, we propose two new decay-based pruning methods, namely\"pruning mask decay\"and\"sparse structure decay\". Our evaluations indicate that these proposed methods consistently deliver state-of-the-art (SOTA) model accuracy, comparable to unstructured sparsity, on a Transformer-based model for a translation task. The increase in the accuracy of the sparse model using the new training recipes comes at the cost of marginal increase in the total training compute (FLOPs).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "27057088",
                        "name": "Sheng-Chun Kao"
                    },
                    {
                        "authorId": "2112229",
                        "name": "A. Yazdanbakhsh"
                    },
                    {
                        "authorId": "1929462",
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "authorId": "3504647",
                        "name": "Shivani Agrawal"
                    },
                    {
                        "authorId": "3399348",
                        "name": "Utku Evci"
                    },
                    {
                        "authorId": "145984583",
                        "name": "T. Krishna"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Also, methods such as network pruning (Han et al., 2015b; Renda et al., 2020; Hu et al., 2016; Iandola et al., 2016; Goetschalckx et al., 2018), weight sharing (Chen et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "371cb6c2dca55d24ba6e2c44c8ec3b0bdb5b07c4",
                "externalIds": {
                    "ArXiv": "2209.03450",
                    "CorpusId": 252118521
                },
                "corpusId": 252118521,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/371cb6c2dca55d24ba6e2c44c8ec3b0bdb5b07c4",
                "title": "Seeking Interpretability and Explainability in Binary Activated Neural Networks",
                "abstract": "We study the use of binary activated neural networks as interpretable and explainable predictors in the context of regression tasks on tabular data; more specifically, we provide guarantees on their expressiveness, present an approach based on the efficient computation of SHAP values for quantifying the relative importance of the features, hidden neurons and even weights. As the model's simplicity is instrumental in achieving interpretability, we propose a greedy algorithm for building compact binary activated networks. This approach doesn't need to fix an architecture for the network in advance: it is built one layer at a time, one neuron at a time, leading to predictors that aren't needlessly complex for a given task.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50003031",
                        "name": "Benjamin J. LeBlanc"
                    },
                    {
                        "authorId": "31580144",
                        "name": "Pascal Germain"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5e350a4b617a4ab3062cbbc4eb826841f709f786",
                "externalIds": {
                    "DBLP": "journals/ijon/LvWZXY22",
                    "DOI": "10.1016/j.neucom.2022.09.057",
                    "CorpusId": 252253872
                },
                "corpusId": 252253872,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5e350a4b617a4ab3062cbbc4eb826841f709f786",
                "title": "Realistic acceleration of neural networks with fine-grained tensor decomposition",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2180572352",
                        "name": "Rui Lv"
                    },
                    {
                        "authorId": "1452735766",
                        "name": "Dingheng Wang"
                    },
                    {
                        "authorId": "3227620",
                        "name": "Jiangbin Zheng"
                    },
                    {
                        "authorId": "81741622",
                        "name": "Yefan Xie"
                    },
                    {
                        "authorId": "2155030201",
                        "name": "Zhaohui Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Later, weight/learning rate rewinding techniques (Frankle et al. 2020; Renda, Frankle, and Carbin 2020) was proposed to scale up LTs to larger networks and datasets."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "173a41e7f0eebca7deadaa623d36f83ad93c9bf5",
                "externalIds": {
                    "DBLP": "conf/aaai/0006LFHMP23",
                    "ArXiv": "2208.10842",
                    "DOI": "10.48550/arXiv.2208.10842",
                    "CorpusId": 251741428
                },
                "corpusId": 251741428,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/173a41e7f0eebca7deadaa623d36f83ad93c9bf5",
                "title": "Lottery Pools: Winning More by Interpolating Tickets without Increasing Training or Inference Cost",
                "abstract": "Lottery tickets (LTs) is able to discover accurate and sparse subnetworks that could be trained in isolation to match the performance of dense networks. Ensemble, in parallel, is one of the oldest time-proven tricks in machine learning to improve performance by combining the output of multiple independent models. However, the benefits of ensemble in the context of LTs will be diluted since ensemble does not directly lead to stronger sparse subnetworks, but leverages their predictions for a better decision. In this work, we first observe that directly averaging the weights of the adjacent learned subnetworks significantly boosts the performance of LTs. Encouraged by this observation, we further propose an alternative way to perform an \"ensemble'' over the subnetworks identified by iterative magnitude pruning via a simple interpolating strategy. We call our method Lottery Pools. In contrast to the naive ensemble which brings no performance gains to each single subnetwork, Lottery Pools yields much stronger sparse subnetworks than the original LTs without requiring any extra training or inference cost. Across various modern architectures on CIFAR-10/100 and ImageNet, we show that our method achieves significant performance gains in both, in-distribution and out-of-distribution scenarios. Impressively, evaluated with VGG-16 and ResNet-18, the produced sparse subnetworks outperform the original LTs by up to 1.88% on CIFAR-100 and 2.36% on CIFAR-100-C; the resulting dense network surpasses the pre-trained dense-model up to \n 2.22% on CIFAR-100 and 2.38% on CIFAR-100-C. Our source code can be found at https://github.com/luuyin/Lottery-pools.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1410465360",
                        "name": "Lu Yin"
                    },
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2182421180",
                        "name": "Fang Meng"
                    },
                    {
                        "authorId": "8242939",
                        "name": "Tianjin Huang"
                    },
                    {
                        "authorId": "49917515",
                        "name": "V. Menkovski"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use unstructured weight pruning, which 140 can achieve higher sparsities than structured prun- 141 ing (Renda et al., 2020), and has comparatively 142 standard implementations.",
                "We use unstructured weight pruning, which can achieve higher sparsities than structured pruning (Renda et al., 2020), and has comparatively"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4d817fd41790a9b72ddf6e02c1584a618c8809d4",
                "externalIds": {
                    "ACL": "2022.coling-1.252",
                    "DBLP": "journals/corr/abs-2208-09684",
                    "ArXiv": "2208.09684",
                    "DOI": "10.48550/arXiv.2208.09684",
                    "CorpusId": 247741658
                },
                "corpusId": 247741658,
                "publicationVenue": {
                    "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
                    "name": "International Conference on Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Linguistics",
                        "COLING"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/coling/"
                },
                "url": "https://www.semanticscholar.org/paper/4d817fd41790a9b72ddf6e02c1584a618c8809d4",
                "title": "Combining Compressions for Multiplicative Size Scaling on Natural Language Tasks",
                "abstract": "Quantization, knowledge distillation, and magnitude pruning are among the most popular methods for neural network compression in NLP. Independently, these methods reduce model size and can accelerate inference, but their relative benefit and combinatorial interactions have not been rigorously studied. For each of the eight possible subsets of these techniques, we compare accuracy vs. model size tradeoffs across six BERT architecture sizes and eight GLUE tasks. We find that quantization and distillation consistently provide greater benefit than pruning. Surprisingly, except for the pair of pruning and quantization, using multiple methods together rarely yields diminishing returns. Instead, we observe complementary and super-multiplicative reductions to model size. Our work quantitatively demonstrates that combining compression methods can synergistically reduce model size, and that practitioners should prioritize (1) quantization, (2) knowledge distillation, and (3) pruning to maximize accuracy vs. model size tradeoffs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1405369173",
                        "name": "Rajiv Movva"
                    },
                    {
                        "authorId": "33019343",
                        "name": "Jinhao Lei"
                    },
                    {
                        "authorId": "29909347",
                        "name": "S. Longpre"
                    },
                    {
                        "authorId": "2124739758",
                        "name": "Ajay Gupta"
                    },
                    {
                        "authorId": "2126499571",
                        "name": "Chris DuBois"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "723fe6b3d6b8489486cba3fad94efdd25bdba4ac",
                "externalIds": {
                    "DBLP": "journals/ijautcomp/WuWLYYDSL22",
                    "DOI": "10.1007/s11633-022-1340-5",
                    "CorpusId": 251707034
                },
                "corpusId": 251707034,
                "publicationVenue": {
                    "id": "1caabc5e-b06a-4ba8-bccd-8d3b71322232",
                    "name": "Machine Intelligence Research",
                    "alternate_names": [
                        "Mach Intell Res"
                    ],
                    "issn": "2731-538X"
                },
                "url": "https://www.semanticscholar.org/paper/723fe6b3d6b8489486cba3fad94efdd25bdba4ac",
                "title": "Efficient Visual Recognition: A Survey on Recent Advances and Brain-inspired Methodologies",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2038030133",
                        "name": "Yang Wu"
                    },
                    {
                        "authorId": "1452735766",
                        "name": "Dingheng Wang"
                    },
                    {
                        "authorId": "7829127",
                        "name": "Xiaotong Lu"
                    },
                    {
                        "authorId": "2158028718",
                        "name": "Fan Yang"
                    },
                    {
                        "authorId": "2144049133",
                        "name": "Man Yao"
                    },
                    {
                        "authorId": "50702526",
                        "name": "Weifeng Dong"
                    },
                    {
                        "authorId": "2182163977",
                        "name": "Jianbo Shi"
                    },
                    {
                        "authorId": "2190108823",
                        "name": "Guoqi Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "282e92014c3c3f01c1ff55d1146d13700da956e1",
                "externalIds": {
                    "ArXiv": "2208.08003",
                    "CorpusId": 259164949
                },
                "corpusId": 259164949,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/282e92014c3c3f01c1ff55d1146d13700da956e1",
                "title": "Investigating the Impact of Model Width and Density on Generalization in Presence of Label Noise",
                "abstract": "Increasing the size of overparameterized neural networks has been a key in achieving state-of-the-art performance. This is captured by the double descent phenomenon, where the test loss follows a decreasing-increasing-decreasing pattern as model width increases. However, the effect of label noise on the test loss curve has not been fully explored. In this work, we uncover an intriguing phenomenon where label noise leads to a \\textit{final ascent} in the originally observed double descent curve. Specifically, under a sufficiently large noise-to-sample-size ratio, optimal generalization is achieved at intermediate widths. Through theoretical analysis, we attribute this phenomenon to the shape transition of test loss variance induced by label noise. Furthermore, we extend the final ascent phenomenon to model density and provide the first theoretical characterization showing that reducing density by randomly dropping trainable parameters improves generalization under label noise. We also thoroughly examine the roles of regularization and sample size. Surprisingly, we find that larger $\\ell_2$ regularization and robust learning methods against label noise exacerbate the final ascent. We confirm the validity of our findings through extensive experiments on ReLu networks trained on MNIST, ResNets trained on CIFAR-10/100, and InceptionResNet-v2 trained on Stanford Cars with real-world noisy labels.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "150353219",
                        "name": "Yihao Xue"
                    },
                    {
                        "authorId": "2151790153",
                        "name": "Kyle Whitecross"
                    },
                    {
                        "authorId": "2389094",
                        "name": "Baharan Mirzasoleiman"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f8eb940e34a370d2e10d115203fd5255c3b3c218",
                "externalIds": {
                    "DBLP": "conf/dsd/ZhuPWBBM22",
                    "DOI": "10.1109/DSD57027.2022.00062",
                    "CorpusId": 255419315
                },
                "corpusId": 255419315,
                "publicationVenue": {
                    "id": "b7696878-9521-4592-8205-198d53528bd2",
                    "name": "Euromicro Symposium on Digital Systems Design",
                    "type": "conference",
                    "alternate_names": [
                        "Digit Syst Des",
                        "Euromicro Symp Digit Syst Des",
                        "Digital Systems Design",
                        "DSD"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=762"
                },
                "url": "https://www.semanticscholar.org/paper/f8eb940e34a370d2e10d115203fd5255c3b3c218",
                "title": "ARTS: An adaptive regularization training schedule for activation sparsity exploration",
                "abstract": "Brain-inspired event-based processors have attracted considerable attention for edge deployment because of their ability to efficiently process Convolutional Neural Networks (CNNs) by exploiting sparsity. On such processors, one critical feature is that the speed and energy consumption of CNN inference are approximately proportional to the number of non-zero values in the activation maps. Thus, to achieve top performance, an efficient training algorithm is required to largely suppress the activations in CNNs. We propose a novel training method, called Adaptive-Regularization Training Schedule (ARTS), which dramatically decreases the non-zero activations in a model by adaptively altering the regularization coefficient through training. We evaluate our method across an extensive range of computer vision applications, including image classification, object recognition, depth estimation, and semantic segmentation. The results show that our technique can achieve 1.41 \u00d7 to 6.00 \u00d7 more activation suppression on top of ReLU activation across various networks and applications, and outperforms the state-of-the-art methods in terms of training time, activation suppression gains, and accuracy. A case study for a commercially-available event-based processor, Neuronflow, shows that the activation suppression achieved by ARTS effectively reduces CNN inference latency by up to 8.4 \u00d7 and energy consumption by up to 14.1 \u00d7.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109473179",
                        "name": "Zeqi Zhu"
                    },
                    {
                        "authorId": "2625990",
                        "name": "Arash Pourtaherian"
                    },
                    {
                        "authorId": "1803194",
                        "name": "Luc Waeijen"
                    },
                    {
                        "authorId": "8778335",
                        "name": "Lennart Bamberg"
                    },
                    {
                        "authorId": "143742944",
                        "name": "E. Bondarev"
                    },
                    {
                        "authorId": "2070227931",
                        "name": "Orlando Moreira"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We introduce these settings because previous works (Renda et al., 2020; Le & Hua, 2021; Wang et al., 2021a; 2023) have showed that retraining LR has a great impact on the final performance.",
                "Recent papers (Renda et al., 2020; Le & Hua, 2021) report an interesting phenomenon: During retraining, a larger learning rate (LR) helps achieve a significantly better final performance, empowering the two baseline methods, random pruning and magnitude pruning, to match or beat many more complex\u2026",
                "Recent papers (Renda et al., 2020; Le & Hua, 2021) report an interesting phenomenon: During retraining, a larger learning rate (LR) helps achieve a significantly better final performance, empowering the two baseline methods, random pruning and magnitude pruning, to match or beat many more complex pruning algorithms."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c0d6c6427f0ef27c36b52d45503921166c821120",
                "externalIds": {
                    "DBLP": "conf/iclr/Wang023a",
                    "ArXiv": "2207.12534",
                    "CorpusId": 257353246
                },
                "corpusId": 257353246,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c0d6c6427f0ef27c36b52d45503921166c821120",
                "title": "Trainability Preserving Neural Pruning",
                "abstract": "Many recent works have shown trainability plays a central role in neural network pruning -- unattended broken trainability can lead to severe under-performance and unintentionally amplify the effect of retraining learning rate, resulting in biased (or even misinterpreted) benchmark results. This paper introduces trainability preserving pruning (TPP), a scalable method to preserve network trainability against pruning, aiming for improved pruning performance and being more robust to retraining hyper-parameters (e.g., learning rate). Specifically, we propose to penalize the gram matrix of convolutional filters to decorrelate the pruned filters from the retained filters. In addition to the convolutional layers, per the spirit of preserving the trainability of the whole network, we also propose to regularize the batch normalization parameters (scale and bias). Empirical studies on linear MLP networks show that TPP can perform on par with the oracle trainability recovery scheme. On nonlinear ConvNets (ResNet56/VGG19) on CIFAR10/100, TPP outperforms the other counterpart approaches by an obvious margin. Moreover, results on ImageNet-1K with ResNets suggest that TPP consistently performs more favorably against other top-performing structured pruning approaches. Code: https://github.com/MingSun-Tse/TPP.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2113269100",
                        "name": "Huan Wang"
                    },
                    {
                        "authorId": "2156255943",
                        "name": "Yun Fu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Previous work discovers winning tickets in LMs via either unstructured (Frankle and Carbin, 2019; Renda et al., 2020; Chen et al., 2020) or structured pruning techniques (Michel et al., 2019; Prasanna et al., 2020; Chen et al., 2020).",
                "Previous work discovers winning tickets in LMs via either unstructured (Frankle and Carbin, 2019; Renda et al., 2020; Chen et al., 2020) or structured pruning techniques (Michel et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a2ecde1844d19bf4610ad145f36667b2701b78b0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-09638",
                    "ArXiv": "2207.09638",
                    "DOI": "10.48550/arXiv.2207.09638",
                    "CorpusId": 250699061
                },
                "corpusId": 250699061,
                "publicationVenue": {
                    "id": "640a5acb-a481-4a3e-a751-1eb880600a99",
                    "name": "Natural Language Processing and Chinese Computing",
                    "type": "conference",
                    "alternate_names": [
                        "NLPCC",
                        "Nat Lang Process Chin Comput"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a2ecde1844d19bf4610ad145f36667b2701b78b0",
                "title": "Doge Tickets: Uncovering Domain-general Language Models by Playing Lottery Tickets",
                "abstract": "Over-parameterized models, typically pretrained language models (LMs), have shown an appealing expressive power due to their small learning bias. However, the huge learning capacity of LMs can also lead to large learning variance. In a pilot study, we find that, when faced with multiple domains, a critical portion of parameters behave unexpectedly in a domain-specific manner while others behave in a domain-general one. Motivated by this phenomenon, we for the first time posit that domain-general parameters can underpin a domain-general LM that can be derived from the original LM. To uncover the domain-general LM, we propose to identify domain-general parameters by playing lottery tickets (dubbed doge tickets). In order to intervene the lottery, we propose a domain-general score, which depicts how domain-invariant a parameter is by associating it with the variance. Comprehensive experiments are conducted on the Amazon, Mnli and OntoNotes datasets. The results show that the doge tickets obtains an improved out-of-domain generalization in comparison with a range of competitive baselines. Analysis results further hint the existence of domain-general parameters and the performance consistency of doge tickets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2143685866",
                        "name": "Yi Yang"
                    },
                    {
                        "authorId": "145107889",
                        "name": "Chen Zhang"
                    },
                    {
                        "authorId": "2894465",
                        "name": "Benyou Wang"
                    },
                    {
                        "authorId": "2151679983",
                        "name": "Dawei Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[7]: traditional (fine-tuning [11], gradual magnitude pruning [37]), rewinding lottery-tickets (weight-rewinding [8], learning-rate rewinding [30]), and initialization lottery-tickets (edgepopup [27], biprop [6]).",
                "Namely, lottery-ticket style pruning methods [8, 30, 27, 6] were able to provide robustness gains on CIFAR-10-C [7]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "626935a1cb28ced88036ee2551fccc023709e939",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-04075",
                    "ArXiv": "2207.04075",
                    "DOI": "10.48550/arXiv.2207.04075",
                    "CorpusId": 250426113
                },
                "corpusId": 250426113,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/626935a1cb28ced88036ee2551fccc023709e939",
                "title": "Models Out of Line: A Fourier Lens on Distribution Shift Robustness",
                "abstract": "Improving the accuracy of deep neural networks (DNNs) on out-of-distribution (OOD) data is critical to an acceptance of deep learning (DL) in real world applications. It has been observed that accuracies on in-distribution (ID) versus OOD data follow a linear trend and models that outperform this baseline are exceptionally rare (and referred to as\"effectively robust\"). Recently, some promising approaches have been developed to improve OOD robustness: model pruning, data augmentation, and ensembling or zero-shot evaluating large pretrained models. However, there still is no clear understanding of the conditions on OOD data and model properties that are required to observe effective robustness. We approach this issue by conducting a comprehensive empirical study of diverse approaches that are known to impact OOD robustness on a broad range of natural and synthetic distribution shifts of CIFAR-10 and ImageNet. In particular, we view the\"effective robustness puzzle\"through a Fourier lens and ask how spectral properties of both models and OOD data influence the corresponding effective robustness. We find this Fourier lens offers some insight into why certain robust models, particularly those from the CLIP family, achieve OOD robustness. However, our analysis also makes clear that no known metric is consistently the best explanation (or even a strong explanation) of OOD robustness. Thus, to aid future research into the OOD puzzle, we address the gap in publicly-available models with effective robustness by introducing a set of pretrained models--RobustNets--with varying levels of OOD robustness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1405260934",
                        "name": "Sara Fridovich-Keil"
                    },
                    {
                        "authorId": "41053241",
                        "name": "Brian Bartoldson"
                    },
                    {
                        "authorId": "7753616",
                        "name": "James Diffenderfer"
                    },
                    {
                        "authorId": "1749353",
                        "name": "B. Kailkhura"
                    },
                    {
                        "authorId": "145466013",
                        "name": "P. Bremer"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c6640cbf85c7cafc41c50a95026ccf7292181061",
                "externalIds": {
                    "DBLP": "conf/urai/LeeEC22",
                    "DOI": "10.1109/ur55393.2022.9826290",
                    "CorpusId": 250580826
                },
                "corpusId": 250580826,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c6640cbf85c7cafc41c50a95026ccf7292181061",
                "title": "A Novel Filter Pruning Algorithm for Vision Tasks based on Kernel Grouping",
                "abstract": "Although the size and the computation cost of the state of the art deep learning models are tremendously large, they run without any problem when implemented on computers thanks to the remarkable enhancements and advancements of computers. However, the problem is likely to be faced when the need for deploying them on mobile platforms arises. Model compression techniques such as filter pruning or knowledge distillation help to reduce the size of deep learning models. However the conventional methods contain sorting algorithms therefore they cannot be applied to models that have reshaping layers like involution. In this research, we revisit a model compression algorithm named Model Diet that can be both applied to involution and convolution models. Furthermore, we present its application on two different tasks, image segmentation and depth estimation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108693503",
                        "name": "Jongmin Lee"
                    },
                    {
                        "authorId": "3015996",
                        "name": "A. Elibol"
                    },
                    {
                        "authorId": "1771640",
                        "name": "N. Chong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Parameter magnitude is an effective importance metric for model pruning (Han et al., 2015b;a; Paganini & Forde, 2020; Zhu & Gupta, 2018; Renda et al., 2020; Zafrir et al., 2021).",
                "When the context is clear, we simply write S.\nParameter magnitude is an effective importance metric for model pruning (Han et al., 2015b;a; Paganini & Forde, 2020; Zhu & Gupta, 2018; Renda et al., 2020; Zafrir et al., 2021)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "76d40153acfbb35a7eb8272a4215854cafa10e78",
                "externalIds": {
                    "DBLP": "conf/icml/ZhangZLBHCZ22",
                    "ArXiv": "2206.12562",
                    "DOI": "10.48550/arXiv.2206.12562",
                    "CorpusId": 250072480
                },
                "corpusId": 250072480,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/76d40153acfbb35a7eb8272a4215854cafa10e78",
                "title": "PLATON: Pruning Large Transformer Models with Upper Confidence Bound of Weight Importance",
                "abstract": "Large Transformer-based models have exhibited superior performance in various natural language processing and computer vision tasks. However, these models contain enormous amounts of parameters, which restrict their deployment to real-world applications. To reduce the model size, researchers prune these models based on the weights' importance scores. However, such scores are usually estimated on mini-batches during training, which incurs large variability/uncertainty due to mini-batch sampling and complicated training dynamics. As a result, some crucial weights could be pruned by commonly used pruning methods because of such uncertainty, which makes training unstable and hurts generalization. To resolve this issue, we propose PLATON, which captures the uncertainty of importance scores by upper confidence bound (UCB) of importance estimation. In particular, for the weights with low importance scores but high uncertainty, PLATON tends to retain them and explores their capacity. We conduct extensive experiments with several Transformer-based models on natural language understanding, question answering and image classification to validate the effectiveness of PLATON. Results demonstrate that PLATON manifests notable improvement under different sparsity levels. Our code is publicly available at https://github.com/QingruZhang/PLATON.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "153441799",
                        "name": "Qingru Zhang"
                    },
                    {
                        "authorId": "52194893",
                        "name": "Simiao Zuo"
                    },
                    {
                        "authorId": "98703980",
                        "name": "Chen Liang"
                    },
                    {
                        "authorId": "1645394981",
                        "name": "Alexander W. Bukharin"
                    },
                    {
                        "authorId": "50462546",
                        "name": "Pengcheng He"
                    },
                    {
                        "authorId": "2109136147",
                        "name": "Weizhu Chen"
                    },
                    {
                        "authorId": "36345161",
                        "name": "T. Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026covers multiple ways to make use of sparsity during and after model training including static and dynamic sparsity (e.g., \u03b2Lasso (Neyshabur, 2020)), iterative hard thresholding (e.g., Lottery Ticket Hypothesis with various pruning strategies (Frankle & Carbin, 2018; Renda et al., 2020)) and others.",
                ", Lottery Ticket Hypothesis with various pruning strategies (Frankle & Carbin, 2018; Renda et al., 2020)) and others."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dfacf0c04048c0a8c105976e9d2237ee269843ce",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-10915",
                    "ArXiv": "2206.10915",
                    "DOI": "10.48550/arXiv.2206.10915",
                    "CorpusId": 249926776
                },
                "corpusId": 249926776,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dfacf0c04048c0a8c105976e9d2237ee269843ce",
                "title": "Understanding the effect of sparsity on neural networks robustness",
                "abstract": "This paper examines the impact of static sparsity on the robustness of a trained network to weight perturbations, data corruption, and adversarial examples. We show that, up to a certain sparsity achieved by increasing network width and depth while keeping the network capacity \ufb01xed, sparsi\ufb01ed networks consistently match and often outperform their initially dense versions. Robustness and accuracy decline simultaneously for very high sparsity due to loose connectivity between network layers. Our \ufb01ndings show that a rapid robustness drop caused by network compression observed in the literature is due to a reduced network capacity rather than sparsity. after training, network sparsi\ufb01cation does not hurt or even improves robustness to a certain sparsity compared to a dense network of the same capacity. Robustness and accuracy decline simultaneously for very high sparsity due to loose connectivity between network layers. We show that our hypothesis holds when introducing sparsity by increasing network width and depth in separate experiments, applied before and after training. These \ufb01ndings show that a rapid robustness drop caused by network compression observed in the literature is due to a reduced network capacity rather than sparsity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2172099917",
                        "name": "Lukas Timpl"
                    },
                    {
                        "authorId": "8775829",
                        "name": "R. Entezari"
                    },
                    {
                        "authorId": "2812848",
                        "name": "Hanie Sedghi"
                    },
                    {
                        "authorId": "3007442",
                        "name": "Behnam Neyshabur"
                    },
                    {
                        "authorId": "2731697",
                        "name": "O. Saukh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2022 Learning rate rewinding: retrains the pruned model for T \u2212 t epochs from the final parameters, but reuse the learning rate schedule from the iteration t at the early training phase (Renda et al., 2020).",
                ", 2020), it will still be too hard to distinguish from the irregular fluctuations of test accuracy in pruning cases (Frankle & Carbin, 2019; Liu et al., 2019; Renda et al., 2020).",
                "\u2026in test error around the interpolation point of dense models is reported on the noiseless CIFAR dataset (Nakkiran et al., 2020), it will still be too hard to distinguish from the irregular fluctuations of test accuracy in pruning cases (Frankle & Carbin, 2019; Liu et al., 2019; Renda et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f6b0d59776565e7f688ad9b84f52da1653ba021f",
                "externalIds": {
                    "ArXiv": "2206.08684",
                    "DBLP": "journals/corr/abs-2206-08684",
                    "DOI": "10.48550/arXiv.2206.08684",
                    "CorpusId": 249848068
                },
                "corpusId": 249848068,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f6b0d59776565e7f688ad9b84f52da1653ba021f",
                "title": "Sparse Double Descent: Where Network Pruning Aggravates Overfitting",
                "abstract": "People usually believe that network pruning not only reduces the computational cost of deep networks, but also prevents overfitting by decreasing model capacity. However, our work surprisingly discovers that network pruning sometimes even aggravates overfitting. We report an unexpected sparse double descent phenomenon that, as we increase model sparsity via network pruning, test performance first gets worse (due to overfitting), then gets better (due to relieved overfitting), and gets worse at last (due to forgetting useful information). While recent studies focused on the deep double descent with respect to model overparameterization, they failed to recognize that sparsity may also cause double descent. In this paper, we have three main contributions. First, we report the novel sparse double descent phenomenon through extensive experiments. Second, for this phenomenon, we propose a novel learning distance interpretation that the curve of $\\ell_{2}$ learning distance of sparse models (from initialized parameters to final parameters) may correlate with the sparse double descent curve well and reflect generalization better than minima flatness. Third, in the context of sparse double descent, a winning ticket in the lottery ticket hypothesis surprisingly may not always win.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109504295",
                        "name": "Zhengqi He"
                    },
                    {
                        "authorId": "30014947",
                        "name": "Zeke Xie"
                    },
                    {
                        "authorId": "2171121140",
                        "name": "Quanzhi Zhu"
                    },
                    {
                        "authorId": "70565757",
                        "name": "Zengchang Qin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This observation is aligned to previous works [18], where the authors observe the performance improvement when pruning.",
                "Related work [18] shows that iterative pruning achieves better performance than one-shot pruning."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ceb6290727c57b2405eb929e58c54bd4a801f351",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-07422",
                    "ArXiv": "2206.07422",
                    "DOI": "10.48550/arXiv.2206.07422",
                    "CorpusId": 249674477
                },
                "corpusId": 249674477,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ceb6290727c57b2405eb929e58c54bd4a801f351",
                "title": "Deep Neural Network Pruning for Nuclei Instance Segmentation in Hematoxylin & Eosin-Stained Histological Images",
                "abstract": ". Recently, pruning deep neural networks (DNNs) has received a lot of attention for improving accuracy and generalization power, re-ducing network size, and increasing inference speed on specialized hard-wares. Although pruning was mainly tested on computer vision tasks, its application in the context of medical image analysis has hardly been explored. This work investigates the impact of well-known pruning techniques, namely layer-wise and network-wide magnitude pruning, on the nuclei instance segmentation performance in histological images. Our utilised instance segmentation model consists of two main branches: (1) a semantic segmentation branch, and (2) a deep regression branch. We investigate the impact of weight pruning on the performance of both branches separately, and on the \ufb01nal nuclei instance segmentation re-sult. Evaluated on two publicly available datasets, our results show that layer-wise pruning delivers slightly better performance than network-wide pruning for small compression ratios (CRs) while for large CRs, network-wide pruning yields superior performance. For semantic segmentation, deep regression and \ufb01nal instance segmentation, 93.75 %, 95 %, and 80 % of the model weights can be pruned by layer-wise pruning with less than 2 % reduction in the performance of respective models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9545777",
                        "name": "A. Mahbod"
                    },
                    {
                        "authorId": "8775829",
                        "name": "R. Entezari"
                    },
                    {
                        "authorId": "6314165",
                        "name": "I. Ellinger"
                    },
                    {
                        "authorId": "2731697",
                        "name": "O. Saukh"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2019), weight rewinding, and fine-tuning (Renda et al., 2020)."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fead7c8911d4177a90b66193191bdf940443b526",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-06563",
                    "ArXiv": "2206.06563",
                    "DOI": "10.48550/arXiv.2206.06563",
                    "CorpusId": 249642387
                },
                "corpusId": 249642387,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fead7c8911d4177a90b66193191bdf940443b526",
                "title": "Zeroth-Order Topological Insights into Iterative Magnitude Pruning",
                "abstract": "Modern-day neural networks are famously large, yet also highly redundant and compressible; there exist numerous pruning strategies in the deep learning literature that yield over 90% sparser sub-networks of fully-trained, dense architectures while still maintaining their original accuracies. Amongst these many methods though -- thanks to its conceptual simplicity, ease of implementation, and efficacy -- Iterative Magnitude Pruning (IMP) dominates in practice and is the de facto baseline to beat in the pruning community. However, theoretical explanations as to why a simplistic method such as IMP works at all are few and limited. In this work, we leverage the notion of persistent homology to gain insights into the workings of IMP and show that it inherently encourages retention of those weights which preserve topological information in a trained network. Subsequently, we also provide bounds on how much different networks can be pruned while perfectly preserving their zeroth order topological features, and present a modified version of IMP to do the same.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1398245246",
                        "name": "Aishwarya H. Balwani"
                    },
                    {
                        "authorId": "82280359",
                        "name": "J. Krzyston"
                    }
                ]
            }
        },
        {
            "contexts": [
                "After that, LR-Rewinding is applied.",
                "Once the final pruning rate is reached, the network is re-trained following a warm-restart schedule, which can be called LR-Rewinding [17]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "df7388d50f981454629b4e898a3dd31e2609c157",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-06255",
                    "ArXiv": "2206.06255",
                    "DOI": "10.1007/978-3-031-16281-7_52",
                    "CorpusId": 249625599
                },
                "corpusId": 249625599,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/df7388d50f981454629b4e898a3dd31e2609c157",
                "title": "Energy Consumption Analysis of pruned Semantic Segmentation Networks on an Embedded GPU",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "146940144",
                        "name": "Hugo Tessier"
                    },
                    {
                        "authorId": "144916029",
                        "name": "Vincent Gripon"
                    },
                    {
                        "authorId": "65982563",
                        "name": "Mathieu L'eonardon"
                    },
                    {
                        "authorId": "2409852",
                        "name": "M. Arzel"
                    },
                    {
                        "authorId": "2060222860",
                        "name": "David Bertrand"
                    },
                    {
                        "authorId": "3312711",
                        "name": "T. Hannagan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(Frankle et al., 2019; Renda et al., 2020) further scale up LTH to larger datasets and networks by weight rewinding techniques that re-initialize the subnetworks to the weight from the early training stage instead of scratch."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "637e093fb97863fc9bd4b1b69722bbe70804e4e0",
                "externalIds": {
                    "ArXiv": "2206.04762",
                    "DBLP": "journals/corr/abs-2206-04762",
                    "DOI": "10.48550/arXiv.2206.04762",
                    "CorpusId": 249605768
                },
                "corpusId": 249605768,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/637e093fb97863fc9bd4b1b69722bbe70804e4e0",
                "title": "Data-Efficient Double-Win Lottery Tickets from Robust Pre-training",
                "abstract": "Pre-training serves as a broadly adopted starting point for transfer learning on various downstream tasks. Recent investigations of lottery tickets hypothesis (LTH) demonstrate such enormous pre-trained models can be replaced by extremely sparse subnetworks (a.k.a. matching subnetworks) without sacrificing transferability. However, practical security-crucial applications usually pose more challenging requirements beyond standard transfer, which also demand these subnetworks to overcome adversarial vulnerability. In this paper, we formulate a more rigorous concept, Double-Win Lottery Tickets, in which a located subnetwork from a pre-trained model can be independently transferred on diverse downstream tasks, to reach BOTH the same standard and robust generalization, under BOTH standard and adversarial training regimes, as the full pre-trained model can do. We comprehensively examine various pre-training mechanisms and find that robust pre-training tends to craft sparser double-win lottery tickets with superior performance over the standard counterparts. For example, on downstream CIFAR-10/100 datasets, we identify double-win matching subnetworks with the standard, fast adversarial, and adversarial pre-training from ImageNet, at 89.26%/73.79%, 89.26%/79.03%, and 91.41%/83.22% sparsity, respectively. Furthermore, we observe the obtained double-win lottery tickets can be more data-efficient to transfer, under practical data-limited (e.g., 1% and 10%) downstream schemes. Our results show that the benefits from robust pre-training are amplified by the lottery ticket scheme, as well as the data-limited transfer setting. Codes are available at https://github.com/VITA-Group/Double-Win-LTH.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2109338656",
                        "name": "Zhenyu (Allen) Zhang"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "37873860",
                        "name": "Yang Zhang"
                    },
                    {
                        "authorId": "2122374354",
                        "name": "Shiyu Chang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Fine-Tuned Deep Model Fine-tuning is a process that takes a model that has already been trained (pre-trained) for one task and returns it or tweaks the same model to perform a classification task [43,44].",
                "Fine-tuning is a process that takes a model that has already been trained (pre-trained) for one task and returns it or tweaks the same model to perform a classification task [43,44]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "03236bc1fe9bf67b09d673ea4cea46544a8ad2c9",
                "externalIds": {
                    "DBLP": "journals/entropy/RasoolIBASYE22",
                    "PubMedCentral": "9222774",
                    "DOI": "10.3390/e24060799",
                    "CorpusId": 249593156,
                    "PubMed": "35741521"
                },
                "corpusId": 249593156,
                "publicationVenue": {
                    "id": "8270cfe1-3713-4325-a7bd-c6a87eed889e",
                    "name": "Entropy",
                    "type": "journal",
                    "issn": "1099-4300",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-155606",
                    "alternate_urls": [
                        "http://www.mdpi.com/journal/entropy/",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-155606",
                        "https://www.mdpi.com/journal/entropy"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/03236bc1fe9bf67b09d673ea4cea46544a8ad2c9",
                "title": "A Hybrid Deep Learning Model for Brain Tumour Classification",
                "abstract": "A brain tumour is one of the major reasons for death in humans, and it is the tenth most common type of tumour that affects people of all ages. However, if detected early, it is one of the most treatable types of tumours. Brain tumours are classified using biopsy, which is not usually performed before definitive brain surgery. An image classification technique for tumour diseases is important for accelerating the treatment process and avoiding surgery and errors from manual diagnosis by radiologists. The advancement of technology and machine learning (ML) can assist radiologists in tumour diagnostics using magnetic resonance imaging (MRI) images without invasive procedures. This work introduced a new hybrid CNN-based architecture to classify three brain tumour types through MRI images. The method suggested in this paper uses hybrid deep learning classification based on CNN with two methods. The first method combines a pre-trained Google-Net model of the CNN algorithm for feature extraction with SVM for pattern classification. The second method integrates a finely tuned Google-Net with a soft-max classifier. The proposed approach was evaluated using MRI brain images that contain a total of 1426 glioma images, 708 meningioma images, 930 pituitary tumour images, and 396 normal brain images. The reported results showed that an accuracy of 93.1% was achieved from the finely tuned Google-Net model. However, the synergy of Google-Net as a feature extractor with an SVM classifier improved recognition accuracy to 98.1%.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "16301543",
                        "name": "Mohammed Rasool"
                    },
                    {
                        "authorId": "2707775",
                        "name": "N. A. Ismail"
                    },
                    {
                        "authorId": "3151219",
                        "name": "W. Boulila"
                    },
                    {
                        "authorId": "36383788",
                        "name": "Adel Ammar"
                    },
                    {
                        "authorId": "1967709",
                        "name": "Hussein Samma"
                    },
                    {
                        "authorId": "121617615",
                        "name": "W. Yafooz"
                    },
                    {
                        "authorId": "1411372480",
                        "name": "Abdel-Hamid M. Emara"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0c7400b8c2e58aa68b06b98dc4f13ff15c3e1c49",
                "externalIds": {
                    "ArXiv": "2205.13574",
                    "DBLP": "journals/corr/abs-2205-13574",
                    "DOI": "10.48550/arXiv.2205.13574",
                    "CorpusId": 249152254
                },
                "corpusId": 249152254,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0c7400b8c2e58aa68b06b98dc4f13ff15c3e1c49",
                "title": "Pruning has a disparate impact on model accuracy",
                "abstract": "Network pruning is a widely-used compression technique that is able to significantly scale down overparameterized models with minimal loss of accuracy. This paper shows that pruning may create or exacerbate disparate impacts. The paper sheds light on the factors to cause such disparities, suggesting differences in gradient norms and distance to decision boundary across groups to be responsible for this critical issue. It analyzes these factors in detail, providing both theoretical and empirical support, and proposes a simple, yet effective, solution that mitigates the disparate impacts caused by pruning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2055333757",
                        "name": "Cuong D. Tran"
                    },
                    {
                        "authorId": "2141569789",
                        "name": "Ferdinando Fioretto"
                    },
                    {
                        "authorId": "2109169708",
                        "name": "Jung-Eun Kim"
                    },
                    {
                        "authorId": "1767752585",
                        "name": "Rakshit Naidu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Previous work (Renda et al.,\n2020) presents evidence that rewinding remaining weights to earlier learned values may be beneficial for compressibility.",
                "In the standard pruning scenario (Renda et al., 2020; Han et al., 2015a), training simply resumes with the remaining weights after each iteration of pruning.",
                "Previous work (Renda et al., 2020) presents evidence that rewinding remaining weights to earlier learned values may be beneficial for compressibility."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "294a315631d8a76697ecc9fd9cd0ed4bc5ce90fd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-12694",
                    "ArXiv": "2205.12694",
                    "DOI": "10.48550/arXiv.2205.12694",
                    "CorpusId": 249063170
                },
                "corpusId": 249063170,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/294a315631d8a76697ecc9fd9cd0ed4bc5ce90fd",
                "title": "Train Flat, Then Compress: Sharpness-Aware Minimization Learns More Compressible Models",
                "abstract": "Model compression by way of parameter pruning, quantization, or distillation has recently gained popularity as an approach for reducing the computational requirements of modern deep neural network models for NLP. Inspired by prior works suggesting a connection between simpler, more generalizable models and those that lie within wider loss basins, we hypothesize that optimizing for flat minima should lead to simpler parameterizations and thus more compressible models. We propose to combine sharpness-aware minimization (SAM) with various task-specific model compression methods, including iterative magnitude pruning (IMP), structured pruning with a distillation objective, and post-training dynamic quantization. Empirically, we show that optimizing for flatter minima consistently leads to greater compressibility of parameters compared to vanilla Adam when fine-tuning BERT models, with little to no loss in accuracy on the GLUE text classification and SQuAD question answering benchmarks. Moreover, SAM finds superior winning tickets during IMP that 1) are amenable to vanilla Adam optimization, and 2) transfer more effectively across tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2166313248",
                        "name": "Clara Na"
                    },
                    {
                        "authorId": "47613860",
                        "name": "Sanket Vaibhav Mehta"
                    },
                    {
                        "authorId": "2268272",
                        "name": "Emma Strubell"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Later investigations point out [19, 67] that the original LTH can not scale up to larger networks and datasets unless leveraging the weight rewinding techniques [19, 67]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6735752e3349fd2ce5dd67eaa63ab4c246617410",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-11819",
                    "ArXiv": "2205.11819",
                    "DOI": "10.1109/CVPR52688.2022.00068",
                    "CorpusId": 249017949
                },
                "corpusId": 249017949,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6735752e3349fd2ce5dd67eaa63ab4c246617410",
                "title": "Quarantine: Sparsity Can Uncover the Trojan Attack Trigger for Free",
                "abstract": "Trojan attacks threaten deep neural networks (DNNs) by poisoning them to behave normally on most samples, yet to produce manipulated results for inputs attached with a particular trigger. Several works attempt to detect whether a given DNN has been injected with a specific trigger during the training. In a parallel line of research, the lottery ticket hypothesis reveals the existence of sparse sub-networks which are capable of reaching competitive performance as the dense network after independent training. Connecting these two dots, we investigate the problem of Trojan DNN detection from the brand new lens of sparsity, even when no clean training data is available. Our crucial observation is that the Trojan features are significantly more stable to network pruning than benign features. Leveraging that, we propose a novel Trojan network detection regime: first locating a \u201cwinning Trojan lottery ticket\u201d which preserves nearly full Trojan information yet only chance-level performance on clean inputs; then recovering the trigger embedded in this already isolated sub-network. Extensive experiments on various datasets, i.e., CIFAR-10, CIFAR-100, and ImageNet, with different network architectures, i.e., VGG-16, ResNet-18, ResNet-20s, and DenseNet-100 demonstrate the effectiveness of our proposal. Codes are available at https://github.com/VITA-Group/Backdoor-LTH.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2109338656",
                        "name": "Zhenyu (Allen) Zhang"
                    },
                    {
                        "authorId": "2155369380",
                        "name": "Yihua Zhang"
                    },
                    {
                        "authorId": "2122374354",
                        "name": "Shiyu Chang"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[159], Le and Hua [160] show that fine-tuning a pruned network with a learning rate schedule rewound to earlier stages in training outperforms classical fine-tuning of sparse networks with small learning rates."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "de3ccbae89aa1462b207888f232ba82b8398f6e7",
                "externalIds": {
                    "ArXiv": "2205.08099",
                    "DBLP": "journals/corr/abs-2205-08099",
                    "DOI": "10.1007/s10462-023-10489-1",
                    "CorpusId": 248834207
                },
                "corpusId": 248834207,
                "publicationVenue": {
                    "id": "ea8553fe-2467-4367-afee-c4deb3754820",
                    "name": "Artificial Intelligence Review",
                    "type": "journal",
                    "alternate_names": [
                        "Artif Intell Rev"
                    ],
                    "issn": "0269-2821",
                    "url": "https://link.springer.com/journal/10462"
                },
                "url": "https://www.semanticscholar.org/paper/de3ccbae89aa1462b207888f232ba82b8398f6e7",
                "title": "Dimensionality reduced training by pruning and freezing parts of a deep neural network: a survey",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2029488873",
                        "name": "Paul Wimmer"
                    },
                    {
                        "authorId": "144442281",
                        "name": "Jens Mehnert"
                    },
                    {
                        "authorId": "2063161",
                        "name": "A. Condurache"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[13], n\u00f6ral a\u011flar\u0131n budanmas\u0131nda en yayg\u0131n kullan\u0131lan \u201cince ayar\u201d y\u00f6ntemi yerine \u201cgeri sarma\u201d y\u00f6ntemini \u00f6nermi\u015ftir."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0e64698cac49bee13d6dd15d77ab99d656729c0b",
                "externalIds": {
                    "DBLP": "conf/siu/DoganUU22",
                    "DOI": "10.1109/SIU55565.2022.9864848",
                    "CorpusId": 251936955
                },
                "corpusId": 251936955,
                "publicationVenue": {
                    "id": "c5f8f5c8-58d0-46aa-bbb5-44ce60315635",
                    "name": "Signal Processing and Communications Applications Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Signal Process Commun Appl Conf",
                        "SIU"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0e64698cac49bee13d6dd15d77ab99d656729c0b",
                "title": "Using Deep Compression on PyTorch Models for Autonomous Systems",
                "abstract": "Applications of artificial neural networks on low-cost embedded systems and microcontrollers (MCUs), has recently been attracting more attention than ever. Since MCUs have limited memory capacity as well as limited compute-speed compared to workstations, employment of current deep learning algorithms on MCUs becomes more practical with the help of model compression. This makes MCUs common and practical alternative solution for autonomous systems. In this paper, we add model compression, specifically Deep Compression, to an existing work, which efficiently deploys PyTorch models on MCUs, in order to increase neural network speed and save electrical power. First, we prune the weight values close to zero in convolutional and fully connected layers. Secondly, the remaining weights and activations are quantized to 8-bit integers from 32-bit floating-point. Finally, forward pass functions are compressed using special data structures for sparse matrices, which store only nonzero weights. In the case of the LeNet-5 model, the memory footprint was reduced by 12.5x, and the inference speed was boosted by 2.6x.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2061320570",
                        "name": "E. Dogan"
                    },
                    {
                        "authorId": "145968116",
                        "name": "H. F. Ugurdag"
                    },
                    {
                        "authorId": "2065243138",
                        "name": "Hasan Unlu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, some pruning techniques may result in more training efforts during model building stage, such as methods that are based on the procedures of train, prune and fine-tune/re-train [34], [35]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0af1e03219f841852d558172661ecc60697ef55a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-03824",
                    "ArXiv": "2205.03824",
                    "DOI": "10.48550/arXiv.2205.03824",
                    "CorpusId": 248571582
                },
                "corpusId": 248571582,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0af1e03219f841852d558172661ecc60697ef55a",
                "title": "A Survey on AI Sustainability: Emerging Trends on Learning Algorithms and Research Challenges",
                "abstract": "Artificial Intelligence (AI) is a fast-growing research and development (R&D) discipline which is attracting increasing attention because of its promises to bring vast benefits for consumers and businesses, with considerable benefits promised in productivity growth and innovation. To date it has reported significant accomplishments in many areas that have been deemed as challenging for machines, ranging from computer vision, natural language processing, audio analysis to smart sensing and many others. The technical trend in realizing the successes has been towards increasing complex and large size AI models so as to solve more complex problems at superior performance and robustness. This rapid progress, however, has taken place at the expense of substantial environmental costs and resources. Besides, debates on the societal impacts of AI, such as fairness, safety and privacy, have continued to grow in intensity. These issues have presented major concerns pertaining to the sustainable development of AI. In this work, we review major trends in machine learning approaches that can address the sustainability problem of AI. Specifically, we examine emerging AI methodologies and algorithms for addressing the sustainability issue of AI in two major aspects, i.e., environmental sustainability and social sustainability of AI. We will also highlight the major limitations of existing studies and propose potential research challenges and directions for the development of next generation of sustainable AI techniques. We believe that this technical review can help to promote a sustainable development of AI R&D activities for the research community.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48354147",
                        "name": "Zhenghua Chen"
                    },
                    {
                        "authorId": "1390606776",
                        "name": "Min Wu"
                    },
                    {
                        "authorId": "143962358",
                        "name": "Alvin Chan"
                    },
                    {
                        "authorId": "2108674591",
                        "name": "Xiaoli Li"
                    },
                    {
                        "authorId": "144848119",
                        "name": "Y. Ong"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2021; Fischer & Burkholz, 2022) and whether LTs are identifiable by contemporary pruning algorithms to solve complex problems with large scale architectures (Frankle et al., 2020; Renda et al., 2020).",
                "Most LT experiments are conducted in the context of image classification and thus rely heavily on pruning convolutional and residual neural network architectures to reduce the number of trainable parameters of a neural network (LeCun et al., 1990a; Mozer & Smolensky, 1989; Han et al., 2015; Frankle & Carbin, 2019; Srinivas & Babu, 2016; Lee et al., 2020; You et al., 2020; Frankle et al., 2020; Renda et al., 2020; Liu et al., 2021a; Liu et al.; Weigend et al., 1991; Savarese et al., 2020a; Chen et al., 2021c; Savarese et al., 2020b; LeCun et al., 1990b; Hassibi & Stork, 1992; Dong et al., 2017; Li et al., 2017; Molchanov et al., 2017; Zhang et al., 2021c).",
                "\u2026in finding task specific computational neural network structures (Su et al., 2020; Ma et al., 2021; Fischer & Burkholz, 2022) and whether LTs are identifiable by contemporary pruning algorithms to solve complex problems with large scale architectures (Frankle et al., 2020; Renda et al., 2020).",
                "\u2026Mozer & Smolensky, 1989; Han et al., 2015; Frankle & Carbin, 2019; Srinivas & Babu, 2016; Lee et al., 2020; You et al., 2020; Frankle et al., 2020; Renda et al., 2020; Liu et al., 2021a; Liu et al.; Weigend et al., 1991; Savarese et al., 2020a; Chen et al., 2021c; Savarese et al., 2020b; LeCun et\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "993100a085dbda650fa7f2742bbebcc1c23e66b7",
                "externalIds": {
                    "DBLP": "conf/icml/Burkholz22",
                    "ArXiv": "2205.02343",
                    "DOI": "10.48550/arXiv.2205.02343",
                    "CorpusId": 248524888
                },
                "corpusId": 248524888,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/993100a085dbda650fa7f2742bbebcc1c23e66b7",
                "title": "Convolutional and Residual Networks Provably Contain Lottery Tickets",
                "abstract": "The Lottery Ticket Hypothesis continues to have a profound practical impact on the quest for small scale deep neural networks that solve modern deep learning tasks at competitive performance. These lottery tickets are identified by pruning large randomly initialized neural networks with architectures that are as diverse as their applica-tions. Yet, theoretical insights that attest their existence have been mostly focused on deep fully-connected feed forward networks with ReLU activation functions. We prove that also modern architectures consisting of convolutional and residual layers that can be equipped with almost arbitrary activation functions can contain lottery tickets with high probability.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8813467",
                        "name": "R. Burkholz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many pruning methods have been proposed to reduce the number of neural network parameters during training (LeCun et al., 1990a; Mozer & Smolensky, 1989; Han et al., 2015; Frankle & Carbin, 2019; Srinivas & Babu, 2016; Lee et al., 2020; You et al., 2020; Frankle et al., 2020; Renda et al., 2020; Liu et al., 2021a; Liu et al.; Weigend et al., 1991; Savarese et al., 2020a; Chen et al., 2021c) or thereafter (Savarese et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "613f0a3f6fad7fca07f50aaf49ad7d53ff4dce78",
                "externalIds": {
                    "ArXiv": "2205.02321",
                    "DBLP": "journals/corr/abs-2205-02321",
                    "DOI": "10.48550/arXiv.2205.02321",
                    "CorpusId": 248524809
                },
                "corpusId": 248524809,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/613f0a3f6fad7fca07f50aaf49ad7d53ff4dce78",
                "title": "Most Activation Functions Can Win the Lottery Without Excessive Depth",
                "abstract": "The strong lottery ticket hypothesis has highlighted the potential for training deep neural networks by pruning, which has inspired interesting practical and theoretical insights into how neural networks can represent functions. For networks with ReLU activation functions, it has been proven that a target network with depth $L$ can be approximated by the subnetwork of a randomly initialized neural network that has double the target's depth $2L$ and is wider by a logarithmic factor. We show that a depth $L+1$ network is sufficient. This result indicates that we can expect to find lottery tickets at realistic, commonly used depths while only requiring logarithmic overparametrization. Our novel construction approach applies to a large class of activation functions and is not limited to ReLUs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8813467",
                        "name": "R. Burkholz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While these methods can achieve superior theoretical compression rates (Renda et al., 2020), their use remains impractical without specialised hardware that can take advantage of sparsity (Han et al., 2016).",
                "While these methods can achieve superior theoretical compression rates (Renda et al., 2020), their use remains impractical without specialised hardware that can take advantage of sparsity (Han et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f38c4a9910951baa7ac67f869e2a6d7478751b63",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-06404",
                    "ArXiv": "2204.06404",
                    "DOI": "10.48550/arXiv.2204.06404",
                    "CorpusId": 248157483
                },
                "corpusId": 248157483,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f38c4a9910951baa7ac67f869e2a6d7478751b63",
                "title": "Receding Neuron Importances for Structured Pruning",
                "abstract": "Structured pruning efficiently compresses networks by identifying and removing unimportant neurons. While this can be elegantly achieved by applying sparsity-inducing regularisation on BatchNorm parameters, an L1 penalty would shrink all scaling factors rather than just those of superfluous neurons. To tackle this issue, we introduce a simple BatchNorm variation with bounded scaling parameters, based on which we design a novel regularisation term that suppresses only neurons with low importance. Under our method, the weights of unnecessary neurons effectively recede, producing a polarised bimodal distribution of importances. We show that neural networks trained this way can be pruned to a larger extent and with less deterioration. We one-shot prune VGG and ResNet architectures at different ratios on CIFAR and ImagenNet datasets. In the case of VGG-style networks, our method significantly outperforms existing approaches particularly under a severe pruning regime.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47633431",
                        "name": "M. \u0218uteu"
                    },
                    {
                        "authorId": "49813691",
                        "name": "Yike Guo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The third class of methods, perform the pruning while training [22, 23, 33, 55, 61, 64]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f7bd7fec504ef63839bdb43594111cffa06853f1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-02965",
                    "ArXiv": "2204.02965",
                    "DOI": "10.48550/arXiv.2204.02965",
                    "CorpusId": 247996981
                },
                "corpusId": 247996981,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f7bd7fec504ef63839bdb43594111cffa06853f1",
                "title": "LilNetX: Lightweight Networks with EXtreme Model Compression and Structured Sparsification",
                "abstract": "We introduce LilNetX, an end-to-end trainable technique for neural networks that enables learning models with specified accuracy-rate-computation trade-off. Prior works approach these problems one at a time and often require post-processing or multistage training which become less practical and do not scale very well for large datasets or architectures. Our method constructs a joint training objective that penalizes the self-information of network parameters in a reparameterized latent space to encourage small model size while also introducing priors to increase structured sparsity in the parameter space to reduce computation. We achieve up to 50% smaller model size and 98% model sparsity on ResNet-20 while retaining the same accuracy on the CIFAR-10 dataset as well as 35% smaller model size and 42% structured sparsity on ResNet-50 trained on ImageNet, when compared to existing state-of-the-art model compression methods. Code is available at https://github.com/Sharath-girish/LilNetX.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143720888",
                        "name": "Sharath Girish"
                    },
                    {
                        "authorId": "145428082",
                        "name": "Kamal Gupta"
                    },
                    {
                        "authorId": "2108498897",
                        "name": "Saurabh Singh"
                    },
                    {
                        "authorId": "1781242",
                        "name": "Abhinav Shrivastava"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Besides a few papers that discuss post-training pruning [74, 40], most existing studies such as [37, 75, 8, 71, 76] are implemented as in-training pruning."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b87eb6145081f435714c0743621e6d5a2a95e25f",
                "externalIds": {
                    "ArXiv": "2204.00783",
                    "DBLP": "journals/corr/abs-2204-00783",
                    "DOI": "10.48550/arXiv.2204.00783",
                    "CorpusId": 247939985
                },
                "corpusId": 247939985,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b87eb6145081f435714c0743621e6d5a2a95e25f",
                "title": "Paoding: Supervised Robustness-preserving Data-free Neural Network Pruning",
                "abstract": "When deploying pre-trained neural network models in real-world applications, model consumers often encounter resource-constraint platforms such as mobile and smart devices. They typically use the pruning technique to reduce the size and complexity of the model, generating a lighter one with less resource consumption. Nonetheless, most existing pruning methods are proposed with a premise that the model after being pruned has a chance to be \ufb01ne-tuned or even retrained based on the original training data. This may be unrealistic in practice, as the data controllers are often reluctant to provide their model consumers with the original data. In this work, we study the neural network pruning in the data-free context, aim-ing to yield lightweight models that are not only accurate in prediction but also robust against undesired inputs in open-world deployments. Considering the ab-sence of the \ufb01ne-tuning and retraining that can \ufb01x the mis-pruned units, we replace the traditional aggressive one-shot strategy with a conservative one that treats the pruning as a progressive process. We propose a pruning method based on stochastic optimization that uses robustness-related metrics to guide the pruning process. Our method is implemented as a Python package named P AODING and evaluated with a series of experiments on diverse neural network models. The experimental results show that it signi\ufb01cantly outperforms existing one-shot data-free pruning approaches in terms of robustness preservation and accuracy. pruning with of of the pruning request for and annealing to",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143654918",
                        "name": "M. H. Meng"
                    },
                    {
                        "authorId": "2729535",
                        "name": "Guangdong Bai"
                    },
                    {
                        "authorId": "2766305",
                        "name": "S. Teo"
                    },
                    {
                        "authorId": "2152487387",
                        "name": "J. Dong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Unstructured pruning is another major research direction, especially gaining popularity in the theory of Lottery Ticket Hypothesis (Frankle and Carbin, 2019; Zhou et al., 2019; Renda et al., 2020; Frankle et al., 2020; Chen et al., 2020a).",
                "Un- 535 structured pruning is another major research direc- 536 tion, especially gaining popularity in the theory 537 of Lottery Ticket Hypothesis (Frankle and Carbin, 538 2019; Zhou et al., 2019; Renda et al., 2020; Frankle 539 et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "86c101dc2924c9147957faebf8f46f3af184dce6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-00408",
                    "ACL": "2022.acl-long.107",
                    "ArXiv": "2204.00408",
                    "DOI": "10.48550/arXiv.2204.00408",
                    "CorpusId": 247922354
                },
                "corpusId": 247922354,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/86c101dc2924c9147957faebf8f46f3af184dce6",
                "title": "Structured Pruning Learns Compact and Accurate Models",
                "abstract": "The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi (Coarse- and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10X speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "67284811",
                        "name": "M. Xia"
                    },
                    {
                        "authorId": "49164966",
                        "name": "Zexuan Zhong"
                    },
                    {
                        "authorId": "50536468",
                        "name": "Danqi Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We put the emphasis on the efficiency of BN-folding for DNN acceleration by comparing the cost in terms of accuracy to reach similar acceleration with pruning and quantization methods.",
                "There are two main approaches to DNN inference acceleration: pruning and quantization.",
                "Pruning consists in removing elements of the graph defined by the DNN [Renda et al., 2020].",
                "To tackle this problem, many solutions for DNN acceleration have been developed including, but not limited to, pruning [Frankle and Carbin, 2018; Lin and others, 2020; Yvinec et al., 2021] and quantization [Zhao and others, 2019; Meller et al., 2019;\n\u2217Contact Author\nFinkelstein et al., 2019].",
                "The naive approach [Jacob and others, 2018] is widely documented and applied in DNN acceleration.",
                "Deep Neural Networks (DNNs) are ubiquitous in various subdomains of computer vision, e.g. in Image Classification [He et al., 2016], Object Detection [He et al., 2017] or Semantic Segmentation [Chen and others, 2017]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "951468300fd01168588d0e924bcc5221af419b1b",
                "externalIds": {
                    "DBLP": "conf/ijcai/YvinecDB22",
                    "ArXiv": "2203.14646",
                    "DOI": "10.48550/arXiv.2203.14646",
                    "CorpusId": 247761984
                },
                "corpusId": 247761984,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/951468300fd01168588d0e924bcc5221af419b1b",
                "title": "To Fold or Not to Fold: a Necessary and Sufficient Condition on Batch-Normalization Layers Folding",
                "abstract": "Batch-Normalization (BN) layers have become fundamental components in the evermore complex deep neural network architectures. Such models require acceleration processes for deployment on edge devices. However, BN layers add computation bottlenecks due to the sequential operation processing: thus, a key, yet often overlooked component of the acceleration process is BN layers folding. In this paper, we demonstrate that the current BN folding approaches are suboptimal in terms of how many layers can be removed. We therefore provide a necessary and sufficient condition for BN folding and a corresponding optimal algorithm. The proposed approach systematically outperforms existing baselines and allows to dramatically reduce the inference time of deep neural networks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1632928879",
                        "name": "Edouard Yvinec"
                    },
                    {
                        "authorId": "3190846",
                        "name": "Arnaud Dapogny"
                    },
                    {
                        "authorId": "2521061",
                        "name": "K\u00e9vin Bailly"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, the \"rewinding late\" rule is found by [15, 49] to scale up LTH to larger networks and datasets.",
                "\u2022 Rewinding has minor impact: Unlike [15, 49], we find that \u201dlate rewinding\u201d technique does not have a notable effect on style transfer subnetworks.",
                "Then, we prune individual weights with the lowest-magnitudes globally throughout the network [22, 49].",
                "Unlike the feature global transformation strategy employed by AdaIN, SANet is able to flexibly match the local semantically nearest style features onto the content features, i.e., local transformation based, thanks to the introduction of the attention mechanism.",
                "Evidence of the existence of LTH has been shown great success in various fields [5, 17, 49, 57], and its property has been studied widely [6, 15, 31, 44].",
                "Considering that rewinding paradigm is found to be necessary to identify winning tickets [49] for large networks nowadays."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c07a0d12b15e9d3ed1d3f21ed8f065eeaa9d2329",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-13802",
                    "ArXiv": "2203.13802",
                    "DOI": "10.1145/3552487.3556440",
                    "CorpusId": 247748999
                },
                "corpusId": 247748999,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c07a0d12b15e9d3ed1d3f21ed8f065eeaa9d2329",
                "title": "Playing Lottery Tickets in Style Transfer Models",
                "abstract": "Style transfer has achieved great success and attracted a wide range of attention from both academic and industrial communities due to its flexible application scenarios. However, the dependence on a pretty large VGG-based autoencoder leads to existing style transfer models having high parameter complexities, which limits their applications on resource-constrained devices. Compared with many other tasks, the compression of style transfer models has been less explored. Recently, the lottery ticket hypothesis (LTH) has shown great potential in finding extremely sparse matching subnetworks which can achieve on par or even better performance than the original full networks when trained in isolation. In this work, we for the first time perform an empirical study to verify whether such trainable matching subnetworks also exist in style transfer models. Specifically, we take two most popular style transfer models, i.e., AdaIN and SANet, as the main testbeds, which represent global and local transformation based style transfer methods respectively. We carry out extensive experiments and comprehensive analysis, and draw the following conclusions. (1) Compared with fixing the VGG encoder, style transfer models can benefit more from training the whole network together. (2) Using iterative magnitude pruning, we find the matching subnetworks at 89.2% sparsity in AdaIN and 73.7% sparsity in SANet, which demonstrates that Style transfer models can play lottery tickets too. (3) The feature transformation module should also be pruned to obtain a much sparser model without affecting the existence and quality of the matching subnetworks. (4) Besides AdaIN and SANet, other models such as LST, MANet, AdaAttN and MCCNet can also play lottery tickets, which shows that LTH can be generalized to various style transfer models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2120272947",
                        "name": "Meihao Kong"
                    },
                    {
                        "authorId": "2055851838",
                        "name": "Jing Huo"
                    },
                    {
                        "authorId": "35660603",
                        "name": "Wenbin Li"
                    },
                    {
                        "authorId": "2149265937",
                        "name": "Jing Wu"
                    },
                    {
                        "authorId": "7827503",
                        "name": "Yu-Kun Lai"
                    },
                    {
                        "authorId": "49658113",
                        "name": "Yang Gao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dba2c4505dda38a57413064080513a65e1df7a34",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-10991",
                    "ArXiv": "2203.10991",
                    "DOI": "10.48550/arXiv.2203.10991",
                    "CorpusId": 247594087
                },
                "corpusId": 247594087,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dba2c4505dda38a57413064080513a65e1df7a34",
                "title": "Optimal Fine-Grained N: M sparsity for Activations and Neural Gradients",
                "abstract": "In deep learning, fine-grained N:M sparsity reduces the data footprint and bandwidth of a General Matrix multiply (GEMM) by x2, and doubles throughput by skipping computation of zero values. So far, it was only used to prune weights. We examine how this method can be used also for activations and their gradients (i.e.,\"neural gradients\"). To this end, we first establish a tensor-level optimality criteria. Previous works aimed to minimize the mean-square-error (MSE) of each pruned block. We show that while minimization of the MSE works fine for pruning the activations, it catastrophically fails for the neural gradients. Instead, we show that optimal pruning of the neural gradients requires an unbiased minimum-variance pruning mask. We design such specialized masks, and find that in most cases, 1:2 sparsity is sufficient for training, and 2:4 sparsity is usually enough when this is not the case. Further, we suggest combining several such methods together in order to potentially speed up training even more. A reference implementation is supplied in https://github.com/brianchmiel/Act-and-Grad-structured-sparsity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "104387774",
                        "name": "Brian Chmiel"
                    },
                    {
                        "authorId": "2477463",
                        "name": "Itay Hubara"
                    },
                    {
                        "authorId": "2607278",
                        "name": "Ron Banner"
                    },
                    {
                        "authorId": "1912398",
                        "name": "Daniel Soudry"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cd01df0c950e733021d536f9e1a0f28a14941c2e",
                "externalIds": {
                    "DBLP": "conf/cvpr/WimmerMC22",
                    "ArXiv": "2203.07808",
                    "DOI": "10.1109/CVPR52688.2022.01220",
                    "CorpusId": 247451051
                },
                "corpusId": 247451051,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cd01df0c950e733021d536f9e1a0f28a14941c2e",
                "title": "Interspace Pruning: Using Adaptive Filter Representations to Improve Training of Sparse CNNs",
                "abstract": "Unstructured pruning is well suited to reduce the memory footprint of convolutional neural networks (CNNs), both at training and inference time. CNNs contain parameters arranged in K x K filters. Standard unstructured pruning (SP) reduces the memory footprint of CNNs by setting filter elements to zero, thereby specifying a fixed subspace that constrains the filter. Especially if pruning is applied before or during training, this induces a strong bias. To overcome this, we introduce interspace pruning (IP), a general tool to improve existing pruning methods. It uses filters represented in a dynamic interspace by linear combinations of an underlying adaptive filter basis (FB). For IP, FB coefficients are set to zero while un-pruned coefficients and FBs are trained jointly. In this work, we provide mathematical evidence for IP's superior performance and demonstrate that IP outperforms SP on all tested state-of-the-art unstructured pruning methods. Especially in challenging situations, like pruning for ImageNet or pruning to high sparsity, IP greatly exceeds SP with equal runtime and parameter costs. Finally, we show that advances of IP are due to improved trainability and superior generalization ability.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2029488873",
                        "name": "Paul Wimmer"
                    },
                    {
                        "authorId": "144442281",
                        "name": "Jens Mehnert"
                    },
                    {
                        "authorId": "2063161",
                        "name": "A. Condurache"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Once the procedure is finished, we identify the least important graph frequencies and remove them from the model, then we retrain the remaining parameters using the same scheduler as for baseline architectures, what is referred to as LR-rewinding [11] in the literature.",
                "Once the procedure is finished, we identify the least important graph frequencies and remove them from the model, then we retrain the remaining parameters using the same scheduler as for baseline architectures, what is referred to as LR-rewinding [12] in the literature."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7e327280e73453b5a32feab39ad140e4729439e7",
                "externalIds": {
                    "DBLP": "conf/eusipco/OuahidiTLFPG22",
                    "ArXiv": "2203.04455",
                    "DOI": "10.48550/arXiv.2203.04455",
                    "CorpusId": 247318747
                },
                "corpusId": 247318747,
                "publicationVenue": {
                    "id": "ebcbaf26-62a6-4cb2-b637-b29091ca04d6",
                    "name": "European Signal Processing Conference",
                    "type": "conference",
                    "alternate_names": [
                        "EUSIPCO",
                        "Eur Signal Process Conf"
                    ],
                    "issn": "2076-1465",
                    "alternate_issns": [
                        "2219-5491"
                    ],
                    "url": "https://www.eurasip.org/index.php?Itemid=89&id=80&option=com_content&view=article",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conhome/1801907/all-proceedings"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7e327280e73453b5a32feab39ad140e4729439e7",
                "title": "Pruning Graph Convolutional Networks to Select Meaningful Graph Frequencies for FMRI Decoding",
                "abstract": "Graph Signal Processing is a promising framework to manipulate brain signals as it allows to encompass the spatial dependencies between the activity in regions of interest in the brain. In this work, we are interested in better understanding what are the graph frequencies that are the most useful to decode fMRI signals. To this end, we introduce a deep learning architecture and adapt a pruning methodology to automatically identify such frequencies. We experiment with various datasets, architectures and graphs, and show that low graph frequencies are consistently identified as the most important for fMRI decoding, with a stronger contribution for the functional graph over the structural one. We believe that this work provides novel insights on how graph-based methods can be deployed to increase fMRI decoding accuracy and interpretability.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1782335731",
                        "name": "Yassine El Ouahidi"
                    },
                    {
                        "authorId": "146940144",
                        "name": "Hugo Tessier"
                    },
                    {
                        "authorId": "23157078",
                        "name": "G. Lioi"
                    },
                    {
                        "authorId": "2644837",
                        "name": "Nicolas Farrugia"
                    },
                    {
                        "authorId": "3334569",
                        "name": "Bastien Pasdeloup"
                    },
                    {
                        "authorId": "144916029",
                        "name": "Vincent Gripon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, there is an growing body of work on pruning during training or at initialization (Frankle & Carbin, 2019; Lee et al., 2019; Liu et al., 2019; Lee et al., 2020; Wang et al., 2020; Renda et al., 2020; Tanaka et al.,\n2020; Frankle et al., 2021; Zhang et al., 2021).",
                "However, there is an growing body of work on pruning during training or at initialization (Frankle & Carbin, 2019; Lee et al., 2019; Liu et al., 2019; Lee et al., 2020; Wang et al., 2020; Renda et al., 2020; Tanaka et al., 2020; Frankle et al., 2021; Zhang et al., 2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8557604918e072682724613e9bac1f0598b9bdcf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-04466",
                    "ArXiv": "2203.04466",
                    "DOI": "10.48550/arXiv.2203.04466",
                    "CorpusId": 247318543
                },
                "corpusId": 247318543,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8557604918e072682724613e9bac1f0598b9bdcf",
                "title": "The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks",
                "abstract": "Neural networks tend to achieve better accuracy with training if they are larger -- even if the resulting models are overparameterized. Nevertheless, carefully removing such excess parameters before, during, or after training may also produce models with similar or even improved accuracy. In many cases, that can be curiously achieved by heuristics as simple as removing a percentage of the weights with the smallest absolute value -- even though magnitude is not a perfect proxy for weight relevance. With the premise that obtaining significantly better performance from pruning depends on accounting for the combined effect of removing multiple weights, we revisit one of the classic approaches for impact-based pruning: the Optimal Brain Surgeon(OBS). We propose a tractable heuristic for solving the combinatorial extension of OBS, in which we select weights for simultaneous removal, as well as a systematic update of the remaining weights. Our selection method outperforms other methods under high sparsity, and the weight update is advantageous even when combined with the other methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2158599168",
                        "name": "Xin Yu"
                    },
                    {
                        "authorId": "13394773",
                        "name": "Thiago Serra"
                    },
                    {
                        "authorId": "145686644",
                        "name": "S. Ramalingam"
                    },
                    {
                        "authorId": "2390798",
                        "name": "Shandian Zhe"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2cd6a6e77ab4b3041b21dca9570cfa5a2c1db24d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-02549",
                    "ArXiv": "2203.02549",
                    "DOI": "10.48550/arXiv.2203.02549",
                    "CorpusId": 247291783
                },
                "corpusId": 247291783,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2cd6a6e77ab4b3041b21dca9570cfa5a2c1db24d",
                "title": "Structured Pruning is All You Need for Pruning CNNs at Initialization",
                "abstract": "Pruning is a popular technique for reducing the model size and computational cost of convolutional neural networks (CNNs). However, a slow retraining or fine-tuning procedure is often required to recover the accuracy loss caused by pruning. Recently, a new research direction on weight pruning, pruning-at-initialization (PAI), is proposed to directly prune CNNs before training so that fine-tuning or retraining can be avoided. While PAI has shown promising results in reducing the model size, existing approaches rely on fine-grained weight pruning which requires unstructured sparse matrix computation, making it difficult to achieve real speedup in practice unless the sparsity is very high. This work is the first to show that fine-grained weight pruning is in fact not necessary for PAI. Instead, the layerwise compression ratio is the main critical factor to determine the accuracy of a CNN model pruned at initialization. Based on this key observation, we propose PreCropping, a structured hardware-efficient model compression scheme. PreCropping directly compresses the model at the channel level following the layerwise compression ratio. Compared to weight pruning, the proposed scheme is regular and dense in both storage and computation without sacrificing accuracy. In addition, since PreCropping compresses CNNs at initialization, the computational and memory costs of CNNs are reduced for both training and inference on commodity hardware. We empirically demonstrate our approaches on several modern CNN architectures, including ResNet, ShuffleNet, and MobileNet for both CIFAR-10 and ImageNet.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "5358865",
                        "name": "Yaohui Cai"
                    },
                    {
                        "authorId": "37794086",
                        "name": "Weizhe Hua"
                    },
                    {
                        "authorId": "2108844339",
                        "name": "Hongzheng Chen"
                    },
                    {
                        "authorId": "145691878",
                        "name": "G. Suh"
                    },
                    {
                        "authorId": "1801197",
                        "name": "Christopher De Sa"
                    },
                    {
                        "authorId": "50316001",
                        "name": "Zhiru Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "in the latest methods [16, 20, 21], making it the best in this field ."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a90bea64ac5c494877c03663ba6df73b4ef903cd",
                "externalIds": {
                    "DOI": "10.1109/ICNLP55136.2022.00089",
                    "CorpusId": 252398639
                },
                "corpusId": 252398639,
                "publicationVenue": {
                    "id": "8a6e871b-6c73-419c-98a8-27e437270a12",
                    "name": "ICON",
                    "type": "conference",
                    "alternate_names": [
                        "ICNLP",
                        "Int conf nat lang process",
                        "International conference natural language processing",
                        "Int Conf Nat Lang Process",
                        "TAL",
                        "IEEE International Conference on Networks",
                        "IEEE Int Conf Netw",
                        "International Conference on Natural Language Processing"
                    ],
                    "issn": "1361-8113",
                    "url": "http://www.icohtec.org/publications-icon.html",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conhome/1000494/all-proceedings",
                        "http://www.wikicfp.com/cfp/program?id=1360",
                        "http://www.jstor.org/journal/icon"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a90bea64ac5c494877c03663ba6df73b4ef903cd",
                "title": "Model Compression and Acceleration Based on Progressive Random Pruning Algorithm",
                "abstract": "The deep neural network model usually has a large number of weight parameters. In order to reduce its occupation of storage space, a large number of weights pruning algorithms haw been proposed successively. Given the above problems, we found that the traditional weight pruning algorithm needs a lot of computation to evaluate the importance of parameters before pruning. This paper omits the step of evaluating the importance of parameters, proposes random pruning, and proves that the model after random pruning can still have a good performance after fine- tuning. In addition, a sparsity function is introduced in the pruning process, and the model sparsity is used to guide the pruning to enhance the stability of the network. Finally, the low-rank decomposition algorithm is combined to further realize the acceleration of the network. This paper uses the CFAR-10 dataset to compare the proposed algorithm with similar algorithms on ResNet-50 and VGG-16 networks, and it is superior to similar algorithms in terms of compression rate, FLOP and stability. Compared with the benchmark network, the algorithm in this paper achieves 1.99 x and 2. 8x the compression rate, 1.87 x and 1.83 x the acceleration effect on the two networks, respectively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1476818237",
                        "name": "Haibo Ge"
                    },
                    {
                        "authorId": "2185921407",
                        "name": "Ting Zhou"
                    },
                    {
                        "authorId": "3378446",
                        "name": "Chaofeng Huang"
                    },
                    {
                        "authorId": "2146262706",
                        "name": "Qiang Li"
                    },
                    {
                        "authorId": "2185581586",
                        "name": "Xing Song"
                    },
                    {
                        "authorId": "2185516344",
                        "name": "Shuxian Wang"
                    },
                    {
                        "authorId": "2185854734",
                        "name": "Shixiong Ma"
                    },
                    {
                        "authorId": "2185482839",
                        "name": "Mengyang Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This shows that given equal number of epochs, GEM-MINER, which prunes at initialization, can close the gap to Learning rate rewinding [34] which is a prune-after-training method.",
                "[34] is a pruning after training algorithm and just outputs a high accuracy subnetwork and hence the sanity checks do not apply to it.",
                "[34] show that rewinding the learning rate as opposed to weights(like in IMP) leads to the best performing",
                "IMP [9] \u2717 \u2713 \u2713 2850 epochs SNIP [24] \u2713 \u2713 \u2717 1 epoch GraSP [43] \u2713 \u2713 \u2717 1 epoch SynFlow [42] \u2713 \u2713 \u2717 1 epoch Edge-popup [33] \u2713 \u2717 \u2717 150 epochs Smart Ratio [41] \u2713 \u2713 \u2013 O(1) Learning Rate Rewinding [34] \u2717 \u2013 \u2013 3000 epochs GEM-MINER \u2713 \u2713 \u2713 150 epochs",
                "We tested our method against the following baselines: dense weight training and four pruning algorithms: (i) IMP [10], (ii) Learning rate rewinding [34], denoted by Renda et al.",
                "[34] tested on ResNet-20, CIFAR-10 at 98."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a8e86e3267a39ed8562360f3cc15ee5576525384",
                "externalIds": {
                    "ArXiv": "2202.12002",
                    "DBLP": "conf/nips/SreenivasanSYGN22",
                    "CorpusId": 247084008
                },
                "corpusId": 247084008,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a8e86e3267a39ed8562360f3cc15ee5576525384",
                "title": "Rare Gems: Finding Lottery Tickets at Initialization",
                "abstract": "Large neural networks can be pruned to a small fraction of their original size, with little loss in accuracy, by following a time-consuming\"train, prune, re-train\"approach. Frankle&Carbin conjecture that we can avoid this by training\"lottery tickets\", i.e., special sparse subnetworks found at initialization, that can be trained to high accuracy. However, a subsequent line of work by Frankle et al. and Su et al. presents concrete evidence that current algorithms for finding trainable networks at initialization, fail simple baseline comparisons, e.g., against training random sparse subnetworks. Finding lottery tickets that train to better accuracy compared to simple baselines remains an open problem. In this work, we resolve this open problem by proposing Gem-Miner which finds lottery tickets at initialization that beat current baselines. Gem-Miner finds lottery tickets trainable to accuracy competitive or better than Iterative Magnitude Pruning (IMP), and does so up to $19\\times$ faster.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "34824875",
                        "name": "Kartik K. Sreenivasan"
                    },
                    {
                        "authorId": "8414722",
                        "name": "Jy-yong Sohn"
                    },
                    {
                        "authorId": "46554639",
                        "name": "Liu Yang"
                    },
                    {
                        "authorId": "2156114773",
                        "name": "Matthew Grinde"
                    },
                    {
                        "authorId": "1750526537",
                        "name": "Alliot Nagle"
                    },
                    {
                        "authorId": "2109798334",
                        "name": "Hongyi Wang"
                    },
                    {
                        "authorId": "2115495251",
                        "name": "Kangwook Lee"
                    },
                    {
                        "authorId": "1740595",
                        "name": "Dimitris Papailiopoulos"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In [27], the authors find that rewinding the parameter to the early training stage of the neural network, rather than the initial value, can bring profits to winning tickets in complicated datasets.",
                "[27] Alex Renda, Jonathan Frankle, and Michael Carbin."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5f2fe6889ac91655ea49d941db6694dcdbb849bb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-11484",
                    "ArXiv": "2202.11484",
                    "CorpusId": 247058456
                },
                "corpusId": 247058456,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5f2fe6889ac91655ea49d941db6694dcdbb849bb",
                "title": "Reconstruction Task Finds Universal Winning Tickets",
                "abstract": "Pruning well-trained neural networks is effective to achieve a promising accuracy-efficiency trade-off in computer vision regimes. However, most of existing pruning algorithms only focus on the classification task defined on the source domain. Different from the strong transferability of the original model, a pruned network is hard to transfer to complicated downstream tasks such as object detection arXiv:arch-ive/2012.04643. In this paper, we show that the image-level pretrain task is not capable of pruning models for diverse downstream tasks. To mitigate this problem, we introduce image reconstruction, a pixel-level task, into the traditional pruning framework. Concretely, an autoencoder is trained based on the original model, and then the pruning process is optimized with both autoencoder and classification losses. The empirical study on benchmark downstream tasks shows that the proposed method can outperform state-of-the-art results explicitly.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Ruichen Li"
                    },
                    {
                        "authorId": "2145728702",
                        "name": "Binghui Li"
                    },
                    {
                        "authorId": "2066197601",
                        "name": "Qi Qian"
                    },
                    {
                        "authorId": "39060743",
                        "name": "Liwei Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although sparsity is beneficial, the current methods (Frankle & Carbin, 2019; Frankle et al., 2020; Renda et al., 2020) often empirically locate sparse critical subnetworks by Iterative Magnitude Pruning (IMP)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "01594f00b0deed32cba4fc4ea8c74b60be31db4a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-09844",
                    "ArXiv": "2202.09844",
                    "CorpusId": 247011143
                },
                "corpusId": 247011143,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/01594f00b0deed32cba4fc4ea8c74b60be31db4a",
                "title": "Sparsity Winning Twice: Better Robust Generalization from More Efficient Training",
                "abstract": "Recent studies demonstrate that deep networks, even robustified by the state-of-the-art adversarial training (AT), still suffer from large robust generalization gaps, in addition to the much more expensive training costs than standard training. In this paper, we investigate this intriguing problem from a new perspective, i.e., injecting appropriate forms of sparsity during adversarial training. We introduce two alternatives for sparse adversarial training: (i) static sparsity, by leveraging recent results from the lottery ticket hypothesis to identify critical sparse subnetworks arising from the early training; (ii) dynamic sparsity, by allowing the sparse subnetwork to adaptively adjust its connectivity pattern (while sticking to the same sparsity ratio) throughout training. We find both static and dynamic sparse methods to yield win-win: substantially shrinking the robust generalization gap and alleviating the robust overfitting, meanwhile significantly saving training and inference FLOPs. Extensive experiments validate our proposals with multiple network architectures on diverse datasets, including CIFAR-10/100 and Tiny-ImageNet. For example, our methods reduce robust generalization gap and overfitting by 34.44% and 4.02%, with comparable robust/standard accuracy boosts and 87.83%/87.82% training/inference FLOPs savings on CIFAR-100 with ResNet-18. Besides, our approaches can be organically combined with existing regularizers, establishing new state-of-the-art results in AT. Codes are available in https://github.com/VITA-Group/Sparsity-Win-Robust-Generalization.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2109338656",
                        "name": "Zhenyu (Allen) Zhang"
                    },
                    {
                        "authorId": "2108815093",
                        "name": "Pengju Wang"
                    },
                    {
                        "authorId": "2155513986",
                        "name": "Santosh Balachandra"
                    },
                    {
                        "authorId": "2126795",
                        "name": "Haoyu Ma"
                    },
                    {
                        "authorId": "2155897901",
                        "name": "Zehao Wang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although LTH-based low-complexity neural models have proven competitive prediction performance on several image classification tasks, machine translation [28, 29] and acoustic scene classification [30], and recently have been supported with some theoretical findings [31] related to overparameterization, the effect of LTH on our multimodal task of audio-visual WWS is not unknown."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "11cbc40b3dd30abdea56aa09467587e08e254b9c",
                "externalIds": {
                    "DBLP": "conf/icassp/ZhouDYXL22",
                    "ArXiv": "2202.08509",
                    "DOI": "10.1109/icassp43922.2022.9746360",
                    "CorpusId": 246904759
                },
                "corpusId": 246904759,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/11cbc40b3dd30abdea56aa09467587e08e254b9c",
                "title": "A Study of Designing Compact Audio-Visual Wake Word Spotting System Based on Iterative Fine-Tuning in Neural Network Pruning",
                "abstract": "Audio-only based wake word spotting (WWS) is challenging under noisy conditions due to the environmental interference in signal transmission. In this paper, we investigate on designing a compact audio-visual WWS system by utilizing the visual information to alleviate the degradation. Specifically, in order to use visual information, we first encode the detected lips to fixed-size vectors with MobileNet and concatenate them with acoustic features followed by the fusion network for WWS. However, the audio-visual model based on neural network requires a large footprint and a high computational complexity. To meet the application requirements, we introduce a neural network pruning strategy via the lottery ticket hypothesis in an iterative fine-tuning manner (LTH-IF), to the single-modal and multi-modal models, respectively. Tested on our in-house corpus for audio-visual WWS in a home TV scene, the proposed audiovisual system achieves significant performance improvements over the single-modality (audio-only or video-only) system under different noisy conditions. Moreover, LTH-IF pruning can largely reduce the network parameters and computations with no degradation of WWS performance, leading to a potential product solution for the TV wake-up scenario.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48054592",
                        "name": "Hengshun Zhou"
                    },
                    {
                        "authorId": "145419855",
                        "name": "Jun Du"
                    },
                    {
                        "authorId": "46962482",
                        "name": "C. Yang"
                    },
                    {
                        "authorId": "3285999",
                        "name": "Shifu Xiong"
                    },
                    {
                        "authorId": "9391905",
                        "name": "Chin-Hui Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "They report the best achievable results by these methods and highlight the gap between their performance and two pruning-at-convergence methods, weight rewinding and magnitude pruning (Renda et al., 2020; Frankle et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "be1210aa1ddbe7d6a654045a5aabbdc2a4827e6f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-08132",
                    "ArXiv": "2202.08132",
                    "CorpusId": 246867209
                },
                "corpusId": 246867209,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/be1210aa1ddbe7d6a654045a5aabbdc2a4827e6f",
                "title": "Prospect Pruning: Finding Trainable Weights at Initialization using Meta-Gradients",
                "abstract": "Pruning neural networks at initialization would enable us to find sparse models that retain the accuracy of the original network while consuming fewer computational resources for training and inference. However, current methods are insufficient to enable this optimization and lead to a large degradation in model performance. In this paper, we identify a fundamental limitation in the formulation of current methods, namely that their saliency criteria look at a single step at the start of training without taking into account the trainability of the network. While pruning iteratively and gradually has been shown to improve pruning performance, explicit consideration of the training stage that will immediately follow pruning has so far been absent from the computation of the saliency criterion. To overcome the short-sightedness of existing methods, we propose Prospect Pruning (ProsPr), which uses meta-gradients through the first few steps of optimization to determine which weights to prune. ProsPr combines an estimate of the higherorder effects of pruning on the loss and the optimization trajectory to identify the trainable sub-network. Our method achieves state-of-the-art pruning performance on a variety of vision classification tasks, with less data and in a single shot compared to existing pruning-at-initialization methods. Our code is available online at https://github.com/mil-ad/prospr.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2522169",
                        "name": "Milad Alizadeh"
                    },
                    {
                        "authorId": "1879365383",
                        "name": "Shyam A. Tailor"
                    },
                    {
                        "authorId": "3378188",
                        "name": "L. Zintgraf"
                    },
                    {
                        "authorId": "3038326",
                        "name": "Joost R. van Amersfoort"
                    },
                    {
                        "authorId": "33859827",
                        "name": "Sebastian Farquhar"
                    },
                    {
                        "authorId": "2059229468",
                        "name": "N. Lane"
                    },
                    {
                        "authorId": "2681954",
                        "name": "Y. Gal"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 90%, 95%) can be achieved without sacrificing the test accuracy; (ii) the located winning ticket maintains undamaged expressive power as its dense counterpart, and can be easily trained from scratch or early-epoch weights (Renda et al., 2020; Frankle et al., 2020a) to recover the full performance.",
                "%) can be achieved without sacrificing the test accuracy; (ii) the located winning ticket maintains undamaged expressive power as its dense counterpart, and can be easily trained from scratch or early-epoch weights (Renda et al., 2020; Frankle et al., 2020a) to recover the full performance.",
                "With the assistance of weight rewinding techniques (Renda et al., 2020; Frankle et al., 2020a), the original LTH can be scaled up to larger networks and datasets.",
                "Initialization (Frankle & Carbin, 2019; Renda et al., 2020) as another\nkey factor in LTH, also contributes significantly to the existence of winning tickets.",
                "In this paper, we mainly follow the routine notations in (Frankle & Carbin, 2019; Renda et al., 2020)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "e468f74ebffa8bbdd99bf8d0233822a1d2a9b430",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-04736",
                    "ArXiv": "2202.04736",
                    "CorpusId": 246706034
                },
                "corpusId": 246706034,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e468f74ebffa8bbdd99bf8d0233822a1d2a9b430",
                "title": "Coarsening the Granularity: Towards Structurally Sparse Lottery Tickets",
                "abstract": "The lottery ticket hypothesis (LTH) has shown that dense models contain highly sparse subnetworks (i.e., winning tickets) that can be trained in isolation to match full accuracy. Despite many exciting efforts being made, there is one\"commonsense\"rarely challenged: a winning ticket is found by iterative magnitude pruning (IMP) and hence the resultant pruned subnetworks have only unstructured sparsity. That gap limits the appeal of winning tickets in practice, since the highly irregular sparse patterns are challenging to accelerate on hardware. Meanwhile, directly substituting structured pruning for unstructured pruning in IMP damages performance more severely and is usually unable to locate winning tickets. In this paper, we demonstrate the first positive result that a structurally sparse winning ticket can be effectively found in general. The core idea is to append\"post-processing techniques\"after each round of (unstructured) IMP, to enforce the formation of structural sparsity. Specifically, we first\"re-fill\"pruned elements back in some channels deemed to be important, and then\"re-group\"non-zero elements to create flexible group-wise structural patterns. Both our identified channel- and group-wise structural subnetworks win the lottery, with substantial inference speedups readily supported by existing hardware. Extensive experiments, conducted on diverse datasets across multiple network backbones, consistently validate our proposal, showing that the hardware acceleration roadblock of LTH is now removed. Specifically, the structural winning tickets obtain up to {64.93%, 64.84%, 60.23%} running time savings at {36%~80%, 74%, 58%} sparsity on {CIFAR, Tiny-ImageNet, ImageNet}, while maintaining comparable accuracy. Code is at https://github.com/VITA-Group/Structure-LTH.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "1753619492",
                        "name": "Xuxi Chen"
                    },
                    {
                        "authorId": "151480882",
                        "name": "Xiaolong Ma"
                    },
                    {
                        "authorId": "46393431",
                        "name": "Yanzhi Wang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "633b3435b4ddd48bf8430a0d9e4872572f6a18f2",
                "externalIds": {
                    "DBLP": "conf/uss/Yuan022",
                    "ArXiv": "2202.03335",
                    "CorpusId": 246634340
                },
                "corpusId": 246634340,
                "publicationVenue": {
                    "id": "54649c1d-6bcc-4232-9cd1-aa446867b8d0",
                    "name": "USENIX Security Symposium",
                    "type": "conference",
                    "alternate_names": [
                        "USENIX Secur Symp"
                    ],
                    "url": "http://www.usenix.org/events/bytopic/security.html"
                },
                "url": "https://www.semanticscholar.org/paper/633b3435b4ddd48bf8430a0d9e4872572f6a18f2",
                "title": "Membership Inference Attacks and Defenses in Neural Network Pruning",
                "abstract": "Neural network pruning has been an essential technique to reduce the computation and memory requirements for using deep neural networks for resource-constrained devices. Most existing research focuses primarily on balancing the sparsity and accuracy of a pruned neural network by strategically removing insignificant parameters and retraining the pruned model. Such efforts on reusing training samples pose serious privacy risks due to increased memorization, which, however, has not been investigated yet. In this paper, we conduct the first analysis of privacy risks in neural network pruning. Specifically, we investigate the impacts of neural network pruning on training data privacy, i.e., membership inference attacks. We first explore the impact of neural network pruning on prediction divergence, where the pruning process disproportionately affects the pruned model's behavior for members and non-members. Meanwhile, the influence of divergence even varies among different classes in a fine-grained manner. Enlighten by such divergence, we proposed a self-attention membership inference attack against the pruned neural networks. Extensive experiments are conducted to rigorously evaluate the privacy impacts of different pruning approaches, sparsity levels, and adversary knowledge. The proposed attack shows the higher attack performance on the pruned models when compared with eight existing membership inference attacks. In addition, we propose a new defense mechanism to protect the pruning process by mitigating the prediction divergence based on KL-divergence distance, whose effectiveness has been experimentally demonstrated to effectively mitigate the privacy risks while maintaining the sparsity and accuracy of the pruned models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152162529",
                        "name": "Xiaoyong Yuan"
                    },
                    {
                        "authorId": "2131620609",
                        "name": "Lan Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020a) and weight rewinding (Frankle et al., 2020; Renda et al., 2020) could serve as good starting points."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "821b08d595b6482e3d1f5bab6835b72d67ebd894",
                "externalIds": {
                    "ArXiv": "2202.02643",
                    "DBLP": "journals/corr/abs-2202-02643",
                    "CorpusId": 246634950
                },
                "corpusId": 246634950,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/821b08d595b6482e3d1f5bab6835b72d67ebd894",
                "title": "The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training",
                "abstract": "Random pruning is arguably the most naive way to attain sparsity in neural networks, but has been deemed uncompetitive by either post-training pruning or sparse training. In this paper, we focus on sparse training and highlight a perhaps counter-intuitive finding, that random pruning at initialization can be quite powerful for the sparse training of modern neural networks. Without any delicate pruning criteria or carefully pursued sparsity structures, we empirically demonstrate that sparsely training a randomly pruned network from scratch can match the performance of its dense equivalent. There are two key factors that contribute to this revival: (i) the network sizes matter: as the original dense networks grow wider and deeper, the performance of training a randomly pruned sparse network will quickly grow to matching that of its dense equivalent, even at high sparsity ratios; (ii) appropriate layer-wise sparsity ratios can be pre-chosen for sparse training, which shows to be another important performance booster. Simple as it looks, a randomly pruned subnetwork of Wide ResNet-50 can be sparsely trained to outperforming a dense Wide ResNet-50, on ImageNet. We also observed such randomly pruned networks outperform dense counterparts in other favorable aspects, such as out-of-distribution detection, uncertainty estimation, and adversarial robustness. Overall, our results strongly suggest there is larger-than-expected room for sparse training at scale, and the benefits of sparsity might be more universal beyond carefully designed pruning. Our source code can be found at https://github.com/VITA-Group/Random_Pruning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2116298570",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": null,
                        "name": "Li Shen"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "437541051017839da0e8a1401f1d90b67ee253f5",
                "externalIds": {
                    "DBLP": "journals/kbs/WangSNY22",
                    "DOI": "10.1016/j.knosys.2022.108427",
                    "CorpusId": 246929699
                },
                "corpusId": 246929699,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/437541051017839da0e8a1401f1d90b67ee253f5",
                "title": "SNIP-FSL: Finding task-specific lottery jackpots for few-shot learning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108876280",
                        "name": "Ren Wang"
                    },
                    {
                        "authorId": "2776765",
                        "name": "Haoliang Sun"
                    },
                    {
                        "authorId": "3082612",
                        "name": "Xiushan Nie"
                    },
                    {
                        "authorId": "102446355",
                        "name": "Yilong Yin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our baseline is iterative L1-norm weight-based pruning technique (Li et al. 2016; Renda, Frankle, and Carbin 2020) applied iteratively with rewinding.",
                "Among these efforts, DNN pruning is a promising approach (Li et al. 2016; Han, Mao, and Dally 2015; Molchanov et al. 2016; Theis et al. 2018; Renda, Frankle, and Carbin 2020), which identifies the parameters (or weight elements) that do not contribute significantly to the accuracy, and prunes them\u2026",
                "There are two types of rewinding\u2014weight rewinding and learning rate rewinding (Renda, Frankle, and Carbin 2020)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ec80a589b2b45226683aae939663a00d90f66bf4",
                "externalIds": {
                    "ArXiv": "2201.09881",
                    "DBLP": "journals/corr/abs-2201-09881",
                    "CorpusId": 246276158
                },
                "corpusId": 246276158,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ec80a589b2b45226683aae939663a00d90f66bf4",
                "title": "Iterative Activation-based Structured Pruning",
                "abstract": "Deploying complex deep learning models on edge devices is challenging because they have substantial compute and memory resource requirements, whereas edge devices' resource budget is limited. To solve this problem, extensive pruning techniques have been proposed for compressing networks. Recent advances based on the Lottery Ticket Hypothesis (LTH) show that iterative model pruning tends to produce smaller and more accurate models. However, LTH research focuses on unstructured pruning, which is hardware-inefficient and difficult to accelerate on hardware platforms. In this paper, we investigate iterative pruning in the context of structured pruning because structurally pruned models map well on commodity hardware. We find that directly applying a structured weight-based pruning technique iteratively, called iterative L1-norm based pruning (ILP), does not produce accurate pruned models. To solve this problem, we propose two activation-based pruning methods, Iterative Activation-based Pruning (IAP) and Adaptive Iterative Activation-based Pruning (AIAP). We observe that, with only 1% accuracy loss, IAP and AIAP achieve 7.75X and 15.88$X compression on LeNet-5, and 1.25X and 1.71X compression on ResNet-50, whereas ILP achieves 4.77X and 1.13X, respectively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1995855",
                        "name": "Kaiqi Zhao"
                    },
                    {
                        "authorId": "101682296",
                        "name": "Animesh Jain"
                    },
                    {
                        "authorId": "2152527896",
                        "name": "Ming Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1b34b95dbf604178faa57832880800be0676eeb9",
                "externalIds": {
                    "ArXiv": "2201.10520",
                    "DBLP": "journals/corr/abs-2201-10520",
                    "CorpusId": 246276168
                },
                "corpusId": 246276168,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1b34b95dbf604178faa57832880800be0676eeb9",
                "title": "Adaptive Activation-based Structured Pruning",
                "abstract": "Pruning is a promising approach to compress complex deep learning models in order to deploy them on resource-constrained edge devices. However, many existing pruning solutions are based on unstructured pruning, which yields models that cannot efficiently run on commodity hardware and require users to manually explore and tune the pruning process, which is time-consuming and often leads to sub-optimal results. To address these limitations, this paper presents an adaptive, activation-based, structured pruning approach to automatically and efficiently generate small, accurate, and hardware-efficient models that meet user requirements. First, it proposes iterative structured pruning using activation-based attention feature maps to effectively identify and prune unimportant filters. Then, it proposes adaptive pruning policies for automatically meeting the pruning objectives of accuracy-critical, memory-constrained, and latency-sensitive tasks. A comprehensive evaluation shows that the proposed method can substantially outperform the state-of-the-art structured pruning works on CIFAR-10 and ImageNet datasets. For example, on ResNet-56 with CIFAR-10, without any accuracy drop, our method achieves the largest parameter reduction (79.11%), outperforming the related works by 22.81% to 66.07%, and the largest FLOPs reduction (70.13%), outperforming the related works by 14.13% to 26.53%.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1995855",
                        "name": "Kaiqi Zhao"
                    },
                    {
                        "authorId": "101682296",
                        "name": "Animesh Jain"
                    },
                    {
                        "authorId": "2152527896",
                        "name": "Ming Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model pruning adversaries first prune the victim model using some pruning methods, then finetune the model using a small set of data [26], [35]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c3111e374ad14357172ef63e7063e0182f8030d4",
                "externalIds": {
                    "ArXiv": "2112.05588",
                    "DBLP": "conf/sp/ChenWPS0JM0S22",
                    "DOI": "10.1109/sp46214.2022.9833747",
                    "CorpusId": 245117504
                },
                "corpusId": 245117504,
                "publicationVenue": {
                    "id": "29b9c461-963e-4d11-b2ab-92c182243942",
                    "name": "IEEE Symposium on Security and Privacy",
                    "type": "conference",
                    "alternate_names": [
                        "S&P",
                        "IEEE Symp Secur Priv"
                    ],
                    "url": "http://www.ieee-security.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c3111e374ad14357172ef63e7063e0182f8030d4",
                "title": "Copy, Right? A Testing Framework for Copyright Protection of Deep Learning Models",
                "abstract": "Deep learning models, especially those large-scale and high-performance ones, can be very costly to train, demanding a considerable amount of data and computational resources. As a result, deep learning models have become one of the most valuable assets in modern artificial intelligence. Unauthorized duplication or reproduction of deep learning models can lead to copyright infringement and cause huge economic losses to model owners, calling for effective copyright protection techniques. Existing protection techniques are mostly based on watermarking, which embeds an owner-specified watermark into the model. While being able to provide exact ownership verification, these techniques are 1) invasive, i.e., they need to tamper with the training process, which may affect the model utility or introduce new security risks into the model; 2) prone to adaptive attacks that attempt to remove/replace the watermark or adversarially block the retrieval of the watermark; and 3) not robust to the emerging model extraction attacks. Latest fingerprinting work on deep learning models, though being non-invasive, also falls short when facing the diverse and ever-growing attack scenarios.In this paper, we propose a novel testing framework for deep learning copyright protection: DEEPJUDGE. DEEPJUDGE quantitatively tests the similarities between two deep learning models: a victim model and a suspect model. It leverages a diverse set of testing metrics and efficient test case generation algorithms to produce a chain of supporting evidence to help determine whether a suspect model is a copy of the victim model. Advantages of DEEPJUDGE include: 1) non-invasive, as it works directly on the model and does not tamper with the training process; 2) efficient, as it only needs a small set of seed test cases and a quick scan of the two models; 3) flexible, i.e., it can easily incorporate new testing metrics or test case generation methods to obtain more confident and robust judgement; and 4) fairly robust to model extraction attacks and adaptive attacks. We verify the effectiveness of DEEPJUDGE under three typical copyright infringement scenarios, including model finetuning, pruning and extraction, via extensive experiments on both image classification and speech recognition datasets with a variety of model architectures.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2144184915",
                        "name": "Jialuo Chen"
                    },
                    {
                        "authorId": "2115889967",
                        "name": "Jingyi Wang"
                    },
                    {
                        "authorId": "2149325570",
                        "name": "Tinglan Peng"
                    },
                    {
                        "authorId": "2116605723",
                        "name": "Youcheng Sun"
                    },
                    {
                        "authorId": "2112624803",
                        "name": "Peng Cheng"
                    },
                    {
                        "authorId": "2081160",
                        "name": "S. Ji"
                    },
                    {
                        "authorId": "9576855",
                        "name": "Xingjun Ma"
                    },
                    {
                        "authorId": "2151288080",
                        "name": "Bo Li"
                    },
                    {
                        "authorId": "143711382",
                        "name": "D. Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026and perform structure learning, including dynamic sparse training (Evci et al., 2020; Liu et al., 2021b), adaptations (Frankle et al., 2020; Renda et al., 2020; Liu et al., 2021a; Liu et al.) of Iterative Magnitude Pruning (IMP) Han et al. (2015); Frankle & Carbin (2019) and sparse\u2026",
                "Pruning algorithms that search for strong LTs achieve sparsity levels of around 0.5 but not substantially smaller if the resulting models should be able to compete with the accuracy of the entire, trained mother network (Ramanujan et al., 2020a).",
                "As we have discussed, the LTs that exist with high probability rarely fulfill criteria of interest, such as low sparsity, favorable generalization properties, or adversarial robustness.",
                "The existence of strong LTs has also been proven formally for networks without (Malach et al., 2020; Pensia et al., 2020; Orseau et al., 2020) and with potentially nonzero biases (Fischer & Burkholz, 2021).",
                "Our results indicate that state-of-the-art LT pruning methods achieve in general sub-optimal sparsity levels, and are not able to recover LTs that are competitive with a planted ground truth.",
                ", 2020) nor LT fine-tuning techniques (Liu et al., 2021a; Renda et al., 2020) find architectures that are competitive with our constructed ground truth tickets.",
                "We utilize our planting framework to answer the question whether LT pruning algorithms that identify subnetworks of randomly initialized neural networks are able to identify highly sparse LTs, ideally in a strong sense but we also analyze weak LTs. Hypothetically, it could be possible that pruning algorithms for weak LTs only have to resort to training the identified LT because a highly sparse strong LT does not exist with high probability.",
                "Strong tickets based on trained neural networks Even though we cannot expect to construct sparse baseline solutions for benchmark image classification tasks, we can leverage the fact that weak LTs can currently be identified at lower sparsity levels than strong LTs (see our experiments).",
                "As many relevant targets have known representations of lower sparsity than what is covered by this bound, we will afterwards propose a planting algorithm to design experiments that can distinguish between algorithmic and fundamental limitations of pruning for strong LTs.",
                "As these approaches do not identify LTs as subnetworks of randomly initialized NNs, they do not rely on the existence of planted tickets and are therefore beyond the scope of our experimental analysis.",
                "Similarly, if we insist on finding extremely sparse architectures, it might be necessary to give up the search for initial LTs (Frankle et al., 2020; Renda et al., 2020; Liu et al., 2021a).",
                "To answer the question whether state-of-the-art pruning algorithms can find sparse strong LTs in the setting of standard benchmark data, we plant a trained weak LT in a randomly initialized (VGG\nlike) neural network.",
                "If this were true, we should be able to find highly sparse LTs with the original pruning algorithm if we ensure the existence of a solution by planting.",
                "We analyze the performance of tickets before training to assess whether they qualify as strong LTs and after training to evaluate whether at least pruning for weak LTs is feasible and can identify LT of sparsities that can compete with our planted ground truth.",
                "In addition, we identify an opportunity to improve state-of-the-art pruning algorithms in order to find strong LTs of better sparsity.",
                "Usually, these methods try to find LTs in a \u2018weak\u2019 (but powerful) sense, that is to identify a sparse neural network architecture that is well trainable starting from its initial parameters.",
                "L G\n] 2\n2 N\nov 2\n02 1\ning LT pruning algorithms solely on standard benchmark datasets (Frankle et al., 2021), but demand the comparison with known ground truth LTs.",
                "We show in Appendix B that neither dynamic sparse training (Evci et al., 2020) nor LT fine-tuning techniques (Liu et al., 2021a; Renda et al., 2020) find architectures that are competitive with our constructed ground truth tickets.",
                "Strong LTs are sparse sub-networks that perform well with the initial parameters, hence do not need to be trained any further (Zhou et al., 2019; Ramanujan et al., 2020b)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "67618071e2e63921dde7471bc3c835f0cebe5a41",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-11153",
                    "ArXiv": "2111.11153",
                    "CorpusId": 244478279
                },
                "corpusId": 244478279,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/67618071e2e63921dde7471bc3c835f0cebe5a41",
                "title": "Plant 'n' Seek: Can You Find the Winning Ticket?",
                "abstract": "The lottery ticket hypothesis has sparked the rapid development of pruning algorithms that aim to reduce the computational costs associated with deep learning during training and model deployment. Currently, such algorithms are primarily evaluated on imaging data, for which we lack ground truth information and thus the understanding of how sparse lottery tickets could be. To fill this gap, we develop a framework that allows us to plant and hide winning tickets with desirable properties in randomly initialized neural networks. To analyze the ability of state-of-the-art pruning to identify tickets of extreme sparsity, we design and hide such tickets solving four challenging tasks. In extensive experiments, we observe similar trends as in imaging studies, indicating that our framework can provide transferable insights into realistic problems. Additionally, we can now see beyond such relative trends and highlight limitations of current pruning methods. Based on our results, we conclude that the current limitations in ticket sparsity are likely of algorithmic rather than fundamental nature. We anticipate that comparisons to planted tickets will facilitate future developments of efficient pruning algorithms.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145285968",
                        "name": "Jonas Fischer"
                    },
                    {
                        "authorId": "8813467",
                        "name": "R. Burkholz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[43] only reports ResNet results (see the paragraph below) on image classification, we implement one-shot and iterative unstructured magnitude pruning baselines [11,43] using PyTorch\u2019s pruning API4.",
                "* Renda et al. 2020.",
                "AQCompress consistently outperforms or achieves competitive performances compared to recently proposed pruning and quantization methods [11, 16, 36, 43, 52].",
                "Many of our experiments compare AQCompress to unstructured pruning [43], which is a well-studied and widely adopted DNN compression method.",
                "We compare AQCompress with state-of-the-art unstructured pruning [11,36,43] and quantization [16,52] approaches that are designed to compress DNNs.",
                "In this paper, we compare AQCompress with iterative unstructured pruning [43] to demonstrate the high compression ratios attained by AQCompress.",
                "(iterative) * Renda et al. 2020."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "7b691b072ffc3d317c5abb099c6005bed7f2bbe5",
                "externalIds": {
                    "ArXiv": "2111.10320",
                    "DBLP": "journals/corr/abs-2111-10320",
                    "CorpusId": 244463075
                },
                "corpusId": 244463075,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7b691b072ffc3d317c5abb099c6005bed7f2bbe5",
                "title": "Toward Compact Parameter Representations for Architecture-Agnostic Neural Network Compression",
                "abstract": "This paper investigates deep neural network (DNN) compression from the perspective of compactly representing and storing trained parameters. We explore the previously overlooked opportunity of cross-layer architecture-agnostic representation sharing for DNN parameters. To do this, we decouple feedforward parameters from DNN architectures and leverage additive quantization, an extreme lossy compression method invented for image descriptors, to compactly represent the parameters. The representations are then finetuned on task objectives to improve task accuracy. We conduct extensive experiments on MobileNet-v2, VGG-11, ResNet-50, Feature Pyramid Networks, and pruned DNNs trained for classification, detection, and segmentation tasks. The conceptually simple scheme consistently outperforms iterative unstructured pruning. Applied to ResNet-50 with 76.1% top-1 accuracy on the ILSVRC12 classification challenge, it achieves a $7.2\\times$ compression ratio with no accuracy loss and a $15.3\\times$ compression ratio at 74.79% accuracy. Further analyses suggest that representation sharing can frequently happen across network layers and that learning shared representations for an entire DNN can achieve better accuracy at the same compression ratio than compressing the model as multiple separate parts. We release PyTorch code to facilitate DNN deployment on resource-constrained devices and spur future research on efficient representations and storage of DNN parameters.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "114371062",
                        "name": "Yuezhou Sun"
                    },
                    {
                        "authorId": "50771250",
                        "name": "Wenlong Zhao"
                    },
                    {
                        "authorId": "2108877321",
                        "name": "Lijun Zhang"
                    },
                    {
                        "authorId": "2111310607",
                        "name": "Xiao Liu"
                    },
                    {
                        "authorId": "2055337890",
                        "name": "Hui Guan"
                    },
                    {
                        "authorId": "143834867",
                        "name": "M. Zaharia"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9202a718ce05395b6e17d5301e3a2e8b1021f31b",
                "externalIds": {
                    "ArXiv": "2111.05754",
                    "DBLP": "journals/corr/abs-2111-05754",
                    "CorpusId": 243938339
                },
                "corpusId": 243938339,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9202a718ce05395b6e17d5301e3a2e8b1021f31b",
                "title": "Prune Once for All: Sparse Pre-Trained Language Models",
                "abstract": "Transformer-based language models are applied to a wide range of applications in natural language processing. However, they are inefficient and difficult to deploy. In recent years, many compression algorithms have been proposed to increase the implementation efficiency of large Transformer-based models on target hardware. In this work we present a new method for training sparse pre-trained Transformer language models by integrating weight pruning and model distillation. These sparse pre-trained models can be used to transfer learning for a wide range of tasks while maintaining their sparsity pattern. We demonstrate our method with three known architectures to create sparse pre-trained BERT-Base, BERT-Large and DistilBERT. We show how the compressed sparse pre-trained models we trained transfer their knowledge to five different downstream natural language tasks with minimal accuracy loss. Moreover, we show how to further compress the sparse models' weights to 8bit precision using quantization-aware training. For example, with our sparse pre-trained BERT-Large fine-tuned on SQuADv1.1 and quantized to 8bit we achieve a compression ratio of $40$X for the encoder with less than $1\\%$ accuracy loss. To the best of our knowledge, our results show the best compression-to-accuracy ratio for BERT-Base, BERT-Large, and DistilBERT.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1387202086",
                        "name": "Ofir Zafrir"
                    },
                    {
                        "authorId": "1581864840",
                        "name": "Ariel Larey"
                    },
                    {
                        "authorId": "3150063",
                        "name": "Guy Boudoukh"
                    },
                    {
                        "authorId": "1921920",
                        "name": "Haihao Shen"
                    },
                    {
                        "authorId": "2134755",
                        "name": "Moshe Wasserblat"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The key idea of existing methods [11, 10, 49, 22, 29, 15, 51, 43, 46, 35, 18] is to develop effective criteria (e."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ed1b1acd7a36b22397f052d10426f1531cfc18ce",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-05685",
                    "ArXiv": "2111.05685",
                    "CorpusId": 243938721
                },
                "corpusId": 243938721,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ed1b1acd7a36b22397f052d10426f1531cfc18ce",
                "title": "Efficient Neural Network Training via Forward and Backward Propagation Sparsification",
                "abstract": "Sparse training is a natural idea to accelerate the training speed of deep neural networks and save the memory usage, especially since large modern neural networks are significantly over-parameterized. However, most of the existing methods cannot achieve this goal in practice because the chain rule based gradient (w.r.t. structure parameters) estimators adopted by previous methods require dense computation at least in the backward propagation step. This paper solves this problem by proposing an efficient sparse training method with completely sparse forward and backward passes. We first formulate the training process as a continuous minimization problem under global sparsity constraint. We then separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, we use the conventional chain rule, which can be sparse via exploiting the sparse structure. For the latter step, instead of using the chain rule based gradient estimators as in existing methods, we propose a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation, thus achieving completely sparse training. We prove that the variance of our gradient estimator is bounded. Extensive experimental results on real-world datasets demonstrate that compared to previous methods, our algorithm is much more effective in accelerating the training process, up to an order of magnitude faster.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109127048",
                        "name": "Xiao Zhou"
                    },
                    {
                        "authorId": "47527753",
                        "name": "Weizhong Zhang"
                    },
                    {
                        "authorId": "2049680620",
                        "name": "Zonghao Chen"
                    },
                    {
                        "authorId": "50826757",
                        "name": "Shizhe Diao"
                    },
                    {
                        "authorId": "50728655",
                        "name": "Tong Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Renda et al. (2020) noted that the learning rate schedule during retraining can have a dramatic impact on the predictive performance of the pruned network and proposed LEARNING RATE REWINDING (LRW), where one retrains the pruned network for Trt epochs using the last T \u2212 Trt learning rates \u03b7T\u2212Trt+1,\u2026",
                "As a baseline performance for a pruned network, we will use the approach suggested by Renda et al. (2020) as it serves as a good benchmark for the current potential of IMP.",
                "While many details of this procedure are not specified or elaborated on in the original paper, Renda et al. (2020) suggested the following complete approach: train a network for T epochs and then iteratively prune 20% percent of the remaining weights and retrain for Trt = T epochs until the desired\u2026",
                "More concretely, for all three levels of sparsity, 90%, 95%, and 98%, IMP meets the baseline laid out by Renda et al. (2020) after only around 100 epochs of retraining, instead of requiring the respectively 2000, 2800, and 3600 epochs used to establish that baseline.",
                "We then study to what degree the retraining phase of IMP can be shortened in the iterative setting compared to the recommendations of Renda et al. (2020) when using an\nappropriate learning rate schedule in Section 3.2.",
                "In its full iterative form, for example formulated by Renda et al. (2020), IMP can require the original train time several times over to produce a pruned network, resulting in hundreds of retraining epochs on top of the original training procedure and leading to its reputation for being\u2026",
                "This further demonstrates the importance of the learning rate scheme during the retraining phase previously already noted by Renda et al. (2020) and Le & Hua (2021).",
                "This approach effectively interpolates between the recommendations of Renda et al. (2020) and Le & Hua (2021) based on a computationally cheap proxy.",
                "We have relied on the simple exponential pruning schedule suggested by Renda et al. (2020) for BIMP while GMP relies on a particular schedule defined by a cubic polynomial that effectively leads to pruning larger amounts initially and progressively smaller amounts later in training when compared to\u2026",
                "We empirically find that the results of Li et al. (2020) regarding the Budgeted Training of Neural Networks apply to the retraining phase of IMP, providing further context for the results of Renda et al. (2020) and Le & Hua (2021)."
            ],
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "dde57499f185730d4dfc9c5df00d6aebef5bb8a9",
                "externalIds": {
                    "ArXiv": "2111.00843",
                    "DBLP": "journals/corr/abs-2111-00843",
                    "CorpusId": 247026123
                },
                "corpusId": 247026123,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/dde57499f185730d4dfc9c5df00d6aebef5bb8a9",
                "title": "How I Learned to Stop Worrying and Love Retraining",
                "abstract": "Many Neural Network Pruning approaches consist of several iterative training and pruning steps, seemingly losing a significant amount of their performance after pruning and then recovering it in the subsequent retraining phase. Recent works of Renda et al. (2020) and Le&Hua (2021) demonstrate the significance of the learning rate schedule during the retraining phase and propose specific heuristics for choosing such a schedule for IMP (Han et al., 2015). We place these findings in the context of the results of Li et al. (2020) regarding the training of models within a fixed training budget and demonstrate that, consequently, the retraining phase can be massively shortened using a simple linear learning rate schedule. Improving on existing retraining approaches, we additionally propose a method to adaptively select the initial value of the linear schedule. Going a step further, we propose similarly imposing a budget on the initial dense training phase and show that the resulting simple and efficient method is capable of outperforming significantly more complex or heavily parameterized state-of-the-art approaches that attempt to sparsify the network during training. These findings not only advance our understanding of the retraining phase, but more broadly question the belief that one should aim to avoid the need for retraining and reduce the negative effects of 'hard' pruning by incorporating the sparsification process into the standard training.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2056708985",
                        "name": "Max Zimmer"
                    },
                    {
                        "authorId": "2064617407",
                        "name": "Christoph Spiegel"
                    },
                    {
                        "authorId": "145729210",
                        "name": "S. Pokutta"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Neural network compression (Renda et al., 2020; Xu et al., 2020) has been proposed to accelerate the deep CNNs computation."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "43550a49b3d740cc9a61d94119910f7b6ff1d526",
                "externalIds": {
                    "PubMedCentral": "8578706",
                    "DBLP": "journals/ficn/ChenGSS21",
                    "DOI": "10.3389/fncom.2021.760554",
                    "CorpusId": 240090576,
                    "PubMed": "34776916"
                },
                "corpusId": 240090576,
                "publicationVenue": {
                    "id": "8c456f98-9892-42ac-9b16-418755f01550",
                    "name": "Frontiers in Computational Neuroscience",
                    "type": "journal",
                    "alternate_names": [
                        "Front Comput Neurosci"
                    ],
                    "issn": "1662-5188",
                    "url": "http://www.frontiersin.org/computational_neuroscience",
                    "alternate_urls": [
                        "http://www.frontiersin.org/computationalneuroscience/",
                        "https://www.frontiersin.org/journals/computational-neuroscience"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/43550a49b3d740cc9a61d94119910f7b6ff1d526",
                "title": "Dynamical Conventional Neural Network Channel Pruning by Genetic Wavelet Channel Search for Image Classification",
                "abstract": "Neural network pruning is critical to alleviating the high computational cost of deep neural networks on resource-limited devices. Conventional network pruning methods compress the network based on the hand-crafted rules with a pre-defined pruning ratio (PR), which fails to consider the variety of channels among different layers, thus, resulting in a sub-optimal pruned model. To alleviate this issue, this study proposes a genetic wavelet channel search (GWCS) based pruning framework, where the pruning process is modeled as a multi-stage genetic optimization procedure. Its main ideas are 2-fold: (1) it encodes all the channels of the pertained network and divide them into multiple searching spaces according to the different functional convolutional layers from concrete to abstract. (2) it develops a wavelet channel aggregation based fitness function to explore the most representative and discriminative channels at each layer and prune the network dynamically. In the experiments, the proposed GWCS is evaluated on CIFAR-10, CIFAR-100, and ImageNet datasets with two kinds of popular deep convolutional neural networks (CNNs) (ResNet and VGGNet). The results demonstrate that GNAS outperforms state-of-the-art pruning algorithms in both accuracy and compression rate. Notably, GNAS reduces more than 73.1% FLOPs by pruning ResNet-32 with even 0.79% accuracy improvement on CIFAR-100.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145130792",
                        "name": "Lin Chen"
                    },
                    {
                        "authorId": "2142481453",
                        "name": "Saijun Gong"
                    },
                    {
                        "authorId": "2119202927",
                        "name": "Xiaoyu Shi"
                    },
                    {
                        "authorId": "3846181",
                        "name": "Mingsheng Shang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The following works aim to: (1) study LTs\u2019 properties [36, 37, 38, 39], (2) improve LTs\u2019 performance [40, 41, 42, 43], and (3) extend LTH to various networks/tasks/trainingpipelines [44, 45, 46, 47, 48, 49, 50]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ffbcbced0ec14a9267f185be87d9386407640a11",
                "externalIds": {
                    "DBLP": "conf/nips/FuYZWOCL21",
                    "ArXiv": "2110.14068",
                    "CorpusId": 239998058
                },
                "corpusId": 239998058,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ffbcbced0ec14a9267f185be87d9386407640a11",
                "title": "Drawing Robust Scratch Tickets: Subnetworks with Inborn Robustness Are Found within Randomly Initialized Networks",
                "abstract": "Deep Neural Networks (DNNs) are known to be vulnerable to adversarial attacks, i.e., an imperceptible perturbation to the input can mislead DNNs trained on clean images into making erroneous predictions. To tackle this, adversarial training is currently the most effective defense method, by augmenting the training set with adversarial samples generated on the fly. Interestingly, we discover for the first time that there exist subnetworks with inborn robustness, matching or surpassing the robust accuracy of the adversarially trained networks with comparable model sizes, within randomly initialized networks without any model training, indicating that adversarial training on model weights is not indispensable towards adversarial robustness. We name such subnetworks Robust Scratch Tickets (RSTs), which are also by nature efficient. Distinct from the popular lottery ticket hypothesis, neither the original dense networks nor the identified RSTs need to be trained. To validate and understand this fascinating finding, we further conduct extensive experiments to study the existence and properties of RSTs under different models, datasets, sparsity patterns, and attacks, drawing insights regarding the relationship between DNNs' robustness and their initialization/overparameterization. Furthermore, we identify the poor adversarial transferability between RSTs of different sparsity ratios drawn from the same randomly initialized dense network, and propose a Random RST Switch (R2S) technique, which randomly switches between different RSTs, as a novel defense method built on top of RSTs. We believe our findings about RSTs have opened up a new perspective to study model robustness and extend the lottery ticket hypothesis.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116099023",
                        "name": "Yonggan Fu"
                    },
                    {
                        "authorId": "2087411479",
                        "name": "Qixuan Yu"
                    },
                    {
                        "authorId": "2145953857",
                        "name": "Yang Zhang"
                    },
                    {
                        "authorId": "2116995771",
                        "name": "Shan-Hung Wu"
                    },
                    {
                        "authorId": "2228250215",
                        "name": "Ouyang Xu"
                    },
                    {
                        "authorId": "2064714013",
                        "name": "David Cox"
                    },
                    {
                        "authorId": "3138925",
                        "name": "Yingyan Lin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b92a3d2b02b0ea3682da9be57851b76c5f1b3719",
                "externalIds": {
                    "PubMedCentral": "8587924",
                    "DOI": "10.3390/s21217074",
                    "CorpusId": 240025743,
                    "PubMed": "34770380"
                },
                "corpusId": 240025743,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b92a3d2b02b0ea3682da9be57851b76c5f1b3719",
                "title": "Deep Convolutional Neural Network Optimization for Defect Detection in Fabric Inspection",
                "abstract": "This research is aimed to detect defects on the surface of the fabric and deep learning model optimization. Since defect detection cannot effectively solve the fabric with complex background by image processing, this research uses deep learning to identify defects. However, the current network architecture mainly focuses on natural images rather than the defect detection. As a result, the network architecture used for defect detection has more redundant neurons, which reduces the inference speed. In order to solve the above problems, we propose network pruning with the Bayesian optimization algorithm to automatically tune the network pruning parameters, and then retrain the network after pruning. The training and detection process uses the above-mentioned pruning network to predict the defect feature map, and then uses the image processing flow proposed in this research for the final judgment during fabric defect detection. The proposed method is verified in the two self-made datasets and the two public datasets. In the part of the proposed network optimization results, the Intersection over Union (IoU) of four datasets are dropped by 1.26%, 1.13%, 1.21%, and 2.15% compared to the original network model, but the inference time is reduced to 20.84%, 40.52%, 23.02%, and 23.33% of the original network model using Geforce 2080 Ti. Furthermore, the inference time is also reduced to 17.56%, 37.03%, 19.67%, and 22.26% using the embedded system AGX Xavier. After the image processing part, the accuracy of the four datasets can reach 92.75%, 94.87%, 95.6%, and 81.82%, respectively. In this research, Yolov4 is also trained with fabric defects, and the results showed this model are not conducive to detecting long and narrow fabric defects.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "6716389",
                        "name": "Chao-Ching Ho"
                    },
                    {
                        "authorId": "2134996761",
                        "name": "Wei-Chi Chou"
                    },
                    {
                        "authorId": "46604147",
                        "name": "Eugene Su"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "142ec7c1a59f145e670b395474482750ab189ca9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-11804",
                    "ArXiv": "2110.11804",
                    "CorpusId": 239616402
                },
                "corpusId": 239616402,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/142ec7c1a59f145e670b395474482750ab189ca9",
                "title": "Probabilistic fine-tuning of pruning masks and PAC-Bayes self-bounded learning",
                "abstract": "We study an approach to learning pruning masks by optimizing the expected loss of stochastic pruning masks, i.e., masks which zero out each weight independently with some weight-specific probability. We analyze the training dynamics of the induced stochastic predictor in the setting of linear regression, and observe a data-adaptive L1 regularization term, in contrast to the dataadaptive L2 regularization term known to underlie dropout in linear regression. We also observe a preference to prune weights that are less well-aligned with the data labels. We evaluate probabilistic fine-tuning for optimizing stochastic pruning masks for neural networks, starting from masks produced by several baselines. In each case, we see improvements in test error over baselines, even after we threshold fine-tuned stochastic pruning masks. Finally, since a stochastic pruning mask induces a stochastic neural network, we consider training the weights and/or pruning probabilities simultaneously to minimize a PAC-Bayes bound on generalization error. Using data-dependent priors, we obtain a selfbounded learning algorithm with strong performance and numerically tight bounds. In the linear model, we show that a PAC-Bayes generalization error bound is controlled by the magnitude of the change in feature alignment between the 'prior' and 'posterior' data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46183987",
                        "name": "Soufiane Hayou"
                    },
                    {
                        "authorId": "2082464935",
                        "name": "Bo He"
                    },
                    {
                        "authorId": "2533850",
                        "name": "G. Dziugaite"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several state-of-the-art pruning methods (Renda, Frankle, and Carbin 2020; Frankle and Carbin 2019; Frankle et al. 2020) have demonstrated that a large amount of parameters can be removed without sacrificing accuracy.",
                "However, as the network is iteratively pruned, LR rewinding leads to a much higher accuracy (see Figs.1 & 2 in (Renda, Frankle, and Carbin 2020)).",
                "The inspiring performance of pruning methods hinges on a key factor - Learning Rate (LR) - as mentioned in prior works (Renda, Frankle, and Carbin 2020; Frankle and Carbin 2019).",
                "\u2026hyperparameters for ResNet-20 are from (Frankle and Carbin 2019; Frankle et al. 2020), hyperparameters for VGG-19 are from (Frankle and Carbin 2019; Frankle et al. 2020; Liu et al. 2019), and hyperparameters for ResNet-50 are adapted from (Frankle et al. 2020; Renda, Frankle, and Carbin 2020).",
                "This agrees with the results stated in prior works (Renda, Frankle, and Carbin 2020).",
                "In a followup work, Renda, Frankle, and Carbin (2020) propose LR rewinding which rewinds the LR schedule to its initial state during iterative pruning and demonstrate that it can outperform standard fine-tuning.",
                "Similarly, Renda, Frankle, and Carbin (2020) propose LR rewinding and demonstrate it outperforms standard fine-tuning.",
                "One follow-up work (Renda, Frankle, and Carbin 2020) further investigates this phenomenon and proposes a retraining technique called LR rewinding.",
                "Several recent works (Renda, Frankle, and Carbin 2020; Frankle and Carbin 2019) have noticed the important role of LR in network pruning.",
                "We chose to focus on iterative pruning of ReLU-based networks for two reasons: (i) Iterative pruning tends to provide better pruning performance than one-shot pruning as reported in the literature (Frankle and Carbin 2019; Renda, Frankle, and Carbin 2020).",
                "Some follow-on\nworks (Zhou et al. 2019; Frankle et al. 2019; Renda, Frankle, and Carbin 2020; Malach et al. 2020) investigate this phenomenon more precisely and apply this method in other fields (e.g., transfer learning (Mehta 2019), reinforcement learning and natural language processing (Yu et al.\u2026",
                "For the proposed S-\nCyc, we evaluate its performance using SGD with momentum = 0.9 and a weight decay of 1e-4 (same as (Renda, Frankle, and Carbin 2020; Frankle and Carbin 2019))."
            ],
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2eb4640775972d8db5a17254352fb62cf5c4241b",
                "externalIds": {
                    "ArXiv": "2110.08764",
                    "DBLP": "journals/corr/abs-2110-08764",
                    "CorpusId": 239016652
                },
                "corpusId": 239016652,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2eb4640775972d8db5a17254352fb62cf5c4241b",
                "title": "S-Cyc: A Learning Rate Schedule for Iterative Pruning of ReLU-based Networks",
                "abstract": "We explore a new perspective on adapting the learning rate (LR) schedule to improve the performance of the ReLU-based network as it is iteratively pruned. Our work and contribution consist of four parts: (i) We find that, as the ReLU-based network is iteratively pruned, the distribution of weight gradients tends to become narrower. This leads to the finding that as the network becomes more sparse, a larger value of LR should be used to train the pruned network. (ii) Motivated by this finding, we propose a novel LR schedule, called S-Cyclical (S-Cyc) which adapts the conventional cyclical LR schedule by gradually increasing the LR upper bound (max_lr) in an S-shape as the network is iteratively pruned.We highlight that S-Cyc is a method agnostic LR schedule that applies to many iterative pruning methods. (iii) We evaluate the performance of the proposed S-Cyc and compare it to four LR schedule benchmarks. Our experimental results on three state-of-the-art networks (e.g., VGG-19, ResNet-20, ResNet-50) and two popular datasets (e.g., CIFAR-10, ImageNet-200) demonstrate that S-Cyc consistently outperforms the best performing benchmark with an improvement of 2.1% - 3.4%, without substantial increase in complexity. (iv) We evaluate S-Cyc against an oracle and show that S-Cyc achieves comparable performance to the oracle, which carefully tunes max_lr via grid search.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1491626698",
                        "name": "Shiyu Liu"
                    },
                    {
                        "authorId": "2111728688",
                        "name": "Chong Min John Tan"
                    },
                    {
                        "authorId": "1770486",
                        "name": "M. Motani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026to make the original algorithm to find winning tickets (Frankle and Carbin, 2019) more stable: after fine-tuning, Frankle et al. (2019) rewind the parameters to their values after a few iterations rather than their values before training, whereas Renda et al. (2020) also rewind the learning rate."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fc58779940abb92166b73f47867763a07368c739",
                "externalIds": {
                    "ACL": "2022.acl-long.125",
                    "DBLP": "conf/acl/AnsellPKV22",
                    "ArXiv": "2110.07560",
                    "DOI": "10.18653/v1/2022.acl-long.125",
                    "CorpusId": 238856900
                },
                "corpusId": 238856900,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/fc58779940abb92166b73f47867763a07368c739",
                "title": "Composable Sparse Fine-Tuning for Cross-Lingual Transfer",
                "abstract": "Fine-tuning the entire set of parameters of a large pretrained model has become the mainstream approach for transfer learning. To increase its efficiency and prevent catastrophic forgetting and interference, techniques like adapters and sparse fine-tuning have been developed. Adapters are modular, as they can be combined to adapt a model towards different facets of knowledge (e.g., dedicated language and/or task adapters). Sparse fine-tuning is expressive, as it controls the behavior of all model components. In this work, we introduce a new fine-tuning method with both these desirable properties. In particular, we learn sparse, real-valued masks based on a simple variant of the Lottery Ticket Hypothesis. Task-specific masks are obtained from annotated data in a source language, and language-specific masks from masked language modeling in a target language. Both these masks can then be composed with the pretrained model. Unlike adapter-based fine-tuning, this method neither increases the number of parameters at inference time nor alters the original model architecture. Most importantly, it outperforms adapters in zero-shot cross-lingual transfer by a large margin in a series of multilingual benchmarks, including Universal Dependencies, MasakhaNER, and AmericasNLI. Based on an in-depth analysis, we additionally find that sparsity is crucial to prevent both 1) interference between the fine-tunings to be composed and 2) overfitting. We release the code and models at https://github.com/cambridgeltl/composable-sft.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "107745709",
                        "name": "Alan Ansell"
                    },
                    {
                        "authorId": "3381663",
                        "name": "E. Ponti"
                    },
                    {
                        "authorId": "145762466",
                        "name": "A. Korhonen"
                    },
                    {
                        "authorId": "1747849",
                        "name": "Ivan Vulic"
                    }
                ]
            }
        },
        {
            "contexts": [
                "IMP and its variations [22, 46] succeed in deeper networks like Residual Networks (Resnet)-50 and Bidirectional Encoder Representations from Transformers (BERT) network [11]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "24fb91f134b9291f7535e7f6305a925a4f63440e",
                "externalIds": {
                    "ArXiv": "2110.05667",
                    "DBLP": "journals/corr/abs-2110-05667",
                    "CorpusId": 238634794
                },
                "corpusId": 238634794,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/24fb91f134b9291f7535e7f6305a925a4f63440e",
                "title": "Why Lottery Ticket Wins? A Theoretical Perspective of Sample Complexity on Pruned Neural Networks",
                "abstract": "The \\textit{lottery ticket hypothesis} (LTH) states that learning on a properly pruned network (the \\textit{winning ticket}) improves test accuracy over the original unpruned network. Although LTH has been justified empirically in a broad range of deep neural network (DNN) involved applications like computer vision and natural language processing, the theoretical validation of the improved generalization of a winning ticket remains elusive. To the best of our knowledge, our work, for the first time, characterizes the performance of training a pruned neural network by analyzing the geometric structure of the objective function and the sample complexity to achieve zero generalization error. We show that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, when the algorithm for training a pruned neural network is specified as an (accelerated) stochastic gradient descent algorithm, we theoretically show that the number of samples required for achieving zero generalization error is proportional to the number of the non-pruned weights in the hidden layer. With a fixed number of samples, training a pruned neural network enjoys a faster convergence rate to the desired model than training the original unpruned one, providing a formal justification of the improved generalization of the winning ticket. Our theoretical results are acquired from learning a pruned neural network of one hidden layer, while experimental results are further provided to justify the implications in pruning multi-layer neural networks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108432203",
                        "name": "Shuai Zhang"
                    },
                    {
                        "authorId": "2146059787",
                        "name": "Meng Wang"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "153191489",
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "authorId": "145042856",
                        "name": "Jinjun Xiong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similarly, F is defined via the refinement procedure used, making it dependent on the choice of optimizer and whether or not the DNN parameters are left alone or \u201crewound\u201d to their value at a previous point during training (Frankle et al., 2020a; Renda et al., 2020)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c3a99e978d806d0426ae0b25f245c66c07e94cb1",
                "externalIds": {
                    "ArXiv": "2110.03210",
                    "DBLP": "conf/icml/RedmanCWD22",
                    "CorpusId": 246430593
                },
                "corpusId": 246430593,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c3a99e978d806d0426ae0b25f245c66c07e94cb1",
                "title": "Universality of Winning Tickets: A Renormalization Group Perspective",
                "abstract": "Foundational work on the Lottery Ticket Hypothesis has suggested an exciting corollary: winning tickets found in the context of one task can be transferred to similar tasks, possibly even across different architectures. This has generated broad interest, but methods to study this universality are lacking. We make use of renormalization group theory, a powerful tool from theoretical physics, to address this need. We \ufb01nd that iterative magnitude pruning, the principal algorithm used for discovering winning tickets, is a renormalization group scheme, and can be viewed as inducing a \ufb02ow in parameter space. We demonstrate that ResNet-50 models with transferable winning tickets have \ufb02ows with common properties, as would be expected from the theory. Similar observations are made for BERT models, with evidence that their \ufb02ows are near \ufb01xed points. Additionally, we leverage our framework to study winning tickets transferred across ResNet architectures, observ-ing that smaller models have \ufb02ows with more uniform properties than larger models, complicating transfer between them.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145036964",
                        "name": "William T. Redman"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "50173356",
                        "name": "Akshunna S. Dogra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As studied in [49], unstructured pruning can easily achieve far better pruning ratios than structured pruning, which is usually more constrained.",
                "As thoroughly studied in [49], pruning methods are divided into either unstructured or structured approaches."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "125e45af77cbec076dcf379b55c682595ac2dd88",
                "externalIds": {
                    "ArXiv": "2110.01397",
                    "DBLP": "journals/pami/YvinecDCB23",
                    "DOI": "10.1109/TPAMI.2022.3179616",
                    "CorpusId": 238259421,
                    "PubMed": "35653454"
                },
                "corpusId": 238259421,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/125e45af77cbec076dcf379b55c682595ac2dd88",
                "title": "RED++ : Data-Free Pruning of Deep Neural Networks via Input Splitting and Output Merging",
                "abstract": "Pruning Deep Neural Networks (DNNs) is a prominent field of study in the goal of inference runtime acceleration. In this paper, we introduce a novel data-free pruning protocol RED++. Only requiring a trained neural network, and not specific to any particular DNN, we exploit an adaptive data-free scalar hashing which exhibits redundancies among neuron weight values. We study the theoretical and empirical guarantees on the preservation of the accuracy from the hashing as well as the expected pruning ratio resulting from the exploitation of said redundancies. We propose a novel data-free pruning technique of DNN layers which removes the input-wise redundant operations. This algorithm is straightforward, parallelizable and offers novel perspective on DNN pruning by shifting the burden of large computation to efficient memory access and allocation. We provide theoretical guarantees on RED++ performance and empirically demonstrate its superiority over other data-free pruning methods and its competitiveness with data-driven ones on ResNets, MobileNets, and EfficientNets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1632928879",
                        "name": "Edouard Yvinec"
                    },
                    {
                        "authorId": "3190846",
                        "name": "Arnaud Dapogny"
                    },
                    {
                        "authorId": "51021910",
                        "name": "M. Cord"
                    },
                    {
                        "authorId": "2521061",
                        "name": "K\u00e9vin Bailly"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, IMP with learning rate rewinding, which repeats the learning rate schedule, shows better results in bigger networks (Renda, Frankle, and Carbin 2020).",
                "We apply unstructured pruning that removes more weights, more precisely LR rewinding [39], to prune the teacher model.",
                "Unstructured Pruning We use the standard iterative magnitude pruning method for unstructured pruning in our framework, specifically learning rate rewinding (LR rewinding) (Renda, Frankle, and Carbin 2020).",
                "The right plot shows the student\u2019s accuracies when different pruning algorithms (LR rewinding [39] and SynFlow [48]) are applied to the teacher.",
                "Recently, IMP with learning rate (LR) rewinding, which repeats the learning rate schedule, shows better results in bigger networks [39].",
                "In Section 5, we apply LR rewinding [39] to prune the model, and apply the vanilla KD [20] to distill the pruned teacher."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "04f41d1ac8f43797103090c7c328fa80bfff09ac",
                "externalIds": {
                    "ArXiv": "2109.14960",
                    "DBLP": "journals/corr/abs-2109-14960",
                    "DOI": "10.1007/978-3-031-20083-0_8",
                    "CorpusId": 238227092
                },
                "corpusId": 238227092,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/04f41d1ac8f43797103090c7c328fa80bfff09ac",
                "title": "Prune Your Model Before Distill It",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "93625509",
                        "name": "Jinhyuk Park"
                    },
                    {
                        "authorId": "3268846",
                        "name": "Albert No"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, interesting solutions for the third stage have been suggested that involve weight rewinding [10] and learning rate rewinding [17]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d4189bbd22e4e1417ce28f57d5046e5f377bc6cf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-10795",
                    "ArXiv": "2109.10795",
                    "CorpusId": 237592650
                },
                "corpusId": 237592650,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d4189bbd22e4e1417ce28f57d5046e5f377bc6cf",
                "title": "Neural network relief: a pruning algorithm based on neural activity",
                "abstract": "Current deep neural networks (DNNs) are overparameterized and use most of their neuronal connections during inference for each task. The human brain, however, developed specialized regions for different tasks and performs inference with a small fraction of its neuronal connections. We propose an iterative pruning strategy introducing a simple importance-score metric that deactivates unimportant connections, tackling overparameterization in DNNs and modulating the \ufb01ring patterns. The aim is to \ufb01nd the smallest number of connections that is still capable of solving a given task with comparable accuracy, i.e. a simpler subnetwork. We achieve comparable performance for LeNet architectures on MNIST, and signi\ufb01cantly higher parameter compression than state-of-the-art algorithms for VGG and ResNet architectures on CIFAR-10/100 and Tiny-ImageNet 2 . Our approach also performs well for the two different optimizers considered \u2013 Adam and SGD. The algorithm is not designed to minimize FLOPs when considering current hardware and software implementations, although it performs reasonably when compared to the state of the art.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2127734041",
                        "name": "Aleksandr Dekhovich"
                    },
                    {
                        "authorId": "2743835",
                        "name": "D. Tax"
                    },
                    {
                        "authorId": "143640415",
                        "name": "M. Sluiter"
                    },
                    {
                        "authorId": "73014519",
                        "name": "M. Bessa"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods. We used CIFAR10 for most reproductions along with additional experiments on the larger CIFAR100, which extends the results originally provided by the authors. We have also extended the list of tested network architectures to include Wide ResNets (Zagoruyko and Komodakis [2016]). The new experiments led us to discover the limitations of learning rate rewinding which can worsen pruning results on large architectures. Methodology We implemented the code ourselves in Python with TensorFlow 2, basing our implementation of the paper alone and without consulting the source code provided by the authors. We ran two sets of experiments. In the reproduction set, we have striven to exactly reproduce the experimental conditions of Renda et al. [2020]. We have also conducted additional experiments, which use other network architectures, effectively showing results previously unreported by the authors.",
                "[2019], Renda et al. [2020]). Lottery Ticket Hypothesis from Frankle and Carbin [2019] formulates a hypothesis that for every dense neural network, there exists a smaller subnetwork that matches or exceeds results of the original.",
                "2 Scope of reproducibility Renda et al. [2020] formulated the following claims: Claim 1: Widely used method of training after pruning: finetuning yields worse results than rewinding based methods (supported by figures 1, 2, 3, 4 and Table 5) Claim 2: Newly introduced learning rate rewinding works as good or better as weight rewinding in all scenarios (supported by figures 1, 2, 3, 4 and Table 5, but not supported by Figure 5) Claim 3: Iterative pruning with learning rate rewinding matches state-of-the-art pruning methods (supported by figures 1, 2, 3, 4 and Table 5, but not supported by Figure 5)",
                "Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods.",
                "For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones.",
                "Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019].",
                "Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods. We used CIFAR10 for most reproductions along with additional experiments on the larger CIFAR100, which extends the results originally provided by the authors. We have also extended the list of tested network architectures to include Wide ResNets (Zagoruyko and Komodakis [2016]).",
                "For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones. Therefore, most of the used networks are residual networks, which were originally proposed in He et al. [2016a]. Additionally, to verify the general usefulness of pruning and retraining methods proposed in Renda et al.",
                "For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones. Therefore, most of the used networks are residual networks, which were originally proposed in He et al. [2016a]. Additionally, to verify the general usefulness of pruning and retraining methods proposed in Renda et al. [2020] we extend the list of tested network architectures to much larger wide residual networks from Zagoruyko and Komodakis [2016]."
            ],
            "intents": [
                "result",
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "7daa84872b144388ea47096f0982252627e35e43",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-09670",
                    "ArXiv": "2109.09670",
                    "CorpusId": 237581693
                },
                "corpusId": 237581693,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7daa84872b144388ea47096f0982252627e35e43",
                "title": "Reproducibility Study: Comparing Rewinding and Fine-tuning in Neural Network Pruning",
                "abstract": "Scope of reproducibility: We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks from arXiv:2003.02389. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in arXiv:1803.03635 and 3) a new, original method involving learning rate rewinding, building upon Lottery Ticket Hypothesis. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods. We used CIFAR10 for most reproductions along with additional experiments on the larger CIFAR100, which extends the results originally provided by the authors. We have also extended the list of tested network architectures to include Wide ResNets. The new experiments led us to discover the limitations of learning rate rewinding which can worsen pruning results on large architectures. Results: We were able to reproduce the exact results reported by the authors in all originally reported scenarios. However, extended results on larger Wide Residual Networks have demonstrated the limitations of the newly proposed learning rate rewinding -- we observed a previously unreported accuracy degradation for low sparsity ranges. Nevertheless, the general conclusion of the paper still holds and was indeed reproduced.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2127471828",
                        "name": "Szymon Mikler"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026to modern neural networks (Han et al. 2015), many works have shown success through various pruning criteria in attaining a much sparse network that performs on par with or even better than the original unpruned network (Guo, Yao, and Chen 2016; Xiao and Wang 2019; Renda, Frankle, and Carbin 2020).",
                "Since then, many methods have been proposed in both unstructured pruning (Renda, Frankle, and Carbin 2020; Xiao and Wang 2019) and structured pruning (He, Zhang, and Sun 2017; Li et al. 2017)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3e9b48ef096863d697e1566e5d38d4ea2d53e6d2",
                "externalIds": {
                    "ArXiv": "2109.04660",
                    "DBLP": "journals/corr/abs-2109-04660",
                    "CorpusId": 237485532
                },
                "corpusId": 237485532,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3e9b48ef096863d697e1566e5d38d4ea2d53e6d2",
                "title": "Dynamic Collective Intelligence Learning: Finding Efficient Sparse Model via Refined Gradients for Pruned Weights",
                "abstract": "With the growth of deep neural networks (DNN), the number of DNN parameters has drastically increased. This makes DNN models hard to be deployed on resource-limited embedded systems. To alleviate this problem, dynamic pruning methods have emerged, which try to find diverse sparsity patterns during training by utilizing Straight-Through-Estimator (STE) to approximate gradients of pruned weights. STE can help the pruned weights revive in the process of finding dynamic sparsity patterns. However, using these coarse gradients causes training instability and performance degradation owing to the unreliable gradient signal of the STE approximation. In this work, to tackle this issue, we introduce refined gradients to update the pruned weights by forming dual forwarding paths from two sets (pruned and unpruned) of weights. We propose a novel Dynamic Collective Intelligence Learning (DCIL) which makes use of the learning synergy between the collective intelligence of both weight sets. We verify the usefulness of the refined gradients by showing enhancements in the training stability and the model performance on the CIFAR and ImageNet datasets. DCIL outperforms various previously proposed pruning schemes including other dynamic pruning methods with enhanced stability during training.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116315847",
                        "name": "Jang-Hyun Kim"
                    },
                    {
                        "authorId": "2048444163",
                        "name": "Jayeon Yoo"
                    },
                    {
                        "authorId": "2110792830",
                        "name": "Yeji Song"
                    },
                    {
                        "authorId": "1713608836",
                        "name": "Kiyoon Yoo"
                    },
                    {
                        "authorId": "101880623",
                        "name": "N. Kwak"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We compare our proposed approach with the latest state-ofthe-art in trojan mitigation techniques, including fine-tuning, bridge mode connectivity (BMC), Neural Attention Distillation (NAD), Maxup and Cutmix augmentation [18], [19], and our own version of fine-pruning based on learning rate rewinding [20], which we refer to as Learning-Rate rewinding and Compression, or LRComp."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0f237022dd17c2701fd45fd16acaaa94b9364d7a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-04566",
                    "ArXiv": "2109.04566",
                    "DOI": "10.1109/DASC/PiCom/CBDCom/Cy55231.2022.9927771",
                    "CorpusId": 237485502
                },
                "corpusId": 237485502,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0f237022dd17c2701fd45fd16acaaa94b9364d7a",
                "title": "SanitAIs: Unsupervised Data Augmentation to Sanitize Trojaned Neural Networks",
                "abstract": "Self-supervised learning (SSL) methods have resulted in broad improvements to neural network performance by leveraging large, untapped collections of unlabeled data to learn generalized underlying structure. In this work, we harness unsupervised data augmentation (UDA), an SSL technique, to mitigate backdoor or Trojan attacks on deep neural networks. We show that UDA is more effective at removing trojans than current state-of-the-art methods for both feature space and point triggers, over a range of model architectures, trojans, and data quantities provided for trojan removal. These results demonstrate that UDA is both an effective and practical approach to mitigating the effects of backdoors on neural networks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "38081739",
                        "name": "Kiran Karra"
                    },
                    {
                        "authorId": "32339320",
                        "name": "C. Ashcraft"
                    }
                ]
            }
        },
        {
            "contexts": [
                "LSTMs (Renda et al., 2020), and fully-weighted per-trained BERT (Chen et al.",
                "In NLP, previous works find that matching subnetworks exist early in training with Transformers (Yu et al., 2019), LSTMs (Renda et al., 2020), and fully-weighted per-trained BERT (Chen et al., 2020; Prasanna et al., 2020) or Vison-and-Language model (Gan et al., 2021), but not at initialization."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cebd54ea966172111d08b4f215a73fe84350562e",
                "externalIds": {
                    "DBLP": "conf/emnlp/ShenYKKM21",
                    "ACL": "2021.emnlp-main.231",
                    "ArXiv": "2109.03939",
                    "DOI": "10.18653/v1/2021.emnlp-main.231",
                    "CorpusId": 237454597
                },
                "corpusId": 237454597,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/cebd54ea966172111d08b4f215a73fe84350562e",
                "title": "What\u2019s Hidden in a One-layer Randomly Weighted Transformer?",
                "abstract": "We demonstrate that, hidden within one-layer randomly weighted neural networks, there exist subnetworks that can achieve impressive performance, without ever modifying the weight initializations, on machine translation tasks. To find subnetworks for one-layer randomly weighted neural networks, we apply different binary masks to the same weight matrix to generate different layers. Hidden within a one-layer randomly weighted Transformer, we find that subnetworks that can achieve 29.45/17.29 BLEU on IWSLT14/WMT14. Using a fixed pre-trained embedding layer, the previously found subnetworks are smaller than, but can match 98%/92% (34.14/25.24 BLEU) of the performance of, a trained Transformer_\\text{small/base} on IWSLT14/WMT14. Furthermore, we demonstrate the effectiveness of larger and deeper transformers in this setting, as well as the impact of different initialization methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2191455",
                        "name": "Sheng Shen"
                    },
                    {
                        "authorId": "9088433",
                        "name": "Z. Yao"
                    },
                    {
                        "authorId": "2111313627",
                        "name": "Douwe Kiela"
                    },
                    {
                        "authorId": "1732330",
                        "name": "K. Keutzer"
                    },
                    {
                        "authorId": "1717098",
                        "name": "Michael W. Mahoney"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026in large neural networks, the overhead of the pruning-reset-retraining cycle is unaffordable, and Frankle et al. then tried using weights after trained for iterations to reset the pruned network (Frankle et al. 2019; Renda, Frankle, and Carbin 2020) instead of the totally original initial weights."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "11ceb3e87711f0341345884cc0f9c4706e0f1069",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-03862",
                    "ArXiv": "2109.03862",
                    "CorpusId": 237452483
                },
                "corpusId": 237452483,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/11ceb3e87711f0341345884cc0f9c4706e0f1069",
                "title": "Juvenile state hypothesis: What we can learn from lottery ticket hypothesis researches?",
                "abstract": "The proposition of lottery ticket hypothesis revealed the relationship between network structure and initialization parameters and the learning potential of neural networks. The original lottery ticket hypothesis performs pruning and weight resetting after training convergence, exposing it to the problem of forgotten learning knowledge and potential high cost of training. Therefore, we propose a strategy that combines the idea of neural network structure search with a pruning algorithm to alleviate this problem. This algorithm searches and extends the network structure on existing winning ticket sub-network to producing new winning ticket recursively. This allows the training and pruning process to continue without compromising performance. A new winning ticket sub-network with deeper network structure, better generalization ability and better test performance can be obtained in this recursive manner. This method can solve: the difficulty of training or performance degradation of the sub-networks after pruning, the forgetting of the weights of the original lottery ticket hypothesis and the difficulty of generating winning ticket sub-network when the final network structure is not given. We validate this strategy on the MNIST and CIFAR-10 datasets. And after relating it to similar biological phenomena and relevant lottery ticket hypothesis studies in recent years, we will further propose a new hypothesis to discuss which factors that can keep a network juvenile, i.e., those possible factors that influence the learning potential or generalization performance of a neural network during training.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2141082713",
                        "name": "Di Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Renda et al.[42] provide experimental results suggesting that the training strategy of re-training is more significant to the final result.",
                "However, some recent analyses [39], [40], [42] give experimental evidence to show that retraining the model from pre-trained parameters makes no significant difference from retraining from randomly initialized weights."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0cb5bc408e8b362a21c7bb7f800d5ea9d37c4691",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-13728",
                    "ArXiv": "2108.13728",
                    "CorpusId": 237363414
                },
                "corpusId": 237363414,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0cb5bc408e8b362a21c7bb7f800d5ea9d37c4691",
                "title": "Pruning with Compensation: Efficient Channel Pruning for Deep Convolutional Neural Networks",
                "abstract": "Channel pruning is a promising technique to compress the parameters of deep convolutional neural networks(DCNN) and to speed up the inference. This paper aims to address the long-standing inefficiency of channel pruning. Most channel pruning methods recover the prediction accuracy by re-training the pruned model from the remaining parameters or random initialization. This re-training process is heavily dependent on the sufficiency of computational resources, training data, and human interference(tuning the training strategy). In this paper, a highly efficient pruning method is proposed to significantly reduce the cost of pruning DCNN. The main contributions of our method include: 1) pruning compensation, a fast and data-efficient substitute of re-training to minimize the post-pruning reconstruction loss of features, 2) compensation-aware pruning(CaP), a novel pruning algorithm to remove redundant or less-weighted channels by minimizing the loss of information, and 3) binary structural search with step constraint to minimize human interference. On benchmarks including CIFAR-10/100 and ImageNet, our method shows competitive pruning performance among the state-of-the-art retraining-based pruning methods and, more importantly, reduces the processing time by 95% and data usage by 90%.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2153722882",
                        "name": "Zhouyang Xie"
                    },
                    {
                        "authorId": "49978782",
                        "name": "Yan Fu"
                    },
                    {
                        "authorId": "2088673811",
                        "name": "Sheng-Zhao Tian"
                    },
                    {
                        "authorId": "3000618",
                        "name": "Junlin Zhou"
                    },
                    {
                        "authorId": "2400681",
                        "name": "Duanbing Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4a76823760d56dfee67803795890678654dab275",
                "externalIds": {
                    "ArXiv": "2108.13055",
                    "DBLP": "journals/corr/abs-2108-13055",
                    "DOI": "10.1007/s11633-022-1340-5",
                    "CorpusId": 237353190
                },
                "corpusId": 237353190,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4a76823760d56dfee67803795890678654dab275",
                "title": "Efficient Visual Recognition with Deep Neural Networks: A Survey on Recent Advances and New Directions",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2038030133",
                        "name": "Yang Wu"
                    },
                    {
                        "authorId": "1452735766",
                        "name": "Dingheng Wang"
                    },
                    {
                        "authorId": "7829127",
                        "name": "Xiaotong Lu"
                    },
                    {
                        "authorId": null,
                        "name": "Fan Yang"
                    },
                    {
                        "authorId": "1730243",
                        "name": "Guoqi Li"
                    },
                    {
                        "authorId": "2872774",
                        "name": "W. Dong"
                    },
                    {
                        "authorId": "46865129",
                        "name": "Jianbo Shi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8a19acc6e026a58f56f7a78ef02f1fc97a6eb25c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-00944",
                    "ArXiv": "2108.00944",
                    "DOI": "10.1002/int.22827",
                    "CorpusId": 236777012
                },
                "corpusId": 236777012,
                "publicationVenue": {
                    "id": "05528bac-d212-46a6-9c84-314d4bd77368",
                    "name": "International Journal of Intelligent Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Intell Syst"
                    ],
                    "issn": "0884-8173",
                    "url": "http://www3.interscience.wiley.com/cgi-bin/jhome/36062",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/1098111X"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8a19acc6e026a58f56f7a78ef02f1fc97a6eb25c",
                "title": "Exploring lottery ticket hypothesis in media recommender systems",
                "abstract": "Media recommender systems aim to capture users\u2019 preferences and provide precise personalized recommendation of media content. There are two critical components in the common paradigm of modern recommender models: (1) representation learning, which generates an embedding for each user and item; and (2) interaction modeling, which fits user preferences toward items based on their representations. In spite of great success, when a great amount of users and items exist, it usually needs to create, store, and optimize a huge embedding table, where the scale of model parameters easily reach millions or even larger. Hence, it naturally raises questions about the heavy recommender models: Do we really need such large\u2010scale parameters? We get inspirations from the recently proposed lottery ticket hypothesis (LTH), which argues that the dense and over\u2010parameterized model contains a much smaller and sparser sub\u2010model that can reach comparable performance to the full model. In this paper, we extend LTH to media recommender systems, aiming to find the winning tickets in deep recommender models. To the best of our knowledge, this is the first work to study LTH in media recommender systems. With Matrix Factorization and Light Graph Convolution Networks as the backbone models, we found that there widely exist winning tickets in recommender models. On three media convergence data sets\u2014Yelp2018, TikTok and Kwai, the winning tickets can achieve comparable recommendation performance with only 29 % ~ 48 % , 7 % ~ 10 % , and 3 % ~ 17 % of parameters, respectively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108846876",
                        "name": "Yanfang Wang"
                    },
                    {
                        "authorId": "2003767516",
                        "name": "Yongduo Sui"
                    },
                    {
                        "authorId": "2144796537",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "2145312301",
                        "name": "Zhenguang Liu"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such a finding offers a theoretical insight into the early-bird ticket phenomenon and provides intuition for why discovering high-performing subnetworks is more difficult in large-scale experiments [63, 52, 38]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2e1b34719554d130cfe7f5e6f2352cadec7b60a3",
                "externalIds": {
                    "ArXiv": "2108.00259",
                    "CorpusId": 244921154
                },
                "corpusId": 244921154,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2e1b34719554d130cfe7f5e6f2352cadec7b60a3",
                "title": "How much pre-training is enough to discover a good subnetwork?",
                "abstract": "Neural network pruning is useful for discovering efficient, high-performing subnetworks within pre-trained, dense network architectures. More often than not, it involves a three-step process -- pre-training, pruning, and re-training -- that is computationally expensive, as the dense model must be fully pre-trained. While previous work has revealed through experiments the relationship between the amount of pre-training and the performance of the pruned network, a theoretical characterization of such dependency is still missing. Aiming to mathematically analyze the amount of dense network pre-training needed for a pruned network to perform well, we discover a simple theoretical bound in the number of gradient descent pre-training iterations on a two-layer, fully-connected network, beyond which pruning via greedy forward selection [61] yields a subnetwork that achieves good training error. Interestingly, this threshold is shown to be logarithmically dependent upon the size of the dataset, meaning that experiments with larger datasets require more pre-training for subnetworks obtained via pruning to perform well. Lastly, we empirically validate our theoretical results on a multi-layer perceptron trained on MNIST.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34210029",
                        "name": "Cameron R. Wolfe"
                    },
                    {
                        "authorId": "2154497049",
                        "name": "Qihan Wang"
                    },
                    {
                        "authorId": "2120215686",
                        "name": "J. Kim"
                    },
                    {
                        "authorId": "2126894228",
                        "name": "Anastasios Kyrillidis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To design the progressive pruning schedule, we develop a straightforward heuristic design, following the commonly used schedule in most pruning works [17, 26, 35]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3d583bbc7f9cef30c6b57380148d7dccb05a8967",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-07706",
                    "ArXiv": "2107.07706",
                    "DOI": "10.1145/3510835",
                    "CorpusId": 236034380
                },
                "corpusId": 236034380,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3d583bbc7f9cef30c6b57380148d7dccb05a8967",
                "title": "DANCE: DAta-Network Co-optimization for Efficient Segmentation Model Training and Inference",
                "abstract": "Semantic segmentation for scene understanding is nowadays widely demanded, raising significant challenges for the algorithm efficiency, especially its applications on resource-limited platforms. Current segmentation models are trained and evaluated on massive high-resolution scene images (\u201cdata-level\u201d) and suffer from the expensive computation arising from the required multi-scale aggregation (\u201cnetwork level\u201d). In both folds, the computational and energy costs in training and inference are notable due to the often desired large input resolutions and heavy computational burden of segmentation models. To this end, we propose DANCE, general automated DAta-Network Co-optimization for Efficient segmentation model training and inference. Distinct from existing efficient segmentation approaches that focus merely on light-weight network design, DANCE distinguishes itself as an automated simultaneous data-network co-optimization via both input data manipulation and network architecture slimming. Specifically, DANCE integrates automated data slimming which adaptively downsamples/drops input images and controls their corresponding contribution to the training loss guided by the images\u2019 spatial complexity. Such a downsampling operation, in addition to slimming down the cost associated with the input size directly, also shrinks the dynamic range of input object and context scales, therefore motivating us to also adaptively slim the network to match the downsampled data. Extensive experiments and ablating studies (on four SOTA segmentation models with three popular segmentation datasets under two training settings) demonstrate that DANCE can achieve \u201call-win\u201d towards efficient segmentation (reduced training cost, less expensive inference, and better mean Intersection-over-Union (mIoU)). Specifically, DANCE can reduce \u219325%\u2013\u219377% energy consumption in training, \u219331%\u2013\u219356% in inference, while boosting the mIoU by \u21930.71%\u2013\u2191 13.34%.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "28987646",
                        "name": "Chaojian Li"
                    },
                    {
                        "authorId": "2005440168",
                        "name": "Wuyang Chen"
                    },
                    {
                        "authorId": "2008146673",
                        "name": "Yuchen Gu"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2116099023",
                        "name": "Yonggan Fu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "3138925",
                        "name": "Yingyan Lin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For CNN compression, the general procedure can be largely summarized as: (i) train a full model; (ii) identify and prune the redundant structures to build a slimmer model based on various criteria, including (structured) sparsity (58; 85; 14; 56; 102; 27; 102; 62; 91), Bayesian pruning (101; 65; 59; 81), ranking importance (54; 60; 41; 36; 57; 100), reinforcement learning (37; 7), adversarial robustness (76), scientific control (79), lottery ticket (23; 24; 72), joint quantization learning (80; 90), etc."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "88ad1104e61d25c6e8919cb6d2af0aaedd6f0526",
                "externalIds": {
                    "ArXiv": "2107.07467",
                    "DBLP": "conf/nips/ChenJDFWZLSYT21",
                    "CorpusId": 235899080
                },
                "corpusId": 235899080,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/88ad1104e61d25c6e8919cb6d2af0aaedd6f0526",
                "title": "Only Train Once: A One-Shot Neural Network Training And Pruning Framework",
                "abstract": "Structured pruning is a commonly used technique in deploying deep neural networks (DNNs) onto resource-constrained devices. However, the existing pruning methods are usually heuristic, task-specified, and require an extra fine-tuning procedure. To overcome these limitations, we propose a framework that compresses DNNs into slimmer architectures with competitive performances and significant FLOPs reductions by Only-Train-Once (OTO). OTO contains two keys: (i) we partition the parameters of DNNs into zero-invariant groups, enabling us to prune zero groups without affecting the output; and (ii) to promote zero groups, we then formulate a structured-sparsity optimization problem and propose a novel optimization algorithm, Half-Space Stochastic Projected Gradient (HSPG), to solve it, which outperforms the standard proximal methods on group sparsity exploration and maintains comparable convergence. To demonstrate the effectiveness of OTO, we train and compress full models simultaneously from scratch without fine-tuning for inference speedup and parameter reduction, and achieve state-of-the-art results on VGG16 for CIFAR10, ResNet50 for CIFAR10 and Bert for SQuAD and competitive result on ResNet50 for ImageNet. The source code is available at https://github.com/tianyic/only_train_once.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143874667",
                        "name": "Tianyi Chen"
                    },
                    {
                        "authorId": "46688866",
                        "name": "Bo Ji"
                    },
                    {
                        "authorId": "1937503",
                        "name": "Tianyu Ding"
                    },
                    {
                        "authorId": "2058001858",
                        "name": "Biyi Fang"
                    },
                    {
                        "authorId": "2230843",
                        "name": "Guanyi Wang"
                    },
                    {
                        "authorId": "145687539",
                        "name": "Zhihui Zhu"
                    },
                    {
                        "authorId": "46225943",
                        "name": "Luming Liang"
                    },
                    {
                        "authorId": "2118898072",
                        "name": "Yixin Shi"
                    },
                    {
                        "authorId": "1599125512",
                        "name": "Sheng Yi"
                    },
                    {
                        "authorId": "2053747574",
                        "name": "Xiao Tu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We distinguish our definition of the lottery ticket hypothesis from the weight rewinding technique [5, 12].",
                "In this paper, we follow the notations from [1, 5].",
                "Unfortunately, among the various researches on the lottery ticket hypothesis [2, 3, 4, 5, 6, 7, 8], there are many inconsistencies regarding the settings of training recipe, and they further lead to the controversies over the conditions for identifying winning tickets.",
                "In the lottery ticket hypothesis studies, it is a standard setting to use the same learning rate in pretraining (for finding the mask by pruning thereafter) and subnetwork training (for training the sparse model) [1, 5, 12].",
                "IMP(\u00b7) prunes 20% of remaining weights per iteration until arriving at target sparsity s [5].",
                "We find the weight rewinding technique [5] consistently improves the subnetwork accuracy.",
                "For IMP(\u00b7), we follow the settings in [1, 5] that 20% of the weights are pruned in each iteration.",
                "The following works [5, 12] extend the subnetwork training from initial weights to the weights at early stage of pretraining (rewinding), and improve the accuracy in more challenging tasks at nontrivial sparsity."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2dbd1fc62f13cceb223f1caa70424cc3ccaeea4f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-00166",
                    "ArXiv": "2107.00166",
                    "CorpusId": 235694458
                },
                "corpusId": 235694458,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2dbd1fc62f13cceb223f1caa70424cc3ccaeea4f",
                "title": "Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?",
                "abstract": "There have been long-standing controversies and inconsistencies over the experiment setup and criteria for identifying the\"winning ticket\"in literature. To reconcile such, we revisit the definition of lottery ticket hypothesis, with comprehensive and more rigorous conditions. Under our new definition, we show concrete evidence to clarify whether the winning ticket exists across the major DNN architectures and/or applications. Through extensive experiments, we perform quantitative analysis on the correlations between winning tickets and various experimental factors, and empirically study the patterns of our observations. We find that the key training hyperparameters, such as learning rate and training epochs, as well as the architecture characteristics such as capacities and residual connections, are all highly correlated with whether and when the winning tickets can be identified. Based on our analysis, we summarize a guideline for parameter settings in regards of specific architecture characteristics, which we hope to catalyze the research progress on the topic of lottery ticket hypothesis. Our codes are publicly available at: https://github.com/boone891214/sanity-check-LTH.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "151480882",
                        "name": "Xiaolong Ma"
                    },
                    {
                        "authorId": "9347641",
                        "name": "Geng Yuan"
                    },
                    {
                        "authorId": "2007668856",
                        "name": "Xuan Shen"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "1753619492",
                        "name": "Xuxi Chen"
                    },
                    {
                        "authorId": "2116298570",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "2152354569",
                        "name": "Ning Liu"
                    },
                    {
                        "authorId": "39449475",
                        "name": "Minghai Qin"
                    },
                    {
                        "authorId": "143743061",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "46393431",
                        "name": "Yanzhi Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This finding helps to explain several observations (1) for gradual magnitude pruning (GMP), it is always optimal to end pruning before the second learning rate drop [77, 13]; (2) dynamic sparse training (DST) benefits from a monotonically decreasing pruning rate with cosine or linear update schedule [8, 9]; (3) rewinding techniques [12, 54] outperform fine-tuning as rewinding retrains subnetworks with the original learning rate schedule whereas fine-tuning often retrains with the smallest learning rate.",
                "The process of post-training pruning typically involves fully pre-training a dense network as well as many cycles of retraining (either fine-tuning [18, 17, 39] or rewinding [12, 54]).",
                "Recently, posttraining pruning [49, 29, 18, 47, 10, 54, 74, 5, 57, 75] and before-training pruning [31, 30, 67, 63, 6, 11] have been two fast-rising fields, boosted by lottery tickets hypothesis (LTH) [10] and singleshot network pruning (SNIP) [31].",
                "Later on, learning rate rewinding (LRR) [54] was proposed further to improve the re-training performance by only rewinding the learning rate.",
                "This finding makes a connection to the success of the iterative magnitude pruning [10, 54, 5, 6, 65], where usually a pruning process with a small pruning rate (e."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a85ba5bb3e97c999f5f6dbc78f277b107af1dba2",
                "externalIds": {
                    "DBLP": "conf/nips/LiuCCAYKSPWM21",
                    "ArXiv": "2106.10404",
                    "CorpusId": 235490153
                },
                "corpusId": 235490153,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a85ba5bb3e97c999f5f6dbc78f277b107af1dba2",
                "title": "Sparse Training via Boosting Pruning Plasticity with Neuroregeneration",
                "abstract": "Works on lottery ticket hypothesis (LTH) and single-shot network pruning (SNIP) have raised a lot of attention currently on post-training pruning (iterative magnitude pruning), and before-training pruning (pruning at initialization). The former method suffers from an extremely large computation cost and the latter usually struggles with insufficient performance. In comparison, during-training pruning, a class of pruning methods that simultaneously enjoys the training/inference efficiency and the comparable performance, temporarily, has been less explored. To better understand during-training pruning, we quantitatively study the effect of pruning throughout training from the perspective of pruning plasticity (the ability of the pruned networks to recover the original performance). Pruning plasticity can help explain several other empirical observations about neural network pruning in literature. We further find that pruning plasticity can be substantially improved by injecting a brain-inspired mechanism called neuroregeneration, i.e., to regenerate the same number of connections as pruned. We design a novel gradual magnitude pruning (GMP) method, named gradual pruning with zero-cost neuroregeneration (\\textbf{GraNet}), that advances state of the art. Perhaps most impressively, its sparse-to-sparse version for the first time boosts the sparse-to-sparse training performance over various dense-to-sparse methods with ResNet-50 on ImageNet without extending the training time. We release all codes in https://github.com/Shiweiliuiiiiiii/GraNet.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2116298570",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "1768130522",
                        "name": "Zahra Atashgahi"
                    },
                    {
                        "authorId": "1410465360",
                        "name": "Lu Yin"
                    },
                    {
                        "authorId": "2113957014",
                        "name": "Huanyu Kou"
                    },
                    {
                        "authorId": null,
                        "name": "Li Shen"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[32] reported similar findings for different pruning approaches [43, 2] applied to a ResNet-20 model [19] tested on an analogously corrupted dataset, CIFAR-10-C.",
                "We experiment with four models of increasing size (ResNeXt-29, ResNet-18, ResNet-50, WideResNet-18-2), three data augmentation methods (clean, AugMix, Gaussian), two sparsity levels (90%, 95%), and six compression methods (LTH, LRR, EP (layerwise and global), BP (layerwise and global)).",
                "To test the CARD hypothesis, we use: five models (VGG [12, 46] and ResNet [19] style architectures of varying size), five sparsity levels (50%, 60%, 80%, 90%, 95%), and six model compression methods (FT, GMP, LTH, LRR, EP, BP).",
                "Our best performing 6-CARD-Deck using LRR WideResNet-18 models (53.58 MB) sets a new state-of-the-art for CIFAR-10 and CIFAR-10-C accuracies of 96.8% and 92.75%, respectively.",
                "Notably, we found a single LRR CARD (a WideResNet-18 at 96% sparsity) trained with AugMix can attain 91.24% CIFAR-10-C accuracy, outperforming dense ResNeXt-29 trained with AugMix (a state-of-the-art among methods that do not require non-CIFAR-10 training data) by more than 2 percentage points simply by pruning a larger model, i.e., WideResNet-18.",
                ", fine tuning [18] and gradual magnitude pruning [60]), \u201clottery ticket-style\u201d compression approaches [12, 43, 41, 9] can surprisingly be used to create CARDs.",
                "Similar experiments are performed for CIFAR-100 and CIFAR-100-C, however only WideResNet-18-2 and four model compression methods (LTH, LRR, EP (global), BP (global)) are used.",
                "The difference heatmaps show that rewinding methods offer mild to moderate improvements across much of the frequency spectrum with LRR outperforming LTH in a few regions of the heatmap.",
                "Weight rewinding (LTH) [12, 13] is iterative like GMP but fully trains the network, prunes, rewinds the unpruned weights (and learning rate schedule) to their values early in training, then fully trains the subnetwork before pruning again; learning rate rewinding (LRR) [43] is identical to LTH, except only the learning rate schedule is rewound, not the unpruned weights.",
                "For a comprehensive analysis of existing pruning methods, we introduce a framework inspired by those in [43, 51] that covers traditional-through-emerging pruning methodologies.",
                "To summarize, CARD-Decks can maintain compactness while leveraging additional robustness improvement techniques, LRR CARD-Decks set a new SOTA on CIFAR-10-C and CIFAR-100-C in terms of accuracy and robustness, binary-weight CARD-Decks can provide up to \u223c105x reduction in memory while providing comparable accuracy and robustness, and the domain-adaptive CARD-Decks used here are \u223c2x faster than the domain-agnostic CARD-Decks as only half of the CARDs are used at inference."
            ],
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "7bb118ed195e5d39ffbeab40dae2e5800eb33beb",
                "externalIds": {
                    "DBLP": "conf/nips/DiffenderferBCZ21",
                    "ArXiv": "2106.09129",
                    "CorpusId": 235458254
                },
                "corpusId": 235458254,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7bb118ed195e5d39ffbeab40dae2e5800eb33beb",
                "title": "A Winning Hand: Compressing Deep Networks Can Improve Out-Of-Distribution Robustness",
                "abstract": "Successful adoption of deep learning (DL) in the wild requires models to be: (1) compact, (2) accurate, and (3) robust to distributional shifts. Unfortunately, efforts towards simultaneously meeting these requirements have mostly been unsuccessful. This raises an important question: Is the inability to create Compact, Accurate, and Robust Deep neural networks (CARDs) fundamental? To answer this question, we perform a large-scale analysis of popular model compression techniques which uncovers several intriguing patterns. Notably, in contrast to traditional pruning approaches (e.g., fine tuning and gradual magnitude pruning), we find that\"lottery ticket-style\"approaches can surprisingly be used to produce CARDs, including binary-weight CARDs. Specifically, we are able to create extremely compact CARDs that, compared to their larger counterparts, have similar test accuracy and matching (or better) robustness -- simply by pruning and (optionally) quantizing. Leveraging the compactness of CARDs, we develop a simple domain-adaptive test-time ensembling approach (CARD-Decks) that uses a gating module to dynamically select appropriate CARDs from the CARD-Deck based on their spectral-similarity with test samples. The proposed approach builds a\"winning hand'' of CARDs that establishes a new state-of-the-art (on RobustBench) on CIFAR-10-C accuracies (i.e., 96.8% standard and 92.75% robust) and CIFAR-100-C accuracies (80.6% standard and 71.3% robust) with better memory usage than non-compressed baselines (pretrained CARDs and CARD-Decks available at https://github.com/RobustBench/robustbench). Finally, we provide theoretical support for our empirical findings.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7753616",
                        "name": "James Diffenderfer"
                    },
                    {
                        "authorId": "41053241",
                        "name": "Brian Bartoldson"
                    },
                    {
                        "authorId": "2113244850",
                        "name": "Shreya Chaganti"
                    },
                    {
                        "authorId": "2107967251",
                        "name": "Jize Zhang"
                    },
                    {
                        "authorId": "1749353",
                        "name": "B. Kailkhura"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For the best results, this can be iterated, alternatingly pruning weights and retraining the network (Renda et al., 2019).",
                "For example, Renda et al. (2019) show that complete retraining is superior to just fine-tuning when pruning iteratively."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1f31db2bf95133655c29fd3bedfc2912bc76eb85",
                "externalIds": {
                    "ArXiv": "2106.06955",
                    "DBLP": "journals/corr/abs-2106-06955",
                    "CorpusId": 235421894
                },
                "corpusId": 235421894,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1f31db2bf95133655c29fd3bedfc2912bc76eb85",
                "title": "Towards Understanding Iterative Magnitude Pruning: Why Lottery Tickets Win",
                "abstract": "The lottery ticket hypothesis states that sparse subnetworks exist in randomly initialized dense networks that can be trained to the same accuracy as the dense network they reside in. However, the subsequent work has failed to replicate this on large-scale models and required rewinding to an early stable state instead of initialization. We show that by using a training method that is stable with respect to linear mode connectivity, large networks can also be entirely rewound to initialization. Our subsequent experiments on common vision tasks give strong credence to the hypothesis in Evci et al. (2020b) that lottery tickets simply retrain to the same regions (although not necessarily to the same basin). These results imply that existing lottery tickets could not have been found without the preceding dense training by iterative magnitude pruning, raising doubts about the use of the lottery ticket hypothesis.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2051865997",
                        "name": "Jaron Maene"
                    },
                    {
                        "authorId": "2112132080",
                        "name": "Mingxiao Li"
                    },
                    {
                        "authorId": "100781843",
                        "name": "Marie-Francine Moens"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also examined the effectiveness of IMP with different rewinding starting points as studied in [29, 69], and found rewinding initializations bear minimal effect on downstream ASR.",
                "This step is referred to as subnetwork finetuning in the pruning literature [53, 69, 9]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "362635eb7cd72d4ca7414cb257dadbced12fbe8f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-05933",
                    "ArXiv": "2106.05933",
                    "CorpusId": 235390847
                },
                "corpusId": 235390847,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/362635eb7cd72d4ca7414cb257dadbced12fbe8f",
                "title": "PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition",
                "abstract": "Recent work on speech self-supervised learning (speech SSL) demonstrated the benefits of scale in learning rich and transferable representations for Automatic Speech Recognition (ASR) with limited parallel data. It is then natural to investigate the existence of sparse and transferrable subnetworks in pre-trained speech SSL models that can achieve even better low-resource ASR performance. However, directly applying widely adopted pruning methods such as the Lottery Ticket Hypothesis (LTH) is suboptimal in the computational cost needed. Moreover, contrary to what LTH predicts, the discovered subnetworks yield minimal performance gain compared to the original dense network. In this work, we propose Prune-AdjustRe-Prune (PARP), which discovers and finetunes subnetworks for much better ASR performance, while only requiring a single downstream finetuning run. PARP is inspired by our surprising observation that subnetworks pruned for pre-training tasks only needed to be slightly adjusted to achieve a sizeable performance boost in downstream ASR tasks. Extensive experiments on low-resource English and multilingual ASR show (1) sparse subnetworks exist in pre-trained speech SSL, and (2) the computational advantage and performance gain of PARP over baseline pruning methods. On the 10min Librispeech split without LM decoding, PARP discovers subnetworks from wav2vec 2.0 with an absolute 10.9%/12.6% WER decrease compared to the full model. We demonstrate PARP mitigates performance degradation in cross-lingual mask transfer, and investigate the possibility of discovering a single subnetwork for 10 spoken languages in one run.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51266409",
                        "name": "Cheng-I Lai"
                    },
                    {
                        "authorId": "37873860",
                        "name": "Yang Zhang"
                    },
                    {
                        "authorId": "49285584",
                        "name": "Alexander H. Liu"
                    },
                    {
                        "authorId": "3307026",
                        "name": "Shiyu Chang"
                    },
                    {
                        "authorId": "2048004675",
                        "name": "Yi Liao"
                    },
                    {
                        "authorId": "2475831",
                        "name": "Yung-Sung Chuang"
                    },
                    {
                        "authorId": "9566679",
                        "name": "Kaizhi Qian"
                    },
                    {
                        "authorId": "40570741",
                        "name": "Sameer Khurana"
                    },
                    {
                        "authorId": "2064714013",
                        "name": "David Cox"
                    },
                    {
                        "authorId": "145898106",
                        "name": "James R. Glass"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Pruningmethods [7, 22, 28, 46, 47, 52] require training of a dense network as well as iterative cycles of pruning and retraining."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d59324d2534051042ed575e98f656a9e5dfe041c",
                "externalIds": {
                    "ArXiv": "2106.04217",
                    "DBLP": "journals/corr/abs-2106-04217",
                    "DOI": "10.24963/ijcai.2022/477",
                    "CorpusId": 235368271
                },
                "corpusId": 235368271,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d59324d2534051042ed575e98f656a9e5dfe041c",
                "title": "Dynamic Sparse Training for Deep Reinforcement Learning",
                "abstract": "Deep reinforcement learning (DRL) agents are trained through trial-and-error interactions with the environment. This leads to a long training time for dense neural networks to achieve good performance. Hence, prohibitive computation and memory resources are consumed. Recently, learning efficient DRL agents has received increasing attention. Yet, current methods focus on accelerating inference time. In this paper, we introduce for the first time a dynamic sparse training approach for deep reinforcement learning to accelerate the training process. The proposed approach trains a sparse neural network from scratch and dynamically adapts its topology to the changing data distribution during training. Experiments on continuous control tasks show that our dynamic sparse agents achieve higher performance than the equivalent dense methods, reduce the parameter count and floating-point operations (FLOPs) by 50%, and have a faster learning speed that enables reaching the performance of dense agents with 40\u221250% reduction in the training steps.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "67102129",
                        "name": "Ghada Sokar"
                    },
                    {
                        "authorId": "146634864",
                        "name": "Elena Mocanu"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "144848112",
                        "name": "P. Stone"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Later on, (Frankle et al., 2019a; Renda et al., 2020) scaled up LTH to larger models by early weight rewinding that relaxes the use of original random initialization."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6bc4681828143f5ecc49b7ecd388a86c70c7237a",
                "externalIds": {
                    "DBLP": "conf/icml/ZhangCCW21",
                    "ArXiv": "2106.03225",
                    "CorpusId": 235358439
                },
                "corpusId": 235358439,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6bc4681828143f5ecc49b7ecd388a86c70c7237a",
                "title": "Efficient Lottery Ticket Finding: Less Data is More",
                "abstract": "The lottery ticket hypothesis (LTH) reveals the existence of winning tickets (sparse but critical subnetworks) for dense networks, that can be trained in isolation from random initialization to match the latter's accuracies. However, finding winning tickets requires burdensome computations in the train-prune-retrain process, especially on large-scale datasets (e.g., ImageNet), restricting their practical benefits. This paper explores a new perspective on finding lottery tickets more efficiently, by doing so only with a specially selected subset of data, called Pruning-Aware Critical set (PrAC set), rather than using the full training set. The concept of PrAC set was inspired by the recent observation, that deep networks have samples that are either hard to memorize during training, or easy to forget during pruning. A PrAC set is thus hypothesized to capture those most challenging and informative examples for the dense model. We observe that a high-quality winning ticket can be found with training and pruning the dense network on the very compact PrAC set, which can substantially save training iterations for the ticket finding process. Extensive experiments validate our proposal across diverse datasets and network architectures. Specifically, on CIFAR-10, CIFAR-100, and Tiny ImageNet, we locate effective PrAC sets at 35.32%~78.19% of their training set sizes. On top of them, we can obtain the same competitive winning tickets for the corresponding dense networks, yet saving up to 82.85%~92.77%, 63.54%~74.92%, and 76.14%~86.56% training iterations, respectively. Crucially, we show that a PrAC set found is reusable across different network architectures, which can amortize the extra cost of finding PrAC sets, yielding a practical regime for efficient lottery ticket finding.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109338656",
                        "name": "Zhenyu (Allen) Zhang"
                    },
                    {
                        "authorId": "1753619492",
                        "name": "Xuxi Chen"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "de377e20a7a4e82a26b56c1d073a4306aaac853d",
                "externalIds": {
                    "DBLP": "conf/aicas/NakataMMTSDF21",
                    "DOI": "10.1109/AICAS51828.2021.9458452",
                    "CorpusId": 235616547
                },
                "corpusId": 235616547,
                "publicationVenue": {
                    "id": "c00492c7-e775-4f0a-a843-9f5bbb219dbe",
                    "name": "International Conference on Artificial Intelligence Circuits and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "AICAS",
                        "Int Conf Artif Intell Circuit Syst"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/de377e20a7a4e82a26b56c1d073a4306aaac853d",
                "title": "Quantization Strategy for Pareto-optimally Low-cost and Accurate CNN",
                "abstract": "Quantization is an effective technique to reduce memory and computational costs for inference of convolutional neural networks (CNNs). However, it has not been clarified which model can achieve higher recognition accuracy with lower memory and computational costs: a fat model (large number of parameters) quantized to an extremely low bit width (e.g., 1 or 2 bits) or a slim model (small number of parameters) quantized to moderately low bit width (e.g., 4 or 5 bits). To answer this question, we define a metric that combines the number of parameters and computations with bit widths of quantized weight parameters. Using this metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given memory or computational cost, is achieved when a slim model is moderately quantized rather than when a fat model is extremely quantized. Moreover, employing a strategy based on this finding, we empirically show that the Pareto frontier is improved by 4.3\u00d7 under a post-training quantization scenario on the ImageNet dataset.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2047738",
                        "name": "K. Nakata"
                    },
                    {
                        "authorId": "2441156",
                        "name": "D. Miyashita"
                    },
                    {
                        "authorId": "38136807",
                        "name": "A. Maki"
                    },
                    {
                        "authorId": "38257658",
                        "name": "F. Tachibana"
                    },
                    {
                        "authorId": "47483363",
                        "name": "S. Sasaki"
                    },
                    {
                        "authorId": "49192096",
                        "name": "J. Deguchi"
                    },
                    {
                        "authorId": "2113811899",
                        "name": "Ryuichi Fujimoto"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, a recent study [11] shows that a 5:96 parameter reduction of ResNet-50 can well retain the accuracy performance of the original network by weight pruning, however, it is only a 1 reduction in filter pruning."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0338285d0ae36a1d6ce9cc3d44e132492212f7ac",
                "externalIds": {
                    "ArXiv": "2105.14713",
                    "DBLP": "journals/pami/LinZLCCWLTJ23",
                    "DOI": "10.1109/TPAMI.2022.3195774",
                    "CorpusId": 244347939,
                    "PubMed": "35917571"
                },
                "corpusId": 244347939,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0338285d0ae36a1d6ce9cc3d44e132492212f7ac",
                "title": "1xN Pattern for Pruning Convolutional Neural Networks",
                "abstract": "Though network pruning receives popularity in reducing the complexity of convolutional neural networks (CNNs), it remains an open issue to concurrently maintain model accuracy as well as achieve significant speedups on general CPUs. In this paper, we propose a novel 1\u00d7N pruning pattern to break this limitation. In particular, consecutive N output kernels with the same input channel index are grouped into one block, which serves as a basic pruning granularity of our pruning pattern. Our 1\u00d7N pattern prunes these blocks considered unimportant. We also provide a workflow of filter rearrangement that first rearranges the weight matrix in the output channel dimension to derive more influential blocks for accuracy improvements and then applies similar rearrangement to the next-layer weights in the input channel dimension to ensure correct convolutional operations. Moreover, the output computation after our 1\u00d7N pruning can be realized via a parallelized block-wise vectorized operation, leading to significant speedups on general CPUs. The efficacy of our pruning pattern is proved with experiments on ILSVRC-2012. For example, given the pruning rate of 50% and N=4, our pattern obtains about 3.0% improvements over filter pruning in the top-1 accuracy of MobileNet-V2. Meanwhile, it obtains 56.04ms inference savings on Cortex-A7 CPU over weight pruning. Our project is made available at https://github.com/lmbxmu/1xN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49352079",
                        "name": "Mingbao Lin"
                    },
                    {
                        "authorId": "2145067719",
                        "name": "Yu-xin Zhang"
                    },
                    {
                        "authorId": "2110482496",
                        "name": "Yuchao Li"
                    },
                    {
                        "authorId": "2152690044",
                        "name": "Bohong Chen"
                    },
                    {
                        "authorId": "2070266415",
                        "name": "Fei Chao"
                    },
                    {
                        "authorId": "50468734",
                        "name": "Mengdi Wang"
                    },
                    {
                        "authorId": "2153701890",
                        "name": "Shen Li"
                    },
                    {
                        "authorId": "40161651",
                        "name": "Yonghong Tian"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The hypothesis has successfully shown its success in various fields (Yu et al., 2020; Renda et al., 2020; Chen et al., 2020b), and its property has been studied widely (Malach et al., 2020; Pensia et al., 2020; Elesedy et al., 2020).",
                "It was pointed out by Renda et al. (2020) that subnetworks found by IMP and rewound early in training can be trained to achieve the same accuracy at the same sparsity as subnetworks found by the standard pruning, providing a possibility that rewinding can also help GAN subnetworks.",
                "The hypothesis has successfully shown its success in various fields (Yu et al., 2020; Renda et al., 2020; Chen et al., 2020b), and its property has been studied widely (Malach et al.",
                "In order to scale up LTH to larger networks and datasets, the \u201clate rewinding\u201d technique is proposed by Frankle et al. (2019); Renda et al. (2020)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "23fa2f604f73785b638eb49df4c1bbf293e16cd5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-00134",
                    "ArXiv": "2106.00134",
                    "CorpusId": 231800078
                },
                "corpusId": 231800078,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/23fa2f604f73785b638eb49df4c1bbf293e16cd5",
                "title": "GANs Can Play Lottery Tickets Too",
                "abstract": "Deep generative adversarial networks (GANs) have gained growing popularity in numerous scenarios, while usually suffer from high parameter complexities for resource-constrained real-world applications. However, the compression of GANs has less been explored. A few works show that heuristically applying compression techniques normally leads to unsatisfactory results, due to the notorious training instability of GANs. In parallel, the lottery ticket hypothesis shows prevailing success on discriminative models, in locating sparse matching subnetworks capable of training in isolation to full model performance. In this work, we for the first time study the existence of such trainable matching subnetworks in deep GANs. For a range of GANs, we certainly find matching subnetworks at 67%-74% sparsity. We observe that with or without pruning discriminator has a minor effect on the existence and quality of matching subnetworks, while the initialization weights used in the discriminator play a significant role. We then show the powerful transferability of these subnetworks to unseen tasks. Furthermore, extensive experimental results demonstrate that our found subnetworks substantially outperform previous state-of-the-art GAN compression approaches in both image generation (e.g. SNGAN) and image-to-image translation GANs (e.g. CycleGAN). Codes available at https://github.com/VITA-Group/GAN-LTH.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1753619492",
                        "name": "Xuxi Chen"
                    },
                    {
                        "authorId": "2109338656",
                        "name": "Zhenyu (Allen) Zhang"
                    },
                    {
                        "authorId": "2003767516",
                        "name": "Yongduo Sui"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026labelled data, such as LDI (Lee et al., 2020), DPF (Lin et al., 2020b), PFP (Liebenwein et al., 2020), FT (Li et al., 2017), SoftNet (He et al., 2018), Lottery (Frankle & Carbin, 2018), PoReg (Zhuang et al., 2020), PFF (Meng et al., 2020), OS (Renda et al., 2020) and SCOP (Tang et al., 2020).",
                "Most architecture compression methods rely on an underlying approximation of the predictive function to later perform pruning, wether it can is unstructured or structured (as stated in Renda et al. (2020)), data-driven or data-free, magnitude-based or similarity-based."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "75437ae4cf8c8c04d68a9063440f802d211197d9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-14797",
                    "ArXiv": "2105.14797",
                    "CorpusId": 235253836
                },
                "corpusId": 235253836,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/75437ae4cf8c8c04d68a9063440f802d211197d9",
                "title": "RED : Looking for Redundancies for Data-Free Structured Compression of Deep Neural Networks",
                "abstract": "Deep Neural Networks (DNNs) are ubiquitous in today's computer vision land-scape, despite involving considerable computational costs. The mainstream approaches for runtime acceleration consist in pruning connections (unstructured pruning) or, better, filters (structured pruning), both often requiring data to re-train the model. In this paper, we present RED, a data-free structured, unified approach to tackle structured pruning. First, we propose a novel adaptive hashing of the scalar DNN weight distribution densities to increase the number of identical neurons represented by their weight vectors. Second, we prune the network by merging redundant neurons based on their relative similarities, as defined by their distance. Third, we propose a novel uneven depthwise separation technique to further prune convolutional layers. We demonstrate through a large variety of benchmarks that RED largely outperforms other data-free pruning methods, often reaching performance similar to unconstrained, data-driven methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1632928879",
                        "name": "Edouard Yvinec"
                    },
                    {
                        "authorId": "3190846",
                        "name": "Arnaud Dapogny"
                    },
                    {
                        "authorId": "51021910",
                        "name": "M. Cord"
                    },
                    {
                        "authorId": "2521061",
                        "name": "K\u00e9vin Bailly"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Interestingly, neural models are capable of performing tasks on sizes smaller than they were trained for [9, 10]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4862141c0283502fe30d0c3b2f01c87b30fd15dd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-12920",
                    "ArXiv": "2105.12920",
                    "CorpusId": 235212519
                },
                "corpusId": 235212519,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4862141c0283502fe30d0c3b2f01c87b30fd15dd",
                "title": "Search Spaces for Neural Model Training",
                "abstract": "While larger neural models are pushing the boundaries of what deep learning can do, often more weights are needed to train models rather than to run inference for tasks. This paper seeks to understand this behavior using search spaces -- adding weights creates extra degrees of freedom that form new paths for optimization (or wider search spaces) rendering neural model training more effective. We then show how we can augment search spaces to train sparse models attaining competitive scores across dozens of deep learning workloads. They are also are tolerant of structures targeting current hardware, opening avenues for training and inference acceleration. Our work encourages research to explore beyond massive neural models being used today.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "33749574",
                        "name": "Darko Stosic"
                    },
                    {
                        "authorId": "2737605",
                        "name": "D. Stosic"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Most of existing results focus on finding unstructured winning tickets via iterative magnitude pruning and rewinding in randomly initialized networks (Frankle et al., 2019; Renda et al., 2020), where each ticket is a single neuron.",
                "We adopt the weight rewinding technique in Renda et al. (2020): We reset the parameters of the winning tickets to their values in the pre-trained weights, and subsequently fine-tune the subnetwork with the original learning rate schedule."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e638b9e6ee09ab4fa748b748099e0f03d471d803",
                "externalIds": {
                    "ACL": "2021.acl-long.510",
                    "DBLP": "conf/acl/LiangZCJLHZC20",
                    "ArXiv": "2105.12002",
                    "DOI": "10.18653/v1/2021.acl-long.510",
                    "CorpusId": 235186841
                },
                "corpusId": 235186841,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/e638b9e6ee09ab4fa748b748099e0f03d471d803",
                "title": "Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization",
                "abstract": "The Lottery Ticket Hypothesis suggests that an over-parametrized network consists of \u201dlottery tickets\u201d, and training a certain collection of them (i.e., a subnetwork) can match the performance of the full model. In this paper, we study such a collection of tickets, which is referred to as \u201dwinning tickets\u201d, in extremely over-parametrized models, e.g., pre-trained language models. We observe that at certain compression ratios, the generalization performance of the winning tickets can not only match but also exceed that of the full model. In particular, we observe a phase transition phenomenon: As the compression ratio increases, generalization performance of the winning tickets first improves then deteriorates after a certain threshold. We refer to the tickets on the threshold as \u201dsuper tickets\u201d. We further show that the phase transition is task and model dependent \u2014 as the model size becomes larger and the training data set becomes smaller, the transition becomes more pronounced. Our experiments on the GLUE benchmark show that the super tickets improve single task fine-tuning by 0.9 points on BERT-base and 1.0 points on BERT-large, in terms of task-average score. We also demonstrate that adaptively sharing the super tickets across tasks benefits multi-task learning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "98703980",
                        "name": "Chen Liang"
                    },
                    {
                        "authorId": "52194893",
                        "name": "Simiao Zuo"
                    },
                    {
                        "authorId": "2108809403",
                        "name": "Minshuo Chen"
                    },
                    {
                        "authorId": "2152630772",
                        "name": "Haoming Jiang"
                    },
                    {
                        "authorId": "46522098",
                        "name": "Xiaodong Liu"
                    },
                    {
                        "authorId": "50462546",
                        "name": "Pengcheng He"
                    },
                    {
                        "authorId": "36345161",
                        "name": "T. Zhao"
                    },
                    {
                        "authorId": "2109136147",
                        "name": "Weizhu Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2022 With the insights from dynamical isometry and OrthP, we unveil two mysteries in pruning: why a larger finetuning LR can improve the final performance significantly [40, 24] (Sec.",
                "Several recent works [40, 24] observed an interesting phenomenon in neural network pruning: A larger finetuning learning rate can improve the final performance significantly.",
                "Specifically, [40, 24] found that the learning rate (LR) in finetuning holds a critical role in the final performance.",
                "3 answers why a larger finetuning LR can improve the final performance in pruning [40, 24]; Sec.",
                "Concretely, we propose the following plausible explanation to the effect of a larger LR in finetuning [40, 24]: A larger LR helps the network converge faster, thus the dynamical isometry (measured by mean JSV) recovers faster."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ddd0e248b1f32e66c12218427625aa1126943ca9",
                "externalIds": {
                    "ArXiv": "2105.05916",
                    "DBLP": "journals/corr/abs-2105-05916",
                    "CorpusId": 234482474
                },
                "corpusId": 234482474,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ddd0e248b1f32e66c12218427625aa1126943ca9",
                "title": "Dynamical Isometry: The Missing Ingredient for Neural Network Pruning",
                "abstract": "Several recent works [40, 24] observed an interesting phenomenon in neural network pruning: A larger finetuning learning rate can improve the final performance significantly. Unfortunately, the reason behind it remains elusive up to date. This paper is meant to explain it through the lens of dynamical isometry [42]. Specifically, we examine neural network pruning from an unusual perspective: pruning as initialization for finetuning, and ask whether the inherited weights serve as a good initialization for the finetuning? The insights from dynamical isometry suggest a negative answer. Despite its critical role, this issue has not been well-recognized by the community so far. In this paper, we will show the understanding of this problem is very important -- on top of explaining the aforementioned mystery about the larger finetuning rate, it also unveils the mystery about the value of pruning [5, 30]. Besides a clearer theoretical understanding of pruning, resolving the problem can also bring us considerable performance benefits in practice.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2197900626",
                        "name": "Huan Wang"
                    },
                    {
                        "authorId": "12282768",
                        "name": "Can Qin"
                    },
                    {
                        "authorId": "153802755",
                        "name": "Yue Bai"
                    },
                    {
                        "authorId": "46956675",
                        "name": "Y. Fu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "LEARNING RATE REWINDING (LRW) Renda et al. (2020) propose to reuse the learning rate schedule of the original training when retraining pruned networks.",
                "Although in the previous work, Renda et al. (2020) demonstrated the efficacy of learning rate rewinding across datasets and pruning criteria, there is a lack of understanding of the actual reason behind the success of this technique.",
                "As the performance of larger learning rate schedules such as LRW, SLR, and CLR are rather similar, we select CLR for use in this experiment.",
                "To verify this conjecture empirically, we conduct experiments with different learning rate schedules including learning rate rewinding (Renda et al., 2020) while varying pruning algorithms, network architectures and datasets.",
                "To analyze the effect of retraining a pruned network, we based on learning rate rewinding (Renda et al., 2020) and experiment with different retraining settings.",
                "We selected these works because they do not utilize a similar learning rate value as LRW in the original implementation, i.e., the authors applied a smaller value then the heuristic of LRW.",
                "Renda et al. (2020) found that learning rate rewinding usually saturate at half of original training, thus, we perform on retraining for 80 epochs on CIFAR-10 and 45 epochs on ImageNet.",
                "The retraining step is a critical part in implementing network pruning, but it has been largely overlooked and tend to vary in each implementation including differences in learning rate schedules, retraining budget, hyperparameter choices, etc.\nRecently, Renda et al. (2020) proposed a state-of-the-art technique for retraining pruned networks namely learning rate rewinding (LRW).",
                "We first experiment with iterative `1-norm filters pruning on CIFAR-10 and report the results in Figure 4(a,b), we can observe that SLR and CLR also perform comparable or better than LRW in this setting.",
                "\u2026been largely overlooked and tend to vary in each implementation including differences in learning rate schedules, retraining budget, hyperparameter choices, etc.\nRecently, Renda et al. (2020) proposed a state-of-the-art technique for retraining pruned networks namely learning rate rewinding (LRW).",
                "In our experiments, CLR usually reaches slightly higher accuracy than LRW and SLR.",
                "This seemingly subtle change in learning rate schedule led to an important result: LRW was shown to achieve comparable performance to more complex and computationally expensive\nar X\niv :2\n10 5.",
                "Thus, the value of LRW can be a good heuristic to choose the learning rate for retraining after pruning.",
                "In our results, cyclic learning rate restarting (CLR) is slightly more efficient than scaled learning rate restarting (SLR) and learning rate rewinding (LRW)."
            ],
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "dfe2efeea8889a1937be16538220f5e3477d42fb",
                "externalIds": {
                    "DBLP": "conf/iclr/LeH21",
                    "ArXiv": "2105.03193",
                    "CorpusId": 234096198
                },
                "corpusId": 234096198,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/dfe2efeea8889a1937be16538220f5e3477d42fb",
                "title": "Network Pruning That Matters: A Case Study on Retraining Variants",
                "abstract": "Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning. One-sentence Summary: We study the effective of different retraining mechanisms while doing pruning",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2059393417",
                        "name": "Duong H. Le"
                    },
                    {
                        "authorId": "143807806",
                        "name": "Binh-Son Hua"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Based on the Lottery Winning Ticket hypothesis (Frankle and Carbin, 2018), (Renda et al., 2020) suggests a weight-rewinding method to explore sub-networks from full-trained models."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cfcc55816a7b6f952e3010d998240cbff7fdfdb7",
                "externalIds": {
                    "DBLP": "conf/iclr/ParkKOKL22",
                    "ArXiv": "2105.01869",
                    "CorpusId": 246430598
                },
                "corpusId": 246430598,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/cfcc55816a7b6f952e3010d998240cbff7fdfdb7",
                "title": "Encoding Weights of Irregular Sparsity for Fixed-to-Fixed Model Compression",
                "abstract": "Even though fine-grained pruning techniques achieve a high compression ratio, conventional sparsity representations (such as CSR) associated with irregular sparsity degrade parallelism significantly. Practical pruning methods, thus, usually lower pruning rates (by structured pruning) to improve parallelism. In this paper, we study fixed-to-fixed (lossless) encoding architecture/algorithm to support fine-grained pruning methods such that sparse neural networks can be stored in a highly regular structure. We first estimate the maximum compression ratio of encoding-based compression using entropy. Then, as an effort to push the compression ratio to the theoretical maximum (by entropy), we propose a sequential fixed-to-fixed encoding scheme. We demonstrate that our proposed compression scheme achieves almost the maximum compression ratio for the Transformer and ResNet-50 pruned by various fine-grained pruning methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "120751934",
                        "name": "Baeseong Park"
                    },
                    {
                        "authorId": "12693169",
                        "name": "Se Jung Kwon"
                    },
                    {
                        "authorId": "2088384139",
                        "name": "Daehwan Oh"
                    },
                    {
                        "authorId": "46239568",
                        "name": "Byeongwook Kim"
                    },
                    {
                        "authorId": "122808525",
                        "name": "Dongsoo Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Based on the Lottery Winning Ticket hypothesis (Frankle & Carbin, 2018), (Renda et al., 2020) suggests a weight-rewinding method to explore sub-networks from full-trained models."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cb25c4f008a49e3ec546229f691154006b7f4b08",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-01869",
                    "MAG": "3158754073",
                    "CorpusId": 233739749
                },
                "corpusId": 233739749,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cb25c4f008a49e3ec546229f691154006b7f4b08",
                "title": "Sequential Encryption of Sparse Neural Networks Toward Optimum Representation of Irregular Sparsity",
                "abstract": "Even though fine-grained pruning techniques achieve a high compression ratio, conventional sparsity representations (such as CSR) associated with irregular sparsity degrade parallelism significantly. Practical pruning methods, thus, usually lower pruning rates (by structured pruning) to improve parallelism. In this paper, we study fixed-to-fixed (lossless) encryption architecture/algorithm to support fine-grained pruning methods such that sparse neural networks can be stored in a highly regular structure. We first estimate the maximum compression ratio of encryption-based compression using entropy. Then, as an effort to push the compression ratio to the theoretical maximum (by entropy), we propose a sequential fixed-to-fixed encryption scheme. We demonstrate that our proposed compression scheme achieves almost the maximum compression ratio for the Transformer and ResNet-50 pruned by various fine-grained pruning methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "120751934",
                        "name": "Baeseong Park"
                    },
                    {
                        "authorId": "12693169",
                        "name": "Se Jung Kwon"
                    },
                    {
                        "authorId": "122808525",
                        "name": "Dongsoo Lee"
                    },
                    {
                        "authorId": "2088384139",
                        "name": "Daehwan Oh"
                    },
                    {
                        "authorId": "46239568",
                        "name": "Byeongwook Kim"
                    },
                    {
                        "authorId": "2279059",
                        "name": "Yongkweon Jeon"
                    },
                    {
                        "authorId": "2462957",
                        "name": "Yeonju Ro"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Pruning ResNet32 is more challenging since VGG19 has about 10 times parameters than ResNet32.",
                "Foresight Pruning\nLottery Ticket Hypothesis was proposed in [6], which conjectures and verifies that there exists sparse subnetworks which can be trained directly to achieve even better performance than dense counterparts with less training time.",
                "[9, 41, 6, 30] follows the idea of using weight magnitude as the criterion.",
                "Network Pruning [10, 8, 39, 21, 24, 14, 41, 17, 34, 30, 38] has been extensively studied in recent years to reduce the model size and improve the inference efficiency of deep neural networks.",
                "[Gradually Increasing Pruning Rate] We increase the pruning rate gradually to make a smooth transformation from dense to sparse status.",
                "Finally we review another line of research on Lottery Tickets Hypothesis, SuperMask and Foresight Pruning.",
                "Dense-to-sparse training starts with a dense network and obtains a sparse network at the end of the training [10, 41, 27, 6, 30, 36, 32, 23, 35].",
                "The results in the literature [8, 22, 39, 21, 30, 18, 5, 35] demonstrate that pruning methods can significantly improve the inference efficiency of DNNs with minimal performance degradation, making the deployment of modern neural networks on resource-limited devices possible.",
                "[30] achieves strong results but needs multiple rounds of pruning",
                "We choose six representative methods PBW (Pruning by Weight, [10]), MLPrune [39], RIGL [5], STR [18], DNW[25], GMP [41]) as baselines."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a5e56209623c52f3f7ddfaa92f9e44cfc6ffc972",
                "externalIds": {
                    "MAG": "3158839075",
                    "DBLP": "journals/corr/abs-2105-01571",
                    "ArXiv": "2105.01571",
                    "DOI": "10.1109/CVPR46437.2021.00360",
                    "CorpusId": 233714694
                },
                "corpusId": 233714694,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a5e56209623c52f3f7ddfaa92f9e44cfc6ffc972",
                "title": "Effective Sparsification of Neural Networks with Global Sparsity Constraint",
                "abstract": "Weight pruning is an effective technique to reduce the model size and inference time for deep neural networks in real-world deployments. However, since magnitudes and relative importance of weights are very different for different layers of a neural network, existing methods rely on either manual tuning or handcrafted heuristic rules to find appropriate pruning rates individually for each layer. This approach generally leads to suboptimal performance. In this paper, by directly working on the probability space, we propose an effective network sparsification method called probabilistic masking (ProbMask), which solves a natural sparsification formulation under global sparsity constraint. The key idea is to use probability as a global criterion for all layers to measure the weight importance. An appealing feature of ProbMask is that the amounts of weight redundancy can be learned automatically via our constraint and thus we avoid the problem of tuning pruning rates individually for different layers in a network. Extensive experimental results on CIFAR-10/100 and ImageNet demonstrate that our method is highly effective, and can outperform previous state-of-the-art methods by a significant margin, especially in the high pruning rate situation. Notably, the gap of Top-1 accuracy between our ProbMask and existing methods can be up to 10%. As a by-product, we show ProbMask is also highly effective in identifying supermasks, which are sub-networks with high performance in a randomly weighted dense neural network.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109127048",
                        "name": "Xiao Zhou"
                    },
                    {
                        "authorId": "47527753",
                        "name": "Weizhong Zhang"
                    },
                    {
                        "authorId": "47995165",
                        "name": "Hang Xu"
                    },
                    {
                        "authorId": "50728655",
                        "name": "Tong Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                ": Best of (Frankle & Carbin, 2019; Renda et al., 2020; Su et al., 2020), obtained from Wang et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5d866a10a7a9b9f784a618e9b48d03456e6282b0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-01029",
                    "ArXiv": "2105.01029",
                    "CorpusId": 233481638
                },
                "corpusId": 233481638,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5d866a10a7a9b9f784a618e9b48d03456e6282b0",
                "title": "Initialization and Regularization of Factorized Neural Layers",
                "abstract": "Factorized layers--operations parameterized by products of two or more matrices--occur in a variety of deep learning contexts, including compressed model training, certain types of knowledge distillation, and multi-head self-attention architectures. We study how to initialize and regularize deep nets containing such layers, examining two simple, understudied schemes, spectral initialization and Frobenius decay, for improving their performance. The guiding insight is to design optimization routines for these networks that are as close as possible to that of their well-tuned, non-decomposed counterparts; we back this intuition with an analysis of how the initialization and regularization schemes impact training with gradient descent, drawing on modern attempts to understand the interplay of weight-decay and batch-normalization. Empirically, we highlight the benefits of spectral initialization and Frobenius decay across a variety of settings. In model compression, we show that they enable low-rank methods to significantly outperform both unstructured sparsity and tensor methods on the task of training low-memory residual networks; analogs of the schemes also improve the performance of tensor decomposition techniques. For knowledge distillation, Frobenius decay enables a simple, overcomplete baseline that yields a compact model from over-parameterized training without requiring retraining with or pruning a teacher network. Finally, we show how both schemes applied to multi-head attention lead to improved performance on both translation and unsupervised pre-training.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "10398264",
                        "name": "M. Khodak"
                    },
                    {
                        "authorId": "3333746",
                        "name": "Neil A. Tenenholtz"
                    },
                    {
                        "authorId": "143722101",
                        "name": "Lester W. Mackey"
                    },
                    {
                        "authorId": "2723245",
                        "name": "Nicol\u00f3 Fusi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other studies concerned hyperparameters modifications [24, 25], and concentrated on the transferability [16, 26] of the pruned networks."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "eca3791904a4b5d0f74cfe2300f2be0900ebeff6",
                "externalIds": {
                    "ArXiv": "2104.13343",
                    "DBLP": "journals/corr/abs-2104-13343",
                    "CorpusId": 233407723
                },
                "corpusId": 233407723,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/eca3791904a4b5d0f74cfe2300f2be0900ebeff6",
                "title": "Sifting out the features by pruning: Are convolutional networks the winning lottery ticket of fully connected ones?",
                "abstract": "Pruning methods can considerably reduce the size of artificial neural networks without harming their performance. In some cases, they can even uncover sub-networks that, when trained in isolation, match or surpass the test accuracy of their dense counterparts. Here we study the inductive bias that pruning imprints in such\"winning lottery tickets\". Focusing on visual tasks, we analyze the architecture resulting from iterative magnitude pruning of a simple fully connected network (FCN). We show that the surviving node connectivity is local in input space, and organized in patterns reminiscent of the ones found in convolutional networks (CNN). We investigate the role played by data and tasks in shaping the architecture of pruned sub-networks. Our results show that the winning lottery tickets of FCNs display the key features of CNNs. The ability of such automatic network-simplifying procedure to recover the key features\"hand-crafted\"in the design of CNNs suggests interesting applications to other datasets and tasks, in order to discover new and efficient architectural inductive biases.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "39324024",
                        "name": "F. Pellegrini"
                    },
                    {
                        "authorId": "2188423",
                        "name": "G. Biroli"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "22299b440277b4bc887168a669408d5547c1461a",
                "externalIds": {
                    "ArXiv": "2104.11832",
                    "DBLP": "conf/aaai/GanCLC0WLW022",
                    "DOI": "10.1609/aaai.v36i1.19945",
                    "CorpusId": 233394482
                },
                "corpusId": 233394482,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/22299b440277b4bc887168a669408d5547c1461a",
                "title": "Playing Lottery Tickets with Vision and Language",
                "abstract": "Large-scale pre-training has recently revolutionized vision-and-language (VL) research. Models such as LXMERT and UNITER have significantly lifted the state of the art over a wide range of VL tasks. However, the large number of parameters in such models hinders their application in practice. In parallel, work on the lottery ticket hypothesis (LTH) has shown that deep neural networks contain small matching subnetworks that can achieve on par or even better performance than the dense networks when trained in isolation. In this work, we perform the first empirical study to assess whether such trainable subnetworks also exist in pre-trained VL models. We use UNITER as the main testbed (also test on LXMERT and ViLT), and consolidate 7 representative VL tasks for experiments, including visual question answering, visual commonsense reasoning, visual entailment, referring expression comprehension, image-text retrieval, GQA, and NLVR2. Through comprehensive analysis, we summarize our main findings as follows. (i) It is difficult to find subnetworks that strictly match the performance of the full model. However, we can find relaxed winning tickets at 50%-70% sparsity that maintain 99% of the full accuracy. (ii) Subnetworks found by task-specific pruning transfer reasonably well to the other tasks, while those found on the pre-training tasks at 60%/70% sparsity transfer universally, matching 98%/96% of the full accuracy on average over all the tasks. (iii) Besides UNITER, other models such as LXMERT and ViLT can also play lottery tickets. However, the highest sparsity we can achieve for ViLT is far lower than LXMERT and UNITER (30% vs. 70%). (iv) LTH also remains relevant when using other training methods (e.g., adversarial training).",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144702900",
                        "name": "Zhe Gan"
                    },
                    {
                        "authorId": "2118386757",
                        "name": "Yen-Chun Chen"
                    },
                    {
                        "authorId": "2107923860",
                        "name": "Linjie Li"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "153655416",
                        "name": "Yu Cheng"
                    },
                    {
                        "authorId": "2992833",
                        "name": "Shuohang Wang"
                    },
                    {
                        "authorId": "46700348",
                        "name": "Jingjing Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "If, instead, structured pruning [35], [40] is applied to improve regularity, model quality will then drop quickly."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c0573a1d175f0c1ebbef318bba9f2a1b46275d18",
                "externalIds": {
                    "ArXiv": "2104.09056",
                    "DBLP": "conf/isca/Huang21",
                    "DOI": "10.1109/ISCA52012.2021.00089",
                    "CorpusId": 233296556
                },
                "corpusId": 233296556,
                "publicationVenue": {
                    "id": "deedf64a-dd5c-4b33-b345-ff83bfb93d71",
                    "name": "International Symposium on Computer Architecture",
                    "type": "conference",
                    "alternate_names": [
                        "Int Symp Comput Archit",
                        "ISCA"
                    ],
                    "url": "http://www.cs.wisc.edu/~arch/www/"
                },
                "url": "https://www.semanticscholar.org/paper/c0573a1d175f0c1ebbef318bba9f2a1b46275d18",
                "title": "RingCNN: Exploiting Algebraically-Sparse Ring Tensors for Energy-Efficient CNN-Based Computational Imaging",
                "abstract": "In the era of artificial intelligence, convolutional neural networks (CNNs) are emerging as a powerful technique for computational imaging. They have shown superior quality for reconstructing fine textures from badly-distorted images and have potential to bring next-generation cameras and displays to our daily life. However, CNNs demand intensive computing power for generating high-resolution videos and defy conventional sparsity techniques when rendering dense details. Therefore, finding new possibilities in regular sparsity is crucial to enable large-scale deployment of CNN-based computational imaging.In this paper, we consider a fundamental but yet well-explored approach\u2014algebraic sparsity\u2014for energy-efficient CNN acceleration. We propose to build CNN models based on ring algebra that defines multiplication, addition, and non-linearity for n-tuples properly. Then the essential sparsity will immediately follow, e.g. n-times reduction for the number of real-valued weights. We define and unify several variants of ring algebras into a modeling framework, RingCNN, and make comparisons in terms of image quality and hardware complexity. On top of that, we further devise a novel ring algebra which minimizes complexity with component-wise product and achieves the best quality using directional ReLU. Finally, we design an accelerator, eRingCNN, to accommodate to the proposed ring algebra, in particular with regular ring-convolution arrays for efficient inference and on-the-fly directional ReLU blocks for fixed-point computation. We implement two configurations, n = 2 and 4 (50% and 75% sparsity), with 40 nm technology to support advanced denoising and super-resolution at up to 4K UHD 30 fps. Layout results show that they can deliver equivalent 41 TOPS using 3.76 W and 2.22 W, respectively. Compared to the real-valued counterpart, our ring convolution engines for n = 2 achieve 2.00\u00d7 energy efficiency and 2.08\u00d7 area efficiency with similar or even better image quality. With n = 4, the efficiency gains of energy and area are further increased to 3.84\u00d7 and 3.77\u00d7 with only 0.11 dB drop of peak signal-to-noise ratio (PSNR). The results show that RingCNN exhibits great architectural advantages for providing near-maximum hardware efficiencies and graceful quality degradation simultaneously.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3246963",
                        "name": "Chao-Tsung Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "40235eded15f44c8c4a7f48468adcc7df4e171fb",
                "externalIds": {
                    "MAG": "3168125510",
                    "DBLP": "conf/naacl/XuYZX21",
                    "ArXiv": "2104.08682",
                    "ACL": "2021.naacl-main.188",
                    "DOI": "10.18653/V1/2021.NAACL-MAIN.188",
                    "CorpusId": 233297003
                },
                "corpusId": 233297003,
                "publicationVenue": {
                    "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
                    "name": "North American Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "North Am Chapter Assoc Comput Linguistics",
                        "NAACL"
                    ],
                    "url": "https://www.aclweb.org/portal/naacl"
                },
                "url": "https://www.semanticscholar.org/paper/40235eded15f44c8c4a7f48468adcc7df4e171fb",
                "title": "Rethinking Network Pruning \u2013 under the Pre-train and Fine-tune Paradigm",
                "abstract": "Transformer-based pre-trained language models have significantly improved the performance of various natural language processing (NLP) tasks in the recent years. While effective and prevalent, these models are usually prohibitively large for resource-limited deployment scenarios. A thread of research has thus been working on applying network pruning techniques under the pretrain-then-finetune paradigm widely adopted in NLP. However, the existing pruning results on benchmark transformers, such as BERT, are not as remarkable as the pruning results in the literature of convolutional neural networks (CNNs). In particular, common wisdom in pruning CNN states that sparse pruning technique compresses a model more than that obtained by reducing number of channels and layers, while existing works on sparse pruning of BERT yields inferior results than its small-dense counterparts such as TinyBERT. In this work, we aim to fill this gap by studying how knowledge are transferred and lost during the pre-train, fine-tune, and pruning process, and proposing a knowledge-aware sparse pruning process that achieves significantly superior results than existing literature. We show for the first time that sparse pruning compresses a BERT model significantly more than reducing its number of channels and layers. Experiments on multiple data sets of GLUE benchmark show that our method outperforms the leading competitors with a 20-times weight/FLOPs compression and neglectable loss in prediction accuracy.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116459424",
                        "name": "Dongkuan Xu"
                    },
                    {
                        "authorId": "2527556",
                        "name": "I. E. Yen"
                    },
                    {
                        "authorId": "1690543",
                        "name": "Jinxi Zhao"
                    },
                    {
                        "authorId": "2405075",
                        "name": "Zhibin Xiao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Lottery Ticket Hypothesis (LTH) [13] suggests the existence of sparse subnetworks in overparameterized neural networks at their random initialization, early training stage, or pre-trained initialization [35, 44, 5, 3, 2].",
                "That implies ETTs might be able to transfer pruned solutions in general [35] \u2013 this is out of the current work\u2019s focus, but would definitely be our future work."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5c0705d856eb18666db4318cf76416560764a856",
                "externalIds": {
                    "DBLP": "conf/nips/ChenCWGLW21",
                    "ArXiv": "2103.16547",
                    "CorpusId": 232417266
                },
                "corpusId": 232417266,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5c0705d856eb18666db4318cf76416560764a856",
                "title": "The Elastic Lottery Ticket Hypothesis",
                "abstract": "Lottery Ticket Hypothesis (LTH) raises keen attention to identifying sparse trainable subnetworks, or winning tickets, which can be trained in isolation to achieve similar or even better performance compared to the full models. Despite many efforts being made, the most effective method to identify such winning tickets is still Iterative Magnitude-based Pruning (IMP), which is computationally expensive and has to be run thoroughly for every different network. A natural question that comes in is: can we\"transform\"the winning ticket found in one network to another with a different architecture, yielding a winning ticket for the latter at the beginning, without re-doing the expensive IMP? Answering this question is not only practically relevant for efficient\"once-for-all\"winning ticket finding, but also theoretically appealing for uncovering inherently scalable sparse patterns in networks. We conduct extensive experiments on CIFAR-10 and ImageNet, and propose a variety of strategies to tweak the winning tickets found from different networks of the same model family (e.g., ResNets). Based on these results, we articulate the Elastic Lottery Ticket Hypothesis (E-LTH): by mindfully replicating (or dropping) and re-ordering layers for one network, its corresponding winning ticket could be stretched (or squeezed) into a subnetwork for another deeper (or shallower) network from the same family, whose performance is nearly the same competitive as the latter's winning ticket directly found by IMP. We have also extensively compared E-LTH with pruning-at-initialization and dynamic sparse training methods, as well as discussed the generalizability of E-LTH to different model families, layer types, and across datasets. Code is available at https://github.com/VITA-Group/ElasticLTH.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "66273769",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "153655416",
                        "name": "Yu Cheng"
                    },
                    {
                        "authorId": "2992833",
                        "name": "Shuohang Wang"
                    },
                    {
                        "authorId": "144702900",
                        "name": "Zhe Gan"
                    },
                    {
                        "authorId": "46700348",
                        "name": "Jingjing Liu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", [7] reported iterative GMP to outperform the single-shot variant, [33] reported occasional near-catastrophic performance hits while pruning, [22] reported weight and learning-rate rewinding methods to outperform fine-tuning and [1, 12] showed that the keep-ratio time-traces of their schemes resembled an exponential-decay, despite their methods not imposing any explicit pruning schedule.",
                "The cyclic learning-rate schedule is similar to the learning-rate rewinding scheme of [22], albeit with a much shorter cycle length.",
                "Another application of GMP was by [22] where weight and learningrate rewinding schemes were used to achieve competitive pruning performances.",
                "\u2022 We shed light on weight and learning-rate rewinding methods of re-training [22].",
                "For example, [22] reports that fine-tuning a ResNet50 with a momentum of 0."
            ],
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c0a4af29499db95bbc492b0de8eb97a58e186f7a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-10629",
                    "ArXiv": "2103.10629",
                    "CorpusId": 232290568
                },
                "corpusId": 232290568,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c0a4af29499db95bbc492b0de8eb97a58e186f7a",
                "title": "Cascade Weight Shedding in Deep Neural Networks: Benefits and Pitfalls for Network Pruning",
                "abstract": "We report, for the first time, on the cascade weight shedding phenomenon in deep neural networks where in response to pruning a small percentage of a network's weights, a large percentage of the remaining is shed over a few epochs during the ensuing fine-tuning phase. We show that cascade weight shedding, when present, can significantly improve the performance of an otherwise sub-optimal scheme such as random pruning. This explains why some pruning methods may perform well under certain circumstances, but poorly under others, e.g., ResNet50 vs. MobileNetV3. We provide insight into why the global magnitude-based pruning, i.e., GMP, despite its simplicity, provides a competitive performance for a wide range of scenarios. We also demonstrate cascade weight shedding's potential for improving GMP's accuracy, and reduce its computational complexity. In doing so, we highlight the importance of pruning and learning-rate schedules. We shed light on weight and learning-rate rewinding methods of re-training, showing their possible connections to the cascade weight shedding and reason for their advantage over fine-tuning. We also investigate cascade weight shedding's effect on the set of kept weights, and its implications for semi-structured pruning. Finally, we give directions for future research.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2075053",
                        "name": "K. Azarian"
                    },
                    {
                        "authorId": "29905643",
                        "name": "F. Porikli"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026parameters for removal; we focus on randomly selected parameters (random pruning) and removing the smallestmagnitude parameters (magnitude pruning), though other approaches exist in the literature (Han et al., 2015; Renda et al., 2020; Lee et al., 2019; Wang et al., 2020; Blalock et al., 2020).",
                "There are many approaches to selecting parameters for removal; we focus on randomly selected parameters (random pruning) and removing the smallestmagnitude parameters (magnitude pruning), though other approaches exist in the literature (Han et al., 2015; Renda et al., 2020; Lee et al., 2019; Wang et al., 2020; Blalock et al., 2020)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "752f3187cb6d60bf60dff6715fd7f070bd03aea3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-06002",
                    "ArXiv": "2103.06002",
                    "CorpusId": 232170627
                },
                "corpusId": 232170627,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/752f3187cb6d60bf60dff6715fd7f070bd03aea3",
                "title": "Robustness to Pruning Predicts Generalization in Deep Neural Networks",
                "abstract": "Existing generalization measures that aim to capture a model's simplicity based on parameter counts or norms fail to explain generalization in overparameterized deep neural networks. In this paper, we introduce a new, theoretically motivated measure of a network's simplicity which we call prunability: the smallest \\emph{fraction} of the network's parameters that can be kept while pruning without adversely affecting its training loss. We show that this measure is highly predictive of a model's generalization performance across a large set of convolutional networks trained on CIFAR-10, does not grow with network size unlike existing pruning-based measures, and exhibits high correlation with test set loss even in a particularly challenging double descent setting. Lastly, we show that the success of prunability cannot be explained by its relation to known complexity measures based on models' margin, flatness of minima and optimization speed, finding that our new measure is similar to -- but more predictive than -- existing flatness-based measures, and that its predictions exhibit low mutual information with those of other baselines.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "39879848",
                        "name": "Lorenz Kuhn"
                    },
                    {
                        "authorId": "39439114",
                        "name": "Clare Lyle"
                    },
                    {
                        "authorId": "19177000",
                        "name": "Aidan N. Gomez"
                    },
                    {
                        "authorId": "35309584",
                        "name": "Jonas Rothfuss"
                    },
                    {
                        "authorId": "2681954",
                        "name": "Y. Gal"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0204707f16dabd53d43a3a8874d64914152fd864",
                "externalIds": {
                    "ArXiv": "2103.05579",
                    "DBLP": "journals/corr/abs-2103-05579",
                    "CorpusId": 232168344
                },
                "corpusId": 232168344,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0204707f16dabd53d43a3a8874d64914152fd864",
                "title": "hls4ml: An Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices",
                "abstract": "Accessible machine learning algorithms, software, and diagnostic tools for energy-efficient devices and systems are extremely valuable across a broad range of application domains. In scientific domains, real-time near-sensor processing can drastically improve experimental design and accelerate scientific discoveries. To support domain scientists, we have developed hls4ml, an open-source software-hardware codesign workflow to interpret and translate machine learning algorithms for implementation with both FPGA and ASIC technologies. We expand on previous hls4ml work by extending capabilities and techniques towards low-power implementations and increased usability: new Python APIs, quantization-aware pruning, end-to-end FPGA workflows, long pipeline kernels for low power, and new device backends include an ASIC workflow. Taken together, these and continued efforts in hls4ml will arm a new generation of domain scientists with accessible, efficient, and powerful tools for machine-learning-accelerated discovery.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145334779",
                        "name": "F. Fahim"
                    },
                    {
                        "authorId": "50140234",
                        "name": "B. Hawks"
                    },
                    {
                        "authorId": "150057758",
                        "name": "C. Herwig"
                    },
                    {
                        "authorId": "9652891",
                        "name": "J. Hirschauer"
                    },
                    {
                        "authorId": "91226838",
                        "name": "S. Jindariani"
                    },
                    {
                        "authorId": "151698976",
                        "name": "N. Tran"
                    },
                    {
                        "authorId": "1715891",
                        "name": "L. Carloni"
                    },
                    {
                        "authorId": "20609731",
                        "name": "G. D. Guglielmo"
                    },
                    {
                        "authorId": "144326960",
                        "name": "P. Harris"
                    },
                    {
                        "authorId": "1750878974",
                        "name": "J. Krupa"
                    },
                    {
                        "authorId": "145881831",
                        "name": "D. Rankin"
                    },
                    {
                        "authorId": "3438028",
                        "name": "M. B. Valent\u00edn"
                    },
                    {
                        "authorId": "15918293",
                        "name": "Josiah D. Hester"
                    },
                    {
                        "authorId": "2118264744",
                        "name": "Yingyi Luo"
                    },
                    {
                        "authorId": "35988543",
                        "name": "John Mamish"
                    },
                    {
                        "authorId": "2052300996",
                        "name": "Seda Orgrenci-Memik"
                    },
                    {
                        "authorId": "1389781932",
                        "name": "T. Aarrestad"
                    },
                    {
                        "authorId": "2140340008",
                        "name": "Hamza Javed"
                    },
                    {
                        "authorId": "19203702",
                        "name": "V. Loncar"
                    },
                    {
                        "authorId": "1858059",
                        "name": "M. Pierini"
                    },
                    {
                        "authorId": "51231557",
                        "name": "A. A. Pol"
                    },
                    {
                        "authorId": "49143070",
                        "name": "S. Summers"
                    },
                    {
                        "authorId": "1389378372",
                        "name": "Javier Mauricio Duarte"
                    },
                    {
                        "authorId": "1694228",
                        "name": "S. Hauck"
                    },
                    {
                        "authorId": "2072776572",
                        "name": "Shih-Chieh Hsu"
                    },
                    {
                        "authorId": "41016473",
                        "name": "J. Ngadiuba"
                    },
                    {
                        "authorId": "47842372",
                        "name": "Miaoyuan Liu"
                    },
                    {
                        "authorId": "143980870",
                        "name": "Duc Hoang"
                    },
                    {
                        "authorId": "49946671",
                        "name": "E. Kreinar"
                    },
                    {
                        "authorId": "6293775",
                        "name": "Zhenbin Wu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6e466c1bca96e2847be32f60677517aae628a664",
                "externalIds": {
                    "DBLP": "conf/nips/ChenCGLW21",
                    "ArXiv": "2103.00397",
                    "CorpusId": 235262814
                },
                "corpusId": 235262814,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6e466c1bca96e2847be32f60677517aae628a664",
                "title": "Data-Efficient GAN Training Beyond (Just) Augmentations: A Lottery Ticket Perspective",
                "abstract": "Training generative adversarial networks (GANs) with limited real image data generally results in deteriorated performance and collapsed models. To conquer this challenge, we are inspired by the latest observation, that one can discover independently trainable and highly sparse subnetworks (a.k.a., lottery tickets) from GANs. Treating this as an inductive prior, we suggest a brand-new angle towards data-efficient GAN training: by first identifying the lottery ticket from the original GAN using the small training set of real images; and then focusing on training that sparse subnetwork by re-using the same set. We find our coordinated framework to offer orthogonal gains to existing real image data augmentation methods, and we additionally present a new feature-level augmentation that can be applied together with them. Comprehensive experiments endorse the effectiveness of our proposed framework, across various GAN architectures (SNGAN, BigGAN, and StyleGAN-V2) and diverse datasets (CIFAR-10, CIFAR-100, Tiny-ImageNet, ImageNet, and multiple few-shot generation datasets). Codes are available at: https://github.com/VITA-Group/Ultra-Data-Efficient-GAN-Training.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "153655416",
                        "name": "Yu Cheng"
                    },
                    {
                        "authorId": "144702900",
                        "name": "Zhe Gan"
                    },
                    {
                        "authorId": "46700348",
                        "name": "Jingjing Liu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[5] proposes learning rate rewinding in addition to weight rewinding to more efficiently find the winning lottery tickets.",
                "Here we focus specifically on parameter pruning: the selective removal of weights based on a particular ranking [4, 5, 7, 45, 57, 58]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f6a7982725f93b1af9519911f94ce6cc25774b2b",
                "externalIds": {
                    "PubMedCentral": "8299073",
                    "DBLP": "journals/corr/abs-2102-11289",
                    "ArXiv": "2102.11289",
                    "DOI": "10.3389/frai.2021.676564",
                    "CorpusId": 232013782,
                    "PubMed": "34308339"
                },
                "corpusId": 232013782,
                "publicationVenue": {
                    "id": "6a8c0041-d0b7-4e32-b52c-33adef005c7e",
                    "name": "Frontiers in Artificial Intelligence",
                    "alternate_names": [
                        "Front Artif Intell"
                    ],
                    "issn": "2624-8212",
                    "url": "https://www.frontiersin.org/journals/artificial-intelligence#"
                },
                "url": "https://www.semanticscholar.org/paper/f6a7982725f93b1af9519911f94ce6cc25774b2b",
                "title": "Ps and Qs: Quantization-Aware Pruning for Efficient Low Latency Neural Network Inference",
                "abstract": "Efficient machine learning implementations optimized for inference in hardware have wide-ranging benefits, depending on the application, from lower inference latency to higher data throughput and reduced energy consumption. Two popular techniques for reducing computation in neural networks are pruning, removing insignificant synapses, and quantization, reducing the precision of the calculations. In this work, we explore the interplay between pruning and quantization during the training of neural networks for ultra low latency applications targeting high energy physics use cases. Techniques developed for this study have potential applications across many other domains. We study various configurations of pruning during quantization-aware training, which we term quantization-aware pruning, and the effect of techniques like regularization, batch normalization, and different pruning schemes on performance, computational complexity, and information content metrics. We find that quantization-aware pruning yields more computationally efficient models than either pruning or quantization alone for our task. Further, quantization-aware pruning typically performs similar to or better in terms of computational efficiency compared to other neural architecture search techniques like Bayesian optimization. Surprisingly, while networks with different training configurations can have similar performance for the benchmark application, the information content in the network can vary significantly, affecting its generalizability.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50140234",
                        "name": "B. Hawks"
                    },
                    {
                        "authorId": "1389378372",
                        "name": "Javier Mauricio Duarte"
                    },
                    {
                        "authorId": "1809409",
                        "name": "Nicholas J. Fraser"
                    },
                    {
                        "authorId": "2077258779",
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "authorId": "151698976",
                        "name": "N. Tran"
                    },
                    {
                        "authorId": "2067445",
                        "name": "Yaman Umuroglu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1596a43910858bf793860f79811b0ee3f239161b",
                "externalIds": {
                    "DBLP": "journals/ivc/JiaWGLQQ21",
                    "MAG": "3129584051",
                    "DOI": "10.1016/J.IMAVIS.2021.104143",
                    "CorpusId": 233955775
                },
                "corpusId": 233955775,
                "publicationVenue": {
                    "id": "6cc36eeb-d056-42c4-a306-7bcb239cc442",
                    "name": "Image and Vision Computing",
                    "type": "journal",
                    "alternate_names": [
                        "Image Vis Comput"
                    ],
                    "issn": "0262-8856",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525443/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/02628856",
                        "https://www.journals.elsevier.com/image-and-vision-computing"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1596a43910858bf793860f79811b0ee3f239161b",
                "title": "WRGPruner: A new model pruning solution for tiny salient object detection",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "24736862",
                        "name": "Fengwei Jia"
                    },
                    {
                        "authorId": "2108318697",
                        "name": "Xuan Wang"
                    },
                    {
                        "authorId": "2054060172",
                        "name": "Jian Guan"
                    },
                    {
                        "authorId": "1500379589",
                        "name": "Huale Li"
                    },
                    {
                        "authorId": "2089090874",
                        "name": "Chen Qiu"
                    },
                    {
                        "authorId": "1679286",
                        "name": "Shuhan Qi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many works have been proposed to investigate the behaviors on weight pruning (Tanaka et al., 2020; Ye et al., 2020; Renda et al., 2020; Malach et al., 2020).",
                "The following work (Frankle et al., 2019; Renda et al., 2020) also confirms that, for deeper networks and using relatively large learning rates, the winning property can hardly be observed."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6c48ed6999b3415e2a92386658bf2f74ddabc458",
                "externalIds": {
                    "ArXiv": "2102.11068",
                    "DBLP": "conf/icml/0007YCSMJRT0W21",
                    "CorpusId": 235826360
                },
                "corpusId": 235826360,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6c48ed6999b3415e2a92386658bf2f74ddabc458",
                "title": "Lottery Ticket Preserves Weight Correlation: Is It Desirable or Not?",
                "abstract": "In deep model compression, the recent finding\"Lottery Ticket Hypothesis\"(LTH) (Frankle&Carbin, 2018) pointed out that there could exist a winning ticket (i.e., a properly pruned sub-network together with original weight initialization) that can achieve competitive performance than the original dense network. However, it is not easy to observe such winning property in many scenarios, where for example, a relatively large learning rate is used even if it benefits training the original dense model. In this work, we investigate the underlying condition and rationale behind the winning property, and find that the underlying reason is largely attributed to the correlation between initialized weights and final-trained weights when the learning rate is not sufficiently large. Thus, the existence of winning property is correlated with an insufficient DNN pretraining, and is unlikely to occur for a well-trained DNN. To overcome this limitation, we propose the\"pruning&fine-tuning\"method that consistently outperforms lottery ticket sparse training under the same pruning algorithm and the same total training epochs. Extensive experiments over multiple deep models (VGG, ResNet, MobileNet-v2) on different datasets have been conducted to justify our proposals.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2152354569",
                        "name": "Ning Liu"
                    },
                    {
                        "authorId": "9347641",
                        "name": "Geng Yuan"
                    },
                    {
                        "authorId": "1939695",
                        "name": "Zhengping Che"
                    },
                    {
                        "authorId": "2007668856",
                        "name": "Xuan Shen"
                    },
                    {
                        "authorId": "151480882",
                        "name": "Xiaolong Ma"
                    },
                    {
                        "authorId": "153792333",
                        "name": "Qing Jin"
                    },
                    {
                        "authorId": "2111473627",
                        "name": "Jian Ren"
                    },
                    {
                        "authorId": "2115854503",
                        "name": "Jian Tang"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "46393431",
                        "name": "Yanzhi Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We perform preliminary experiments to investigate the potential for incorporating compression techniques such as pruning [22, 41, 52] as part of our tuning framework.",
                "We create pruned models (using the approach in [52]) for MobileNet, VGG16, and ResNet18 on CIFAR-10."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1e13a5283b6f9db9d85e57cd141f09cfdc02361a",
                "externalIds": {
                    "DBLP": "conf/ppopp/SharifZKKSWSZJA21",
                    "DOI": "10.1145/3437801.3446108",
                    "CorpusId": 231591333
                },
                "corpusId": 231591333,
                "publicationVenue": {
                    "id": "11e94c04-6b12-41c9-963d-1a08cdbc306d",
                    "name": "ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming",
                    "type": "conference",
                    "alternate_names": [
                        "PPoPP",
                        "ACM SIGPLAN Symp Princ Pract Parallel Program",
                        "ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming",
                        "ACM SIGPLAN Symp Princ  Pract Parallel Program"
                    ],
                    "url": "http://www.acm.org/sigplan/ppopp.htm"
                },
                "url": "https://www.semanticscholar.org/paper/1e13a5283b6f9db9d85e57cd141f09cfdc02361a",
                "title": "ApproxTuner: a compiler and runtime system for adaptive approximations",
                "abstract": "Manually optimizing the tradeoffs between accuracy, performance and energy for resource-intensive applications with flexible accuracy or precision requirements is extremely difficult. We present ApproxTuner, an automatic framework for accuracy-aware optimization of tensor-based applications while requiring only high-level end-to-end quality specifications. ApproxTuner implements and manages approximations in algorithms, system software, and hardware. The key contribution in ApproxTuner is a novel three-phase approach to approximation-tuning that consists of development-time, install-time, and run-time phases. Our approach decouples tuning of hardware-independent and hardware-specific approximations, thus providing retargetability across devices. To enable efficient autotuning of approximation choices, we present a novel accuracy-aware tuning technique called predictive approximation-tuning, which significantly speeds up autotuning by analytically predicting the accuracy impacts of approximations. We evaluate ApproxTuner across 10 convolutional neural networks (CNNs) and a combined CNN and image processing benchmark. For the evaluated CNNs, using only hardware-independent approximation choices we achieve a mean speedup of 2.1x (max 2.7x) on a GPU, and 1.3x mean speedup (max 1.9x) on the CPU, while staying within 1 percentage point of inference accuracy loss. For two different accuracy-prediction models, ApproxTuner speeds up tuning by 12.8x and 20.4x compared to conventional empirical tuning while achieving comparable benefits.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "36829056",
                        "name": "Hashim Sharif"
                    },
                    {
                        "authorId": "2124210780",
                        "name": "Yifan Zhao"
                    },
                    {
                        "authorId": "1986260",
                        "name": "Maria Kotsifakou"
                    },
                    {
                        "authorId": "2054741175",
                        "name": "Akash Kothari"
                    },
                    {
                        "authorId": "2051156528",
                        "name": "Ben Schreiber"
                    },
                    {
                        "authorId": "2113762625",
                        "name": "Elizabeth Wang"
                    },
                    {
                        "authorId": "1381404241",
                        "name": "Yasmin Sarita"
                    },
                    {
                        "authorId": "2051156747",
                        "name": "Nathan Zhao"
                    },
                    {
                        "authorId": "2053408577",
                        "name": "Keyur Joshi"
                    },
                    {
                        "authorId": "1720525",
                        "name": "Vikram S. Adve"
                    },
                    {
                        "authorId": "1704478",
                        "name": "Sasa Misailovic"
                    },
                    {
                        "authorId": "3196444",
                        "name": "S. Adve"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026on pruning neural networks at initialization instead of after training (Lee et al., 2019; 2020; Wang et al., 2020; Tanaka et al., 2020; Frankle et al., 2021) as well as on what parameters to use when these networks are retrained (Frankle & Carbin, 2019; Liu et al., 2019b; Renda et al., 2020).",
                ", 2021) as well as on what parameters to use when these networks are retrained (Frankle & Carbin, 2019; Liu et al., 2019b; Renda et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "161bf0c900552c96b75f3435ac7f11932eb568ba",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-07804",
                    "ArXiv": "2102.07804",
                    "CorpusId": 231933741
                },
                "corpusId": 231933741,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/161bf0c900552c96b75f3435ac7f11932eb568ba",
                "title": "Scaling Up Exact Neural Network Compression by ReLU Stability",
                "abstract": "We can compress a rectifier network while exactly preserving its underlying functionality with respect to a given input domain if some of its neurons are stable. However, current approaches to determine the stability of neurons with Rectified Linear Unit (ReLU) activations require solving or finding a good approximation to multiple discrete optimization problems. In this work, we introduce an algorithm based on solving a single optimization problem to identify all stable neurons. Our approach is on median 183 times faster than the state-of-art method on CIFAR-10, which allows us to explore exact compression on deeper (5 x 100) and wider (2 x 800) networks within minutes. For classifiers trained under an amount of L1 regularization that does not worsen accuracy, we can remove up to 56% of the connections on the CIFAR-10 dataset. The code is available at the following link, https://github.com/yuxwind/ExactCompression.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "13394773",
                        "name": "Thiago Serra"
                    },
                    {
                        "authorId": "2109224582",
                        "name": "Abhinav Kumar"
                    },
                    {
                        "authorId": "145686644",
                        "name": "S. Ramalingam"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Frankle et al. (2019); Renda et al. (2020) introduced the \u201clate rewinding\u201d techniques to scale up LTH."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "01217fd88d07b05affa75213672d3d31dbcb6617",
                "externalIds": {
                    "ArXiv": "2102.06790",
                    "DBLP": "journals/corr/abs-2102-06790",
                    "CorpusId": 231925028
                },
                "corpusId": 231925028,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/01217fd88d07b05affa75213672d3d31dbcb6617",
                "title": "A Unified Lottery Ticket Hypothesis for Graph Neural Networks",
                "abstract": "With graphs rapidly growing in size and deeper graph neural networks (GNNs) emerging, the training and inference of GNNs become increasingly expensive. Existing network weight pruning algorithms cannot address the main space and computational bottleneck in GNNs, caused by the size and connectivity of the graph. To this end, this paper first presents a unified GNN sparsification (UGS) framework that simultaneously prunes the graph adjacency matrix and the model weights, for effectively accelerating GNN inference on large-scale graphs. Leveraging this new tool, we further generalize the recently popular lottery ticket hypothesis to GNNs for the first time, by defining a graph lottery ticket (GLT) as a pair of core sub-dataset and sparse sub-network, which can be jointly identified from the original GNN and the full dense graph by iteratively applying UGS. Like its counterpart in convolutional neural networks, GLT can be trained in isolation to match the performance of training with the full model and graph, and can be drawn from both randomly initialized and self-supervised pre-trained GNNs. Our proposal has been experimentally verified across various GNN architectures and diverse tasks, on both small-scale graph datasets (Cora, Citeseer and PubMed), and large-scale datasets from the challenging Open Graph Benchmark (OGB). Specifically, for node classification, our found GLTs achieve the same accuracies with 20%~98% MACs saving on small graphs and 25%~85% MACs saving on large ones. For link prediction, GLTs lead to 48%~97% and 70% MACs saving on small and large graph datasets, respectively, without compromising predictive performance. Codes available at https://github.com/VITA-Group/Unified-LTH-GNN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2003767516",
                        "name": "Yongduo Sui"
                    },
                    {
                        "authorId": "1753619492",
                        "name": "Xuxi Chen"
                    },
                    {
                        "authorId": "2085709",
                        "name": "Aston Zhang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026ResNet-50 network generated by unstructured pruning can achieve a 5.96\u00d7 compression ratio, with the same accuracy as the original network, but it can only achieve 1\u00d7 com-\n\u2217The first two authors equally contribute to this paper.\npression in the case of structured sparsity (Renda et al., 2020).",
                "pression in the case of structured sparsity (Renda et al., 2020).",
                "Specifically, we employ the magnitude-based pruning method (Renda et al., 2020; Gale et al., 2019) during the forward process."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5e38dc1ccf33ac1df09b8eb6476f110cb3d1966f",
                "externalIds": {
                    "MAG": "3127067080",
                    "DBLP": "journals/corr/abs-2102-04010",
                    "ArXiv": "2102.04010",
                    "CorpusId": 231847094
                },
                "corpusId": 231847094,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5e38dc1ccf33ac1df09b8eb6476f110cb3d1966f",
                "title": "Learning N: M Fine-grained Structured Sparse Neural Networks From Scratch",
                "abstract": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and decent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing comprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9548994",
                        "name": "Aojun Zhou"
                    },
                    {
                        "authorId": "2109238929",
                        "name": "Yukun Ma"
                    },
                    {
                        "authorId": "24925751",
                        "name": "Junnan Zhu"
                    },
                    {
                        "authorId": "2124809722",
                        "name": "Jianbo Liu"
                    },
                    {
                        "authorId": "1490508571",
                        "name": "Zhijie Zhang"
                    },
                    {
                        "authorId": "50492964",
                        "name": "Kun Yuan"
                    },
                    {
                        "authorId": "8397576",
                        "name": "Wenxiu Sun"
                    },
                    {
                        "authorId": "47893312",
                        "name": "Hongsheng Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Therefore, we tested the elementwise pruning on the relatively high compression rates (\u00d72, \u00d73, \u00d74) compared to the filter pruning [39].",
                "However, the sparsity that the filter pruning can achieve is often lower than that of element-wise pruning [39]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "16ba95b567b73615610b490766d3c96491626348",
                "externalIds": {
                    "MAG": "3121156375",
                    "DOI": "10.3390/APP11031093",
                    "CorpusId": 234032397
                },
                "corpusId": 234032397,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/16ba95b567b73615610b490766d3c96491626348",
                "title": "Robust CNN Compression Framework for Security-Sensitive Embedded Systems",
                "abstract": "Convolutional neural networks (CNNs) have achieved tremendous success in solving complex classification problems. Motivated by this success, there have been proposed various compression methods for downsizing the CNNs to deploy them on resource-constrained embedded systems. However, a new type of vulnerability of compressed CNNs known as the adversarial examples has been discovered recently, which is critical for security-sensitive systems because the adversarial examples can cause malfunction of CNNs and can be crafted easily in many cases. In this paper, we proposed a compression framework to produce compressed CNNs robust against such adversarial examples. To achieve the goal, our framework uses both pruning and knowledge distillation with adversarial training. We formulate our framework as an optimization problem and provide a solution algorithm based on the proximal gradient method, which is more memory-efficient than the popular ADMM-based compression approaches. In experiments, we show that our framework can improve the trade-off between adversarial robustness and compression rate compared to the existing state-of-the-art adversarial pruning approach.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118475889",
                        "name": "Jeonghyun Lee"
                    },
                    {
                        "authorId": "39525147",
                        "name": "Sangkyun Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In most (if not all) LTH literature (Frankle & Carbin, 2019; Frankle et al., 2019; Renda et al., 2020), the re-training step takes care of the masked subnetwork, which is re-trained with the same initialization (or rewinding) and same training recipe as its dense network."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "34c879388bbe213a79a2d60d672af158952691ae",
                "externalIds": {
                    "ArXiv": "2101.03255",
                    "CorpusId": 238583154
                },
                "corpusId": 238583154,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/34c879388bbe213a79a2d60d672af158952691ae",
                "title": "Spending Your Winning Lottery Better After Drawing It",
                "abstract": "Lottery Ticket Hypothesis (LTH) suggests that a dense neural network contains a sparse sub-network that can match the performance of the original dense network when trained in isolation from scratch. Most works retrain the sparse sub-network with the same training protocols as its dense network, such as initialization, architecture blocks, and training recipes. However, till now it is unclear that whether these training protocols are optimal for sparse networks. In this paper, we demonstrate that it is unnecessary for spare retraining to strictly inherit those properties from the dense network. Instead, by plugging in purposeful\"tweaks\"of the sparse subnetwork architecture or its training recipe, its retraining can be significantly improved than the default, especially at high sparsity levels. Combining all our proposed\"tweaks\"can yield the new state-of-the-art performance of LTH, and these modifications can be easily adapted to other sparse training algorithms in general. Specifically, we have achieved a significant and consistent performance gain of1.05% - 4.93% for ResNet18 on CIFAR-100 over vanilla-LTH. Moreover, our methods are shown to generalize across datasets (CIFAR10, CIFAR100, TinyImageNet) and architectures (Vgg16, ResNet-18/ResNet-34, MobileNet). All codes will be publicly available.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145018564",
                        "name": "Ajay Jaiswal"
                    },
                    {
                        "authorId": null,
                        "name": "Haoyu Ma"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "119663804",
                        "name": "Ying Ding"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For NLP models, previous work has also found that matching subnetworks exist in transformers and LSTMs (Yu et al., 2019; Renda et al., 2020)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0c9d97d2ba489256d4f1760598dc2c7be6d90d96",
                "externalIds": {
                    "ACL": "2021.acl-long.171",
                    "DBLP": "journals/corr/abs-2101-00063",
                    "ArXiv": "2101.00063",
                    "DOI": "10.18653/v1/2021.acl-long.171",
                    "CorpusId": 230438816
                },
                "corpusId": 230438816,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/0c9d97d2ba489256d4f1760598dc2c7be6d90d96",
                "title": "EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets",
                "abstract": "Heavily overparameterized language models such as BERT, XLNet and T5 have achieved impressive success in many NLP tasks. However, their high model complexity requires enormous computation resources and extremely long training time for both pre-training and fine-tuning. Many works have studied model compression on large NLP models, but only focusing on reducing inference time while still requiring an expensive training process. Other works use extremely large batch sizes to shorten the pre-training time, at the expense of higher computational resource demands. In this paper, inspired by the Early-Bird Lottery Tickets recently studied for computer vision tasks, we propose EarlyBERT, a general computationally-efficient training algorithm applicable to both pre-training and fine-tuning of large-scale language models. By slimming the self-attention and fully-connected sub-layers inside a transformer, we are the first to identify structured winning tickets in the early stage of BERT training. We apply those tickets towards efficient BERT training, and conduct comprehensive pre-training and fine-tuning experiments on GLUE and SQuAD downstream tasks. Our results show that EarlyBERT achieves comparable performance to standard BERT, with 35 45% less training time. Code is available at https://github.com/VITA-Group/EarlyBERT.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "66273769",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "153655416",
                        "name": "Yu Cheng"
                    },
                    {
                        "authorId": "2992833",
                        "name": "Shuohang Wang"
                    },
                    {
                        "authorId": "144702900",
                        "name": "Zhe Gan"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "46700348",
                        "name": "Jingjing Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The evaluation metrics also follow the standards [69, 16, 7, 3].",
                "LTH has been widely explored in image classification [31, 55, 76, 28, 34, 72, 80, 81, 56, 15], natural language processing [35, 83, 69, 67, 9], generative adversarial networks [17, 8], graph neural networks [14], and reinforcement learning [83].",
                "Subnetworks (mTi , \u03b80) and (mTi , \u03b85%) are found on the task Ti with the random initialization \u03b80 [31] and an early rewinding weights \u03b85% [69].",
                "Followup investigations [55, 35] scale up LTH by rewinding approaches [33, 69], that re-initializes the subnetwork from",
                "6Early weight rewinding [69, 32] improves the quality of found matching subnetworks.",
                "In the previous larger-scale settings of LTH for CV [31, 69], the matching subnetworks are found at an early point in training.",
                "We use the default implementations and hyperparameters [69, 10, 40, 16, 7, 51, 3].",
                ", applying At ) and then removing a portion of weights with the globally smallest magnitudes [38, 69]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5f6fccc32953f57fe29b2316eb8351e84b0179dc",
                "externalIds": {
                    "MAG": "3111921445",
                    "DBLP": "journals/corr/abs-2012-06908",
                    "ArXiv": "2012.06908",
                    "DOI": "10.1109/CVPR46437.2021.01604",
                    "CorpusId": 229152261
                },
                "corpusId": 229152261,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5f6fccc32953f57fe29b2316eb8351e84b0179dc",
                "title": "The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models",
                "abstract": "The computer vision world has been re-gaining enthusiasm in various pre-trained models, including both classical ImageNet supervised pre-training and recently emerged self-supervised pre-training such as simCLR [10] and MoCo [40]. Pre-trained weights often boost a wide range of downstream tasks including classification, detection, and segmentation. Latest studies suggest that pre-training benefits from gigantic model capacity [11]. We are hereby curious and ask: after pre-training, does a pre-trained model indeed have to stay large for its downstream transferability? In this paper, we examine supervised and self-supervised pre-trained models through the lens of the lottery ticket hypothesis (LTH) [31]. LTH identifies highly sparse matching subnetworks that can be trained in isolation from (nearly) scratch yet still reach the full models' performance. We extend the scope of LTH and question whether matching subnetworks still exist in pre-trained computer vision models, that enjoy the same downstream transfer performance. Our extensive experiments convey an overall positive message: from all pre-trained weights obtained by ImageNet classification, simCLR, and MoCo, we are consistently able to locate such matching subnetworks at 59.04% to 96.48% sparsity that transfer universally to multiple downstream tasks, whose performance see no degradation compared to using full pre-trained weights. Further analyses reveal that subnetworks found from different pre-training tend to yield diverse mask structures and perturbation sensitivities. We conclude that the core LTH observations remain generally relevant in the pre-training paradigm of computer vision, but more delicate discussions are needed in some cases. Codes and pre-trained models will be made available at: https://github.com/VITA-Group/CV_LTH_Pre-training.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2648459",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "3307026",
                        "name": "Shiyu Chang"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "37873860",
                        "name": "Yang Zhang"
                    },
                    {
                        "authorId": "1701041",
                        "name": "Michael Carbin"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also investigate the efficacy of methods introduced by [12, 38, 13, 42] such as iterative magnitude pruning, late resetting, early bird training, and layerwise pruning in the context of object recognition."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "65c826271c6d352d92d0519a569763d6ff913c27",
                "externalIds": {
                    "DBLP": "conf/cvpr/GirishMGCDS21",
                    "MAG": "3111265704",
                    "ArXiv": "2012.04643",
                    "DOI": "10.1109/CVPR46437.2021.00082",
                    "CorpusId": 227739001
                },
                "corpusId": 227739001,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/65c826271c6d352d92d0519a569763d6ff913c27",
                "title": "The Lottery Ticket Hypothesis for Object Recognition",
                "abstract": "Recognition tasks, such as object recognition and key-point estimation, have seen widespread adoption in recent years. Most state-of-the-art methods for these tasks use deep networks that are computationally expensive and have huge memory footprints. This makes it exceedingly difficult to deploy these systems on low power embedded devices. Hence, the importance of decreasing the storage requirements and the amount of computation in such models is paramount. The recently proposed Lottery Ticket Hypothesis (LTH) states that deep neural networks trained on large datasets contain smaller subnetworks that achieve on par performance as the dense networks. In this work, we perform the first empirical study investigating LTH for model pruning in the context of object detection, instance segmentation, and keypoint estimation. Our studies reveal that lottery tickets obtained from Imagenet pretraining do not transfer well to the downstream tasks. We provide guidance on how to find lottery tickets with up to 80% overall sparsity on different sub-tasks without incurring any drop in the performance. Finally, we analyse the behavior of trained tickets with respect to various task attributes such as object size, frequency, and difficulty of detection.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "143720888",
                        "name": "Sharath Girish"
                    },
                    {
                        "authorId": "51469126",
                        "name": "Shishira R. Maiya"
                    },
                    {
                        "authorId": "145428082",
                        "name": "Kamal Gupta"
                    },
                    {
                        "authorId": "50688939",
                        "name": "Hao Chen"
                    },
                    {
                        "authorId": "1693428",
                        "name": "L. Davis"
                    },
                    {
                        "authorId": "1781242",
                        "name": "Abhinav Shrivastava"
                    }
                ]
            }
        },
        {
            "contexts": [
                "More recently, the Rewind [57] and Group-Sparsity [49] algorithms have been demonstrated to be state-of-the-art compression algorithms.",
                "[57] proposed the rewind algorithm which is similar to simple fine-tuning of the network to regain the loss in accuracy incurred during the pruning step.",
                "We prune the model using unstructured pruning at a sparsity of 81% and the rewind algorithm [57] (see Section 2 for a more detailed description of rewinding)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d7a52afe313b5b82fba4f4fbcb44483ea513af07",
                "externalIds": {
                    "MAG": "3109051940",
                    "DBLP": "journals/corr/abs-2012-01604",
                    "CorpusId": 227253734
                },
                "corpusId": 227253734,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d7a52afe313b5b82fba4f4fbcb44483ea513af07",
                "title": "Reliable Model Compression via Label-Preservation-Aware Loss Functions",
                "abstract": "Model compression is a ubiquitous tool that brings the power of modern deep learning to edge devices with power and latency constraints. The goal of model compression is to take a large reference neural network and output a smaller and less expensive compressed network that is functionally equivalent to the reference. Compression typically involves pruning and/or quantization, followed by re-training to maintain the reference accuracy. However, it has been observed that compression can lead to a considerable mismatch in the labels produced by the reference and the compressed models, resulting in bias and unreliability. To combat this, we present a framework that uses a teacher-student learning paradigm to better preserve labels. We investigate the role of additional terms to the loss function and show how to automatically tune the associated parameters. We demonstrate the effectiveness of our approach both quantitatively and qualitatively on multiple compression schemes and accuracy recovery algorithms using a set of 8 different real-world network architectures. We obtain a significant reduction of up to 4.1X in the number of mismatches between the compressed and reference models, and up to 5.7X in cases where the reference model makes the correct prediction.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50665773",
                        "name": "Vinu Joseph"
                    },
                    {
                        "authorId": "29005173",
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "authorId": "1736564",
                        "name": "Aditya Bhaskara"
                    },
                    {
                        "authorId": "48730765",
                        "name": "Ganesh Gopalakrishnan"
                    },
                    {
                        "authorId": "31225166",
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "authorId": "144764367",
                        "name": "M. Garland"
                    },
                    {
                        "authorId": "1734717217",
                        "name": "Sheraz Ahmed"
                    },
                    {
                        "authorId": "145279674",
                        "name": "A. Dengel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[61], while studying the lottery ticket hypothesis, came up with a method called learning rate rewinding, which proposes replacing the fine-tuning step with a full retraining stage that uses the weights of the trained and pruned network as a new initialization."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e4ca4f0cf2d8dc5327c04e193064297fd63f1f60",
                "externalIds": {
                    "DBLP": "journals/jimaging/TessierGLAHB22",
                    "PubMedCentral": "8950981",
                    "ArXiv": "2011.10520",
                    "DOI": "10.3390/jimaging8030064",
                    "CorpusId": 247303139,
                    "PubMed": "35324619"
                },
                "corpusId": 247303139,
                "publicationVenue": {
                    "id": "c0fc53c7-b0ed-487d-9191-1262c8322621",
                    "name": "Journal of Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "J Imaging"
                    ],
                    "issn": "2313-433X",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-556372",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/jimaging",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-556372"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e4ca4f0cf2d8dc5327c04e193064297fd63f1f60",
                "title": "Rethinking Weight Decay for Efficient Neural Network Pruning",
                "abstract": "Introduced in the late 1980s for generalization purposes, pruning has now become a staple for compressing deep neural networks. Despite many innovations in recent decades, pruning approaches still face core issues that hinder their performance or scalability. Drawing inspiration from early work in the field, and especially the use of weight decay to achieve sparsity, we introduce Selective Weight Decay (SWD), which carries out efficient, continuous pruning throughout training. Our approach, theoretically grounded on Lagrangian smoothing, is versatile and can be applied to multiple tasks, networks, and pruning structures. We show that SWD compares favorably to state-of-the-art approaches, in terms of performance-to-parameters ratio, on the CIFAR-10, Cora, and ImageNet ILSVRC2012 datasets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "146940144",
                        "name": "Hugo Tessier"
                    },
                    {
                        "authorId": "144916029",
                        "name": "Vincent Gripon"
                    },
                    {
                        "authorId": "27532368",
                        "name": "Mathieu L\u00e9onardon"
                    },
                    {
                        "authorId": "2409852",
                        "name": "M. Arzel"
                    },
                    {
                        "authorId": "3312711",
                        "name": "T. Hannagan"
                    },
                    {
                        "authorId": "2060222860",
                        "name": "David Bertrand"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "748c5f3d41ccf8ec9956f26179de57d1643a7b7e",
                "externalIds": {
                    "MAG": "3101335452",
                    "ArXiv": "2011.06923",
                    "DBLP": "journals/corr/abs-2011-06923",
                    "CorpusId": 226955870
                },
                "corpusId": 226955870,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/748c5f3d41ccf8ec9956f26179de57d1643a7b7e",
                "title": "LEAN: graph-based pruning for convolutional neural networks by extracting longest chains",
                "abstract": "Convolutional neural networks (CNNs) have proven to be highly successful at a range of image-to-image tasks. CNNs can be computationally expensive, which can limit their applicability in practice. Model pruning can improve computational efficiency by sparsifying trained networks. Common methods for pruning CNNs determine what convolutional filters to remove by ranking filters on an individual basis. However, filters are not independent, as CNNs consist of chains of convolutions, which can result in sub-optimal filter selection. \nWe propose a novel pruning method, LongEst-chAiN (LEAN) pruning, which takes the interdependency between the convolution operations into account. We propose to prune CNNs by using graph-based algorithms to select relevant chains of convolutions. A CNN is interpreted as a graph, with the operator norm of each convolution as distance metric for the edges. LEAN pruning iteratively extracts the highest value path from the graph to keep. In our experiments, we test LEAN pruning for several image-to-image tasks, including the well-known CamVid dataset. LEAN pruning enables us to keep just 0.5%-2% of the convolutions without significant loss of accuracy. When pruning CNNs with LEAN, we achieve a higher accuracy than pruning filters individually, and different pruned substructures emerge.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2066082560",
                        "name": "R. Schoonhoven"
                    },
                    {
                        "authorId": "153338877",
                        "name": "A. Hendriksen"
                    },
                    {
                        "authorId": "7319530",
                        "name": "D. Pelt"
                    },
                    {
                        "authorId": "8147628",
                        "name": "K. Batenburg"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Inspired by this finding, we rewound the unimportant component to the initialization values Frankle and Carbin (2019; Renda et al. (2020) and fine-tune\nthem together with the other trained components for a few more steps."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ca957919dded903bcddda96a9e3590edbdb230f4",
                "externalIds": {
                    "MAG": "3101193295",
                    "ACL": "2020.coling-main.529",
                    "DBLP": "conf/coling/WangT20a",
                    "ArXiv": "2011.03803",
                    "DOI": "10.18653/V1/2020.COLING-MAIN.529",
                    "CorpusId": 226281941
                },
                "corpusId": 226281941,
                "publicationVenue": {
                    "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
                    "name": "International Conference on Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Linguistics",
                        "COLING"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/coling/"
                },
                "url": "https://www.semanticscholar.org/paper/ca957919dded903bcddda96a9e3590edbdb230f4",
                "title": "Rethinking the Value of Transformer Components",
                "abstract": "Transformer becomes the state-of-the-art translation model, while it is not well studied how each intermediate component contributes to the model performance, which poses significant challenges for designing optimal architectures. In this work, we bridge this gap by evaluating the impact of individual component (sub-layer) in trained Transformer models from different perspectives. Experimental results across language pairs, training strategies, and model capacities show that certain components are consistently more important than the others. We also report a number of interesting findings that might help humans better analyze, understand and improve Transformer models. Based on these observations, we further propose a new training strategy that can improves translation performance by distinguishing the unimportant components in training.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2144328160",
                        "name": "Wenxuan Wang"
                    },
                    {
                        "authorId": "2909321",
                        "name": "Zhaopeng Tu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Network Pruning On the other hand, our work is also related to the studies on network pruning, including but not limited to [39, 40, 41, 42, 43, 44, 45, 46]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9e6428e9ddc3ed7b5b5d84ddcaa9a61c98500ed5",
                "externalIds": {
                    "DBLP": "journals/nn/DengZ22",
                    "ArXiv": "2011.00580",
                    "DOI": "10.1016/j.neunet.2021.10.018",
                    "CorpusId": 239616550,
                    "PubMed": "34773898"
                },
                "corpusId": 239616550,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9e6428e9ddc3ed7b5b5d84ddcaa9a61c98500ed5",
                "title": "Sparsity-control ternary weight networks",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2150478789",
                        "name": "Xiang Deng"
                    },
                    {
                        "authorId": "2155968395",
                        "name": "Zhongfei Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In light of the aforementioned related works, we observed that most claims and conclusions are based on experiments with unstructured pruning, with the exception of Renda et al. (2020) [20] who present preliminary experiments with structured pruning.",
                "More recently, Renda et al. (2020) [20] introduced the Learning Rate Rewinding method, a novel retraining approach, as a variation of weight rewinding.",
                "\u2026iteratively pruned networks found by weight rewinding, learning rate rewinding and\n2The percentage of remaining parameters was calculated from the reported compression ratio values of Renda et al. (2020) [20].\ntheir randomly initialized counterparts and considering local and global pruning.",
                "2) Learning Rate Rewinding: In Figure 5, we present the results from applying the learning rate (LR) rewinding retraining technique as proposed by Renda et al. (2020) [20].",
                "For comparison, Renda et al. (2020) [20] apply structured pruning with learning rate Rewinding for a small range of compression levels, from approximately 86.96% to 58.82% of the remaining parameters on ResNet-56 and 92.60% to 79.36% of the remaining parameters on ResNet-34.",
                "In addition, despite we considered an architecture (VGG16 [6]) different from those investigated by Renda et al. (2020) [20] (ResNet-56 and ResNet-34 [35]), they were all convolutional neural networks.",
                "2) Global pruning: In the global pruning, as applied by Frankle et al. (2019) [18], Salama et al. (2019) [25], Renda et al. (2020) [20], the elements are removed without taking into account their location."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1910705fdcbfaaa9ae3c18c8923bc7f37a83df08",
                "externalIds": {
                    "DBLP": "conf/sibgrapi/MagalhaesFGMS20",
                    "MAG": "3109815239",
                    "DOI": "10.1109/SIBGRAPI51738.2020.00044",
                    "CorpusId": 227222061
                },
                "corpusId": 227222061,
                "publicationVenue": {
                    "id": "e8fb3e28-e9d7-4585-9e72-660cef7f6802",
                    "name": "SIBGRAPI Conference on Graphics, Patterns and Images",
                    "type": "conference",
                    "alternate_names": [
                        "SIBGRAPI Conf Graph Pattern Image",
                        "Braz Symp Comput Graph Image Process",
                        "SIBGRAPI",
                        "Brazilian Symposium on Computer Graphics and Image Processing"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1910705fdcbfaaa9ae3c18c8923bc7f37a83df08",
                "title": "Evaluating the Emergence of Winning Tickets by Structured Pruning of Convolutional Networks",
                "abstract": "The recently introduced Lottery Ticket Hypothesis has created a new investigation front in neural network pruning. The hypothesis states that it is possible to find subnetworks with high generalization capabilities (winning tickets) from an over-parameterized neural network. One step of the algorithm implementing the hypothesis requires resetting the weights of the pruned network to their initial random values. More recent variations of this step may involve: (i) resetting the weights to the values they had at an early epoch of the unpruned network training, or (ii) keeping the final training weights and resetting only the learning rate schedule. Despite some studies have investigated the above variations, mostly with unstructured pruning, we do not know of existing evaluations focusing on structured pruning regarding local and global pruning variations. In this context, this paper presents novel empirical evidence that it is possible to obtain winning tickets when performing structured pruning of convolutional neural networks. We setup an experiment using the VGG-16 network trained on the CIFAR-10 dataset and compared networks (pruned at different compression levels) got by weight rewinding and learning rate rewinding methods, under local and global pruning regimes. We use the unpruned network as baseline and also compare the resulting pruned networks with their versions trained with randomly initialized weights. Overall, local pruning failed to find winning tickets for both rewinding methods. When using global pruning, weight rewinding produced a few winning tickets (limited to low pruning levels only) and performed nearly the same or worse compared to random initialization. Learning rate rewinding, under global pruning, produced the best results, since it has found winning tickets at most pruning levels and outperformed the baseline.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2029365233",
                        "name": "W. F. Magalh\u00e3es"
                    },
                    {
                        "authorId": "2112745722",
                        "name": "Jeferson Ferreira"
                    },
                    {
                        "authorId": "2092178",
                        "name": "H. Gomes"
                    },
                    {
                        "authorId": "1739660",
                        "name": "L. Marinho"
                    },
                    {
                        "authorId": "2066129695",
                        "name": "P. Silveira"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, our work is also related to the studies on network pruning, including but not limited to [40], [41], [42], [43], [44], [45], [46], [47]."
            ],
            "intents": [
                "result"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e9445d96280a3560958ed43f956f63ab6e2bc85d",
                "externalIds": {
                    "MAG": "3094757741",
                    "DBLP": "journals/corr/abs-2011-00580",
                    "CorpusId": 226226472
                },
                "corpusId": 226226472,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e9445d96280a3560958ed43f956f63ab6e2bc85d",
                "title": "An Embarrassingly Simple Approach to Training Ternary Weight Networks",
                "abstract": "Deep neural networks (DNNs) have achieved great successes in various domains of artificial intelligence, but they require large amounts of memory and computational power. This severely restricts their implementation on resource-limited hardware. One approach to solving this problem is to train DNNs with ternary weights \\{-1, 0, +1\\}, thus avoiding multiplications and dramatically reducing the memory and computation requirements. However, the existing approaches to training ternary weight networks either have a large performance gap to the full precision counterparts or have a complex training process, which makes ternary weight networks not widely used. In this paper, we propose an embarrassingly simple approach (ESA) to training ternary weight networks. Specifically, ESA first parameterizes the weights $W$ in a DNN with $\\tanh(\\Theta)$ where $\\Theta$ are the parameters, so that the weight values are limited in the range between -1 and +1, and then a weight discretization regularization (WDR) is used to force the weights to be ternary. Consequently, ESA has an extremely high code reuse rate when converting a full precision weight DNN to the ternary version. More importantly, ESA is able to control the sparsity (i.e., the percentage of 0s) of the ternary weights through a controller $\\alpha$ in WDR. We theoretically and empirically show that the sparsity of the trained ternary weights is positively related to $\\alpha$. To the best of our knowledge, ESA is the first sparsity-controlling approach to training ternary weight networks. Extensive experiments on several benchmark datasets demonstrate that ESA beats the state-of-the-art approaches significantly and matches the performances of the full precision weight networks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2150478789",
                        "name": "Xiang Deng"
                    },
                    {
                        "authorId": "2118748124",
                        "name": "Zhongfei Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As to our choice of LC, recent work [35], [36] has shown that for high-sparsity regimes, LC performs really well, especially when the optimization becomes difficult with stringent resource constraints imposed."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4b3da4ec8866a00a4505ecf5892784d28a4bfc7c",
                "externalIds": {
                    "DBLP": "conf/sc/JosephCBGPZ20",
                    "DOI": "10.1109/Correctness51934.2020.00006",
                    "CorpusId": 229376299
                },
                "corpusId": 229376299,
                "publicationVenue": {
                    "id": "67d1fd46-1cef-45b3-9f57-a8e9317f4408",
                    "name": "International Workshop on Software Correctness for HPC Applications",
                    "type": "conference",
                    "alternate_names": [
                        "Correctness",
                        "Int Workshop Softw Correctness HPC Appl"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4b3da4ec8866a00a4505ecf5892784d28a4bfc7c",
                "title": "Correctness-preserving Compression of Datasets and Neural Network Models",
                "abstract": "Neural networks deployed on edge devices must be efficient both in terms of their model size and the amount of data movement they cause when classifying inputs. These efficiencies are typically achieved through model compression: pruning a fully trained network model by zeroing out the weights. Given the overall challenge of neural network correctness, we argue that focusing on correctness preservation may allow the community to make measurable progress. We present a state-of-the-art model compression framework called Condensa around which we have launched correctness preservation studies. After presenting Condensa, we describe our initial efforts at understanding the effect of model compression in semantic terms, going beyond the top n% accuracy that Condensa is currently based on. We also take up the relatively unexplored direction of data compression that may help reduce data movement. We report preliminary results of learning from decompressed data to understand the effects of compression artifacts. Learning without decompressing input data also holds promise in terms of boosting efficiency, and we also report preliminary results in this regard. Our experiments centered around a state-of-the-art model compression framework called Condensa and two data compression algorithms, namely JPEG and ZFP, demonstrate the potential for employing model-and dataset compression without adversely affecting correctness.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50665773",
                        "name": "Vinu Joseph"
                    },
                    {
                        "authorId": "1466404605",
                        "name": "N. Chalapathi"
                    },
                    {
                        "authorId": "1736564",
                        "name": "Aditya Bhaskara"
                    },
                    {
                        "authorId": "48730765",
                        "name": "Ganesh Gopalakrishnan"
                    },
                    {
                        "authorId": "2333441",
                        "name": "P. Panchekha"
                    },
                    {
                        "authorId": "2112138754",
                        "name": "Mu Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent works show that their parameters can be reduced by more than 90% without accuracy drop [3], [4].",
                "One of popular frameworks for compressing a neural network consists of three steps: pre-training the network, removing unimportant components, and re-training the remaining structure [3], [7], [4]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f82b7f354d2afb476fbcf8a4b74dc55ca5b6632f",
                "externalIds": {
                    "DBLP": "conf/icdm/PhanNNK20",
                    "DOI": "10.1109/ICDM50108.2020.00152",
                    "CorpusId": 231914938
                },
                "corpusId": 231914938,
                "publicationVenue": {
                    "id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
                    "name": "Industrial Conference on Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Ind Conf Data Min",
                        "ICDM"
                    ],
                    "url": "http://www.data-mining-forum.de/"
                },
                "url": "https://www.semanticscholar.org/paper/f82b7f354d2afb476fbcf8a4b74dc55ca5b6632f",
                "title": "Pruning Deep Neural Networks with $\\ell_{0}$-constrained Optimization",
                "abstract": "Deep neural networks (DNNs) give state-of-the-art accuracy in many tasks, but they can require large amounts of memory storage, energy consumption, and long inference times. Modern DNNs can have hundreds of million parameters, which make it difficult for DNNs to be deployed in some applications with low-resource environments. Pruning redundant connections without sacrificing accuracy is one of popular approaches to overcome these limitations. We propose two $\\ell_{0}$-constrained optimization models for pruning deep neural networks layer-by-layer. The first model is devoted to a general activation function, while the second one is specifically for a ReLU. We introduce an efficient cutting plane algorithm to solve the latter to optimality. Our experiments show that the proposed approach achieves competitive compression rates over several state-of-the-art baseline methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "41128347",
                        "name": "D. Phan"
                    },
                    {
                        "authorId": "144274166",
                        "name": "Lam M. Nguyen"
                    },
                    {
                        "authorId": "144547425",
                        "name": "Nam H. Nguyen"
                    },
                    {
                        "authorId": "1682581",
                        "name": "J. Kalagnanam"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, previous studies do not consider time constraints for the pruning phase [12, 19], which differs from human sleep that lasts for a fixed amount of time."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "854f90809b50d67a2b4eb94d22a1010f509e1b0c",
                "externalIds": {
                    "ArXiv": "2010.15187",
                    "DBLP": "journals/corr/abs-2010-15187",
                    "MAG": "3094926694",
                    "CorpusId": 225103161
                },
                "corpusId": 225103161,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/854f90809b50d67a2b4eb94d22a1010f509e1b0c",
                "title": "A Study on Efficiency in Continual Learning Inspired by Human Learning",
                "abstract": "Humans are efficient continual learning systems; we continually learn new skills from birth with finite cells and resources. Our learning is highly optimized both in terms of capacity and time while not suffering from catastrophic forgetting. In this work we study the efficiency of continual learning systems, taking inspiration from human learning. In particular, inspired by the mechanisms of sleep, we evaluate popular pruning-based continual learning algorithms, using PackNet as a case study. First, we identify that weight freezing, which is used in continual learning without biological justification, can result in over $2\\times$ as many weights being used for a given level of performance. Secondly, we note the similarity in human day and night time behaviors to the training and pruning phases respectively of PackNet. We study a setting where the pruning phase is given a time budget, and identify connections between iterative pruning and multiple sleep cycles in humans. We show there exists an optimal choice of iteration v.s. epochs given different tasks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2053179501",
                        "name": "Philip J. Ball"
                    },
                    {
                        "authorId": "2672661",
                        "name": "Yingzhen Li"
                    },
                    {
                        "authorId": "1828778293",
                        "name": "A. Lamb"
                    },
                    {
                        "authorId": "2155988713",
                        "name": "Cheng Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The model without fine-tuning (dotted) outperforms some of the related works [1, 2, 4, 7, 8, 10, 15, 21, 22, 23].",
                "parameter count against state-ofthe-art pruning approaches including [1, 2, 4, 7, 8, 10, 15, 21, 22, 23].",
                "Note that models outperforming the proposed approach for stronger compression levels [6, 10], see Fig.",
                "Iterative pruning [4, 5, 6] and access to substantial fine-tuning [7, 8, 9] or even re-training from scratch [5, 10] allows many techniques to arrive at the (nearly) original levels of performance at the cost of additional 30% to 300% of the original amount of training."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6e6b75ea8bb3b656b6637a359559e3cc51937cf7",
                "externalIds": {
                    "DBLP": "conf/icassp/UlicnyKD21",
                    "ArXiv": "2010.12110",
                    "MAG": "3094557303",
                    "DOI": "10.1109/ICASSP39728.2021.9413944",
                    "CorpusId": 225062476
                },
                "corpusId": 225062476,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/6e6b75ea8bb3b656b6637a359559e3cc51937cf7",
                "title": "Tensor Reordering for CNN Compression",
                "abstract": "We show how parameter redundancy in Convolutional Neural Network (CNN) filters can be effectively reduced by pruning in spectral domain. Specifically, the representation extracted via Discrete Co-sine Transform (DCT) is more conducive for pruning than the original space. By relying on a combination of weight tensor reshaping and reordering we achieve high levels of layer compression with just minor accuracy loss. Our approach is applied to compress pre-trained CNNs and we show that minor additional fine-tuning allows our method to recover the original model performance after a significant parameter reduction. We validate our approach on ResNet-50 and MobileNet-V2 architectures for ImageNet classification task.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51883316",
                        "name": "Matej Ulicny"
                    },
                    {
                        "authorId": "1750277",
                        "name": "V. Krylov"
                    },
                    {
                        "authorId": "1680022",
                        "name": "Rozenn Dahyot"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, a very clever method was recently presented by Renda et al. (2020), with much success on a variety of neural networks.",
                "Simple yet powerful algorithms, such as the one by Renda et al. (2020) have already supplied an empirical confirmation of this hypothesis, by providing a procedure through which to obtain high-performing pruned versions of a variety of models."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f48da6e30ae5c8d651aa0646645bf733a511e25e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-08512",
                    "MAG": "3092811698",
                    "ArXiv": "2010.08512",
                    "CorpusId": 223956781
                },
                "corpusId": 223956781,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f48da6e30ae5c8d651aa0646645bf733a511e25e",
                "title": "An Approximation Algorithm for Optimal Subarchitecture Extraction",
                "abstract": "We consider the problem of finding the set of architectural parameters for a chosen deep neural network which is optimal under three metrics: parameter size, inference speed, and error rate. In this paper we state the problem formally, and present an approximation algorithm that, for a large subset of instances behaves like an FPTAS with an approximation error of $\\rho \\leq |{1- \\epsilon}|$, and that runs in $O(|{\\Xi}| + |{W^*_T}|(1 + |{\\Theta}||{B}||{\\Xi}|/({\\epsilon\\, s^{3/2})}))$ steps, where $\\epsilon$ and $s$ are input parameters; $|{B}|$ is the batch size; $|{W^*_T}|$ denotes the cardinality of the largest weight set assignment; and $|{\\Xi}|$ and $|{\\Theta}|$ are the cardinalities of the candidate architecture and hyperparameter spaces, respectively.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1388060354",
                        "name": "Adrian de Wynter"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is partially confirmed by Renda, Frankle, and Carbin (2020) which shows that restarting the learning rate schedule from the pruning solution performs better than rewinding the weights."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "917b18b8dad23284c0a42f665f2ba1984fa360de",
                "externalIds": {
                    "DBLP": "conf/aaai/EvciIKD22",
                    "MAG": "3092446983",
                    "ArXiv": "2010.03533",
                    "DOI": "10.1609/aaai.v36i6.20611",
                    "CorpusId": 222177996
                },
                "corpusId": 222177996,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/917b18b8dad23284c0a42f665f2ba1984fa360de",
                "title": "Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win",
                "abstract": "Sparse Neural Networks (NNs) can match the generalization of dense NNs using a fraction of the compute/storage for inference, and have the potential to enable efficient training. However, naively training unstructured sparse NNs from random initialization results in significantly worse generalization, with the notable exceptions of Lottery Tickets (LTs) and Dynamic Sparse Training (DST). In this work, we attempt to answer: (1) why training unstructured sparse networks from random initialization performs poorly and; (2) what makes LTs and DST the exceptions? We show that sparse NNs have poor gradient flow at initialization and propose a modified initialization for unstructured connectivity. Furthermore, we find that DST methods significantly improve gradient flow during training over traditional sparse training methods. Finally, we show that LTs do not improve gradient flow, rather their success lies in re-learning the pruning solution they are derived from \u2014 however, this comes at the cost of learning novel solutions.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3399348",
                        "name": "Utku Evci"
                    },
                    {
                        "authorId": "3067465",
                        "name": "Yani Andrew Ioannou"
                    },
                    {
                        "authorId": "3860190",
                        "name": "Cem Keskin"
                    },
                    {
                        "authorId": "2921469",
                        "name": "Yann Dauphin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another line of research loosely connected to our work is to reduce inference time via pruning less significant weights and/or converting the model to low-precision (aka quantization) (Han et al., 2016; Howard et al., 2017; Iandola et al., 2016; Renda et al., 2020; Frankle and Carbin, 2019).",
                "Another line of research loosely connected to our work is to reduce inference time via pruning less significant weights and/or converting the model to low-precision (aka quantization) (Han et al., 2016; Howard et al., 2017; Iandola et al., 2016; Renda\net al., 2020; Frankle and Carbin, 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e1f4dd596cdbff6fd1edc804a851b8783a546fcf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-03099",
                    "MAG": "3092510499",
                    "ACL": "2020.findings-emnlp.264",
                    "ArXiv": "2010.03099",
                    "DOI": "10.18653/v1/2020.findings-emnlp.264",
                    "CorpusId": 222177208
                },
                "corpusId": 222177208,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/e1f4dd596cdbff6fd1edc804a851b8783a546fcf",
                "title": "DiPair: Fast and Accurate Distillation for Trillion-ScaleText Matching and Pair Modeling",
                "abstract": "Pre-trained models like BERT ((Devlin et al., 2018) have dominated NLP / IR applications such as single sentence classification, text pair classification, and question answering. However, deploying these models in real systems is highly non-trivial due to their exorbitant computational costs. A common remedy to this is knowledge distillation (Hinton et al., 2015), leading to faster inference. However \u2013 as we show here \u2013 existing works are not optimized for dealing with pairs (or tuples) of texts. Consequently, they are either not scalable or demonstrate subpar performance. In this work, we propose DiPair \u2014 a novel framework for distilling fast and accurate models on text pair tasks. Coupled with an end-to-end training strategy, DiPair is both highly scalable and offers improved quality-speed tradeoffs. Empirical studies conducted on both academic and real-world e-commerce benchmarks demonstrate the efficacy of the proposed approach with speedups of over 350x and minimal quality drop relative to the cross-attention teacher BERT model.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2809410",
                        "name": "Jiecao Chen"
                    },
                    {
                        "authorId": "1988888746",
                        "name": "Liu Yang"
                    },
                    {
                        "authorId": "2062947723",
                        "name": "K. Raman"
                    },
                    {
                        "authorId": "1815447",
                        "name": "Michael Bendersky"
                    },
                    {
                        "authorId": "34727699",
                        "name": "Jung-Jung Yeh"
                    },
                    {
                        "authorId": "2118116642",
                        "name": "Yun Zhou"
                    },
                    {
                        "authorId": "1763978",
                        "name": "Marc Najork"
                    },
                    {
                        "authorId": "1833259140",
                        "name": "Danyang Cai"
                    },
                    {
                        "authorId": "2465392",
                        "name": "Ehsan Emadzadeh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "And as pointed out in [37], learning rate rewinding usually surpasses weights rewinding, so we mostly focus on learning rate rewinding.",
                "iterative pruning cannot consistently outperform one shot pruning, and even when iterative pruning is better, the gap is small, as shown in [27] and [37].",
                "Different from initial tickets, [37] propose a learning rate rewinding method that improves beyond weights rewinding [10].",
                "In addition, we also find a very recent pruning method in ICLR 2020 [37] for obtaining \u201cpartiallytrained tickets\u201d can pass our sanity checks.",
                "\u2022 Partially-trained tickets: Different from initial tickets, partially-trained tickets are constructed by first training the network, pruning it, and then rewinding the weights to some middle stage [37].",
                "In this section, we study pruning methods in a very recent ICLR 2020 paper [37], which is classified as partially-trained tickets (Section 2.",
                "We then apply our sanity checks on the pruning methods in four recent papers from ICLR 2019 and 2020 [23, 9, 41, 37].",
                "The methods used in [37] include weights rewinding and learning rate rewinding.",
                "We then combine our insights of random tickets with these partially-trained tickets and propose a method called \u201chybrid tickets\u201d (Figure 2) which further improves upon [37]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5a04c3d28ae074bf382f8d831e40b6db7c8de509",
                "externalIds": {
                    "DBLP": "conf/nips/SuCCWG0L20",
                    "ArXiv": "2009.11094",
                    "MAG": "3088750390",
                    "CorpusId": 221857593
                },
                "corpusId": 221857593,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5a04c3d28ae074bf382f8d831e40b6db7c8de509",
                "title": "Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot",
                "abstract": "Network pruning is a method for reducing test-time computational resource requirements with minimal performance degradation. Conventional wisdom of pruning algorithms suggests that: (1) Pruning methods exploit information from training data to find good subnetworks; (2) The architecture of the pruned network is crucial for good performance. In this paper, we conduct sanity checks for the above beliefs on several recent unstructured pruning methods and surprisingly find that: (1) A set of methods which aims to find good subnetworks of the randomly-initialized network (which we call \"initial tickets\"), hardly exploits any information from the training data; (2) For the pruned networks obtained by these methods, randomly changing the preserved weights in each layer, while keeping the total number of preserved weights unchanged per layer, does not affect the final performance. These findings inspire us to choose a series of simple \\emph{data-independent} prune ratios for each layer, and randomly prune each layer accordingly to get a subnetwork (which we call \"random tickets\"). Experimental results show that our zero-shot random tickets outperform or attain a similar performance compared to existing \"initial tickets\". In addition, we identify one existing pruning method that passes our sanity checks. We hybridize the ratios in our random ticket with this method and propose a new method called \"hybrid tickets\", which achieves further improvement. (Our code is publicly available at this https URL)",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1962384229",
                        "name": "Jingtong Su"
                    },
                    {
                        "authorId": "2116613902",
                        "name": "Yihang Chen"
                    },
                    {
                        "authorId": "123970124",
                        "name": "Tianle Cai"
                    },
                    {
                        "authorId": "47353876",
                        "name": "Tianhao Wu"
                    },
                    {
                        "authorId": "9659905",
                        "name": "Ruiqi Gao"
                    },
                    {
                        "authorId": "24952249",
                        "name": "Liwei Wang"
                    },
                    {
                        "authorId": "2421201",
                        "name": "J. Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Magnitude pruning is a state-of-the-art method for one-shot pruning after training (Renda et al., 2020).",
                "After pruning at step t of training, we subsequently train the network further by repeating the entire learning rate schedule from the start (Renda et al., 2020).",
                "These subnetworks are as small as those found by inference-focused pruning methods after training (Appendix B, Renda et al., 2020), meaning it may be possible to maintain this level of sparsity for much or all of training."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "0932abfd0fb90e8a28f7bd195633c9891bfd7ecb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2009-08576",
                    "MAG": "3087194612",
                    "ArXiv": "2009.08576",
                    "CorpusId": 221802286
                },
                "corpusId": 221802286,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0932abfd0fb90e8a28f7bd195633c9891bfd7ecb",
                "title": "Pruning Neural Networks at Initialization: Why are We Missing the Mark?",
                "abstract": "Recent work has explored the possibility of pruning neural networks at initialization. We assess proposals for doing so: SNIP (Lee et al., 2019), GraSP (Wang et al., 2020), SynFlow (Tanaka et al., 2020), and magnitude pruning. Although these methods surpass the trivial baseline of random pruning, they remain below the accuracy of magnitude pruning after training, and we endeavor to understand why. We show that, unlike pruning after training, accuracy is the same or higher when randomly shuffling which weights these methods prune within each layer or sampling new initial values. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune. This property undermines the claimed justifications for these methods and suggests broader challenges with the underlying pruning heuristics, the desire to prune at initialization, or both.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "2533850",
                        "name": "G. Dziugaite"
                    },
                    {
                        "authorId": "39331522",
                        "name": "Daniel M. Roy"
                    },
                    {
                        "authorId": "1701041",
                        "name": "Michael Carbin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We\n\u2217Equal contribution.\nprune in an iterative, lottery-ticket fashion to identify Transformers at competitive sparsities with no drop in task performance (Renda et al., 2020; Yu et al., 2020; Brix et al., 2020).",
                "Renda et al. (2020) propose learning rate (LR) rewinding, where the learning rate is rewound to a value earlier in training, but the weights remain unchanged.",
                "prune in an iterative, lottery-ticket fashion to identify Transformers at competitive sparsities with no drop in task performance (Renda et al., 2020; Yu et al., 2020; Brix et al., 2020)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8b2067065e67a0952a32894ba5693d2b07ee5516",
                "externalIds": {
                    "ACL": "2020.blackboxnlp-1.19",
                    "DBLP": "journals/corr/abs-2009-13270",
                    "MAG": "3087835661",
                    "ArXiv": "2009.13270",
                    "DOI": "10.18653/v1/2020.blackboxnlp-1.19",
                    "CorpusId": 221970983
                },
                "corpusId": 221970983,
                "publicationVenue": {
                    "id": "738626d7-5b8c-497d-9fd6-64bdb6dbf440",
                    "name": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
                    "type": "conference",
                    "alternate_names": [
                        "BlackboxNLP",
                        "Blackboxnlp Workshop Anal Interpr\u00e8t Neural Netw NLP"
                    ],
                    "url": "https://aclanthology.org/venues/blackboxnlp/"
                },
                "url": "https://www.semanticscholar.org/paper/8b2067065e67a0952a32894ba5693d2b07ee5516",
                "title": "Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation",
                "abstract": "Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU. However, it is unclear how such pruning techniques affect a model\u2019s learned representations. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded. Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts. Meanwhile, early layers of sparse models begin to perform more encoding. Attention mechanisms remain remarkably consistent as sparsity increases.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1405369173",
                        "name": "Rajiv Movva"
                    },
                    {
                        "authorId": "1972123165",
                        "name": "Jason Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note that, in addition to the polynomial terms, the relationship between accuracy and model sparsity is further modeled through an additional exponential term \u2013 a reasonable modeling assumption supported by prior knowledge of accuracy-sparsity curves in the pruning literature [9, 12, 25, 38, 31, 1, 10]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1b24ff8a707bba953a23d85c4971c8925405a180",
                "externalIds": {
                    "ArXiv": "2009.09936",
                    "DBLP": "journals/corr/abs-2009-09936",
                    "MAG": "3088702560",
                    "CorpusId": 221818901
                },
                "corpusId": 221818901,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1b24ff8a707bba953a23d85c4971c8925405a180",
                "title": "Prune Responsibly",
                "abstract": "Irrespective of the speci\ufb01c de\ufb01nition of fairness in a machine learning application, pruning the underlying model affects it. We investigate and document the emer-gence and exacerbation of undesirable per-class performance imbalances, across tasks and architectures, for almost one million categories considered across over 100K image classi\ufb01cation models that undergo a pruning process. We demonstrate the need for transparent reporting, inclusive of bias, fairness, and inclusion metrics, in real-life engineering decision-making around neural network pruning. In response to the calls for quantitative evaluation of AI models to be population-aware, we present neural network pruning as a tangible application domain where the ways in which accuracy-ef\ufb01ciency trade-offs disproportionately affect underrepresented or outlier groups have historically been overlooked. We provide a simple, Pareto-based framework to insert fairness considerations into value-based operating point selection processes, and to re-evaluate pruning technique choices. ways by pruning interventions. We are interested in quantifying the inequality of treatment among classes, cohorts, and individuals as network capacity is reduced. Speci\ufb01cally, the hypothesis we seek to test is that class imbalance and class complexity affect the per-class performance of pruned models. We model and measure the contribution of these factors to the observed changes in performance in pruned models, while controlling for other factors of variations, such as model type, dataset, pruning technique, and choice of rewinding or \ufb01netuning weights after pruning. We demonstrate that, as networks are pruned and sparsity increases, model performance deteriorates more substantially on underrepresented and complex classes than it does on others, thus exacerbating pre-existing disparities among classes. In addition, as model capacity is reduced, networks progressively lose the ability to remain invariant to example properties such as angular orientation in images, which results in higher performance losses for groups, sub-groups, and individuals whose examples exhibit these properties.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35550664",
                        "name": "Michela Paganini"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The intuition behind mapping specific ranges of weights per layer to approximate multiplication derives from the weight pruning based on magnitude [42], [43].",
                "In order to find an efficient weight-to-approximate mode mapping and reduce the number of evaluated solutions, we employ a four-step methodology based on the concepts of layer significance and weight magnitude [42], [43] (Figure 5)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0174d7d1d4b6778b280bc60c8bc1369987f940ca",
                "externalIds": {
                    "MAG": "3083485280",
                    "DBLP": "journals/tcas/TasoulasZAAH20",
                    "DOI": "10.1109/TCSI.2020.3019460",
                    "CorpusId": 225254316
                },
                "corpusId": 225254316,
                "publicationVenue": {
                    "id": "65967e36-f7db-476f-9d00-fd080a5a8483",
                    "name": "IEEE Transactions on Circuits and Systems Part 1: Regular Papers",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Circuit Syst Part 1 Regul Pap",
                        "IEEE Trans Circuit Syst I-regular Pap",
                        "IEEE Transactions on Circuits and Systems I-regular Papers"
                    ],
                    "issn": "1549-8328",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=8919",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8919"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0174d7d1d4b6778b280bc60c8bc1369987f940ca",
                "title": "Weight-Oriented Approximation for Energy-Efficient Neural Network Inference Accelerators",
                "abstract": "Current research in the area of Neural Networks (NN) has resulted in performance advancements for a variety of complex problems. Especially, embedded system applications rely more and more on the utilization of convolutional NNs to provide services such as image/audio classification and object detection. The core arithmetic computation performed during NN inference is the multiply-accumulate (MAC) operation. In order to meet tighter and tighter throughput constraints, NN accelerators integrate thousands of MAC units resulting in a significant increase in power consumption. Approximate computing is established as a design alternative to improve the efficiency of computing systems by trading computational accuracy for high energy savings. In this work, we bring approximate computing principles and NN inference together by designing NN specific approximate multipliers that feature multiple accuracy levels at run-time. We propose a time-efficient automated framework for mapping the NN weights to the accuracy levels of the approximate reconfigurable accelerator. The proposed weight-oriented approximation mapping is able to satisfy tight accuracy loss thresholds, while significantly reducing energy consumption without any need for intensive NN retraining. Our approach is evaluated against several NNs demonstrating that it delivers high energy savings (17.8% on average) with a minimal loss in inference accuracy (0.5%).",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "66697657",
                        "name": "Zois-Gerasimos Tasoulas"
                    },
                    {
                        "authorId": "50481255",
                        "name": "Georgios Zervakis"
                    },
                    {
                        "authorId": "145614289",
                        "name": "Iraklis Anagnostopoulos"
                    },
                    {
                        "authorId": "1894736",
                        "name": "H. Amrouch"
                    },
                    {
                        "authorId": "144271439",
                        "name": "J. Henkel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[18] show that, in other settings, IMP subnetworks rewound early in training reach the same accuracies at the same sparsities as subnetworks found by this standard pruning procedure.",
                "Winning tickets have not been found in larger-scale settings, including transformers [23, 17] and LSTMs [17, 18] for NLP tasks.",
                "Although the lottery ticket hypothesis has been evaluated in the context of NLP [17, 18] and transformers [17, 23], it remains poorly understood in the context of pre-trained BERT models.",
                "The lottery ticket hypothesis in NLP. Previous work has found that matching subnetworks exist early in training on Transformers and LSTMs [17, 18] but not at initialization [23].",
                "Practically speaking, this would allow us to replace a pre-trained BERT with a smaller subnetwork while retaining the capabilities that make it so popular for NLP work.",
                "We use standard hyperparameters for several downstream NLP tasks as shown in Table 1.",
                "\u2022 Unlike previous work in NLP, we find these subnetworks at (pre-trained) initialization rather after some amount of training.",
                "We conclude that the lottery ticket observations from other computer vision and NLP settings extend to BERT models with a pre-trained initialization.",
                "Although many ideas from this literature have been applied to Transformer models for NLP, compression ratios are typically lower than in computer vision (e.g., 2x vs. 5x in [23]).",
                "These trends have been especially pronounced in natural language processing (NLP), where massive BERT models\u2014built on the Transformer architecture [5] and pre-trained in a selfsupervised fashion\u2014have become the standard starting point for a variety of downstream tasks [6].",
                ", using At ) and pruning individual weights with the lowest-magnitudes globally throughout the network [41, 18].",
                "To do so, we adopt a strategy in which we iteratively prune the 10% of lowestmagnitude weights and train the network for a further t iterations from there (without any rewinding) until we have reached the target sparsity [41, 40, 18].",
                "Although the lottery ticket hypothesis has been evaluated in the context of NLP [17, 18] and transformers [17, 23], it remains poorly understood in the context of pre-trained BERT models.2 To address this gap in the literature, we investigate how the transformer architecture and the initialization resulting from the lengthy BERT pre-training regime behave in comparison to existing lottery ticket results.",
                "The existence of winning tickets here implies that the BERT pre-trained initialization has different properties than other NLP settings with random initializations.",
                "Previous work has found that matching subnetworks exist early in training on Transformers and LSTMs [17, 18] but not at initialization [23].",
                "Claim 4: When matching subnetworks are found, they reach the same accuracies at the same sparsities as subnetworks found using standard pruning [18].",
                "As pre-training becomes increasingly central in NLP and other areas of deep learning [7, 8], our results demonstrate that the lottery ticket observations\u2014and the tantalizing possibility that we can train smaller networks from the beginning\u2014hold for the exemplar of this class of learning algorithms."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "389036b1366b64579725457993c1f63a4f3370ba",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-12223",
                    "MAG": "3104263050",
                    "ArXiv": "2007.12223",
                    "CorpusId": 220768628
                },
                "corpusId": 220768628,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/389036b1366b64579725457993c1f63a4f3370ba",
                "title": "The Lottery Ticket Hypothesis for Pre-trained BERT Networks",
                "abstract": "In natural language processing (NLP), enormous pre-trained models like BERT have become the standard starting point for training on a range of downstream tasks, and similar trends are emerging in other areas of deep learning. In parallel, work on the lottery ticket hypothesis has shown that models for NLP and computer vision contain smaller matching subnetworks capable of training in isolation to full accuracy and transferring to other tasks. In this work, we combine these observations to assess whether such trainable, transferrable subnetworks exist in pre-trained BERT models. For a range of downstream tasks, we indeed find matching subnetworks at 40% to 90% sparsity. We find these subnetworks at (pre-trained) initialization, a deviation from prior NLP research where they emerge only after some amount of training. Subnetworks found on the masked language modeling task (the same task used to pre-train the model) transfer universally; those found on other tasks transfer in a limited fashion if at all. As large-scale pre-training becomes an increasingly central paradigm in deep learning, our results demonstrate that the main lottery ticket observations remain relevant in this context. Codes available at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2648459",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "3307026",
                        "name": "Shiyu Chang"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "37873860",
                        "name": "Yang Zhang"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "1701041",
                        "name": "Michael Carbin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In a similar setting, as demonstrated in the model pruning literature [7, 10, 26, 33, 38], having different pruning ratios for different layers of the network can further improve results over a single ratio across layers."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a07fc48b62c098977af6988aecf046de73712850",
                "externalIds": {
                    "MAG": "3045204521",
                    "DBLP": "journals/corr/abs-2007-11752",
                    "CorpusId": 220713375
                },
                "corpusId": 220713375,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a07fc48b62c098977af6988aecf046de73712850",
                "title": "PareCO: Pareto-aware Channel Optimization for Slimmable Neural Networks",
                "abstract": "Slimmable neural networks provide a flexible trade-off front between prediction error and computational cost (such as the number of floating-point operations or FLOPs) with the same storage cost as a single model. They have been proposed recently for resource-constrained settings such as mobile devices. However, current slimmable neural networks use a single width-multiplier for all the layers to arrive at sub-networks with different performance profiles, which neglects that different layers affect the network's prediction accuracy differently and have different FLOP requirements. Hence, developing a principled approach for deciding width-multipliers across different layers could potentially improve the performance of slimmable networks. To allow for heterogeneous width-multipliers across different layers, we formulate the problem of optimizing slimmable networks from a multi-objective optimization lens, which leads to a novel algorithm for optimizing both the shared weights and the width-multipliers for the sub-networks. We perform extensive empirical analysis with 14 network and dataset combinations and find that less over-parameterized networks benefit more from a joint channel and weight optimization than extremely over-parameterized networks. Quantitatively, improvements up to 1.7% and 1% in top-1 accuracy on the ImageNet dataset can be attained for MobileNetV2 and MobileNetV3, respectively. Our results highlight the potential of optimizing the channel counts for different layers jointly with the weights for slimmable networks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144774026",
                        "name": "Ting-Wu Chin"
                    },
                    {
                        "authorId": "4690624",
                        "name": "Ari S. Morcos"
                    },
                    {
                        "authorId": "1704073",
                        "name": "Diana Marculescu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following works try to extend [39], theoretically prove [34], understand [53], and improve the training process [40] of LTH."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "db373bb8b75d74baef533e58c3f3d08b0aa20b3f",
                "externalIds": {
                    "DBLP": "conf/cvpr/Li0D0GGT21",
                    "ArXiv": "2006.16242",
                    "DOI": "10.1109/CVPR46437.2021.00218",
                    "CorpusId": 235185353
                },
                "corpusId": 235185353,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/db373bb8b75d74baef533e58c3f3d08b0aa20b3f",
                "title": "The Heterogeneity Hypothesis: Finding Layer-Wise Differentiated Network Architectures",
                "abstract": "In this paper, we tackle the problem of convolutional neural network design. Instead of focusing on the design of the overall architecture, we investigate a design space that is usually overlooked, i.e. adjusting the channel configurations of predefined networks. We find that this adjustment can be achieved by shrinking widened baseline networks and leads to superior performance. Based on that, we articulate the \"heterogeneity hypothesis\": with the same training protocol, there exists a layer-wise differentiated net-work architecture (LW-DNA) that can outperform the original network with regular channel configurations but with a lower level of model complexity.The LW-DNA models are identified without extra computational cost or training time compared with the original network. This constraint leads to controlled experiments which direct the focus to the importance of layer-wise specific channel configurations. LW-DNA models come with advantages related to overfitting, i.e. the relative relationship between model complexity and dataset size. Experiments are conducted on various networks and datasets for image classification, visual tracking and image restoration. The resultant LW-DNA models consistently outperform the baseline models. Code is available at https://github.com/ofsoundof/Heterogeneity_Hypothesis.git.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2110414511",
                        "name": "Yawei Li"
                    },
                    {
                        "authorId": "98280253",
                        "name": "Wen Li"
                    },
                    {
                        "authorId": "2488938",
                        "name": "Martin Danelljan"
                    },
                    {
                        "authorId": "144110274",
                        "name": "K. Zhang"
                    },
                    {
                        "authorId": "2476317",
                        "name": "Shuhang Gu"
                    },
                    {
                        "authorId": "1681236",
                        "name": "L. Gool"
                    },
                    {
                        "authorId": "1732855",
                        "name": "R. Timofte"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "96743ec8130288a1c0edd2b6fdc0b56d7e1f7407",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-15741",
                    "ArXiv": "2006.15741",
                    "MAG": "3037960716",
                    "DOI": "10.1109/DSLW51110.2021.9523404",
                    "CorpusId": 220250886
                },
                "corpusId": 220250886,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/96743ec8130288a1c0edd2b6fdc0b56d7e1f7407",
                "title": "ESPN: Extremely Sparse Pruned Networks",
                "abstract": "Deep neural networks are often highly over-parameterized, prohibiting their use in compute-limited systems. However, a line of recent works has shown that the size of deep networks can be considerably reduced by identifying a subset of neuron indicators (or mask) that correspond to significant weights prior to training. We demonstrate that a simple iterative mask discovery method can achieve state-of-the-art compression of very deep networks. Our algorithm represents a hybrid approach between single-shot network pruning methods (such as SNIP) with Lottery-Ticket type approaches. We validate our approach on several datasets and outperform several existing pruning approaches in both test accuracy and compression ratio.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "72643925",
                        "name": "Minsu Cho"
                    },
                    {
                        "authorId": "144931667",
                        "name": "Ameya Joshi"
                    },
                    {
                        "authorId": "144398138",
                        "name": "C. Hegde"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, state-of-the-art unstructured pruning methods typically rely on Magnitude Pruning (MP) (Han et al., 2015), a simple and computationally cheap criterion based on weight magnitude, that works extremely well in practice (Renda et al., 2020).",
                ", 2015), a simple and computationally cheap criterion based on weight magnitude, that works extremely well in practice (Renda et al., 2020).",
                "While the fine tuning-phase would require hyper-parameters optimisation, Renda et al. (2020) showed that using the same ones as for the original training usually leads to good results.",
                "The later is typically used in the literature (Zeng & Urtasun, 2019; Wang et al., 2019; Frankle & Carbin, 2018; Renda et al., 2020).",
                ", 2019), and is used in current state-of-the-art methods (Renda et al., 2020).",
                ", 2019), or in an iterative training/fine-tuning fashion (LeCun et al., 1990; Hassibi & Stork, 1993; Han et al., 2015; Frankle & Carbin, 2018; Renda et al., 2020).",
                "\u2026Pruning (MP) (Han et al., 2015), is a popular pruning criterion in which the saliency is simply based on the norm of the parameter:\nsMPk = \u03b8 2 k (4)\nDespite its simplicity, MP works extremely well in practice (Gale et al., 2019), and is used in current state-of-the-art methods (Renda et al., 2020).",
                "We hypothesize that this could be one of the reasons behind the success of the Lottery Ticket and Rewinding experiments (Frankle & Carbin, 2018; Frankle et al., 2019; Renda et al., 2020).",
                "\u2026before training (Lee et al., 2019b; Wang et al., 2020), during training (Louizos et al., 2017; Molchanov et al., 2017; Ding et al., 2019), or in an iterative training/fine-tuning fashion (LeCun et al., 1990; Hassibi & Stork, 1993; Han et al., 2015; Frankle & Carbin, 2018; Renda et al., 2020)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "bae557f07ddca6ac7b0fb2f28dac914f40fbe1fe",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-12279",
                    "ArXiv": "2006.12279",
                    "MAG": "3036340313",
                    "CorpusId": 219966566
                },
                "corpusId": 219966566,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bae557f07ddca6ac7b0fb2f28dac914f40fbe1fe",
                "title": "Revisiting Loss Modelling for Unstructured Pruning",
                "abstract": "By removing parameters from deep neural networks, unstructured pruning methods aim at cutting down memory footprint and computational cost, while maintaining prediction accuracy. In order to tackle this otherwise intractable problem, many of these methods model the loss landscape using first or second order Taylor expansions to identify which parameters can be discarded. We revisit loss modelling for unstructured pruning: we show the importance of ensuring locality of the pruning steps. We systematically compare first and second order Taylor expansions and empirically show that both can reach similar levels of performance. Finally, we show that better preserving the original network function does not necessarily transfer to better performing networks after fine-tuning, suggesting that only considering the impact of pruning on the loss might not be a sufficient objective to design good pruning criteria.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "40201308",
                        "name": "C\u00e9sar Laurent"
                    },
                    {
                        "authorId": "1414808684",
                        "name": "Camille Ballas"
                    },
                    {
                        "authorId": "49917441",
                        "name": "Thomas George"
                    },
                    {
                        "authorId": "2482072",
                        "name": "Nicolas Ballas"
                    },
                    {
                        "authorId": "120247189",
                        "name": "Pascal Vincent"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a6b499136aa70dfad66fca5efaaaead5c616d59b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-11487",
                    "ArXiv": "2006.11487",
                    "MAG": "3035804116",
                    "CorpusId": 219966170
                },
                "corpusId": 219966170,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/a6b499136aa70dfad66fca5efaaaead5c616d59b",
                "title": "Paying more Attention to Snapshots of Iterative Pruning: Improving Model Compression via Ensemble Distillation",
                "abstract": "Network pruning is one of the most dominant methods for reducing the heavy inference cost of deep neural networks. Existing methods often iteratively prune networks to attain high compression ratio without incurring significant loss in performance. However, we argue that conventional methods for retraining pruned networks (i.e., using small, fixed learning rate) are inadequate as they completely ignore the benefits from snapshots of iterative pruning. In this work, we show that strong ensembles can be constructed from snapshots of iterative pruning, which achieve competitive performance and vary in network structure. Furthermore, we present simple, general and effective pipeline that generates strong ensembles of networks during pruning with large learning rate restarting, and utilizes knowledge distillation with those ensembles to improve the predictive power of compact models. In standard image classification benchmarks such as CIFAR and Tiny-Imagenet, we advance state-of-the-art pruning ratio of structured pruning by integrating simple l1-norm filters pruning into our pipeline. Specifically, we reduce 75-80% of total parameters and 65-70% MACs of numerous variants of ResNet architectures while having comparable or better performance than that of original networks. Code associate with this paper is made publicly available at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2059393417",
                        "name": "Duong H. Le"
                    },
                    {
                        "authorId": "1753622419",
                        "name": "Vo Trung Nhan"
                    },
                    {
                        "authorId": "145189320",
                        "name": "N. Thoai"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2015) that gets stateof-the-art tradeoffs between error and unstructured density (Gale et al., 2019; Renda et al., 2020).",
                "This method is a standard way to prune (Han et al., 2015) that gets stateof-the-art tradeoffs between error and unstructured density (Gale et al., 2019; Renda et al., 2020).",
                "For IMP, we use a practice called weight rewinding (Frankle et al., 2020; Renda et al., 2020), in which the values of unpruned weights are rewound to their values earlier in training (in our case, epoch 10) and the training process is repeated from there to completion.",
                ", 2020) to 5x (Renda et al., 2020) with no increase in error.",
                "In practice, pruning can reduce the parameter-counts of contemporary models by 2x (Gordon et al., 2020) to 5x (Renda et al., 2020) with no increase in error.",
                "We use per-weight magnitude pruning because it is generic, well-studied (Han et al., 2015), and produces stateof-the-art tradeoffs between density and error (Gale et al., 2019; Blalock et al., 2020; Renda et al., 2020).",
                ", 2015), and produces stateof-the-art tradeoffs between density and error (Gale et al., 2019; Blalock et al., 2020; Renda et al., 2020)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6ea7201aad5d146ba481051d26b884d19a34af15",
                "externalIds": {
                    "MAG": "3036991069",
                    "DBLP": "conf/icml/RosenfeldFCS21",
                    "ArXiv": "2006.10621",
                    "CorpusId": 219792934
                },
                "corpusId": 219792934,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6ea7201aad5d146ba481051d26b884d19a34af15",
                "title": "On the Predictability of Pruning Across Scales",
                "abstract": "We show that the error of iteratively-pruned networks empirically follows a scaling law with interpretable coefficients that depend on the architecture and task. We functionally approximate the error of the pruned networks, showing that it is predictable in terms of an invariant tying width, depth, and pruning level, such that networks of vastly different sparsities are freely interchangeable. We demonstrate the accuracy of this functional approximation over scales spanning orders of magnitude in depth, width, dataset size, and sparsity. We show that the scaling law functional form holds (generalizes) for large scale data (CIFAR-10, ImageNet), architectures (ResNets, VGGs) and iterative pruning algorithms (IMP, SynFlow). As neural networks become ever larger and more expensive to train, our findings suggest a framework for reasoning conceptually and analytically about pruning.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1380165568",
                        "name": "Jonathan S. Rosenfeld"
                    },
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "1701041",
                        "name": "Michael Carbin"
                    },
                    {
                        "authorId": "2613669",
                        "name": "N. Shavit"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6cf55ba562dc29be78c55039c6c79c014c45ae83",
                "externalIds": {
                    "DBLP": "journals/sncs/JohnMR20",
                    "MAG": "3036668951",
                    "DOI": "10.1007/s42979-020-00208-w",
                    "CorpusId": 220794675
                },
                "corpusId": 220794675,
                "publicationVenue": {
                    "id": "7a7dc89b-e1a6-44df-a496-46c330a87840",
                    "name": "SN Computer Science",
                    "type": "journal",
                    "alternate_names": [
                        "SN Comput Sci"
                    ],
                    "issn": "2661-8907",
                    "alternate_issns": [
                        "2662-995X"
                    ],
                    "url": "https://link.springer.com/journal/42979"
                },
                "url": "https://www.semanticscholar.org/paper/6cf55ba562dc29be78c55039c6c79c014c45ae83",
                "title": "Retraining a Pruned Network: A Unified Theory of Time Complexity",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1434558384",
                        "name": "Soumya Sara John"
                    },
                    {
                        "authorId": "145848801",
                        "name": "Deepak Mishra"
                    },
                    {
                        "authorId": "2209774116",
                        "name": "Sheeba Rani Johnson"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "97bac618fc866ae7656660f3965e9aae37993232",
                "externalIds": {
                    "MAG": "3037261029",
                    "DBLP": "series/synthesis/2020Sze",
                    "DOI": "10.2200/s01004ed1v01y202004cac050",
                    "CorpusId": 211266813
                },
                "corpusId": 211266813,
                "publicationVenue": {
                    "id": "3c9d8c2a-178a-4468-a24c-796c7c5bfd03",
                    "name": "Synthesis Lectures on Computer Architecture",
                    "type": "journal",
                    "alternate_names": [
                        "Synth Lect Comput Archit"
                    ],
                    "issn": "1935-3235",
                    "url": "https://www.morganclaypool.com/toc/cac/1/1"
                },
                "url": "https://www.semanticscholar.org/paper/97bac618fc866ae7656660f3965e9aae37993232",
                "title": "Efficient Processing of Deep Neural Networks",
                "abstract": "Abstract This book provides a structured treatment of the key principles and techniques for enabling efficient processing of deep neural networks (DNNs). DNNs are currently widely used for many art...",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1691305",
                        "name": "V. Sze"
                    },
                    {
                        "authorId": "50579876",
                        "name": "Yu-hsin Chen"
                    },
                    {
                        "authorId": "1950815",
                        "name": "Tien-Ju Yang"
                    },
                    {
                        "authorId": "1775477",
                        "name": "J. Emer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent works have focused on pruning methods that include the pruning process in the training phase [28, 32, 25, 22]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f8956c1225fe44eb61440cdf3a0796717a5f344b",
                "externalIds": {
                    "ArXiv": "2005.11035",
                    "MAG": "3028224987",
                    "DBLP": "journals/corr/abs-2005-11035",
                    "CorpusId": 218862856
                },
                "corpusId": 218862856,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f8956c1225fe44eb61440cdf3a0796717a5f344b",
                "title": "Position-based Scaled Gradient for Model Quantization and Sparse Training",
                "abstract": "We propose the position-based scaled gradient (PSG) that scales the gradient depending on the position of a weight vector to make it more compression-friendly. First, we theoretically show that applying PSG to the standard gradient descent (GD), which is called PSGD, is equivalent to the GD in the warped weight space, a space made by warping the original weight space via an appropriately designed invertible function. Second, we empirically show that PSG acting as a regularizer to a weight vector is very useful in model compression domains such as quantization and sparse training. PSG reduces the gap between the weight distributions of a full-precision model and its compressed counterpart. This enables the versatile deployment of a model either as an uncompressed mode or as a compressed mode depending on the availability of resources. The experimental results on CIFAR-10/100 and Imagenet datasets show the effectiveness of the proposed PSG in both domains of sparse training and quantization even for extremely low bits.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "49476045",
                        "name": "Jangho Kim"
                    },
                    {
                        "authorId": "1713608836",
                        "name": "Kiyoon Yoo"
                    },
                    {
                        "authorId": "3160425",
                        "name": "Nojun Kwak"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent works have focused on pruning methods that include the pruning process in the training phase [35, 42, 30, 27]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "836562bc282cfcfd6856de072a1204fc2e467b91",
                "externalIds": {
                    "MAG": "3102934511",
                    "DBLP": "conf/nips/KimYK20",
                    "CorpusId": 226299742
                },
                "corpusId": 226299742,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/836562bc282cfcfd6856de072a1204fc2e467b91",
                "title": "Position-based Scaled Gradient for Model Quantization and Pruning",
                "abstract": "We propose the position-based scaled gradient (PSG) that scales the gradient depending on the position of a weight vector to make it more compression-friendly. First, we theoretically show that applying PSG to the standard gradient descent (GD), which is called PSGD, is equivalent to the GD in the warped weight space, a space made by warping the original weight space via an appropriately designed invertible function. Second, we empirically show that PSG acting as a regularizer to a weight vector is favorable for model compression domains such as quantization and pruning. PSG reduces the gap between the weight distributions of a full-precision model and its compressed counterpart. This enables the versatile deployment of a model either as an uncompressed mode or as a compressed mode depending on the availability of resources. The experimental results on CIFAR-10/100 and ImageNet datasets show the effectiveness of the proposed PSG in both domains of pruning and quantization even for extremely low bits. The code is released in Github.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "49476045",
                        "name": "Jangho Kim"
                    },
                    {
                        "authorId": "1713608836",
                        "name": "Kiyoon Yoo"
                    },
                    {
                        "authorId": "3160425",
                        "name": "Nojun Kwak"
                    }
                ]
            }
        },
        {
            "contexts": [
                "They also argue that using the initial weights values is fundamental to achieve competitive performance, which is degraded when starting from a random initialization [18]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "97bf4fba885aaa32d40371b643ad838dda06c09c",
                "externalIds": {
                    "MAG": "3096276501",
                    "DBLP": "conf/icann/TartaglioneBG20",
                    "ArXiv": "2004.14765",
                    "DOI": "10.1007/978-3-030-61616-8_6",
                    "CorpusId": 216869727
                },
                "corpusId": 216869727,
                "publicationVenue": {
                    "id": "3e64b1c1-745f-4edf-bd92-b8ef122bb49c",
                    "name": "International Conference on Artificial Neural Networks",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Artif Neural Netw",
                        "ICANN"
                    ],
                    "url": "http://www.e-nns.org/"
                },
                "url": "https://www.semanticscholar.org/paper/97bf4fba885aaa32d40371b643ad838dda06c09c",
                "title": "Pruning artificial neural networks: a way to find well-generalizing, high-entropy sharp minima",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "47376816",
                        "name": "Enzo Tartaglione"
                    },
                    {
                        "authorId": "16248506",
                        "name": "Andrea Bragagnolo"
                    },
                    {
                        "authorId": "1691141",
                        "name": "Marco Grangetto"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2ddcd0db339457c9a67207ecdce155d96c24c0b0",
                "externalIds": {
                    "MAG": "3018249510",
                    "DOI": "10.1117/12.2558606",
                    "CorpusId": 219095654
                },
                "corpusId": 219095654,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2ddcd0db339457c9a67207ecdce155d96c24c0b0",
                "title": "Underwater exploration by AUV using deep neural network implemented on FPGA",
                "abstract": "Performing underwater exploration with Autonomous Underwater Vehicles (AUV) requires low power and high resolution techniques. New computer vision techniques can be used for underwater image classification on embedded devices. These techniques must face machine resource constraints to offer high performance and low power consumption. This paper presents how to implement a Deep Neural Network (DNN) on Field Programmable Gate Array (FPGA) to perform underwater exploration with an AUV. We introduce tools and methodology to adapt the technology to the underwater context. This paper is part of a work to create an embedded system that can fit into an AUV to perform real time analysis of the underwater environment (using video camera as main sensor) with high autonomy and endurance. This will be achieved by overcoming underwater exploration challenges as : low power consumption, high classification performance, shortage of high-quality labeled data to train algorithm.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "148214381",
                        "name": "T. Le Pennec"
                    },
                    {
                        "authorId": "2382007",
                        "name": "M. Jridi"
                    },
                    {
                        "authorId": "2321702",
                        "name": "C. Dezan"
                    },
                    {
                        "authorId": "2830224",
                        "name": "A. Alfalou"
                    },
                    {
                        "authorId": "97920866",
                        "name": "F. Florin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[11, 19, 27] pruned the least important weights in the network."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "39f8cc684f09ea2b43767f5b9590896774802759",
                "externalIds": {
                    "ArXiv": "2004.03844",
                    "DBLP": "journals/csl/SajjadDDN23",
                    "DOI": "10.1016/j.csl.2022.101429",
                    "CorpusId": 251005814
                },
                "corpusId": 251005814,
                "publicationVenue": {
                    "id": "44548551-68db-44cc-a7bf-6908b58fd313",
                    "name": "Computer Speech and Language",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Speech Lang",
                        "Comput Speech  Lang",
                        "Computer Speech & Language"
                    ],
                    "issn": "0885-2308",
                    "url": "https://www.journals.elsevier.com/computer-speech-and-language",
                    "alternate_urls": [
                        "http://www.idealibrary.com/",
                        "http://www.sciencedirect.com/science/journal/08852308"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/39f8cc684f09ea2b43767f5b9590896774802759",
                "title": "On the effect of dropping layers of pre-trained transformer models",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145775792",
                        "name": "Hassan Sajjad"
                    },
                    {
                        "authorId": "6415321",
                        "name": "Fahim Dalvi"
                    },
                    {
                        "authorId": "145938140",
                        "name": "Nadir Durrani"
                    },
                    {
                        "authorId": "1683562",
                        "name": "Preslav Nakov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This resulted in a renewed interest in sparse deep learning and model pruning (Renda et al., 2020; Chen et al., 2020; 2021) and more recently in the area of sparse reinforcement learning (Arnob et al., 2021; Sokar et al., 2021).",
                "This resulted in a renewed interest in sparse deep learning and model pruning (Renda et al., 2020; Chen et al., 2020; 2021) and more recently in the area of sparse reinforcement learning (Arnob et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "63a63f5463112d02aa3147de74228da9dc945171",
                "externalIds": {
                    "ArXiv": "1912.03896",
                    "DBLP": "journals/tmlr/OhibGDSPP22",
                    "CorpusId": 247011560
                },
                "corpusId": 247011560,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/63a63f5463112d02aa3147de74228da9dc945171",
                "title": "Explicit Group Sparse Projection with Applications to Deep Learning and NMF",
                "abstract": "We design a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure (an affine function of the ratio of the $\\ell_1$ and $\\ell_2$ norms). Existing approaches either project each vector individually or require the use of a regularization parameter which implicitly maps to the average $\\ell_0$-measure of sparsity. Instead, in our approach we set the sparsity level for the whole set explicitly and simultaneously project a group of vectors with the sparsity level of each vector tuned automatically. We show that the computational complexity of our projection operator is linear in the size of the problem. Additionally, we propose a generalization of this projection by replacing the $\\ell_1$ norm by its weighted version. We showcase the efficacy of our approach in both supervised and unsupervised learning tasks on image datasets including CIFAR10 and ImageNet. In deep neural network pruning, the sparse models produced by our method on ResNet50 have significantly higher accuracies at corresponding sparsity values compared to existing competitors. In nonnegative matrix factorization, our approach yields competitive reconstruction errors against state-of-the-art algorithms.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "31048326",
                        "name": "Riyasat Ohib"
                    },
                    {
                        "authorId": "2039115",
                        "name": "Nicolas Gillis"
                    },
                    {
                        "authorId": "2207655",
                        "name": "Niccol\u00f2 Dalmasso"
                    },
                    {
                        "authorId": "36532736",
                        "name": "Sameena Shah"
                    },
                    {
                        "authorId": "2330666",
                        "name": "V. Potluru"
                    },
                    {
                        "authorId": "2122479",
                        "name": "S. Plis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The comparisons in Table 2 further show that inheriting weights from the EB tickets favor the generalization of retraining as compared to both the random initialization and \u201cover-cooked\u201d weights, aligning well with the recent discussion between rewinding and fine-tuning (Renda et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "336868be817536e7c7fc88c391a2860cd869ea2b",
                "externalIds": {
                    "DBLP": "conf/iclr/YouL0FWCBWL20",
                    "MAG": "2995197005",
                    "ArXiv": "1909.11957",
                    "CorpusId": 202888885
                },
                "corpusId": 202888885,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/336868be817536e7c7fc88c391a2860cd869ea2b",
                "title": "Drawing early-bird tickets: Towards more efficient training of deep networks",
                "abstract": "(Frankle & Carbin, 2019) shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve comparable accuracies to the latter in a similar number of iterations. However, the identification of these winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits. In this paper, we discover for the first time that the winning tickets can be identified at the very early training stage, which we term as early-bird (EB) tickets, via low-cost training schemes (e.g., early stopping and low-precision training) at large learning rates. Our finding of EB tickets is consistent with recently reported observations that the key connectivity patterns of neural networks emerge early. Furthermore, we propose a mask distance metric that can be used to identify EB tickets with low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, we leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low-cost schemes, and then continuing to train merely the EB tickets towards the target accuracy. Experiments based on various deep networks and datasets validate: 1) the existence of EB tickets, and the effectiveness of mask distance in efficiently identifying them; and 2) that the proposed efficient training via EB tickets can achieve up to 4.7x energy savings while maintaining comparable or even better accuracy, demonstrating a promising and easily adopted method for tackling cost-prohibitive deep network training.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "47113848",
                        "name": "Haoran You"
                    },
                    {
                        "authorId": "28987646",
                        "name": "Chaojian Li"
                    },
                    {
                        "authorId": "2153916160",
                        "name": "Pengfei Xu"
                    },
                    {
                        "authorId": "108145103",
                        "name": "Y. Fu"
                    },
                    {
                        "authorId": "2118461722",
                        "name": "Yue Wang"
                    },
                    {
                        "authorId": "66273769",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "3138925",
                        "name": "Yingyan Lin"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "144908066",
                        "name": "Richard Baraniuk"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We implemented structured pruning and LR factorization based on the original papers \u2013 (Renda et al., 2020) for pruning and (Tai et al., 2016) for LR factorization.",
                "ApproxCaliper currently supports two existing approximation techniques (which can be applied in combination): (1) structured pruning based on Learning Rate Rewinding (LRR) (Renda et al., 2020) and (2) low-rank factorization (or LR factorization) based on (Tai et al., 2016).",
                "We implemented structured pruning and LR factorization based on the original papers \u2013 (Renda et al., 2020) for pruning and (Tai et al.",
                "As baselines for comparison, we apply pruning using LRR (Renda et al., 2020) or LR factorization using the technique in (Tai et al.",
                "As baselines for comparison, we apply pruning using LRR (Renda et al., 2020) or LR factorization using the technique in (Tai et al., 2016) to all the candidate neural network architectures with the constraint to retain the same accuracy as the original model, and pick the most efficient pruned\u2026",
                "ApproxCaliper currently supports two existing approximation techniques (which can be applied in combination): (1) structured pruning based on Learning Rate Rewinding (LRR) (Renda et al., 2020) and (2) low-rank factorization (or LR factorization) based on (Tai et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5edcec05425895ac2320ac8b969a19ad780f1358",
                "externalIds": {
                    "CorpusId": 258559887
                },
                "corpusId": 258559887,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5edcec05425895ac2320ac8b969a19ad780f1358",
                "title": "A PPROX C ALIPER : A P ROGRAMMABLE F RAMEWORK FOR A PPLICATION - AWARE N EURAL N ETWORK O PTIMIZATION",
                "abstract": "To deploy compute-intensive neural networks on resource-constrained edge systems, developers use model optimization techniques that reduce model size and computational cost. Existing optimization tools are application-agnostic \u2013 they optimize model parameters solely in view of the neural network accuracy \u2013 and can thus miss optimization opportunities. We propose ApproxCaliper , the first programmable framework for application-aware neural network optimization . By incorporating application-specific goals, ApproxCaliper facilitates more aggressive optimization of the neural networks compared to application-agnostic techniques. We perform experiments on five different neural networks used in two real-world robotics systems: a commercial agriculture robot and a simulation of an autonomous electric cart. Compared to Learning Rate Rewinding (LRR), a state-of-the-art structured pruning tool used in an application agnostic setting, ApproxCaliper achieves 5.3 \u00d7 higher speedup and 2.9 \u00d7 lower GPU resource utilization, and 36 \u00d7 and 6.1 \u00d7 additional model size reduction for the two evaluated benchmarks, respectively.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2124210780",
                        "name": "Yifan Zhao"
                    },
                    {
                        "authorId": "36829056",
                        "name": "Hashim Sharif"
                    },
                    {
                        "authorId": "2213559458",
                        "name": "Peter Pao-Huang"
                    },
                    {
                        "authorId": "2216574662",
                        "name": "Vatsin Ninad Shah"
                    },
                    {
                        "authorId": "1807483525",
                        "name": "A. N. Sivakumar"
                    },
                    {
                        "authorId": "116528679",
                        "name": "M. V. Gasparino"
                    },
                    {
                        "authorId": "144510873",
                        "name": "Abdulrahman Mahmoud"
                    },
                    {
                        "authorId": "2051156747",
                        "name": "Nathan Zhao"
                    },
                    {
                        "authorId": "3196444",
                        "name": "S. Adve"
                    },
                    {
                        "authorId": "1733356",
                        "name": "Girish V. Chowdhary"
                    },
                    {
                        "authorId": "1704478",
                        "name": "Sasa Misailovic"
                    },
                    {
                        "authorId": "1720525",
                        "name": "Vikram S. Adve"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7c1e69a8889aacd80a504bebbe9f3860e5a5f77a",
                "externalIds": {
                    "DBLP": "journals/access/ZullichMPP23",
                    "DOI": "10.1109/ACCESS.2023.3236502",
                    "CorpusId": 255927356
                },
                "corpusId": 255927356,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7c1e69a8889aacd80a504bebbe9f3860e5a5f77a",
                "title": "An Artificial Intelligence System for Automatic Recognition of Punches in Fourteenth-Century Panel Painting",
                "abstract": "In Late-Medieval panel paintings from the Tuscan area, mechanical tools called punches were used to impress repeated motifs on gold foils to create decorative patterns. Such patterns can be used as clues to objectively support the attribution of the paintings, as proposed by art historian Erling S. Skaug in his decades-long study on punches. We investigate the feasibility of employing automatic pattern recognition techniques for accelerating the process of classification of punches by experts working in the field. We propose a system composed of (a) a Convolutional Neural Network for categorizing a punch contained in a frame, and (b) an additional component for uncertainty estimation, aimed at recognizing possible Out-of-Distribution (OOD) samples. After collecting a set of 14th century panel paintings from Tuscany, we train a Convolutional Neural Network which achieves very high test-set accuracy. As far as the uncertainty estimation is concerned, we experiment with two techniques, OpenGAN and II-loss, both exhibiting very positive results. The former seems to work better on specific data extracted from images of panel paintings, while the latter showcases a more consistent behavior when considering additional OOD data obtained randomly. These outcomes indicate that an application of our system in support of experts is feasible, although we subsequently show that additional experiments on larger datasets might be required.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1638341256",
                        "name": "Marco Zullich"
                    },
                    {
                        "authorId": "2201241764",
                        "name": "Vanja Macovaz"
                    },
                    {
                        "authorId": "2201248546",
                        "name": "Giovanni Pinna"
                    },
                    {
                        "authorId": "2598889",
                        "name": "F. A. Pellegrino"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7617f49e3178c3dbeaa8ce55ab9f08eaa50f71e6",
                "externalIds": {
                    "CorpusId": 256827947
                },
                "corpusId": 256827947,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7617f49e3178c3dbeaa8ce55ab9f08eaa50f71e6",
                "title": "Regularization Compression Collapse Performance Sparsity Performance Sparsity Performance Sparsity",
                "abstract": "In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may underprune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a limit that appears to correspond to the beginning of underfitting. Subsequently, PQI decreases again when the model collapse and significant deterioration in the performance of the model start to occur. Additionally, our experiments demonstrate that the proposed adaptive pruning algorithm with proper choice of hyper-parameters is superior to the iterative pruning algorithms such as the lottery ticket-based pruning methods, in terms of both compression efficiency and robustness. Our code is available here.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51298945",
                        "name": "Enmao Diao"
                    },
                    {
                        "authorId": "2096527",
                        "name": "G. Wang"
                    },
                    {
                        "authorId": "2111183794",
                        "name": "Jie Ding"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020), heads (Renda et al., 2019; Wang et al., 2020), and layers (Fan et al.",
                "\u2026methods (He et al., 2017; Molchanov et al., 2019; Guo et al., 2020) aim to search a sub-model for large-size models by pruning unimportant dimensions (McCarley et al., 2019; Prasanna et al., 2020), heads (Renda et al., 2019; Wang et al., 2020), and layers (Fan et al., 2019; Sajjad et al., 2020)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ef2f297648c120e9e7315e1f39816026432abb22",
                "externalIds": {
                    "DBLP": "conf/acl/LiangLWCZ23",
                    "ACL": "2023.acl-long.162",
                    "DOI": "10.18653/v1/2023.acl-long.162",
                    "CorpusId": 259370858
                },
                "corpusId": 259370858,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/ef2f297648c120e9e7315e1f39816026432abb22",
                "title": "Dynamic and Efficient Inference for Text Generation via BERT Family",
                "abstract": "Despite the excellent performance of Pre-trained Language Models on many text generation tasks, they suffer from inefficient inference on computation and memory due to their large-scale parameters and the universal autoregressive decoding paradigm. In this work, we propose a novel fine-tuning method DEER, which can make a single pre-trained model support Dynamic and Efficient infERence and achieve an adaptive trade-off between model performance and latency. In particular, our critical insight is to jointly utilize the non-autoregressive (NAR) generation and dynamic parameter pruning techniques, which can flexibly control the decoding iteration steps and model sizes according to memory and latency limitations. Besides, we also explore the effectiveness of the pre-trained MLMs (i.e., the BERT family) for text generation tasks since their bidirectional attention nature is more suitable for the NAR training objective. Extensive experiments on both monolingual and multilingual pre-trained MLMs demonstrate the effectiveness of our proposed DEER method by consistently achieving (1) higher BLEU scores than the strong autoregressive Transformer model on three neural machine translation tasks with 3 \\to 12 times speedup, (2) competitive performance (but with much faster inference speed) compared with the BART model on four GLGE benchmark tasks. Our code will be publicly available at GitHubhttps://github.com/dropreg/DEER.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48083523",
                        "name": "Xiaobo Liang"
                    },
                    {
                        "authorId": "2109013629",
                        "name": "Juntao Li"
                    },
                    {
                        "authorId": "47767791",
                        "name": "Lijun Wu"
                    },
                    {
                        "authorId": "2314396",
                        "name": "Ziqiang Cao"
                    },
                    {
                        "authorId": "50495870",
                        "name": "M. Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "899c7fd82d9afe38f76ffca105f2e60fe289ef09",
                "externalIds": {
                    "CorpusId": 259840476
                },
                "corpusId": 259840476,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/899c7fd82d9afe38f76ffca105f2e60fe289ef09",
                "title": "U NMASKING THE L OTTERY T ICKET H YPOTHESIS : W HAT \u2019 S E NCODED IN A W INNING T ICKET \u2019 S M ASK ?",
                "abstract": "limits the fraction of weights that can be pruned at each iteration of IMP. This analysis yields a new quantitative link between IMP performance and the Hessian eigenspectrum. Finally, we show that the role of retraining in IMP is to find a network with new small weights to prune. Overall, these results make progress toward de-mystifying the existence of winning tickets by revealing the fundamental role of error landscape geometry in the algorithms used to find them.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "152574768",
                        "name": "Brett W. Larsen"
                    },
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "25769960",
                        "name": "S. Ganguli"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent works also introduce gradual magnitude pruning which can outperform iterative pruning algorithms Gale et al. (2019); Renda et al. (2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "81f1c35ea36c9361d23c1c5e51eec62c9666417a",
                "externalIds": {
                    "CorpusId": 260547735
                },
                "corpusId": 260547735,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/81f1c35ea36c9361d23c1c5e51eec62c9666417a",
                "title": "P RUNING D EEP N EURAL N ETWORKS FROM A S PAR - SITY P ERSPECTIVE",
                "abstract": "In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may underprune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a limit that appears to correspond to the beginning of underfitting. Subsequently, PQI decreases again when the model collapse and significant deterioration in the performance of the model start to occur. Additionally, our experiments demonstrate that the proposed adaptive pruning algorithm with proper choice of hyper-parameters is superior to the iterative pruning algorithms such as the lottery ticket-based pruning methods, in terms of both compression efficiency and robustness. Our code is available here.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2228612435",
                        "name": "Enmao Diao"
                    },
                    {
                        "authorId": "2228132272",
                        "name": "Ganghua Wang"
                    },
                    {
                        "authorId": "2229243954",
                        "name": "Jie Ding"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "085e7f588aa922f82480a3c197f6dd80abe43350",
                "externalIds": {
                    "CorpusId": 261076565
                },
                "corpusId": 261076565,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/085e7f588aa922f82480a3c197f6dd80abe43350",
                "title": "P RUNING D EEP N EURAL N ETWORKS FROM A S PAR - SITY P ERSPECTIVE",
                "abstract": "In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may underprune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a limit that appears to correspond to the beginning of underfitting. Subsequently, PQI decreases again when the model collapse and significant deterioration in the performance of the model start to occur. Additionally, our experiments demonstrate that the proposed adaptive pruning algorithm with proper choice of hyper-parameters is superior to the iterative pruning algorithms such as the lottery ticket-based pruning methods, in terms of both compression efficiency and robustness. Our code is available here.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2228612435",
                        "name": "Enmao Diao"
                    },
                    {
                        "authorId": "2228132272",
                        "name": "Ganghua Wang"
                    },
                    {
                        "authorId": "2107988208",
                        "name": "Jiawei Zhang"
                    },
                    {
                        "authorId": "2233113808",
                        "name": "Yuhong Yang"
                    },
                    {
                        "authorId": "2229243954",
                        "name": "Jie Ding"
                    },
                    {
                        "authorId": "101661851",
                        "name": "Vahid Tarokh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e747dce851ce8f302814eaf1d65ac99aa82963ff",
                "externalIds": {
                    "CorpusId": 261257952
                },
                "corpusId": 261257952,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e747dce851ce8f302814eaf1d65ac99aa82963ff",
                "title": "Chasing Better Deep Image Priors between Over-and Under-parameterization",
                "abstract": ".",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2650694",
                        "name": "Qiming Wu"
                    },
                    {
                        "authorId": "2116298570",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "2235293441",
                        "name": "Yifan Jiang"
                    },
                    {
                        "authorId": "2227945855",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "acb7ee5a3fe1f4364c6b1e0c685630821d5c52dd",
                "externalIds": {
                    "DBLP": "journals/lgrs/WangWDZWZ23",
                    "DOI": "10.1109/LGRS.2023.3312677",
                    "CorpusId": 261649424
                },
                "corpusId": 261649424,
                "publicationVenue": {
                    "id": "290335d6-cddc-465d-87f1-807e86d8efee",
                    "name": "IEEE Geoscience and Remote Sensing Letters",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Geosci Remote Sens Lett"
                    ],
                    "issn": "1545-598X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=8859",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8859"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/acb7ee5a3fe1f4364c6b1e0c685630821d5c52dd",
                "title": "Highly Maneuvering Target Detection Based on Neural Network and Generalized Radon-Fourier Transform",
                "abstract": "Detection of highly maneuvering targets often suffers from the problem of range migration (RM) and Doppler frequency migration (DFM) within coherent processing interval (CPI), which results in performance degradation in coherent integration. In this letter, we propose a new coherent integration method which is based on neural networks and generalized Radon-Fourier transform (GRFT). Specifically, this method develops a neural network that can infer the target trajectory from the radar echo, and the trajectory reduces the search ranges of the GRFT method\u2019s parameters according to some judgment criteria. After that, the RM and DFM are compensated via GRFT, and thereby it improves the performance of coherent integration. Besides, we introduce a customized regularization loss into the development of the neural network, which improves the detection performance. The superiority of the proposed method over GRFT is that it reduces the computational cost significantly with similar detection performance, which is confirmed by the results of experiments.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2243357557",
                        "name": "Jiachen Wang"
                    },
                    {
                        "authorId": "2129446940",
                        "name": "Yifeng Wu"
                    },
                    {
                        "authorId": "2238893683",
                        "name": "Xiaobo Deng"
                    },
                    {
                        "authorId": "2152830118",
                        "name": "Lei Zhang"
                    },
                    {
                        "authorId": "2243329333",
                        "name": "Juan Wang"
                    },
                    {
                        "authorId": "2238714825",
                        "name": "Lichao Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026on ImageNet dataset and VGG-19 on Tiny-ImageNet dataset, comparing our method with PFB (Liebenwein et al. (2019)), HRank (Lin et al. (2020)), ILP (Renda et al. (2020)), Sparse Structure Selection (SSS) (Huang & Wang (2018)), NN Slimming (Liu et al. (2017)), and Eigendamage (Wang et al. (2019)).",
                "For example, on ResNet-56, with 1% of accuracy loss, the amount of parameter reduction achieved by our method is higher than that of ILP (Renda et al. (2020)) by 34.",
                "We compare our method with the state-of-the-art works, including SCOP (Tang et al. (2020)), HRank (Lin et al. (2020)), ILP (Renda et al. (2020)), PPR (Zhuang et al. (2020)), RFR (He et al. (2017)), GAL (Lin et al. (2019)), DCP (Zhuang et al. (2018)), GBN (You et al. (2019)), CP (He et al. (2017)),\u2026",
                "For ResNet-50 on ImageNet, the learning rate increases to 0.256 in a warmup mechanism during the first 5 epochs, and decays with a factor of 0.1 at epochs 30, 60, 80 (Renda et al. (2020); Frankle et al. (2019)).",
                "For example, on ResNet-56, with 1% of accuracy loss, the amount of parameter reduction achieved by our method is higher than that of ILP (Renda et al. (2020)) by 34.51%; and with 75% FLOPs reduction, the accuracy achieved by our method is higher than PPR (Zhuang et al. (2020)) by 0.87%.",
                "The widely used structured pruning method\u2014L1-norm based structured pruning Renda et al. (2020) removes filters with the lowest L1-norm values, whereas the most recent method\u2014Polarization-based structured pruning Zhuang et al. (2020) improves it by using a regularizer on scaling factors of filters and pruning filters whose scaling factors are below than a threshold.",
                "For example, on ResNet-56, with 1% of accuracy loss, the amount of parameter reduction achieved by our method is higher than that of ILP (Renda et al. (2020)) by 34.51%; and with 75% FLOPs reduction, the accuracy achieved by our method is higher than PPR (Zhuang et al. (2020)) by 0.",
                "The widely used structured pruning method\u2014L1-norm based structured pruning Renda et al. (2020) removes filters with the lowest L1-norm values, whereas the most recent method\u2014Polarization-based structured pruning Zhuang et al. (2020) improves it by using a regularizer on scaling factors of filters and pruning filters whose scaling factors are below than a threshold. These two methods both assume that the weight values of a filter can be used as an indicator about the importance of that filter, much like how LTH uses weight values in unstructured pruning. However, we observe that weight-based structured pruning methods cannot produce accurate pruned models. For example, to prune ResNet-56 on CIFAR-10 with no loss in top-1 accuracy, L1-norm based structured pruning can achieve at most only 1.15\u02c6 model compression, and Polarization-norm based structured pruning can achieve at most only 1.89\u02c6 inference speedup. The reason is that, some filters, even though their weight values are small, can still produce useful non-zero activation values that are important for learning features during backpropagation. That is, filters with small values may have large activations. We propose that the activation values of filters are more effective in finding unimportant filters to prune. Activations like ReLu enable non-linear operations, and enable convolutional layers to act as feature detectors. If an activation value is small, then its corresponding feature detector is not important for prediction tasks. So activation values, i.e., the intermediate output tensors after the non-linear activation, not only detect features of training dataset, but also contain the information of convolution layers that act as feature detectors for prediction tasks. We present a visual motivation in Figure 1(a). The figure shows the activation output of 16 filters of a convolution layer on one input image. The first image on the left is the original image, and the second image is the input features after data augmentation. We observe that some filters extract image features with high activation patterns, e.g., the 6th and 12th filters. In comparison, the activation outputs of some filters are close to zero, such as the 2nd, 14th, and 16th. Therefore, from visual inspection, removing filters with weak activation patterns is likely to have low impact on the final accuracy of the pruned model. There is a natural connection between our activation-based pruning approach and the related attention-based knowledge transfer works (Zagoruyko & Komodakis (2016)).",
                "DNN pruning is a promising approach (Li et al. (2016); Han et al. (2015); Molchanov et al. (2016); Theis et al. (2018); Renda et al. (2020)), which identifies the parameters (or weight elements) that do not contribute significantly to the accuracy and prunes them from the network.",
                "The widely used structured pruning method\u2014L1-norm based structured pruning Renda et al. (2020) removes filters with the lowest L1-norm values, whereas the most recent method\u2014Polarization-based structured pruning Zhuang et al. (2020) improves it by using a regularizer on scaling factors of filters\u2026",
                "The widely used structured pruning method\u2014L1-norm based structured pruning Renda et al. (2020) removes filters with the lowest L1-norm values, whereas the most recent method\u2014Polarization-based structured pruning Zhuang et al."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5e7f75dd560d9456a88e492172a60bde33e93590",
                "externalIds": {
                    "CorpusId": 246276463
                },
                "corpusId": 246276463,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5e7f75dd560d9456a88e492172a60bde33e93590",
                "title": "Algorithm 1 Adaptive Iterative Structured Pruning Algorithm",
                "abstract": "Pruning is a promising approach to compress complex deep learning models in order to deploy them on resource-constrained edge devices. However, many existing pruning solutions are based on unstructured pruning, which yield models that cannot efficiently run on commodity hardware, and require users to manually explore and tune the pruning process, which is time consuming and often leads to sub-optimal results. To address these limitations, this paper presents an adaptive, activation-based, structured pruning approach to automatically and efficiently generate small, accurate, and hardware-efficient models that meet user requirements. First, it proposes iterative structured pruning using activation-based attention feature maps to effectively identify and prune unimportant filters. Then, it proposes adaptive pruning policies for automatically meeting the pruning objectives of accuracy-critical, memory-constrained, and latency-sensitive tasks. A comprehensive evaluation shows that the proposed method can substantially outperform the state-of-the-art structured pruning works on CIFAR-10 and ImageNet datasets. For example, on ResNet-56 with CIFAR-10, without any accuracy drop, our method achieves the largest parameter reduction (79.11%), outperforming the related works by 22.81% to 66.07%, and the largest FLOPs reduction (70.13%), outperforming the related works by 14.13% to 26.53%.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1995855",
                        "name": "Kaiqi Zhao"
                    },
                    {
                        "authorId": "101682296",
                        "name": "Animesh Jain"
                    },
                    {
                        "authorId": "2152527896",
                        "name": "Ming Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To make matters worse, since we know that there are often multiple winning tickets available from the training process (Renda et al., 2020), picking one ticket but not the other will potentially yield a different set of preferred clustering schemes for the network.",
                "This technique is referred as weights rewinding and we denote the range of [k1, k2] as the tickets window (Renda et al., 2020).",
                "\u2026on the lottery ticket hypothesis, scholars have their disagreements on whether to reinitialize the weights of W \u2032\nt to their initial values (namely, k = 0 for k in W \u2032\nk) (Frankle & Carbin, 2019), to near initialization (0   k t) (Renda et al., 2020), or just to reset randomly (Liu et al., 2019).",
                "Yet some simple tricks like dynamic pruning rate, soft-pruning (keep the pruned components updatable until very end), and weight reinitialization before fine-tuning may often provide most algorithms another performance boost (Li et al., 2017; He et al., 2018; Renda et al., 2020; Liu et al., 2019).",
                "This is because, for early or even concurrent arts on the lottery ticket hypothesis, scholars have their disagreements on whether to reinitialize the weights of W \u2032 t to their initial values (namely, k = 0 for k in W \u2032 k) (Frankle & Carbin, 2019), to near initialization (0 < k t) (Renda et al., 2020), or just to reset randomly (Liu et al.",
                "We address the first challenge by consulting model-generated information \u2014 in this case, the empirical findings on Lottery Ticket Hypothesis (LTH) and related literature on weights shifting \u2014 to develop a scoring system that identifies the optimal clustering scheme among options per each convolutional layer (Frankle & Carbin, 2019; Renda et al., 2020).",
                "Our proposed method can also be applied to experiments outside of Renda et al. (2020).",
                "In addition, our method consults empirical findings on the lottery ticket hypothesis and its derived literature regarding weights shifting (Frankle & Carbin, 2019; Renda et al., 2020; Zhou et al., 2019), and we propose a novel greedy kernel pruning algorithm that is again simple, efficient, yet effective \u2014 more on this in Section 3.",
                "\u2026information \u2014 in this case, the empirical findings on Lottery Ticket Hypothesis (LTH) and related literature on weights shifting \u2014 to develop a scoring system that identifies the optimal clustering scheme among options per each convolutional layer (Frankle & Carbin, 2019; Renda et al., 2020).",
                "Thus, we can decide which model we will prune on, identify its tickets window by consulting experiment results from Renda et al. (2020), and truncate some epochs in such window to conduct multiple evaluations.",
                "\u2026method consults empirical findings on the lottery ticket hypothesis and its derived literature regarding weights shifting (Frankle & Carbin, 2019; Renda et al., 2020; Zhou et al., 2019), and we propose a novel greedy kernel pruning algorithm that is again simple, efficient, yet effective \u2014 more\u2026",
                "We address both challenges by relying on the same finding from Renda et al. (2020), which demonstrates that for any k where 0 < k1 \u2264 k \u2264 k2 < t, W \u2032\nk can be a winning ticket.",
                "A vast amount of research has been done to demonstrate the existence of winning tickets across different networks and datasets, making the lottery ticket hypothesis one of the most tested inductive biases among neural networks (Renda et al., 2020)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "786bb76f24c470b004b5f227abda6936dda4dd71",
                "externalIds": {
                    "CorpusId": 251736011
                },
                "corpusId": 251736011,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/786bb76f24c470b004b5f227abda6936dda4dd71",
                "title": "R EVISIT K ERNEL P RUNING WITH L OTTERY R EGU LATED G ROUPED C ONVOLUTIONS",
                "abstract": "Structured pruning methods which are capable of delivering a densely pruned network are among the most popular techniques in the realm of neural network pruning, where most methods prune the original network at a filter or layer level. Although such methods may provide immediate compression and acceleration benefits, we argue that the blanket removal of an entire filter or layer may result in undesired accuracy loss. In this paper, we revisit the idea of kernel pruning (to only prune one or several k \u00d7 k kernels out of a 3D-filter), a heavily overlooked approach under the context of structured pruning. This is because kernel pruning will naturally introduce sparsity to filters within the same convolutional layer \u2014 thus, making the remaining network no longer dense. We address this problem by proposing a versatile grouped pruning framework where we first cluster filters from each convolutional layer into equal-sized groups, prune the grouped kernels we deem unimportant from each filter group, then permute the remaining filters to form a densely grouped convolutional architecture (which also enables the parallel computing capability) for fine-tuning. Specifically, we consult empirical findings from a series of literature regarding Lottery Ticket Hypothesis to determine the optimal clustering scheme per layer, and develop a simple yet cost-efficient greedy approximation algorithm to determine which group kernels to keep within each filter group. Extensive experiments also demonstrate our method often outperforms comparable SOTA methods with lesser data augmentation needed, smaller finetuning budget required, and sometimes even much simpler procedure executed (e.g., one-shot v. iterative). Please refer to our GitHub repository for code.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2182378625",
                        "name": "Henry Zhong"
                    },
                    {
                        "authorId": "2119056364",
                        "name": "Guanqun Zhang"
                    },
                    {
                        "authorId": "2181913772",
                        "name": "Ningjia Huang"
                    },
                    {
                        "authorId": "2149230615",
                        "name": "Shuai Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, unstructured pruning is widely considered in academia (LeCun, Denker, and Solla 1989; Renda, Frankle, and Carbin 2019) and with improvements in sparse tensor support on embedded hardware might become the predominant method for practical applications in the future."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b12d48435ebd20b9b35fc52ce863916bb1aa0bb5",
                "externalIds": {
                    "DBLP": "conf/aaai/SchwaigerSR22",
                    "CorpusId": 247321084
                },
                "corpusId": 247321084,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b12d48435ebd20b9b35fc52ce863916bb1aa0bb5",
                "title": "Beyond Test Accuracy: The Effects of Model Compression on CNNs",
                "abstract": "Model compression is widely employed to deploy convolutional neural networks on devices with limited computational resources or power limitations. For high stakes applications, such as autonomous driving, it is, however, important that compression techniques do not impair the safety of the system. In this paper, we therefore investigate the changes introduced by three compression methods \u2013 post-training quantization, global unstructured pruning, and the combination of both \u2013 that go beyond the test accuracy. To this end, we trained three image classifiers on two datasets and compared them regarding their performance on the class level and regarding their attention to different input regions. Although the deviations in test accuracy were minimal, our results show that the considered compression techniques introduce substantial changes to the models that reflect in the quality of predictions of individual classes and in the salience of input regions. While we did not observe the introduction of systematic errors or biases towards certain classes, these changes can significantly impact the failure modes of CNNs and thus are highly relevant for safety analyses. We therefore conclude that it is important to be aware of the changes caused by model compression and to already consider them in the early stages of the development process.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "151473517",
                        "name": "A. Schwaiger"
                    },
                    {
                        "authorId": "2158183059",
                        "name": "Kristian Schwienbacher"
                    },
                    {
                        "authorId": "2706641",
                        "name": "Karsten Roscher"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[30] \u2014 retraining for 25% of the original training time (within the rewinding safe zone [30]) and, notably, rewinding the learning rate schedule to its value before the last 25% of the training schedule."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "18ca53bc269b5d532256b5a7805f111427ef53cf",
                "externalIds": {
                    "CorpusId": 250391896
                },
                "corpusId": 250391896,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/18ca53bc269b5d532256b5a7805f111427ef53cf",
                "title": "Evaluating Neural Network Pruning Techniques on Visual Transformers",
                "abstract": "As Transformer models grow ever larger, their efficiency becomes of increasing importance. As such, the field of model compression, specifically network pruning, is of par-ticular relevance. In this work, we rigorously evaluate standard magnitude-based pruning techniques on Vision Transformers (ViTs) to observe ViT-specific behaviour and gain a deeper understanding of the unique properties which con-tribute to their high performance. Our findings suggest that ViTs can achieve high sparsity ratios without significant accuracy loss, both in structured and unstructured pruning settings. We observe that the effects of pruning vary when applied to different layers, and our results from this analysis highlight the key role played by attention units in Transformers. We also find surprising spatial patterns in the weight matrices yielded by unstructured pruning, which reveal an implicit structure in the underlying model. We observe that fine-tuning after pruning has a strong effect of recovering accuracy, mitigating sub-optimal or one-shot pruning strategies. Lastly, we find that magnitude-based structured pruning only marginally outperforms a random pruning baseline, suggesting a direction for future work.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115706650",
                        "name": "Sarah Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We introduce these settings because previous works [47,23,56] showed that finetuning LR has a great impact on the final performance.",
                "Recent structured pruning works [47,23] showed an interesting phenomenon: During finetuning, a larger learning rate (LR) helps achieve a significantly better final performance (e."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3b054fa8497793b667a98be3a713f494c307ab91",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-12534",
                    "DOI": "10.48550/arXiv.2207.12534",
                    "CorpusId": 251067207
                },
                "corpusId": 251067207,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3b054fa8497793b667a98be3a713f494c307ab91",
                "title": "Trainability Preserving Neural Structured Pruning",
                "abstract": ". Several recent works empirically \ufb01nd \ufb01netuning learning rate is critical to the \ufb01nal performance in neural network structured pruning. Further researches \ufb01nd that the network trainability broken by pruning answers for it, thus calling for an urgent need to recover trainability before \ufb01netuning. Existing attempts propose to exploit weight orthogonalization to achieve dynamical isometry for improved trainability. However, they only work for linear MLP networks. How to develop a \ufb01lter pruning method that maintains or recovers trainability and is scalable to modern deep networks remains elusive. In this paper, we present trainability preserving pruning (TPP), a regularization-based structured pruning method that can e\ufb00ectively maintain trainability during sparsi\ufb01cation. Speci\ufb01cally, TPP regularizes the gram matrix of convolutional kernels so as to de-correlate the pruned \ufb01lters from the kept \ufb01lters. Beside the convolutional layers, we also propose to regularize the BN parameters for better preserving trainability. Empirically, TPP can compete with the ground-truth dynamical isometry recovery method on linear MLP networks. On non-linear networks (ResNet56/VGG19, CIFAR datasets), it outperforms the other counterpart solutions by a large margin . Moreover, TPP can also work e\ufb00ectively with modern deep networks (ResNets) on ImageNet, delivering encouraging performance in comparison to many top-performing \ufb01lter pruning methods. To our best knowledge, this is the \ufb01rst approach that e\ufb00ectively maintains trainability during pruning for the large-scale deep neural networks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2113269100",
                        "name": "Huan Wang"
                    },
                    {
                        "authorId": "2156255943",
                        "name": "Yun Fu"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", applyingAt ); (2) eliminating a portion of insignificant weights with the globally smallest magnitudes (Han et al., 2015; Renda et al., 2020) so that the model only has si% of weights remaining (i.",
                "Follow-up efforts (Renda et al., 2020; Frankle et al., 2020b) introduce the effective weight rewinding techniques to scale up LTH to large networks on large-scale datasets.",
                "\u2026on a datasetD (i.e., applyingADt ); (2) eliminating a portion of insignificant weights with the globally smallest magnitudes (Han et al., 2015; Renda et al., 2020) so that the model only has si% of weights remaining (i.e., the sparsity); (3) rewinding model weights to \u03b8 (\u03b8 = \u03b80, the original\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b30673f3d48e3b2106158ff7a3b3fff9b748cf3d",
                "externalIds": {
                    "CorpusId": 251734698
                },
                "corpusId": 251734698,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b30673f3d48e3b2106158ff7a3b3fff9b748cf3d",
                "title": "A UDIO L OTTERY : S PEECH R ECOGNITION M ADE U LTRA -L IGHTWEIGHT , T RANSFERABLE , AND N OISE R OBUST",
                "abstract": "Lightweight speech recognition models have seen explosive demands owing to a growing amount of speech-interactive features on mobile devices. Since designing such systems from scratch is non-trivial, practitioners typically choose to compress large (pre-trained) speech models. Recently, lottery ticket hypothesis reveals the existence of highly sparse subnetworks that can be trained in isolation without sacrificing the performance of the full models. In this paper, we investigate the tantalizing possibility of using lottery ticket hypothesis to discover lightweight speech recognition models, that are (1) robust to various noise existing in speech; (2) transferable to fit the open-world personalization; and 3) compatible with structured sparsity. We conducted extensive experiments on CNN-LSTM, RNNTransducer, and Transformer models, and verified the existence of highly sparse \u201cwinning tickets\u201d that can match the full model performance across those backbones. We obtained winning tickets that have less than 20% of full model weights on all backbones, while the most lightweight one only keeps 4.4% weights. Those winning tickets generalize to structured sparsity with no performance loss, and transfer exceptionally from large source datasets to various target datasets. Perhaps most surprisingly, when the training utterances have high background noises, the winning tickets even substantially outperform the full models, showing the extra bonus of noise robustness by inducing sparsity. Codes are available at https://github.com/VITA-Group/Audio-Lottery.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51267247",
                        "name": "Shaojin Ding"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2022) or follow a computationally expensive cycle of pruning and retraining for multiple iterations (Savarese et al., 2020; You et al., 2019; Frankle & Carbin, 2019; Renda et al., 2019; Dettmers & Zettlemoyer, 2019).",
                "\u2026(Lee et al., 2018; Wang et al., 2020; Tanaka et al., 2020; Sreenivasan et al., 2022) or follow a computationally expensive cycle of pruning and retraining for multiple iterations (Savarese et al., 2020; You et al., 2019; Frankle & Carbin, 2019; Renda et al., 2019; Dettmers & Zettlemoyer, 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e0ce7e0befcf7554f3db3ddb50079bf59b548906",
                "externalIds": {
                    "CorpusId": 252715754
                },
                "corpusId": 252715754,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e0ce7e0befcf7554f3db3ddb50079bf59b548906",
                "title": "H OW E RD \u00a8 OS AND R \u00b4 ENYI W IN THE L OTTERY",
                "abstract": "Random masks define surprisingly effective sparse neural network models, as has been shown empirically. The resulting Erd\u00f6s-R\u00e9nyi (ER) random graphs can often compete with dense architectures and state-of-the-art lottery ticket pruning algorithms struggle to outperform them, even though the random baselines do not rely on computationally expensive pruning-training iterations but can be drawn initially without significant computational overhead. We offer a theoretical explanation of how such ER masks can approximate arbitrary target networks if they are wider by a logarithmic factor in the inverse sparsity 1/ log(1/sparsity). While we are the first to show theoretically and experimentally that random ER source networks contain strong lottery tickets, we also prove the existence of weak lottery tickets that require a lower degree of overparametrization than strong lottery tickets. These unusual results are based on the observation that ER masks are well trainable in practice, which we verify in experiments with varied choices of random masks. Some of these data-free choices outperform previously proposed random approaches on standard image classification benchmark datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2218573299",
                        "name": "R\u00e9nyi Win"
                    },
                    {
                        "authorId": "2218578381",
                        "name": "The Lottery"
                    },
                    {
                        "authorId": "2048027747",
                        "name": "Advait Gadhikar"
                    },
                    {
                        "authorId": "79760097",
                        "name": "Sohom Mukherjee"
                    },
                    {
                        "authorId": "8813467",
                        "name": "R. Burkholz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We note that this learning rate schedule is different from prior work on pruning, which typically uses a single decay cycle [Kusupati et al., 2020, Singh and Alistarh, 2020, Peste et al., 2021], or dynamic learning rate rewinding, e.g. [Frankle et al., 2019, Renda et al., 2020]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5c0148075627f5c5c7c0f3fd4d6041b929c3c8d0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-09223",
                    "DOI": "10.48550/arXiv.2210.09223",
                    "CorpusId": 252918510
                },
                "corpusId": 252918510,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5c0148075627f5c5c7c0f3fd4d6041b929c3c8d0",
                "title": "oViT: An Accurate Second-Order Pruning Framework for Vision Transformers",
                "abstract": "Models from the Vision Transformer (ViT) family have recently provided break-through results across image classi\ufb01cation tasks such as ImageNet. Yet, they still face barriers to deployment, notably the fact that their accuracy can be severely impacted by compression techniques such as pruning. In this paper, we take a step towards addressing this issue by introducing Optimal ViT Surgeon (oViT) , a new state-of-the-art method for the weight sparsi\ufb01cation of Vision Transformers (ViT) models. At the technical level, oViT introduces a new weight pruning algorithm which leverages second-order information, speci\ufb01cally adapted to be both highly-accurate and ef\ufb01cient in the context of ViTs. We complement this accurate one-shot pruner with an in-depth investigation of gradual pruning, augmentation, and recovery schedules for ViTs, which we show to be critical for successful ViT compression. We validate our method via extensive experiments on classical ViT and DeiT models, as well as on newer variants, such as XCiT, Ef\ufb01cientFormer and Swin. Moreover, our results are even relevant to recently-proposed highly-accurate ResNets. Our results show for the \ufb01rst time that ViT-family models can in fact be pruned to high sparsity levels (e.g. \u2265 75% ) with low impact on accuracy ( \u2264 1% relative drop), and that our approach outperforms prior methods by signi\ufb01cant margins at high sparsities. In addition, we show that our method is compatible with structured pruning methods and quantization, and that it can lead to signi\ufb01cant speedups on a sparsity-aware inference engine.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2006108901",
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "authorId": "40992614",
                        "name": "Eldar Kurtic"
                    },
                    {
                        "authorId": "1502248377",
                        "name": "Elias Frantar"
                    },
                    {
                        "authorId": "3311387",
                        "name": "Dan Alistarh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1de4d197c23f6adea4df27977a6c8be2f7928962",
                "externalIds": {
                    "CorpusId": 252991214
                },
                "corpusId": 252991214,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1de4d197c23f6adea4df27977a6c8be2f7928962",
                "title": "Improved Sparse Optimization for Machine Learning",
                "abstract": "We consider the fundamental task of recovering sparse minimizers of a (convex) real function. Instances of this problem are often encountered in machine learning, where the goal is to find small models that can fit the training data. This is especially relevant today, when the mere size of models employed becomes so prohibitive that even transfering them across devices becomes an important challenge [HABN+21]. Previously, sparse minimization had found a lot of traction in the field of compressed sensing [MZ93, CT05, C+06, CRT06, BD09, JTK14]. A series of exciting works have shown that it is possible to recover sparse signals from a small number of measurements. This was surprising as it turned out that recovering an s-sparse signal x1 can be done efficiently using e O (s)2 linear measurements of x. While the underlying problem of finding a sparse solution to an underdetermined linear system is NP-hard in general, it turns out that structure can be leveraged to obtain beyond the worst case guarantees. In this case, the structure of the measurements was crucial to show that recovery can be performed in polynomial time. Furthermore, sparse optimization has been succesfully applied in multiple areas of machine learning and signal processing, including recommender systems, matrix factorization, or robust principal components analysis. While this line of work has been extremely successful, the specific limits of these methods are far from being understood. For example, the current state of the art shows that one can obtain a minimizer as good as the best s-sparse one, while sacrificing a factor of O (\uf8ff) in sparsity, where \uf8ff is a certain condition number of the underlying function [AS20, AS22]. Classical works in compressed sensing considered only the case of constant \uf8ff, but in many situations this can be quite large, thus giving a prohibitive blow up in sparsity. It is therefore important to ask",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2869958",
                        "name": "Adrian Vladu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e352f5753cbb4cee32f1d5f8d922d1ce415183ac",
                "externalIds": {
                    "CorpusId": 253180918
                },
                "corpusId": 253180918,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e352f5753cbb4cee32f1d5f8d922d1ce415183ac",
                "title": "Unmasking the Lottery Ticket Hypothesis: Efficient Adaptive Pruning for Finding Winning Tickets",
                "abstract": "Modern deep learning involves training costly, highly overparameterized networks, thus motivating the search for sparser networks that require less compute and memory but can still be trained to the same accuracy as the full network ( i.e. matching ). Iterative magnitude pruning (IMP) is a state of the art algorithm that can find such highly sparse matching subnetworks , known as winning tickets , that can be retrained from initialization or an early training stage. IMP operates by iterative cycles of training, masking a fraction of smallest magnitude weights, rewinding unmasked weights back to an early training point, and repeating. Despite its simplicity, the underlying principles for when and how IMP finds winning tickets remain elusive. In particular, what useful information does an IMP mask found at the end of training convey to a rewound network near the beginning of training? We find that\u2014at higher sparsities\u2014pairs of pruned networks at successive pruning iterations are connected by a linear path with zero error barrier if and only if they are matching. This indicates that masks found at the end of training encodes information about the identity of an axial subspace that intersects a desired linearly connected mode of a matching sublevel set. We leverage this observation to design a simple adaptive pruning heuristic for speeding up the discovery of winning tickets and achieve a 30% reduction in computation time on CIFAR-100. These results make progress toward demystifying the existence of winning tickets with an eye towards enabling the development of more efficient pruning algorithms.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1690452",
                        "name": "Mansheej Paul"
                    },
                    {
                        "authorId": "2650184",
                        "name": "F. Chen"
                    },
                    {
                        "authorId": "152574768",
                        "name": "Brett W. Larsen"
                    },
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "25769960",
                        "name": "S. Ganguli"
                    },
                    {
                        "authorId": "2533850",
                        "name": "G. Dziugaite"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The works (Gale, Elsen, and Hooker 2019; Yu et al. 2019; Renda, Frankle, and Carbin 2020) show that the subnetworks exist early in the training instead of initialization on Transformers.",
                "The following work (Renda, Frankle, and Carbin 2020) extends (rewinds) the training of the subnetwork from initialization to the early stage of pretraining, which improves the accuracy of the subnetwork in more challenging tasks.",
                "\u2026from the image classification tasks, the LTH is also imported in many other research areas (Chen et al. 2020; Mallya, Davis, and Lazebnik 2018; Gale, Elsen, and Hooker 2019; Yu et al. 2019; Renda, Frankle, and Carbin 2020; Chen et al. 2020; Prasanna, Rogers, and Rumshisky 2020; Girish et al. 2021)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ff8cb0aa6d9b6430132da1c050c4ea7e5cfcdeca",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-01484",
                    "DOI": "10.48550/arXiv.2211.01484",
                    "CorpusId": 253265206
                },
                "corpusId": 253265206,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ff8cb0aa6d9b6430132da1c050c4ea7e5cfcdeca",
                "title": "The Lottery Ticket Hypothesis for Vision Transformers",
                "abstract": "The conventional lottery ticket hypothesis (LTH) claims that there exists a sparse subnetwork within a dense neural network and a proper random initialization method, called the winning ticket , such that it can be trained from scratch to al-most as good as the dense counterpart. Meanwhile, the re- search of LTH in vision transformers (ViTs) is scarcely evaluated. In this paper, we \ufb01rst show that the conventional win- ning ticket is hard to \ufb01nd at weight level of ViTs by existing methods. Then, we generalize the LTH for ViTs to input im- ages consisting of image patches inspired by the input depen-dence of ViTs. That is, there exists a subset of input image patches such that a ViT can be trained from scratch by using only this subset of patches and achieve similar accuracy to the ViTs trained by using all image patches. We call this subset of input patches the winning tickets , which represent a signi\ufb01cant amount of information in the input. Furthermore, we present a simple yet effective method to \ufb01nd the winning tickets in input patches for various types of ViT, including DeiT, LV-ViT, and Swin Transformers. More speci\ufb01cally, we use a ticket selector to generate the winning tickets based on the informativeness of patches. Meanwhile, we build another randomly selected subset of patches for comparison, and the experiments show that there is a clear difference between the performance of models trained with winning tickets and ran- domly selected subsets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2007668856",
                        "name": "Xuan Shen"
                    },
                    {
                        "authorId": "32409528",
                        "name": "Zhenglun Kong"
                    },
                    {
                        "authorId": "39449475",
                        "name": "Minghai Qin"
                    },
                    {
                        "authorId": "2052289825",
                        "name": "Peiyan Dong"
                    },
                    {
                        "authorId": "9347641",
                        "name": "Geng Yuan"
                    },
                    {
                        "authorId": "2148338860",
                        "name": "Xin Meng"
                    },
                    {
                        "authorId": "2168478659",
                        "name": "Hao Tang"
                    },
                    {
                        "authorId": "151480882",
                        "name": "Xiaolong Ma"
                    },
                    {
                        "authorId": "2136922252",
                        "name": "Yanzhi Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other studies concerned hyperparameters modifications (Frankle et al., 2020b; Renda et al., 2020), and concentrated on the transferability (Morcos et al., 2019; Sabatelli et al., 2020) of the pruned networks.",
                "Other studies concerned hyperparameters modifications (Frankle et al., 2020b; Renda et al., 2020), and concentrated on the transferability (Morcos et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c8432138866db832d44f51952da3d49633c2ea30",
                "externalIds": {
                    "DBLP": "conf/icml/PellegriniB22",
                    "CorpusId": 250340597
                },
                "corpusId": 250340597,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c8432138866db832d44f51952da3d49633c2ea30",
                "title": "Neural Network Pruning Denoises the Features and Makes Local Connectivity Emerge in Visual Tasks",
                "abstract": "Pruning methods can considerably reduce the size of arti\ufb01cial neural networks without harming their performance and in some cases they can even uncover sub-networks that, when trained in isolation, match or surpass the test accuracy of their dense counterparts. Here, we characterize the inductive bias that pruning imprints in such \u201cwinning lottery tickets\u201d: focusing on visual tasks, we analyze the architecture resulting from iterative magnitude pruning of a simple fully connected network. We show that the surviving node connectivity is local in input space, and organized in patterns reminiscent of the ones found in convolutional networks. We investigate the role played by data and tasks in shaping the architecture of the pruned sub-network. We \ufb01nd that pruning performances, and the ability to sift out the noise and make local features emerge improve by increasing the size of the training set, and the semantic value of the data. We also study different pruning procedures, and \ufb01nd that iterative magnitude pruning is particularly effective in distilling meaningful connectivity out of features present in the original task. Our results suggest the possibility to auto-matically discover new and ef\ufb01cient architectural inductive biases in other datasets and tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39324024",
                        "name": "F. Pellegrini"
                    },
                    {
                        "authorId": "2188423",
                        "name": "G. Biroli"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2019), fine tuning the learning rates (Renda et al., 2020), more efficient training (You et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9fc03e03200528b3a2877fa4b801f1c3bada7e2e",
                "externalIds": {
                    "DBLP": "conf/icml/Pal0KMB22",
                    "CorpusId": 250340745
                },
                "corpusId": 250340745,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9fc03e03200528b3a2877fa4b801f1c3bada7e2e",
                "title": "A Study on the Ramanujan Graph Property of Winning Lottery Tickets",
                "abstract": "Winning lottery tickets refer to sparse subgraphs of deep neural networks which have classification accuracy close to the original dense networks. Resilient connectivity properties of such sparse networks play an important role in their performance. The attempt is to identify a sparse and yet well-connected network to guarantee unhindered information flow. Connectivity in a graph is best characterized by its spectral expansion property. Ramanujan graphs are robust expanders which lead to sparse but highly-connected networks, and thus aid in studying the winning tickets. A feed-forward neural network consists of a sequence of bipartite graphs representing its layers. We analyze the Ramanujan graph property of such bipartite layers in terms of their spectral characteristics using the Cheeger\u2019s inequality for irregular graphs. It is empirically observed that the winning ticket networks preserve the Ramanujan graph property and achieve a high accuracy even when the layers are sparse. Accuracy and robustness to noise start declining as many of the layers lose the property. Next we find a robust winning lottery ticket by pruning individual layers while retaining their respective Ramanujan graph property. This strategy is observed to improve the performance of existing network pruning algorithms.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40915587",
                        "name": "Bithika Pal"
                    },
                    {
                        "authorId": "144984407",
                        "name": "A. Biswas"
                    },
                    {
                        "authorId": "1730875",
                        "name": "Sudeshna Kolay"
                    },
                    {
                        "authorId": "144240262",
                        "name": "Pabitra Mitra"
                    },
                    {
                        "authorId": "144407488",
                        "name": "B. Basu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, as training continues (e.g., at the 2,000th iteration), these non-active weights with small gradients will have large magnitude and hence are important to model accuracy (Renda, Frankle, and Carbin 2020; Zafrir et al. 2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7d87152d834a8537289825aef3d612bca901e028",
                "externalIds": {
                    "CorpusId": 254636641
                },
                "corpusId": 254636641,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7d87152d834a8537289825aef3d612bca901e028",
                "title": "Dynamic Sparse Training via More Exploration",
                "abstract": "Over-parameterization of deep neural networks (DNNs) has shown high prediction accuracy for many applications. Al-though effective, the large number of parameters hinders its popularity on resource-limited devices and has an outsize en-vironmental impact. Sparse training (using a \ufb01xed number of nonzero weights in each iteration) could signi\ufb01cantly mitigate the training costs by reducing the model size. However, exist- ing sparse training methods mainly use either random-based or greedy-based drop-and-grow strategies, resulting in local min- imal and low accuracy. In this work, we consider the dynamic sparse training as a sparse connectivity search problem and design an exploitation and exploration acquisition function to escape from local optima and saddle points. We further design an acquisition function and provide the theoretical guarantees for the proposed method and clarify its convergence prop- erty. Experimental results show that sparse models (up to 98% sparsity) obtained by our proposed method outperform the SOTA sparse training methods on a wide variety of deep learning tasks. On VGG-19 / CIFAR-100, ResNet-50 / CIFAR-10, ResNet-50 / CIFAR-100, our method has even higher accuracy than dense models. On ResNet-50 / ImageNet, the proposed method has up to 8.2% accuracy improvement compared to SOTA sparse training methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2122804649",
                        "name": "Shaoyi Huang"
                    },
                    {
                        "authorId": "2144399315",
                        "name": "Bowen Lei"
                    },
                    {
                        "authorId": "2116459424",
                        "name": "Dongkuan Xu"
                    },
                    {
                        "authorId": "144490597",
                        "name": "Hongwu Peng"
                    },
                    {
                        "authorId": "2116969722",
                        "name": "Yue Sun"
                    },
                    {
                        "authorId": "3197711",
                        "name": "Mimi Xie"
                    },
                    {
                        "authorId": "2881873",
                        "name": "Caiwen Ding"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many works prune the model in the training phase [33], [34], [35], [36], [37]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "36bce04de9cd6fa72ce373d64a8e6fb3aa5c40be",
                "externalIds": {
                    "DBLP": "journals/access/KimYK22a",
                    "DOI": "10.1109/ACCESS.2022.3231455",
                    "CorpusId": 255030735
                },
                "corpusId": 255030735,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/36bce04de9cd6fa72ce373d64a8e6fb3aa5c40be",
                "title": "Model Compression via Position-Based Scaled Gradient",
                "abstract": "We propose the position-based scaled gradient (PSG) that scales the gradient depending on the position of a weight vector to make it more compression-friendly. First, we theoretically show that applying PSG to the standard gradient descent (GD), which is called PSGD, is equivalent to the GD in the warped weight space, a space made by warping the original weight space via an appropriately designed invertible function. Second, we empirically show that PSG acting as a regularizer to the weight vectors is favorable for model compression domains such as quantization, pruning, and knowledge distillation. PSG reduces the gap between the weight distributions of a full-precision model and its compressed counterpart. This enables the versatile deployment of a model either as an uncompressed mode or as a compressed mode depending on the availability of resources. The experimental results on CIFAR-10/100 and ImageNet datasets show the effectiveness of the proposed PSG in model compression including an iterative pruning method and the knowledge distillation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49476045",
                        "name": "Jangho Kim"
                    },
                    {
                        "authorId": "1713608836",
                        "name": "Kiyoon Yoo"
                    },
                    {
                        "authorId": "101880623",
                        "name": "N. Kwak"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It was first noticed by Renda et al. (2020) that the precise learning rate schedule during retraining can have a dramatic impact on the predictive performance of the pruned network.",
                "Clearly, the choice of the weight decay affects both the accuracy of the dense model as well as the accuracy of the pruned model when using the approach by Renda et al. (2020).",
                "Clearly, we see that SLR benefits from a larger weight decay, while the approach by Renda et al. (2020) is suffering from an increased penalty on the weights.",
                "As a baseline performance for a pruned network, we will use the approach suggested by Renda et al. (2020) as it serves as a good benchmark for the current potential of IMP.",
                "-sparsity tradeoffs (Han et al., 2015; Renda et al., 2020).",
                "For the respective results of the algorithm by Renda et al. (2020) we observe test accuracies of 92.91% (\u00b10.39%), 93.08% (\u00b10.54",
                "Our interest lies in exploring these claimed disadvantages of IMP through rigorous and consistent computational experimentation with a focus on recent advancements concerning the retraining phase, see the results of Renda et al. (2020) and Le and Hua (2021).",
                "\u2026is repeatedly removing only a small fraction of the parameters followed by extensive retraining, is said to achieve results on the Pareto frontier (Renda et al., 2020), its iterative nature is also considered to be computationally tedious, if not impractical: \u201citerative pruning is computationally\u2026",
                "This is in fact how it appears to be used by Han et al. (2015) and how it is for example presented by Renda et al. (2020).",
                "While only the iterative approach, that is repeatedly removing only a small fraction of the parameters followed by extensive retraining, is said to achieve results on the Pareto frontier (Renda et al., 2020), its iterative nature is also considered to be computationally tedious, if not impractical: \u201citerative pruning is computationally intensive, requiring training a network 15 or more times consecutively for multiple trials\u201d (Frankle and Carbin, 2018), leading Liu et al.",
                "Renda et al. (2020) for example suggested the following approach: train a network for T epochs and then iteratively prune 20% percent of the weights and retrain for Trt = T epochs using LRW, i.e., use the same learning rate scheme as during training, until the desired sparsity is reached.",
                "\u2026baseline in the case of a 1e-4 weight decay within the given retraining time frame, we note that SLR easily outperforms the LRW-based proposal by Renda et al. (2020) when considering the weight decays that also lead to the best performing dense model, which is a strong indicator that it is\u2026"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "46edddc44e6793824b69802eb1322dc7d504d006",
                "externalIds": {
                    "CorpusId": 240354174
                },
                "corpusId": 240354174,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/46edddc44e6793824b69802eb1322dc7d504d006",
                "title": "Back to Basics: Efficient Network Compression via IMP",
                "abstract": "Network pruning is a widely used technique for effectively compressing Deep Neural Networks with little to no degradation in performance during inference. Iterative Magnitude Pruning (IMP) (Han et al., 2015) is one of the most established approaches for network pruning, consisting of several iterative training and pruning steps, where a significant amount of the network\u2019s performance is lost after pruning and then recovered in the subsequent retraining phase. While commonly used as a benchmark reference, it is often argued that a) it reaches suboptimal states by not incorporating sparsification into the training phase, b) its global selection criterion fails to properly determine optimal layer-wise pruning rates and c) its iterative nature makes it slow and non-competitive. In light of recently proposed retraining techniques, we investigate these claims through rigorous and consistent experiments where we compare IMP to pruning-during-training algorithms, evaluate proposed modifications of its selection criterion and study the number of iterations and total training time actually required. We find that IMP with SLR (Le and Hua, 2021) for retraining can outperform state-of-the-art pruning-duringtraining approaches without or with only little computational overhead, that the global magnitude selection criterion is largely competitive with more complex approaches and that only few retraining epochs are needed in practice to achieve most of the sparsity-vs.-performance tradeoff of IMP. Our goals are both to demonstrate that basic IMP can already provide state-of-the-art pruning results on par with or even outperforming more complex or heavily parameterized approaches and also to establish a more realistic yet easily realisable baseline for future research.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2056708985",
                        "name": "Max Zimmer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For a fair comparison we also include LRR (Renda et al., 2020) which uses a pre-trained network and multiple rounds of pruning and retraining by leveraging learning rate rewinding.",
                "\u2026N-BEATS\nWER FLOPs PPL FLOPs SMAPE FLOPs\nDense 12.2 4.53G 18.6 927.73G 8.3 41.26M\nSNIP (Lee et al., 2019) 14.3 2.74G 24.6 398.92G 10.1 21.45M LRR (Renda et al., 2020) 13.7 2.61G 23.1 339.21G 9.3 14.47M RigL (Evci et al., 2020) 13.9 2.69G 22.4 326.56G 10.2 15.13M SIS (Ours) 13.1 2.34G 21.1\u2026",
                "(Renda et al., 2020) proposed weight rewinding technique instead of vanilla fine-tuning post-pruning."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ddac4cda451abf2377c5b536903bf31e38f5a3cc",
                "externalIds": {
                    "DBLP": "conf/icml/VermaP21",
                    "CorpusId": 235825404
                },
                "corpusId": 235825404,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ddac4cda451abf2377c5b536903bf31e38f5a3cc",
                "title": "Sparsifying Networks via Subdifferential Inclusion",
                "abstract": "Sparsifying deep neural networks is of paramount interest in many areas, especially when those networks have to be implemented on lowmemory devices. In this article, we propose a new formulation of the problem of generating sparse weights for a pre-trained neural network. By leveraging the properties of standard nonlinear activation functions, we show that the problem is equivalent to an approximate subdifferential inclusion problem. The accuracy of the approximation controls the sparsity. We show that the proposed approach is valid for a broad class of activation functions (ReLU, sigmoid, softmax). We propose an iterative optimization algorithm to induce sparsity whose convergence is guaranteed. Because of the algorithm flexibility, the sparsity can be ensured from partial training data in a minibatch manner. To demonstrate the effectiveness of our method, we perform experiments on various networks in different applicative contexts: image classification, speech recognition, natural language processing, and time-series forecasting. Project page: https://sagarverma.github.io/compression",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2114072073",
                        "name": "S. Verma"
                    },
                    {
                        "authorId": "1737505",
                        "name": "J. Pesquet"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods. We used CIFAR10 for most reproductions along with additional experiments on the larger CIFAR100, which extends the results originally provided by the authors. We have also extended the list of tested network architectures to include Wide ResNets (Zagoruyko and Komodakis [2016]). The new experiments led us to discover the limitations of learning rate rewinding which can worsen pruning results on large architectures. Methodology We implemented the code ourselves in Python with TensorFlow 2, basing our implementation of the paper alone and without consulting the source code provided by the authors. We ran two sets of experiments. In the reproduction set, we have striven to exactly reproduce the experimental conditions of Renda et al. [2020]. We have also conducted additional experiments, which use other network architectures, effectively showing results previously unreported by the authors.",
                "[2019], Renda et al. [2020]). Lottery Ticket Hypothesis from Frankle and Carbin [2019] formulates a hypothesis that for every dense neural network, there exists a smaller subnetwork that matches or exceeds results of the original.",
                "2 Scope of reproducibility Renda et al. [2020] formulated the following claims: Claim 1: Widely used method of training after pruning: finetuning yields worse results than rewinding based methods (supported by figures 1, 2, 3, 4 and table 5) Claim 2: Newly introduced learning rate rewinding works as good or better as weight rewinding in all scenarios (supported by figures 1, 2, 3, 4 and table 5, but not supported by figure 5) Claim 3: Iterative pruning with learning rate rewinding matches state-of-the-art pruning methods (supported by figures 1, 2, 3, 4 and table 5, but not supported by figure 5)",
                "Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods.",
                "For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones.",
                "Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019].",
                "Reproduction Summary Scope of Reproducibility We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods. We used CIFAR10 for most reproductions along with additional experiments on the larger CIFAR100, which extends the results originally provided by the authors. We have also extended the list of tested network architectures to include Wide ResNets (Zagoruyko and Komodakis [2016]).",
                "For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones. Therefore, most of the used networks are residual networks, which were originally proposed in He et al. [2016a]. Additionally, to verify the general usefulness of pruning and retraining methods proposed in Renda et al.",
                "For most of our experiments, we chose to use identical architectures as Renda et al. [2020] to better validate their claims and double-check their results, rather than provide additional ones. Therefore, most of the used networks are residual networks, which were originally proposed in He et al. [2016a]. Additionally, to verify the general usefulness of pruning and retraining methods proposed in Renda et al. [2020] we extend the list of tested network architectures to much larger wide residual networks from Zagoruyko and Komodakis [2016]."
            ],
            "intents": [
                "result",
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "b124978dfc00b324615f72a3e1dcfd600d973021",
                "externalIds": {
                    "CorpusId": 237572192
                },
                "corpusId": 237572192,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b124978dfc00b324615f72a3e1dcfd600d973021",
                "title": "Comparing Rewinding and Fine-tuning in Neural Network Pruning Reproducibility Challenge 2021",
                "abstract": "We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks, by Renda et al. [2020]. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in Frankle and Carbin [2019] and 3) a new, original method involving learning rate rewinding, building upon Frankle and Carbin [2019]. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2127471828",
                        "name": "Szymon Mikler"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As the specific rewind points, we follow the setting in [3].",
                "As a remedy, recent studies [2, 3] demonstrate that a sparse sub-network can still be obtained by using the rewinding technique, which is to re-train it from early-phase training weights or learning rates of the dense model, rather than from random initialization.",
                "We use Pytorch[18] to implement all experiments and follow hyperparameters identically in [3].",
                "The recent work [3] demonstrates that an early training point at 18]% of the dense training process is a good choice for ResNet[c]-56 on CIFAR-10.",
                "[3] presented a simplified version called learning rate rewinding, i.",
                "The rewinding studies [2, 3] find that directly inheriting late stage model weights cannot yield any competitive lottery ticket.",
                "The follow-up works [2, 3] prove that for deeper networks, either rewinding the weights to the ones at early training iteration, or rewinding the learning rate schedule to the early phase, can achieve better performance than rewinding to iteration 0, when seeking lottery tickets in larger networks.",
                "Scaling up LTH to larger networks calls on the \u201clate rewinding\" technique [19, 3].",
                "Similar to previous studies [1, 3], we perform experiments on CIFAR-10, Tiny-ImageNet and ImageNet."
            ],
            "intents": [],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "e21cb9cc7470f3c94c44ac58c6de22303add3bf8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-03255",
                    "CorpusId": 231572951
                },
                "corpusId": 231572951,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e21cb9cc7470f3c94c44ac58c6de22303add3bf8",
                "title": "Good Students Play Big Lottery Better",
                "abstract": "Lottery ticket hypothesis [1] suggests that a dense neural network contains a sparse sub-network that can match the test accuracy of the original dense net when trained in isolation from (the same) random initialization. However, the hypothesis failed to generalize to larger dense networks such as ResNet-50. As a remedy, recent studies [2, 3] demonstrate that a sparse sub-network can still be obtained by using a rewinding technique, which is to re-train it from early-phase training weights or learning rates of the dense model, rather than from random initialization. Is rewinding the only or the best way to scale up lottery tickets ? This paper proposes a new, simpler and yet powerful technique for re-training the sub-network, called \"Knowledge Distillation ticket\" ( KD ticket ). Rewinding exploits the value of inheriting knowledge from the early training phase to improve lottery tickets in large networks. In comparison, KD ticket addresses a complementary possibility - inheriting useful knowledge from the late training phase of the dense model. It is achieved by leveraging the soft labels generated by the trained dense model to re-train the sub-network, instead of the hard labels. Extensive experiments are conducted using several large deep networks (e.g ResNet-50 and ResNet-110) on CIFAR-10 and ImageNet datasets. Without bells and whistles, when applied by itself, KD ticket performs on par or better than rewinding, while being nearly free of hyperparameters or ad-hoc selection. KD ticket can be further applied together with rewinding, yielding state-of-the-art results for large-scale lottery tickets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2126795",
                        "name": "Haoyu Ma"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "3236115",
                        "name": "Ting-Kuei Hu"
                    },
                    {
                        "authorId": "2061592207",
                        "name": "Chenyu You"
                    },
                    {
                        "authorId": "2111366691",
                        "name": "Xiaohui Xie"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Liu et al. (2019); Gale et al. (2019) further scale up LTH by rewinding (Frankle et al., 2020a; Renda et al., 2020).",
                ", 2020b), natural language processing (Gale et al., 2019; Yu et al., 2020; Renda et al., 2020; Chen et al., 2020c; Desai et al., 2019; Chen et al., 2020e), graph neural network (Chen et al.",
                "(2019) further scale up LTH by rewinding (Frankle et al., 2020a; Renda et al., 2020).",
                "\u20262020; You et al., 2020; Chen et al., 2021c; Ma et al., 2021; Chen et al., 2020b), natural language processing (Gale et al., 2019; Yu et al., 2020; Renda et al., 2020; Chen et al., 2020c; Desai et al., 2019; Chen et al., 2020e), graph neural network (Chen et al., 2021b), and reinforcement\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "d5906006e6efc5dbc02878d76407326eb56c363a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-00397",
                    "CorpusId": 232075755
                },
                "corpusId": 232075755,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d5906006e6efc5dbc02878d76407326eb56c363a",
                "title": "Ultra-Data-Efficient GAN Training: Drawing A Lottery Ticket First, Then Training It Toughly",
                "abstract": "Training generative adversarial networks (GANs) with limited data generally results in deteriorated performance and collapsed models. To conquer this challenge, we are inspired by the latest observation of Kalibhat et al. (2020); Chen et al. (2021d), that one can discover independently trainable and highly sparse subnetworks (a.k.a., lottery tickets) from GANs. Treating this as an inductive prior, we decompose the data-hungry GAN training into two sequential sub-problems: ( i ) identifying the lottery ticket from the original GAN; then ( ii ) training the found sparse subnetwork with aggressive data and feature augmentations. Both sub-problems re-use the same small training set of real images. Such a coordinated framework enables us to focus on lower-complexity and more data-ef\ufb01cient sub-problems, effectively stabilizing training and improving convergence. Comprehensive experiments endorse the effectiveness of our proposed ultra-data-ef\ufb01cient training framework, across various GAN architectures (SNGAN, BigGAN, and StyleGAN2) and diverse datasets (CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet). Besides, our training framework also displays powerful few-shot generalization ability, i.e., generating high-\ufb01delity images by training from scratch with just 100 real images, without any pre-training. Codes are available at: https://github.com/VITA-Group/ Ultra-Data-Efficient-GAN-Training .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "153655416",
                        "name": "Yu Cheng"
                    },
                    {
                        "authorId": "144702900",
                        "name": "Zhe Gan"
                    },
                    {
                        "authorId": "46700348",
                        "name": "Jingjing Liu"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many works have been proposed to investigate the behaviors on weight pruning (Tanaka et al., 2020; Ye et al., 2020; Renda et al., 2020; Malach et al., 2020).",
                "The following work (Frankle et al., 2019; Renda et al., 2020) also confirms that, for deeper networks and using relatively large learning rates, the winning property can hardly be observed."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dfbafb4bf4e972faf38af944b37828f3865f9af9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-11068",
                    "CorpusId": 231986132
                },
                "corpusId": 231986132,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dfbafb4bf4e972faf38af944b37828f3865f9af9",
                "title": "Lottery Ticket Implies Accuracy Degradation, Is It a Desirable Phenomenon?",
                "abstract": "In deep model compression, the recent finding \u201cLottery Ticket Hypothesis\u201d (LTH) (Frankle & Carbin, 2018) pointed out that there could exist a winning ticket (i.e., a properly pruned subnetwork together with original weight initialization) that can achieve competitive performance than the original dense network. However, it is not easy to observe such winning property in many scenarios, where for example, a relatively large learning rate is used even if it benefits training the original dense model. In this work, we investigate the underlying condition and rationale behind the winning property, and find that the underlying reason is largely attributed to the correlation between initialized weights and final-trained weights when the learning rate is not sufficiently large. Thus, the existence of winning property is correlated with an insufficient DNN pretraining, and is unlikely to occur for a well-trained DNN. To overcome this limitation, we propose the \u201cpruning & fine-tuning\u201d method that consistently outperforms lottery ticket sparse training under the same pruning algorithm and the same total training epochs. Extensive experiments over multiple deep models (VGG, ResNet, MobileNetv2) on different datasets have been conducted to justify our proposals.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145915285",
                        "name": "Ning Liu"
                    },
                    {
                        "authorId": "9347641",
                        "name": "Geng Yuan"
                    },
                    {
                        "authorId": "1939695",
                        "name": "Zhengping Che"
                    },
                    {
                        "authorId": "2007668856",
                        "name": "Xuan Shen"
                    },
                    {
                        "authorId": "151480882",
                        "name": "Xiaolong Ma"
                    },
                    {
                        "authorId": "153792333",
                        "name": "Qing Jin"
                    },
                    {
                        "authorId": "144139198",
                        "name": "Jian Ren"
                    },
                    {
                        "authorId": "2115854503",
                        "name": "Jian Tang"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "46393431",
                        "name": "Yanzhi Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                ": Best of (Frankle & Carbin, 2019; Renda et al., 2020; Su et al., 2020), obtained from Wang et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f93f2476972228de142fde13913bccbec76859b8",
                "externalIds": {
                    "CorpusId": 233284212
                },
                "corpusId": 233284212,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f93f2476972228de142fde13913bccbec76859b8",
                "title": "FACTORIZED NEURAL LAYERS",
                "abstract": "Factorized layers\u2014operations parameterized by products of two or more matrices\u2014occur in a variety of deep learning contexts, including compressed model training, certain types of knowledge distillation, and multi-head selfattention architectures. We study how to initialize and regularize deep nets containing such layers, examining two simple, understudied schemes, spectral initialization and Frobenius decay, for improving their performance. The guiding insight is to design optimization routines for these networks that are as close as possible to that of their well-tuned, non-decomposed counterparts; we back this intuition with an analysis of how the initialization and regularization schemes impact training with gradient descent, drawing on modern attempts to understand the interplay of weight-decay and batch-normalization. Empirically, we highlight the benefits of spectral initialization and Frobenius decay across a variety of settings. In model compression, we show that they enable low-rank methods to significantly outperform both unstructured sparsity and tensor methods on the task of training low-memory residual networks; analogs of the schemes also improve the performance of tensor decomposition techniques. For knowledge distillation, Frobenius decay enables a simple, overcomplete baseline that yields a compact model from over-parameterized training without requiring retraining with or pruning a teacher network. Finally, we show how both schemes applied to multi-head attention lead to improved performance on both translation and unsupervised pre-training.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3333746",
                        "name": "Neil A. Tenenholtz"
                    },
                    {
                        "authorId": "143722101",
                        "name": "Lester W. Mackey"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, a recent study [30] showed that a 5."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0f32fd7eb3fcb1c500546f7ee942f581358ff46d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-14713",
                    "CorpusId": 235254579
                },
                "corpusId": 235254579,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0f32fd7eb3fcb1c500546f7ee942f581358ff46d",
                "title": "1\u00d7N Block Pattern for Network Sparsity",
                "abstract": "Though network sparsity emerges as a promising direction to overcome the drastically increasing size of neural networks, it remains an open problem to concurrently maintain model accuracy as well as achieve signi\ufb01cant speedups on general CPUs. In this paper, we propose one novel concept of 1 \u00d7 N block sparsity pattern (block pruning) to break this limitation. In particular, consecutive N output kernels with the same input channel index are grouped into one block, which serves as a basic pruning granularity of our pruning pattern. Our 1 \u00d7 N sparsity pattern prunes these blocks considered unimportant. We also provide a work\ufb02ow of \ufb01lter rearrangement that \ufb01rst rearranges the weight matrix in the output channel dimension to derive more in\ufb02uential blocks for accuracy improvements, and then applies similar rearrangement to the next-layer weights in the input channel dimension to ensure correct convolutional operations. Moreover, the output computation after our 1 \u00d7 N block sparsity can be realized via a parallelized block-wise vectorized operation, leading to signi\ufb01cant speedups on general CPUs-based platforms. The ef\ufb01cacy of our pruning pattern is proved with experiments on ILSVRC-2012. For example, in the case of 50% sparsity and N = 4 , our pattern obtains about 3.0% improvements over \ufb01lter pruning in the top-1 accuracy of MobileNet-V2. Meanwhile, it obtains 56.04ms inference savings on Cortex-A7 CPU over weight pruning. Code is available at https://github.com/lmbxmu/1xN .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49352079",
                        "name": "Mingbao Lin"
                    },
                    {
                        "authorId": "2110482496",
                        "name": "Yuchao Li"
                    },
                    {
                        "authorId": "2108078624",
                        "name": "Yu-xin Zhang"
                    },
                    {
                        "authorId": "146335182",
                        "name": "Bohong Chen"
                    },
                    {
                        "authorId": "2070266415",
                        "name": "Fei Chao"
                    },
                    {
                        "authorId": "50468734",
                        "name": "Mengdi Wang"
                    },
                    {
                        "authorId": "2153701890",
                        "name": "Shen Li"
                    },
                    {
                        "authorId": "2146156715",
                        "name": "Jun Yang"
                    },
                    {
                        "authorId": "145592290",
                        "name": "R. Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although subsequent studies of LTH [8], [25], [29] dramatically reduced the training time,"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2c443cc87a1bd847fe38905840f09228cf08f1f2",
                "externalIds": {
                    "DBLP": "journals/spl/WangPL21",
                    "DOI": "10.1109/LSP.2021.3082036",
                    "CorpusId": 235308713
                },
                "corpusId": 235308713,
                "publicationVenue": {
                    "id": "d5da7004-7b61-450a-9c7d-a39500de7acf",
                    "name": "IEEE Signal Processing Letters",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Signal Process Lett"
                    ],
                    "issn": "1070-9908",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=97"
                },
                "url": "https://www.semanticscholar.org/paper/2c443cc87a1bd847fe38905840f09228cf08f1f2",
                "title": "Memory-Free Stochastic Weight Averaging by One-Way Variational Pruning",
                "abstract": "Recent works on convolutional neural networks (CNN) have attempted to find the local optima with ensemble-based approaches. Fast Geometric Ensemble (FGE) showed that captured weight points at the end of training time circulate local optima. This led to the Stochastic Weight Averaging (SWA) approach, which averages multiple model weights to find the local optima. However, they are limited by their output of fully-parameterized models, including needless parameters, after the training procedure. To solve this problem, we propose a novel training procedure: Stochastic Weight Averaging by One-way Variational Pruning (SWA-OVP). SWA-OVP reduces the number of model parameters by variationally updating the mask of weights for pruning. SWA-OVP variationally generates a mask for pruned weights in each iteration while recent pruning approaches produce the mask at the end of each training. In addition, our SWA-OVP prunes the model in a one-way training procedure, while other recent approaches prune the model weights in iterative training or require additional computation. Our experiment shows that SWA-OVP using only a 0.5x%$\\sim$0.7x% parameter size achieves even higher accuracy than SWA and FGE on several networks, such as Pre-ResNet110, Pre-ResNet164 and WideResNet28x10 on CIFAR10 and CIFAR100 datasets. SWA-OVP also achieves better performance compared to state-of-the-art pruning approaches.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108776112",
                        "name": "Yooseung Wang"
                    },
                    {
                        "authorId": "2110354994",
                        "name": "Hyunseong Park"
                    },
                    {
                        "authorId": "2108287028",
                        "name": "Jwajin Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Later on, [20, 61] scaled up LTH to larger models on large-scale datasets via weight rewinding techniques."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cf1b59d11acce66875b59902688e3b1b8b0764f9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-14568",
                    "CorpusId": 235658057
                },
                "corpusId": 235658057,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cf1b59d11acce66875b59902688e3b1b8b0764f9",
                "title": "FreeTickets: Accurate, Robust and Efficient Deep Ensemble by Training with Dynamic Sparsity",
                "abstract": "Recent works on sparse neural networks have demonstrated that it is possible to train a sparse network in isolation to match the performance of the corresponding dense networks with a fraction of parameters. However, the iden-ti\ufb01cation of these performant sparse neural networks (win-ning tickets) either involves a costly iterative train-prune-retrain process (e.g., Lottery Ticket Hypothesis) or an over-extended sparse training time (e.g., Training with Dynamic Sparsity), both of which would raise \ufb01nancial and environmental concerns. In this work, we attempt to address this cost-reducing problem by introducing the FreeTickets concept, as the \ufb01rst solution which can boost the performance of sparse convolutional neural networks over their dense network equivalents by a large margin, while using for complete training only a fraction of the computational resources required by the latter. Concretely, we instantiate the FreeTickets concept, by proposing two novel ef\ufb01cient ensemble methods with dynamic sparsity, which yield in one shot many diverse and accurate tickets \u201cfor free\u201d during the sparse training process. The combination of these free tickets into an ensemble demonstrates a signi\ufb01cant improvement in accuracy, uncertainty estimation, robustness, and ef\ufb01ciency over the corresponding dense (ensemble) networks. Our results provide new",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "1768130522",
                        "name": "Zahra Atashgahi"
                    },
                    {
                        "authorId": "2116298570",
                        "name": "Xiaohan Chen"
                    },
                    {
                        "authorId": "67102129",
                        "name": "Ghada Sokar"
                    },
                    {
                        "authorId": "146634864",
                        "name": "Elena Mocanu"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1a83f0a48b3e91a18e1fa075fe9b6660692c34de",
                "externalIds": {
                    "CorpusId": 236202420
                },
                "corpusId": 236202420,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1a83f0a48b3e91a18e1fa075fe9b6660692c34de",
                "title": "Towards improving robustness of compressed CNNs",
                "abstract": "High capacity CNN models trained on large datasets with strong data augmentation are known to improve robustness to distribution shifts. However, in resource constrained scenarios, such as embedded devices, it is not always feasible to deploy such large CNNs. Model compression techniques, such as distillation and pruning, help reduce model size, however their robustness tradeoffs are not known. In this work, we evaluate several distillation and pruning techniques to better understand their influence on out-of-distribution performance. We find that knowledge distillation and pruning combined with data augmentation help transfer much of the robustness to smaller models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2121552910",
                        "name": "Jasper Hoffmann"
                    },
                    {
                        "authorId": "145119099",
                        "name": "Shashank Agnihotri"
                    },
                    {
                        "authorId": "2872102",
                        "name": "Tonmoy Saikia"
                    },
                    {
                        "authorId": "1710872",
                        "name": "T. Brox"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", either from their initial values or some later point) [9, 23, 58].",
                "[58] Alex Renda, Jonathan Frankle, and Michael Carbin."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "45b31d499ed297927bebcd22fa7a6e2bbe191c48",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-00259",
                    "CorpusId": 236772801
                },
                "corpusId": 236772801,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/45b31d499ed297927bebcd22fa7a6e2bbe191c48",
                "title": "Provably Efficient Lottery Ticket Discovery",
                "abstract": "The lottery ticket hypothesis (LTH) [19] claims that randomly-initialized, dense neural networks contain (sparse) subnetworks that, when trained an equal amount in isolation, can match the dense network\u2019s performance. Although LTH is useful for discovering ef\ufb01cient network architectures, its three-step process\u2014pre-training, pruning, and re-training\u2014is computationally expensive, as the dense model must be fully pre-trained. Luckily, \u201cearly-bird\u201d tickets can be discovered within neural networks that are minimally pre-trained [67], allowing for the creation of ef\ufb01cient, LTH-inspired training procedures. Yet, no theoretical foundation of this phenomenon exists. We derive an analytical bound for the number of pre-training iterations that must be performed for a winning ticket to be discovered, thus providing a theoretical understanding of when and why such early-bird tickets exist. By adopting a greedy forward selection pruning strategy [65], we directly connect the pruned network\u2019s performance to the loss of the dense network from which it was derived, revealing a threshold in the number of pre-training iterations beyond which high-performing subnetworks are guaranteed to exist. We demonstrate the validity of our theoretical results across a variety of architectures and datasets, including multi-layer perceptrons (MLPs) trained on MNIST and several deep convolutional neural network (CNN) architectures trained on CIFAR10 and ImageNet.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34210029",
                        "name": "Cameron R. Wolfe"
                    },
                    {
                        "authorId": "2154497049",
                        "name": "Qihan Wang"
                    },
                    {
                        "authorId": "2120215686",
                        "name": "J. Kim"
                    },
                    {
                        "authorId": "3393746",
                        "name": "Anastasios Kyrillidis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Hybrid Ticket: the idea of smart-ratios can be applied to the recently proposed learning rate rewinding method [3] with further performance improvement.",
                "The recently proposed learning rate rewinding method [3] passes the sanity-checking, meaning the data information is important to keep the performance the same."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bb3d94fdb9b02fc4f59129a3b3b3b0eeea4a32ae",
                "externalIds": {
                    "CorpusId": 236941997
                },
                "corpusId": 236941997,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bb3d94fdb9b02fc4f59129a3b3b3b0eeea4a32ae",
                "title": "[Re] Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot",
                "abstract": "We find that simple data corruption and random weight shuffling interventions do not hurt the performance as claimed in the original paper for the baseline methods. For Random Tickets, we reproduce the accuracy to within 1% for most of the settings. Hybrid Tickets and its baseline method are slightly different from the original paper for some settings on CIFAR-100. However, we did not find evidence that Hybrid Tickets improve the baseline\u2019s method performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35933097",
                        "name": "Andrei Atanov"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6ee98d9b218fcace923fe5fef742cee54ebd32f3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-03210",
                    "CorpusId": 238419190
                },
                "corpusId": 238419190,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6ee98d9b218fcace923fe5fef742cee54ebd32f3",
                "title": "Universality of Deep Neural Network Lottery Tickets: A Renormalization Group Perspective",
                "abstract": "Foundational work on the Lottery Ticket Hypothesis has suggested an exciting corollary: winning tickets found in the context of one task can be transferred to similar tasks, possibly even across different architectures. While this has become of broad practical and theoretical interest, to date, there exists no detailed understanding of why winning ticket universality exists, or any way of knowing a priori whether a given ticket can be transferred to a given task. To address these outstanding open questions, we make use of renormalization group theory, one of the most successful tools in theoretical physics. We find that iterative magnitude pruning, the method used for discovering winning tickets, is a renormalization group scheme. This opens the door to a wealth of existing numerical and theoretical tools, some of which we leverage here to examine winning ticket universality in large scale lottery ticket experiments, as well as sheds new light on the success iterative magnitude pruning has found in the field of sparse machine learning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145036964",
                        "name": "William T. Redman"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": null,
                        "name": "Akshunna S. Dogra"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the algorithm, we prune the language model using gradual pruning (Liu et al., 2018b; Renda et al., 2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "145049fa32d6650bfb19d2d4fe8fbdceb649db44",
                "externalIds": {
                    "ACL": "2021.ranlp-srw.17",
                    "DOI": "10.26615/issn.2603-2821.2021_017",
                    "CorpusId": 244464100
                },
                "corpusId": 244464100,
                "publicationVenue": {
                    "id": "3413b6f7-e718-4940-a26a-e208f732ada0",
                    "name": "Recent Advances in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "RANLP",
                        "Recent Adv Nat Lang Process"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/145049fa32d6650bfb19d2d4fe8fbdceb649db44",
                "title": "Does local pruning offer task-specific models to learn effectively ?",
                "abstract": "The need to deploy large-scale pre-trained models on edge devices under limited computational resources has led to substantial research to compress these large models. However, less attention has been given to compress the task-specific models. In this work, we investigate the different methods of unstructured pruning on task-specific models for Aspect-based Sentiment Analysis (ABSA) tasks. Specifically, we analyze differences in the learning dynamics of pruned models by using the standard pruning techniques to achieve high-performing sparse networks. We develop a hypothesis to demonstrate the effectiveness of local pruning over global pruning considering a simple CNN model. Later, we utilize the hypothesis to demonstrate the efficacy of the pruned state-of-the-art model compared to the over-parameterized state-of-the-art model under two settings, the first considering the baselines for the same task used for generating the hypothesis, i.e., aspect extraction and the second considering a different task, i.e., sentiment analysis. We also provide discussion related to the generalization of the pruning hypothesis.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2112891513",
                        "name": "Abhishek Mishra"
                    },
                    {
                        "authorId": "2124868748",
                        "name": "Mohna Chakraborty"
                    }
                ]
            }
        },
        {
            "contexts": [
                "from neural networks that do not contribute significantly to the model performance, and remove them for compressing original dense neural networks [49, 68, 52, 57, 3, 30, 35, 64, 39, 27, 29, 4, 40, 61, 33, 56, 16, 21].",
                "However, the majority of neural network pruning methods require to train a dense neural network to generate a pruned sparse network, as it is difficult to train a pruned network from scratch [20, 62, 34, 23, 41, 67, 49, 68, 52, 57, 3, 30, 74, 24, 25, 63, 35, 64, 39, 27, 29, 4, 40], although there are few pruning before training approaches [32, 61, 33, 56, 16, 21]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d5513752425a079195656580e15857d34e4d6941",
                "externalIds": {
                    "DBLP": "conf/nips/ZhangJZZZRLWJD21",
                    "CorpusId": 244958525
                },
                "corpusId": 244958525,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d5513752425a079195656580e15857d34e4d6941",
                "title": "Validating the Lottery Ticket Hypothesis with Inertial Manifold Theory",
                "abstract": "Despite achieving remarkable ef\ufb01ciency, traditional network pruning techniques often follow manually-crafted heuristics to generate pruned sparse networks. Such heuristic pruning strategies are hard to guarantee that the pruned networks achieve test accuracy comparable to the original dense ones. Recent works have empirically identi\ufb01ed and veri\ufb01ed the Lottery Ticket Hypothesis (LTH): a randomly-initialized dense neural network contains an extremely sparse subnetwork, which can be trained to achieve similar accuracy to the former. Due to the lack of theoretical evidence, they often need to run multiple rounds of expensive training and pruning over the original large networks to discover the sparse subnetworks with low accuracy loss. By leveraging dynamical systems theory and inertial manifold theory, this work theoretically veri\ufb01es the validity of the LTH. We explore the possibility of theoretically lossless pruning as well as one-time pruning, compared with existing neural network pruning and LTH techniques. We reformulate the neural network optimization problem as a gradient dynamical system and reduce this high-dimensional system onto inertial manifolds to obtain a low-dimensional system regarding pruned subnetworks. We demonstrate the precondition and existence of pruned subnetworks and prune the original networks in",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118690556",
                        "name": "Zeru Zhang"
                    },
                    {
                        "authorId": "103340106",
                        "name": "Jiayin Jin"
                    },
                    {
                        "authorId": "48806049",
                        "name": "Zijie Zhang"
                    },
                    {
                        "authorId": "2145499198",
                        "name": "Yang Zhou"
                    },
                    {
                        "authorId": "2145735267",
                        "name": "Xin Zhao"
                    },
                    {
                        "authorId": "48115953",
                        "name": "Jiaxiang Ren"
                    },
                    {
                        "authorId": "2118971193",
                        "name": "Ji Liu"
                    },
                    {
                        "authorId": "3008832",
                        "name": "Lingfei Wu"
                    },
                    {
                        "authorId": "1740308",
                        "name": "R. Jin"
                    },
                    {
                        "authorId": "1721158",
                        "name": "D. Dou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A workflow to generate networks that satisfy this 2:4 constraint, using a form of Learning Rate Rewinding [28], has been empirically verified to maintain accuracy across a wide range of networks and tasks (with the exception of the aforementioned small, parameter-efficient networks)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "121c3860d5e3917c658dfac7e1f56ba297e87a6c",
                "externalIds": {
                    "DBLP": "conf/nips/PoolY21",
                    "CorpusId": 245002847
                },
                "corpusId": 245002847,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/121c3860d5e3917c658dfac7e1f56ba297e87a6c",
                "title": "Channel Permutations for N: M Sparsity",
                "abstract": "We introduce channel permutations as a method to maximize the accuracy of N:M sparse networks. N:M sparsity requires N out of M consecutive elements to be zero and has been shown to maintain accuracy for many models and tasks with a simple prune and \ufb01ne-tune work\ufb02ow. By permuting weight matrices along their channel dimension and adjusting the surrounding layers appropriately, we demonstrate accuracy recovery for even small, parameter-ef\ufb01cient networks, without affecting inference run-time. We also present both a quality metric to simplify judging permutations as well as ef\ufb01cient methods to search for high-quality permutations, including two optimizations to escape local minima. Finally, we share an ablation study to show the importance of each part of our search algorithm, experimental results showing correlation between our quality metric and \ufb01nal network accuracy, improved sparse network accuracy using our techniques with insigni\ufb01cant overhead to training time, and the transformation of unstructured to structured sparse workloads. Code to use these techniques when generating a 2:4 sparse network is available at https://github.com/NVIDIA/apex/tree/master/apex/contrib/sparsity.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47325862",
                        "name": "Jeff Pool"
                    },
                    {
                        "authorId": "145698413",
                        "name": "Chong Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is not only resource-intensive but there is also an on-going discussion on whether and how we should fine-tune [36] or rewind to initial weights [23] or train from scratch [29], making it difficult to prefer one approach over another."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1245b5d83fac36a94d8e53045d623b80688944fb",
                "externalIds": {
                    "CorpusId": 245122875
                },
                "corpusId": 245122875,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1245b5d83fac36a94d8e53045d623b80688944fb",
                "title": "Learning Compact Representations of Neural Networks using DiscriminAtive Masking (DAM)",
                "abstract": "A central goal in deep learning is to learn compact representations of features at every layer of a neural network, which is useful for both unsupervised representation learning and structured network pruning. While there is a growing body of work in structured pruning, current state-of-the-art methods suffer from two key limitations: (i) instability during training, and (ii) need for an additional step of fine-tuning, which is resource-intensive. At the core of these limitations is the lack of a systematic approach that jointly prunes and refines weights during training in a single stage, and does not require any fine-tuning upon convergence to achieve state-of-the-art performance. We present a novel single-stage structured pruning method termed DiscriminAtive Masking (DAM). The key intuition behind DAM is to discriminatively prefer some of the neurons to be refined during the training process, while gradually masking out other neurons. We show that our proposed DAM approach has remarkably good performance over a diverse range of applications in representation learning and structured pruning, including dimensionality reduction, recommendation system, graph representation learning, and structured pruning for image classification. We also theoretically show that the learning objective of DAM is directly related to minimizing the L0 norm of the masking layer. All of our codes and datasets are available https://github.com/jayroxis/ dam-pytorch.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49221613",
                        "name": "Jie Bu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[29] and Gale, Elsen, and Hooker [36] show that none of the OSP techniques dominate one another, and IMP outperforms them all [41].",
                "However, state-of-the-art applications of magnitude pruning now employ iterative magnitude pruning (IMP) [3, 40], where a fraction of the smallest weights are pruned on each round, followed by rewinding training back to an earlier iteration [40] or simply rewinding the learning rates [41]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "abfe5e9183008414741c209683d0d24f12025442",
                "externalIds": {
                    "CorpusId": 245910928
                },
                "corpusId": 245910928,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/abfe5e9183008414741c209683d0d24f12025442",
                "title": "Stochastic Pruning: Fine-Tuning, and PAC-Bayes bound optimization",
                "abstract": "We introduce an algorithmic framework for stochastic fine-tuning of pruning masks, starting from masks produced by several baselines. We further show that by minimizing a PAC-Bayes bound with data-dependent priors [1], we obtain a self-bounded learning algorithm with numerically tight bounds. In the linear model, we show that a PAC-Bayes generalization error bound is controlled by the magnitude of the change in feature alignment between the \u201cprior\u201d and \u201cposterior\u201d data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46183987",
                        "name": "Soufiane Hayou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(1) Unstructured pruning : Rewind Renda et al. (2020)."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ea9bf418e9612c3f575f18297afde88fd29a5d53",
                "externalIds": {
                    "DBLP": "conf/acml/ZengLSLFL21",
                    "CorpusId": 245355750
                },
                "corpusId": 245355750,
                "publicationVenue": {
                    "id": "2486528b-036c-4f3c-953f-c574eb381d12",
                    "name": "Asian Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "Asian Conf Mach Learn",
                        "ACML"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=40"
                },
                "url": "https://www.semanticscholar.org/paper/ea9bf418e9612c3f575f18297afde88fd29a5d53",
                "title": "Iterative Deep Model Compression and Acceleration in the Frequency Domain",
                "abstract": "Deep Convolutional Neural Networks (CNNs) are successfully applied in many complex tasks, but their storage and huge computational costs hinder their deployment on edge devices. CNN model compression techniques have been widely studied in the past five years, most of which are conducted in the spatial domain. Inspired by the sparsity and low-rank properties of weight matrices in the frequency domain, we propose a novel frequency pruning framework for model compression and acceleration while maintaining highperformance. We firstly apply Discrete Cosine Transform (DCT) on convolutional kernels and train them in the frequency domain to get sparse representations. Then we propose an iterative model compression method to decompose the frequency matrices with a samplebased low-rank approximation algorithm, and then fine-tune and recompose the low-rank matrices gradually until a predefined compression ratio is reached. We further demonstrate that model inference can be conducted with the decomposed frequency matrices, where model parameters and inference cost can be significantly reduced. Extensive experiments using well-known CNN models based on three open datasets show that the proposed method outperforms the state-of-the-arts in reduction of both the number of parameters and floating-point operations (FLOPs) without sacrificing too much model accuracy.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2111152177",
                        "name": "Y. Zeng"
                    },
                    {
                        "authorId": "2110646070",
                        "name": "Xu-Sheng Liu"
                    },
                    {
                        "authorId": "1643532168",
                        "name": "Lintan Sun"
                    },
                    {
                        "authorId": "2108677420",
                        "name": "Wenzhong Li"
                    },
                    {
                        "authorId": "2149502045",
                        "name": "Yuchu Fang"
                    },
                    {
                        "authorId": "145174807",
                        "name": "Sanglu Lu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is not only resource-intensive but there is also an on-going discussion on whether and how we should fine-tune [36] or rewind to initial weights [23] or train from scratch [29], making it difficult to prefer one approach over another."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1234d030ba105d8fae9fe34f21474f2ff19ddaf4",
                "externalIds": {
                    "CorpusId": 247613192
                },
                "corpusId": 247613192,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1234d030ba105d8fae9fe34f21474f2ff19ddaf4",
                "title": "Learning Compact Representations of Neural Networks using DiscriminAtive Masking (DAM)",
                "abstract": "A central goal in deep learning is to learn compact representations of features at every layer of a neural network, which is useful for both unsupervised representation learning and structured network pruning. While there is a growing body of work in structured pruning, current state-of-the-art methods suffer from two key limitations: (i) instability during training, and (ii) need for an additional step of fine-tuning, which is resource-intensive. At the core of these limitations is the lack of a systematic approach that jointly prunes and refines weights during training in a single stage, and does not require any fine-tuning upon convergence to achieve state-of-the-art performance. We present a novel single-stage structured pruning method termed DiscriminAtive Masking (DAM). The key intuition behind DAM is to discriminatively prefer some of the neurons to be refined during the training process, while gradually masking out other neurons. We show that our proposed DAM approach has remarkably good performance over a diverse range of applications in representation learning and structured pruning, including dimensionality reduction, recommendation system, graph representation learning, and structured pruning for image classification. We also theoretically show that the learning objective of DAM is directly related to minimizing the L0 norm of the masking layer. All of our codes and datasets are available https://github.com/jayroxis/ dam-pytorch.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49221613",
                        "name": "Jie Bu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f3f8064175950225f0c5ed1d45a24bd8df53296a",
                "externalIds": {
                    "CorpusId": 249985174
                },
                "corpusId": 249985174,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f3f8064175950225f0c5ed1d45a24bd8df53296a",
                "title": "R ECONCILING S PARSE AND S TRUCTURED P RUNING : A S CIENTIFIC S TUDY OF B LOCK S PARSITY",
                "abstract": "We systematically study block sparsity as a middle ground that interpolates between (a) sparse pruning of individual weights in an unstructured manner and (b) structured pruning at larger granularities, such as neurons or channels. These two forms of pruning are known to exhibit different properties. Unstructured pruning reaches higher sparsities than neuron or channel pruning before losing accuracy; however, neuron and channel pruning is easier to exploit to reduce the real-world costs of inference. We investigate block sparsity as a potential middle ground between these alternatives. Unstructured pruning is also known to be sensitive to reinitialization, while neuron- and channel-pruned networks can train to the same accuracy even after being reinitialized. We use block sparsity to examine the extent to which these two regimes apply at intermediate pruning granularities. We observe an intrinsic tradeoff between granularity and accuracy: increasing block size results in a gradual decrease in accuracy, even for the smallest blocks. Sur-prisingly, we also obtain similar accuracy across all pruning granularities when reinitializing; for larger blocks, this is closer to the accuracy without reinitializing, potentially explaining the neuron and channel pruning results.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2172788113",
                        "name": "Reconciling Sparse"
                    },
                    {
                        "authorId": "2172848483",
                        "name": "Block Sparsity"
                    },
                    {
                        "authorId": "46930863",
                        "name": "Arlene Siswanto"
                    },
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "1701041",
                        "name": "Michael Carbin"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2019a), fine tuning the learning rates (Renda et al., 2020), more efficient training (You et al.",
                "In the subsequent studies, the focus goes on finding this lottery ticket for more competitive tasks by pruning with weight rewinding(Frankle et al., 2019a), fine tuning the learning rates (Renda et al., 2020), more efficient training (You et al., 2019; Brix et al., 2020; Girish et al., 2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d56026be1ff8a6cd7790d9d1a84c9f14e75e5e86",
                "externalIds": {
                    "CorpusId": 250981501
                },
                "corpusId": 250981501,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d56026be1ff8a6cd7790d9d1a84c9f14e75e5e86",
                "title": "R EVISITING THE L OTTERY T ICKET H YPOTHESIS : A R AMANUJAN G RAPH P ERSPECTIVE",
                "abstract": "Neural networks for machine learning applications often yield to weight pruning resulting in a sparse subnetwork that is adequate for a given task. Retraining these \u2018lottery ticket\u2019 subnetworks from their initialization minimizes the computational burden while preserving the test set accuracy of the original network. The existing literature only confirms that pruning is needed and it can be achieved up to a certain sparsity. We analyze the pruned network in the context of the properties of Ramanujan expander graphs. We consider the feed-forward network (both multi-layer perceptron and convolutional network) as a series of bipartite graphs which establish the connection from input to output. Now, as the fraction of remaining weights reduce with increasingly aggressive pruning, distinct regimes are observed: initially, no significant decrease in accuracy is demonstrated, and then the accuracy starts dropping rapidly. We empirically show that in the first regime, the pruned lottery ticket sub-network remains a Ramanujan graph. Subsequently, with the loss of Ramanujan graph property, accuracy begins to reduce sharply. This characterizes an absence of resilient connectivity in the pruned sub-network. We also propose a modified iterative pruning algorithm which removes edges in only the layers that are Ramanujan graphs thus preserving global connectivity even for heavily pruned networks. We perform experiments on MNIST and CIFAR10 datasets using different established feed-forward architectures to support the criteria for obtaining the winning ticket using the proposed algorithm.",
                "year": 2021,
                "authors": []
            }
        },
        {
            "contexts": [
                "For some experiments, we also modified line 6 for techniques such as late rewinding and learning rate rewinding (Renda et al., 2020).",
                "We ran late rewinding experiments from Renda et al. (2020), where instead of rewinding the weights to \u03b80 in line 6 of Algorithm A1, we set the weights to their values at epoch 2.",
                "Our main experiments are mainly based on the lottery ticket procedure from Frankle & Carbin (2018) along with follow up work (Renda et al., 2020) for unstructured pruning.",
                "Our learning rate rewinding experiments also encapsulate the fine-tuning experiments (Renda et al., 2020).",
                "Methods that fine-tune weights after pruning typically train at a smaller learning rate than the training phase to find the mask (Han et al., 2015; Renda et al., 2020), but other hyperparameters are held constant.",
                "1 MODIFIED PRUNING PROCEDURES Our main experiments are mainly based on the lottery ticket procedure from Frankle & Carbin (2018) along with follow up work (Renda et al., 2020) for unstructured pruning.",
                "This also explains why fine-tuning in Renda et al. (2020) did worse than learning rate rewinding: a larger LReval is better, and fine-tuning is effectively the same as learning rate rewinding but with a smaller and worse LReval."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "af49b9fd741c16964193e7be780ca34b4e2a637d",
                "externalIds": {
                    "CorpusId": 251795137
                },
                "corpusId": 251795137,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/af49b9fd741c16964193e7be780ca34b4e2a637d",
                "title": "U NCOVERING THE IMPACT OF HYPERPARAMETERS FOR GLOBAL MAGNITUDE PRUNING",
                "abstract": "A common paradigm in model pruning is to train a model, prune, and then either fine-tune or, in the lottery ticket framework, reinitialize and retrain. Prior work has implicitly assumed that the best training configuration for model evaluation is also the best configuration for mask discovery. However, what if a training configuration which yields worse performance actually yields a mask which trains to higher performance? To test this, we decoupled the hyperparameters for mask discovery (Hfind) and mask evaluation (Heval). Using unstructured magnitude pruning on vision classification tasks, we discovered the \u201cdecoupled find-eval phenomenon,\u201d in which certain Hfind values lead to models which have lower performance, but generate masks with substantially higher eventual performance compared to using the same hyperparameters for both stages. We show that this phenomenon holds across a number of models, datasets, configurations, and also for one-shot structured pruning. Finally, we demonstrate that different Hfind values yield masks with materially different layerwise pruning ratios and that the decoupled find-eval phenomenon is causally mediated by these ratios. Our results demonstrate the practical utility of decoupling hyperparameters and provide clear insights into the mechanisms underlying this counterintuitive effect.",
                "year": 2020,
                "authors": []
            }
        },
        {
            "contexts": [
                "Moreover, the introduction of weight rewinding is spurred by the so-called rewinding ticket (Renda et al., 2020; Frankle et al., 2020).",
                "Following (Frankle & Carbin, 2019; Renda et al., 2020), IMP iteratively prunes p 1 n(i) (%) non-zero weights of m(i\u22121) \u03b8(i\u22121) over n rounds at T .",
                "Following (Frankle & Carbin, 2019; Renda et al., 2020), IMP iteratively prunes p 1\nn(i) (",
                "More Technical Details of Top-down Pruning In our implementation, we set p 1\nn(i) = 20% as (Frankle & Carbin, 2019; Renda et al., 2020) and adjust {n(i)} to control the pruning schedule of IMP over sequential tasks.",
                "(Renda et al., 2020) further compares different retraining techniques and endorses the effectiveness of rewinding."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "e7a09a66f74a5eb0125ca96cc49623786156c99b",
                "externalIds": {
                    "CorpusId": 250075828
                },
                "corpusId": 250075828,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e7a09a66f74a5eb0125ca96cc49623786156c99b",
                "title": "L ONG L IVE THE L OTTERY : T HE E XISTENCE OF W IN NING T ICKETS IN L IFELONG L EARNING",
                "abstract": "The lottery ticket hypothesis states that a highly sparsified sub-network can be trained in isolation, given the appropriate weight initialization. This paper extends that hypothesis from one-shot task leaning, and demonstrates for the first time that such extremely compact and independently trainable sub-networks can be also identified in the lifelong learning scenario, which we call lifelong tickets. We show that the resulting lifelong ticket can further be leveraged to improve the performance of learning over continual tasks. However, it is highly non-trivial to conduct network pruning in the lifelong setting. Two critical roadblocks arise: i) As many tasks now arrive sequentially, finding tickets in a greedy weight pruning fashion will inevitably suffer from the intrinsic bias, that the earlier emerging tasks impact more; ii) As lifelong learning is consistently challenged by catastrophic forgetting, the compact network capacity of tickets might amplify the risk of forgetting. In view of those, we introduce two pruning options, e.g., top-down and bottom-up, for finding lifelong tickets. Compared to the top-down pruning that extends vanilla (iterative) pruning over sequential tasks, we show that the bottomup one, which can dynamically shrink and (re-)expand model capacity, effectively avoids the undesirable excessive pruning in the early stage. We additionally introduce lottery teaching that further overcomes forgetting via knowledge distillation aided by external unlabeled data. Unifying those ingredients, we demonstrate the existence of very competitive lifelong tickets, e.g., achieving 3\u2212 8% of the dense model size with even higher accuracy, compared to strong class-incremental learning baselines on CIFAR-10/CIFAR-100/Tiny-ImageNet datasets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2094291466",
                        "name": "Long Live"
                    },
                    {
                        "authorId": "1403334137",
                        "name": "lifeloNg leaRNiNg"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The early phase of the project will consist of implementing a range of established methods for network sparsification [3, 8, 11, 16], in order to put in place a solid testbed for our projects.",
                "A great body of work [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16] has been dedicated to this task."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0c7b76ef2e7353d348aa8e2c784bd3dd24a66d1b",
                "externalIds": {
                    "CorpusId": 227125605
                },
                "corpusId": 227125605,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0c7b76ef2e7353d348aa8e2c784bd3dd24a66d1b",
                "title": "A Principled Investigation of the Lottery Ticket Hypothesis for Deep Neural Networks",
                "abstract": "Sparsifying deep neural networks is a fundamental challenge in deep learning. Deploying massive trained models into the wild faces significant obstacles, since even for basic tasks like image classification, the size of the model can easily become prohibitively large. As the model size directly affects the time required to perform inference, methods for reducing network size by removing connections and neurons have become highly important. The Lottery Ticket Hypothesis (LTH) defined a suprising phenomenon that was noticed when attempting to obtain such sparse models. In a recent paper, Frankle and Carbin [3] advanced the hypothesis that dense, randomly initialized neural networks contain small subnetworks which, when trained in isolation, reach training accuracy comparable to the original network in the same number of passes. Hence finding such a small subnetwork (i.e. winning lottery ticket) would enable one to increase efficiency for both training and inference. While this highly intriguing result posits the existence of good sparse subnetworks, it is unclear how to find them. A great body of work [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16] has been dedicated to this task. However, current results are mostly empirical and present understanding of why is sparsification even possible is far from being complete.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2869958",
                        "name": "Adrian Vladu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent works show that their parameters can be reduced by more than 90% without accuracy drop [4], [5].",
                "One of popular frameworks for compressing a neural network consists of three steps: pre-training the network, removing unimportant components, and re-training the remaining structure [4], [8], [5]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "19dcfe5d78c518f2658806e47d3dbc09428a7170",
                "externalIds": {
                    "CorpusId": 231619605
                },
                "corpusId": 231619605,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/19dcfe5d78c518f2658806e47d3dbc09428a7170",
                "title": "Pruning Deep Neural Networks with `0-constrained Optimization",
                "abstract": "Deep neural networks (DNNs) give state-of-the-art accuracy in many tasks, but they can require large amounts of memory storage, energy consumption, and long inference times. Modern DNNs can have hundreds of million parameters, which make it difficult for DNNs to be deployed in some applications with low-resource environments. Pruning redundant connections without sacrificing accuracy is one of popular approaches to overcome these limitations. We propose two `0-constrained optimization models for pruning deep neural networks layerby-layer. The first model is devoted to a general activation function, while the second one is specifically for a ReLU. We introduce an efficient cutting plane algorithm to solve the latter to optimality. Our experiments show that the proposed approach achieves competitive compression rates over several state-of-the-art baseline methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "41128347",
                        "name": "D. Phan"
                    },
                    {
                        "authorId": "144274166",
                        "name": "Lam M. Nguyen"
                    },
                    {
                        "authorId": "144547425",
                        "name": "Nam H. Nguyen"
                    },
                    {
                        "authorId": "1682581",
                        "name": "J. Kalagnanam"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In a similar setting, as demonstrated in the model pruning literature (Gordon et al., 2018; Liu et al., 2019b; Morcos et al., 2019; Renda et al., 2020), having different pruning ratios for different layers of",
                "In a similar setting, as demonstrated in the model pruning literature (Gordon et al., 2018; Liu et al., 2019b; Morcos et al., 2019; Renda et al., 2020), having different pruning ratios for different layers of\nthe network can further improve results over a single ratio across layers."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2b39434f84fed4d7187d8edb92d37c6538c5ed9f",
                "externalIds": {
                    "CorpusId": 236783748
                },
                "corpusId": 236783748,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2b39434f84fed4d7187d8edb92d37c6538c5ed9f",
                "title": "PARECO: PARETO-AWARE CHANNEL OPTIMIZATION",
                "abstract": "Slimmable neural networks provide a flexible trade-off front between prediction error and computational cost (such as the number of floating-point operations or FLOPs) with the same storage cost as a single model. They have been proposed recently for resource-constrained settings such as mobile devices. However, current slimmable neural networks use a single width-multiplier for all the layers to arrive at sub-networks with different performance profiles, which neglects that different layers affect the network\u2019s prediction accuracy differently and have different FLOP requirements. Hence, developing a principled approach for deciding widthmultipliers across different layers could potentially improve the performance of slimmable networks. To allow for heterogeneous width-multipliers across different layers, we formulate the problem of optimizing slimmable networks from a multiobjective optimization lens, which leads to a novel algorithm for optimizing both the shared weights and the width-multipliers for the sub-networks. We perform extensive empirical analysis with 15 network and dataset combinations and two types of cost objectives, i.e., FLOPs and memory footprint, to demonstrate the effectiveness of the proposed method compared to existing alternatives. Quantitatively, improvements up to 1.7% and 8% in top-1 accuracy on the ImageNet dataset can be attained for MobileNetV2 considering FLOPs and memory footprint, respectively. Our results highlight the potential of optimizing the channel counts for different layers jointly with the weights for slimmable networks.",
                "year": 2020,
                "authors": []
            }
        },
        {
            "contexts": [
                "\u2026rate schedules other than compressing the original learning rate schedule from the fist phase of training; it is plausible that other learning rate schedules better tuned to the technique or network could train even faster (Renda et al., 2020), but we do not evaluate these alternative schedules.",
                "We also do not evaluate learning rate schedules other than compressing the original learning rate schedule from the fist phase of training; it is plausible that other learning rate schedules better tuned to the technique or network could train even faster (Renda et al., 2020), but we do not evaluate these alternative schedules."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5e55119ee4d1ae2325efa29bea29a0fa6c80cc23",
                "externalIds": {
                    "CorpusId": 236784377
                },
                "corpusId": 236784377,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5e55119ee4d1ae2325efa29bea29a0fa6c80cc23",
                "title": "FAST BINARIZED NEURAL NETWORK TRAINING",
                "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more timeand resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between 1.26\u00d7 and 1.61\u00d7 faster than when training a binarized network from scratch using standard low-precision training.",
                "year": 2020,
                "authors": []
            }
        },
        {
            "contexts": [
                "Ex157 amples include weight pruning (dropping/skipping weights or filters) [34, 35, 10, 11], perforated 158 convolutions [36, 37] (skipping and interpolating some output computations), integer quantiza159 tion [35] (INT8, INT4) and others.",
                "neural network models [10, 11, 12] (used for visual perception) can be used to trade off accuracy 43 for computation time improvements in closed-loop visual navigation tasks, even making it possible 44 to run both navigation and a real-time object detection task concurrently (three CNNs in total) on a 45 single Raspberry Pi 4, without significantly hurting system robustness."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "af90b034313bdc09667991fb4d39c76fe30973e9",
                "externalIds": {
                    "CorpusId": 237259342
                },
                "corpusId": 237259342,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/af90b034313bdc09667991fb4d39c76fe30973e9",
                "title": "ApproxRobotics: The Cost and Accuracy Tradeoff for Small Mobile AgRobots",
                "abstract": "Autonomous robots are increasingly relying on high-dimensional vi1 sual information for perception, planning, and control. The deep neural network 2 pipelines that perform inference tasks on these robots require significant com3 putational resources, which increases the robot energy consumption and the cost. 4 This has been a barrier in adopting learning based methods in cost-constrained do5 mains, such as agriculture. In this paper we show that structured pruning on neural 6 networks can enable agricultural robots to employ lower-cost computational hard7 ware, without losing task robustness in visual navigation and visual phenotyping 8 tasks. We expose key trade-offs between computational cost and prediction accu9 racy in the perception module of an autonomous navigation stack in a production 10 agricultural robot. Our key finding is that, for closed-loop control systems used 11 in robots, it is often possible to relax the accuracy of CNNs used for computer 12 vision without significantly hurting the end-to-end task outcomes. We show that 13 computational approximations enable us to deploy a state-of-the-art vision-based 14 autonomous navigation pipeline and a real-time video analytics task on a single 15 resource-constrained Raspberry Pi4. Our results show that it is possible to use 16 learning-based control for small mobile robots using low-cost compute hardware. 17",
                "year": null,
                "authors": []
            }
        },
        {
            "contexts": [
                "To scale up LTH for large networks and large-scale datasets, weight rewinding techniques (Renda et al., 2020; Frankle et al., 2020a) is proposed.",
                "For a fair comparison, we follow the standard implementations and hyperparameters in (Renda et al., 2020) for OMP, LTH, RP, and PI experiments, as shown in Table 1."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7c0b26c232b47b8567be1cdf675154d271209cae",
                "externalIds": {
                    "CorpusId": 250580346
                },
                "corpusId": 250580346,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7c0b26c232b47b8567be1cdf675154d271209cae",
                "title": "Can You Win Everything with A Lottery Ticket?",
                "abstract": "Lottery ticket hypothesis (LTH) has demonstrated to yield independently trainable and highly sparse neural networks (a.k.a. winning tickets ), whose test set accuracies can be surprisingly on par or even better than dense models. However, accuracy is far from the only evaluation metric, and perhaps not always the most important one. Hence it might be myopic to conclude that a sparse subnetwork can replace its dense counterpart, even if the accuracy is preserved. Spurred by that, we perform the first comprehensive assessment of lottery tickets from diverse aspects beyond test accuracy, including (i) generalization to distribution shifts, (ii) prediction uncertainty, (iii) interpretability, and (iv) geometry of loss landscapes. With extensive experiments across datasets {CIFAR-10, CIFAR-100, and ImageNet}, model architectures, as well as seven sparsification methods, we thoroughly characterize the trade-off between model sparsity and the all-dimension model capabilities. We find that an appropriate sparsity (e.g., 20% \u223c 99 . 53%) can yield the winning ticket to perform comparably or even better in all above four aspects , although some aspects (generalization to certain distribution shifts, and uncertainty) appear more sensitive to the sparsification than others. We term it as a LTH-PASS . Overall, our results endorse choosing a good sparse subnetwork of a larger dense model, over directly training a small dense model of similar parameter counts. We hope that our study can offer more in-depth insights on pruning, for researchers and engineers who seek to incorporate sparse neural networks for user-facing deployments. Codes are available in https://github.com/VITA-Group/LTH-Pass .",
                "year": null,
                "authors": [
                    {
                        "authorId": "2109338656",
                        "name": "Zhenyu (Allen) Zhang"
                    },
                    {
                        "authorId": "2147189509",
                        "name": "Jerome Friedman"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c8d6a12a973d9624699ef3e5a9e0085b4ff62533",
                "externalIds": {
                    "CorpusId": 263222223
                },
                "corpusId": 263222223,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c8d6a12a973d9624699ef3e5a9e0085b4ff62533",
                "title": "AP: Selective Activation for De-sparsifying Pruned Networks",
                "abstract": "The rectified linear unit (ReLU) is a highly successful activation function in neural networks as it allows networks to easily obtain sparse representations, which reduces overfitting in overparameterized networks. However, in the context of network pruning, we find that the sparsity introduced by ReLU, which we quantify by a term called dynamic dead neuron rate (DNR), is not beneficial for the pruned network. Interestingly, the more the network is pruned, the smaller the dynamic DNR becomes after optimization. This motivates us to propose a method to explicitly reduce the dynamic DNR for the pruned network, i.e., de-sparsify the network. We refer to our method as Activate-while-Pruning (AP). We note that AP does not function as a stand-alone method, as it does not evaluate the importance of weights. Instead, it works in tandem with existing pruning methods and aims to improve their performance by selective activation of nodes to reduce the dynamic DNR. We conduct extensive experiments using various popular networks (e.g., ResNet, VGG, DenseNet, Mo-bileNet) via two classical and three competitive pruning methods. The experimental results on public datasets (e.g., CIFAR-10, CIFAR-100) suggest that AP works well with existing pruning methods and improves the performance by 3% - 4%. For larger scale datasets (e.g., ImageNet) and competitive networks (e.g., vision transformer), we observe an improvement of 2% - 3% with AP as opposed to without. Lastly, we conduct an ablation study and a substitution study to examine the effectiveness of the components comprising AP",
                "year": null,
                "authors": [
                    {
                        "authorId": "50151902",
                        "name": "Shiyu Liu"
                    },
                    {
                        "authorId": "2978590",
                        "name": "Rohan Ghosh"
                    },
                    {
                        "authorId": "1770486",
                        "name": "M. Motani"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "427b88e24dd5a01460626d212780bf552f8ff876",
                "externalIds": {
                    "CorpusId": 263768788
                },
                "corpusId": 263768788,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/427b88e24dd5a01460626d212780bf552f8ff876",
                "title": "Single-Shot Pruning for Pre-trained Models: Rethinking the Importance of Magnitude Pruning",
                "abstract": "Transformer models with large-scale pre-training have performed excellently in various computer vision tasks. However, such models are huge and dif\ufb01cult to apply to mobile devices with limited computational resources. More-over, the computational cost of \ufb01ne-tuning is high when the model is optimized for a downstream task. Therefore, our goal is to compress the large pre-trained models with minimal performance degradation before \ufb01ne-tuning. In this paper, we \ufb01rst present the preliminary experimental results on the parameter change by using pre-trained or scratch models when training in a downstream task. We found that the parameter magnitudes of pre-trained models remained largely unchanged before and after training compared with scratch models. With this in mind, we propose an unstructured pruning method for pre-trained models. Our method evaluates the parameters without training and prunes in a single shot to obtain sparse models. Our experiment re-sults show that the sparse model pruned by our method has higher accuracy is more than previous methods on the CIFAR-10, CIFAR-100, and ImageNet classi\ufb01cation tasks.",
                "year": null,
                "authors": [
                    {
                        "authorId": "2256419008",
                        "name": "Hirokazu Kohama"
                    },
                    {
                        "authorId": "2256418792",
                        "name": "Hiroaki Minoura"
                    },
                    {
                        "authorId": "134790239",
                        "name": "Tsubasa Hirakawa"
                    },
                    {
                        "authorId": "1687819",
                        "name": "Takayoshi Yamashita"
                    },
                    {
                        "authorId": "1687968",
                        "name": "H. Fujiyoshi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This phenomenon has been extensively discussed in the literature, as evidenced by studies such as [17,21]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6f618821309562504b723d73474c06623c1d7bf6",
                "externalIds": {
                    "CorpusId": 263778920
                },
                "corpusId": 263778920,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6f618821309562504b723d73474c06623c1d7bf6",
                "title": "Shannon Strikes Again! Entropy-based Pruning in Deep Neural Networks for Transfer Learning under Extreme Memory and Computation Budgets",
                "abstract": "Deep neural networks have become the de-facto standard across various computer science domains. Nonethe-less, effectively training these deep networks remains challenging and resource-intensive. This paper investigates the ef\ufb01cacy of pruned deep learning models in transfer learning scenarios under extremely low memory bud-gets, tailored for TinyML models. Our study reveals that the source task\u2019s model with the highest activation entropy outperforms others in the target task. Motivated by this, we propose an entropy-based Ef\ufb01cient Neural Transfer with Reduced Overhead via PrunIng (ENTROPI) al-gorithm. Through comprehensive experiments on diverse models (ResNet18 and MobileNet-v3) and target datasets (CIFAR-100, VLCS, and PACS), we substantiate the superior generalization achieved by transfer learning from the entropy-pruned model. Quantitative measures for entropy provide valuable insights into the reasons behind the observed performance improvements. The results underscore ENTROPI\u2019s potential as an ef\ufb01cient solution for enhancing generalization in data-limited transfer learning tasks.",
                "year": null,
                "authors": [
                    {
                        "authorId": "2221055515",
                        "name": "Gabriele Spadaro"
                    },
                    {
                        "authorId": "2100075726",
                        "name": "R. Renzulli"
                    },
                    {
                        "authorId": "16248506",
                        "name": "Andrea Bragagnolo"
                    },
                    {
                        "authorId": "39802095",
                        "name": "Jhony H. Giraldo"
                    },
                    {
                        "authorId": "1768915",
                        "name": "A. Fiandrotti"
                    },
                    {
                        "authorId": "1691141",
                        "name": "Marco Grangetto"
                    },
                    {
                        "authorId": "47376816",
                        "name": "Enzo Tartaglione"
                    }
                ]
            }
        }
    ]
}