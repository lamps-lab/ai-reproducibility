{
    "offset": 0,
    "data": [
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f4becd9b3e4efbc23ad08ebefdb6662552650e02",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-04875",
                    "ArXiv": "2309.04875",
                    "DOI": "10.48550/arXiv.2309.04875",
                    "CorpusId": 261682179
                },
                "corpusId": 261682179,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f4becd9b3e4efbc23ad08ebefdb6662552650e02",
                "title": "Approximating ReLU on a Reduced Ring for Efficient MPC-based Private Inference",
                "abstract": "Secure multi-party computation (MPC) allows users to offload machine learning inference on untrusted servers without having to share their privacy-sensitive data. Despite their strong security properties, MPC-based private inference has not been widely adopted in the real world due to their high communication overhead. When evaluating ReLU layers, MPC protocols incur a significant amount of communication between the parties, making the end-to-end execution time multiple orders slower than its non-private counterpart. This paper presents HummingBird, an MPC framework that reduces the ReLU communication overhead significantly by using only a subset of the bits to evaluate ReLU on a smaller ring. Based on theoretical analyses, HummingBird identifies bits in the secret share that are not crucial for accuracy and excludes them during ReLU evaluation to reduce communication. With its efficient search engine, HummingBird discards 87--91% of the bits during ReLU and still maintains high accuracy. On a real MPC setup involving multiple servers, HummingBird achieves on average 2.03--2.67x end-to-end speedup without introducing any errors, and up to 8.64x average speedup when some amount of accuracy degradation can be tolerated, due to its up to 8.76x communication reduction.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "10995410",
                        "name": "Kiwan Maeng"
                    },
                    {
                        "authorId": "2238952548",
                        "name": "G. E. Suh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "2e840e763fe0f74ab1b173296a833a16c59624b0",
                "externalIds": {
                    "ArXiv": "2309.02820",
                    "DBLP": "journals/corr/abs-2309-02820",
                    "DOI": "10.1109/tmc.2023.3312304",
                    "CorpusId": 261557346
                },
                "corpusId": 261557346,
                "publicationVenue": {
                    "id": "4e46790b-e240-4236-9b8d-a70ed74f900a",
                    "name": "IEEE Transactions on Mobile Computing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Mob Comput"
                    ],
                    "issn": "1536-1233",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=7755",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tmc"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2e840e763fe0f74ab1b173296a833a16c59624b0",
                "title": "Roulette: A Semantic Privacy-Preserving Device-Edge Collaborative Inference Framework for Deep Learning Classification Tasks",
                "abstract": "Deep learning classifiers are crucial in the age of artificial intelligence. The device-edge-based collaborative inference has been widely adopted as an efficient framework for promoting its applications in IoT and 5G/6G networks. However, it suffers from accuracy degradation under non-i.i.d. data distribution and privacy disclosure. For accuracy degradation, direct use of transfer learning and split learning is high cost and privacy issues remain. For privacy disclosure, cryptography-based approaches lead to a huge overhead. Other lightweight methods assume that the ground truth is non-sensitive and can be exposed. But for many applications, the ground truth is the user's crucial privacy-sensitive information. In this paper, we propose a framework of Roulette, which is a task-oriented semantic privacy-preserving collaborative inference framework for deep learning classifiers. More than input data, we treat the ground truth of the data as private information. We develop a novel paradigm of split learning where the back-end DNN is frozen and the front-end DNN is retrained to be both a feature extractor and an encryptor. Moreover, we provide a differential privacy guarantee and analyze the hardness of ground truth inference attacks. To validate the proposed Roulette, we conduct extensive performance evaluations using realistic datasets, which demonstrate that Roulette can effectively defend against various attacks and meanwhile achieve good model accuracy. In a situation where the non-i.i.d. is very severe, Roulette improves the inference accuracy by 21\\% averaged over benchmarks, while making the accuracy of discrimination attacks almost equivalent to random guessing.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2225558123",
                        "name": "Jingyi Li"
                    },
                    {
                        "authorId": "32742585",
                        "name": "Guocheng Liao"
                    },
                    {
                        "authorId": "2238036628",
                        "name": "Lin Chen"
                    },
                    {
                        "authorId": "2238103706",
                        "name": "Xu Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", Xiang\u2019s [48] and Lee\u2019s [49] algorithms can be utilized to encrypt the CNN",
                "On the other hand, even if our model and transmitted images can be intercepted by attackers, we can encrypt the CNN model and secret information by encryption algorithms, e.g., Xiang\u2019s [48] and Lee\u2019s [49] algorithms can be utilized to encrypt the CNN models, and the AES and DES algorithms can be used to encrypt secret information, to further improve the security."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ed656daea71362553cebb9b827a5c9ce0d5e0c88",
                "externalIds": {
                    "DBLP": "journals/tcsv/MengJZLS23",
                    "DOI": "10.1109/TCSVT.2022.3232790",
                    "CorpusId": 255251897
                },
                "corpusId": 255251897,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ed656daea71362553cebb9b827a5c9ce0d5e0c88",
                "title": "A Robust Coverless Image Steganography Based on an End-to-End Hash Generation Model",
                "abstract": "Recently, coverless steganography algorithms have attracted increased research attention due to their ability to completely resist steganalysis algorithms. However, the existing algorithms do not attain the same robust balance against geometric and non-geometric attacks. In addition, most of the existing methods need to transmit some auxiliary information along with the stego-images, which increases the cost of the hidden information. In this paper, a robust coverless image steganography algorithm based on a hash generation model is proposed. Different from the existing methods, the hash sequences are generated by an end-to-end CNN model, where the input is the original images, and the output is the corresponding hash sequences. Therefore, no auxiliary information needs to be transmitted when hiding the secret information. Moreover, the attention mechanism and adversarial training are introduced to improve the robustness of the model. The loss function is redesigned to accommodate these operations. Finally, an index structure is built to enhance the mapping efficiency. The experimental results show that the proposed method possesses better robustness and security compared with the state-of-the-art coverless image steganography algorithms.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "152128595",
                        "name": "Laijin Meng"
                    },
                    {
                        "authorId": "1888867",
                        "name": "Xinghao Jiang"
                    },
                    {
                        "authorId": "48805686",
                        "name": "Zhenzhen Zhang"
                    },
                    {
                        "authorId": "2123874",
                        "name": "Zhaohong Li"
                    },
                    {
                        "authorId": "3307728",
                        "name": "Tanfeng Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The concept of instance encoding is widespread under many different names: learnable encryption (Huang et al., 2020; Yala et al., 2021; Xiao & Devadas, 2021; Xiang et al., 2020), split learning (Vepakomma et al.",
                "Alternatively, Yala et al. (2021); Xiao & Devadas (2021); Xiang et al. (2020) proposed to use a secret encoder network for private training, whose privacy guarantee relies on the secrecy of the encoder network.",
                "\u2026of instance encoding is widespread under many different names: learnable encryption (Huang et al., 2020; Yala et al., 2021; Xiao & Devadas, 2021; Xiang et al., 2020), split learning (Vepakomma et al., 2018; Poirot et al., 2019), split inference (Kang et al., 2017; Dong et al., 2022), and\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4668246cb777383beeeb00d8768ced10ef2bf388",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-04146",
                    "ArXiv": "2305.04146",
                    "DOI": "10.48550/arXiv.2305.04146",
                    "CorpusId": 258556819
                },
                "corpusId": 258556819,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4668246cb777383beeeb00d8768ced10ef2bf388",
                "title": "Bounding the Invertibility of Privacy-preserving Instance Encoding using Fisher Information",
                "abstract": "Privacy-preserving instance encoding aims to encode raw data as feature vectors without revealing their privacy-sensitive information. When designed properly, these encodings can be used for downstream ML applications such as training and inference with limited privacy risk. However, the vast majority of existing instance encoding schemes are based on heuristics and their privacy-preserving properties are only validated empirically against a limited set of attacks. In this paper, we propose a theoretically-principled measure for the privacy of instance encoding based on Fisher information. We show that our privacy measure is intuitive, easily applicable, and can be used to bound the invertibility of encodings both theoretically and empirically.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "10995410",
                        "name": "Kiwan Maeng"
                    },
                    {
                        "authorId": "2110228691",
                        "name": "Chuan Guo"
                    },
                    {
                        "authorId": "51110538",
                        "name": "S. Kariyappa"
                    },
                    {
                        "authorId": "145691878",
                        "name": "G. Suh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026al., 2016; Arjovsky et al., 2016; Trabelsi et al., 2018; Wang et al., 2020; Trouillon et al., 2016), complex-valued NNs also contribute to faster learning (Arjovsky et al., 2016; Danihelka et al., 2016) and increased model robustness (Danihelka et al., 2016; Yeats et al., 2021; Xiang et al., 2020).",
                "\u2026capacity (Wisdom et al., 2016; Arjovsky et al., 2016; Trabelsi et al., 2018; Wang et al., 2020; Trouillon et al., 2016), faster learning speed (Arjovsky et al., 2016; Danihelka et al., 2016), and increased model robustness (Danihelka et al., 2016; Yeats et al., 2021; Xiang et al., 2020).",
                ", 2016), and increased model robustness (Danihelka et al., 2016; Yeats et al., 2021; Xiang et al., 2020).",
                "Complexvalued NNs have also been used in privacy detection (Xiang et al., 2020) and knowledge graph completion (Trouillon et al., 2016; Trouillon & Nickel, 2017).",
                ", 2016) and increased model robustness (Danihelka et al., 2016; Yeats et al., 2021; Xiang et al., 2020).",
                "Complex-valued neural networks (NNs) have been long studied (Georgiou & Koutsougeras, 1992; Nitta, 2002; Hirose, 2011; Trabelsi et al., 2018; Xiang et al., 2020; Yang et al., 2020) with various NN building blocks including RNN (Wisdom et al., 2016), CNN (Guberman, 2016) and Transformers (Yang et\u2026",
                "Complexvalued NNs have also been used in privacy detection (Xiang et al., 2020) and knowledge graph completion (Trouillon et al.",
                "Complex values have been used in various NNs across domains (Arjovsky et al., 2016; Danihelka et al., 2016; Wisdom et al., 2016; Trouillon et al., 2016; Hirose, 2011; Trabelsi et al., 2018; Xiang et al., 2020; Yang et al., 2020; Guberman, 2016; Wang et al., 2019).",
                "Complex-valued neural networks (NNs) have been long studied (Georgiou & Koutsougeras, 1992; Nitta, 2002; Hirose, 2011; Trabelsi et al., 2018; Xiang et al., 2020; Yang et al., 2020) with various NN building blocks including RNN (Wisdom et al."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b3771b815de2c7ce9ea5763a0d496fe74044d821",
                "externalIds": {
                    "ArXiv": "2302.13812",
                    "DBLP": "journals/corr/abs-2302-13812",
                    "DOI": "10.48550/arXiv.2302.13812",
                    "CorpusId": 257219649
                },
                "corpusId": 257219649,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b3771b815de2c7ce9ea5763a0d496fe74044d821",
                "title": "Adapting Pre-trained Language Models for Quantum Natural Language Processing",
                "abstract": "The emerging classical-quantum transfer learning paradigm has brought a decent performance to quantum computational models in many tasks, such as computer vision, by enabling a combination of quantum models and classical pre-trained neural networks. However, using quantum computing with pre-trained models has yet to be explored in natural language processing (NLP). Due to the high linearity constraints of the underlying quantum computing infrastructures, existing Quantum NLP models are limited in performance on real tasks. We fill this gap by pre-training a sentence state with complex-valued BERT-like architecture, and adapting it to the classical-quantum transfer learning scheme for sentence classification. On quantum simulation experiments, the pre-trained representation can bring 50\\% to 60\\% increases to the capacity of end-to-end quantum models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108273524",
                        "name": "Qiuchi Li"
                    },
                    {
                        "authorId": "2894465",
                        "name": "Benyou Wang"
                    },
                    {
                        "authorId": "2109481405",
                        "name": "Yudong Zhu"
                    },
                    {
                        "authorId": "1784800",
                        "name": "C. Lioma"
                    },
                    {
                        "authorId": "30738758",
                        "name": "Qun Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The attacker merely accesses the smashed data [11].",
                "Most of the existing works have been devoted to privacypreserving inference [10], [11] rather than split learning, as inference only requires the features transmitted in the one-time forward propagation is privacy-preserving, while it is much more difficult to protect the training data in multiple rounds of forward and backward propagations.",
                "An inversion attack can be launched to infer private information from smashed data [11].",
                "Differential privacy and other transform-based approaches applied to the training inputs [13], [14], or intermediate features [10], [11], usually sacrifice significant accuracy performance to achieve the privacy guarantee."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f53e5cdc7eaec3b0a6388cb0f9570f9e66e45cf9",
                "externalIds": {
                    "DBLP": "conf/icdm/YaoXXYC22",
                    "DOI": "10.1109/ICDM54844.2022.00074",
                    "CorpusId": 256463409
                },
                "corpusId": 256463409,
                "publicationVenue": {
                    "id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
                    "name": "Industrial Conference on Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Ind Conf Data Min",
                        "ICDM"
                    ],
                    "url": "http://www.data-mining-forum.de/"
                },
                "url": "https://www.semanticscholar.org/paper/f53e5cdc7eaec3b0a6388cb0f9570f9e66e45cf9",
                "title": "Privacy-Preserving Split Learning via Patch Shuffling over Transformers",
                "abstract": "We focus on the privacy-preserving problem in split learning in this work. In vanilla split learning, a neural network is split to different devices to be trained, risking leaking the private training data in the process. We novelly propose a patch shuffling scheme on transformers to preserve training data privacy, yet without degrading overall model performance. Formal privacy guarantees are provided and we further introduce the batch shuffling and the spectral shuffling schemes to enhance the guarantee. We show through experiments that our methods successfully defend the black-box, white-box, and adaptive attacks in split learning, with superior performance over baselines, and are efficient to deploy with negligible overhead compared to the vanilla split learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2051655806",
                        "name": "Dixi Yao"
                    },
                    {
                        "authorId": "1850278",
                        "name": "Liyao Xiang"
                    },
                    {
                        "authorId": "2203966676",
                        "name": "Hengyuan Xu"
                    },
                    {
                        "authorId": "2111972696",
                        "name": "Hang Ye"
                    },
                    {
                        "authorId": "2109014970",
                        "name": "Yingqi Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, some schemes [4, 16, 41, 49] employ several technologies, such as adversarial learning, domain-transformation, and encryption, to achieve privacy-preserving inference."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6e7206a2d2538c0d57140e6b5a7bf1b58787ea7d",
                "externalIds": {
                    "DBLP": "conf/cikm/SunLPC22",
                    "DOI": "10.1145/3511808.3557450",
                    "CorpusId": 252904972
                },
                "corpusId": 252904972,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/6e7206a2d2538c0d57140e6b5a7bf1b58787ea7d",
                "title": "Serpens: Privacy-Preserving Inference through Conditional Separable of Convolutional Neural Networks",
                "abstract": "With the extensive usage of convolutional neural networks (CNNs), privacy issues within practical applications have attracted much attention, especially when deep learning services are provided by third-party clouds. Many private inference schemes have been proposed, but their overheads are still too large. In this work, we find that the inference procedure of CNNs can be separated and performed synergistically by many parties. Following this observation, we present a pair of novel notions, namely separable and conditional separable, to tell whether a layer in CNNs can be exactly computed over multiple parties or not. Besides, we also prove that CNNs are conditionally separable. Accordingly, we propose Serpens, a private inference framework under multi-server settings. Serpens reduces the overhead of linear layers to almost zero, and now the computing bottleneck is ReLU. To address that, we design two secure ReLU protocols based on homomorphic encryption and random masks for two- and three-server settings. Experimental results show that Serpens is 78x-105x faster than the state-of-the-art private inference scheme in the two-server setting, and the superiority of Serpens is even larger in the three-server setting, only 11x-64x slower than performing the same inference over plaintext images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "151479052",
                        "name": "Longlong Sun"
                    },
                    {
                        "authorId": "2155493639",
                        "name": "Hui Li"
                    },
                    {
                        "authorId": "7541720",
                        "name": "Yanguo Peng"
                    },
                    {
                        "authorId": "143700599",
                        "name": "Jiangtao Cui"
                    }
                ]
            }
        },
        {
            "contexts": [
                "All prior works that aimed to quantify or improve the privacy of the split layer activation failed to meet one or both of the above criteria [1, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "093d1e8e671f4fc57a821cb5cdf37d49a7b40d20",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-10119",
                    "ArXiv": "2209.10119",
                    "DOI": "10.48550/arXiv.2209.10119",
                    "CorpusId": 252407572
                },
                "corpusId": 252407572,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/093d1e8e671f4fc57a821cb5cdf37d49a7b40d20",
                "title": "Measuring and Controlling Split Layer Privacy Leakage Using Fisher Information",
                "abstract": "Split learning and inference propose to run training/inference of a large model that is split across client devices and the cloud. However, such a model splitting imposes privacy concerns, because the activation flowing through the split layer may leak information about the clients' private input data. There is currently no good way to quantify how much private information is being leaked through the split layer, nor a good way to improve privacy up to the desired level. In this work, we propose to use Fisher information as a privacy metric to measure and control the information leakage. We show that Fisher information can provide an intuitive understanding of how much private information is leaking through the split layer, in the form of an error bound for an unbiased reconstruction attacker. We then propose a privacy-enhancing technique, ReFIL, that can enforce a user-desired level of Fisher information leakage at the split layer to achieve high privacy, while maintaining reasonable utility.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "10995410",
                        "name": "Kiwan Maeng"
                    },
                    {
                        "authorId": "2110228691",
                        "name": "Chuan Guo"
                    },
                    {
                        "authorId": "51110538",
                        "name": "S. Kariyappa"
                    },
                    {
                        "authorId": "12661461",
                        "name": "Ed Suh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There are growing concerns over the interpretability of NLP models, especially when language models are being rapidly applied on various critical fields (Lipton, 2016; Du et al., 2019; Xiang et al., 2019; Miller, 2019; Sun et al., 2021)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "63648343419ccfbaee3aef6c7dcc00ce8a74c2fe",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-08285",
                    "ArXiv": "2209.08285",
                    "DOI": "10.48550/arXiv.2209.08285",
                    "CorpusId": 252368269
                },
                "corpusId": 252368269,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/63648343419ccfbaee3aef6c7dcc00ce8a74c2fe",
                "title": "FR: Folded Rationalization with a Unified Encoder",
                "abstract": "Conventional works generally employ a two-phase model in which a generator selects the most important pieces, followed by a predictor that makes predictions based on the selected pieces. However, such a two-phase model may incur the degeneration problem where the predictor overfits to the noise generated by a not yet well-trained generator and in turn, leads the generator to converge to a sub-optimal model that tends to select senseless pieces. To tackle this challenge, we propose Folded Rationalization (FR) that folds the two phases of the rationale model into one from the perspective of text semantic extraction. The key idea of FR is to employ a unified encoder between the generator and predictor, based on which FR can facilitate a better predictor by access to valuable information blocked by the generator in the traditional two-phase model and thus bring a better generator. Empirically, we show that FR improves the F1 score by up to 10.3% as compared to state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1438943526",
                        "name": "Wei Liu"
                    },
                    {
                        "authorId": "51175126",
                        "name": "Haozhao Wang"
                    },
                    {
                        "authorId": "2152813098",
                        "name": "Jun Wang"
                    },
                    {
                        "authorId": "145765726",
                        "name": "Rui Li"
                    },
                    {
                        "authorId": "2185437000",
                        "name": "Chao Yue"
                    },
                    {
                        "authorId": "2145782537",
                        "name": "Yuankai Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Xiang et al. [27] proposed a complexvalued network, CVDNN, which concealed the input data into a randomized phase.",
                "TABLE I PRIVATE DEEP LEARNING SOLUTIONS Inference privacy Training privacy Intrusive GHOST, ARDEN [23] With SDP and DPFE [24], CVDNN [27] federate learning [7]\u2013[11],",
                "[27] proposed a complexvalued network, CVDNN, which concealed the input data into a randomized phase."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e39531379e590e22b18cfb17aecff758a51b1b7e",
                "externalIds": {
                    "DBLP": "conf/infocom/LiuYJWPWW22",
                    "DOI": "10.1109/INFOCOM48880.2022.9796975",
                    "CorpusId": 246975936
                },
                "corpusId": 246975936,
                "publicationVenue": {
                    "id": "7f92b1d2-f2b3-454d-adbe-ff02c83fe404",
                    "name": "IEEE Conference on Computer Communications",
                    "type": "conference",
                    "alternate_names": [
                        "INFOCOM",
                        "IEEE Conf Comput Commun"
                    ],
                    "url": "http://www.ieee-infocom.org/"
                },
                "url": "https://www.semanticscholar.org/paper/e39531379e590e22b18cfb17aecff758a51b1b7e",
                "title": "When Deep Learning Meets Steganography: Protecting Inference Privacy in the Dark",
                "abstract": "While cloud-based deep learning benefits for high-accuracy inference, it leads to potential privacy risks when exposing sensitive data to untrusted servers. In this paper, we work on exploring the feasibility of steganography in preserving inference privacy. Specifically, we devise GHOST and GHOST+, two private inference solutions employing steganography to make sensitive images invisible in the inference phase. Motivated by the fact that deep neural networks (DNNs) are inherently vulnerable to adversarial attacks, our main idea is turning this vulnerability into the weapon for data privacy, enabling the DNN to misclassify a stego image into the class of the sensitive image hidden in it. The main difference is that GHOST retrains the DNN into a poisoned network to learn the hidden features of sensitive images, but GHOST+ leverages a generative adversarial network (GAN) to produce adversarial perturbations without altering the DNN. For enhanced privacy and a better computation-communication trade-off, both solutions adopt the edge-cloud collaborative framework. Compared with the previous solutions, this is the first work that successfully integrates steganography and the nature of DNNs to achieve private inference while ensuring high accuracy. Extensive experiments validate that steganography has excellent ability in accuracy-aware privacy protection of deep learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "122746265",
                        "name": "Qin Liu"
                    },
                    {
                        "authorId": "2157835874",
                        "name": "Jiamin Yang"
                    },
                    {
                        "authorId": "145730363",
                        "name": "Hongbo Jiang"
                    },
                    {
                        "authorId": "2118432533",
                        "name": "Jie Wu"
                    },
                    {
                        "authorId": "143743992",
                        "name": "Tao Peng"
                    },
                    {
                        "authorId": "2118915621",
                        "name": "Tian Wang"
                    },
                    {
                        "authorId": "2142449608",
                        "name": "Guojun Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Consequently, several works have used split learning for inference to build defense [7, 45, 50, 57, 61, 69, 70, 74, 84, 87] and attack mechanisms [32, 53, 64]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c934d6d54c2900f4462843cdca866d15e2d85152",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-01637",
                    "ArXiv": "2112.01637",
                    "CorpusId": 244037085
                },
                "corpusId": 244037085,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c934d6d54c2900f4462843cdca866d15e2d85152",
                "title": "AdaSplit: Adaptive Trade-offs for Resource-constrained Distributed Deep Learning",
                "abstract": "Distributed deep learning frameworks like federated learning (FL) and its variants are enabling personalized experiences across a wide range of web clients and mobile/IoT devices. However, FL-based frameworks are constrained by computational resources at clients due to the exploding growth of model parameters (eg. billion parameter model). Split learning (SL), a recent framework, reduces client compute load by splitting the model training between client and server. This flexibility is extremely useful for low-compute setups but is often achieved at cost of increase in bandwidth consumption and may result in sub-optimal convergence, especially when client data is heterogeneous. In this work, we introduce AdaSplit which enables efficiently scaling SL to low resource scenarios by reducing bandwidth consumption and improving performance across heterogeneous clients. To capture and benchmark this multi-dimensional nature of distributed deep learning, we also introduce C3-Score, a metric to evaluate performance under resource budgets. We validate the effectiveness of AdaSplit under limited resources through extensive experimental comparison with strong federated and split learning baselines. We also present a sensitivity analysis of key design choices in AdaSplit which validates the ability of AdaSplit to provide adaptive trade-offs across variable resource budgets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9551558",
                        "name": "Ayush Chopra"
                    },
                    {
                        "authorId": "2047973136",
                        "name": "Surya Kant Sahu"
                    },
                    {
                        "authorId": "2034349211",
                        "name": "Abhishek Singh"
                    },
                    {
                        "authorId": "2047972825",
                        "name": "Abhinav Java"
                    },
                    {
                        "authorId": "2133347389",
                        "name": "Praneeth Vepakomma"
                    },
                    {
                        "authorId": "2019383703",
                        "name": "Vivek Sharma"
                    },
                    {
                        "authorId": "145711633",
                        "name": "R. Raskar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "TargetingG1, researchers proposed to use encryption for privacy protection [4, 6, 33].",
                "Several works [4, 6, 33] propose to use encryption to protect the user privacy and thus defend against the direct attacks.",
                "First, comparingwith the data encryption [4, 6, 33], distillation [21, 22], and adversarial training [8, 14, 26] approaches, Fake Gradient reduces huge protection overhead by eliminating the need of re-training.",
                "In [33], the proposed approach transforms the real-valued features into complex-valued ones, in which the input is hidden in a randomized phase of the transformed features."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2d9629d311e26d26c036d5e5abc497e9ac109725",
                "externalIds": {
                    "DBLP": "conf/mm/Feng0YT0021",
                    "DOI": "10.1145/3474085.3475685",
                    "CorpusId": 239012045
                },
                "corpusId": 239012045,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2d9629d311e26d26c036d5e5abc497e9ac109725",
                "title": "Fake Gradient: A Security and Privacy Protection Framework for DNN-based Image Classification",
                "abstract": "Deep neural networks (DNNs) have demonstrated phenomenal success in image classification applications and are widely adopted in multimedia internet of things (IoT) use cases, such as smart home systems. To compensate for the limited resources on the IoT devices, the computation-intensive image classification tasks are often offloaded to remote cloud services. However, the offloading-based image classification could pose significant security and privacy concerns to the user data and the DNN model, leading to effective adversarial attacks that compromise the classification accuracy. The existing defense methods either impact the original functionality or result in high computation or model re-training overhead. In this paper, we develop a novel defense approach, namely Fake Gradient, to protect the privacy of the data and defend against adversarial attacks based on encryption of the output. Fake Gradient can hide the real output information by generating fake classes and further mislead the adversarial perturbation generation based on fake gradient knowledge, which helps maintain a high classification accuracy on the perturbed data. Our evaluations using ImageNet and 7 popular DNN models indicate that Fake Gradient is effective in protecting the privacy and defending against adversarial attacks targeting image classification applications.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "26937848",
                        "name": "Xianglong Feng"
                    },
                    {
                        "authorId": "2149182305",
                        "name": "Yi Xie"
                    },
                    {
                        "authorId": "8721036",
                        "name": "Mengmei Ye"
                    },
                    {
                        "authorId": "3438895",
                        "name": "Zhongze Tang"
                    },
                    {
                        "authorId": "2149891180",
                        "name": "Bo Yuan"
                    },
                    {
                        "authorId": "2152136949",
                        "name": "Sheng Wei"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3411e3b3008c60dca293d0164484bad4799b82cd",
                "externalIds": {
                    "DBLP": "journals/vcomm/HanCM21",
                    "MAG": "3165817011",
                    "DOI": "10.1016/J.VEHCOM.2021.100374",
                    "CorpusId": 236252677
                },
                "corpusId": 236252677,
                "publicationVenue": {
                    "id": "63d338d9-43f7-427b-9481-4a4b41ba9468",
                    "name": "Vehicular Communications",
                    "type": "journal",
                    "alternate_names": [
                        "Veh Commun"
                    ],
                    "issn": "2214-2096",
                    "url": "https://www.journals.elsevier.com/vehicular-communications/"
                },
                "url": "https://www.semanticscholar.org/paper/3411e3b3008c60dca293d0164484bad4799b82cd",
                "title": "PPM-InVIDS: Privacy protection model for in-vehicle intrusion detection system based complex-valued neural network",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145355489",
                        "name": "Mu Han"
                    },
                    {
                        "authorId": "1988975660",
                        "name": "Pengzhou Cheng"
                    },
                    {
                        "authorId": "8093343",
                        "name": "Shidian Ma"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "8e6f2c97a4fbe2f2c9483f7b1552ad50337b0e44",
                "externalIds": {
                    "DBLP": "journals/isci/WangML21",
                    "MAG": "3167088367",
                    "DOI": "10.1016/J.INS.2021.05.054",
                    "CorpusId": 236249305
                },
                "corpusId": 236249305,
                "publicationVenue": {
                    "id": "e46002a1-d7a6-4681-aae9-36bc3a6a1f93",
                    "name": "Information Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Information Scientist",
                        "Inf Sci"
                    ],
                    "issn": "0020-0255",
                    "alternate_issns": [
                        "0020-0263"
                    ],
                    "url": "http://www.sciencedirect.com/science/journal/00200255"
                },
                "url": "https://www.semanticscholar.org/paper/8e6f2c97a4fbe2f2c9483f7b1552ad50337b0e44",
                "title": "SieveNet: Decoupling activation function neural network for privacy-preserving deep learning",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109122266",
                        "name": "Qizheng Wang"
                    },
                    {
                        "authorId": "38872041",
                        "name": "Wenping Ma"
                    },
                    {
                        "authorId": "2109614860",
                        "name": "Ge Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "2e9cff85a2bfc17f62a0bd9671068f8cf2dcd5a6",
                "externalIds": {
                    "DBLP": "journals/ml/ChenGZC21",
                    "MAG": "3156534051",
                    "DOI": "10.1007/s10994-021-05951-6",
                    "CorpusId": 234785882
                },
                "corpusId": 234785882,
                "publicationVenue": {
                    "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
                    "name": "Machine-mediated learning",
                    "type": "journal",
                    "alternate_names": [
                        "Mach learn",
                        "Machine Learning",
                        "Mach Learn"
                    ],
                    "issn": "0732-6718",
                    "alternate_issns": [
                        "0885-6125"
                    ],
                    "url": "http://www.springer.com/computer/artificial/journal/10994",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10994",
                        "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2e9cff85a2bfc17f62a0bd9671068f8cf2dcd5a6",
                "title": "Protect privacy of deep classification networks by exploiting their generative power",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "67313577",
                        "name": "Jiyu Chen"
                    },
                    {
                        "authorId": "2527106",
                        "name": "Yiwen Guo"
                    },
                    {
                        "authorId": "2098809297",
                        "name": "Qianjun Zheng"
                    },
                    {
                        "authorId": "2149051171",
                        "name": "Hao Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ca6b5a397af4562359e17e2185de21ea4ba2505e",
                "externalIds": {
                    "ArXiv": "2103.07287",
                    "DBLP": "journals/corr/abs-2103-07287",
                    "CorpusId": 232223194
                },
                "corpusId": 232223194,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ca6b5a397af4562359e17e2185de21ea4ba2505e",
                "title": "Neural Networks with Complex-Valued Weights Have No Spurious Local Minima",
                "abstract": "We study the bene\ufb01ts of complex-valued weights for neural networks. We prove that shallow complex neural networks with quadratic activations have no spurious local minima. In contrast, shallow real neural networks with quadratic activations have in\ufb01nitely many spurious local minima under the same conditions. In addition, we provide speci\ufb01c examples to demonstrate that complex-valued weights turn poor local minima into saddle points. The activation function C ReLU is also discussed to illustrate the superiority of analytic activations in complex-valued neural networks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "13186188",
                        "name": "Xingtu Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "16814b3c764510d992d11c900f9a5c2ffa3c4443",
                "externalIds": {
                    "DBLP": "journals/ccfthpc/TangWLL21",
                    "MAG": "3093669792",
                    "DOI": "10.1007/s42514-020-00052-7",
                    "CorpusId": 226328238
                },
                "corpusId": 226328238,
                "publicationVenue": {
                    "id": "24061e5a-667f-47e0-8af6-00edcfe20c88",
                    "name": "CCF Transactions on High Performance Computing",
                    "type": "journal",
                    "alternate_names": [
                        "CCF Trans High Perform Comput"
                    ],
                    "issn": "2524-4922",
                    "url": "https://link.springer.com/journal/volumesAndIssues/42514"
                },
                "url": "https://www.semanticscholar.org/paper/16814b3c764510d992d11c900f9a5c2ffa3c4443",
                "title": "To cloud or not to cloud: an on-line scheduler for dynamic privacy-protection of deep learning workload on edge devices",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2115420545",
                        "name": "Yibin Tang"
                    },
                    {
                        "authorId": "144810556",
                        "name": "Ying Wang"
                    },
                    {
                        "authorId": "47892692",
                        "name": "Huawei Li"
                    },
                    {
                        "authorId": "40613624",
                        "name": "Xiaowei Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "21389cc0274566a288e683b5c56818e333418943",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-13016",
                    "ArXiv": "2006.13016",
                    "MAG": "3036085893",
                    "CorpusId": 219981157
                },
                "corpusId": 219981157,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/21389cc0274566a288e683b5c56818e333418943",
                "title": "Rotation-Equivariant Neural Networks for Privacy Protection",
                "abstract": "In order to prevent leaking input information from intermediate-layer features, this paper proposes a method to revise the traditional neural network into the rotation-equivariant neural network (RENN). Compared to the traditional neural network, the RENN uses d-ary vectors/tensors as features, in which each element is a d-ary number. These d-ary features can be rotated (analogous to the rotation of a d-dimensional vector) with a random angle as the encryption process. Input information is hidden in this target phase of d-ary features for attribute obfuscation. Even if attackers have obtained network parameters and intermediate-layer features, they cannot extract input information without knowing the target phase. Hence, the input privacy can be effectively protected by the RENN. Besides, the output accuracy of RENNs only degrades mildly compared to traditional neural networks, and the computational cost is significantly less than the homomorphic encryption.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Hao Zhang"
                    },
                    {
                        "authorId": "2109060480",
                        "name": "Yiting Chen"
                    },
                    {
                        "authorId": "11314683",
                        "name": "Haotian Ma"
                    },
                    {
                        "authorId": "2110251893",
                        "name": "Xu Cheng"
                    },
                    {
                        "authorId": "147863855",
                        "name": "Qihan Ren"
                    },
                    {
                        "authorId": "1850278",
                        "name": "Liyao Xiang"
                    },
                    {
                        "authorId": "2112364113",
                        "name": "Jie Shi"
                    },
                    {
                        "authorId": "22063226",
                        "name": "Quanshi Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The forth baseline was the (Xiang et al., 2019), which was constructed using the same division of modules of the QNN.",
                "(Xiang et al., 2019) used complex-valued neural networks to protect privacy."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "7f0e0712ae4d2820682f023bb0c86a1bd3d3d180",
                "externalIds": {
                    "ArXiv": "2003.08365",
                    "DBLP": "journals/corr/abs-2003-08365",
                    "MAG": "3012065399",
                    "CorpusId": 212747516
                },
                "corpusId": 212747516,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7f0e0712ae4d2820682f023bb0c86a1bd3d3d180",
                "title": "Deep Quaternion Features for Privacy Protection",
                "abstract": "We propose a method to revise the neural network to construct the quaternion-valued neural network (QNN), in order to prevent intermediate-layer features from leaking input information. The QNN uses quaternion-valued features, where each element is a quaternion. The QNN hides input information into a random phase of quaternion-valued features. Even if attackers have obtained network parameters and intermediate-layer features, they cannot extract input information without knowing the target phase. In this way, the QNN can effectively protect the input privacy. Besides, the output accuracy of QNNs only degrades mildly compared to traditional neural networks, and the computational cost is much less than other privacy-preserving methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Hao Zhang"
                    },
                    {
                        "authorId": "2109060480",
                        "name": "Yiting Chen"
                    },
                    {
                        "authorId": "1850278",
                        "name": "Liyao Xiang"
                    },
                    {
                        "authorId": "11314683",
                        "name": "Haotian Ma"
                    },
                    {
                        "authorId": "2112364113",
                        "name": "Jie Shi"
                    },
                    {
                        "authorId": "22063226",
                        "name": "Quanshi Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "13827fdbde1413e4ff5f8ab03efb10e512325edd",
                "externalIds": {
                    "ArXiv": "1911.09040",
                    "DBLP": "conf/eccv/ShenZHWZ20",
                    "MAG": "2990996678",
                    "DOI": "10.1007/978-3-030-58565-5_32",
                    "CorpusId": 208176413
                },
                "corpusId": 208176413,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/13827fdbde1413e4ff5f8ab03efb10e512325edd",
                "title": "3D-Rotation-Equivariant Quaternion Neural Networks",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2145925040",
                        "name": "Binbin Zhang"
                    },
                    {
                        "authorId": "2117226245",
                        "name": "Wen Shen"
                    },
                    {
                        "authorId": "98321260",
                        "name": "Shikun Huang"
                    },
                    {
                        "authorId": "143628849",
                        "name": "Zhihua Wei"
                    },
                    {
                        "authorId": "22063226",
                        "name": "Quanshi Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The paper by Xiang et al. (2020) looks at the privacy problems that are associated with running data processing operations on the cloud.",
                "This report investigates the reproducibility of the paper \u2018Interpretable Complex-Valued Neural Networks For Privacy Protection\u2019 by Xiang et al. (2020). The paper proposes a new method of creating complex-valued DNNs, for which the privacy of the model is better protected against potential attackers while the performance of the models is largely preserved.",
                "\u2026and Reinier Bekkenutte (13438557)\nReproducibility Summary\nScope of Reproducibility\nIn this reproducibility report, the following two main claims of Xiang et al. (2020)\u2019s paper are tested:\n\u2022 The performance of a Deep Neural Network (DNN) is largely preserved when comparing DNNs with complex\u2026",
                "Scope of Reproducibility In this reproducibility report, the following two main claims of Xiang et al. (2020)\u2019s paper are tested:",
                "This report investigates the reproducibility of the paper \u2018Interpretable Complex-Valued Neural Networks For Privacy Protection\u2019 by Xiang et al. (2020).",
                "The paper by Xiang et al. (2020) proposes a new model architecture that encrypts the data in complex-valued and rotated features, to prevent an attacker from inferring or reconstructing privacy sensitive information."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "176ab1f973c53fe9effed2be84103e2e2d452a12",
                "externalIds": {
                    "CorpusId": 237263844
                },
                "corpusId": 237263844,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/176ab1f973c53fe9effed2be84103e2e2d452a12",
                "title": "Reproducibility report: Interpretable Complex-Valued Neural Networks For Privacy Protection",
                "abstract": "Since the code was not made publicly available we implemented our own version of the reported DNNs. Baseline DNNs were created using the default model architecture. The figures and math of the original paper were used to recreate the structure of the complex-valued DNNs, in which the model is divided into an encoder, a processing module on the cloud, and a decoder. The goal of the complex-valued DNN is to make sure that the features are rotated and obfuscated to ensure that the privacy of the data is secured. We compare the performance of the baseline and complex-valued DNNs. Then, we test the robustness of the models against privacy attacks, where potential attackers were mimicked using inversion attacks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2104791087",
                        "name": "Maxpool"
                    },
                    {
                        "authorId": "2124157924",
                        "name": "Preact"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[29] conceal intermediate features by rotating the features in a complex space."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7425b98efe1b212a5faf842324620c3a63a64cbd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-09023",
                    "CorpusId": 237571367
                },
                "corpusId": 237571367,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7425b98efe1b212a5faf842324620c3a63a64cbd",
                "title": "Anti-Neuron Watermarking: Protecting Personal Data Against Unauthorized Neural Model Training",
                "abstract": "In this paper, we raise up an emerging personal data protection problem where user personal data (e.g. images) could be inappropriately exploited to train deep neural network models without authorization. To solve this problem, we revisit traditional watermarking in advanced machine learning settings. By embedding a watermarking signature using specialized linear color transformation to user images, neural models will be imprinted with such a signature if training data include watermarked images. Then, a third-party verifier can verify potential unauthorized usage by inferring the watermark signature from neural models. We further explore the desired properties of watermarking and signature space for convincing verification. Through extensive experiments, we show empirically that linear color transformation is effective in protecting user\u2019s personal images for various realistic settings. To the best of our knowledge, this is the first work to protect users\u2019 personal data from unauthorized usage in neural network training.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2000395493",
                        "name": "Zihang Zou"
                    },
                    {
                        "authorId": "40206014",
                        "name": "Boqing Gong"
                    },
                    {
                        "authorId": "1390771606",
                        "name": "Liqiang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To a certain extent, it also reduces the amount of data that can be processed, which is more effective than DPI in terms of protecting personal privacy [6, 7] and consuming computing resources.",
                "Other studies [7,13] monitor each inserted flow entry and check whether it has any negative impact on the data plane.",
                "Specifically, FortNOX [13] can detect flow conflicts and VeriFlow [7] verifies network invariants upon forwarding state changes.",
                "Some studies [7,13] monitor each inserted flow entry and check whether it has any negative impact on the data plane, but they are unable to identify the threats which are caused by a set of flow entries.",
                "Therefore, how to effectively use a large amount of existing knowledge and historical accumulation in the field of cybersecurity to achieve the specification and integration of security data, to continuously store and update malicious traffic information under massively connected terminals, has become a critical issue to be solved urgently [7].",
                "Moreover, previous research on link-separation multipath selection schemes [5,7,17] did not solve the Head of Line (HOL) blocking problem due to the fact that different routing paths had varied latency, resulting in out-of-order arrival of packets at the same destination node [12].",
                "RBAC supports generally accepted security principles: minimum privilege principle, separation of responsibility principle and data abstraction principle [7].",
                "Eventually, if the controller cannot provide normal services, the entire network will be greatly affected or even paralyzed [7].",
                "In our previous research work [7], we present a differential constellation shifting (DCS) aided RF watermark scheme to improve reliability performance while providing secure transmissions.",
                "Exll [6] and C2TCP [7] are proposed to achieve low latency in cellular networks."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5aec1343bfb7dbc3675f83435f94b75100d6ca01",
                "externalIds": {
                    "DBLP": "conf/spde/2020",
                    "DOI": "10.1007/978-981-15-9129-7",
                    "CorpusId": 224826295
                },
                "corpusId": 224826295,
                "publicationVenue": {
                    "id": "2e71fe54-bb08-4ce8-9243-27bebb232934",
                    "name": "International Conference on Security and Privacy in Digital Economy",
                    "type": "conference",
                    "alternate_names": [
                        "SPDE",
                        "Int Conf Secur Priv Digit Econ"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5aec1343bfb7dbc3675f83435f94b75100d6ca01",
                "title": "Security and Privacy in Digital Economy: First International Conference, SPDE 2020, Quzhou, China, October 30 \u2013 November 1, 2020, Proceedings",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2068408796",
                        "name": "Simone Diniz Junqueira Barbosa"
                    },
                    {
                        "authorId": "40913232",
                        "name": "Phoebe Chen"
                    },
                    {
                        "authorId": "145046124",
                        "name": "A. Cuzzocrea"
                    },
                    {
                        "authorId": "46993406",
                        "name": "Xiaoyong Du"
                    },
                    {
                        "authorId": "144408238",
                        "name": "Orhun Kara"
                    },
                    {
                        "authorId": "145161755",
                        "name": "Ting Liu"
                    },
                    {
                        "authorId": "1731022",
                        "name": "K. Sivalingam"
                    },
                    {
                        "authorId": "145514740",
                        "name": "D. \u015al\u0119zak"
                    },
                    {
                        "authorId": "1704749",
                        "name": "T. Washio"
                    },
                    {
                        "authorId": "50031361",
                        "name": "Xiaokang Yang"
                    },
                    {
                        "authorId": "145078769",
                        "name": "Junsong Yuan"
                    },
                    {
                        "authorId": "1690892",
                        "name": "R. Prates"
                    },
                    {
                        "authorId": "2116617855",
                        "name": "Shui Yu"
                    },
                    {
                        "authorId": "49569787",
                        "name": "Peter Mueller"
                    },
                    {
                        "authorId": "2672831",
                        "name": "Jiangbo Qian"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[40] put real number features in complex numbers to hide through rotation, and use GAN model to generate confusion samples to achieve k-anonymity."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "7bd2ed6a4d9d5726cf2f4a4bd163997dc74a8cb6",
                "externalIds": {
                    "DBLP": "journals/iacr/WangMLL20",
                    "MAG": "3081577411",
                    "CorpusId": 221356128
                },
                "corpusId": 221356128,
                "publicationVenue": {
                    "id": "166fd2b5-a928-4a98-a449-3b90935cc101",
                    "name": "IACR Cryptology ePrint Archive",
                    "type": "journal",
                    "alternate_names": [
                        "IACR Cryptol eprint Arch"
                    ],
                    "url": "http://eprint.iacr.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7bd2ed6a4d9d5726cf2f4a4bd163997dc74a8cb6",
                "title": "Enable Dynamic Parameters Combination to Boost Linear Convolutional Neural Network for Sensitive Data Inference",
                "abstract": "As cloud computing matures, Machine Learning as a Service(MLaaS) has received more attention. In many scenarios, sensitive information also has a demand for MLaaS, but it should not be exposed to others, which brings a dilemma. In order to solve this dilemma, many works have proposed some privacyprotected machine learning frameworks. Compared with plaintext tasks, cipher-text inference has higher computation and communication overhead. In addition to the difficulties caused by cipher-text calculations, the nonlinear activation functions in machine learning models are not friendly to Homomorphic Encryption(HE) and Secure Multi-Party Computation(MPC). The nonlinear activation function can effectively improve the performance of the network, and it seems that the high overhead brought by it is inevitable. In order to solve this problem, this paper re-explains the mechanism of the nonlinear activation function in forward propagation from another perspective, and based on this observation, proposed a dynamic parameters combination scheme as an alternative, called DPC. DPC allows the decoupling of nonlinear operations and linear operations in neural networks. This work further uses this feature to design the HE-based framework and MPC-based framework, so that nonlinear operations can be completed locally by the user through pre-computation, which greatly improves the efficiency of privacy protection data prediction. The evaluation result shows that the linear neural networks with DPC can perform high accuracy. Without other optimizations, the HE-based proposed in this work shows 2x faster executions than CryptoNets only relying on the advantage of the DPC. The MPC-based framework proposed in this work can achieve similar efficiency to plain-text prediction, and has advantages over other work in terms of communication complexity and computational complexity.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2109122266",
                        "name": "Qizheng Wang"
                    },
                    {
                        "authorId": "38872041",
                        "name": "Wenping Ma"
                    },
                    {
                        "authorId": "2155871775",
                        "name": "Jie Li"
                    },
                    {
                        "authorId": "2109614860",
                        "name": "Ge Liu"
                    }
                ]
            }
        }
    ]
}