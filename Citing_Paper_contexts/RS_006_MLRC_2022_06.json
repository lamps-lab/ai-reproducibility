{
    "offset": 0,
    "data": [
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cc34f7ccc789503492f3f229de0afc9ef655bef4",
                "externalIds": {
                    "ArXiv": "2310.01680",
                    "CorpusId": 263608446
                },
                "corpusId": 263608446,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cc34f7ccc789503492f3f229de0afc9ef655bef4",
                "title": "Keypoint-Augmented Self-Supervised Learning for Medical Image Segmentation with Limited Annotation",
                "abstract": "Pretraining CNN models (i.e., UNet) through self-supervision has become a powerful approach to facilitate medical image segmentation under low annotation regimes. Recent contrastive learning methods encourage similar global representations when the same image undergoes different transformations, or enforce invariance across different image/patch features that are intrinsically correlated. However, CNN-extracted global and local features are limited in capturing long-range spatial dependencies that are essential in biological anatomy. To this end, we present a keypoint-augmented fusion layer that extracts representations preserving both short- and long-range self-attention. In particular, we augment the CNN feature map at multiple scales by incorporating an additional input that learns long-range spatial self-attention among localized keypoint features. Further, we introduce both global and local self-supervised pretraining for the framework. At the global scale, we obtain global representations from both the bottleneck of the UNet, and by aggregating multiscale keypoint features. These global features are subsequently regularized through image-level contrastive objectives. At the local scale, we define a distance-based criterion to first establish correspondences among keypoints and encourage similarity between their features. Through extensive experiments on both MRI and CT segmentation tasks, we demonstrate the architectural advantages of our proposed method in comparison to both CNN and Transformer-based UNets, when all architectures are trained with randomly initialized weights. With our proposed pretraining strategy, our method further outperforms existing SSL methods by producing more robust self-attention and achieving state-of-the-art segmentation results. The code is available at https://github.com/zshyang/kaf.git.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51126028",
                        "name": "Zhangsihao Yang"
                    },
                    {
                        "authorId": "30106897",
                        "name": "Mengwei Ren"
                    },
                    {
                        "authorId": "2253460992",
                        "name": "Kaize Ding"
                    },
                    {
                        "authorId": "2238310544",
                        "name": "Guido Gerig"
                    },
                    {
                        "authorId": "2253866665",
                        "name": "Yalin Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "We find the optimal \u03b8 to be around [1, 20], depending on the specific attributes of interests.",
                "Our work differ in the way that we include a contrastive loss that measures the similarity between the input embeddings and extractor embeddings, which allows the style extractor to capture more precise text style representations[1, 22]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "02f17e5918e4a803f81410901c1375fc323eccd2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-10929",
                    "ArXiv": "2309.10929",
                    "DOI": "10.48550/arXiv.2309.10929",
                    "CorpusId": 262064711
                },
                "corpusId": 262064711,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/02f17e5918e4a803f81410901c1375fc323eccd2",
                "title": "Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training",
                "abstract": "In this work, we introduce the concept of complex text style transfer tasks, and constructed complex text datasets based on two widely applicable scenarios. Our dataset is the first large-scale data set of its kind, with 700 rephrased sentences and 1,000 sentences from the game Genshin Impact. While large language models (LLM) have shown promise in complex text style transfer, they have drawbacks such as data privacy concerns, network instability, and high deployment costs. To address these issues, we explore the effectiveness of small models (less than T5-3B) with implicit style pre-training through contrastive learning. We also propose a method for automated evaluation of text generation quality based on alignment with human evaluations using ChatGPT. Finally, we compare our approach with existing methods and show that our model achieves state-of-art performances of few-shot text style transfer models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2243405915",
                        "name": "Ruiqi Xu"
                    },
                    {
                        "authorId": "2192183418",
                        "name": "Y. Huang"
                    },
                    {
                        "authorId": "48283677",
                        "name": "Xin Chen"
                    },
                    {
                        "authorId": "2243033035",
                        "name": "Lin Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "the anchor example) by contrasting its positive and negative pairs5, which allows models to improve their capabilities on multiple dimensions, such as scalability [6], generalization ability [58], global and hierarchical local features learning [11] and performance on downstream tasks [10, 61]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "58f581ea709591cf6a23f66bb9ea6f80f30da4e6",
                "externalIds": {
                    "ArXiv": "2309.04828",
                    "DBLP": "journals/corr/abs-2309-04828",
                    "DOI": "10.48550/arXiv.2309.04828",
                    "CorpusId": 261682528
                },
                "corpusId": 261682528,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/58f581ea709591cf6a23f66bb9ea6f80f30da4e6",
                "title": "FAIR: Flow Type-Aware Pre-Training of Compiler Intermediate Representations",
                "abstract": "While the majority of existing pre-trained models from code learn source code features such as code tokens and abstract syntax trees, there are some other works that focus on learning from compiler intermediate representations (IRs). Existing IR-based models typically utilize IR features such as instructions, control and data flow graphs (CDFGs), call graphs, etc. However, these methods confuse variable nodes and instruction nodes in a CDFG and fail to distinguish different types of flows, and the neural networks they use fail to capture long-distance dependencies and have over-smoothing and over-squashing problems. To address these weaknesses, we propose FAIR, a Flow type-Aware pre-trained model for IR that involves employing (1) a novel input representation of IR programs; (2) Graph Transformer to address over-smoothing, over-squashing and long-dependencies problems; and (3) five pre-training tasks that we specifically propose to enable FAIR to learn the semantics of IR tokens, flow type information, and the overall representation of IR. Experimental results show that FAIR can achieve state-of-the-art results on four code-related downstream tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2069657833",
                        "name": "Changan Niu"
                    },
                    {
                        "authorId": "2523792",
                        "name": "Chuanyi Li"
                    },
                    {
                        "authorId": "2140903756",
                        "name": "Vincent Ng"
                    },
                    {
                        "authorId": "2238953212",
                        "name": "David Lo"
                    },
                    {
                        "authorId": "2238954114",
                        "name": "Bin Luo"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In [25], it was shown that differences between contrastive losses are small with a deep projection head.",
                "to be discarded for a better generalization [8, 22, 23, 24, 25].",
                "The RandBit dataset of [25] is also similar, but their goal is to study explicit and controllable competing features.",
                "needs to be discarded for a better generalization [8, 22, 23, 24, 25]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ff059312fc6e240725d3789a0bb011cce40a7049",
                "externalIds": {
                    "ArXiv": "2308.15704",
                    "DBLP": "journals/corr/abs-2308-15704",
                    "DOI": "10.48550/arXiv.2308.15704",
                    "CorpusId": 261339623
                },
                "corpusId": 261339623,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ff059312fc6e240725d3789a0bb011cce40a7049",
                "title": "Towards a Rigorous Analysis of Mutual Information in Contrastive Learning",
                "abstract": "Contrastive learning has emerged as a cornerstone in recent achievements of unsupervised representation learning. Its primary paradigm involves an instance discrimination task with a mutual information loss. The loss is known as InfoNCE and it has yielded vital insights into contrastive learning through the lens of mutual information analysis. However, the estimation of mutual information can prove challenging, creating a gap between the elegance of its mathematical foundation and the complexity of its estimation. As a result, drawing rigorous insights or conclusions from mutual information analysis becomes intricate. In this study, we introduce three novel methods and a few related theorems, aimed at enhancing the rigor of mutual information analysis. Despite their simplicity, these methods can carry substantial utility. Leveraging these approaches, we reassess three instances of contrastive learning analysis, illustrating their capacity to facilitate deeper comprehension or to rectify pre-existing misconceptions. Specifically, we investigate small batch size, mutual information as a measure, and the InfoMin principle.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2110057640",
                        "name": "Kyungeun Lee"
                    },
                    {
                        "authorId": "2157223778",
                        "name": "Jaeill Kim"
                    },
                    {
                        "authorId": "15230387",
                        "name": "Suhyun Kang"
                    },
                    {
                        "authorId": "143884871",
                        "name": "Wonjong Rhee"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "As the primary loss function, we use the normalized temperature-scaled cross-entropy loss (NT-Xent loss) function [27] because minimizing it also guarantees maximizing a lower bound on the mutual information between the input and the representation [41].",
                "Existing contrastive learning methods critically rely on specific data augmentation to favor certain sets of features over others [41]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7aa571ea5c1ccd2e2cc143adc210a4c8f9ec85bd",
                "externalIds": {
                    "DBLP": "conf/case/AvellanedaMS23",
                    "DOI": "10.1109/CASE56687.2023.10260493",
                    "CorpusId": 263229709
                },
                "corpusId": 263229709,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7aa571ea5c1ccd2e2cc143adc210a4c8f9ec85bd",
                "title": "Cross-Modal Self-Supervised Feature Extraction for Anomaly Detection in Human Monitoring",
                "abstract": "This paper proposes to extract cross-modal self-supervised features to detect anomalies in human monitoring. Our previous works that use deep captioning in addition to monitoring images were successful. However, their use of unimodally trained image and text features shows deficiencies in capturing contextual information across the modalities. We devise a self-supervised method that creates cross-modal features by maximizing the mutual information between both modalities in a common subspace. It allows capturing different complex distributions between modalities, improving the detection performance of clustering methods. Extensive experimental results show improvements in both AUC and AUPRC scores when compared to the best baselines on two real-world datasets. The AUC has improved from 0.895 to 0.969, and from 0.97 to 0.98. The AUPRC has improved from 0.681 to 0.850, and from 0.840 to 0.894.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2248808559",
                        "name": "Jose Alejandro Avellaneda"
                    },
                    {
                        "authorId": "2816822",
                        "name": "Tetsu Matsukawa"
                    },
                    {
                        "authorId": "2248298000",
                        "name": "Einoshin Suzuki"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f4e26ad29143332c478280bbfec5a2a23b282c5d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-11448",
                    "ArXiv": "2308.11448",
                    "DOI": "10.48550/arXiv.2308.11448",
                    "CorpusId": 261064943
                },
                "corpusId": 261064943,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f4e26ad29143332c478280bbfec5a2a23b282c5d",
                "title": "Masked Momentum Contrastive Learning for Zero-shot Semantic Understanding",
                "abstract": "Self-supervised pretraining (SSP) has emerged as a popular technique in machine learning, enabling the extraction of meaningful feature representations without labelled data. In the realm of computer vision, pretrained vision transformers (ViTs) have played a pivotal role in advancing transfer learning. Nonetheless, the escalating cost of finetuning these large models has posed a challenge due to the explosion of model size. This study endeavours to evaluate the effectiveness of pure self-supervised learning (SSL) techniques in computer vision tasks, obviating the need for finetuning, with the intention of emulating human-like capabilities in generalisation and recognition of unseen objects. To this end, we propose an evaluation protocol for zero-shot segmentation based on a prompting patch. Given a point on the target object as a prompt, the algorithm calculates the similarity map between the selected patch and other patches, upon that, a simple thresholding is applied to segment the target. Another evaluation is intra-object and inter-object similarity to gauge discriminatory ability of SSP ViTs. Insights from zero-shot segmentation from prompting and discriminatory abilities of SSP led to the design of a simple SSP approach, termed MMC. This approaches combines Masked image modelling for encouraging similarity of local features, Momentum based self-distillation for transferring semantics from global to local features, and global Contrast for promoting semantics of global features, to enhance discriminative representations of SSP ViTs. Consequently, our proposed method significantly reduces the overlap of intra-object and inter-object similarities, thereby facilitating effective object segmentation within an image. Our experiments reveal that MMC delivers top-tier results in zero-shot semantic segmentation across various datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2110422360",
                        "name": "Jiantao Wu"
                    },
                    {
                        "authorId": "2066123456",
                        "name": "Shentong Mo"
                    },
                    {
                        "authorId": "144987292",
                        "name": "Muhammad Awais"
                    },
                    {
                        "authorId": "22692781",
                        "name": "Sara Atito"
                    },
                    {
                        "authorId": "2976854",
                        "name": "Zhenhua Feng"
                    },
                    {
                        "authorId": "145801638",
                        "name": "J. Kittler"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Furthermore, the work by [27] investigated several intriguing properties of contrastive learning."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a4d8fc9ef925431108aaa447983c2d300b624210",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-01140",
                    "ArXiv": "2308.01140",
                    "DOI": "10.48550/arXiv.2308.01140",
                    "CorpusId": 260378635
                },
                "corpusId": 260378635,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a4d8fc9ef925431108aaa447983c2d300b624210",
                "title": "DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning",
                "abstract": "In contemporary self-supervised contrastive algorithms like SimCLR, MoCo, etc., the task of balancing attraction between two semantically similar samples and repulsion between two samples from different classes is primarily affected by the presence of hard negative samples. While the InfoNCE loss has been shown to impose penalties based on hardness, the temperature hyper-parameter is the key to regulating the penalties and the trade-off between uniformity and tolerance. In this work, we focus our attention to improve the performance of InfoNCE loss in SSL by studying the effect of temperature hyper-parameter values. We propose a cosine similarity-dependent temperature scaling function to effectively optimize the distribution of the samples in the feature space. We further analyze the uniformity and tolerance metrics to investigate the optimal regions in the cosine similarity space for better optimization. Additionally, we offer a comprehensive examination of the behavior of local and global structures in the feature space throughout the pre-training phase, as the temperature varies. Experimental evidence shows that the proposed framework outperforms or is at par with the contrastive loss-based SSL algorithms. We believe our work (DySTreSS) on temperature scaling in SSL provides a foundation for future research in contrastive learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1816706542",
                        "name": "Siladittya Manna"
                    },
                    {
                        "authorId": "2146696888",
                        "name": "Soumitri Chattopadhyay"
                    },
                    {
                        "authorId": "2226438844",
                        "name": "Rakesh Dey"
                    },
                    {
                        "authorId": "1990389",
                        "name": "Saumik Bhattacharya"
                    },
                    {
                        "authorId": "144167309",
                        "name": "U. Pal"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "81396281cc0eec47d6f8c0bb4c97f4ab31df92fc",
                "externalIds": {
                    "DOI": "10.1167/jov.23.9.5938",
                    "CorpusId": 260696117
                },
                "corpusId": 260696117,
                "publicationVenue": {
                    "id": "c3faa921-3f7d-4435-906f-25cdb7d6a885",
                    "name": "Journal of Vision",
                    "type": "journal",
                    "alternate_names": [
                        "J Vis",
                        "Journal of Visualization"
                    ],
                    "issn": "1534-7362",
                    "alternate_issns": [
                        "1343-8875"
                    ],
                    "url": "http://www.journalofvision.org/4/6/",
                    "alternate_urls": [
                        "https://link.springer.com/journal/12650",
                        "http://journalofvision.org/",
                        "https://www.iospress.nl/html/13438875.php"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/81396281cc0eec47d6f8c0bb4c97f4ab31df92fc",
                "title": "Evaluating machine comprehension of sketch meaning at different levels of abstraction",
                "abstract": "People can reliably understand images that vary in visual abstraction\u2014from detailed illustrations to schematic icons. To what degree are current vision algorithms robust to such variation when attributing meaning to abstract images? We \ufb01rst obtained > 90 K human-generated sketches produced under different time limits (4s, 8s, 16s, 32s; N =5,563 participants) and AI-generated sketches (Vinker et al., 2022) produced under different ink limits (4, 8, 16, 32 strokes) of 2,048 real-world object concepts spanning 128 categories from the THINGS dataset (Hebart et al., 2019). We then evaluated how well 12 state-of-the-art vision algorithms could (1) predict which concept each sketch was intended to convey and (2) match human performance and response patterns when presented with the same sketches. We found that models achieving generally higher recognition accuracy also tracked human error patterns better, although there remains a sizable gap between human and machine sketch understanding. We also found that, on average, different models expressed similar uncertainty about sketches of the same concept across different levels of abstraction. We hope that public release of this dataset and evaluation protocol will lead to algorithms that display more human-like visual abstraction.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1382262357",
                        "name": "Kushin Mukherjee"
                    },
                    {
                        "authorId": "103197266",
                        "name": "Holly Huey"
                    },
                    {
                        "authorId": "2133701211",
                        "name": "Xuanchen Lu"
                    },
                    {
                        "authorId": "1630270601",
                        "name": "Yael Vinker"
                    },
                    {
                        "authorId": "2230328086",
                        "name": "Rio Aguina-Kang"
                    },
                    {
                        "authorId": "2947946",
                        "name": "Ariel Shamir"
                    },
                    {
                        "authorId": "5586582",
                        "name": "Judy Fan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "This is achieved through the use of contrastive loss functions, either in unsupervised contrastive learning where labels are absent [6,7,12,25,30,40], or in supervised contrastive learning where labels are available [8,22,32, 39]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "44f451d7723e162f52e021b525718c4f37a9d161",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-03967",
                    "ArXiv": "2307.03967",
                    "DOI": "10.48550/arXiv.2307.03967",
                    "CorpusId": 259501647
                },
                "corpusId": 259501647,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/44f451d7723e162f52e021b525718c4f37a9d161",
                "title": "End-to-End Supervised Multilabel Contrastive Learning",
                "abstract": "Multilabel representation learning is recognized as a challenging problem that can be associated with either label dependencies between object categories or data-related issues such as the inherent imbalance of positive/negative samples. Recent advances address these challenges from model- and data-centric viewpoints. In model-centric, the label correlation is obtained by an external model designs (e.g., graph CNN) to incorporate an inductive bias for training. However, they fail to design an end-to-end training framework, leading to high computational complexity. On the contrary, in data-centric, the realistic nature of the dataset is considered for improving the classification while ignoring the label dependencies. In this paper, we propose a new end-to-end training framework -- dubbed KMCL (Kernel-based Mutlilabel Contrastive Learning) -- to address the shortcomings of both model- and data-centric designs. The KMCL first transforms the embedded features into a mixture of exponential kernels in Gaussian RKHS. It is then followed by encoding an objective loss that is comprised of (a) reconstruction loss to reconstruct kernel representation, (b) asymmetric classification loss to address the inherent imbalance problem, and (c) contrastive loss to capture label correlation. The KMCL models the uncertainty of the feature encoder while maintaining a low computational footprint. Extensive experiments are conducted on image classification tasks to showcase the consistent improvements of KMCL over the SOTA methods. PyTorch implementation is provided in \\url{https://github.com/mahdihosseini/KMCL}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "4400341",
                        "name": "A. Sajedi"
                    },
                    {
                        "authorId": "2132439321",
                        "name": "Samir Khaki"
                    },
                    {
                        "authorId": "1705037",
                        "name": "K. Plataniotis"
                    },
                    {
                        "authorId": "143854143",
                        "name": "M. S. Hosseini"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "Feature Suppression in Unsupervised CL. Feature suppression has been empirically observed by Tian et al. (2020); Chen et al. (2021); Robinson et al. (2021) but we lack a theoretical formulation of this phenomenon.",
                "Effect of embedding size on feature suppression in MNIST RandBit(Chen et al., 2021).",
                "Empirically, feature suppression can be observed due to a variety of reasons (Li et al., 2020; Chen et al., 2021; Robinson et al., 2021).",
                "To provide empirical evidence for this, we conduct two sets of experiments:\nFirst, we train 5-layer convolutional networks on the RandomBit dataset with the same setup as in (Chen et al., 2021), but we vary the embedding size (see details in Appendix H).",
                "First, we train 5-layer convolutional networks on the RandomBit dataset with the same setup as in (Chen et al., 2021), but we vary the embedding size (see details in Appendix H).",
                "Second, we train ResNet18 (He et al., 2016) on the CIFAR-10/100 RandBit Dataset, constructed similarly to the MNIST RandBit dataset but with images from CIFAR10/100 (Krizhevsky et al., 2009) (see Appendix H.1).",
                "We empirically examine the impact of the joint loss on MNIST RandBit, CIFAR-100, and CIFAR-100 RandBit.",
                "Similarly, unsupervised CL can be afflicted with feature suppression (Chen et al., 2021; Robinson et al., 2021) where easy but class-irrelevant features suppress the learning of harder class-relevant ones; deteriorating the generalizability of the obtained representations."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c6d35e571c561aeaa68e896ab9b07c32f778d50e",
                "externalIds": {
                    "ArXiv": "2305.16536",
                    "DBLP": "conf/icml/XueJGCM23",
                    "DOI": "10.48550/arXiv.2305.16536",
                    "CorpusId": 258947662
                },
                "corpusId": 258947662,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c6d35e571c561aeaa68e896ab9b07c32f778d50e",
                "title": "Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression",
                "abstract": "Contrastive learning (CL) has emerged as a powerful technique for representation learning, with or without label supervision. However, supervised CL is prone to collapsing representations of subclasses within a class by not capturing all their features, and unsupervised CL may suppress harder class-relevant features by focusing on learning easy class-irrelevant features; both significantly compromise representation quality. Yet, there is no theoretical understanding of \\textit{class collapse} or \\textit{feature suppression} at \\textit{test} time. We provide the first unified theoretically rigorous framework to determine \\textit{which} features are learnt by CL. Our analysis indicate that, perhaps surprisingly, bias of (stochastic) gradient descent towards finding simpler solutions is a key factor in collapsing subclass representations and suppressing harder class-relevant features. Moreover, we present increasing embedding dimensionality and improving the quality of data augmentations as two theoretically motivated solutions to {feature suppression}. We also provide the first theoretical explanation for why employing supervised and unsupervised CL together yields higher-quality representations, even when using commonly-used stochastic gradient methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "150353219",
                        "name": "Yihao Xue"
                    },
                    {
                        "authorId": "2114034723",
                        "name": "S. Joshi"
                    },
                    {
                        "authorId": "2214450654",
                        "name": "Eric Gan"
                    },
                    {
                        "authorId": "2158177808",
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "authorId": "2389094",
                        "name": "Baharan Mirzasoleiman"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "360f500780e3716c2df40e030ac3b4c51a533a60",
                "externalIds": {
                    "PubMedCentral": "10483029",
                    "DOI": "10.1101/2023.05.17.541163",
                    "CorpusId": 258834857,
                    "PubMed": "37589603"
                },
                "corpusId": 258834857,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/360f500780e3716c2df40e030ac3b4c51a533a60",
                "title": "iDeLUCS: a deep learning interactive tool for alignment-free clustering of DNA sequences",
                "abstract": "Summary We present an interactive Deep Learning-based software tool for Unsupervised Clustering of DNA Sequences (iDeLUCS), that detects genomic signatures and uses them to cluster DNA sequences, without the need for sequence alignment or taxonomic identifiers. iDeLUCS is scalable and user-friendly: Its graphical user interface, with support for hardware acceleration, allows the practitioner to fine-tune the different hyper-parameters involved in the training process without requiring extensive knowledge of deep learning. The performance of iDeLUCS was evaluated on a diverse set of datasets: several real genomic datasets from organisms in kingdoms Animalia, Protista, Fungi, Bacteria, and Archaea, three datasets of viral genomes, a dataset of simulated metagenomic reads from microbial genomes, and multiple datasets of synthetic DNA sequences. The performance of iDeLUCS was compared to that of two classical clustering algorithms (k-means++ and GMM) and two clustering algorithms specialized in DNA sequences (MeShClust v3.0 and DeLUCS), using both intrinsic cluster evaluation metrics and external evaluation metrics. In terms of unsupervised clustering accuracy, iDeLUCS outperforms the two classical algorithms by an average of \u223c 20%, and the two specialized algorithms by an average of \u223c 12%, on the datasets of real DNA sequences analyzed. Overall, our results indicate that iDeLUCS is a robust clustering method suitable for the clustering of large and diverse datasets of unlabelled DNA sequences. Availability and implementation iDeLUCS is available at our github repository under the terms of the MIT licence. Contact pmillana@uwaterloo.ca Supplementary information Supplementary data are available at Bioinformatics online.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2218058287",
                        "name": "Pablo Mill\u00e1n Arias"
                    },
                    {
                        "authorId": "31640060",
                        "name": "K. Hill"
                    },
                    {
                        "authorId": "144073818",
                        "name": "L. Kari"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Additionally, contrastive methods can perform poorly when the task of interest is not determined by the dominant features of an image [Chen et al., 2021].",
                "If the task of interest is not determined by the dominant features of an image, or is obscured by the transformations used to train the model, the self-supervised model may perform poorly at clustering the data into relevant groups [Chen et al., 2021]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f31073e208318b7be2b9e5c56b97197024bd1ec3",
                "externalIds": {
                    "ArXiv": "2305.10071",
                    "CorpusId": 259088461
                },
                "corpusId": 259088461,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f31073e208318b7be2b9e5c56b97197024bd1ec3",
                "title": "Cold PAWS: Unsupervised class discovery and addressing the cold-start problem for semi-supervised learning",
                "abstract": "In many machine learning applications, labeling datasets can be an arduous and time-consuming task. Although research has shown that semi-supervised learning techniques can achieve high accuracy with very few labels within the field of computer vision, little attention has been given to how images within a dataset should be selected for labeling. In this paper, we propose a novel approach based on well-established self-supervised learning, clustering, and manifold learning techniques that address this challenge of selecting an informative image subset to label in the first instance, which is known as the cold-start or unsupervised selective labelling problem. We test our approach using several publicly available datasets, namely CIFAR10, Imagenette, DeepWeeds, and EuroSAT, and observe improved performance with both supervised and semi-supervised learning strategies when our label selection strategy is used, in comparison to random sampling. We also obtain superior performance for the datasets considered with a much simpler approach compared to other methods in the literature.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2217684532",
                        "name": "Evelyn J. Mannix"
                    },
                    {
                        "authorId": "2821071",
                        "name": "H. Bondell"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "\u2026in the machine learning field has shown that the learned Spiro-CLF feature space may encode other latent representations of the underlying data (Chen et al., 2021) and has been used for clustering (Li et al., 2021; Caron et al., 2020) as well as combined with generative models (Kim et al.,\u2026",
                "Existing literature in the machine learning field has shown that the learned Spiro-CLF feature space may encode other latent representations of the underlying data (Chen et al., 2021) and has been used for clustering (Li et al."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "df4a76174367887df57cfa78bee48656f4b6b3a4",
                "externalIds": {
                    "PubMedCentral": "10168495",
                    "DOI": "10.1101/2023.04.28.23289178",
                    "CorpusId": 258396281,
                    "PubMed": "37162978"
                },
                "corpusId": 258396281,
                "publicationVenue": {
                    "id": "d5e5b5e7-54b1-4f53-82fc-4853f3e71c58",
                    "name": "medRxiv",
                    "type": "journal",
                    "url": "https://www.medrxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/df4a76174367887df57cfa78bee48656f4b6b3a4",
                "title": "Deep Learning Utilizing Suboptimal Spirometry Data to Improve Lung Function and Mortality Prediction in the UK Biobank",
                "abstract": "Background: Spirometry measures lung function by selecting the best of multiple efforts meeting pre-specified quality control (QC), and reporting two key metrics: forced expiratory volume in 1 second (FEV1) and forced vital capacity (FVC). We hypothesize that discarded submaximal and QC- failing data meaningfully contribute to the prediction of airflow obstruction and all-cause mortality. Methods: We evaluated volume-time spirometry data from the UK Biobank. We identified \"best\" spirometry efforts as those passing QC with the maximum FVC. \"Discarded\" efforts were either submaximal or failed QC. To create a combined representation of lung function we implemented a contrastive learning approach, Spirogram-based Contrastive Learning Framework (Spiro-CLF), which utilized all recorded volume-time curves per participant and applied different transformations (e.g. flow-volume, flow-time). In a held-out 20% testing subset we applied the Spiro-CLF representation of a participant's overall lung function to 1) binary predictions of FEV1/FVC < 0.7 and FEV1 Percent Predicted (FEV1PP) < 80%, indicative of airflow obstruction, and 2) Cox regression for all-cause mortality. Findings: We included 940,705 volume-time curves from 352,684 UK Biobank participants with 2-3 spirometry efforts per individual (66.7% with 3 efforts) and at least one QC-passing spirometry effort. Of all spirometry efforts, 24.1% failed QC and 37.5% were submaximal. Spiro-CLF prediction of FEV1/FVC < 0.7 utilizing discarded spirometry efforts had an Area under the Receiver Operating Characteristics (AUROC) of 0.981 (0.863 for FEV1PP prediction). Incorporating discarded spirometry efforts in all-cause mortality prediction was associated with a concordance index (c-index)of 0.654, which exceeded the c-indices from FEV1 (0.590), FVC (0.559), or FEV1/FVC (0.599) from each participant's single best effort. Interpretation: A contrastive learning model using raw spirometry curves can accurately predict lung function using submaximal and QC-failing efforts. This model also has superior prediction of all-cause mortality compared to standard lung function measurements.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2164482366",
                        "name": "Davin Hill"
                    },
                    {
                        "authorId": "1468795232",
                        "name": "M. Torop"
                    },
                    {
                        "authorId": "81326211",
                        "name": "A. Masoomi"
                    },
                    {
                        "authorId": "35190471",
                        "name": "P. Castaldi"
                    },
                    {
                        "authorId": "144639847",
                        "name": "E. Silverman"
                    },
                    {
                        "authorId": "14949877",
                        "name": "S. Bodduluri"
                    },
                    {
                        "authorId": "2135086935",
                        "name": "S. Bhatt"
                    },
                    {
                        "authorId": "2520251",
                        "name": "T. Yun"
                    },
                    {
                        "authorId": "2059066071",
                        "name": "C. McLean"
                    },
                    {
                        "authorId": "2420527",
                        "name": "F. Hormozdiari"
                    },
                    {
                        "authorId": "2101977865",
                        "name": "J. Dy"
                    },
                    {
                        "authorId": "2149542041",
                        "name": "M. Cho"
                    },
                    {
                        "authorId": "152582115",
                        "name": "B. D. Hobbs"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "140f0b294938f89e2d80009c1555b5b473452399",
                "externalIds": {
                    "DBLP": "conf/cvpr/KimKHSR23",
                    "ArXiv": "2304.01434",
                    "DOI": "10.1109/CVPR52729.2023.00370",
                    "CorpusId": 257921290
                },
                "corpusId": 257921290,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/140f0b294938f89e2d80009c1555b5b473452399",
                "title": "VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution",
                "abstract": "Since the introduction of deep learning, a wide scope of representation properties, such as decorrelation, whitening, disentanglement, rank, isotropy, and mutual information, have been studied to improve the quality of representation. However, manipulating such properties can be challenging in terms of implementational effectiveness and general applicability. To address these limitations, we propose to regularize von Neumann entropy (VNE) of representation. First, we demonstrate that the mathematical formulation of VNE is superior in effectively manipulating the eigenvalues of the representation autocorrelation matrix. Then, we demonstrate that it is widely applicable in improving state-of-the-art algorithms or popular benchmark algorithms by investigating domain-generalization, meta-learning, self-supervised learning, and generative models. In addition, we formally establish theoretical connections with rank, disentanglement, and isotropy of representation. Finally, we provide discussions on the dimension control of VNE and the relationship with Shannon entropy. Code is available at: https://github.com/jaeill/CVPR23-VNE.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2157223778",
                        "name": "Jaeill Kim"
                    },
                    {
                        "authorId": "15230387",
                        "name": "Suhyun Kang"
                    },
                    {
                        "authorId": "2030714842",
                        "name": "Duhun Hwang"
                    },
                    {
                        "authorId": "4975377",
                        "name": "Jungwook Shin"
                    },
                    {
                        "authorId": "143884871",
                        "name": "Wonjong Rhee"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "It is shown by Tu et al. (2019) that if F0 is a function space of feedforward (deep) neural networks, where each neural networks have weight matrices whose norms are bounded by some universal constant, and Lipschitz activation functions that vanish at the origin, then D(F0, \u2016 \u00b7 \u2016\u221e) < +\u221e holds.",
                "In this proof, we follow the proof idea of Tu et al. (2019) (see Lemma 5 in Tu et al. (2019)). Since our setup is different from Tu et al. (2019), we need to modify the proof and add several new techniques.",
                "In this proof, we follow the proof idea of Tu et al. (2019) (see Lemma 5 in Tu et al.",
                "In this proof, we follow the proof idea of Tu et al. (2019) (see Lemma 5 in Tu et al. (2019))."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "d87ecdcb490241ec204bd94b89158e06dd0747e2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-00395",
                    "ArXiv": "2304.00395",
                    "DOI": "10.48550/arXiv.2304.00395",
                    "CorpusId": 257912793
                },
                "corpusId": 257912793,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d87ecdcb490241ec204bd94b89158e06dd0747e2",
                "title": "Towards Understanding the Mechanism of Contrastive Learning via Similarity Structure: A Theoretical Analysis",
                "abstract": "Contrastive learning is an efficient approach to self-supervised representation learning. Although recent studies have made progress in the theoretical understanding of contrastive learning, the investigation of how to characterize the clusters of the learned representations is still limited. In this paper, we aim to elucidate the characterization from theoretical perspectives. To this end, we consider a kernel-based contrastive learning framework termed Kernel Contrastive Learning (KCL), where kernel functions play an important role when applying our theoretical results to other frameworks. We introduce a formulation of the similarity structure of learned representations by utilizing a statistical dependency viewpoint. We investigate the theoretical properties of the kernel-based contrastive loss via this formulation. We first prove that the formulation characterizes the structure of representations learned with the kernel-based contrastive learning framework. We show a new upper bound of the classification error of a downstream task, which explains that our theory is consistent with the empirical success of contrastive learning. We also establish a generalization error bound of KCL. Finally, we show a guarantee for the generalization ability of KCL to the downstream classification task via a surrogate bound.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2210799453",
                        "name": "Hiroki Waida"
                    },
                    {
                        "authorId": "2057979107",
                        "name": "Yuichiro Wada"
                    },
                    {
                        "authorId": "2202704811",
                        "name": "L'eo And'eol"
                    },
                    {
                        "authorId": "2185326325",
                        "name": "Takumi Nakagawa"
                    },
                    {
                        "authorId": "2145061398",
                        "name": "Yuhui Zhang"
                    },
                    {
                        "authorId": "1897617",
                        "name": "T. Kanamori"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "\u2026information theory (Tsai et al., 2020; 2021; Tosh et al., 2021a;b), loss landscapes and training dynamics (Tian et al., 2020; Wang & Isola, 2020; Chen et al., 2021; Tian et al., 2021; Jing et al., 2021; Wen & Li, 2021; Pokle et al., 2022; Ziyin et al., 2022; Assran et al., 2022a), and kernel\u2026",
                ", 2021a;b), loss landscapes and training dynamics (Tian et al., 2020; Wang & Isola, 2020; Chen et al., 2021; Tian et al., 2021; Jing et al., 2021; Wen & Li, 2021; Pokle et al., 2022; Ziyin et al., 2022; Assran et al., 2022a), and kernel and spectral methods (Kiani et al."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0a16511a5a0952f5530b8434a91cdf9df912f28d",
                "externalIds": {
                    "DBLP": "conf/icml/SimonKLGFA23",
                    "ArXiv": "2303.15438",
                    "DOI": "10.48550/arXiv.2303.15438",
                    "CorpusId": 257767106
                },
                "corpusId": 257767106,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0a16511a5a0952f5530b8434a91cdf9df912f28d",
                "title": "On the stepwise nature of self-supervised learning",
                "abstract": "We present a simple picture of the training process of joint embedding self-supervised learning methods. We find that these methods learn their high-dimensional embeddings one dimension at a time in a sequence of discrete, well-separated steps. We arrive at this conclusion via the study of a linearized model of Barlow Twins applicable to the case in which the trained network is infinitely wide. We solve the training dynamics of this model from small initialization, finding that the model learns the top eigenmodes of a certain contrastive kernel in a stepwise fashion, and obtain a closed-form expression for the final learned representations. Remarkably, we then see the same stepwise learning phenomenon when training deep ResNets using the Barlow Twins, SimCLR, and VICReg losses. Our theory suggests that, just as kernel regression can be thought of as a model of supervised learning, kernel PCA may serve as a useful model of self-supervised learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2093422698",
                        "name": "James B. Simon"
                    },
                    {
                        "authorId": "1453608917",
                        "name": "Maksis Knutins"
                    },
                    {
                        "authorId": "12907562",
                        "name": "Liu Ziyin"
                    },
                    {
                        "authorId": "2212857568",
                        "name": "Daniel Geisz"
                    },
                    {
                        "authorId": "2196564612",
                        "name": "Abraham J. Fetterman"
                    },
                    {
                        "authorId": "144803472",
                        "name": "Joshua Albrecht"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fd4f02958c6454d67a0caa0e9166f4eccd2df57b",
                "externalIds": {
                    "DBLP": "conf/iclr/KuklevaBSK023",
                    "ArXiv": "2303.13664",
                    "DOI": "10.48550/arXiv.2303.13664",
                    "CorpusId": 257757379
                },
                "corpusId": 257757379,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/fd4f02958c6454d67a0caa0e9166f4eccd2df57b",
                "title": "Temperature Schedules for Self-Supervised Contrastive Methods on Long-Tail Data",
                "abstract": "Most approaches for self-supervised learning (SSL) are optimised on curated balanced datasets, e.g. ImageNet, despite the fact that natural data usually exhibits long-tail distributions. In this paper, we analyse the behaviour of one of the most popular variants of SSL, i.e. contrastive methods, on long-tail data. In particular, we investigate the role of the temperature parameter $\\tau$ in the contrastive loss, by analysing the loss through the lens of average distance maximisation, and find that a large $\\tau$ emphasises group-wise discrimination, whereas a small $\\tau$ leads to a higher degree of instance discrimination. While $\\tau$ has thus far been treated exclusively as a constant hyperparameter, in this work, we propose to employ a dynamic $\\tau$ and show that a simple cosine schedule can yield significant improvements in the learnt representations. Such a schedule results in a constant `task switching' between an emphasis on instance discrimination and group-wise discrimination and thereby ensures that the model learns both group-wise features, as well as instance-specific details. Since frequent classes benefit from the former, while infrequent classes require the latter, we find this method to consistently improve separation between the classes in long-tail data without any additional computational cost.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "146108424",
                        "name": "A. Kukleva"
                    },
                    {
                        "authorId": "1388723597",
                        "name": "Moritz Bohle"
                    },
                    {
                        "authorId": "48920094",
                        "name": "B. Schiele"
                    },
                    {
                        "authorId": "2077580009",
                        "name": "Hilde Kuehne"
                    },
                    {
                        "authorId": "49359942",
                        "name": "C. Rupprecht"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Several theoretical studies show that contrastive loss optimizes data representations by aligning the same image\u2019s two views (positive pairs) while pushing different images (negative pairs) away on the hypersphere [4, 15, 66, 68]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6e91f0649bbb5f5ed408e24c458bc337df1e98e5",
                "externalIds": {
                    "DBLP": "conf/cvpr/Zhang0Z23",
                    "ArXiv": "2303.11526",
                    "DOI": "10.1109/CVPR52729.2023.01267",
                    "CorpusId": 257637160
                },
                "corpusId": 257637160,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6e91f0649bbb5f5ed408e24c458bc337df1e98e5",
                "title": "PRISE: Demystifying Deep Lucas-Kanade with Strongly Star-Convex Constraints for Multimodel Image Alignment",
                "abstract": "The Lucas-Kanade (LK) method is a classic iterative homography estimation algorithm for image alignment, but often suffers from poor local optimality especially when image pairs have large distortions. To address this challenge, in this paper we propose a novel Deep Star-Convexified Lucas-Kanade (PRISE) method for multimodel image alignment by introducing strongly star-convex constraints into the optimization problem. Our basic idea is to enforce the neural network to approximately learn a star-convex loss landscape around the ground truth give any data to facilitate the convergence of the LK method to the ground truth through the high dimensional space defined by the network. This leads to a minimax learning problem, with contrastive (hinge) losses due to the definition of strong star-convexity that are appended to the original loss for training. We also provide an efficient sampling based algorithm to leverage the training cost, as well as some analysis on the quality of the solutions from PRISE. We further evaluate our approach on benchmark datasets such as MSCOCO, GoogleEarth, and GoogleMap, and demonstrate state-of-the-art results, especially for small pixel errors. Code can be downloaded from https://github.com/Zhang-VISLab.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2145047031",
                        "name": "Yiqing Zhang"
                    },
                    {
                        "authorId": "71269276",
                        "name": "Xinming Huang"
                    },
                    {
                        "authorId": "7969330",
                        "name": "Ziming Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "While classical approaches include colorization [38], inpainting [28] and solving jigsaw puzzles [25], recent SoTA approaches have focused on contrastive learning [7,8,11], which aims at pulling similar data points (positive samples) closer together, along with pushing apart dissimilar points (negative samples) in the embedding space.",
                "It is well-known [7, 8] that contrastive approaches work better with higher batch sizes (or greater amount of data [11]), due to the availability of more negative samples.",
                "Contrastive: We treat contrastive SSL as a baseline and employ the SoTA SimCLR [7] algorithm, which uses the InfoNCE [8, 15, 26] contrastive loss to push embeddings of different views of the same image closer and pull apart those of different images.",
                "This may be attributed to the severe decrease in the local client data (as the datasets are even smaller; \u2248 10x smaller than the Organ datasets) which hurts the learning ability of self-supervision models in general [8, 9].",
                "Contrastive learning methods like SimCLR are heavily dependent on the number of negative samples available, and thus perform better under higher batch sizes [7, 8], while non-contrastive methods, not having such a dependency, are not affected as much."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "4e74709758dd8cd911798a1e0bab9fdc6a0c124f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-05556",
                    "ArXiv": "2303.05556",
                    "DOI": "10.48550/arXiv.2303.05556",
                    "CorpusId": 257482594
                },
                "corpusId": 257482594,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4e74709758dd8cd911798a1e0bab9fdc6a0c124f",
                "title": "An Evaluation of Non-Contrastive Self-Supervised Learning for Federated Medical Image Analysis",
                "abstract": "Privacy and annotation bottlenecks are two major issues that profoundly affect the practicality of machine learning-based medical image analysis. Although significant progress has been made in these areas, these issues are not yet fully resolved. In this paper, we seek to tackle these concerns head-on and systematically explore the applicability of non-contrastive self-supervised learning (SSL) algorithms under federated learning (FL) simulations for medical image analysis. We conduct thorough experimentation of recently proposed state-of-the-art non-contrastive frameworks under standard FL setups. With the SoTA Contrastive Learning algorithm, SimCLR as our comparative baseline, we benchmark the performances of our 4 chosen non-contrastive algorithms under non-i.i.d. data conditions and with a varying number of clients. We present a holistic evaluation of these techniques on 6 standardized medical imaging datasets. We further analyse different trends inferred from the findings of our research, with the aim to find directions for further research based on ours. To the best of our knowledge, ours is the first to perform such a thorough analysis of federated self-supervised learning for medical imaging. All of our source code will be made public upon acceptance of the paper.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2146696888",
                        "name": "Soumitri Chattopadhyay"
                    },
                    {
                        "authorId": "9254628",
                        "name": "Soham Ganguly"
                    },
                    {
                        "authorId": "2172974355",
                        "name": "Sreejit Chaudhury"
                    },
                    {
                        "authorId": "40648427",
                        "name": "Sayan Nag"
                    },
                    {
                        "authorId": "1379534518",
                        "name": "S. Chattopadhyay"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "SSL techniques, on the other hand, can learn very robust features without the need for labels, making them immune to such problems [1, 10].",
                "For CL, higher batch size ensures increase in negative samples [10, 4], while in NCL methods, feature normalization is done along the batch dimension [3].",
                "CL methods are expected to show improvement with increase in batch size [1, 10] due to the rise in negative samples.",
                "We attribute this to the fact that NCL learning algorithms can work effectively with fewer number of data samples, since there is no explicit need for a higher number of samples, as in the case of CL methods [1, 10]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "18df231e349aa06dcd679fc9b93437a6f405e72f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-02245",
                    "ArXiv": "2303.02245",
                    "DOI": "10.48550/arXiv.2303.02245",
                    "CorpusId": 257364816
                },
                "corpusId": 257364816,
                "publicationVenue": {
                    "id": "b6369c33-5d70-463c-8e82-95a54efa3cc8",
                    "name": "International Conference on Information Photonics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Image Process",
                        "ICIP",
                        "Int Conf Inf Photonics",
                        "International Conference on Image Processing"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/18df231e349aa06dcd679fc9b93437a6f405e72f",
                "title": "Exploring Self-Supervised Representation Learning For Low-Resource Medical Image Analysis",
                "abstract": "The success of self-supervised learning (SSL) has mostly been attributed to the availability of unlabeled yet large-scale datasets. However, in a specialized domain such as medical imaging which is a lot different from natural images, the assumption of data availability is unrealistic and impractical, as the data itself is scanty and found in small databases, collected for specific prognosis tasks. To this end, we seek to investigate the applicability of self-supervised learning algorithms on small-scale medical imaging datasets. In particular, we evaluate $4$ state-of-the-art SSL methods on three publicly accessible \\emph{small} medical imaging datasets. Our investigation reveals that in-domain low-resource SSL pre-training can yield competitive performance to transfer learning from large-scale datasets (such as ImageNet). Furthermore, we extensively analyse our empirical findings to provide valuable insights that can motivate for further research towards circumventing the need for pre-training on a large image corpus. To the best of our knowledge, this is the first attempt to holistically explore self-supervision on low-resource medical datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2146696888",
                        "name": "Soumitri Chattopadhyay"
                    },
                    {
                        "authorId": "9254628",
                        "name": "Soham Ganguly"
                    },
                    {
                        "authorId": "2172974355",
                        "name": "Sreejit Chaudhury"
                    },
                    {
                        "authorId": "40648427",
                        "name": "Sayan Nag"
                    },
                    {
                        "authorId": "2239807458",
                        "name": "Samiran Chattopadhyay"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "result"
            ],
            "contexts": [
                "We note that this loss is similar to the generalized InfoNCE loss proposed by Chen et al. (2021).",
                "The work in Chen et al. (2021) generalizes the InfoNCE loss to a larger family of losses with alignment\nand uniformity terms weighted according to a hyperparameter."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a3e5ff30947da8ba9688f2d28bac5cf7d3d49985",
                "externalIds": {
                    "DBLP": "conf/colt/ParulekarCSMS23",
                    "ArXiv": "2302.07920",
                    "DOI": "10.48550/arXiv.2302.07920",
                    "CorpusId": 256901224
                },
                "corpusId": 256901224,
                "publicationVenue": {
                    "id": "24b0721b-0592-414a-ac79-7271515aaab0",
                    "name": "Annual Conference Computational Learning Theory",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Learn Theory",
                        "COLT",
                        "Conference on Learning Theory",
                        "Annu Conf Comput Learn Theory"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=536"
                },
                "url": "https://www.semanticscholar.org/paper/a3e5ff30947da8ba9688f2d28bac5cf7d3d49985",
                "title": "InfoNCE Loss Provably Learns Cluster-Preserving Representations",
                "abstract": "The goal of contrasting learning is to learn a representation that preserves underlying clusters by keeping samples with similar content, e.g. the ``dogness'' of a dog, close to each other in the space generated by the representation. A common and successful approach for tackling this unsupervised learning problem is minimizing the InfoNCE loss associated with the training samples, where each sample is associated with their augmentations (positive samples such as rotation, crop) and a batch of negative samples (unrelated samples). To the best of our knowledge, it was unanswered if the representation learned by minimizing the InfoNCE loss preserves the underlying data clusters, as it only promotes learning a representation that is faithful to augmentations, i.e., an image and its augmentations have the same representation. Our main result is to show that the representation learned by InfoNCE with a finite number of negative samples is also consistent with respect to clusters in the data, under the condition that the augmentation sets within clusters may be non-overlapping but are close and intertwined, relative to the complexity of the learning function class.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "103473489",
                        "name": "Advait Parulekar"
                    },
                    {
                        "authorId": "2057634000",
                        "name": "Liam Collins"
                    },
                    {
                        "authorId": "145455859",
                        "name": "Karthikeyan Shanmugam"
                    },
                    {
                        "authorId": "2706423",
                        "name": "Aryan Mokhtari"
                    },
                    {
                        "authorId": "1688634",
                        "name": "S. Shakkottai"
                    }
                ]
            }
        },
        {
            "intents": [
                "result"
            ],
            "contexts": [
                "This is consistent with the observations uncovered by the recent empirical findings [19, 92]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8ebc684a24d373a3c52435688019ae81c8197286",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-01735",
                    "ArXiv": "2302.01735",
                    "DOI": "10.48550/arXiv.2302.01735",
                    "CorpusId": 256598085
                },
                "corpusId": 256598085,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8ebc684a24d373a3c52435688019ae81c8197286",
                "title": "Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective",
                "abstract": "For medical image segmentation, contrastive learning is the dominant practice to improve the quality of visual representations by contrasting semantically similar and dissimilar pairs of samples. This is enabled by the observation that without accessing ground truth labels, negative examples with truly dissimilar anatomical features, if sampled, can significantly improve the performance. In reality, however, these samples may come from similar anatomical regions and the models may struggle to distinguish the minority tail-class samples, making the tail classes more prone to misclassification, both of which typically lead to model collapse. In this paper, we propose ARCO, a semi-supervised contrastive learning (CL) framework with stratified group theory for medical image segmentation. In particular, we first propose building ARCO through the concept of variance-reduced estimation and show that certain variance-reduction techniques are particularly beneficial in pixel/voxel-level segmentation tasks with extremely limited labels. Furthermore, we theoretically prove these sampling techniques are universal in variance reduction. Finally, we experimentally validate our approaches on eight benchmarks, i.e., five 2D/3D medical and three semantic segmentation datasets, with different label settings, and our methods consistently outperform state-of-the-art semi-supervised methods. Additionally, we augment the CL frameworks with these sampling techniques and demonstrate significant gains over previous methods. We believe our work is an important step towards semi-supervised medical image segmentation by quantifying the limitation of current self-supervision objectives for accomplishing such challenging safety-critical tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "7694093",
                        "name": "Chenyu You"
                    },
                    {
                        "authorId": "2054962899",
                        "name": "Weicheng Dai"
                    },
                    {
                        "authorId": "51270420",
                        "name": "Yifei Min"
                    },
                    {
                        "authorId": "1927674",
                        "name": "Fenglin Liu"
                    },
                    {
                        "authorId": "1635440219",
                        "name": "Xiaoran Zhang"
                    },
                    {
                        "authorId": null,
                        "name": "Chen Feng"
                    },
                    {
                        "authorId": "31799453",
                        "name": "D. Clifton"
                    },
                    {
                        "authorId": "2107323185",
                        "name": "S. K. Zhou"
                    },
                    {
                        "authorId": "1700330",
                        "name": "L. Staib"
                    },
                    {
                        "authorId": "2140555772",
                        "name": "J. Duncan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Several theoretical studies show that self-supervised contrastive loss optimizes data representations by aligning the same image\u2019s two views (positive pairs) while pushing different images (negative pairs) away on the hypersphere [2, 15, 81, 82]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f353744b4615a47ef7ab8f5c80d36e6906da652f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-01409",
                    "ArXiv": "2302.01409",
                    "DOI": "10.48550/arXiv.2302.01409",
                    "CorpusId": 256598203
                },
                "corpusId": 256598203,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f353744b4615a47ef7ab8f5c80d36e6906da652f",
                "title": "Hyperbolic Contrastive Learning",
                "abstract": "Learning good image representations that are beneficial to downstream tasks is a challenging task in computer vision. As such, a wide variety of self-supervised learning approaches have been proposed. Among them, contrastive learning has shown competitive performance on several benchmark datasets. The embeddings of contrastive learning are arranged on a hypersphere that results in using the inner (dot) product as a distance measurement in Euclidean space. However, the underlying structure of many scientific fields like social networks, brain imaging, and computer graphics data exhibit highly non-Euclidean latent geometry. We propose a novel contrastive learning framework to learn semantic relationships in the hyperbolic space. Hyperbolic space is a continuous version of trees that naturally owns the ability to model hierarchical structures and is thus beneficial for efficient contrastive representation learning. We also extend the proposed Hyperbolic Contrastive Learning (HCL) to the supervised domain and studied the adversarial robustness of HCL. The comprehensive experiments show that our proposed method achieves better results on self-supervised pretraining, supervised classification, and higher robust accuracy than baseline methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51315458",
                        "name": "Yun Yue"
                    },
                    {
                        "authorId": "34033841",
                        "name": "Fangzhou Lin"
                    },
                    {
                        "authorId": "2109752225",
                        "name": "Kazunori D. Yamada"
                    },
                    {
                        "authorId": "7969330",
                        "name": "Ziming Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Several recent works have demonstrated SB on a variety of semi-real constructed datasets (Geirhos et al., 2018; Shah et al., 2020; Chen et al., 2021), and have hypothesized SB to be the key reason for NN\u2019s brittleness to distribution shifts (Shah et al.",
                "Existing works like (Geirhos et al., 2018; Shah et al., 2020; Chen et al., 2021) avoid this challenge of vague feature defini-",
                "Existing works like (Geirhos et al., 2018; Shah et al., 2020; Chen et al., 2021) avoid this challenge of vague feature defini-\n2Image source: Wikipedia (swa), (bea).\nar X\niv :2\n30 2.",
                "Several recent works have demonstrated SB on a variety of semi-real constructed datasets (Geirhos et al., 2018; Shah et al., 2020; Chen et al., 2021), and have hypothesized SB to be the key reason for NN\u2019s brittleness to distribution shifts (Shah et al., 2020).",
                "Abstract Recent works (Shah et al., 2020; Chen et al., 2021) have demonstrated that neural networks exhibit extreme simplicity bias (SB)."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2b7bec81a6ece230ae9361c22336f8a0c70ada5d",
                "externalIds": {
                    "ArXiv": "2302.00457",
                    "DBLP": "journals/corr/abs-2302-00457",
                    "DOI": "10.48550/arXiv.2302.00457",
                    "CorpusId": 256459772
                },
                "corpusId": 256459772,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2b7bec81a6ece230ae9361c22336f8a0c70ada5d",
                "title": "Simplicity Bias in 1-Hidden Layer Neural Networks",
                "abstract": "Recent works have demonstrated that neural networks exhibit extreme simplicity bias(SB). That is, they learn only the simplest features to solve a task at hand, even in the presence of other, more robust but more complex features. Due to the lack of a general and rigorous definition of features, these works showcase SB on semi-synthetic datasets such as Color-MNIST, MNIST-CIFAR where defining features is relatively easier. In this work, we rigorously define as well as thoroughly establish SB for one hidden layer neural networks. More concretely, (i) we define SB as the network essentially being a function of a low dimensional projection of the inputs (ii) theoretically, we show that when the data is linearly separable, the network primarily depends on only the linearly separable ($1$-dimensional) subspace even in the presence of an arbitrarily large number of other, more complex features which could have led to a significantly more robust classifier, (iii) empirically, we show that models trained on real datasets such as Imagenette and Waterbirds-Landbirds indeed depend on a low dimensional projection of the inputs, thereby demonstrating SB on these datasets, iv) finally, we present a natural ensemble approach that encourages diversity in models by training successive models on features not used by earlier models, and demonstrate that it yields models that are significantly more robust to Gaussian noise.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2003598814",
                        "name": "Depen Morwani"
                    },
                    {
                        "authorId": "33653179",
                        "name": "Jatin Batra"
                    },
                    {
                        "authorId": "48964143",
                        "name": "Prateek Jain"
                    },
                    {
                        "authorId": "1751626",
                        "name": "Praneeth Netrapalli"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4dddd1aca14589219c16860cc774986da459861d",
                "externalIds": {
                    "ArXiv": "2301.09164",
                    "DBLP": "journals/corr/abs-2301-09164",
                    "DOI": "10.48550/arXiv.2301.09164",
                    "CorpusId": 256105045
                },
                "corpusId": 256105045,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4dddd1aca14589219c16860cc774986da459861d",
                "title": "Unifying Synergies between Self-supervised Learning and Dynamic Computation",
                "abstract": "Computationally expensive training strategies make self-supervised learning (SSL) impractical for resource constrained industrial settings. Techniques like knowledge distillation (KD), dynamic computation (DC), and pruning are often used to obtain a lightweightmodel, which usually involves multiple epochs of fine-tuning (or distilling steps) of a large pre-trained model, making it more computationally challenging. In this work we present a novel perspective on the interplay between SSL and DC paradigms. In particular, we show that it is feasible to simultaneously learn a dense and gated sub-network from scratch in a SSL setting without any additional fine-tuning or pruning steps. The co-evolution during pre-training of both dense and gated encoder offers a good accuracy-efficiency trade-off and therefore yields a generic and multi-purpose architecture for application specific industrial settings. Extensive experiments on several image classification benchmarks including CIFAR-10/100, STL-10 and ImageNet-100, demonstrate that the proposed training strategy provides a dense and corresponding gated sub-network that achieves on-par performance compared with the vanilla self-supervised setting, but at a significant reduction in computation in terms of FLOPs, under a range of target budgets (td ).",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2066146609",
                        "name": "Tarun Krishna"
                    },
                    {
                        "authorId": "2053981547",
                        "name": "Ayush Rai"
                    },
                    {
                        "authorId": "2038393",
                        "name": "A. Drimbarean"
                    },
                    {
                        "authorId": "1680223",
                        "name": "A. Smeaton"
                    },
                    {
                        "authorId": "145470864",
                        "name": "Kevin McGuinness"
                    },
                    {
                        "authorId": "2137567915",
                        "name": "Noel E. O'Connor"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "There are some efforts to theoretically understand the SSL methods [30, 14], the role of data augmentation [28, 26], and some empirical analyses of the contrastive loss [4] and the predictor in the so-called BYOL framework [25].",
                "Furthermore, even with prior knowledge of downstream tasks designing data augmentations to cover the whole style space without affecting the content information is unrealistic [4]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4508280a2aaea2d27444fe15e57766ea09d75383",
                "externalIds": {
                    "ArXiv": "2212.11491",
                    "DBLP": "journals/corr/abs-2212-11491",
                    "DOI": "10.48550/arXiv.2212.11491",
                    "CorpusId": 254974097
                },
                "corpusId": 254974097,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4508280a2aaea2d27444fe15e57766ea09d75383",
                "title": "Understanding and Improving the Role of Projection Head in Self-Supervised Learning",
                "abstract": "Self-supervised learning (SSL) aims to produce useful feature representations without access to any human-labeled data annotations. Due to the success of recent SSL methods based on contrastive learning, such as SimCLR, this problem has gained popularity. Most current contrastive learning approaches append a parametrized projection head to the end of some backbone network to optimize the InfoNCE objective and then discard the learned projection head after training. This raises a fundamental question: Why is a learnable projection head required if we are to discard it after training? In this work, we first perform a systematic study on the behavior of SSL training focusing on the role of the projection head layers. By formulating the projection head as a parametric component for the InfoNCE objective rather than a part of the network, we present an alternative optimization scheme for training contrastive learning based SSL frameworks. Our experimental study on multiple image classification datasets demonstrates the effectiveness of the proposed approach over alternatives in the SSL literature.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1491124544",
                        "name": "Kartik Gupta"
                    },
                    {
                        "authorId": "144722114",
                        "name": "Thalaiyasingam Ajanthan"
                    },
                    {
                        "authorId": "5546141",
                        "name": "A. Hengel"
                    },
                    {
                        "authorId": "145273587",
                        "name": "Stephen Gould"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Contrastive learning implicitly learns relations among instances by optimizing alignment and matching a prior distribution (Wang and Isola, 2020; Chen and Li, 2020)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e191db42e846991f340acdea9d27e45d235055ad",
                "externalIds": {
                    "ArXiv": "2212.11187",
                    "DBLP": "journals/corr/abs-2212-11187",
                    "DOI": "10.1007/s00138-023-01444-9",
                    "CorpusId": 254926500
                },
                "corpusId": 254926500,
                "publicationVenue": {
                    "id": "400d5e36-be35-4097-898f-753f4493156e",
                    "name": "Machine Vision and Applications",
                    "type": "journal",
                    "alternate_names": [
                        "Journal of Machine Vision and Applications",
                        "Mach Vis Appl",
                        "J Mach Vis Appl"
                    ],
                    "issn": "0932-8092",
                    "url": "https://www.springer.com/computer/image+processing/journal/138",
                    "alternate_urls": [
                        "https://link.springer.com/journal/138",
                        "https://www.springer.com/computer/image+processing/journal/138?changeHeader",
                        "http://www.springer.com/computer/image+processing/journal/138"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e191db42e846991f340acdea9d27e45d235055ad",
                "title": "Similarity contrastive estimation for image and video soft contrastive self-supervised learning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51888097",
                        "name": "J. Denize"
                    },
                    {
                        "authorId": "2962220",
                        "name": "Jaonary Rabarisoa"
                    },
                    {
                        "authorId": "19258632",
                        "name": "Astrid Orcesi"
                    },
                    {
                        "authorId": "2197521725",
                        "name": "Romain H'erault"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Followup work (Chen et al., 2021) explores this phenomenon in more detail, characterizing how different hyperparameters and dataset features affect feature suppression."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "996e60ecedc45fc739efbbbfb21b19cb326b8374",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-08378",
                    "ArXiv": "2212.08378",
                    "DOI": "10.48550/arXiv.2212.08378",
                    "CorpusId": 254823430
                },
                "corpusId": 254823430,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/996e60ecedc45fc739efbbbfb21b19cb326b8374",
                "title": "Feature Dropout: Revisiting the Role of Augmentations in Contrastive Learning",
                "abstract": "What role do augmentations play in contrastive learning? Recent work suggests that good augmentations are label-preserving with respect to a specific downstream task. We complicate this picture by showing that label-destroying augmentations can be useful in the foundation model setting, where the goal is to learn diverse, general-purpose representations for multiple downstream tasks. We perform contrastive learning experiments on a range of image and audio datasets with multiple downstream tasks (e.g. for digits superimposed on photographs, predicting the class of one vs. the other). We find that Viewmaker Networks, a recently proposed model for learning augmentations for contrastive learning, produce label-destroying augmentations that stochastically destroy features needed for different downstream tasks. These augmentations are interpretable (e.g. altering shapes, digits, or letters added to images) and surprisingly often result in better performance compared to expert-designed augmentations, despite not preserving label information. To support our empirical results, we theoretically analyze a simple contrastive learning setting with a linear model. In this setting, label-destroying augmentations are crucial for preventing one set of features from suppressing the learning of features useful for another downstream task. Our results highlight the need for analyzing the interaction between multiple downstream tasks when trying to explain the success of foundation models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "88726969",
                        "name": "Alex Tamkin"
                    },
                    {
                        "authorId": "1678348367",
                        "name": "Margalit Glasgow"
                    },
                    {
                        "authorId": "2161615637",
                        "name": "Xiluo He"
                    },
                    {
                        "authorId": "144002017",
                        "name": "Noah D. Goodman"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b8c3f2317342f9426f88610b8b5d2755d2341bc3",
                "externalIds": {
                    "ArXiv": "2212.03273",
                    "DBLP": "journals/corr/abs-2212-03273",
                    "DOI": "10.1109/CVPRW59228.2023.00453",
                    "CorpusId": 254366794
                },
                "corpusId": 254366794,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b8c3f2317342f9426f88610b8b5d2755d2341bc3",
                "title": "Giga-SSL: Self-Supervised Learning for Gigapixel Images",
                "abstract": "Whole slide images (WSI) are microscopy images of stained tissue slides routinely prepared for diagnosis and treatment selection in medical practice. WSI are very large (gigapixel size) and complex (made of up to millions of cells). The current state-of-the-art (SoTA) approach to classify WSI subdivides them into tiles, encodes them by pre-trained networks and applies Multiple Instance Learning (MIL) to train for specific downstream tasks. However, annotated datasets are often small, typically a few hundred to a few thousand WSI, which may cause overfitting and underperforming models. Conversely, the number of unannotated WSI is ever increasing, with datasets of tens of thousands (soon to be millions) of images available. While it has been previously proposed to use these unannotated data to identify suitable tile representations by self-supervised learning (SSL), downstream classification tasks still require full supervision because parts of the MIL architecture is not trained during tile level SSL pre-training. Here, we propose a strategy of slide level SSL to leverage the large number of WSI without annotations to infer powerful slide representations. Applying our method to The Cancer-Genome Atlas, one of the most widely used data resources in cancer research (16 TB image data), we are able to downsize the dataset to 23 MB without any loss in predictive power: we show that a linear classifier trained on top of these embeddings maintains or improves previous SoTA performances on various benchmark WSI classification tasks. Finally, we observe that training a classifier on these representations with tiny datasets (e.g. 50 slides) improved performances over SoTA by an average of +6.3 AUC points over all downstream tasks. Altogether, our Giga-SSL representations of whole slide images are agnostic of downstream classification tasks and are well-suited for small datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2126530861",
                        "name": "Tristan Lazard"
                    },
                    {
                        "authorId": "115262706",
                        "name": "Marvin Lerousseau"
                    },
                    {
                        "authorId": "3204485",
                        "name": "Etienne Decenci\u00e8re"
                    },
                    {
                        "authorId": "2058106236",
                        "name": "Thomas Walter"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ed009b7423dcfec47708fb5817ec4955e4265757",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-10831",
                    "ArXiv": "2211.10831",
                    "DOI": "10.48550/arXiv.2211.10831",
                    "CorpusId": 253735342
                },
                "corpusId": 253735342,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ed009b7423dcfec47708fb5817ec4955e4265757",
                "title": "Joint Embedding Predictive Architectures Focus on Slow Features",
                "abstract": "Many common methods for learning a world model for pixel-based environments use generative architectures trained with pixel-level reconstruction objectives. Recently proposed Joint Embedding Predictive Architectures (JEPA) offer a reconstruction-free alternative. In this work, we analyze performance of JEPA trained with VICReg and SimCLR objectives in the fully offline setting without access to rewards, and compare the results to the performance of the generative architecture. We test the methods in a simple environment with a moving dot with various background distractors, and probe learned representations for the dot's location. We find that JEPA methods perform on par or better than reconstruction when distractor noise changes every time step, but fail when the noise is fixed. Furthermore, we provide a theoretical explanation for the poor performance of JEPA-based methods with fixed noise, highlighting an important limitation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2162736903",
                        "name": "Vlad Sobal"
                    },
                    {
                        "authorId": "2191646240",
                        "name": "V. JyothirS"
                    },
                    {
                        "authorId": "2191610598",
                        "name": "Siddhartha Jalagam"
                    },
                    {
                        "authorId": "3422899",
                        "name": "Nicolas Carion"
                    },
                    {
                        "authorId": "1979489",
                        "name": "Kyunghyun Cho"
                    },
                    {
                        "authorId": "1688882",
                        "name": "Yann LeCun"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "Another distinction is that autoencoders encourage information preservation in latent representations, whilst contrastive learning could suppress features (Chen et al., 2021a; Robinson et al., 2021b).",
                "Works such as DINO (Caron et al., 2021) and MoCo-v3 (Chen et al., 2021b) demonstrated that techniques developed with ConvNet backbones in mind could also perform competitively using ViTs after proper tuning to suit the new architecture.",
                "Finetuning CAN achieves 83.6% with ViT-B, outperforming other contrastive approaches such as MoCo-v3 (83.0%), and is competitive with other state-of-theart approaches such as CAE (83.9%).",
                "CAN is only outperformed by MoCo-v3 and DINO, both of which use momentum encoders and two full image views, and in the case of DINO a further 10 multi-crop views.",
                "CAN on ViT-L outperforms MoCLR with R200\u00d72 backbone (similar parameter counts), where we note that MoCLR performs as well or better than BYOL and MoCo-v3 on IN-1K (Tian et al., 2021)."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "4be4de9df5f036c0a785852634d79b0d52926a26",
                "externalIds": {
                    "ArXiv": "2210.16870",
                    "DBLP": "journals/corr/abs-2210-16870",
                    "DOI": "10.48550/arXiv.2210.16870",
                    "CorpusId": 253237813
                },
                "corpusId": 253237813,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4be4de9df5f036c0a785852634d79b0d52926a26",
                "title": "A simple, efficient and scalable contrastive masked autoencoder for learning visual representations",
                "abstract": "We introduce CAN, a simple, efficient and scalable method for self-supervised learning of visual representations. Our framework is a minimal and conceptually clean synthesis of (C) contrastive learning, (A) masked autoencoders, and (N) the noise prediction approach used in diffusion models. The learning mechanisms are complementary to one another: contrastive learning shapes the embedding space across a batch of image samples; masked autoencoders focus on reconstruction of the low-frequency spatial correlations in a single image sample; and noise prediction encourages the reconstruction of the high-frequency components of an image. The combined approach results in a robust, scalable and simple-to-implement algorithm. The training process is symmetric, with 50% of patches in both views being masked at random, yielding a considerable efficiency improvement over prior contrastive learning methods. Extensive empirical studies demonstrate that CAN achieves strong downstream performance under both linear and finetuning evaluations on transfer learning and robustness tasks. CAN outperforms MAE and SimCLR when pre-training on ImageNet, but is especially useful for pre-training on larger uncurated datasets such as JFT-300M: for linear probe on ImageNet, CAN achieves 75.4% compared to 73.4% for SimCLR and 64.1% for MAE. The finetuned performance on ImageNet of our ViT-L model is 86.1%, compared to 85.5% for SimCLR, and 85.4% for MAE. The overall FLOPs load of SimCLR is 70% higher than CAN for ViT-L models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2850880",
                        "name": "Shlok Kumar Mishra"
                    },
                    {
                        "authorId": "4594310",
                        "name": "Joshua Robinson"
                    },
                    {
                        "authorId": "2914394",
                        "name": "Huiwen Chang"
                    },
                    {
                        "authorId": "2059096514",
                        "name": "David Jacobs"
                    },
                    {
                        "authorId": "8707513",
                        "name": "Aaron Sarna"
                    },
                    {
                        "authorId": "2064102741",
                        "name": "Aaron Maschinot"
                    },
                    {
                        "authorId": "1707347",
                        "name": "Dilip Krishnan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Furthermore, several works [11, 7] indicate that contrastive loss tends to learn more global and coarse-grained features.",
                "Blocked masking also better synergizes with the contrastive objective, which is biased to learn global features [7, 11].",
                "Therefore, we identify two main drawbacks of Masked Siamese ConvNets: (I) Regular erase-based masking operation disrupts the global features that are important for the contrastive objective [7, 11]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "12443fd4adfc3674eb263d5cfdb179a9820fd50d",
                "externalIds": {
                    "ArXiv": "2210.11456",
                    "CorpusId": 257637234
                },
                "corpusId": 257637234,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/12443fd4adfc3674eb263d5cfdb179a9820fd50d",
                "title": "MixMask: Revisiting Masking Strategy for Siamese ConvNets",
                "abstract": "Recent advances in self-supervised learning have integrated Masked Image Modeling (MIM) and Siamese Networks into a unified framework that leverages the benefits of both techniques. However, several issues remain unaddressed when applying conventional erase-based masking with Siamese ConvNets. These include (I) the inability to drop uninformative masked regions in ConvNets as they process data continuously, resulting in low training efficiency compared to ViT models; and (II) the mismatch between erase-based masking and the contrastive-based objective in Siamese ConvNets, which differs from the MIM approach. In this paper, we propose a filling-based masking strategy called MixMask to prevent information incompleteness caused by the randomly erased regions in an image in the vanilla masking method. Furthermore, we introduce a flexible loss function design that considers the semantic distance change between two different mixed views to adapt the integrated architecture and prevent mismatches between the transformed input and objective in Masked Siamese ConvNets (MSCN). We conducted extensive experiments on various datasets, including CIFAR-100, Tiny-ImageNet, and ImageNet-1K. The results demonstrate that our proposed framework achieves superior accuracy on linear probing, semi-supervised, and supervised finetuning, outperforming the state-of-the-art MSCN by a significant margin. Additionally, we demonstrate the superiority of our approach in object detection and segmentation tasks. Our source code is available at https://github.com/LightnessOfBeing/MixMask.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2113246513",
                        "name": "Kirill Vishniakov"
                    },
                    {
                        "authorId": "2064963077",
                        "name": "Eric P. Xing"
                    },
                    {
                        "authorId": "145314568",
                        "name": "Zhiqiang Shen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Other related works [6, 33] generalize the instancewise contrastive loss to the alignment of representations from positive pairs and uniformity of the induced distribution of the normalized embeddings on the hyper-sphere.",
                "80 SWD [6] ResNet-50+MLP 28 2048 800 70."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "18312bd0309c836e0b69fe80c34789e8a9a2ae8d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-10194",
                    "ArXiv": "2210.10194",
                    "DOI": "10.48550/arXiv.2210.10194",
                    "CorpusId": 252992486
                },
                "corpusId": 252992486,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/18312bd0309c836e0b69fe80c34789e8a9a2ae8d",
                "title": "Rethinking Prototypical Contrastive Learning through Alignment, Uniformity and Correlation",
                "abstract": "Contrastive self-supervised learning (CSL) with a prototypical regularization has been introduced in learning meaningful representations for downstream tasks that require strong semantic information. However, to optimize CSL with a loss that performs the prototypical regularization aggressively, e.g., the ProtoNCE loss, might cause the\"coagulation\"of examples in the embedding space. That is, the intra-prototype diversity of samples collapses to trivial solutions for their prototype being well-separated from others. Motivated by previous works, we propose to mitigate this phenomenon by learning Prototypical representation through Alignment, Uniformity and Correlation (PAUC). Specifically, the ordinary ProtoNCE loss is revised with: (1) an alignment loss that pulls embeddings from positive prototypes together; (2) a uniformity loss that distributes the prototypical level features uniformly; (3) a correlation loss that increases the diversity and discriminability between prototypical level features. We conduct extensive experiments on various benchmarks where the results demonstrate the effectiveness of our method in improving the quality of prototypical contrastive representations. Particularly, in the classification down-stream tasks with linear probes, our proposed method outperforms the state-of-the-art instance-wise and prototypical contrastive learning methods on the ImageNet-100 dataset by 2.96% and the ImageNet-1K dataset by 2.46% under the same settings of batch size and epochs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2066123456",
                        "name": "Shentong Mo"
                    },
                    {
                        "authorId": "3458345",
                        "name": "Zhun Sun"
                    },
                    {
                        "authorId": "2150356887",
                        "name": "Chao Li"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "38e16cc5c5af9b625c128d283d59a672eae66ce7",
                "externalIds": {
                    "ArXiv": "2210.09224",
                    "DBLP": "journals/corr/abs-2210-09224",
                    "DOI": "10.48550/arXiv.2210.09224",
                    "CorpusId": 252918085
                },
                "corpusId": 252918085,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/38e16cc5c5af9b625c128d283d59a672eae66ce7",
                "title": "Self-Supervised Learning Through Efference Copies",
                "abstract": "Self-supervised learning (SSL) methods aim to exploit the abundance of unlabelled data for machine learning (ML), however the underlying principles are often method-specific. An SSL framework derived from biological first principles of embodied learning could unify the various SSL methods, help elucidate learning in the brain, and possibly improve ML. SSL commonly transforms each training datapoint into a pair of views, uses the knowledge of this pairing as a positive (i.e. non-contrastive) self-supervisory sign, and potentially opposes it to unrelated, (i.e. contrastive) negative examples. Here, we show that this type of self-supervision is an incomplete implementation of a concept from neuroscience, the Efference Copy (EC). Specifically, the brain also transforms the environment through efference, i.e. motor commands, however it sends to itself an EC of the full commands, i.e. more than a mere SSL sign. In addition, its action representations are likely egocentric. From such a principled foundation we formally recover and extend SSL methods such as SimCLR, BYOL, and ReLIC under a common theoretical framework, i.e. Self-supervision Through Efference Copies (S-TEC). Empirically, S-TEC restructures meaningfully the within- and between-class representations. This manifests as improvement in recent strong SSL baselines in image classification, segmentation, object detection, and in audio. These results hypothesize a testable positive influence from the brain's motor outputs onto its sensory representations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "65825675",
                        "name": "Franz Scherr"
                    },
                    {
                        "authorId": "47747957",
                        "name": "Qinghai Guo"
                    },
                    {
                        "authorId": "4584346",
                        "name": "Timoleon Moraitis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Currently, a large body of works on self-supervised learning focus on discriminative approaches [1], [2], [3], [4], [5], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], which regard each image as a different class and trains the model by discriminating them up to data augmentations."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8a3d81f1d4ee013c12801b59f5537fbc7af30511",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-08458",
                    "ArXiv": "2210.08458",
                    "DOI": "10.48550/arXiv.2210.08458",
                    "CorpusId": 252918112
                },
                "corpusId": 252918112,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8a3d81f1d4ee013c12801b59f5537fbc7af30511",
                "title": "Learning Self-Regularized Adversarial Views for Self-Supervised Vision Transformers",
                "abstract": "Automatic data augmentation (AutoAugment) strategies are indispensable in supervised data-efficient training protocols of vision transformers, and have led to state-of-the-art results in supervised learning. Despite the success, its development and application on self-supervised vision transformers have been hindered by several barriers, including the high search cost, the lack of supervision, and the unsuitable search space. In this work, we propose AutoView, a self-regularized adversarial AutoAugment method, to learn views for self-supervised vision transformers, by addressing the above barriers. First, we reduce the search cost of AutoView to nearly zero by learning views and network parameters simultaneously in a single forward-backward step, minimizing and maximizing the mutual information among different augmented views, respectively. Then, to avoid information collapse caused by the lack of label supervision, we propose a self-regularized loss term to guarantee the information propagation. Additionally, we present a curated augmentation policy search space for self-supervised learning, by modifying the generally used search space designed for supervised learning. On ImageNet, our AutoView achieves remarkable improvement over RandAug baseline (+10.2% k-NN accuracy), and consistently outperforms sota manually tuned view policy by a clear margin (up to +1.3% k-NN accuracy). Extensive experiments show that AutoView pretraining also benefits downstream tasks (+1.2% mAcc on ADE20K Semantic Segmentation and +2.8% mAP on revisited Oxford Image Retrieval benchmark) and improves model robustness (+2.3% Top-1 Acc on ImageNet-A and +1.0% AUPR on ImageNet-O). Code and models will be available at https://github.com/Trent-tangtao/AutoView.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2069746552",
                        "name": "Tao Tang"
                    },
                    {
                        "authorId": "46651877",
                        "name": "Changlin Li"
                    },
                    {
                        "authorId": "2749191",
                        "name": "Guangrun Wang"
                    },
                    {
                        "authorId": "9945144",
                        "name": "Kaicheng Yu"
                    },
                    {
                        "authorId": "144950946",
                        "name": "Xiaojun Chang"
                    },
                    {
                        "authorId": "13246332",
                        "name": "Xiaodan Liang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Following this observation, other works (Chen et al., 2021) have sought to reformulate contrastive losses to scatter representations either (a) uniformly on the unit hypercube, or (b) onto Gaussian distributions (which have the highest entropy amongst all distributions with a given variance)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8be831a07abe8c5475c2bd91cc41bf4c1c2be771",
                "externalIds": {
                    "DBLP": "conf/iclr/AssranBDBMBVRB23",
                    "ArXiv": "2210.07277",
                    "DOI": "10.48550/arXiv.2210.07277",
                    "CorpusId": 252907682
                },
                "corpusId": 252907682,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8be831a07abe8c5475c2bd91cc41bf4c1c2be771",
                "title": "The Hidden Uniform Cluster Prior in Self-Supervised Learning",
                "abstract": "A successful paradigm in representation learning is to perform self-supervised pretraining using tasks based on mini-batch statistics (e.g., SimCLR, VICReg, SwAV, MSN). We show that in the formulation of all these methods is an overlooked prior to learn features that enable uniform clustering of the data. While this prior has led to remarkably semantic representations when pretraining on class-balanced data, such as ImageNet, we demonstrate that it can hamper performance when pretraining on class-imbalanced data. By moving away from conventional uniformity priors and instead preferring power-law distributed feature clusters, we show that one can improve the quality of the learned representations on real-world class-imbalanced datasets. To demonstrate this, we develop an extension of the Masked Siamese Networks (MSN) method to support the use of arbitrary features priors.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "38698856",
                        "name": "Mahmoud Assran"
                    },
                    {
                        "authorId": "3201463",
                        "name": "Randall Balestriero"
                    },
                    {
                        "authorId": "2101830371",
                        "name": "Quentin Duval"
                    },
                    {
                        "authorId": "34651419",
                        "name": "Florian Bordes"
                    },
                    {
                        "authorId": "1806773",
                        "name": "Ishan Misra"
                    },
                    {
                        "authorId": "2329288",
                        "name": "Piotr Bojanowski"
                    },
                    {
                        "authorId": "145467703",
                        "name": "Pascal Vincent"
                    },
                    {
                        "authorId": "2066127975",
                        "name": "Michael G. Rabbat"
                    },
                    {
                        "authorId": "2482072",
                        "name": "Nicolas Ballas"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Moreover, recent works on self-supervised learning have reported that self-supervised learning may still suffer from poor OOD generalization (Geirhos et al., 2020; Chen et al., 2021; Robinson et al., 2021; Tsai et al., 2021) when such dataset bias still remains after applying data augmentations.",
                "Figure 4b shows that the rank regularization exacerbates the \u201cfeature suppression\u201d phenomenon revealed by Chen et al. (2021)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3e1f0f476315ac2ff9a853054adbb1cd8287e1fe",
                "externalIds": {
                    "ArXiv": "2210.05248",
                    "DBLP": "journals/corr/abs-2210-05248",
                    "DOI": "10.48550/arXiv.2210.05248",
                    "CorpusId": 252816018
                },
                "corpusId": 252816018,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3e1f0f476315ac2ff9a853054adbb1cd8287e1fe",
                "title": "Self-supervised debiasing using low rank regularization",
                "abstract": "Spurious correlations can cause strong biases in deep neural networks, impairing generalization ability. While most of existing debiasing methods require full supervisions on either spurious attributes or target labels, training a debiased model from a limited amount of both annotations is still an open issue. To overcome such limitations, we first examined an interesting phenomenon by the spectral analysis of latent representations: spuriously correlated, easy-to-learn attributes make neural networks inductively biased towards encoding lower effective rank representations. We also show that a rank regularization can amplify this bias in a way that encourages highly correlated features. Motivated by these observations, we propose a self-supervised debiasing framework that is potentially compatible with unlabeled samples. We first pretrain a biased encoder in a self-supervised manner with the rank regularization, serving as a semantic bottleneck to enforce the encoder to learn the spuriously correlated attributes. This biased encoder is then used to discover and upweight bias-conflicting samples in a downstream task, serving as a boosting to effectively debias the main model. Remarkably, the proposed debiasing framework significantly improves the generalization performance of self-supervised learning baselines and, in some cases, even outperforms state-of-the-art supervised debiasing approaches.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "153118937",
                        "name": "Geon Yeong Park"
                    },
                    {
                        "authorId": "2069020074",
                        "name": "Chanyong Jung"
                    },
                    {
                        "authorId": "30547794",
                        "name": "Jong-Chul Ye"
                    },
                    {
                        "authorId": "2152577026",
                        "name": "Sang Wan Lee"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The standard choice of optimization function for contrastive ICR methods is the triplet loss with in-batch hard negative mining [9, 13, 16, 18, 20, 21, 28] or the cross-entropy based NT-Xent loss [10, 19]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "30e35301e6ff50f414a58fda2be82574aee98f79",
                "externalIds": {
                    "DBLP": "conf/mm/Bleeker22",
                    "DOI": "10.1145/3503161.3548757",
                    "CorpusId": 252782099
                },
                "corpusId": 252782099,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/30e35301e6ff50f414a58fda2be82574aee98f79",
                "title": "Multi-modal Learning Algorithms and Network Architectures for Information Extraction and Retrieval",
                "abstract": "Large-scale (pre-)training has recently achieved great success on both uni- and multi-modal downstream evaluation tasks. However, this training paradigm generally comes with a high cost, both in the amount of compute and data needed for training. In my Ph.D. thesis, I study the problem of multi-modal learning for information extraction and retrieval, with the main focus on new learning algorithms and network architectures to make the learning process more efficient. First, I introduce a novel network architecture for bidirectional decoding for the scene text recognition (STR) task. Next, I focus on the image-caption retrieval (ICR) task. I question if the results obtained in the metric learning field generalize to the ICR task. Finally, I focus on the reduction of shortcut learning for the ICR task. I introduce latent target decoding (LTD), a novel constraint-based learning algorithm which reduces shortcut feature learning by decoding the input caption in a semantic latent space.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1452678770",
                        "name": "Maurits J. R. Bleeker"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "From the results, we can reach to three observations: (1) The GCL-based methods generally yield higher performances than classical unsupervised learning methods, indicating the effectiveness of utilizing instance-level supervision; (2) RGCL, AD-GCL, and GASSL achieve better performances than GraphCL, which empirically proves the conclusion that InfoMax object could bring overwhelmed redundant information and thus suffer from feature suppression issue; (3) Our proposed GraphCV and DGCL consistently outperform other baselines, proving the advantage of disentangled representation.",
                "There exist the situation (e.g., OOD setting) that the latent space of learned representation is dominated by non-predictive features in SSL [7] and it is no more informative enough to make correct prediction.",
                "The results are shown in left two subplots of Figure 4, we compare our method with GASSL under different perturbation bounds and attack steps to demonstrate its robustness against adversarial attacks.",
                "Therefore, many recent works [29, 14, 33] study how to fully utilize the unlabeled information on graph and further stimulate the application of self-supervised learning (SSL) for GRL where only limited or even no label is needed.",
                "As a prevalent and effective strategy of SSL, contrastive learning follows the mutual information maximization principle (InfoMax) [39] to maximize the agreements of the positive pairs while minimizing that of the negative pairs in the embedding space.",
                "(2)\nAlthough GCL-based methods are usaully capable to extract useful information for label identification, it is unavoidable to include non-predictive features under the SSL setting owing lack of explicit domain knowledge.",
                "Under the unsupervised representation learning setting, we compare GraphCV with the eight SOTA self-supervised learning methods GraphCL [51], InfoGraph[33], MVGRL [14], AD-GCL[34], GASSL[48], InfoGCL[45], RGCL [22] and DGCL[21], as well as three classical unsupervised representation learning methods, including node2vec [11], graph2vec [28], and GVAE[19].",
                ", OOD setting) that the latent space of learned representation is dominated by non-predictive features in SSL [7] and it is no more informative enough to make correct prediction.",
                "Although aggressive adversarial attacks can largely deteriorate the performance, our proposed GraphCV still achieves more robust performance than GASSL.",
                "Since both our model and GASSL use GIN as the backbone network, we hereby add the performance of GIN as the compared baseline.",
                "Therefore, feature suppression is not just a prevalent issue in\nsupervised learning, but also in SSL. Due to the page limitation, we provide more discussion about the relation between feature suppression and GCL in Appendix B"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "12c5b11208fa0785f14cc4f95f2bbdeaa537fce0",
                "externalIds": {
                    "ArXiv": "2209.07699",
                    "CorpusId": 253018960
                },
                "corpusId": 253018960,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/12c5b11208fa0785f14cc4f95f2bbdeaa537fce0",
                "title": "Graph Contrastive Learning with Cross-view Reconstruction",
                "abstract": "Graph self-supervised learning is commonly taken as an effective framework to tackle the supervision shortage issue in the graph learning task. Among different existing graph self-supervised learning strategies, graph contrastive learning (GCL) has been one of the most prevalent approaches to this problem. Despite the remarkable performance those GCL methods have achieved, existing GCL methods that heavily depend on various manually designed augmentation techniques still struggle to alleviate the feature suppression issue without risking losing task-relevant information. Consequently, the learned representation is either brittle or unilluminating. In light of this, we introduce the Graph Contrastive Learning with C ross- V iew Reconstruction (GraphCV), which follows the information bottleneck principle to learn minimal yet sufficient representation from graph data. Specifically, GraphCV aims to elicit the predictive (useful for downstream instance discrimination) and other non-predictive features separately. Except for the con-ventional contrastive loss which guarantees the consistency and sufficiency of the representation across different augmentation views, we introduce a cross-view reconstruction mechanism to pursue the disentanglement of the two learned representations. Besides, an adversarial view perturbed from the original view is added as the third view for the contrastive loss to guarantee the intactness of the global semantics and improve the representation robustness. We empirically demonstrate that our proposed model outperforms the state-of-the-art on graph classification task over multiple benchmark datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1818661152",
                        "name": "Qianlong Wen"
                    },
                    {
                        "authorId": "51253105",
                        "name": "Z. Ouyang"
                    },
                    {
                        "authorId": "2155135917",
                        "name": "Chunhui Zhang"
                    },
                    {
                        "authorId": "1596820688",
                        "name": "Y. Qian"
                    },
                    {
                        "authorId": "2093920413",
                        "name": "Yanfang Ye"
                    },
                    {
                        "authorId": "2117879943",
                        "name": "Chuxu Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Furthermore, our theory suggests why projection heads empirically improve performances [1, 25, 43] in general SSL, i."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bd1eb081b9e88857bf0f62ac5b51969004e5593f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-06235",
                    "ArXiv": "2209.06235",
                    "DOI": "10.48550/arXiv.2209.06235",
                    "CorpusId": 252222375
                },
                "corpusId": 252222375,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/bd1eb081b9e88857bf0f62ac5b51969004e5593f",
                "title": "Improving Self-Supervised Learning by Characterizing Idealized Representations",
                "abstract": "Despite the empirical successes of self-supervised learning (SSL) methods, it is unclear what characteristics of their representations lead to high downstream accuracies. In this work, we characterize properties that SSL representations should ideally satisfy. Specifically, we prove necessary and sufficient conditions such that for any task invariant to given data augmentations, desired probes (e.g., linear or MLP) trained on that representation attain perfect accuracy. These requirements lead to a unifying conceptual framework for improving existing SSL methods and deriving new ones. For contrastive learning, our framework prescribes simple but significant improvements to previous methods such as using asymmetric projection heads. For non-contrastive learning, we use our framework to derive a simple and novel objective. Our resulting SSL algorithms outperform baselines on standard benchmarks, including SwAV+multicrops on linear probing of ImageNet.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47367640",
                        "name": "Yann Dubois"
                    },
                    {
                        "authorId": "2117567142",
                        "name": "Tatsunori Hashimoto"
                    },
                    {
                        "authorId": "2490652",
                        "name": "Stefano Ermon"
                    },
                    {
                        "authorId": "145419642",
                        "name": "Percy Liang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8726629cbe5500ea1745a2516287a2195d9449e4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-02459",
                    "ArXiv": "2209.02459",
                    "DOI": "10.48550/arXiv.2209.02459",
                    "CorpusId": 252090165
                },
                "corpusId": 252090165,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8726629cbe5500ea1745a2516287a2195d9449e4",
                "title": "Robust and Efficient Imbalanced Positive-Unlabeled Learning with Self-supervision",
                "abstract": "Learning from positive and unlabeled (PU) data is a setting where the learner only has access to positive and unlabeled samples while having no information on negative examples. Such PU setting is of great importance in various tasks such as medical diagnosis, social network analysis, financial markets analysis, and knowledge base completion, which also tend to be intrinsically imbalanced, i.e., where most examples are actually negatives. Most existing approaches for PU learning, however, only consider artificially balanced datasets and it is unclear how well they perform in the realistic scenario of imbalanced and long-tail data distribution. This paper proposes to tackle this challenge via robust and efficient self-supervised pretraining. However, training conventional self-supervised learning methods when applied with highly imbalanced PU distribution needs better reformulation. In this paper, we present \\textit{ImPULSeS}, a unified representation learning framework for \\underline{Im}balanced \\underline{P}ositive \\underline{U}nlabeled \\underline{L}earning leveraging \\underline{Se}lf-\\underline{S}upervised debiase pre-training. ImPULSeS uses a generic combination of large-scale unsupervised learning with debiased contrastive loss and additional reweighted PU loss. We performed different experiments across multiple datasets to show that ImPULSeS is able to halve the error rate of the previous state-of-the-art, even compared with previous methods that are given the true prior. Moreover, our method showed increased robustness to prior misspecification and superior performance even when pretraining was performed on an unrelated dataset. We anticipate such robustness and efficiency will make it much easier for practitioners to obtain excellent results on other PU datasets of interest. The source code is available at \\url{https://github.com/JSchweisthal/ImPULSeS}",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51020045",
                        "name": "Emilio Dorigatti"
                    },
                    {
                        "authorId": "14001634",
                        "name": "J. Schweisthal"
                    },
                    {
                        "authorId": "2133449619",
                        "name": "Bernd Bischl"
                    },
                    {
                        "authorId": "35593430",
                        "name": "Mina Rezaei"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "On the contrary, appropriate optimization strategies are often shown to be useful in learning complicated systems Chen et al. (2021b); Steiner et al. (2021).",
                "It is worth mentioning that, in recent studies Radford et al. (2021); Chen et al. (2021c), the contrastive loss is usually implemented as the cross-entropy between one-hot labels and the class probability obtained by softmax within a mini-batch SM .",
                "Contrastive Learning: Following the definition in Oord et al. (2018); Wang & Isola (2020); Chen et al. (2021a); Radford et al. (2021), we formulate the contrastive loss as\nLc(f\u03b8, g\u03c6; \u03c4,S) := E U ,V \u223cS U\u2212i 6=U V \u2212j 6=V\n[ \u2212 log e\n\u2212\u03c4d(f\u03b8(U),g\u03c6(V ))\u2211 j\u2208[M ] e \u2212\u03c4d(f\u03b8(U),g\u03c6(V \u2212j )) + \u2211 i\u2208[M ] e\u2026",
                "\u2026feature representations Bojanowski & Joulin (2017); Mettes et al. (2019), researchers also demonstrate that given a properly defined prior distribution of samples, the performance of the contrastive loss will not degrade regardless of the topology where samples are embedded on Chen et al. (2021a).",
                "Other hyperparameters follow the setup of MoCo v3 (Chen et al., 2021c)."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "29ac542838974c75a3ac40e5855ec7d346ea87ee",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-02127",
                    "ArXiv": "2209.02127",
                    "DOI": "10.48550/arXiv.2209.02127",
                    "CorpusId": 252089918
                },
                "corpusId": 252089918,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/29ac542838974c75a3ac40e5855ec7d346ea87ee",
                "title": "Design of the topology for contrastive visual-textual alignment",
                "abstract": "Pre-training weakly related image-text pairs in the contrastive style shows great power in learning semantic aligning cross-modal models. The common choice to measure the distance between the feature representations of the image-text pairs is the cosine similarity, which can be considered as the negative inner product of features embedded on a sphere mathematically. While such topology bene\ufb01ts from the low computational resources consumption and a properly de\ufb01ned uniformity, typically, there are two major drawbacks when applied. First, it is vulnerable to the semantic ambiguity phenomenon resulting from the noise in the weakly-related image-text pairs. Second, the learning progress is unstable and fragile at the beginning. Although, in the practice of former studies, a learnable softmax temperature parameter and a long warmup scheme are employed to meliorate the training progress, still there lacks an in-depth analysis of these problems. In this work, we discuss the desired properties of the topology and its endowed distance function for the embedding vectors of feature representations from the view of optimization. We then propose a rather simple solution to improve the aforementioned problem. That is, we map the feature representations onto the oblique manifold endowed with the negative inner product as the distance function. In the experimental analysis, we show that we can improve the baseline performance by a large margin ( e.g. 4% in the zero-shot image to text retrieval task) by changing only two lines of the training codes.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3458345",
                        "name": "Zhun Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "A generalized version of contrastive learning with data augmentation in [28] performs representation learning by computing the cosine similarity in the feature space to group same-class samples and repel different classes."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b900a392ce0abb51f66aee880bb284772a24baec",
                "externalIds": {
                    "DBLP": "conf/icpr/DionelisTY22",
                    "DOI": "10.1109/ICPR56361.2022.9956206",
                    "CorpusId": 254102138
                },
                "corpusId": 254102138,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b900a392ce0abb51f66aee880bb284772a24baec",
                "title": "CTR: Contrastive Training Recognition Classifier for Few-Shot Open-World Recognition",
                "abstract": "AI-enabled systems in security, autonomous systems, safety, and healthcare do not only need to effectively detect Out-of-Distribution (OoD) samples, but also to recognize Objects of Concern (OoC), e.g. multiple thorax diseases, efficiently with few-shots. Detecting OoD samples is crucial, because reporting an out-of-domain input as abnormal is better than falsely classifying it. Data samples, during inference, are not confined to a finite labelled set, and thus closed-set approaches are limiting, as they misclassify OoD inputs, and they may assign them high prediction confidence. Furthermore, although anomaly detection is possible, recognizing new OoC fast using only few-shot samples remains challenging. There is a lack of methodologies for joint anomaly detection and few-shot OoC classification. Our contribution is the development of a framework for joint few-shot OoC detection and classification and anomaly detection in the unknown previously-unseen, in the wild, environment, which is known as Open-World Recognition (OWR). We propose a novel methodology, the data distribution boundary Contrastive Training Recognition (CTR) classifier for few-shot OWR. CTR takes advantage of labels and classes to learn the normal (and few-shot abnormal) data better, to more accurately detect OoD. The proposed model: (i) reduces failures to detect anomalies in health- and safety-critical applications for avoiding unfavourable consequences, (ii) decreases false alarms, and (iii) improves performance overall. Our framework differs from existing approaches because: (a) anomaly and OoC detection are combined, which has several benefits, including improved OoD performance, (b) the performance, accuracy, and robustness of OoD and few-shot OoC detection are improved by strengthening the estimation of the normal class distribution at the boundary of its support, self-generating samples and setting them as abnormal, and (c) the knowledge base of models is also augmented by learning class-incrementally, alleviating forgetting. CTR outperforms baselines in several settings, including on the SVHN, CIFAR-FS, and BSCD-FS ChestX and ISIC datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8471111",
                        "name": "Nikolaos Dionelis"
                    },
                    {
                        "authorId": "1919157",
                        "name": "S. Tsaftaris"
                    },
                    {
                        "authorId": "2390699",
                        "name": "Mehrdad Yaghoobi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Because the learned feature spaces closely match the distribution of ImageNet, which easily overfit some similar downstream tasks but hamper others [27, 28, 29]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bb93c408d7b3249a92189dbe0bff159b193caef0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-00238",
                    "ArXiv": "2208.00238",
                    "DOI": "10.48550/arXiv.2208.00238",
                    "CorpusId": 251224061,
                    "PubMed": "36584625"
                },
                "corpusId": 251224061,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bb93c408d7b3249a92189dbe0bff159b193caef0",
                "title": "Improving Fine-tuning of Self-supervised Models with Contrastive Initialization",
                "abstract": "Self-supervised learning (SSL) has achieved remarkable performance in pre-training the models that can be further used in downstream tasks via fine-tuning. However, these self-supervised models may not capture meaningful semantic information since the images belonging to the same class are often regarded as negative pairs in the contrastive loss. Consequently, the images of the same class are often located far away from each other in the learned feature space, which would inevitably hamper the fine-tuning process. To address this issue, we seek to explicitly enhance the semantic relation among instances on the targeted downstream task and provide a better initialization for the subsequent fine-tuning. To this end, we propose a Contrastive Initialization (COIN) method that breaks the standard fine-tuning pipeline by introducing an extra class-aware initialization stage before fine-tuning. Specifically, we exploit a supervised contrastive loss to increase inter-class discrepancy and intra-class compactness of features on the target dataset. In this way, self-supervised models can be easily trained to discriminate instances of different classes during the final fine-tuning stage. Extensive experiments show that, with the enriched semantics, our COIN significantly outperforms existing methods without introducing extra training cost and sets new state-of-the-arts on multiple downstream tasks. For example, compared with the baseline method, our COIN improves the accuracy by 5% on ImageNet-20 and 2.57% on CIFAR100, respectively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2179889695",
                        "name": "Haolin Pan"
                    },
                    {
                        "authorId": "143670622",
                        "name": "Yong Guo"
                    },
                    {
                        "authorId": "2179884288",
                        "name": "Qinyi Deng"
                    },
                    {
                        "authorId": "25055486",
                        "name": "Hao-Fan Yang"
                    },
                    {
                        "authorId": "2109361111",
                        "name": "Yiqun Chen"
                    },
                    {
                        "authorId": "2118446738",
                        "name": "Jian Chen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cde535c74312f61d71a7649afb474259c600433d",
                "externalIds": {
                    "DBLP": "conf/iclr/ZhengPRPG23",
                    "ArXiv": "2208.00789",
                    "DOI": "10.48550/arXiv.2208.00789",
                    "CorpusId": 251223862
                },
                "corpusId": 251223862,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/cde535c74312f61d71a7649afb474259c600433d",
                "title": "Self-supervised learning with rotation-invariant kernels",
                "abstract": "We introduce a regularization loss based on kernel mean embeddings with rotation-invariant kernels on the hypersphere (also known as dot-product kernels) for self-supervised learning of image representations. Besides being fully competitive with the state of the art, our method significantly reduces time and memory complexity for self-supervised training, making it implementable for very large embedding dimensions on existing devices and more easily adjustable than previous methods to settings with limited resources. Our work follows the major paradigm where the model learns to be invariant to some predefined image transformations (cropping, blurring, color jittering, etc.), while avoiding a degenerate solution by regularizing the embedding distribution. Our particular contribution is to propose a loss family promoting the embedding distribution to be close to the uniform distribution on the hypersphere, with respect to the maximum mean discrepancy pseudometric. We demonstrate that this family encompasses several regularizers of former methods, including uniformity-based and information-maximization methods, which are variants of our flexible regularization loss with different kernels. Beyond its practical consequences for state-of-the-art self-supervised learning with limited resources, the proposed generic regularization approach opens perspectives to leverage more widely the literature on kernel methods in order to improve self-supervised learning methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2143390934",
                        "name": "L\u00e9on Zheng"
                    },
                    {
                        "authorId": "3242930",
                        "name": "Gilles Puy"
                    },
                    {
                        "authorId": "3395846",
                        "name": "E. Riccietti"
                    },
                    {
                        "authorId": "2173636176",
                        "name": "Patrick P'erez"
                    },
                    {
                        "authorId": "1731535",
                        "name": "R. Gribonval"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "91fa90034eb88bc31c46d4e4fa242c3b3c16b2f1",
                "externalIds": {
                    "DBLP": "conf/aaai/0012AMS23",
                    "ArXiv": "2207.10862",
                    "DOI": "10.1609/aaai.v37i12.26733",
                    "CorpusId": 253735235
                },
                "corpusId": 253735235,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/91fa90034eb88bc31c46d4e4fa242c3b3c16b2f1",
                "title": "Contrastive Self-Supervised Learning Leads to Higher Adversarial Susceptibility",
                "abstract": "Contrastive self-supervised learning (CSL) has managed to match or surpass the performance of supervised learning in image and video classification. However, it is still largely unknown if the nature of the representations induced by the two learning paradigms is similar. We investigate this under the lens of adversarial robustness. Our analysis of the problem reveals that CSL has intrinsically higher sensitivity to perturbations over supervised learning. We identify the uniform distribution of data representation over a unit hypersphere in the CSL representation space as the key contributor to this phenomenon. We establish that this is a result of the presence of false negative pairs in the training process, which increases model sensitivity to input perturbations. Our finding is supported by extensive experiments for image and video classification using adversarial perturbations and other input corruptions. We devise a strategy to detect and remove false negative pairs that is simple, yet effective in improving model robustness with CSL training. We close up to 68% of the robustness gap between CSL and its supervised counterpart. Finally, we contribute to adversarial learning by incorporating our method in CSL. We demonstrate an average gain of about 5% over two different state-of-the-art methods in this domain.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110003398",
                        "name": "Rohit Gupta"
                    },
                    {
                        "authorId": "47398812",
                        "name": "Naveed Akhtar"
                    },
                    {
                        "authorId": "1747500",
                        "name": "A. Mian"
                    },
                    {
                        "authorId": "2111863589",
                        "name": "M. Shah"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "As a result, different images with similar visual concepts are grouped together, inducing a latent space with rich semantic information [62,10,63]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "70f8d4029c3af108037897012acaab249b854359",
                "externalIds": {
                    "DBLP": "conf/eccv/HuWZG22",
                    "ArXiv": "2207.10456",
                    "DOI": "10.48550/arXiv.2207.10456",
                    "CorpusId": 250917885
                },
                "corpusId": 250917885,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/70f8d4029c3af108037897012acaab249b854359",
                "title": "Semantic-Aware Fine-Grained Correspondence",
                "abstract": ". Establishing visual correspondence across images is a chal-lenging and essential task. Recently, an influx of self-supervised methods have been proposed to better learn representations for visual correspondence. However, we find that these methods often fail to leverage semantic information and over-rely on the matching of low-level features. In contrast, human vision is capable of distinguishing between distinct objects as a pretext to tracking. Inspired by this paradigm, we propose to learn semantic-aware fine-grained correspondence. Firstly, we demonstrate that semantic correspondence is implicitly available through a rich set of image-level self-supervised methods. We further design a pixel-level self-supervised learning objective which specifically targets fine-grained correspondence. For downstream tasks, we fuse these two kinds of complementary correspondence representations together, demonstrating that they boost performance synergistically. Our method surpasses previous state-of-the-art self-supervised methods using convolutional networks on a variety of visual correspondence tasks, including video object segmentation, human pose tracking, and human part tracking.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2149297811",
                        "name": "Yingdong Hu"
                    },
                    {
                        "authorId": "152465861",
                        "name": "Renhao Wang"
                    },
                    {
                        "authorId": "2153281281",
                        "name": "Kaifeng Zhang"
                    },
                    {
                        "authorId": "2145974630",
                        "name": "Yang Gao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Since InfoNCE can be decomposed into alignment and uniformity terms [9, 76], many works introduce new forms of uniformity (and/or alignment) to design new objectives.",
                "Chen and Li [9] propose to explicitly match the distribution of representations to a prior distribution of high entropy as a new uniformity term."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "32ff43704a04191cf0b90a1dac7cbbc8f5df12a3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-09176",
                    "ArXiv": "2207.09176",
                    "DOI": "10.48550/arXiv.2207.09176",
                    "CorpusId": 250643930
                },
                "corpusId": 250643930,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/32ff43704a04191cf0b90a1dac7cbbc8f5df12a3",
                "title": "Self-Supervision Can Be a Good Few-Shot Learner",
                "abstract": "Existing few-shot learning (FSL) methods rely on training with a large labeled dataset, which prevents them from leveraging abundant unlabeled data. From an information-theoretic perspective, we propose an effective unsupervised FSL method, learning representations with self-supervision. Following the InfoMax principle, our method learns comprehensive representations by capturing the intrinsic structure of the data. Specifically, we maximize the mutual information (MI) of instances and their representations with a low-bias MI estimator to perform self-supervised pre-training. Rather than supervised pre-training focusing on the discriminable features of the seen classes, our self-supervised model has less bias toward the seen classes, resulting in better generalization for unseen classes. We explain that supervised pre-training and self-supervised pre-training are actually maximizing different MI objectives. Extensive experiments are further conducted to analyze their FSL performance with various training settings. Surprisingly, the results show that self-supervised pre-training can outperform supervised pre-training under the appropriate conditions. Compared with state-of-the-art FSL methods, our approach achieves comparable performance on widely used FSL benchmarks without any labels of the base classes.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2159316456",
                        "name": "Yuning Lu"
                    },
                    {
                        "authorId": "147383784",
                        "name": "Liangjiang Wen"
                    },
                    {
                        "authorId": "2144167531",
                        "name": "Jianzhuang Liu"
                    },
                    {
                        "authorId": "2144470300",
                        "name": "Yajing Liu"
                    },
                    {
                        "authorId": "40434674",
                        "name": "Xinmei Tian"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Another problem that may arise is insufficient informativeness: the contrastive objective does not prevent ignoring some of the relevant attributes [30]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "60f9b2904753d37c02164b7f0ae1436d92b7506b",
                "externalIds": {
                    "ArXiv": "2207.03478",
                    "DBLP": "journals/corr/abs-2207-03478",
                    "DOI": "10.48550/arXiv.2207.03478",
                    "CorpusId": 250334531
                },
                "corpusId": 250334531,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/60f9b2904753d37c02164b7f0ae1436d92b7506b",
                "title": "Red PANDA: Disambiguating Anomaly Detection by Removing Nuisance Factors",
                "abstract": "Anomaly detection methods strive to discover patterns that differ from the norm in a semantic way. This goal is ambiguous as a data point differing from the norm by an attribute e.g., age, race or gender, may be considered anomalous by some operators while others may consider this attribute irrelevant. Breaking from previous research, we present a new anomaly detection method that allows operators to exclude an attribute from being considered as relevant for anomaly detection. Our approach then learns representations which do not contain information over the nuisance attributes. Anomaly scoring is performed using a density-based approach. Importantly, our approach does not require specifying the attributes that are relevant for detecting anomalies, which is typically impossible in anomaly detection, but only attributes to ignore. An empirical investigation is presented verifying the effectiveness of our approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "22021547",
                        "name": "Niv Cohen"
                    },
                    {
                        "authorId": "113000774",
                        "name": "Jonathan Kahana"
                    },
                    {
                        "authorId": "2776254",
                        "name": "Yedid Hoshen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "result"
            ],
            "contexts": [
                "[8] identifies three intriguing properties of contrastive learning: a generalized version of the loss, learning with the presence of multiple objects, and feature suppression induced by competing augmentations.",
                "Such hypothesis aligns well with several recent attempts to understand CL better [8,39,68]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "968a96579d96c4cb026f94acb1820df8a5a7339e",
                "externalIds": {
                    "ArXiv": "2206.05259",
                    "DBLP": "journals/corr/abs-2206-05259",
                    "DOI": "10.48550/arXiv.2206.05259",
                    "CorpusId": 249605518
                },
                "corpusId": 249605518,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/968a96579d96c4cb026f94acb1820df8a5a7339e",
                "title": "Is Self-Supervised Learning More Robust Than Supervised Learning?",
                "abstract": "Self-supervised contrastive learning is a powerful tool to learn visual representation without labels. Prior work has primarily focused on evaluating the recognition accuracy of various pre-training algorithms, but has overlooked other behavioral aspects. In addition to accuracy, distributional robustness plays a critical role in the reliability of machine learning models. We design and conduct a series of robustness tests to quantify the behavioral differences between contrastive learning and supervised learning to downstream or pre-training data distribution changes. These tests leverage data corruptions at multiple levels, ranging from pixel-level gamma distortion to patch-level shuffling and to dataset-level distribution shift. Our tests unveil intriguing robustness behaviors of contrastive and supervised learning. On the one hand, under downstream corruptions, we generally observe that contrastive learning is surprisingly more robust than supervised learning. On the other hand, under pre-training corruptions, we find contrastive learning vulnerable to patch shuffling and pixel intensity change, yet less sensitive to dataset-level distribution change. We attempt to explain these results through the role of data augmentation and feature space properties. Our insight has implications in improving the downstream robustness of supervised learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8802368",
                        "name": "Yuanyi Zhong"
                    },
                    {
                        "authorId": "4990833",
                        "name": "Haoran Tang"
                    },
                    {
                        "authorId": "47739808",
                        "name": "Junkun Chen"
                    },
                    {
                        "authorId": "143718676",
                        "name": "Jian Peng"
                    },
                    {
                        "authorId": "2302062",
                        "name": "Yu-Xiong Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Our use of uniform distribution as negative examples is inspired by that feature uniformity is a desirable property for contrastive loss [56, 5], and so a good representation prefers such uniformity."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d9cfcc03374cd67141af93a143a65080869e9090",
                "externalIds": {
                    "ArXiv": "2206.04679",
                    "DBLP": "journals/corr/abs-2206-04679",
                    "DOI": "10.48550/arXiv.2206.04679",
                    "CorpusId": 248498306
                },
                "corpusId": 248498306,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d9cfcc03374cd67141af93a143a65080869e9090",
                "title": "POODLE: Improving Few-shot Learning via Penalizing Out-of-Distribution Samples",
                "abstract": "In this work, we propose to use out-of-distribution samples, i.e., unlabeled samples coming from outside the target classes, to improve few-shot learning. Specifically, we exploit the easily available out-of-distribution samples to drive the classifier to avoid irrelevant features by maximizing the distance from prototypes to out-of-distribution samples while minimizing that of in-distribution samples (i.e., support, query data). Our approach is simple to implement, agnostic to feature extractors, lightweight without any additional cost for pre-training, and applicable to both inductive and transductive settings. Extensive experiments on various standard benchmarks demonstrate that the proposed method consistently improves the performance of pretrained networks with different architectures.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2059393417",
                        "name": "Duong H. Le"
                    },
                    {
                        "authorId": "144295869",
                        "name": "Khoi Duc Minh Nguyen"
                    },
                    {
                        "authorId": "144295869",
                        "name": "Khoi Duc Minh Nguyen"
                    },
                    {
                        "authorId": "3090093",
                        "name": "Quoc-Huy Tran"
                    },
                    {
                        "authorId": "2789311",
                        "name": "R. Nguyen"
                    },
                    {
                        "authorId": "143807806",
                        "name": "Binh-Son Hua"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "To the best of our knowledge, the highest top-1 accuracies reported on ImageNet with SimCLR in 100 epochs are around 66.8% (Chen et al., 2021a).",
                "Practical properties of contrastive methods have been studied in Chen et al. (2021a).",
                "One such modification is illustrated in MoCo (He et al., 2020; Chen et al., 2020b; 2021b) where a memory bank of sample is combined with an exponential moving average (EMA) of the encoder to provide better negative pairs and thus improve training.",
                "Many works in this direction have recently flourished (Chen et al., 2020a; He et al., 2020; Chen et al., 2020b; 2021b; Yeh et al., 2021), most of them using the InfoNCE criterion (Oord et al., 2018), except HaoChen et al. (2021), that uses squared similarities between the samples.",
                "\u2026Grill et al., 2020; Lee et al., 2021b; Caron et al., 2020; Zbontar et al., 2021; Bardes et al., 2021; Tomasev et al., 2022; Caron et al., 2021; Chen et al., 2021b; Li et al., 2022a; Zhou et al., 2022a;b; HaoChen et al., 2021), approaching, and sometime even surpassing, the performance of\u2026"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "11c16254f7b61687b5d9b7637de032461a6ebb5f",
                "externalIds": {
                    "DBLP": "conf/iclr/GarridoCBNL23",
                    "ArXiv": "2206.02574",
                    "DOI": "10.48550/arXiv.2206.02574",
                    "CorpusId": 249395677
                },
                "corpusId": 249395677,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/11c16254f7b61687b5d9b7637de032461a6ebb5f",
                "title": "On the duality between contrastive and non-contrastive self-supervised learning",
                "abstract": "Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-contrastive approaches. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them. By designing contrastive and covariance based non-contrastive criteria that can be related algebraically and shown to be equivalent under limited assumptions, we show how close those families can be. We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show the influence (or lack thereof) of design choices on downstream performance. Motivated by our equivalence result, we investigate the low performance of SimCLR and show how it can match VICReg's with careful hyperparameter tuning, improving significantly over known baselines. We also challenge the popular assumption that non-contrastive methods need large output dimensions. Our theoretical and quantitative results suggest that the numerical gaps between contrastive and non-contrastive methods in certain regimes can be closed given better network design choices and hyperparameter tuning. The evidence shows that unifying different SOTA methods is an important direction to build a better understanding of self-supervised learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2048163343",
                        "name": "Q. Garrido"
                    },
                    {
                        "authorId": "2109184424",
                        "name": "Yubei Chen"
                    },
                    {
                        "authorId": "1453740540",
                        "name": "Adrien Bardes"
                    },
                    {
                        "authorId": "1688714",
                        "name": "Laurent Najman"
                    },
                    {
                        "authorId": "1688882",
                        "name": "Yann LeCun"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "Previous investigations (Chen et al., 2021) have shown that a few easy-tolearn irrelevant features not removed by augmentations can prevent CL model from learning all semantic features inside images.",
                "We propose here a first solution to this issue by studying RandBits-CIFAR10 (Chen et al., 2021), a CIFAR10 based dataset where k noisy bits are added and shared between views of the same image (see Appendix D.3).",
                ", 2021) (which is not apparent in object-centric datasets such as ImageNet) and few, irrelevant and easy-to-learn features, that are shared among views, are sufficient to collapse the representation (Chen et al., 2021) (a.",
                "Specifically, dominant objects inside images can prevent the model from learning features of smaller objects (Chen et al., 2021) (which is not apparent in object-centric datasets such as ImageNet) and few, irrelevant and easy-to-learn features, that are shared among views, are sufficient to\u2026",
                "\u2026learning features of smaller objects (Chen et al., 2021) (which is not apparent in object-centric datasets such as ImageNet) and few, irrelevant and easy-to-learn features, that are shared among views, are sufficient to collapse the representation (Chen et al., 2021) (a.k.a feature suppression).",
                "RandBits-CIFAR10 (Chen et al., 2021).",
                "noted previously (Chen et al., 2021), that \u03b2-VAE is the only method insensitive to the number of added bits, but its representation quality remains low compared to other selfsupervised approaches.",
                "We propose here a first solution to this issue by studying RandBits-CIFAR10 (Chen et al., 2021), a CIFAR10 based dataset where k noisy bits are added and shared between views of the same image (see Appendix D.",
                "Once trained, we use its representation to define the kernel KV AE .\nnoted previously (Chen et al., 2021), that \u03b2-VAE is the only method insensitive to the number of added bits, but its representation quality remains low compared to other selfsupervised approaches.",
                "We provide a solution to the feature suppression issue in CL (Chen et al., 2021) and also demonstrate SOTA results with weaker augmentations on visual benchmarks (both on natural and medical images).",
                "Specifically, dominant objects inside images can prevent the model from learning features of smaller objects (Chen et al., 2021) (which is not apparent in object-centric datasets such as ImageNet) and few, irrelevant and easy-to-learn features, that are shared among views, are sufficient to collapse the representation (Chen et al.",
                "For RandBits experiments, the VAE is trained with the same setup as for CIFAR-10/100 on RandBits-CIFAR10."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6856f79000407bc599ec8cce7ae651a30c3adafc",
                "externalIds": {
                    "DBLP": "conf/icml/DufumierBLDG23",
                    "ArXiv": "2206.01646",
                    "CorpusId": 258968171
                },
                "corpusId": 258968171,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6856f79000407bc599ec8cce7ae651a30c3adafc",
                "title": "Integrating Prior Knowledge in Contrastive Learning with Kernel",
                "abstract": "Data augmentation is a crucial component in unsupervised contrastive learning (CL). It determines how positive samples are defined and, ultimately, the quality of the learned representation. In this work, we open the door to new perspectives for CL by integrating prior knowledge, given either by generative models -- viewed as prior representations -- or weak attributes in the positive and negative sampling. To this end, we use kernel theory to propose a novel loss, called decoupled uniformity, that i) allows the integration of prior knowledge and ii) removes the negative-positive coupling in the original InfoNCE loss. We draw a connection between contrastive learning and conditional mean embedding theory to derive tight bounds on the downstream classification loss. In an unsupervised setting, we empirically demonstrate that CL benefits from generative models to improve its representation both on natural and medical images. In a weakly supervised scenario, our framework outperforms other unconditional and conditional CL approaches.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "89653117",
                        "name": "Benoit Dufumier"
                    },
                    {
                        "authorId": "12691936",
                        "name": "C. Barbano"
                    },
                    {
                        "authorId": "2117586389",
                        "name": "Robin Louiset"
                    },
                    {
                        "authorId": "1710398",
                        "name": "E. Duchesnay"
                    },
                    {
                        "authorId": "1742163277",
                        "name": "P. Gori"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The principles behind the success of all these methods are still the subject of active research [10, 40, 44]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "def9a4c84dc67b51eb7781a5bc7a25d1120ef177",
                "externalIds": {
                    "DBLP": "conf/cvpr/JeanSPB22",
                    "DOI": "10.1109/CVPRW56347.2022.00446",
                    "CorpusId": 251020478
                },
                "corpusId": 251020478,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/def9a4c84dc67b51eb7781a5bc7a25d1120ef177",
                "title": "Self-Supervised Learning of Pose-Informed Latents",
                "abstract": "Siamese network architectures trained for self-supervised instance recognition can learn powerful visual representations that are useful in various tasks. Many such approaches maximize the similarity between representations of augmented images of the same object. In this paper, we depart from traditional self-supervised learning benchmarks by defining a novel methodology for new challenging tasks such as zero shot pose estimation. Our goal is to show that common Siamese networks can effectively be trained on frame pairs from video sequences to generate pose-informed representations. Unlike parallel efforts that focus on introducing new image-space operators for data augmentation, we argue that extending the augmentation strategy by using different frames of a video leads to more powerful representations. To show the effectiveness of this approach, we use the Objectron and UCF101 datasets to learn representations and evaluate them on pose estimation, action recognition, and object re-identification. Furthermore, we carefully validate our method against a number of baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2179021714",
                        "name": "Rapha\u00ebl Jean"
                    },
                    {
                        "authorId": "1399252442",
                        "name": "P. St-Charles"
                    },
                    {
                        "authorId": "2604835",
                        "name": "S. Pirk"
                    },
                    {
                        "authorId": "33622538",
                        "name": "Simon Brodeur"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cc539f54083bde6441676bb39ee91c946b9078c8",
                "externalIds": {
                    "DBLP": "conf/eccv/MoskalevSFS22",
                    "ArXiv": "2205.15814",
                    "DOI": "10.48550/arXiv.2205.15814",
                    "CorpusId": 249210193
                },
                "corpusId": 249210193,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/cc539f54083bde6441676bb39ee91c946b9078c8",
                "title": "Contrasting quadratic assignments for set-based representation learning",
                "abstract": "The standard approach to contrastive learning is to maximize the agreement between different views of the data. The views are ordered in pairs, such that they are either positive, encoding different views of the same object, or negative, corresponding to views of different objects. The supervisory signal comes from maximizing the total similarity over positive pairs, while the negative pairs are needed to avoid collapse. In this work, we note that the approach of considering individual pairs cannot account for both intra-set and inter-set similarities when the sets are formed from the views of the data. It thus limits the information content of the supervisory signal available to train representations. We propose to go beyond contrasting individual pairs of objects by focusing on contrasting objects as sets. For this, we use combinatorial quadratic assignment theory designed to evaluate set and graph similarities and derive set-contrastive objective as a regularizer for contrastive learning methods. We conduct experiments and demonstrate that our method improves learned representations for the tasks of metric learning and self-supervised classification.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48127549",
                        "name": "A. Moskalev"
                    },
                    {
                        "authorId": "48819806",
                        "name": "Ivan Sosnovik"
                    },
                    {
                        "authorId": "2144816511",
                        "name": "Volker Fischer"
                    },
                    {
                        "authorId": "144638781",
                        "name": "A. Smeulders"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "591a33c988dcb2e1f66a34acdb6b90c2bc6fdc04",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-13279",
                    "ArXiv": "2205.13279",
                    "DOI": "10.48550/arXiv.2205.13279",
                    "CorpusId": 249097879
                },
                "corpusId": 249097879,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/591a33c988dcb2e1f66a34acdb6b90c2bc6fdc04",
                "title": "Triangular Contrastive Learning on Molecular Graphs",
                "abstract": "Recent contrastive learning methods have shown to be effective in various tasks, learning generalizable representations invariant to data augmentation thereby leading to state of the art performances. Regarding the multifaceted nature of large unlabeled data used in self-supervised learning while majority of real-word downstream tasks use single format of data, a multimodal framework that can train single modality to learn diverse perspectives from other modalities is an important challenge. In this paper, we propose TriCL (Triangular Contrastive Learning), a universal framework for trimodal contrastive learning. TriCL takes advantage of Triangular Area Loss, a novel intermodal contrastive loss that learns the angular geometry of the embedding space through simultaneously contrasting the area of positive and negative triplets. Systematic observation on embedding space in terms of alignment and uniformity showed that Triangular Area Loss can address the line-collapsing problem by discriminating modalities by angle. Our experimental results also demonstrate the outperformance of TriCL on downstream task of molecular property prediction which implies that the advantages of the embedding space indeed benefits the performance on downstream tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2166916702",
                        "name": "MinGyu Choi"
                    },
                    {
                        "authorId": "2056461082",
                        "name": "Wonseok Shin"
                    },
                    {
                        "authorId": "2141540963",
                        "name": "Yijingxiu Lu"
                    },
                    {
                        "authorId": "1863576",
                        "name": "Sun Kim"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "\u2026)\n= \u2212 1\nN\n\u2211\ni,j\u2208MB\nsim(zi, zj)/\u03c4\n\ufe38 \ufe37\ufe37 \ufe38\nLalignment\n+ 1\nN\nN\u2211\ni\nlog ( 2N\u2211\nk=1\n1[k 6=i]exp(sim(zi, zk)/\u03c4)\n\ufe38 \ufe37\ufe37 \ufe38\nLdistribution\n) (3)\nand as such there are two parts of the loss, namely, alignment and distribution, as proposed by [Wang and Isola, 2020] and further studied by [Chen et al., 2020b].",
                "Here, the authors propose the temperature scaled loss NT-Xent which is based on noise contrastive estimation and cross entropy [Chen et al., 2020a].",
                "Equation (2) can be rewritten accordingly by applying the logarithmic rules\nLNT\u2212Xent = \u2212 1\nN\n\u2211\ni,j\u2208MB\n(\nsim(zi, zj)/\u03c4 \u2212 log\n2N\u2211\nk=1\n1[k 6=i]exp(sim(zi, zk)/\u03c4) )\n= \u2212 1\nN\n\u2211\ni,j\u2208MB\nsim(zi, zj)/\u03c4\n\ufe38 \ufe37\ufe37 \ufe38\nLalignment\n+ 1\nN\nN\u2211\ni\nlog ( 2N\u2211\nk=1\n1[k 6=i]exp(sim(zi, zk)/\u03c4)\n\ufe38 \ufe37\ufe37 \ufe38\nLdistribution\n) (3)\nand as such there are two parts of the loss, namely, alignment and distribution, as proposed by [Wang and Isola, 2020] and further studied by [Chen et al., 2020b].",
                "Self-supervised learning models and frameworks like CPC[van den Oord et al., 2019], SimCLR [Chen et al., 2020a], BYOL [Grill et al., 2020], and SimSIAM [Chen and He, 2020] show promising results in low-labeled data regimes, with representations generalizing well in downstream tasks like visual\u2026",
                "The simple framework for contrastive learning on visual representations (SimCLR) proposed by [Chen et al., 2020a] builds upon properties of noise contrastive estimation (NCE) and is inspired by previous work in the related field."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c2254d458bd8137b7fec081be9a8e93f3f7b3f47",
                "externalIds": {
                    "ArXiv": "2205.03169",
                    "CorpusId": 248563011
                },
                "corpusId": 248563011,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c2254d458bd8137b7fec081be9a8e93f3f7b3f47",
                "title": "The NT-Xent loss upper bound",
                "abstract": "Self-supervised learning is a growing paradigm in deep representation learning, showing great generalization capabilities and competitive performance in low-labeled data regimes. The SimCLR framework proposes the NT-Xent loss for contrastive representation learning. The objective of the loss function is to maximize agreement, similarity, between sampled positive pairs. This short paper derives and proposes an upper bound for the loss and average similarity. An analysis of the implications is however not provided, but we strongly encourage anyone in the field to conduct this.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2164341313",
                        "name": "Wilhelm AAgren"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "2010) (also known as Contrastive Learning) has emerged as a highly effective approach for unsupervised representation learning using deep networks (Chen et al., 2020; Chen & Li, 2020; Tian et al., 2021; Grill et al., 2020)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bdbfba030b0acf263f5f78e7a064cd96097e5de2",
                "externalIds": {
                    "ArXiv": "2205.01789",
                    "DBLP": "journals/corr/abs-2205-01789",
                    "DOI": "10.48550/arXiv.2205.01789",
                    "CorpusId": 248512556
                },
                "corpusId": 248512556,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/bdbfba030b0acf263f5f78e7a064cd96097e5de2",
                "title": "Do More Negative Samples Necessarily Hurt in Contrastive Learning?",
                "abstract": "Recent investigations in noise contrastive estimation suggest, both empirically as well as theoretically, that while having more\"negative samples\"in the contrastive loss improves downstream classification performance initially, beyond a threshold, it hurts downstream performance due to a\"collision-coverage\"trade-off. But is such a phenomenon inherent in contrastive learning? We show in a simple theoretical setting, where positive pairs are generated by sampling from the underlying latent class (introduced by Saunshi et al. (ICML 2019)), that the downstream performance of the representation optimizing the (population) contrastive loss in fact does not degrade with the number of negative samples. Along the way, we give a structural characterization of the optimal representation in our framework, for noise contrastive estimation. We also provide empirical support for our theoretical results on CIFAR-10 and CIFAR-100 datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144030228",
                        "name": "Pranjal Awasthi"
                    },
                    {
                        "authorId": "2312421",
                        "name": "Nishanth Dikkala"
                    },
                    {
                        "authorId": "48655887",
                        "name": "Pritish Kamath"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Chen et al. (2021) conclude that contrastive losses rely on easy-to-detect features that solve the contrastive objective, while suppressing the remaining (partly irrelevant) information.",
                ", 2021) and mainly relies on easy-to-detect features to contrast between positive and negative pairs (Chen et al., 2021).",
                "Chen et al. (2021) add artificially generated features (i.e., MNIST digits) as an extra overlay to images.",
                "Chen et al. (2021) introduce the notion of feature suppression among competing features.",
                "The contrastive InfoNCE objective itself does not guarantee that all the predictive features in the input data are learned (Robinson et al., 2021) and mainly relies on easy-to-detect features to contrast between positive and negative pairs (Chen et al., 2021).",
                "Chen et al. (2021) refer to this phenomenon as feature suppression among competing features."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5c8029a0b9b154e48e4b845462dc03d914123e8d",
                "externalIds": {
                    "ArXiv": "2204.13382",
                    "DBLP": "journals/tmlr/BleekerYR23",
                    "CorpusId": 258187493
                },
                "corpusId": 258187493,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5c8029a0b9b154e48e4b845462dc03d914123e8d",
                "title": "Reducing Predictive Feature Suppression in Resource-Constrained Contrastive Image-Caption Retrieval",
                "abstract": "To train image-caption retrieval (ICR) methods, contrastive loss functions are a common choice for optimization functions. Unfortunately, contrastive ICR methods are vulnerable to predictive feature suppression. Predictive features are features that correctly indicate the similarity between a query and a candidate item. However, in the presence of multiple predictive features during training, encoder models tend to suppress redundant predictive features, since these features are not needed to learn to discriminate between positive and negative pairs. While some predictive features are redundant during training, these features might be relevant during evaluation. We introduce an approach to reduce predictive feature suppression for resource-constrained ICR methods: latent target decoding (LTD). We add an additional decoder to the contrastive ICR framework, to reconstruct the input caption in a latent space of a general-purpose sentence encoder, which prevents the image and caption encoder from suppressing predictive features. We implement the LTD objective as an optimization constraint, to ensure that the reconstruction loss is below a bound value while primarily optimizing for the contrastive loss. Importantly, LTD does not depend on additional training data or expensive (hard) negative mining strategies. Our experiments show that, unlike reconstructing the input caption in the input space, LTD reduces predictive feature suppression, measured by obtaining higher recall@k, r-precision, and nDCG scores than a contrastive ICR baseline. Moreover, we show that LTD should be implemented as an optimization constraint instead of a dual optimization objective. Finally, we show that LTD can be used with different contrastive learning losses and a wide variety of resource-constrained ICR methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1452678770",
                        "name": "Maurits J. R. Bleeker"
                    },
                    {
                        "authorId": "2136074457",
                        "name": "Andrew Yates"
                    },
                    {
                        "authorId": "1696030",
                        "name": "M. de Rijke"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2643caad4b5b9667ca8a2f5116a96d12474fcb34",
                "externalIds": {
                    "ArXiv": "2204.07344",
                    "DBLP": "conf/midl/TaherHGL22",
                    "DOI": "10.48550/arXiv.2204.07344",
                    "CorpusId": 248218716,
                    "PubMed": "36579134"
                },
                "corpusId": 248218716,
                "publicationVenue": {
                    "id": "2c1c3a62-7d6e-44b9-b225-a9ddb7ebdb8d",
                    "name": "International Conference on Medical Imaging with Deep Learning",
                    "type": "conference",
                    "alternate_names": [
                        "MIDL",
                        "Int Conf Med Imaging Deep Learn"
                    ],
                    "url": "https://www.midl.io/"
                },
                "url": "https://www.semanticscholar.org/paper/2643caad4b5b9667ca8a2f5116a96d12474fcb34",
                "title": "CAiD: Context-Aware Instance Discrimination for Self-supervised Learning in Medical Imaging",
                "abstract": "Recently, self-supervised instance discrimination methods have achieved significant success in learning visual representations from unlabeled photographic images. However, given the marked differences between photographic and medical images, the efficacy of instance-based objectives, focusing on learning the most discriminative global features in the image (i.e., wheels in bicycle), remains unknown in medical imaging. Our preliminary analysis showed that high global similarity of medical images in terms of anatomy hampers instance discrimination methods for capturing a set of distinct features, negatively impacting their performance on medical downstream tasks. To alleviate this limitation, we have developed a simple yet effective self-supervised framework, called Context-Aware instance Discrimination (CAiD). CAiD aims to improve instance discrimination learning by providing finer and more discriminative information encoded from a diverse local context of unlabeled medical images. We conduct a systematic analysis to investigate the utility of the learned features from a three-pronged perspective: (i) generalizability and transferability, (ii) separability in the embedding space, and (iii) reusability. Our extensive experiments demonstrate that CAiD (1) enriches representations learned from existing instance discrimination methods; (2) delivers more discriminative features by adequately capturing finer contextual information from individual medial images; and (3) improves reusability of low/mid-level features compared to standard instance discriminative methods. As open science, all codes and pre-trained models are available on our GitHub page: https://github.com/JLiangLab/CAiD.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "21811029",
                        "name": "M. Taher"
                    },
                    {
                        "authorId": "2073573316",
                        "name": "Fatemeh Haghighi"
                    },
                    {
                        "authorId": "1777226",
                        "name": "M. Gotway"
                    },
                    {
                        "authorId": "1485304039",
                        "name": "Jianming Liang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Can alignment and uniformity properties be preserved in supervised contrastive loss? For self-supervised learning, [8] claims that contrastive loss [7, 33] (see Eq."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b732fdabc9dc6feb6f771d3f392a280e26bd001c",
                "externalIds": {
                    "ArXiv": "2203.15381",
                    "DBLP": "conf/cvpr/PuZZ22",
                    "DOI": "10.1109/CVPR52688.2022.01934",
                    "CorpusId": 247779258
                },
                "corpusId": 247779258,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b732fdabc9dc6feb6f771d3f392a280e26bd001c",
                "title": "Alignment-Uniformity aware Representation Learning for Zero-shot Video Classification",
                "abstract": "Most methods tackle zero-shot video classification by aligning visual-semantic representations within seen classes, which limits generalization to unseen classes. To enhance model generalizability, this paper presents an end-to-end framework that preserves alignment and uniformity properties for representations on both seen and unseen classes. Specifically, we formulate a supervised contrastive loss to simultaneously align visual-semantic features (i.e., alignment) and encourage the learned features to distribute uniformly (i.e., uniformity). Unlike existing methods that only consider the alignment, we propose uniformity to preserve maximal-info of existing features, which improves the probability that unobserved features fall around observed data. Further, we synthesize features of unseen classes by proposing a class generator that interpolates and extrapolates the features of seen classes. Besides, we introduce two metrics, closeness and dispersion, to quantify the two properties and serve as new measurements of model generalirability. Experiments show that our method significantly outperforms SoTA by relative improvements of 28.1% on UCF101 and 27.0% on HMDB51. Code is available11https://github.com/ShipuLoveMili/CVPR2022-AURL.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2357549",
                        "name": "Shi Pu"
                    },
                    {
                        "authorId": "2393320",
                        "name": "Kaili Zhao"
                    },
                    {
                        "authorId": "2114136172",
                        "name": "Mao Zheng"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In addition, purely contrastive models tend to incorporate center bias [9, 8], which makes them less transferable for tasks such as segmentation where non-object centric regions need to be modeled.",
                "For example, the center-bias [9] and small-object feature suppression [8] have been observed in prior works."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4bc8fa1ca83cf4a6b93044b3103fe622b012d90d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-12054",
                    "ArXiv": "2203.12054",
                    "DOI": "10.48550/arXiv.2203.12054",
                    "CorpusId": 247618909
                },
                "corpusId": 247618909,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4bc8fa1ca83cf4a6b93044b3103fe622b012d90d",
                "title": "Self-supervision through Random Segments with Autoregressive Coding (RandSAC)",
                "abstract": "Inspired by the success of self-supervised autoregressive representation learning in natural language (GPT and its variants), and advances in recent visual architecture design with Vision Transformers (ViTs), in this paper, we explore the effect various design choices have on the success of applying such training strategies for visual feature learning. Specifically, we introduce a novel strategy that we call Random Segments with Autoregressive Coding (RandSAC). In RandSAC, we group patch representations (image tokens) into hierarchically arranged segments; within each segment, tokens are predicted in parallel, similar to BERT, while across segment predictions are sequential, similar to GPT. We illustrate that randomized serialization of the segments significantly improves the performance and results in distribution over spatially-long (across-segments) and -short (within-segment) predictions which are effective for feature learning. We illustrate the pertinence of these design choices and explore alternatives on a number of datasets (e.g., CIFAR10, CIFAR100, ImageNet). While our pre-training strategy works with a vanilla Transformer, we also propose a conceptually simple, but highly effective, addition to the decoder that allows learnable skip-connections to encoder$'$s feature layers, which further improves the performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1419971650",
                        "name": "Tianyu Hua"
                    },
                    {
                        "authorId": "2476765",
                        "name": "Yonglong Tian"
                    },
                    {
                        "authorId": "1823941979",
                        "name": "Sucheng Ren"
                    },
                    {
                        "authorId": "2146231364",
                        "name": "Hang Zhao"
                    },
                    {
                        "authorId": "144398147",
                        "name": "L. Sigal"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "This is a case of the documented phenomenon of feature suppression [8, 9, 10]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0f97e31effa8d9f82b57c0bfbee05793ba8347eb",
                "externalIds": {
                    "DBLP": "conf/eccv/KahanaH22",
                    "ArXiv": "2203.11284",
                    "DOI": "10.48550/arXiv.2203.11284",
                    "CorpusId": 247596859
                },
                "corpusId": 247596859,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/0f97e31effa8d9f82b57c0bfbee05793ba8347eb",
                "title": "A Contrastive Objective for Learning Disentangled Representations",
                "abstract": "Learning representations of images that are invariant to sensitive or unwanted attributes is important for many tasks including bias removal and cross domain retrieval. Here, our objective is to learn representations that are invariant to the domain (sensitive attribute) for which labels are provided, while being informative over all other image attributes, which are unlabeled. We present a new approach, proposing a new domain-wise contrastive objective for ensuring invariant representations. This objective crucially restricts negative image pairs to be drawn from the same domain, which enforces domain invariance whereas the standard contrastive objective does not. This domain-wise objective is insufficient on its own as it suffers from shortcut solutions resulting in feature suppression. We overcome this issue by a combination of a reconstruction constraint, image augmentations and initialization with pre-trained weights. Our analysis shows that the choice of augmentations is important, and that a misguided choice of augmentations can harm the invariance and informativeness objectives. In an extensive evaluation, our method convincingly outperforms the state-of-the-art in terms of representation invariance, representation informativeness, and training speed. Furthermore, we find that in some cases our method can achieve excellent results even without the reconstruction constraint, leading to a much faster and resource efficient training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "113000774",
                        "name": "Jonathan Kahana"
                    },
                    {
                        "authorId": "2776254",
                        "name": "Yedid Hoshen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "But such an assumption inevitably fails due to inconsistent learning signals in scene images full of diverse objects [9, 33, 40].",
                "However, as a number of prior works have identified, the semantic context provided by extra \u201cdistractor\u201d classes outside of the main object classes can serve as a useful signal for clustering [5, 9, 24].",
                "Finally, we broadcast the class assignments back to the original dimensions of the image I via nearest neighbor interpolation, akin to [9]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "07f06e1dc3f3ab87d7c020f2576b7070786ecca9",
                "externalIds": {
                    "ArXiv": "2203.09343",
                    "DBLP": "conf/eccv/WangZG22",
                    "DOI": "10.48550/arXiv.2203.09343",
                    "CorpusId": 247519064
                },
                "corpusId": 247519064,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/07f06e1dc3f3ab87d7c020f2576b7070786ecca9",
                "title": "CYBORGS: Contrastively Bootstrapping Object Representations by Grounding in Segmentation",
                "abstract": "Many recent approaches in contrastive learning have worked to close the gap between pretraining on iconic images like ImageNet and pretraining on complex scenes like COCO. This gap exists largely because commonly used random crop augmentations obtain semantically inconsistent content in crowded scene images of diverse objects. Previous works use preprocessing pipelines to localize salient objects for improved cropping, but an end-to-end solution is still elusive. In this work, we propose a framework which accomplishes this goal via joint learning of representations and segmentation. We leverage segmentation masks to train a model with a mask-dependent contrastive loss, and use the partially trained model to bootstrap better masks. By iterating between these two components, we ground the contrastive updates in segmentation information, and simultaneously improve segmentation throughout pretraining. Experiments show our representations transfer robustly to downstream tasks in classification, detection and segmentation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152465861",
                        "name": "Renhao Wang"
                    },
                    {
                        "authorId": "2146232688",
                        "name": "Hang Zhao"
                    },
                    {
                        "authorId": "2154880752",
                        "name": "Yang Gao"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Moreover, video-based methods have not incorporated data augmentation, thereby resulting in feature suppression which degrades representation quality [43]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "330dff1f07f9ca015d7af6c734ad7ac5c4c7f077",
                "externalIds": {
                    "DBLP": "journals/ral/FungBN23",
                    "ArXiv": "2203.00187",
                    "DOI": "10.1109/LRA.2023.3269306",
                    "CorpusId": 247187521
                },
                "corpusId": 247187521,
                "publicationVenue": {
                    "id": "93c335b7-edf4-45f5-8ddc-7c5835154945",
                    "name": "IEEE Robotics and Automation Letters",
                    "alternate_names": [
                        "IEEE Robot Autom Lett"
                    ],
                    "issn": "2377-3766",
                    "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=7083369"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/330dff1f07f9ca015d7af6c734ad7ac5c4c7f077",
                "title": "Robots Autonomously Detecting People: A Multimodal Deep Contrastive Learning Method Robust to Intraclass Variations",
                "abstract": "Robotic detection of people in crowded and/or cluttered human-centered environments including hospitals, stores and airports is challenging as people can become occluded by other people or objects, and deform due to clothing or pose variations. There can also be loss of discriminative visual features due to poor lighting. In this letter, we present a novel multimodal person detection architecture to address the mobile robot problem of person detection under intraclass variations. We present a two-stage training approach using: 1) a unique pretraining method we define as Temporal Invariant Multimodal Contrastive Learning (TimCLR), and 2) a Multimodal YOLOv4 (MYOLOv4) detector for finetuning. TimCLR learns person representations that are invariant under intraclass variations through unsupervised learning. Our approach is unique in that it generates image pairs from natural variations within multimodal image sequences and contrasts crossmodal features to transfer invariances between different modalities. These pretrained features are used by the MYOLOv4 detector for finetuning and person detection from RGB-D images. Extensive experiments validate the performance of our DL architecture in both human-centered crowded and cluttered environments. Results show that our method outperforms existing unimodal and multimodal person detection approaches in detection accuracy when considering body occlusions and pose deformations in different lighting.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1961248314",
                        "name": "Angus Fung"
                    },
                    {
                        "authorId": "1719617",
                        "name": "B. Benhabib"
                    },
                    {
                        "authorId": "2497882",
                        "name": "G. Nejat"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "A related idea of feature suppression (Chen et al., 2021) and shortcut solutions found by contrastive learning was recently studied in Robinson et al.",
                "A related idea of feature suppression (Chen et al., 2021) and shortcut solutions found by contrastive learning was recently studied in Robinson et al. (2021) in certain stylized settings, with a proposed fix through better augmentations strategies."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e64914f9d4d2eabf3fa1352e2d132364a5539aaa",
                "externalIds": {
                    "DBLP": "conf/icml/SaunshiAGMZAKK22",
                    "ArXiv": "2202.14037",
                    "CorpusId": 247158785
                },
                "corpusId": 247158785,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e64914f9d4d2eabf3fa1352e2d132364a5539aaa",
                "title": "Understanding Contrastive Learning Requires Incorporating Inductive Biases",
                "abstract": "Contrastive learning is a popular form of self-supervised learning that encourages augmentations (views) of the same input to have more similar representations compared to augmentations of different inputs. Recent attempts to theoretically explain the success of contrastive learning on downstream classification tasks prove guarantees depending on properties of {\\em augmentations} and the value of {\\em contrastive loss} of representations. We demonstrate that such analyses, that ignore {\\em inductive biases} of the function class and training algorithm, cannot adequately explain the success of contrastive learning, even {\\em provably} leading to vacuous guarantees in some settings. Extensive experiments on image and text domains highlight the ubiquity of this problem -- different function classes and algorithms behave very differently on downstream tasks, despite having the same augmentations and contrastive losses. Theoretical analysis is presented for the class of linear representations, where incorporating inductive biases of the function class allows contrastive learning to work with less stringent conditions compared to prior analyses.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "10769461",
                        "name": "Nikunj Saunshi"
                    },
                    {
                        "authorId": "40401847",
                        "name": "J. Ash"
                    },
                    {
                        "authorId": "9935792",
                        "name": "Surbhi Goel"
                    },
                    {
                        "authorId": "31498163",
                        "name": "Dipendra Kumar Misra"
                    },
                    {
                        "authorId": "15943185",
                        "name": "Cyril Zhang"
                    },
                    {
                        "authorId": "145563459",
                        "name": "Sanjeev Arora"
                    },
                    {
                        "authorId": "144695232",
                        "name": "S. Kakade"
                    },
                    {
                        "authorId": "37019006",
                        "name": "A. Krishnamurthy"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "We follow Chen & Li (2020) to see how local features are agglomerated across layers.",
                "However, this line of works focus on pre-training on ImageNet-like images, and recent attention has been attracted to images with multi-objects and multi-texture presented (Chen & Li, 2020).",
                "To study why the large generalization gap exists, in Figure 4, we follow Chen & Li (2020) to see how features aggregate in space."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "620611fb99f47be149ff2b4b2f5f73432dda5557",
                "externalIds": {
                    "ArXiv": "2202.09059",
                    "DBLP": "journals/corr/abs-2202-09059",
                    "CorpusId": 246996668
                },
                "corpusId": 246996668,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/620611fb99f47be149ff2b4b2f5f73432dda5557",
                "title": "Towards better understanding and better generalization of few-shot classification in histology images with contrastive learning",
                "abstract": "Few-shot learning is an established topic in natural images for years, but few work is attended to histology images, which is of high clinical value since well-labeled datasets and rare abnormal samples are expensive to collect. Here, we facilitate the study of few-shot learning in histology images by setting up three cross-domain tasks that simulate real clinics problems. To enable label-efficient learning and better generalizability, we propose to incorporate contrastive learning (CL) with latent augmentation (LA) to build a few-shot system. CL learns useful representations without manual labels, while LA transfers semantic variations of the base dataset in an unsupervised way. These two components fully exploit unlabeled training data and can scale gracefully to other label-hungry problems. In experiments, we find i) models learned by CL generalize better than supervised learning for histology images in unseen classes, and ii) LA brings consistent gains over baselines. Prior studies of self-supervised learning mainly focus on ImageNet-like images, which only present a dominant object in their centers. Recent attention has been paid to images with multi-objects and multi-textures. Histology images are a natural choice for such a study. We show the superiority of CL over supervised learning in terms of generalization for such data and provide our empirical understanding for this observation. The findings in this work could contribute to understanding how the model generalizes in the context of both representation learning and histological image analysis. Code is available.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47988343",
                        "name": "Jiawei Yang"
                    },
                    {
                        "authorId": "12774806",
                        "name": "Hanbo Chen"
                    },
                    {
                        "authorId": "30411824",
                        "name": "Jiangpeng Yan"
                    },
                    {
                        "authorId": "2151110220",
                        "name": "Xiaoyu Chen"
                    },
                    {
                        "authorId": "1722252",
                        "name": "Jianhua Yao"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "A lot of recent work on ICR relies on (1) pre-training on large amounts of data [16, 26, 35], and (2) more sophisticated (and data-hungry) model architectures [5, 11, 12, 24, 25, 31].",
                "To learn the similarity between a query and candidate representations, most ICR work relies on the standard Triplet loss with semi-hard negatives (Triplet SH) [4, 5, 11, 12, 24, 25, 31] or on the cross-entropy based NT-Xent [6, 16] loss.",
                "The Triplet loss is commonly used as a loss function for ICR methods [5, 11, 12, 24, 25, 31]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0e92365a48b7ac6fba350893fd7e9d355cf3a39d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-07474",
                    "ArXiv": "2202.07474",
                    "DOI": "10.1007/978-3-030-99736-6_36",
                    "CorpusId": 246863448
                },
                "corpusId": 246863448,
                "publicationVenue": {
                    "id": "8ee71e17-421e-43db-ad2d-cc8af6217a0d",
                    "name": "European Conference on Information Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "ECIR",
                        "Eur Conf Inf Retr"
                    ],
                    "url": "https://en.wikipedia.org/wiki/European_Conference_on_Information_Retrieval"
                },
                "url": "https://www.semanticscholar.org/paper/0e92365a48b7ac6fba350893fd7e9d355cf3a39d",
                "title": "Do Lessons from Metric Learning Generalize to Image-Caption Retrieval?",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1452678770",
                        "name": "Maurits J. R. Bleeker"
                    },
                    {
                        "authorId": "1696030",
                        "name": "M. de Rijke"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In addition to framework design, theoretical analyses and empirical studies have also been proposed to better understand the behavior and properties of contrastive learning [1, 3, 6, 9, 24, 31, 35, 39, 39, 41, 44, 52]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a42d557c963f9737ac40111f3a065d842caaf3fc",
                "externalIds": {
                    "ArXiv": "2202.03278",
                    "DBLP": "journals/corr/abs-2202-03278",
                    "DOI": "10.1109/CVPR52688.2022.01556",
                    "CorpusId": 246634338
                },
                "corpusId": 246634338,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a42d557c963f9737ac40111f3a065d842caaf3fc",
                "title": "Crafting Better Contrastive Views for Siamese Representation Learning",
                "abstract": "Recent self-supervised contrastive learning methods greatly benefit from the Siamese structure that aims at minimizing distances between positive pairs. For high performance Siamese representation learning, one of the keys is to design good contrastive pairs. Most previous works simply apply random sampling to make different crops of the same image, which overlooks the semantic information that may degrade the quality of views. In this work, we propose ContrastiveCrop, which could effectively generate better crops for Siamese representation learning. Firstly, a semantic-aware object localization strategy is proposed within the training process in a fully unsupervised manner. This guides us to generate contrastive views which could avoid most false positives (i.e., object vs. background). Moreover, we empirically find that views with similar appearances are trivial for the Siamese model training. Thus, a center-suppressed sampling is further designed to enlarge the variance of crops. Remarkably, our method takes a careful consideration of positive pairs for contrastive learning with negligible extra training overhead. As a plug-and-play and framework-agnostic module, ContrastiveCrop consistently improves SimCLR, MoCo, BYOL, SimSiam by 0.4% \u223c 2.0% classification accuracy on CIFAR-10, CIFAR-100, Tiny ImageNet and STL-10. Superior results are also achieved on downstream detection and segmentation tasks when pre-trained on ImageNet-1K.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115815502",
                        "name": "Xiang Peng"
                    },
                    {
                        "authorId": "2148896193",
                        "name": "Kai Wang"
                    },
                    {
                        "authorId": "46637795",
                        "name": "Zheng Hua Zhu"
                    },
                    {
                        "authorId": "2054451943",
                        "name": "Yang You"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "995ee983a7dd66bf30408a5d1c1a8c3684431998",
                "externalIds": {
                    "DBLP": "conf/icic/WangD23",
                    "ArXiv": "2201.05979",
                    "DOI": "10.1007/978-981-99-4752-2_35",
                    "CorpusId": 246016137
                },
                "corpusId": 246016137,
                "publicationVenue": {
                    "id": "47e4f61e-9b0e-431f-bd17-e534b53c655b",
                    "name": "International Conference on Intelligent Computing",
                    "type": "conference",
                    "alternate_names": [
                        "ICIC",
                        "Int Conf Ind Instrum Control",
                        "Int Conf Intell Comput",
                        "International Conference on Industrial Instrumentation and Control"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1382"
                },
                "url": "https://www.semanticscholar.org/paper/995ee983a7dd66bf30408a5d1c1a8c3684431998",
                "title": "SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144223218",
                        "name": "Hao Wang"
                    },
                    {
                        "authorId": "2154901090",
                        "name": "Yangguang Li"
                    },
                    {
                        "authorId": "2026274588",
                        "name": "Zhen Huang"
                    },
                    {
                        "authorId": "67069932",
                        "name": "Yong Dou"
                    },
                    {
                        "authorId": "47648549",
                        "name": "Lingpeng Kong"
                    },
                    {
                        "authorId": "2111877400",
                        "name": "J. Shao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9dd97465c4f1ee208173d03171d6df847d5dc9f4",
                "externalIds": {
                    "DBLP": "journals/sigpro/HafidiGCS22",
                    "MAG": "3196436997",
                    "DOI": "10.1016/j.sigpro.2021.108310",
                    "CorpusId": 239638135
                },
                "corpusId": 239638135,
                "publicationVenue": {
                    "id": "20298d29-1b7a-4631-8e1d-e0a5d34bcf58",
                    "name": "Signal Processing",
                    "type": "journal",
                    "alternate_names": [
                        "Signal Process"
                    ],
                    "issn": "0165-1684",
                    "url": "https://www.journals.elsevier.com/signal-processing",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/01651684"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9dd97465c4f1ee208173d03171d6df847d5dc9f4",
                "title": "Negative sampling strategies for contrastive self-supervised learning of graph representations",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2006767335",
                        "name": "H. Hafidi"
                    },
                    {
                        "authorId": "1696513",
                        "name": "M. Ghogho"
                    },
                    {
                        "authorId": "1719017",
                        "name": "P. Ciblat"
                    },
                    {
                        "authorId": "144231976",
                        "name": "A. Swami"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Contrastive learning implicitly learns relations among instances by optimizing alignment and matching a prior distribution [51, 9]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1e37ed53c510880c90fb73bb05947574951616f6",
                "externalIds": {
                    "DBLP": "conf/wacv/DenizeROHC23",
                    "ArXiv": "2111.14585",
                    "DOI": "10.1109/WACV56688.2023.00273",
                    "CorpusId": 244714490
                },
                "corpusId": 244714490,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/1e37ed53c510880c90fb73bb05947574951616f6",
                "title": "Similarity Contrastive Estimation for Self-Supervised Soft Contrastive Learning",
                "abstract": "Contrastive representation learning has proven to be an effective self-supervised learning method. Most successful approaches are based on Noise Contrastive Estimation (NCE) and use different views of an instance as positives that should be contrasted with other instances, called negatives, that are considered as noise. However, several instances in a dataset are drawn from the same distribution and share underlying semantic information. A good data representation should contain relations, or semantic similarity, between the instances. Contrastive learning implicitly learns relations but considering all negatives as noise harms the quality of the learned relations. To circumvent this issue, we propose a novel formulation of contrastive learning using semantic similarity between instances called Similarity Contrastive Estimation (SCE). Our training objective is a soft contrastive learning one. Instead of hard classifying positives and negatives, we estimate from one view of a batch a continuous distribution to push or pull instances based on their semantic similarities. This target similarity distribution is sharpened to eliminate noisy relations. The model predicts for each instance, from another view, the target distribution while contrasting its positive with negatives. Experimental results show that SCE is Top-1 on the ImageNet linear evaluation protocol at 100 pretraining epochs with 72.1% accuracy and is competitive with state-of-the-art algorithms by reaching 75.4% for 200 epochs with multi-crop. We also show that SCE is able to generalize to several tasks. Source code is available here: https://github.com/CEA-LIST/SCE.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51888097",
                        "name": "J. Denize"
                    },
                    {
                        "authorId": "2962220",
                        "name": "Jaonary Rabarisoa"
                    },
                    {
                        "authorId": "19258632",
                        "name": "Astrid Orcesi"
                    },
                    {
                        "authorId": "2065928462",
                        "name": "Romain H'erault"
                    },
                    {
                        "authorId": "1794818",
                        "name": "S. Canu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In contrastive learning, augmentations can be used to exploit domain-specific inductive biases, e.g., know symmetries or equivariances [Chen and Li, 2020]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b8395aae1d17bcce339bace56b6882325157a19e",
                "externalIds": {
                    "ArXiv": "2110.15288",
                    "CorpusId": 240070334
                },
                "corpusId": 240070334,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b8395aae1d17bcce339bace56b6882325157a19e",
                "title": "Hyper-Representations: Self-Supervised Representation Learning on Neural Network Weights for Model Characteristic Prediction",
                "abstract": "Self-Supervised Learning (SSL) has been shown to learn useful and information-preserving representations. Neural Networks (NNs) are widely applied, yet their weight space is still not fully understood. Therefore, we propose to use SSL to learn hyper-representations of the weights of populations of NNs. To that end, we introduce domain specific data augmentations and an adapted attention architecture. Our empirical evaluation demonstrates that self-supervised representation learning in this domain is able to recover diverse NN model characteristics. Further, we show that the proposed learned representations outperform prior work for predicting hyper-parameters, test accuracy, and generalization gap as well as transfer to out-of-distribution settings.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1663046633",
                        "name": "Konstantin Sch\u00fcrholt"
                    },
                    {
                        "authorId": "36133844",
                        "name": "Dimche Kostadinov"
                    },
                    {
                        "authorId": "1772549",
                        "name": "Damian Borth"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "To further understand why unsupervised finetuning is nontrivial, we follow the analysis in the work [8] about the contrastive loss, which represents the generalized contrastive loss in the below form:"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "97886237c4ca544ecd10419cfa3d64e571f06fc6",
                "externalIds": {
                    "ArXiv": "2110.09510",
                    "DBLP": "journals/corr/abs-2110-09510",
                    "CorpusId": 239016661
                },
                "corpusId": 239016661,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/97886237c4ca544ecd10419cfa3d64e571f06fc6",
                "title": "Unsupervised Finetuning",
                "abstract": "This paper studies\"unsupervised finetuning\", the symmetrical problem of the well-known\"supervised finetuning\". Given a pretrained model and small-scale unlabeled target data, unsupervised finetuning is to adapt the representation pretrained from the source domain to the target domain so that better transfer performance can be obtained. This problem is more challenging than the supervised counterpart, as the low data density in the small-scale target data is not friendly for unsupervised learning, leading to the damage of the pretrained representation and poor representation in the target domain. In this paper, we find the source data is crucial when shifting the finetuning paradigm from supervise to unsupervise, and propose two simple and effective strategies to combine source and target data into unsupervised finetuning:\"sparse source data replaying\", and\"data mixing\". The motivation of the former strategy is to add a small portion of source data back to occupy their pretrained representation space and help push the target data to reside in a smaller compact space; and the motivation of the latter strategy is to increase the data density and help learn more compact representation. To demonstrate the effectiveness of our proposed ``unsupervised finetuning'' strategy, we conduct extensive experiments on multiple different target datasets, which show better transfer performance than the naive strategy.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35681420",
                        "name": "Suichan Li"
                    },
                    {
                        "authorId": "49025801",
                        "name": "Dongdong Chen"
                    },
                    {
                        "authorId": "2109306087",
                        "name": "Yinpeng Chen"
                    },
                    {
                        "authorId": "145347147",
                        "name": "Lu Yuan"
                    },
                    {
                        "authorId": "2152828578",
                        "name": "Lei Zhang"
                    },
                    {
                        "authorId": "2047192315",
                        "name": "Qi Chu"
                    },
                    {
                        "authorId": "48265485",
                        "name": "B. Liu"
                    },
                    {
                        "authorId": "1708598",
                        "name": "Nenghai Yu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "We follow [5, 42] and rewrite L\u0302 in terms of explicit \u2018pull\u2018 and \u2018push\u2018 terms as :"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5781773f46d2a9012734c2d31d74008b6add0a0e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-07439",
                    "ArXiv": "2110.07439",
                    "CorpusId": 238857258
                },
                "corpusId": 238857258,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5781773f46d2a9012734c2d31d74008b6add0a0e",
                "title": "Inverse Problems Leveraging Pre-trained Contrastive Representations",
                "abstract": "We study a new family of inverse problems for recovering representations of corrupted data. We assume access to a pre-trained representation learning network R(x) that operates on clean images, like CLIP. The problem is to recover the representation of an image R(x), if we are only given a corrupted version A(x), for some known forward operator A. We propose a supervised inversion method that uses a contrastive objective to obtain excellent representations for highly corrupted images. Using a linear probe on our robust representations, we achieve a higher accuracy than end-to-end supervised baselines when classifying images with various types of distortions, including blurring, additive noise, and random pixel masking. We evaluate on a subset of ImageNet and observe that our method is robust to varying levels of distortion. Our method outperforms end-to-end baselines even with a fraction of the labeled data in a wide range of forward operators.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "102667856",
                        "name": "Sriram Ravula"
                    },
                    {
                        "authorId": "1438310376",
                        "name": "Georgios Smyrnis"
                    },
                    {
                        "authorId": "2054400150",
                        "name": "Matt Jordan"
                    },
                    {
                        "authorId": "1718469",
                        "name": "A. Dimakis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "result"
            ],
            "contexts": [
                "While their claim agrees with the large-K benefit, their bound holds only when K > C+1, and hence does not explain the empirical observation that contrastive learning works to some extent even with small K (Chen et al., 2021; Tomasev et al., 2022).",
                "7Unlike the reported results by Chen et al. (2021), smaller dimensionality, i.e., 32 gives better downstream accuracy on CIFAR-100 than 64 or 128.",
                "This inability contradicts the real experiments including Chen et al. (2021); Tomasev et al. (2022), which showed that CURL exhibits reasonable performance even with small K."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0f2aeeb62b6f0e6fd6034715bac08904a59f9ffa",
                "externalIds": {
                    "DBLP": "conf/icml/0002NN22",
                    "ArXiv": "2110.02501",
                    "CorpusId": 249191235
                },
                "corpusId": 249191235,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0f2aeeb62b6f0e6fd6034715bac08904a59f9ffa",
                "title": "On the Surrogate Gap between Contrastive and Supervised Losses",
                "abstract": "Contrastive representation learning encourages data representation to make semantically similar pairs closer than randomly drawn negative samples, which has been successful in various domains such as vision, language, and graphs. Recent theoretical studies have attempted to explain the benefit of the large negative sample size by upper-bounding the downstream classification loss with the contrastive loss. However, the previous surrogate bounds have two drawbacks: they are only legitimate for a limited range of negative sample sizes and prohibitively large even within that range. Due to these drawbacks, there still does not exist a consensus on how negative sample size theoretically correlates with downstream classification performance. Following the simplified setting where positive pairs are drawn from the true distribution (not generated by data augmentation; as supposed in previous studies), this study establishes surrogate upper and lower bounds for the downstream classification loss for all negative sample sizes that best explain the empirical observations on the negative sample size in the earlier studies. Our bounds suggest that the contrastive loss can be viewed as a surrogate objective of the downstream loss and larger negative sample sizes improve downstream classification because the surrogate gap between contrastive and supervised losses decays. We verify that our theory is consistent with experiments on synthetic, vision, and language datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2052378544",
                        "name": "Han Bao"
                    },
                    {
                        "authorId": "2714590",
                        "name": "Yoshihiro Nagano"
                    },
                    {
                        "authorId": "13613520",
                        "name": "Kento Nozawa"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Contrastive learning of is amongst the most successful self-supervised method to achieve linear classification accuracy and outperforming supervised learning tasks by suitable architectures and loss (Caron et al. 2020; Chen et al. 2020a,b; Chen and Li 2020; Zbontar et al. 2021), using pretraining in a task-agnostic fashion (Kolesnikov, Zhai, and Beyer 2019; Shen et al."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fe8c80a7eeb7ce4b0eab5dfbf45a4822e1a64bae",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-07455",
                    "ArXiv": "2109.07455",
                    "CorpusId": 237513813
                },
                "corpusId": 237513813,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fe8c80a7eeb7ce4b0eab5dfbf45a4822e1a64bae",
                "title": "Deep Bregman Divergence for Contrastive Learning of Visual Representations",
                "abstract": "Deep Bregman divergence measures divergence of data points using neural networks which is beyond Euclidean distance and capable of capturing divergence over distributions. In this paper, we propose deep Bregman divergences for contrastive learning of visual representation where we aim to enhance contrastive loss used in self-supervised learning by training additional networks based on functional Bregman divergence. In contrast to the conventional contrastive learning methods which are solely based on divergences between single points, our framework can capture the divergence between distributions which improves the quality of learned representation. We show the combination of conventional contrastive loss and our proposed divergence loss outperforms baseline and most of the previous methods for self-supervised and semi-supervised learning on multiple classifications and object detection tasks and datasets. Moreover, the learned representations generalize well when transferred to the other datasets and tasks. The source code and our models are available in supplementary and will be released with paper.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35593430",
                        "name": "Mina Rezaei"
                    },
                    {
                        "authorId": "2126960554",
                        "name": "Farzin Soleymani"
                    },
                    {
                        "authorId": "1686924",
                        "name": "B. Bischl"
                    },
                    {
                        "authorId": "40151244",
                        "name": "Shekoofeh Azizi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "970db3b8a9f506040dd5811ae08eaba3f50ef2ea",
                "externalIds": {
                    "DBLP": "journals/sensors/DlaminiZ21",
                    "PubMedCentral": "8472616",
                    "DOI": "10.3390/s21186109",
                    "CorpusId": 238200862,
                    "PubMed": "34577319"
                },
                "corpusId": 238200862,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/970db3b8a9f506040dd5811ae08eaba3f50ef2ea",
                "title": "Comparing Class-Aware and Pairwise Loss Functions for Deep Metric Learning in Wildlife Re-Identification",
                "abstract": "Similarity learning using deep convolutional neural networks has been applied extensively in solving computer vision problems. This attraction is supported by its success in one-shot and zero-shot classification applications. The advances in similarity learning are essential for smaller datasets or datasets in which few class labels exist per class such as wildlife re-identification. Improving the performance of similarity learning models comes with developing new sampling techniques and designing loss functions better suited to training similarity in neural networks. However, the impact of these advances is tested on larger datasets, with limited attention given to smaller imbalanced datasets such as those found in unique wildlife re-identification. To this end, we test the advances in loss functions for similarity learning on several animal re-identification tasks. We add two new public datasets, Nyala and Lions, to the challenge of animal re-identification. Our results are state of the art on all public datasets tested except Pandas. The achieved Top-1 Recall is 94.8% on the Zebra dataset, 72.3% on the Nyala dataset, 79.7% on the Chimps dataset and, on the Tiger dataset, it is 88.9%. For the Lion dataset, we set a new benchmark at 94.8%. We find that the best performing loss function across all datasets is generally the triplet loss; however, there is only a marginal improvement compared to the performance achieved by Proxy-NCA models. We demonstrate that no single neural network architecture combined with a loss function is best suited for all datasets, although VGG-11 may be the most robust first choice. Our results highlight the need for broader experimentation and exploration of loss functions and neural network architecture for the more challenging task, over classical benchmarks, of wildlife re-identification.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "8766698",
                        "name": "Nkosikhona Dlamini"
                    },
                    {
                        "authorId": "2504228",
                        "name": "Terence L van Zyl"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "To further understand why unsupervised pretraining has worse clustering quality than supervised pretraining, we follow [34, 9] and decouple the widely used unsupervised learning loss, i."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "00dd39b03fcbfbe89b6b5d2913fadd35dd946610",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-12369",
                    "ArXiv": "2107.12369",
                    "DOI": "10.1109/ICCV48922.2021.01004",
                    "CorpusId": 236428710
                },
                "corpusId": 236428710,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/00dd39b03fcbfbe89b6b5d2913fadd35dd946610",
                "title": "Improve Unsupervised Pretraining for Few-label Transfer",
                "abstract": "Unsupervised pretraining has achieved great success and many recent works have shown unsupervised pretraining can achieve comparable or even slightly better transfer performance than supervised pretraining on downstream target datasets. But in this paper, we find this conclusion may not hold when the target dataset has very few labeled samples for finetuning, i.e., few-label transfer. We analyze the possible reason from the clustering perspective: 1) The clustering quality of target samples is of great importance to few-label transfer; 2) Though contrastive learning is essential to learn how to cluster, its clustering quality is still inferior to supervised pretraining due to lack of label supervision. Based on the analysis, we interestingly discover that only involving some unlabeled target domain into the unsupervised pretraining can improve the clustering quality, subsequently reducing the transfer performance gap with supervised pretraining. This finding also motivates us to propose a new progressive few-label transfer algorithm for real applications, which aims to maximize the transfer performance under a limited annotation budget. To support our analysis and proposed method, we conduct extensive experiments on nine different target datasets. Experimental results show our proposed method can significantly boost the few-label transfer performance of unsupervised pretraining.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35681420",
                        "name": "Suichan Li"
                    },
                    {
                        "authorId": "49025801",
                        "name": "Dongdong Chen"
                    },
                    {
                        "authorId": "2109306087",
                        "name": "Yinpeng Chen"
                    },
                    {
                        "authorId": "145347147",
                        "name": "Lu Yuan"
                    },
                    {
                        "authorId": "2152828578",
                        "name": "Lei Zhang"
                    },
                    {
                        "authorId": "2047192315",
                        "name": "Qi Chu"
                    },
                    {
                        "authorId": "48265485",
                        "name": "B. Liu"
                    },
                    {
                        "authorId": "1708598",
                        "name": "Nenghai Yu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[34] showed that contrastive loss encourages consistent representation of augmented view and matches prior distribution."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "35d11f5e8cc0673424b352f78e162ba7c6f587a4",
                "externalIds": {
                    "MAG": "3183783103",
                    "DOI": "10.3390/APP11156783",
                    "CorpusId": 237644850
                },
                "corpusId": 237644850,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/35d11f5e8cc0673424b352f78e162ba7c6f587a4",
                "title": "Revisiting Low-Resolution Images Retrieval with Attention Mechanism and Contrastive Learning",
                "abstract": "Recent empirical works reveal that visual representation learned by deep neural networks can be successfully used as descriptors for image retrieval. A common technique is to leverage pre-trained models to learn visual descriptors by ranking losses and fine-tuning with labeled data. However, retrieval systems\u2019 performance significantly decreases when querying images of lower resolution than the training images. This study considered a contrastive learning framework fine-tuned on features extracted from a pre-trained neural network encoder equipped with an attention mechanism to address the image retrieval task for low-resolution image retrieval. Our method is simple yet effective since the contrastive learning framework drives similar samples close to each other in feature space by manipulating variants of their augmentations. To benchmark the proposed framework, we conducted quantitative and qualitative analyses of CARS196 (mAP = 0.8804), CUB200-2011 (mAP = 0.9379), and Stanford Online Products datasets (mAP = 0.9141) and analyzed their performances.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1576528478",
                        "name": "Thanh-Vu Dang"
                    },
                    {
                        "authorId": "40938215",
                        "name": "Gwanghyun Yu"
                    },
                    {
                        "authorId": "2152676439",
                        "name": "Jin Young Kim"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Several work demonstrates that mapping the representations into a unit hyperspherical space, where all embeddings are represented as unit vectors, helps to keep a smooth embedding space and brings improvement for various tasks [3, 4, 24, 35, 36].",
                "[4] further adjust the temperature and balance the influence of \u201calignment\u201d and \u201cuniformity\u201d to learn a better embedding space with contrastive training."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "03589298eb81d3da89d4f3cc8e22b227ab6a791e",
                "externalIds": {
                    "DBLP": "conf/ictir/LiLX021",
                    "ArXiv": "2107.07773",
                    "DOI": "10.1145/3471158.3472245",
                    "CorpusId": 236034464
                },
                "corpusId": 236034464,
                "publicationVenue": {
                    "id": "eac972b5-2d1a-4c5a-98a2-c1e373f09163",
                    "name": "International Conference on the Theory of Information Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Theory Inf Retr",
                        "ICTIR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1494"
                },
                "url": "https://www.semanticscholar.org/paper/03589298eb81d3da89d4f3cc8e22b227ab6a791e",
                "title": "More Robust Dense Retrieval with Contrastive Dual Learning",
                "abstract": "Dense retrieval conducts text retrieval in the embedding space and has shown many advantages compared to sparse retrieval. Existing dense retrievers optimize representations of queries and documents with contrastive training and map them to the embedding space. The embedding space is optimized by aligning the matched query-document pairs and pushing the negative documents away from the query. However, in such training paradigm, the queries are only optimized to align to the documents and are coarsely positioned, leading to an anisotropic query embedding space. In this paper, we analyze the embedding space distributions and propose an effective training paradigm, Contrastive Dual Learning for Approximate Nearest Neighbor (DANCE) to learn fine-grained query representations for dense retrieval. DANCE incorporates an additional dual training object of query retrieval, inspired by the classic information retrieval training axiom, query likelihood. With contrastive learning, the dual training object of DANCE learns more tailored representations for queries and documents to keep the embedding space smooth and uniform, thriving on the ranking performance of DANCE on the MS MARCO document retrieval task. Different from ANCE that only optimized with the document retrieval task, DANCE concentrates the query embeddings closer to document representations while making the document distribution more discriminative. Such concentrated query embedding distribution assigns more uniform negative sampling probabilities to queries and helps to sufficiently optimize query representations in the query retrieval task. Our codes are released at https://github.com/thunlp/DANCE.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2129449392",
                        "name": "Yizhi Li"
                    },
                    {
                        "authorId": "49047064",
                        "name": "Zhenghao Liu"
                    },
                    {
                        "authorId": "144628574",
                        "name": "Chenyan Xiong"
                    },
                    {
                        "authorId": "2109232579",
                        "name": "Zhiyuan Liu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "Bias towards shortcut decision rules also hampers transferability in contrastive learning [4], where it is in addition influenced by the instance discrimination task.",
                "1 shows that empirical observations of feature suppression [4] (see also Fig.",
                ", discard) certain input features [4, 11].",
                "While feature learning in contrastive learning has received less attention, recent work finds that low- and mid-level features are more important for transfer learning [55], and feature suppression can occur [4] just as with supervised learning [10, 16]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ebe510dc7b8025e496e11530192f2cccc184d002",
                "externalIds": {
                    "DBLP": "conf/nips/RobinsonSYBJS21",
                    "ArXiv": "2106.11230",
                    "CorpusId": 235489818,
                    "PubMed": "35546903"
                },
                "corpusId": 235489818,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ebe510dc7b8025e496e11530192f2cccc184d002",
                "title": "Can contrastive learning avoid shortcut solutions?",
                "abstract": "The generalization of representations learned via contrastive learning depends crucially on what features of the data are extracted. However, we observe that the contrastive loss does not always sufficiently guide which features are extracted, a behavior that can negatively impact the performance on downstream tasks via \"shortcuts\", i.e., by inadvertently suppressing important predictive features. We find that feature extraction is influenced by the difficulty of the so-called instance discrimination task (i.e., the task of discriminating pairs of similar points from pairs of dissimilar ones). Although harder pairs improve the representation of some features, the improvement comes at the cost of suppressing previously well represented features. In response, we propose implicit feature modification (IFM), a method for altering positive and negative samples in order to guide contrastive models towards capturing a wider variety of predictive features. Empirically, we observe that IFM reduces feature suppression, and as a result improves performance on vision and medical imaging tasks. The code is available at: https://github.com/joshr17/IFM.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "4594310",
                        "name": "Joshua Robinson"
                    },
                    {
                        "authorId": "2110967045",
                        "name": "Li Sun"
                    },
                    {
                        "authorId": "2114076366",
                        "name": "Ke Yu"
                    },
                    {
                        "authorId": "3443176",
                        "name": "K. Batmanghelich"
                    },
                    {
                        "authorId": "2594093",
                        "name": "S. Jegelka"
                    },
                    {
                        "authorId": "3072326",
                        "name": "S. Sra"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Later theories suggest that contrastive losses balance alignment of individual features and uniformity of the feature distribution [23], or in general alignment and some loss-defined distribution [36]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1a3d5d6cec457f8f3b088cb3636a1a197bd248f6",
                "externalIds": {
                    "ArXiv": "2106.08320",
                    "DBLP": "conf/nips/LiPSG21",
                    "CorpusId": 235436250
                },
                "corpusId": 235436250,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1a3d5d6cec457f8f3b088cb3636a1a197bd248f6",
                "title": "Self-Supervised Learning with Kernel Dependence Maximization",
                "abstract": "We approach self-supervised learning of image representations from a statistical dependence perspective, proposing Self-Supervised Learning with the Hilbert-Schmidt Independence Criterion (SSL-HSIC). SSL-HSIC maximizes dependence between representations of transformations of an image and the image identity, while minimizing the kernelized variance of those representations. This framework yields a new understanding of InfoNCE, a variational lower bound on the mutual information (MI) between different transformations. While the MI itself is known to have pathologies which can result in learning meaningless representations, its bound is much better behaved: we show that it implicitly approximates SSL-HSIC (with a slightly different regularizer). Our approach also gives us insight into BYOL, a negative-free SSL method, since SSL-HSIC similarly learns local neighborhoods of samples. SSL-HSIC allows us to directly optimize statistical dependence in time linear in the batch size, without restrictive data assumptions or indirect mutual information estimators. Trained with or without a target network, SSL-HSIC matches the current state-of-the-art for standard linear evaluation on ImageNet, semi-supervised learning and transfer to other classification and vision tasks such as semantic segmentation, depth estimation and object recognition. Code is available at https://github.com/deepmind/ssl_hsic .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2144417088",
                        "name": "Yazhe Li"
                    },
                    {
                        "authorId": "47067953",
                        "name": "Roman Pogodin"
                    },
                    {
                        "authorId": "36326783",
                        "name": "Danica J. Sutherland"
                    },
                    {
                        "authorId": "1708497",
                        "name": "A. Gretton"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "As a follow-up work, [9] even showed that when the network is trained using Lalign + \u03bbLuniform as a loss function, the weight \u03bb is inversely related to the temperature scaling \u03c4 used in contrastive loss."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ad7767d8c410b753f1d5f315991a9f5067965771",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-03259",
                    "ArXiv": "2106.03259",
                    "CorpusId": 235358780
                },
                "corpusId": 235358780,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ad7767d8c410b753f1d5f315991a9f5067965771",
                "title": "Understand and Improve Contrastive Learning Methods for Visual Representation: A Review",
                "abstract": "Traditional supervised learning methods are hitting a bottleneck because of their dependency on expensive manually labeled data and their weaknesses such as limited generalization ability and vulnerability to adversarial attacks. A promising alternative, self-supervised learning, as a type of unsupervised learning, has gained popularity because of its potential to learn effective data representations without manual labeling. Among self-supervised learning algorithms, contrastive learning has achieved state-of-the-art performance in several fields of research. This literature review aims to provide an up-to-date analysis of the efforts of researchers to understand the key components and the limitations of self-supervised learning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2112342083",
                        "name": "Ran Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "This raises an important question \u2014 which features will the augmentation choose to solve the pretext task in the presence of image samples where there are competing features? This question has been studied in [4] where it was shown that it is difficult to predict the dominant feature a method relies on when there are competing features in the augmented views."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e5784f6e9887f59864c92666e7611d88c259d6bf",
                "externalIds": {
                    "DBLP": "conf/cvpr/SahaTKP22",
                    "ArXiv": "2105.10123",
                    "DOI": "10.1109/CVPR52688.2022.01298",
                    "CorpusId": 233207068
                },
                "corpusId": 233207068,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e5784f6e9887f59864c92666e7611d88c259d6bf",
                "title": "Backdoor Attacks on Self-Supervised Learning",
                "abstract": "Large-scale unlabeled data has spurred recent progress in self-supervised learning methods that learn rich vi-sual representations. State-of-the-art self-supervised methods for learning representations from images (e.g., MoCo, BYOL, MSF) use an inductive bias that random augmentations (e.g., random crops) of an image should produce similar embeddings. We show that such methods are vulnerable to backdoor attacks - where an attacker poisons a small part of the unlabeled data by adding a trigger (image patch chosen by the attacker) to the images. The model performance is good on clean test images, but the attacker can manipulate the decision of the model by showing the trigger at test time. Backdoor attacks have been studied extensively in supervised learning and to the best of our knowledge, we are the first to study them for self-supervised learning. Backdoor attacks are more practical in self-supervised learning, since the use of large unlabeled data makes data inspection to remove poisons prohibitive. We show that in our targeted attack, the attacker can produce many false positives for the target category by using the trigger at test time. We also propose a defense method based on knowledge distillation that succeeds in neutralizing the attack. Our code is available here: https://github.com/UMBCvisionISSL-Backdoor",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2056290221",
                        "name": "Aniruddha Saha"
                    },
                    {
                        "authorId": "1471776526",
                        "name": "Ajinkya Tejankar"
                    },
                    {
                        "authorId": "2004045536",
                        "name": "Soroush Abbasi Koohpayegani"
                    },
                    {
                        "authorId": "2367683",
                        "name": "H. Pirsiavash"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "If the contrastive task is too easy, the quality of the representation suffers [4, 10].",
                "[38] examined the invariances learned, [10] showed that easily learned features can inhibit the learning of more discriminative ones, [8, 50, 60] explored the impact of different image augmentations, [10, 50] compared representations from single vs."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bfca930e7ca822ea098dab35a1fe0b624e15d17b",
                "externalIds": {
                    "ArXiv": "2105.05837",
                    "DBLP": "journals/corr/abs-2105-05837",
                    "DOI": "10.1109/CVPR52688.2022.01434",
                    "CorpusId": 234469977
                },
                "corpusId": 234469977,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bfca930e7ca822ea098dab35a1fe0b624e15d17b",
                "title": "When Does Contrastive Visual Representation Learning Work?",
                "abstract": "Recent self-supervised representation learning techniques have largely closed the gap between supervised and unsupervised learning on ImageNet classification. While the particulars of pretraining on ImageNet are now relatively well understood, the field still lacks widely accepted best practices for replicating this success on other datasets. As a first step in this direction, we study contrastive self-supervised learning on four diverse large-scale datasets. By looking through the lenses of data quantity, data domain, data quality, and task granularity, we provide new insights into the necessary conditions for successful self-supervised learning. Our key findings include observations such as: (i) the benefit of additional pretraining data beyond 500k images is modest, (ii) adding pretraining images from another domain does not lead to more general representations, (iii) corrupted pretraining images have a disparate impact on supervised and self-supervised pretraining, and (iv) contrastive learning lags far behind supervised learning on finegrained visual classification tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1630502974",
                        "name": "Elijah Cole"
                    },
                    {
                        "authorId": "3897285",
                        "name": "Xuan S. Yang"
                    },
                    {
                        "authorId": "2047941901",
                        "name": "Kimberly Wilber"
                    },
                    {
                        "authorId": "2918822",
                        "name": "Oisin Mac Aodha"
                    },
                    {
                        "authorId": "2067789287",
                        "name": "S. Belongie"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Nonetheless, recent studies in contrastive learning [124, 125] showed that the features learned through the contrastive loss, a variant of cross-entropy loss (3), exhibit properties analogous to NC."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "24c8adb79ad4ee7070fc0d48807b7f6a42148a74",
                "externalIds": {
                    "ArXiv": "2105.02375",
                    "DBLP": "conf/nips/ZhuDZLYSQ21",
                    "CorpusId": 233864730
                },
                "corpusId": 233864730,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/24c8adb79ad4ee7070fc0d48807b7f6a42148a74",
                "title": "A Geometric Analysis of Neural Collapse with Unconstrained Features",
                "abstract": "We provide the first global optimization landscape analysis of $Neural\\;Collapse$ -- an intriguing empirical phenomenon that arises in the last-layer classifiers and features of neural networks during the terminal phase of training. As recently reported by Papyan et al., this phenomenon implies that ($i$) the class means and the last-layer classifiers all collapse to the vertices of a Simplex Equiangular Tight Frame (ETF) up to scaling, and ($ii$) cross-example within-class variability of last-layer activations collapses to zero. We study the problem based on a simplified $unconstrained\\;feature\\;model$, which isolates the topmost layers from the classifier of the neural network. In this context, we show that the classical cross-entropy loss with weight decay has a benign global landscape, in the sense that the only global minimizers are the Simplex ETFs while all other critical points are strict saddles whose Hessian exhibit negative curvature directions. In contrast to existing landscape analysis for deep neural networks which is often disconnected from practice, our analysis of the simplified model not only does it explain what kind of features are learned in the last layer, but it also shows why they can be efficiently optimized in the simplified settings, matching the empirical observations in practical deep network architectures. These findings could have profound implications for optimization, generalization, and robustness of broad interests. For example, our experiments demonstrate that one may set the feature dimension equal to the number of classes and fix the last-layer classifier to be a Simplex ETF for network training, which reduces memory cost by over $20\\%$ on ResNet18 without sacrificing the generalization performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145687539",
                        "name": "Zhihui Zhu"
                    },
                    {
                        "authorId": "1937503",
                        "name": "Tianyu Ding"
                    },
                    {
                        "authorId": "46618471",
                        "name": "Jinxin Zhou"
                    },
                    {
                        "authorId": "2108785656",
                        "name": "Xiao Li"
                    },
                    {
                        "authorId": "1878841",
                        "name": "Chong You"
                    },
                    {
                        "authorId": "2714145",
                        "name": "Jeremias Sulam"
                    },
                    {
                        "authorId": "144403436",
                        "name": "Qing Qu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The study of contrastive losses has shown that this repulsion effect between dissimilar views is matching the distribution of features in representational space to a distribution of high entropy [7], in otherwords, encouraging uniformity of representations in space [8].",
                "Additionally, [7] extends this work proposing a generic form of the contrastive loss, also identifying the same relations of uniformity to pairwise potential in a Gaussian kernel, tomatch representations to a prior distribution (of high entropy)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2315c5caf636f960755a24a70e7da32d3bc0a385",
                "externalIds": {
                    "DBLP": "journals/ivc/DurrantL22",
                    "ArXiv": "2105.00925",
                    "DOI": "10.1016/j.imavis.2022.104494",
                    "CorpusId": 247761966
                },
                "corpusId": 247761966,
                "publicationVenue": {
                    "id": "6cc36eeb-d056-42c4-a306-7bcb239cc442",
                    "name": "Image and Vision Computing",
                    "type": "journal",
                    "alternate_names": [
                        "Image Vis Comput"
                    ],
                    "issn": "0262-8856",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525443/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/02628856",
                        "https://www.journals.elsevier.com/image-and-vision-computing"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2315c5caf636f960755a24a70e7da32d3bc0a385",
                "title": "Hyperspherically regularized networks for self-supervision",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1392810833",
                        "name": "A. Durrant"
                    },
                    {
                        "authorId": "2042765174",
                        "name": "G. Leontidis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Reconstruction can help in capturing features that are suppressed by contrastive learning/clustering [14]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d9b1bb8053f32c6da9bbbec564d750d55b486f00",
                "externalIds": {
                    "ArXiv": "2104.12671",
                    "DBLP": "journals/corr/abs-2104-12671",
                    "DOI": "10.1109/ICCV48922.2021.00791",
                    "CorpusId": 233393800
                },
                "corpusId": 233393800,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/d9b1bb8053f32c6da9bbbec564d750d55b486f00",
                "title": "Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos",
                "abstract": "Multimodal self-supervised learning is getting more and more attention as it allows not only to train large networks without human supervision but also to search and retrieve data across various modalities. In this context, this paper proposes a framework that, starting from a pre-trained backbone, learns a common multimodal embedding space that, in addition to sharing representations across different modalities, enforces a grouping of semantically similar instances. To this end, we extend the concept of instance-level contrastive learning with a multimodal clustering step in the training pipeline to capture semantic similarities across modalities. The resulting embedding space enables retrieval of samples across all modalities, even from unseen datasets and different domains. To evaluate our approach, we train our model on the HowTo100M dataset and evaluate its zero-shot retrieval capabilities in two challenging domains, namely text-to-video retrieval, and temporal action localization, showing state-of-the-art results on four different datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108342501",
                        "name": "Brian Chen"
                    },
                    {
                        "authorId": "41020711",
                        "name": "Andrew Rouditchenko"
                    },
                    {
                        "authorId": "2064921899",
                        "name": "Kevin Duarte"
                    },
                    {
                        "authorId": "2077580009",
                        "name": "Hilde Kuehne"
                    },
                    {
                        "authorId": "152809214",
                        "name": "Samuel Thomas"
                    },
                    {
                        "authorId": "1394839535",
                        "name": "Angie Boggust"
                    },
                    {
                        "authorId": "1819152",
                        "name": "R. Panda"
                    },
                    {
                        "authorId": "144707379",
                        "name": "Brian Kingsbury"
                    },
                    {
                        "authorId": "1723233",
                        "name": "R. Feris"
                    },
                    {
                        "authorId": "30507748",
                        "name": "David F. Harwath"
                    },
                    {
                        "authorId": "145898106",
                        "name": "James R. Glass"
                    },
                    {
                        "authorId": "1774515",
                        "name": "M. Picheny"
                    },
                    {
                        "authorId": "9546964",
                        "name": "Shih-Fu Chang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "This limitation occurs because CSI uses a hypersphere embedding space that is uniformly distributed [7, 40]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5484999008a0ab9a5adae33dae0a91df25aa0504",
                "externalIds": {
                    "MAG": "3159581073",
                    "ArXiv": "2103.15296",
                    "DBLP": "conf/bmvc/0001S0PC21",
                    "CorpusId": 232404517
                },
                "corpusId": 232404517,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/5484999008a0ab9a5adae33dae0a91df25aa0504",
                "title": "Elsa: Energy-based Learning for Semi-supervised Anomaly Detection",
                "abstract": "Anomaly detection aims at identifying deviant instances from the normal data distribution. Many advances have been made in the field, including the innovative use of unsupervised contrastive learning. However, existing methods generally assume clean training data and are limited when the data contain unknown anomalies. This paper presents Elsa, a novel semi-supervised anomaly detection approach that unifies the concept of energy-based models with unsupervised contrastive learning. Elsa instills robustness against any data contamination by a carefully designed fine-tuning step based on the new energy function that forces the normal data to be divided into classes of prototypes. Experiments on multiple contamination scenarios show the proposed model achieves SOTA performance. Extensive analyses also verify the contribution of each component in the proposed model. Beyond the experiments, we also offer a theoretical interpretation of why contrastive learning alone cannot detect anomalies under data contamination.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109776478",
                        "name": "Sungwon Han"
                    },
                    {
                        "authorId": "1645918180",
                        "name": "Hyeonho Song"
                    },
                    {
                        "authorId": "2116971826",
                        "name": "Seungeon Lee"
                    },
                    {
                        "authorId": "2109454735",
                        "name": "Sungwon Park"
                    },
                    {
                        "authorId": "1775511",
                        "name": "M. Cha"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "According to [10], increasing the temperature coefficient concentate the network on the dissimilarity be-"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6c02c407c6f3bf2293d6283ea3be27490ca2152a",
                "externalIds": {
                    "ArXiv": "2103.12371",
                    "DBLP": "journals/corr/abs-2103-12371",
                    "CorpusId": 232320621
                },
                "corpusId": 232320621,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6c02c407c6f3bf2293d6283ea3be27490ca2152a",
                "title": "Unsupervised domain adaptation via coarse-to-fine feature alignment method using contrastive learning",
                "abstract": "Previous feature alignment methods in Unsupervised domain adaptation(UDA) mostly only align global features without considering the mismatch between class-wise features. In this work, we propose a new coarse-to-fine feature alignment method using contrastive learning called CFContra. It draws class-wise features closer than coarse feature alignment or class-wise feature alignment only, therefore improves the model's performance to a great extent. We build it upon one of the most effective methods of UDA called entropy minimization to further improve performance. In particular, to prevent excessive memory occupation when applying contrastive loss in semantic segmentation, we devise a new way to build and update the memory bank. In this way, we make the algorithm more efficient and viable with limited memory. Extensive experiments show the effectiveness of our method and model trained on the GTA5 to Cityscapes dataset has boost mIOU by 3.5 compared to the MinEnt algorithm. Our code will be publicly available.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118811789",
                        "name": "Shiyu Tang"
                    },
                    {
                        "authorId": "1994582479",
                        "name": "Peijun Tang"
                    },
                    {
                        "authorId": "50623362",
                        "name": "Yanxiang Gong"
                    },
                    {
                        "authorId": "145445613",
                        "name": "Zheng Ma"
                    },
                    {
                        "authorId": "144917416",
                        "name": "M. Xie"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Past work addresses this problem by designing handcrafted data augmentations that eliminate the irrelevant features, so that the network may learn the relevant information [24, 5, 6, 8, 7]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4995b33c1e80b95b71624e7802e48bdcd3ffd01b",
                "externalIds": {
                    "DBLP": "conf/wacv/LiFYHTFIK23",
                    "ArXiv": "2012.09962",
                    "DOI": "10.1109/WACV56688.2023.00146",
                    "CorpusId": 244728347
                },
                "corpusId": 244728347,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/4995b33c1e80b95b71624e7802e48bdcd3ffd01b",
                "title": "Addressing Feature Suppression in Unsupervised Visual Representations",
                "abstract": "Contrastive learning is one of the fastest growing research areas in machine learning due to its ability to learn useful representations without labeled data. However, contrastive learning is susceptible to feature suppression \u2013 i.e., it may discard important information relevant to the task of interest, and learn irrelevant features. Past work has addressed this limitation via handcrafted data augmentations that eliminate irrelevant information. This approach however does not work across all datasets and tasks. Further, data augmentations fail in addressing feature suppression in multi-attribute classification when one attribute can suppress features relevant to other attributes. In this paper, we analyze the objective function of contrastive learning and formally prove that it is vulnerable to feature suppression. We then present Predictive Contrastive Learning (PrCL), a framework for learning unsupervised representations that are robust to feature suppression. The key idea is to force the learned representation to predict the input, and hence prevent it from discarding important information. Extensive experiments verify that PrCL is robust to feature suppression and outperforms state-of-the-art contrastive learning methods on a variety of datasets and tasks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2118909373",
                        "name": "Tianhong Li"
                    },
                    {
                        "authorId": "2548303",
                        "name": "Lijie Fan"
                    },
                    {
                        "authorId": "46499812",
                        "name": "Yuan Yuan"
                    },
                    {
                        "authorId": "153168451",
                        "name": "Hao He"
                    },
                    {
                        "authorId": "2476765",
                        "name": "Yonglong Tian"
                    },
                    {
                        "authorId": "1723233",
                        "name": "R. Feris"
                    },
                    {
                        "authorId": "1688317",
                        "name": "P. Indyk"
                    },
                    {
                        "authorId": "1785714",
                        "name": "D. Katabi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "task with pre-trained ResNet-50 and ResNet-152 weights. Temperature Hyperparameter. The temperature scaling hyperparameter is known to play a signi\ufb01cant role in the quality of the simCLR pre-training [8, 9, 10]. It motivates us to investigate the impact of the temperature scaling factor on the transferability of pre-training winning tickets found in Section4. Without loss of the generality, we consider the "
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5f6fccc32953f57fe29b2316eb8351e84b0179dc",
                "externalIds": {
                    "MAG": "3111921445",
                    "DBLP": "journals/corr/abs-2012-06908",
                    "ArXiv": "2012.06908",
                    "DOI": "10.1109/CVPR46437.2021.01604",
                    "CorpusId": 229152261
                },
                "corpusId": 229152261,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5f6fccc32953f57fe29b2316eb8351e84b0179dc",
                "title": "The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models",
                "abstract": "The computer vision world has been re-gaining enthusiasm in various pre-trained models, including both classical ImageNet supervised pre-training and recently emerged self-supervised pre-training such as simCLR [10] and MoCo [40]. Pre-trained weights often boost a wide range of downstream tasks including classification, detection, and segmentation. Latest studies suggest that pre-training benefits from gigantic model capacity [11]. We are hereby curious and ask: after pre-training, does a pre-trained model indeed have to stay large for its downstream transferability? In this paper, we examine supervised and self-supervised pre-trained models through the lens of the lottery ticket hypothesis (LTH) [31]. LTH identifies highly sparse matching subnetworks that can be trained in isolation from (nearly) scratch yet still reach the full models' performance. We extend the scope of LTH and question whether matching subnetworks still exist in pre-trained computer vision models, that enjoy the same downstream transfer performance. Our extensive experiments convey an overall positive message: from all pre-trained weights obtained by ImageNet classification, simCLR, and MoCo, we are consistently able to locate such matching subnetworks at 59.04% to 96.48% sparsity that transfer universally to multiple downstream tasks, whose performance see no degradation compared to using full pre-trained weights. Further analyses reveal that subnetworks found from different pre-training tend to yield diverse mask structures and perturbation sensitivities. We conclude that the core LTH observations remain generally relevant in the pre-training paradigm of computer vision, but more delicate discussions are needed in some cases. Codes and pre-trained models will be made available at: https://github.com/VITA-Group/CV_LTH_Pre-training.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2648459",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "25581960",
                        "name": "Jonathan Frankle"
                    },
                    {
                        "authorId": "3307026",
                        "name": "Shiyu Chang"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "37873860",
                        "name": "Yang Zhang"
                    },
                    {
                        "authorId": "1701041",
                        "name": "Michael Carbin"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The alignment part encourages representations of positive pairs to be similar whereas the distribution part \u2018\u2018encourages representations to match a prior distribution\u2019\u2019 [58].",
                "family of loss functions with an alignment and a distribution part [58]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a0d6bbd1c2cc035710498a3a5124b1337d8d8e7f",
                "externalIds": {
                    "DBLP": "journals/access/SchmarjeSSK21",
                    "ArXiv": "2002.08721",
                    "DOI": "10.1109/ACCESS.2021.3084358",
                    "CorpusId": 235187536
                },
                "corpusId": 235187536,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a0d6bbd1c2cc035710498a3a5124b1337d8d8e7f",
                "title": "A Survey on Semi-, Self- and Unsupervised Learning for Image Classification",
                "abstract": "While deep learning strategies achieve outstanding results in computer vision tasks, one issue remains: The current strategies rely heavily on a huge amount of labeled data. In many real-world problems, it is not feasible to create such an amount of labeled training data. Therefore, it is common to incorporate unlabeled data into the training process to reach equal results with fewer labels. Due to a lot of concurrent research, it is difficult to keep track of recent developments. In this survey, we provide an overview of often used ideas and methods in image classification with fewer labels. We compare 34 methods in detail based on their performance and their commonly used ideas rather than a fine-grained taxonomy. In our analysis, we identify three major trends that lead to future research opportunities. 1. State-of-the-art methods are scalable to real-world applications in theory but issues like class imbalance, robustness, or fuzzy labels are not considered. 2. The degree of supervision which is needed to achieve comparable results to the usage of all labels is decreasing and therefore methods need to be extended to settings with a variable number of classes. 3. All methods share some common ideas but we identify clusters of methods that do not share many ideas. We show that combining ideas from different clusters can lead to better performance.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "15269079",
                        "name": "Lars Schmarje"
                    },
                    {
                        "authorId": "1500374013",
                        "name": "M. Santarossa"
                    },
                    {
                        "authorId": "1832847402",
                        "name": "Simon-Martin Schroder"
                    },
                    {
                        "authorId": "144839904",
                        "name": "R. Koch"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "This policy requires the model to match each instance\u2019s embedding into the predefined prior distribution with high entropy (Chen and Li, 2020; Wang and Isola, 2020)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b68f430419a55796d4fb501e7c0201dc45f575a1",
                "externalIds": {
                    "DBLP": "conf/eacl/HanSPJC23",
                    "ACL": "2023.eacl-main.132",
                    "DOI": "10.18653/v1/2023.eacl-main.132",
                    "CorpusId": 258378163
                },
                "corpusId": 258378163,
                "publicationVenue": {
                    "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
                    "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Eur Chapter Assoc Comput Linguistics",
                        "EACL"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/eacl/"
                },
                "url": "https://www.semanticscholar.org/paper/b68f430419a55796d4fb501e7c0201dc45f575a1",
                "title": "Unified Neural Topic Model via Contrastive Learning and Term Weighting",
                "abstract": "Two types of topic modeling predominate: generative methods that employ probabilistic latent models and clustering methods that identify semantically coherent groups. This paper newly presents UTopic (Unified neural Topic model via contrastive learning and term weighting) that combines the advantages of these two types. UTopic uses contrastive learning and term weighting to learn knowledge from a pretrained language model and discover influential terms from semantically coherent clusters. Experiments show that the generated topics have a high-quality topic-word distribution in terms of topic coherence, outperforming existing baselines across multiple topic coherence measures. We demonstrate how our model can be used as an add-on to existing topic models and improve their performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1444699639",
                        "name": "Sungwon Han"
                    },
                    {
                        "authorId": "134473999",
                        "name": "Mingi Shin"
                    },
                    {
                        "authorId": "2115278236",
                        "name": "S. Park"
                    },
                    {
                        "authorId": "2019095132",
                        "name": "Chang-Ryong Jung"
                    },
                    {
                        "authorId": "2166262174",
                        "name": "Meeyoung Cha"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Additionally, contrastive methods can perform poorly when the task of interest is not determined by the dominant features of an image [Chen et al., 2021].",
                "If the task of interest is not determined by the dominant features of an image, or is obscured by the transformations used to train the model, the self-supervised model may perform poorly at clustering the data into relevant groups [Chen et al., 2021]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fe4df3a13540c2bc329cdb59ccc487e4a11406c8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-10071",
                    "DOI": "10.48550/arXiv.2305.10071",
                    "CorpusId": 258740808
                },
                "corpusId": 258740808,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fe4df3a13540c2bc329cdb59ccc487e4a11406c8",
                "title": "Cold PAWS: Unsupervised class discovery and the cold-start problem",
                "abstract": "In many machine learning applications, labeling datasets can be an arduous and time-consuming task. Although research has shown that semi-supervised learning techniques can achieve high accuracy with very few labels within the \ufb01eld of computer vision, little attention has been given to how images within a dataset should be selected for labeling. In this paper, we propose a novel approach based on well-established self-supervised learning, clustering, and manifold learning techniques that address this challenge of selecting an informative image subset to label in the \ufb01rst instance, which is known as the cold-start or unsupervised selective labelling problem. We test our approach using several publicly available datasets, namely CIFAR10, Imagenette, DeepWeeds, and EuroSAT, and observe improved performance with both supervised and semi-supervised learning strategies when our label selection strategy is used, in comparison to random sampling. We also obtain superior performance for the datasets considered with a much simpler approach compared to other methods in the literature.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2217684532",
                        "name": "Evelyn J. Mannix"
                    },
                    {
                        "authorId": "2821071",
                        "name": "H. Bondell"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[26] find three interesting properties of contrastive learning."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bae81c854da2a465b124fcfe823e66ebddd83ea0",
                "externalIds": {
                    "DBLP": "journals/tgrs/HuangDDS23",
                    "DOI": "10.1109/TGRS.2023.3275644",
                    "CorpusId": 258752751
                },
                "corpusId": 258752751,
                "publicationVenue": {
                    "id": "70628d6a-97aa-4571-9701-bc0eb3989c32",
                    "name": "IEEE Transactions on Geoscience and Remote Sensing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Geosci Remote Sens"
                    ],
                    "issn": "0196-2892",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=36",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bae81c854da2a465b124fcfe823e66ebddd83ea0",
                "title": "Supervised Contrastive Learning Based on Fusion of Global and Local Features for Remote Sensing Image Retrieval",
                "abstract": "With the rapid development of remote sensing sensor technology, the number of remote sensing images (RSIs) has exploded. How to effectively retrieve and manage this massive data have become an urgent problem. At present, content-based image retrieval (CBIR) methods have become a mainstream method due to their excellent performance. However, most of the existing retrieval methods only consider the global features of images, which lacks the ability to discriminate images with the same semantic information but different visual representations. To alleviate this issue, supervised contrastive learning based on the fusion of global and local features method is proposed in this article, named SCFR. First, a fusion module is designed to combine global and local features to enhance the ability of image expression. Second, supervised contrastive learning is introduced into the retrieval task to effectively improve the feature distribution, so that the positive sample pairs are close to each other, and the negative sample pairs are far away from each other in the feature space. Furthermore, to make the distribution of features of the same class more compact, the center contrastive loss is added to the constraints and combines the class centers that change iteratively with the network. Experimental results on three RSI datasets show that our proposed method has a more effective retrieval performance than the state-of-the-art methods. The code and models are available at https://github.com/xdplay17/SCFR.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2216041623",
                        "name": "Mengluan Huang"
                    },
                    {
                        "authorId": "2109479977",
                        "name": "L. Dong"
                    },
                    {
                        "authorId": "2872774",
                        "name": "W. Dong"
                    },
                    {
                        "authorId": "143713952",
                        "name": "Guangming Shi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Dynamicmargin can push boundaries between neighbor classes, they become more separately as mentioned in [56]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "611907201ae5e15edc05db4b586b444938ac4b5d",
                "externalIds": {
                    "DBLP": "journals/access/SoLKOS23",
                    "DOI": "10.1109/ACCESS.2023.3286931",
                    "CorpusId": 259235619
                },
                "corpusId": 259235619,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/611907201ae5e15edc05db4b586b444938ac4b5d",
                "title": "Robust Contrastive Learning With Dynamic Mixed Margin",
                "abstract": "One of the promising ways for the representation learning is contrastive learning. It enforces that positive pairs become close while negative pairs become far. Contrastive learning utilizes the relative proximity or distance between positive and negative pairs. However, contrastive learning might fail to handle the easily distinguished positive-negative pairs because the gradient of easily divided positive-negative pairs comes to vanish. To overcome the problem, we propose a dynamic mixed margin (DMM) loss that generates the augmented hard positive-negative pairs that are not easily clarified. DMM generates hard positive-negative pairs by interpolating the dataset with Mixup. Besides, DMM adopts the dynamic margin incorporating the interpolation ratio, and dynamic adaptation improves representation learning. DMM encourages making close for positive pairs far away, whereas making a little far for strongly nearby positive pairs alleviates overfitting. Our proposed DMM is a plug-and-play module compatible with diverse contrastive learning loss and metric learning. We validate that the DMM is superior to other baselines on various tasks, video-text retrieval, and recommender system task in unimodal and multimodal settings. Besides, representation learned from DMM shows better robustness even if the modality missing occurs that frequently appears on the real-world dataset. Implementation of DMM at downstream tasks is available here: https://github.com/teang1995/DMM",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2127769429",
                        "name": "Junhyuk So"
                    },
                    {
                        "authorId": "2188272447",
                        "name": "Yongtaek Lim"
                    },
                    {
                        "authorId": "2220588090",
                        "name": "Yewon Kim"
                    },
                    {
                        "authorId": "2152046120",
                        "name": "Changdae Oh"
                    },
                    {
                        "authorId": "2490092",
                        "name": "Kyungwoo Song"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "13de028c5e66c67410bd7d8f5deec0f0673e90d4",
                "externalIds": {
                    "CorpusId": 259905528
                },
                "corpusId": 259905528,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/13de028c5e66c67410bd7d8f5deec0f0673e90d4",
                "title": "W HAT C ONTRASTIVE L EARNING L EARNS B EYOND C LASS - WISE F EATURES ?",
                "abstract": "In recent years, contrastive learning has achieved the performance that is comparable to supervised learning in representation learning. However, the transferability of different contrastive learning methods to downstream tasks often varies greatly. In this paper, we study the downstream generalization ability of two contrastive learning methods: SimCLR and Spectral Contrastive Learning (Spectral CL). We find that beyond class-wise features, contrastive learning also learns two types of features, which we call shared features and subclass features, which play an important role in model transferability. SimCLR learns more shared and subclass features than Spectral CL, resulting in better transferability. We theoretically and experimentally reveal the mechanism by which SimCLR can learn more diverse features than Spectral CL. Therefore, we propose a method called High-pass Spectral CL to improve the transferability and generalization of Spectral CL, which achieves better performance than SimCLR and Spectral CL.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2223366527",
                        "name": "Xingyuming Liu"
                    },
                    {
                        "authorId": "2115568564",
                        "name": "Yifei Wang"
                    },
                    {
                        "authorId": "2115869684",
                        "name": "Yisen Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "Previous investigations [10] have shown that a few easy-to-learn irrelevant features not removed by augmentations can prevent the model from learning all semantic features inside images.",
                "We provide a solution to the feature suppression issue in CL [10] and also demonstrate SOTA results with weaker augmentations on visual benchmarks.",
                "g medical imaging [17] or multi-objects images [10]) is still an open question.",
                "Specifically, dominant objects inside images can prevent the model from learning features of smaller objects [10] (which is not apparent in object-centric datasets such as ImageNet) and few, irrelevant and easy-to-learn features, that are shared among views, are sufficient to collapse the representation [10] (a.",
                "[10] Ting Chen, Calvin Luo, and Lala Li.",
                "As noted previously [10], \u03b2-VAE is the only method insensitive to the number of added bits, but its representation quality remains low compared to other discriminative approaches."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "782cea5d726595648e4296ed26ffa6a6bf18b899",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-01646",
                    "DOI": "10.48550/arXiv.2206.01646",
                    "CorpusId": 249375520
                },
                "corpusId": 249375520,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/782cea5d726595648e4296ed26ffa6a6bf18b899",
                "title": "Rethinking Positive Sampling for Contrastive Learning with Kernel",
                "abstract": "Data augmentation is a crucial component in unsupervised contrastive learning (CL). It determines how positive samples are de\ufb01ned and, ultimately, the quality of the representation. While ef\ufb01cient augmentations have been found for standard vision datasets, such as ImageNet, it is still an open problem in other applications, such as medical imaging, or in datasets with easy-to-learn but irrelevant imaging features. In this work, we propose a new way to de\ufb01ne positive samples using kernel theory along with a novel loss called decoupled uniformity . We propose to integrate prior information, learnt from generative models or given as auxiliary attributes, into contrastive learning, to make it less dependent on data augmentation. We draw a connection between contrastive learning and the conditional mean embedding theory to derive tight bounds on the downstream classi\ufb01cation loss. In an unsupervised setting, we empirically demonstrate that CL bene\ufb01ts from generative models, such as VAE and GAN, to less rely on data augmentations. We validate our framework on vision datasets including CIFAR10, CIFAR100, STL10 and ImageNet100 and a brain MRI dataset. In the weakly supervised setting, we demonstrate that our formulation provides state-of-the-art results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "89653117",
                        "name": "Benoit Dufumier"
                    },
                    {
                        "authorId": "12691936",
                        "name": "C. Barbano"
                    },
                    {
                        "authorId": "2117586389",
                        "name": "Robin Louiset"
                    },
                    {
                        "authorId": "1710398",
                        "name": "E. Duchesnay"
                    },
                    {
                        "authorId": "1742163277",
                        "name": "P. Gori"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "[7] and extended to a wider set of prior distributions (e.",
                "Given MI(x,y) = H(x)\u2212H(x|y), the two right-hand side terms can be linked to the following two properties [7, 31]: \u2217 Uniformity H(x): Maximizing entropy leads to uniformly distributed latent vectors.",
                "1) In this work, we investigate the dense feature representation in terms of alignment and uniformity inspired by the pioneering analyses of [7, 31].",
                "To understand the semantic structures and behavior of this method, a few recent studies [7, 31] analyzed the latent space (e.",
                "In contrast to numerous theoretical [1, 13, 20, 28, 29] and empirical analyses [7, 19, 22, 27, 31, 36, 39] to understand instance-level CL, no attempt has been made to understand dense CL."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "571d4f024f98e0b4917d4033bb338940c926696d",
                "externalIds": {
                    "CorpusId": 252918494
                },
                "corpusId": 252918494,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/571d4f024f98e0b4917d4033bb338940c926696d",
                "title": "Overleaf Example",
                "abstract": "Recently, dense contrastive learning has shown superior performance on dense prediction tasks compared to instance-level contrastive learning. Despite its supremacy, the properties of dense contrastive representations have not yet been carefully studied. Therefore, we analyze the theoretical ideas of dense contrastive learning using a standard CNN and straightforward feature matching scheme rather than propose a new complex method. Inspired by the analysis of the properties of instance-level contrastive representations through the lens of alignment and uniformity on the hypersphere, we employ and extend the same lens for the dense contrastive representations to analyze their underexplored properties. We discover the core principle in constructing a positive pair of dense features and empirically proved its validity. Also, we introduces a new scalar metric that summarizes the correlation between alignment-and-uniformity and downstream performance. Using this metric, we study various facets of densely learned contrastive representations such as how the correlation changes over singleand multi-object datasets or linear evaluation and dense prediction tasks. The source code is publicly available at: https://github.com/SuperSupermoon/DenseCL-analysis",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1992706900",
                        "name": "J. Moon"
                    },
                    {
                        "authorId": "2382193",
                        "name": "Wonjae Kim"
                    },
                    {
                        "authorId": "3242613",
                        "name": "E. Choi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026)\n= \u2212 1\nN\n\u2211\ni,j\u2208MB\nsim(zi, zj)/\u03c4\n\ufe38 \ufe37\ufe37 \ufe38\nLalignment\n+ 1\nN\nN\u2211\ni\nlog ( 2N\u2211\nk=1\n1[k 6=i]exp(sim(zi, zk)/\u03c4)\n\ufe38 \ufe37\ufe37 \ufe38\nLdistribution\n) (3)\nand as such there are two parts of the loss, namely, alignment and distribution, as proposed by [Wang and Isola, 2020] and further studied by [Chen et al., 2020b].",
                "Here, the authors propose the temperature scaled loss NT-Xent which is based on noise contrastive estimation and cross entropy [Chen et al., 2020a].",
                "Equation (2) can be rewritten accordingly by applying the logarithmic rules\nLNT\u2212Xent = \u2212 1\nN\n\u2211\ni,j\u2208MB\n(\nsim(zi, zj)/\u03c4 \u2212 log\n2N\u2211\nk=1\n1[k 6=i]exp(sim(zi, zk)/\u03c4) )\n= \u2212 1\nN\n\u2211\ni,j\u2208MB\nsim(zi, zj)/\u03c4\n\ufe38 \ufe37\ufe37 \ufe38\nLalignment\n+ 1\nN\nN\u2211\ni\nlog ( 2N\u2211\nk=1\n1[k 6=i]exp(sim(zi, zk)/\u03c4)\n\ufe38 \ufe37\ufe37 \ufe38\nLdistribution\n) (3)\nand as such there are two parts of the loss, namely, alignment and distribution, as proposed by [Wang and Isola, 2020] and further studied by [Chen et al., 2020b].",
                "Self-supervised learning models and frameworks like CPC[van den Oord et al., 2019], SimCLR [Chen et al., 2020a], BYOL [Grill et al., 2020], and SimSIAM [Chen and He, 2020] show promising results in low-labeled data regimes, with representations generalizing well in downstream tasks like visual\u2026",
                "The simple framework for contrastive learning on visual representations (SimCLR) proposed by [Chen et al., 2020a] builds upon properties of noise contrastive estimation (NCE) and is inspired by previous work in the related field."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "57f65b61007ddba5dcc450ad8391aee6b10c19f3",
                "externalIds": {
                    "CorpusId": 248566660
                },
                "corpusId": 248566660,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/57f65b61007ddba5dcc450ad8391aee6b10c19f3",
                "title": "T HE NT-X ENT LOSS UPPER BOUND A N UPPER BOUND FOR AVERAGE SIMILARITY",
                "abstract": "Self-supervised learning is a growing paradigm in deep representation learning, showing great generalization capabilities and competitive performance in low-labeled data regimes. The SimCLR framework proposes the NT-Xent loss for contrastive representation learning. The objective of the loss function is to maximize agreement, similarity, between sampled positive pairs. This short paper derives and proposes an upper bound for the loss and average similarity. An analysis of the implications is however not provided, but we strongly encourage anyone in the \ufb01eld to conduct this.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2164905826",
                        "name": "Wilhelm \u00c5gren"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "Chen et al. (2021a) identifies three intriguing properties of contrastive learning: a generalized version of the loss, learning with the presence of multiple objects, and feature suppression induced by competing augmentations.",
                "For transformer, we leverage pre-trained models on ImageNet (Deng et al., 2009) from ViT (Dosovitskiy et al., 2021), DeiT (Touvron et al., 2021), DINO (Caron et al., 2021), MoCo-v3 (Chen et al., 2021b), and MAE (He et al., 2022).",
                "Specifically, we leverage MoCo-v3 (Chen et al., 2021b), the ViT version of MoCo, and Supervised ViT.",
                "We find that the MoCo-v3 degradation is larger with patch shuffling, but smaller with gamma distortion.",
                "To prevent trivial solution, contrastive learning pushes negative examples apart (MoCo (He et al., 2020; Chen et al., 2020d; 2021b), SimCLR (Chen et al., 2020a;b)), makes use of stop-gradient operation or asymmetric predictor without using negatives (SimSiam (Chen & He, 2021), BYOL (Grill et al.,\u2026"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "64458003d1c637fedd8ef518986397091bc658aa",
                "externalIds": {
                    "CorpusId": 252760543
                },
                "corpusId": 252760543,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/64458003d1c637fedd8ef518986397091bc658aa",
                "title": "Is Self-Supervised Contrastive Learning More Robust Than Supervised Learning?",
                "abstract": "Self-supervised contrastive learning is a power-ful tool to learn visual representation without labels. Prior work has primarily focused on the recognition accuracy of contrastive pre-training algorithms, but has overlooked other behavioral aspects. In addition to accuracy, distributional robustness plays a critical role in the reliability of machine learning models. We design and conduct a series of robustness tests to quantify the behavioral differences between contrastive learning and supervised learning. These tests leverage data corruptions at multiple levels, ranging from pixel-level gamma distortion to patch-level shuffling and to dataset-level distribution shift. Our tests un-veil intriguing robustness behaviors of contrastive and supervised learning. On the one hand, under downstream corruptions, we generally observe that contrastive learning is surprisingly more robust than supervised learning. On the other hand, under pre-training corruptions, we find contrastive learning vulnerable to patch shuffling and pixel intensity change, yet less sensitive to dataset-level distribution change. We attempt to explain these results through the role of data augmentation and feature space properties. Our insight has implica-tions in improving the downstream robustness of supervised learning. corrupted downstream tasks. For comparable results, we fix the data augmentation for all settings that involve training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8802368",
                        "name": "Yuanyi Zhong"
                    },
                    {
                        "authorId": "4990833",
                        "name": "Haoran Tang"
                    },
                    {
                        "authorId": "47739808",
                        "name": "Junkun Chen"
                    },
                    {
                        "authorId": "2122806966",
                        "name": "Jian Peng"
                    },
                    {
                        "authorId": "2302062",
                        "name": "Yu-Xiong Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026that capture feature compositionality are more favorable than solutions that capture a single dominant feature under strong augmentation, addressing empirical puzzles (Chen et al., 2021; Tian et al., 2020b) that strong data augmentation seems to be the key for self-supervised learning to work.",
                "This is consistent with the observation in (Chen et al., 2021; Tian et al., 2020b) where the amount of augmentation leads to different learned features, and one feature may overwhelm the other.",
                "In the case of multiple hidden nodes with disjoint receptive fields, we prove that weight solutions that capture feature compositionality are more favorable than solutions that capture a single dominant feature under strong augmentation, addressing empirical puzzles (Chen et al., 2021; Tian et al., 2020b) that strong data augmentation seems to be the key for self-supervised learning to work."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4227124378ae28bea188bdc50e55f0789c86c715",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-12680",
                    "CorpusId": 246430905
                },
                "corpusId": 246430905,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4227124378ae28bea188bdc50e55f0789c86c715",
                "title": "Deep Contrastive Learning is Provably (almost) Principal Component Analysis",
                "abstract": "We show that Contrastive Learning (CL) under a family of loss functions (including InfoNCE) has a game-theoretical formulation, where the max player \ufb01nds representation to maximize contrastiveness, and the min player puts weights on pairs of samples with similar representation. We show that the max player who does representation learning reduces to Principal Component Analysis for deep linear network, and almost all local minima are global, recovering optimal PCA solutions. Experiments show that the formulation yields comparable (or better) performance on CIFAR10 and STL-10 when extending beyond InfoNCE, resulting in novel contrastive losses. Furthermore, we extend our theoretical analysis to 2-layer ReLU networks, showing its sharp difference from linear ones, and proving that feature composition is preferred over picking single dominant feature under strong augmentation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1932187449",
                        "name": "Yuandong Tian"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "result"
            ],
            "contexts": [
                "Previous work [Chen and Li, 2020] states that the InfoNCE loss, a widely used objective function in contrastive learning [Chen et al., 2020; You et al., 2020], can not guarantee to avoid shortcut solutions that only capture easy-to-learn features.",
                "Previous work [Chen and Li, 2020] states that the InfoNCE loss, a widely used objective function in contrastive learning [Chen et al.",
                "However, recent studies [Tschannen et al., 2020; Chen and Li, 2020] have pointed out that there are gaps between the InfoMax principle and the performance of embeddings in the downstream tasks."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "77be1748040f231fdd8daaac44ff00dac4ce579d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-12821",
                    "DOI": "10.48550/arXiv.2203.12821",
                    "CorpusId": 247628270
                },
                "corpusId": 247628270,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/77be1748040f231fdd8daaac44ff00dac4ce579d",
                "title": "GraphCoCo: Graph Complementary Contrastive Learning",
                "abstract": "Graph Contrastive Learning (GCL) has shown promising performance in graph representation learning (GRL) without the supervision of man-ual annotations. GCL can generate graph-level embeddings by maximizing the Mutual Information (MI) between different augmented views of the same graph (positive pairs). However, we identify a obstacle that the optimization of InfoNCE loss only concentrates on a few embeddings dimensions, limiting the distinguishability of embeddings in downstream graph classi\ufb01cation tasks. This paper proposes an effective graph co mplementary co ntrastive learning approach named GraphCoCo to tackle the above issue. Speci\ufb01cally, we set the embedding of the \ufb01rst augmented view as the an-chor embedding to localize \u201chighlighted\u201d dimensions (i.e., the dimensions contribute most in similarity measurement). Then remove these dimensions in the embeddings of the second augmented view to discover neglected complementary representations. Therefore, the combination of an-chor and complementary embeddings signi\ufb01cantly improves the performance in downstream tasks. Comprehensive experiments on various benchmark datasets are conducted to demonstrate the effective-ness of GraphCoCo, and the results show that our model outperforms the state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2159889626",
                        "name": "Jiawei Sun"
                    },
                    {
                        "authorId": "3063894",
                        "name": "Junchi Yan"
                    },
                    {
                        "authorId": "3070171",
                        "name": "Chentao Wu"
                    },
                    {
                        "authorId": "2142355138",
                        "name": "Yue Ding"
                    },
                    {
                        "authorId": "1500550984",
                        "name": "Ruoxin Chen"
                    },
                    {
                        "authorId": "2112357961",
                        "name": "Xiang Yu"
                    },
                    {
                        "authorId": "2124827667",
                        "name": "Xinyu Lu"
                    },
                    {
                        "authorId": "49298718",
                        "name": "Jie Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In CPCL-A, the first term is the common alignment loss for augmented data and the second term is a form of uniformity loss [36, 37].",
                "[37] Ting Chen, Calvin Luo, and Lala Li."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e33f723e0efadf2fc49122021a4d6dd9b6ab894d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-00471",
                    "DOI": "10.48550/arXiv.2206.00471",
                    "CorpusId": 249240493
                },
                "corpusId": 249240493,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e33f723e0efadf2fc49122021a4d6dd9b6ab894d",
                "title": "Contrastive Principal Component Learning: Modeling Similarity by Augmentation Overlap",
                "abstract": "Traditional self-supervised contrastive learning methods learn embeddings by pulling views of the same sample together and pushing views of different samples away. Since views of a sample are usually generated via data augmentations, the semantic relationship between samples is ignored. Based on the observation that semantically similar samples are more likely to have similar augmentations, we propose to measure similarity via the distribution of augmentations, i.e, how much the augmentations of two samples overlap. To handle the dimensional and computational complexity, we propose a novel Contrastive Principal Component Learning (CPCL) method composed of a contrastive-like loss and an on-the-fly projection loss to efficiently perform PCA on the augmentation feature, which encodes the augmentation distribution. By CPCL, the learned low-dimensional embeddings theoretically preserve the similarity of augmentation distribution between samples. Empirical results show our method can achieve competitive results against various traditional contrastive learning methods on different benchmarks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112708270",
                        "name": "Lu Han"
                    },
                    {
                        "authorId": "2151459740",
                        "name": "Han-Jia Ye"
                    },
                    {
                        "authorId": "1721819",
                        "name": "De-chuan Zhan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[10], where it is referred to as the \u2018distribution\u2019 property."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "04743f3e681b789a58fc75443d294f02976f5c1f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-10862",
                    "DOI": "10.48550/arXiv.2207.10862",
                    "CorpusId": 251018399
                },
                "corpusId": 251018399,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/04743f3e681b789a58fc75443d294f02976f5c1f",
                "title": "On Higher Adversarial Susceptibility of Contrastive Self-Supervised Learning",
                "abstract": "Contrastive self-supervised learning (CSL) has managed to match or surpass the performance of supervised learning in image and video classification. However, it is still largely unknown if the nature of the representation induced by the two learning paradigms is similar. We investigate this under the lens of adversarial robustness. Our analytical treatment of the problem reveals intrinsic higher sensitivity of CSL over supervised learning. It identifies the uniform distribution of data representation over a unit hypersphere in the CSL representation space as the key contributor to this phenomenon. We establish that this increases model sensitivity to input perturbations in the presence of false negatives in the training data. Our finding is supported by extensive experiments for image and video classification using adversarial perturbations and other input corruptions. Building on the insights, we devise strategies that are simple, yet effective in improving model robustness with CSL training. We demonstrate up to 68% reduction in the performance gap between adversarially attacked CSL and its supervised counterpart. Finally, we contribute to robust CSL paradigm by incorporating our findings in adversarial self-supervised learning. We demonstrate an average gain of about 5% over two different state-of-the-art methods in this domain.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110003398",
                        "name": "Rohit Gupta"
                    },
                    {
                        "authorId": "47398812",
                        "name": "Naveed Akhtar"
                    },
                    {
                        "authorId": "1747500",
                        "name": "A. Mian"
                    },
                    {
                        "authorId": "2111863589",
                        "name": "M. Shah"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "However, as demonstrated in [2], such a contrastive loss suffers from the feature suppression problem where the model only learns the most important feature.",
                "However, current contrastive loss used in both the self-supervised methods and VLC methods suffered from the feature suppression problem [2]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0d90471b9aaa7c2fe1e226de3f08076cc1b75318",
                "externalIds": {
                    "DBLP": "conf/miccai/PanGGMMW22",
                    "DOI": "10.1007/978-3-031-16437-8_68",
                    "CorpusId": 252369633
                },
                "corpusId": 252369633,
                "publicationVenue": {
                    "id": "61a709e3-2060-423c-8de5-ffd3885aa31c",
                    "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                    "type": "conference",
                    "alternate_names": [
                        "Medical Image Computing and Computer-Assisted Intervention",
                        "MICCAI",
                        "Med Image Comput Comput Interv",
                        "Int Conf Med Image Comput Comput Interv"
                    ],
                    "url": "http://www.miccai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/0d90471b9aaa7c2fe1e226de3f08076cc1b75318",
                "title": "Vision-Language Contrastive Learning Approach to Robust Automatic Placenta Analysis Using Photographic Images",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2170050144",
                        "name": "Yimu Pan"
                    },
                    {
                        "authorId": "6936693",
                        "name": "A. Gernand"
                    },
                    {
                        "authorId": "145353639",
                        "name": "J. Goldstein"
                    },
                    {
                        "authorId": "5981960",
                        "name": "L. Mithal"
                    },
                    {
                        "authorId": "2185429443",
                        "name": "Delia Mwinyelle"
                    },
                    {
                        "authorId": "2116439170",
                        "name": "James Z. Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The models usually fail to distinguish textual similarity and semantic similarity, which has been discussed deeply in the vision field (Robinson et al., 2021; Chen et al., 2021)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "38c000c3c45cabe814e0d19ae3c45ee220947961",
                "externalIds": {
                    "ACL": "2022.ccl-1.67",
                    "CorpusId": 252763361
                },
                "corpusId": 252763361,
                "publicationVenue": {
                    "id": "0242a0a8-eac8-4d42-a284-4789a579aa9b",
                    "name": "China National Conference on Chinese Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "CCL",
                        "Constraint Comput Log",
                        "China National Conf Chin Comput Linguistics",
                        "Constraints in Computational Logics"
                    ],
                    "issn": "0319-0080",
                    "url": "http://ccl.uwinnipeg.ca/",
                    "alternate_urls": [
                        "http://ps-www.dfki.uni-sb.de/ccl/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/38c000c3c45cabe814e0d19ae3c45ee220947961",
                "title": "ConIsI: A Contrastive Framework with Inter-sentence Interaction for Self-supervised Sentence Representation",
                "abstract": "\u201cLearning sentence representation is a fundamental task in natural language processing and has been studied extensively. Recently, many works have obtained high-quality sentence representation based on contrastive learning from pre-trained models. However, these works suffer the inconsistency of input forms between the pre-training and fine-tuning stages. Also, they typically encode a sentence independently and lack feature interaction between sentences. To conquer these issues, we propose a novel Contrastive framework with Inter-sentence Interaction (ConIsI), which introduces a sentence-level objective to improve sentence representation based on contrastive learning by fine-grained interaction between sentences. The sentence-level objective guides the model to focus on fine-grained semantic information by feature interaction between sentences, and we design three different sentence construction strategies to explore its effect. We conduct experiments on seven Semantic Textual Similarity (STS) tasks. The experimental results show that our ConIsI models based on BERTbase and RoBERTabase achieve state-ofthe-art performance, substantially outperforming previous best models SimCSE-BERTbase and SimCSE-RoBERTabase by 2.05% and 0.77% respectively.\u201d",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2061298370",
                        "name": "Sun Meng"
                    },
                    {
                        "authorId": "2187216070",
                        "name": "Huang Degen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The first and the second terms in (2) relates to the alignment and distribution terms [3] and reassemble the formulation of the normalized temperature cross entropy loss (InfoNCE) [9, 1] up to a normalization constant."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "301de5742435701a56981fe1a6f98785036eaac3",
                "externalIds": {
                    "CorpusId": 253546705
                },
                "corpusId": 253546705,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/301de5742435701a56981fe1a6f98785036eaac3",
                "title": "Proof of Proposition 2 Recall",
                "abstract": "Note that the term qij can only take the values in {\u22121, 0, 1} as both matrices Ygt and Y \u2217 are binary. Consider the case when qij = 0, which indicates that the objects i and j are matched correctly and that [S]ij +m \u2264 [S]ik for any k \u0338= j. The latter implies that the distance between the positive pair (i, j) is at least m smaller than the distance with all other pairs in the batch. Next, note that the optimal value of the loss function L(Sm,Ygt) = 0 is achieved when all qij equate to zero, which implies that at the optimum, distances in all positives pairs are at least m smaller than the distances in all negative pairs. This optimality condition reassembles the formulation of margin Triplet loss [6]. \u25a1",
                "year": 2022,
                "authors": []
            }
        },
        {
            "intents": [],
            "contexts": [
                "M\u00f4 h\u00ecnh h\u1ecdc m\u00e1y \u0111\u1ea7u ti\u00ean b\u1ecb t\u1ea5n c\u00f4ng l\u00e0 M\u00e1y h\u1ecdc vect\u01a1 h\u1ed7 tr\u1ee3 [12], sau \u0111\u00f3, trong c\u00e1c t\u00e1c gi\u1ea3 ph\u00e1t hi\u1ec7n ra r\u1eb1ng NN c\u00f3 xu h\u01b0\u1edbng t\u1ea5n c\u00f4ng m\u1ed9t c\u00e1ch kh\u00e1 \u0111\u01a1n gi\u1ea3n b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng m\u1ed9t s\u1ed1 thu\u1eadt to\u00e1n d\u1ef1a tr\u00ean gradient [13], b\u1eb1ng c\u00e1ch thu th\u1eadp th\u00f4ng tin gradient c\u1ee7a h\u00ecnh \u1ea3nh b\u1ecb t\u1ea5n c\u00f4ng v\u00e0 do \u0111\u00f3 \u0111\u00e1nh l\u1eeba m\u00f4 h\u00ecnh."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "04340fb066c33a76257d593b246af13cdbfda021",
                "externalIds": {
                    "CorpusId": 259768540
                },
                "corpusId": 259768540,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/04340fb066c33a76257d593b246af13cdbfda021",
                "title": "ADVERSARIAL ATTACKS INTO DEEP LEARNING MODELS USING PIXEL TRANFORMATION",
                "abstract": "ABSTRACT",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2177343762",
                        "name": "Tr\u01b0\u01a1ng Phi H\u1ed3"
                    },
                    {
                        "authorId": "2177210574",
                        "name": "Ho\u00e0ng Thanh Nam"
                    },
                    {
                        "authorId": "47327891",
                        "name": "Tran Quang Tuan"
                    },
                    {
                        "authorId": "2222853782",
                        "name": "Ph\u1ea1m Minh Thu\u1ea5n"
                    },
                    {
                        "authorId": "1471372099",
                        "name": "Pham Duy Trung"
                    },
                    {
                        "authorId": "2222772943",
                        "name": "Tr\u01a3\u01a1ng Phi H\u1ed3"
                    },
                    {
                        "authorId": "2177210574",
                        "name": "Ho\u00e0ng Thanh Nam"
                    },
                    {
                        "authorId": "47327891",
                        "name": "Tran Quang Tuan"
                    },
                    {
                        "authorId": "2222853782",
                        "name": "Ph\u1ea1m Minh Thu\u1ea5n"
                    },
                    {
                        "authorId": "1471372099",
                        "name": "Pham Duy Trung"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "Note that SimCLR style loss functions have been shown to lead to \"linearly separable\" representations [20] and hence aligns well with the clustering objective [55, 10].",
                "We make several observations: 1) Even without using any attribute information, our method performs significantly better as compared to other structure-only based methods like Spectral Clustering and Node2Vec, which demonstrates the effectiveness of our loss formulation and training methodology that promotes clusterability, which is also in line with recent observations [10, 55]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "13065b7f4525d743f2856b8b258480176f60b932",
                "externalIds": {
                    "DBLP": "conf/nips/DevvritSD022",
                    "CorpusId": 258509070
                },
                "corpusId": 258509070,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/13065b7f4525d743f2856b8b258480176f60b932",
                "title": "S3GC: Scalable Self-Supervised Graph Clustering",
                "abstract": "We study the problem of clustering graphs with additional side-information of node features. The problem is extensively studied, and several existing methods exploit Graph Neural Networks to learn node representations [29]. However, most of the existing methods focus on generic representations instead of their cluster-ability or do not scale to large scale graph datasets. In this work, we propose S 3 GC which uses contrastive learning along with Graph Neural Networks and node features to learn clusterable features. We empirically demonstrate that S 3 GC is able to learn the correct cluster structure even when graph information or node features are individually not informative enough to learn correct clusters. Finally, using extensive evaluation on a variety of benchmarks, we demonstrate that S 3 GC is able to signi\ufb01cantly outperform state-of-the-art methods in terms of clustering accuracy \u2013 with as much as 5% gain in NMI \u2013 while being scalable to graphs of size 100M.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1477624640",
                        "name": "Fnu Devvrit"
                    },
                    {
                        "authorId": "2151613320",
                        "name": "Aditya Sinha"
                    },
                    {
                        "authorId": "1783667",
                        "name": "I. Dhillon"
                    },
                    {
                        "authorId": "48964143",
                        "name": "Prateek Jain"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "contexts": [
                "\u2026popularity of discriminative unsupervised representational learning, specifically contrastive methods, has sparked keen interest in the theoretical understanding of their underpinnings, emerging from their performance rivaling that of supervised methodologies [Chen et al., 2020a; He et al., 2020].",
                "\u2026contrastive methods [Caron et al., 2020] have been proposed alleviating some of the computational drawbacks associated to contrastive losses, primarily the necessity of large numbers of negative pairs generally requiring increased batch sizes [Chen et al., 2020a] or memory banks [He et al., 2020].",
                "For all three datasets we omit the Gaussian blur and solarization as described in [Chen et al.,\n2020a].",
                "As to correspond with the BYOL procedure, we employ the same image augmentations as described in [Chen et al., 2020a; Grill et al., 2020].",
                "M = 2B \u2212 1 in [Chen et al., 2020a] whereB is the batch size.",
                "This 0.4% improvement in CIFAR-10 is comparable to the improvements found between SimCLR and BYOL, a substantial move towards the supervised baseline of 95.1% reported in [Chen et al., 2020a].",
                "Unsupervised visual representational learning methods [Chen et al., 2020a; Grill et al., 2020] have recently demonstrated performance on downstream tasks that continues to narrow the gap to supervised pre-training, excelling specifically in classification and few-shot learning tasks [Caron et al.,\u2026",
                "To evaluate the quality of representations learned during selfsupervised training we employ the standard linear evaluation protocol described in [Chen et al., 2020a; Grill et al., 2020].",
                "Augmentation procedure is key to the success of selfsupervised learning, therefore to compare our performance against BYOL, we employ the same image augmentations reported in [Grill et al., 2020; Chen et al., 2020a]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "cb081a561a756c56c58a85a213117d166fcfc66d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-00925",
                    "CorpusId": 233481108
                },
                "corpusId": 233481108,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cb081a561a756c56c58a85a213117d166fcfc66d",
                "title": "Hyperspherically Regularized Networks for BYOL Improves Feature Uniformity and Separability",
                "abstract": "Bootstrap Your Own Latent (BYOL) introduced an approach to self-supervised learning avoiding the contrastive paradigm and subsequently remov-ing the computational burden of negative sampling. However, feature representations under this paradigm are poorly distributed on the surface of the unit-hypersphere representation space compared to contrastive methods. This work empirically demonstrates that feature diversity enforced by contrastive losses is bene\ufb01cial when employed in BYOL, and as such, provides greater inter-class feature separability. Therefore to achieve a more uniform distribution of features, we advocate the minimization of hyperspherical energy (i.e. maximization of entropy) in BYOL network weights. We show that directly optimizing a measure of uniformity alongside the standard loss, or regularizing the networks of the BYOL architecture to minimize the hyperspherical energy of neurons can produce more uniformly distributed and better performing representations for downstream tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1392810833",
                        "name": "A. Durrant"
                    },
                    {
                        "authorId": "2042765174",
                        "name": "G. Leontidis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Past work addresses this problem by designing handcrafted data augmentations to break such shortcuts, so that the network may learn the relevant information [18, 6, 7, 9, 8]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "82a75a2b57105271ec08ca759b07275befbe1ea5",
                "externalIds": {
                    "CorpusId": 236912821
                },
                "corpusId": 236912821,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/82a75a2b57105271ec08ca759b07275befbe1ea5",
                "title": "Making Contrastive Learning Robust to Shortcuts",
                "abstract": "Contrastive learning is effective at learning useful representations without supervision. Yet contrastive learning is susceptible to shortcuts \u2013 i.e., it may learn shortcut features irrelevant to the downstream task and discard relevant information. Past work has addressed this limitation via handcrafted data augmentations that eliminate the shortcut. However, handcrafted augmentations are infeasible for data modalities that are not interpretable by humans (e.g., radio signals). Further, even when the modality is interpretable (e.g., RGB), sometimes eliminating the shortcut information may be undesirable. For example, in multi-attribute classification, information related to one attribute may act as a shortcut around other attributes. This paper presents reconstructive contrastive learning (RCL), a framework for learning unsupervised representations that are robust to shortcuts. The key idea is to force the learned representation to reconstruct the input, which naturally counters potential shortcuts. Extensive experiments verify that RCL is highly robust to shortcuts and outperforms state-of-the-art contrastive learning methods on both RGB and RF datasets for a variety of tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118909373",
                        "name": "Tianhong Li"
                    },
                    {
                        "authorId": "2548303",
                        "name": "Lijie Fan"
                    },
                    {
                        "authorId": "46499812",
                        "name": "Yuan Yuan"
                    },
                    {
                        "authorId": "153168451",
                        "name": "Hao He"
                    },
                    {
                        "authorId": "2476765",
                        "name": "Yonglong Tian"
                    },
                    {
                        "authorId": "1785714",
                        "name": "D. Katabi"
                    },
                    {
                        "authorId": "1723233",
                        "name": "R. Feris"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "result"
            ],
            "contexts": [
                "Chen & Li (2020) have since systematically demonstrated that the suppression of certain features by the presence or emphasis (e.g. through augmentations) of others is a demonstrably important influence on the learned representation and its usefulness for downstream tasks.",
                "This is in line with the findings of Chen & Li (2020), who have studied contrastive learning with artificial data-sets, for which independent features can be specifically controlled, and summarize that \"a few bits of easy-to-learn features could suppress, or even fully prevent, the learning of\u2026",
                "Moreover, the choice of which inputs are contrasted is highly influential on the learned representations and consequently their usefulness for a given downstream task (Chen & Li, 2020)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "85c7271a5be9569ad8880085bf07216b76a9fca0",
                "externalIds": {
                    "CorpusId": 249319706
                },
                "corpusId": 249319706,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/85c7271a5be9569ad8880085bf07216b76a9fca0",
                "title": "Learning Task-Relevant Representations with Selective Contrast for Reinforcement Learning in a Real-World Application",
                "abstract": "We use contrastive learning to obtain task-relevant state-representations from images for reinforcement learning in a real-world system. To test the quality of the representations, an agent is trained with reinforcement learning in the Neuro-Slot-Car environment (Kietzmann & Riedmiller, 2009; Lange et al., 2012). In our experiments, we restrict the distribution from which samples are drawn for comparison in the contrastive loss. Our results show, that the choice of sampling distribution for negative samples is essential to allow task-relevant features to be represented in the presence of more prevalent, but irrelevant features. This adds to recent research on feature suppression and feature invariance in contrastive representation learning. With the training of the reinforcement learning agent, we present to our knowledge a \ufb01rst approach of using contrastive learning of state-representations for control in a real-world environment, using only images from one static camera.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51037438",
                        "name": "Flemming Brieger"
                    },
                    {
                        "authorId": "2354563",
                        "name": "Daniel A. Braun"
                    },
                    {
                        "authorId": "144202584",
                        "name": "S. Lange"
                    }
                ]
            }
        }
    ]
}