{
    "offset": 0,
    "data": [
        {
            "isInfluential": false,
            "contexts": [
                "Work [7], [8] investigated optimal fair raking algorithm to solve online selection problems where decisions are often biased like hiring and credit risk estimating."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a05f836b5da73d4d8a62fe41011e9462bc197d0c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-04167",
                    "ArXiv": "2306.04167",
                    "DOI": "10.48550/arXiv.2306.04167",
                    "CorpusId": 259095515
                },
                "corpusId": 259095515,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a05f836b5da73d4d8a62fe41011e9462bc197d0c",
                "title": "Fairness-Sensitive Policy-Gradient Reinforcement Learning for Reducing Bias in Robotic Assistance",
                "abstract": "Robots assist humans in various activities, from daily living public service (e.g., airports and restaurants), and to collaborative manufacturing. However, it is risky to assume that the knowledge and strategies robots learned from one group of people can apply to other groups. The discriminatory performance of robots will undermine their service quality for some people, ignore their service requests, and even offend them. Therefore, it is critically important to mitigate bias in robot decision-making for more fair services. In this paper, we designed a self-reflective mechanism -- Fairness-Sensitive Policy Gradient Reinforcement Learning (FSPGRL), to help robots to self-identify biased behaviors during interactions with humans. FSPGRL identifies bias by examining the abnormal update along particular gradients and updates the policy network to support fair decision-making for robots. To validate FSPGRL's effectiveness, a human-centered service scenario,\"A robot is serving people in a restaurant,\"was designed. A user study was conducted; 24 human subjects participated in generating 1,000 service demonstrations. Four commonly-seen issues\"Willingness Issue,\"\"Priority Issue,\"\"Quality Issue,\"\"Risk Issue\"were observed from robot behaviors. By using FSPGRL to improve robot decisions, robots were proven to have a self-bias detection capability for a more fair service. We have achieved the suppression of bias and improved the quality during the process of robot learning to realize a relatively fair model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2156569119",
                        "name": "Jie Zhu"
                    },
                    {
                        "authorId": "40653345",
                        "name": "Mengsha Hu"
                    },
                    {
                        "authorId": "2143594515",
                        "name": "Xueyao Liang"
                    },
                    {
                        "authorId": "2111672235",
                        "name": "Amy Zhang"
                    },
                    {
                        "authorId": "2146543545",
                        "name": "Ruoming Jin"
                    },
                    {
                        "authorId": "2186804488",
                        "name": "Rui Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                ", 2018b], online selection [Correa et al., 2021], and graph problems [Rahmattalabi et al.",
                "\u2026voting [Celis et al., 2018a], matching [Chierichetti et al., 2019], influence maximization [Tsang et al., 2019, Rahmattalabi et al., 2021], data summarization [Celis et al., 2018b], online selection [Correa et al., 2021], and graph problems [Rahmattalabi et al., 2019, Anagnostopoulos et al., 2020]."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8db90d950f73f664f4deacdebe426e48ea4a646b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-15118",
                    "ArXiv": "2305.15118",
                    "DOI": "10.48550/arXiv.2305.15118",
                    "CorpusId": 258866221
                },
                "corpusId": 258866221,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8db90d950f73f664f4deacdebe426e48ea4a646b",
                "title": "Fairness in Streaming Submodular Maximization over a Matroid Constraint",
                "abstract": "Streaming submodular maximization is a natural model for the task of selecting a representative subset from a large-scale dataset. If datapoints have sensitive attributes such as gender or race, it becomes important to enforce fairness to avoid bias and discrimination. This has spurred significant interest in developing fair machine learning algorithms. Recently, such algorithms have been developed for monotone submodular maximization under a cardinality constraint. In this paper, we study the natural generalization of this problem to a matroid constraint. We give streaming algorithms as well as impossibility results that provide trade-offs between efficiency, quality and fairness. We validate our findings empirically on a range of well-known real-world applications: exemplar-based clustering, movie recommendation, and maximum coverage in social networks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2845064",
                        "name": "Marwa El Halabi"
                    },
                    {
                        "authorId": "1616150925",
                        "name": "Federico Fusco"
                    },
                    {
                        "authorId": "1398893199",
                        "name": "A. Norouzi-Fard"
                    },
                    {
                        "authorId": "90777210",
                        "name": "Jakab Tardos"
                    },
                    {
                        "authorId": "2220760",
                        "name": "Jakub Tarnawski"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [],
            "intents": [],
            "citingPaper": {
                "paperId": "5c03aa6fb4680b55e61e9de730df0e0a1948c545",
                "externalIds": {
                    "ArXiv": "2305.13293",
                    "DBLP": "journals/corr/abs-2305-13293",
                    "DOI": "10.48550/arXiv.2305.13293",
                    "CorpusId": 258833272
                },
                "corpusId": 258833272,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5c03aa6fb4680b55e61e9de730df0e0a1948c545",
                "title": "Time Fairness in Online Knapsack Problems",
                "abstract": "The online knapsack problem is a classic problem in the field of online algorithms. Its canonical version asks how to pack items of different values and weights arriving online into a capacity-limited knapsack so as to maximize the total value of the admitted items. Although optimal competitive algorithms are known for this problem, they may be fundamentally unfair, i.e., individual items may be treated inequitably in different ways. Inspired by recent attention to fairness in online settings, we develop a natural and practically-relevant notion of time fairness for the online knapsack problem, and show that the existing optimal algorithms perform poorly under this metric. We propose a parameterized deterministic algorithm where the parameter precisely captures the Pareto-optimal trade-off between fairness and competitiveness. We show that randomization is theoretically powerful enough to be simultaneously competitive and fair; however, it does not work well in practice, using trace-driven experiments. To further improve the trade-off between fairness and competitiveness, we develop a fair, robust (competitive), and consistent learning-augmented algorithm with substantial performance improvement in trace-driven experiments.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2135139454",
                        "name": "Adam Lechowicz"
                    },
                    {
                        "authorId": "2243189",
                        "name": "Rik Sengupta"
                    },
                    {
                        "authorId": "145986708",
                        "name": "Bo Sun"
                    },
                    {
                        "authorId": "34399700",
                        "name": "Shahin Kamali"
                    },
                    {
                        "authorId": "2841354",
                        "name": "M. Hajiesmaili"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "contexts": [
                "Similarly to [10], Salem and Gupta [35] analyze a poset-secretary problem, in which a partially ordered set is revealed sequentially and the goal is to select k-candidates with the highest score.",
                "A closely related prophet problem of the online multi-group selection has been studied in [5] and [10].",
                "[10] Jose Correa, Andres Cristi, Paul Duetting, and Ashkan Norouzi-Fard.",
                "[10] introduce a multi-color secretary problem.",
                "To tackle this issue, several authors have explored variants of the secretary problem with noisy/biased observations of the ranks [10, 35, 16]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "ea00e1bb9b539a0b68b176a26c87bb43f9042094",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-09205",
                    "ArXiv": "2303.09205",
                    "DOI": "10.48550/arXiv.2303.09205",
                    "CorpusId": 257557341
                },
                "corpusId": 257557341,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ea00e1bb9b539a0b68b176a26c87bb43f9042094",
                "title": "Addressing bias in online selection with limited budget of comparisons",
                "abstract": "Consider a hiring process with candidates coming from different universities. It is easy to order candidates who have the same background, yet it can be challenging to compare them otherwise. The latter case requires additional costly assessments and can result in sub-optimal hiring decisions. Given an assigned budget, what would be an optimal strategy to select the most qualified candidate? We model the above problem by introducing a new variant of the secretary problem in which sequentially observed candidates are split into two distinct groups. For each new candidate, the decision maker observes its rank among already seen candidates from the same group and can access its rank among all observed candidates at some fixed cost. To tackle this new problem, we introduce and study the family of Dynamic Double Threshold (DDT) algorithms. We show that, with well-chosen parameters, their success probability converges rapidly to 1/e as the budget grows, recovering the optimal success probability from the usual secretary problem. Finally, focusing on the class of memory-less algorithms, we propose an optimal algorithm in the non-asymptotic regime and show that it belongs to the DDT family when the number of candidates is large.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2114204991",
                        "name": "Ziyad Benomar"
                    },
                    {
                        "authorId": "9958440",
                        "name": "Evgenii Chzhen"
                    },
                    {
                        "authorId": "87914462",
                        "name": "Nicolas Schreuder"
                    },
                    {
                        "authorId": "3087994",
                        "name": "Vianney Perchet"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "Similar insights are obtained when the distributions are identical; this problem was initially studied by Correa et al. [2019c] and subsequently improved several times [Rubinstein et al., 2020, Correa et al., 2020b, Kaplan et al., 2020, Correa et al., 2020a, 2021b].",
                "Alternative measures of performance have also been proposed to capture the behavior of biased (as opposed to rational) agents [Kleinberg et al., 2021], or to address fairness considerations [Correa et al., 2021a]."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "90598fa6e47683cff602a4ef027e61fb3cd0302e",
                "externalIds": {
                    "ArXiv": "2207.03361",
                    "DBLP": "journals/corr/abs-2207-03361",
                    "DOI": "10.48550/arXiv.2207.03361",
                    "CorpusId": 250334759
                },
                "corpusId": 250334759,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/90598fa6e47683cff602a4ef027e61fb3cd0302e",
                "title": "Prophet Inequalities via the Expected Competitive Ratio",
                "abstract": "We consider prophet inequalities under general downward-closed constraints. In a prophet inequality problem, a decision-maker sees a series of online elements and needs to decide immediately and irrevocably whether or not to select each element upon its arrival, subject to an underlying feasibility constraint. Traditionally, the decision-maker's expected performance has been compared to the expected performance of the prophet, i.e., the expected offline optimum. We refer to this measure as the Ratio of Expectations (or, in short, RoE). However, a major limitation of the RoE measure is that it only gives a guarantee against what the optimum would be on average, while, in theory, algorithms still might perform poorly compared to the realized ex-post optimal value. Hence, we study alternative performance measures. In particular, we suggest the Expected Ratio (or, in short, EoR), which is the expectation of the ratio between the value of the algorithm and the value of the prophet. This measure yields desirable guarantees, e.g., a constant EoR implies achieving a constant fraction of the ex-post offline optimum with constant probability. Moreover, in the single-choice setting, we show that the EoR is equivalent (in the worst case) to the probability of selecting the maximum, a well-studied measure in the literature. This is no longer the case for combinatorial constraints (beyond single-choice), which is the main focus of this paper. Our main goal is to understand the relation between RoE and EoR in combinatorial settings. Specifically, we establish a two-way black-box reduction: for every feasibility constraint, the RoE and the EoR are at most a constant factor apart. This implies a wealth of EoR results in multiple settings where RoE results are known.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "14812577",
                        "name": "Tomer Ezra"
                    },
                    {
                        "authorId": "144954554",
                        "name": "S. Leonardi"
                    },
                    {
                        "authorId": "2835986",
                        "name": "Rebecca Reiffenh\u00e4user"
                    },
                    {
                        "authorId": "32680837",
                        "name": "M. Russo"
                    },
                    {
                        "authorId": "1402935456",
                        "name": "Alexandros Tsigonias-Dimitriadis"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [],
            "intents": [],
            "citingPaper": {
                "paperId": "360eec043e96da4a60a968bb51cd7ba6c1f7c093",
                "externalIds": {
                    "DBLP": "conf/sigecom/ArsenisK22",
                    "ArXiv": "2205.10302",
                    "DOI": "10.48550/arXiv.2205.10302",
                    "CorpusId": 248964987
                },
                "corpusId": 248964987,
                "publicationVenue": {
                    "id": "3c2dd17d-6c11-4bb6-9d01-1fc2064cb8df",
                    "name": "ACM Conference on Economics and Computation",
                    "type": "conference",
                    "alternate_names": [
                        "Economics and Computation",
                        "Electron Commer",
                        "EC",
                        "ACM Conf Econ Comput",
                        "Econ Comput",
                        "Electronic Commerce"
                    ],
                    "url": "http://www.acm.org/sigecom/"
                },
                "url": "https://www.semanticscholar.org/paper/360eec043e96da4a60a968bb51cd7ba6c1f7c093",
                "title": "Individual Fairness in Prophet Inequalities",
                "abstract": "Prophet inequalities are performance guarantees for online algorithms (a.k.a. stopping rules) solving the following ''hiring problem'': a decision maker sequentially inspects candidates whose values are independent random numbers and is asked to hire at most one candidate by selecting it before inspecting the values of future candidates in the sequence. A classic result in optimal stopping theory asserts that there exist stopping rules guaranteeing that the decision maker will hire a candidate whose expected value is at least half as good as the expected value of the candidate hired by a ''prophet,'' i.e.one who has simultaneous access to the realizations of all candidates' values. Such stopping rules may indeed have provably good performance but might treat individual candidates unfairly in a number of different ways. In this work we identify two types of individual fairness that might be desirable in optimal stopping problems. We call them identity-independent fairness (IIF) and time-independent fairness (TIF) and give precise definitions in the context of the hiring problem. We give polynomial-time algorithms for finding the optimal IIF/TIF stopping rules for a given instance with discrete support and we manage to recover a prophet inequality with factor 1/2 when the decision maker's stopping rule is required to satisfy both fairness properties while the prophet is unconstrained. We also explore worst-case ratios between optimal selection rules in the presence vs. absence of individual fairness constraints, in both the online and offline settings. We prove an impossibility result showing that there is no prophet inequality with a non-zero factor for either IIF or TIF stopping rules when we further constrain the decision maker to make a hire with probability 1. We finally consider a setting in which the decision maker doesn't know the distributions of candidates' values but has access to a bounded number of independent samples from each distribution. We provide constant-competitive algorithms that satisfy both TIF and IIF, using one sample from each distribution in the offline setting and two samples from each distribution in the online setting. The full version of the paper: https://arxiv.org/abs/2205.10302v1",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1518618282",
                        "name": "Makis Arsenis"
                    },
                    {
                        "authorId": "2633757",
                        "name": "Robert D. Kleinberg"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "While fairness in supervised learning is well-studied (Dwork et al. 2012; Correa et al. 2021; Chikahara et al. 2021; Lee et al. 2021; Mehrabi et al. 2021; Le Quy et al. 2022), fairness in unsupervised learning is still in its formative stages (Deepak et al."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7060df1d6b30d4586579a3e5abeba497b818d4ef",
                "externalIds": {
                    "DBLP": "journals/datamine/GuptaGKJ23",
                    "ArXiv": "2109.00708",
                    "DOI": "10.1007/s10618-023-00928-6",
                    "CorpusId": 237385198
                },
                "corpusId": 237385198,
                "publicationVenue": {
                    "id": "d263025a-9eaf-443f-9bbf-72377e8d22a6",
                    "name": "Data mining and knowledge discovery",
                    "type": "journal",
                    "alternate_names": [
                        "Data Mining and Knowledge Discovery",
                        "Data Min Knowl Discov",
                        "Data min knowl discov"
                    ],
                    "issn": "1384-5810",
                    "url": "https://www.springer.com/computer/database+management+&+information+retrieval/journal/10618",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10618",
                        "http://www.springer.com/computer/database+management+&+information+retrieval/journal/10618"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7060df1d6b30d4586579a3e5abeba497b818d4ef",
                "title": "Efficient algorithms for fair clustering with a new notion of fairness",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118927614",
                        "name": "Shivam Gupta"
                    },
                    {
                        "authorId": "3402720",
                        "name": "Ganesh Ghalme"
                    },
                    {
                        "authorId": "2503137",
                        "name": "N. C. Krishnan"
                    },
                    {
                        "authorId": "8126815",
                        "name": "Shweta Jain"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [],
            "intents": [],
            "citingPaper": {
                "paperId": "9e282ffd5617cd6c8f204c513c9409dcc0026663",
                "externalIds": {
                    "DOI": "10.2139/ssrn.4497747",
                    "CorpusId": 259348970
                },
                "corpusId": 259348970,
                "publicationVenue": {
                    "id": "75d7a8c1-d871-42db-a8e4-7cf5146fdb62",
                    "name": "Social Science Research Network",
                    "type": "journal",
                    "alternate_names": [
                        "SSRN, Social Science Research Network (SSRN) home page",
                        "SSRN Electronic Journal",
                        "Soc Sci Res Netw",
                        "SSRN",
                        "SSRN Home Page",
                        "SSRN Electron J",
                        "Social Science Electronic Publishing presents Social Science Research Network"
                    ],
                    "issn": "1556-5068",
                    "url": "http://www.ssrn.com/",
                    "alternate_urls": [
                        "www.ssrn.com/",
                        "https://fatcat.wiki/container/tol7woxlqjeg5bmzadeg6qrg3e",
                        "https://www.wikidata.org/wiki/Q53949192",
                        "www.ssrn.com/en",
                        "http://www.ssrn.com/en/",
                        "http://umlib.nl/ssrn",
                        "umlib.nl/ssrn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9e282ffd5617cd6c8f204c513c9409dcc0026663",
                "title": "Redesigning VolunteerMatch's Ranking Algorithm: Toward More Equitable Access to Volunteers",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2134716135",
                        "name": "Vahideh H. Manshadi"
                    },
                    {
                        "authorId": "51979289",
                        "name": "Scott Rodilitz"
                    },
                    {
                        "authorId": "1941466",
                        "name": "D. Sab\u00e1n"
                    },
                    {
                        "authorId": "29943364",
                        "name": "Akshaya Suresh"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [],
            "intents": [],
            "citingPaper": {
                "paperId": "4e90482dce29d55bf5fecf4193417b2a9b303b98",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-00708",
                    "CorpusId": 260972980
                },
                "corpusId": 260972980,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4e90482dce29d55bf5fecf4193417b2a9b303b98",
                "title": "Efficient Algorithms For Fair Clustering with a New Fairness Notion",
                "abstract": "We revisit the problem of fair clustering, first introduced by Chierichetti et al. (2017) , that requires each protected attribute to have approximately equal representation in every cluster; i.e., a Balance property. Existing solutions to fair clustering are either not scalable or do not achieve an optimal trade-off between clustering objective and fairness. In this paper, we propose a new notion of fairness, which we call \u03c4 -ratio fairness, that strictly generalizes the Balance property and enables a finegrained efficiency vs. fairness trade-off. Furthermore, we show that simple greedy round-robin based algorithms achieve this trade-off efficiently. Under a more general setting of multi-valued protected attributes, we rigorously analyze the theoretical properties of the our algorithms. Our experimental results suggest that the proposed solution outperforms all the state-of-the-art algorithms and works exceptionally well even for a large number of clusters.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118927614",
                        "name": "Shivam Gupta"
                    },
                    {
                        "authorId": "3402720",
                        "name": "Ganesh Ghalme"
                    },
                    {
                        "authorId": "2503137",
                        "name": "N. C. Krishnan"
                    },
                    {
                        "authorId": "2116971410",
                        "name": "Shweta Jain"
                    }
                ]
            }
        }
    ]
}